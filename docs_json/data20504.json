{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.24448"}, {"@name": "filename", "#text": "8833_marquesjunior_lc_me_bauru.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE ESTADUAL PAULISTA \n\nFACULDADE DE ENGENHARIA DE BAURU \n\n \n\n \n\n \n\n \n\nLUIZ CARLOS MARQUES JUNIOR \n\n \n\n \n\n \n\n \n\n \n\nCLASSIFICA\u00c7\u00c3O DE PLANTAS DANINHAS EM BANCO DE \n\nIMAGENS UTILIZANDO REDES NEURAIS \n\nCONVOLUCIONAIS \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nBAURU \n\n2019 \n\n \n\n\n\n \n\n \n\nLUIZ CARLOS MARQUES JUNIOR \n\n \n\n \n\n \n\n \n\n \n\nCLASSIFICA\u00c7\u00c3O DE PLANTAS DANINHAS EM BANCO DE \n\nIMAGENS UTILIZANDO REDES NEURAIS \n\nCONVOLUCIONAIS \n\n \n\n \n\n \n\n \n\n \n\nDisserta\u00e7\u00e3o apresentada como requisito \u00e0 obten\u00e7\u00e3o \n\ndo t\u00edtulo de mestre em Engenharia El\u00e9trica, pelo \n\nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Engenharia \n\nEl\u00e9trica, da Faculdade de Engenharia de Bauru, da \n\nUniversidade Estadual Paulista. \n\n \n\n \n\n \n\nOrientador: Prof. Dr. Jos\u00e9 Alfredo Covolan Ulson \n\n \n\n \n\n \n\n \n\nBAURU \n\n2019 \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n                  \n          \n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\n \n\n\n\n \n\nAGRADECIMENTOS \n\n \n\nGostaria de agradecer primeiramente a Deus, por ter me capacitado e permitido \n\na execu\u00e7\u00e3o desta pesquisa que culminou nesta tese. \n\n A minha esposa pelo apoio, compreens\u00e3o e companheirismo em todos os \n\nmomentos.  \n\n A fam\u00edlia, em especial aos meus pais, por terem me dado a possibilidade e por \n\nter me apoiado nos estudos. \n\nAo   meu orientador Prof. Dr. Jos\u00e9 Alfredo Covolan Ulson, pelo apoio, tempo \n\ndedicado a este trabalho e confian\u00e7a depositada em mim para elabora\u00e7\u00e3o  desta \n\npesquisa. \n\nA Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel Superior pela bolsa \n\nde mestrado possibilitando a oportunidade de aprofundar meus conhecimentos e \n\ncontribuir para o cen\u00e1rio da ci\u00eancia e tecnologia do Brasil. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nRESUMO  \n\n \n\nAs esp\u00e9cies ex\u00f3ticas invasoras, tamb\u00e9m conhecidas como plantas daninhas, \n\ncompetem por recursos, como sol, \u00e1gua e nutrientes paralelamente a cultura plantada, \n\nimpondo preju\u00edzos econ\u00f4micos ao agricultor. Para minimizar este problema, \n\natualmente os agricultores fazem uso de herbicidas para a elimina\u00e7\u00e3o e/ou controle \n\ndas plantas daninhas. O uso de herbicidas depara-se com problemas: i) algumas \n\nplantas daninhas s\u00e3o resistentes a aplica\u00e7\u00e3o de herbicidas e, ii) quando aplicados em \n\ndemasia pode-se ter a contamina\u00e7\u00e3o da cultura plantada, do len\u00e7ol fre\u00e1tico e dos \n\nmananciais como rios e lagos. Nesse contexto, visando o desenvolvimento de \n\nferramentas que permitam a minimiza\u00e7\u00e3o do emprego de herbicidas, novas \n\nabordagens que fazem uso de vis\u00e3o computacional e intelig\u00eancia artificial aparecem \n\ncomo solu\u00e7\u00f5es promissoras, agregando novas ferramentas a agricultura de precis\u00e3o. \n\nDentre essas solu\u00e7\u00f5es destaca-se o aprendizado profundo (do ingl\u00eas Deep Learning), \n\nque utiliza as redes neurais convolucionais para extrair caracter\u00edsticas relevantes, \n\nprincipalmente em imagens, dessa maneira, permite por exemplo a identifica\u00e7\u00e3o e a \n\nclassifica\u00e7\u00e3o de plantas daninhas, o que possibilita ao agricultor optar tanto pela \n\nelimina\u00e7\u00e3o mec\u00e2nica da planta daninha quanto a aplica\u00e7\u00e3o localizada de herbicidas e \n\nem quantidades adequadas. A partir deste desafio que \u00e9 a correta classifica\u00e7\u00e3o de \n\ndiferentes esp\u00e9cies de plantas daninhas, especialmente plantas resistentes aos \n\nherbicidas comerciais, o objetivo deste trabalho foi aplicar e comparar a performance \n\nde quatro  arquiteturas de redes neurais convolucionais para a classifica\u00e7\u00e3o de plantas \n\ndaninhas de cinco esp\u00e9cies contidas em um banco de imagens desenvolvido para \n\neste trabalho. Para isso foi realizado o treinamento e a classifica\u00e7\u00e3o das esp\u00e9cies nas \n\nseguintes arquiteturas de redes neurais convolucionais: VGG16, ResNet50, \n\nInceptionV3 e InceptionResNetV2 com 20 \u00e9pocas de treinamento. Os resultados \n\nindicam que a arquitetura InceptionV3 apresenta o melhor desempenho, com 84,73% \n\nde exatid\u00e3o na classifica\u00e7\u00e3o nas cinco esp\u00e9cies, seguida pela arquitetura \n\nInceptionResNetV2 com 82,87%, VGG16 com 80,60%. A arquitetura ResNet50 \n\nobteve o pior resultado com 20,00% de exatid\u00e3o, a rede InceptionV3  foi treinada \n\nnovamente com 40 \u00e9pocas, obtendo 88,50%de exatid\u00e3o. \n\n \n\nPalavras-chave: Agricultura de Precis\u00e3o, Redes Neurais Convolucionais, \n\nAprendizado Profundo, Plantas Daninhas. \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nABSTRACT \n\n \n\nExotic invasive species, also known as weeds, compete for resources such as \n\nsun, water and nutrients in parallel with the planted crop, imposing economic losses to \n\nthe farmer. To minimize this problem, farmers are currently using herbicides for the \n\nelimination and / or control of weeds.The use of herbicides has problems: i) some \n\nweeds are resistant to the application of herbicides and ii) when applied too much can \n\ncontaminate the planted crop, groundwater and springs such as rivers and lakes. In \n\nthis context, aiming at developing tools to minimize the use of herbicides, new \n\napproaches that make use of computer vision and artificial intelligence appear as \n\npromising solutions, adding new tools to precision agriculture. Among these solutions \n\nare the Deep Learning, which uses the convolutional neural networks to extract \n\nrelevant features, mainly in images, thus, allows for example the identification and \n\nclassification of weeds, which enables the farmer to opt for the mechanical elimination \n\nof the weeds as well as the localized application of herbicides and in adequate \n\nquantities. From this challenge, which is the correct classification of different weed \n\nspecies, especially plants resistant to commercial herbicides, the objective of this study \n\nwas to apply and compare the performance of four architectures of convolutional \n\nneural networks for classification of weed five species contained in an image bank \n\ndeveloped for this work. The training and classification of the species were carried out \n\nin the following convolutional neural network architectures: VGG16, ResNet50, \n\nInceptionV3 and InceptionResNetV2 with 20 training epochs. The results indicated that \n\nthe InceptionV3 architecture presented the best performance, with 84.73% accuracy \n\nin the classification of the five species, followed by the InceptionResNetV2 architecture \n\nwith 82.87%, VGG16 with 80.60%. The ResNet50 architecture obtained the worst \n\nresult with 20.00% accuracy, the InceptionV3 network was trained again with 40 \n\nepochs, obtaining 88.50% accuracy. \n\n \n\nKeywords: Precision Agriculture, Convolutional Neural Networks, Deep Learning, \n\nWeeds. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nLISTA DE FIGURAS  \n\nFigura 1- Uso de Agrot\u00f3xicos Por Tipo no Brasil ......................................................................... 16 \n\nFigura 2 - Principais Lavouras no Uso de Agrot\u00f3xicos no Brasil ............................................... 17 \n\nFigura 3 - Aumento de Resist\u00eancia de Plantas Daninhas ao Mesmo Herbicida ..................... 20 \n\nFigura 4 - Esp\u00e9cies de Plantas Daninhas Resistentes ao Glifosato,(a) Buva,(b) Capim \n\nAmargoso,(c) Capim-Azev\u00e9m, (d) Capim-P\u00e9-de-Galinha,(e) Caruru ........................................ 21 \n\nFigura 5 - Representa\u00e7\u00e3o do Neur\u00f4nio Biol\u00f3gico......................................................................... 22 \n\nFigura 6 - Representa\u00e7\u00e3o do Neur\u00f4nio Matem\u00e1tico .................................................................... 23 \n\nFigura 7 - Arquitetura de Rede Neural com Propaga\u00e7\u00e3o para Frente e Retro propaga\u00e7\u00e3o .. 25 \n\nFigura 8 - Representa\u00e7\u00e3o da Intelig\u00eancia Artificial e suas sub\u00e1reas ........................................ 26 \n\nFigura 9 -  Rede Neural Convolucional proposta por Lecun, Y .................................................. 28 \n\nFigura 10 -  Arquitetura de Aprendizagem Profunda Para Classifica\u00e7\u00e3o de Plantas Daninhas\n\n ............................................................................................................................................................... 29 \n\nFigura 11 - Imagens Segmentadas Utilizadas para Classifica\u00e7\u00e3o de Plantas Daninhas ...... 31 \n\nFigura 12 - Plataforma de Coleta de Imagens Bonirob e Plantas Analisadas ......................... 32 \n\nFigura 13 - Fluxo metodol\u00f3gico de etapas para a aplica\u00e7\u00e3o das arquiteturas de aprendizado \n\nprofundo e gera\u00e7\u00e3o dos resultados ................................................................................................. 34 \n\nFigura 14 - Arquitetura VGG16........................................................................................................ 37 \n\nFigura 15 -  Bloco Residual Rede Resnet ..................................................................................... 39 \n\nFigura 16 - Arquitetura InceptionV3 ................................................................................................ 40 \n\nFigura 17 - Diferen\u00e7a entre Agrupamento M\u00e9dio e M\u00e1ximo ....................................................... 40 \n\nFigura 18 - Arquitetura InceptionResnetV2 ................................................................................... 41 \n\nFigura 19 - Compara\u00e7\u00e3o entre uma imagem da esp\u00e9cie Capim Azev\u00e9m sem ru\u00eddo (a), e \n\ncom ru\u00eddo inserido (b) ........................................................................................................................ 42 \n\nFigura 20 - Compara\u00e7\u00e3o entre uma imagem da esp\u00e9cie Capim P\u00e9 de Galinha sem ru\u00eddo (a), \n\ne com ru\u00eddo inserido (b) ..................................................................................................................... 43 \n\nFigura 21 - Gr\u00e1fico de Perda de Treinamento das Arquiteturas ................................................ 46 \n\nFigura 22 - Gr\u00e1fico de Exatid\u00e3o de Treinamento das Arquiteturas ........................................... 46 \n\nFigura 23 - Gr\u00e1fico de Perda de Valida\u00e7\u00e3o das Arquiteturas ..................................................... 47 \n\nFigura 24 - Gr\u00e1fico de Exatid\u00e3o de Valida\u00e7\u00e3o das Arquiteturas ................................................ 48 \n\nFigura 25 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o VGG16 .......................................... 49 \n\nFigura 26 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o VGG16 ..................................... 49 \n\nFigura 27- Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o ResNet50 ....................................... 50 \n\nFigura 28 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o ResNet50 ................................. 50 \n\nFigura 29 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o InceptionV3 .................................. 51 \n\n\n\n \n\nFigura 30 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o InceptionV3 ............................. 51 \n\nFigura 31 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o InceptionResNetV2 ..................... 52 \n\nFigura 32- Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o InceptionResNetV2 ................. 52 \n\nFigura 33 - Matriz de Confus\u00e3o  Arquitetura VGG16 ................................................................... 54 \n\nFigura 34 - Matriz de Confus\u00e3o  Arquitetura ResNet50 .............................................................. 54 \n\nFigura 35 - Matriz de Confus\u00e3o  Arquitetura InceptionV3 ........................................................... 55 \n\nFigura 36 - Matriz de Confus\u00e3o  Arquitetura InceptionResNetV2 .............................................. 55 \n\nFigura 37 -  Matriz de Confus\u00e3o  Arquitetura InceptionV3 40 \u00c9pocas ...................................... 57 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nLISTA DE QUADROS \n\nQuadro 1 - Estados L\u00edderes em Vendas e Consumo de Agrot\u00f3xicos no Brasil ...................... 18 \n\nQuadro 2 - Esp\u00e9cies de Plantas Daninhas Selecionadas para este Trabalho ........................ 21 \n\nQuadro 3 - Arquiteturas de Redes Neurais Profundas Armazenadas no Keras e usadas \n\nneste trabalho ..................................................................................................................................... 36 \n\nQuadro 4 - Performance das Arquiteturas com base nas matrizes de confus\u00e3o .................... 56 \n\nQuadro 5 - Performance da Arquitetura InceptionV3 com 40 \u00e9pocas de treinamento, com \n\nbase nas matrizes de confus\u00e3o........................................................................................................ 57 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nLISTA DE SIGLAS \n\n \n\nALS  Acetolactate Synthase \n\nANVISA Ag\u00eancia Nacional de Vigil\u00e2ncia Sanit\u00e1ria \n\nAPI  Application Programming Interface \n\nEMBRAPA  Empresa Brasileira de Pesquisa Agropecu\u00e1ria \n\nIBAMA  Instituto Brasileiro do Meio Ambiente e dos Recursos Naturais Renov\u00e1veis  \n\nILSVRC  Imagenet Large Scale Visual Recognition Challenge  \n\nLMR  Limite M\u00e1ximo de Res\u00edduos \n\nNDVI  Normalized Difference Vegetation Index  \n\nOMS  Organiza\u00e7\u00e3o Mundial da Sa\u00fade \n\nONU  Organiza\u00e7\u00e3o das Na\u00e7\u00f5es Unidas \n\nPIB    Produto Interno Bruto \n\nRGB  Red Green Blue \n\nSINDIVEG Sindicato Nacional da Ind\u00fastria de Produtos para a Defesa Vegetal  \n\nSLIC  Simple Linear Iterative Clustering \n\nVANT  Ve\u00edculo A\u00e9reo N\u00e3o Tripulado \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\nSUM\u00c1RIO  \n\n \n1 INTRODU\u00c7\u00c3O ............................................................................................................................ 12 \n\n1.1 Contextualiza\u00e7\u00e3o do Tema ............................................................................................ 12 \n\n1.2 Objetivos ............................................................................................................................. 14 \n\n1.3 Estrutura da Disserta\u00e7\u00e3o ................................................................................................ 14 \n\n2  REVIS\u00c3O BIBLIOGR\u00c1FICA .................................................................................................... 16 \n\n2.1  A Agricultura no Brasil e o Uso de Pesticidas ......................................................... 16 \n\n2.2.1 Plantas Daninhas do Brasil Resistentes ao Herbicida Glifosato................. 20 \n\n2.3 Redes Neurais Artificiais ................................................................................................ 22 \n\n2.4 Aprendizado Profundo ......................................................................................................... 26 \n\n2.5 Redes Neurais Convolucionais .................................................................................... 27 \n\n2.6 Redes Neurais Convolucionais na Agroind\u00fastria ................................................... 28 \n\n2.6.1 Detec\u00e7\u00e3o Autom\u00e1tica e Classifica\u00e7\u00e3o de Plantas Daninhas sob Condi\u00e7\u00f5es \nNaturais de Ilumina\u00e7\u00e3o ........................................................................................................... 29 \n\n2.6.2 Detec\u00e7\u00e3o de Plantas Daninhas em Planta\u00e7\u00f5es de Soja usando Redes \nConvolucionais ......................................................................................................................... 30 \n\n2.6.3 Segmenta\u00e7\u00e3o Sem\u00e2ntica em Tempo Real de Planta\u00e7\u00f5es e Plantas \nDaninhas para Rob\u00f4s de Agricultura de Precis\u00e3o com o Conhecimento de Plano \n\nde Fundo em CNNs .................................................................................................................. 31 \n\n3 MATERIAIS E M\u00c9TODOS ........................................................................................................ 34 \n\n3.1 Biblioteca Tensorflow ..................................................................................................... 34 \n\n3.2 Biblioteca Keras................................................................................................................ 35 \n\n3.3 Arquiteturas de Redes Neurais Convolucionais ...................................................... 37 \n\n3.3.1 VGG16: Redes Convolucionais Muito Profundas para Reconhecimento de \nImagem em Grande Escala .................................................................................................... 37 \n\n3.3.2 ResNet-50: Aprendizagem Residual Profunda para Reconhecimento de \nImagem 38 \n\n3.3.3 InceptionV3: Repensando a Arquitetura Inception para Vis\u00e3o \nComputacional .......................................................................................................................... 39 \n\n3.2.4 Inception-v4, Inception-ResNet e o Impacto das Conex\u00f5es Residuais no \nAprendizado ............................................................................................................................... 41 \n\n3.4 Obten\u00e7\u00e3o do Banco de Imagens .................................................................................. 41 \n\n3.5 Inser\u00e7\u00e3o de Ru\u00eddos ............................................................................................................... 42 \n\n3.6 Parametriza\u00e7\u00e3o do Algoritmo ............................................................................................. 43 \n\n4 RESULTADOS E DISCUSS\u00d5ES ............................................................................................ 45 \n\n5 CONCLUS\u00d5ES ........................................................................................................................... 58 \n\nREFER\u00caNCIAS .................................................................................................................................. 59 \n\n\n\n \n\n1 INTRODU\u00c7\u00c3O \n\n Este cap\u00edtulo apresenta a contextualiza\u00e7\u00e3o e import\u00e2ncia deste projeto de \n\npesquisa, assim como a motiva\u00e7\u00e3o, o objetivo e metodologia aplicada a este trabalho \n\nde pesquisa. \n\n1.1 Contextualiza\u00e7\u00e3o do Tema \n\nO agroneg\u00f3cio desempenha fun\u00e7\u00e3o de destaque no cen\u00e1rio econ\u00f4mico \n\nnacional, sendo respons\u00e1vel por 25% do Produto Interno Bruto (PIB) do Brasil e 20% \n\ndos postos de trabalho. Sendo ainda um dos l\u00edderes mundiais na produ\u00e7\u00e3o e \n\nexporta\u00e7\u00e3o de gr\u00e3os como soja e milho, de carnes, como a bovina e de frango e cana \n\nde a\u00e7\u00facar para a produ\u00e7\u00e3o de etanol (Vasconcelos, 2018).  \n\nEntretanto, se por um lado ano ap\u00f3s ano o Brasil vem aumentando a sua \n\nprodu\u00e7\u00e3o e exporta\u00e7\u00e3o agr\u00edcola, em contrapartida vem aumentando tamb\u00e9m o uso de \n\nagrot\u00f3xicos nas planta\u00e7\u00f5es.  Sendo que as planta\u00e7\u00f5es de soja, cana-de-a\u00e7\u00facar e \n\nmilho juntas respondem por 75% dos defensivos agr\u00edcolas consumidos no Brasil.  \n\nAtualmente o Brasil gasta cerca de US$ 10 bilh\u00f5es por ano em defensivos agr\u00edcolas, \n\nrepresentando 20% do mercado mundial  estimado em US$ 50 bilh\u00f5es (Vasconcelos, \n\n2018).  \n\nUma das explica\u00e7\u00f5es para o uso em demasia dos defensivos agr\u00edcolas no \n\nBrasil deve-se pelo fato do clima tropical do pa\u00eds, assim, n\u00e3o tendo na maior parte de \n\nsua extens\u00e3o um per\u00edodo de inverno severo para interromper o desenvolvimento das \n\npragas. Deve-se tamb\u00e9m a expans\u00e3o da produ\u00e7\u00e3o agr\u00edcola, por exemplo, a safra de \n\ngr\u00e3os no ano de 2010 era de 149 milh\u00f5es de toneladas, j\u00e1 em 2017 foi de 238 milh\u00f5es, \n\ne principalmente a produ\u00e7\u00e3o de monoculturas na mesma \u00e1rea  como milho e soja. \n\nDados de uma rel\u00e1torio de 2017 feito pela Organiza\u00e7\u00e3o das Na\u00e7\u00f5es Unidas \n\n(ONU) estimou que cerca de 200 mil pessoas morrem anualmente no mundo v\u00edtimas \n\nde envenenamento agudo por pesticidas, principalmente trabalhadores rurais e \n\nmoradores do campo (ESTADOS UNIDOS, 2017). No Brasil, 84,2 mil pessoas \n\nsofreram intoxica\u00e7\u00e3o ap\u00f3s exposi\u00e7\u00e3o a defensivos agr\u00edcolas entre 2007 e 2015, uma \n\nm\u00e9dia de 25 intoxica\u00e7\u00f5es por dia, conforme dados do Relat\u00f3rio Nacional de Vigil\u00e2ncia \n\nem Sa\u00fade de Popula\u00e7\u00f5es Expostas a Agrot\u00f3xicos 2018, elaborado pelo Minist\u00e9rio da \n\nSa\u00fade (BRASIL, 2018). \n\n12\n\n\n\n \n\nDentre os defensivos agr\u00edcolas, o herbicida \u00e9 o mais utilizado nacionalmente, \n\nprincipalmente na lavoura de soja em plantas daninhas que competem pela luz e \n\nnutrientes com a cultura. Um estudo realizado pelo Sindicato Nacional da Ind\u00fastria de \n\nProdutos para a Defesa Vegetal (SINDIVEG) indica que 60% dos defensivos \n\nconsumidos em 2017 no Brasil foram herbicidas. O estudo ainda indica que as culturas \n\nde soja, cana de a\u00e7\u00facar e milho, consumiram respectivamente 52,2%, 11,7% e 10,6% \n\nde todos os defensivos agr\u00edcolas consumidos, representando um total de agrot\u00f3xicos \n\nconsumidos de 74,5% (SINDIVEG, 2017).  \n\nCom base nestas informa\u00e7\u00f5es, surge a necessidade da explora\u00e7\u00e3o e \n\ndesenvolvimento de novas t\u00e9cnicas que possibilitem a identifica\u00e7\u00e3o de plantas \n\ndaninhas para a correta dosagem e aplica\u00e7\u00e3o de agrot\u00f3xicos ou elimina\u00e7\u00e3o mec\u00e2nica. \n\nNesse contexto, a agricultura de precis\u00e3o e as ferramentas que fazem parte dela \n\naparecem como poss\u00edveis solu\u00e7\u00f5es promissoras para este desafio. Dentre as \n\nferramentas que podem ser utilizadas para fornecer solu\u00e7\u00f5es para esta problem\u00e1tica \n\nfoi selecionado para este trabalho a aplica\u00e7\u00e3o do aprendizado profundo (Deep \n\nLearnig), associado \u00e0 vis\u00e3o computacional para a classifica\u00e7\u00e3o de diferentes esp\u00e9cies \n\nde plantas daninhas. \n\nO aprendizado profundo \u00e9 uma t\u00e9cnica de intelig\u00eancia artifical, que vem \n\nobtendo excelentes resultados, principalmente na \u00e1rea de vis\u00e3o computacional, por \n\nmeio das redes neurais convolucionais (Convolutional Neural Networks). Dentre suas \n\naplica\u00e7\u00f5es destacam-se a classifica\u00e7\u00e3o de imagens (Krizhevsky et al., 2012),  \n\n(Simonyan, K;  Zisserman, A, 2015), (Szegedy, C. et al., 2015), (He, K. et al., 2016)  a \n\nclassifica\u00e7\u00e3o de imagens com localiza\u00e7\u00e3o (UIJLINGS, J. R. R.  et al., 2013),(Girshick, \n\nR  et al., 2014), (Girshick, R, 2015) detec\u00e7\u00e3o de objetos (Sermanet, P  et al., 2014), \n\n(Redmon, J et al., 2016) e segmenta\u00e7\u00e3o de objetos (BadrinarayanaN, V et al., 2017), \n\n(HE, K et al., 2017)  .  \n\nPara realizar a classifica\u00e7\u00e3o das esp\u00e9cies de plantas daninhas criou-se um \n\nbanco de imagens com cinco esp\u00e9cies que apresentam resist\u00eancias aos principais \n\nherbicidas comercialmente utilizados no Brasil, assim, caso n\u00e3o seja feito o correto \n\nmanejo dessas pragas o agricultor ter\u00e1  perdas financeiras significativas. \n\n Diante deste desafio t\u00e9cnico cient\u00edfico, foi proposto para este trabalho utilizar \n\nquatro arquiteturas de redes Neurais Profundas (Deep Neural Networks), VGG16, \n\n13\n\nhttps://dl.acm.org/author_page.cfm?id=81467645792&amp;coll=DL&amp;dl=ACM&amp;trk=0\nhttps://dl.acm.org/author_page.cfm?id=81467645792&amp;coll=DL&amp;dl=ACM&amp;trk=0\nhttps://dl.acm.org/author_page.cfm?id=81467645792&amp;coll=DL&amp;dl=ACM&amp;trk=0\nhttps://ieeexplore.ieee.org/author/37547792900\nhttps://ieeexplore.ieee.org/author/37547792900\nhttps://ieeexplore.ieee.org/author/37547792900\n\n\n \n\nResNet50, InceptionV3 e InceptionResNetV2,  para a identifica\u00e7\u00e3o e classifica\u00e7\u00e3o de \n\ncinco esp\u00e9cies de plantas daninhas, verificando assim a exatid\u00e3o de cada arquitetura \n\npara realizar esta tarefa, determinando qual arquitetura obteve a melhor performance \n\ne assim, possibilitando verificar se esta abordagem foi eficaz para o problema \n\nproposto.  \n\n1.2 Objetivos  \n\n O objetivo principal desta disserta\u00e7\u00e3o \u00e9 realizar a classifica\u00e7\u00e3o de 5 esp\u00e9cies \n\nde plantas daninhas encontradas em todo o territ\u00f3rio nacional, utilizando redes neurais \n\nconvolucionais em um banco de imagens. Para a cria\u00e7\u00e3o do banco coletou-se \n\nimagens na internet das esp\u00e9cies de plantas daninhas que s\u00e3o encontradas em todo \n\nterrit\u00f3rio nacional e apresentam resist\u00eancia a aplica\u00e7\u00e3o dos principais herbicidas do \n\nmercado atual, posteriormente foi inserido diversos ru\u00eddos nestas imagens, criando \n\num banco de imagens para o treinamento e valida\u00e7\u00e3o das arquiteturas estudadas. \n\n Tamb\u00e9m \u00e9 objetivo deste trabalho verificar a exatid\u00e3o das arquiteturas de \n\naprendizado profundo na classifica\u00e7\u00e3o, portanto, podendo-se determinar qual \n\narquitetura obteve os melhores. Dessa maneira verificando a performance da \n\nabordagem proposta para o problema de classifica\u00e7\u00e3o de imagens plantas daninhas.  \n\n Espera-se por meio desta disserta\u00e7\u00e3o contribuir para o estudo das t\u00e9cnicas de \n\naprendizado profundo e a aplica\u00e7\u00e3o de intelig\u00eancia artificial na resolu\u00e7\u00e3o dos \n\nproblemas e desafios que a agricultura brasileira vem enfrentando, e possa deparar-\n\nse no futuro. Dentre estes desafios o que foi abordado neste trabalho \u00e9 a possibilidade \n\nda correta classifica\u00e7\u00e3o de plantas daninhas o que permite ao agricultor dosar a \n\nquantidade ideal de herbicida por esp\u00e9cie de planta daninha ou realizar a elimina\u00e7\u00e3o \n\nmec\u00e2nica. \n\n1.3 Estrutura da Disserta\u00e7\u00e3o \n\nNo cap\u00edtulo 2 \u00e9 apresentado a revis\u00e3o da literatura. Neste cap\u00edtulo ser\u00e3o \n\nabordados os temas referentes \u00e0 disserta\u00e7\u00e3o que se encontram na literatura desde \n\ndados referentes ao uso de agrot\u00f3xicos no brasil, as principais plantas daninhas que \n\ncausam preju\u00edzos aos agricultores a apresentam alta resist\u00eancia a herbicidas,  \n\ntamb\u00e9m \u00e9 abordado o hist\u00f3rico das redes neurais at\u00e9 chegar nas redes neurais \n\nconvolucionais e trabalhos recentes que fizeram uso desta t\u00e9cnica na agricultura de \n\n14\n\n\n\n \n\nprecis\u00e3o para a classifica\u00e7\u00e3o de plantas daninhas. O intuito deste cap\u00edtulo \u00e9 posicionar \n\no leitor acerca do tema e da problem\u00e1tica na qual o trabalho se encontra. \n\nNo cap\u00edtulo 3 s\u00e3o apresentados os materiais e m\u00e9todos utilizados. M\u00e9todos \n\nenvolvendo a cria\u00e7\u00e3o do banco de imagens, inser\u00e7\u00e3o de ru\u00eddo e classifica\u00e7\u00e3o destas \n\nimagens por meio das quatro arquiteturas de redes neurais convolucionais estudadas. \n\n No cap\u00edtulo 4 aborda-se os resultados obtidos para cada arquitetura de rede \n\nneural convolucional por meio dos gr\u00e1ficos de exatid\u00e3o e perdas,  posteriormente \n\napresentam-se as matrizes de confus\u00e3o para cada arquitetura, onde poder-se-\u00e1 \n\nverificar a performance de classifica\u00e7\u00e3o para as diferentes esp\u00e9cies de plantas.  \n\n O cap\u00edtulo 5 se concentra em apresentar as conclus\u00f5es gerais do trabalho e \n\npropostas de trabalhos futuros. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n15\n\n\n\n \n\n2  REVIS\u00c3O BIBLIOGR\u00c1FICA \n\n2.1  A Agricultura no Brasil e o Uso de Pesticidas \n\nAo longo dos \u00faltimos 50 anos a agricultura brasileira passou por um per\u00edodo de \n\nexpans\u00e3o e transforma\u00e7\u00e3o em rela\u00e7\u00e3o a produ\u00e7\u00e3o dos insumos agr\u00edcolas, \n\nprincipalmente em culturas de soja, milho, cana de a\u00e7\u00facar, caf\u00e9 e algod\u00e3o . Um \n\ntrabalho que at\u00e9 ent\u00e3o dependia de m\u00e3o de obra humana com aux\u00edlio animal para o \n\npreparo e produ\u00e7\u00e3o dos insumos, atualmente possui alto n\u00edvel de automa\u00e7\u00e3o, seja \n\npelo uso de m\u00e1quinas agr\u00edcolas de colheita, de semeadura ou at\u00e9 mesmo pela \n\naplica\u00e7\u00e3o de defensivos agr\u00edcolas por meio de avi\u00f5es pulverizadores. Entretanto o \n\naumento da produtividade destas culturas tem representado um aumento significativo \n\nno uso de defensivos agr\u00edcolas, em especial os defensivos de finalidade herbicida \n\npara o controle e exterm\u00ednio de plantas daninhas. A figura 1 refor\u00e7a esses dados \n\nmostrando para quais tipos de pragas mais se utilizou defensivos agr\u00edcola no Brasil \n\nno ano de 2017. \n\nFigura 1- Uso de Agrot\u00f3xicos Por Tipo no Brasil \n\n \n\nFonte: SINDIVEG (2017) \n\nOs defensivos com a fun\u00e7\u00e3o de herbicida representaram 60% de todo o \n\nconsumo nacional do ano de 2017, sendo acompanhado pelos fungicidas e \n\ninseticidas, cada um com 15% respectivamente do consumo nacional. Para que se \n\npossa compreender melhor a aplica\u00e7\u00e3o destes defensivos, na figura 2 o consumo de \n\ndefensivo agr\u00edcola por cultura plantada \u00e9 apresentado.  \n\n \n\n \n\n16\n\n\n\n \n\nFigura 2 - Principais Lavouras no Uso de Agrot\u00f3xicos no Brasil \n\n \n\nFonte: SINDIVEG (2017) \n\nA soja lidera o uso de defensivos agr\u00edcolas com 52,2% sendo seguida pela \n\ncana de a\u00e7\u00facar com 11,7% e milho com 10,6%. Juntas as 3 culturas representam \n\n74,5% do uso de defensivos agr\u00edcolas no Brasil em 2017.  A t\u00edtulo de compara\u00e7\u00e3o, os \n\ningredientes ativos com a\u00e7\u00e3o herbicida que lideram a lista dos agrot\u00f3xicos mais \n\ncomercializados, tiveram, em 2009, uma quantidade comercializada da ordem de 127 \n\nmil toneladas. J\u00e1 em 2013, o glifosato sozinho, o herbicida mais vendido, teve mais \n\nde 185 mil toneladas comercializadas (IBAMA, 2013).  Para que se possa verificar a \n\ndistribui\u00e7\u00e3o da comercializa\u00e7\u00e3o destes agrot\u00f3xicos no quadro 1 s\u00e3o apresentados os \n\nprincipais estados comercializadores e consumidores de agrot\u00f3xicos no Brasil no ano \n\nde 2017, adaptado com base nos dados dos Relat\u00f3rios de Comercializa\u00e7\u00e3o de \n\nAgrot\u00f3xicos Publicado pelo IBAMA em 2017. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n17\n\n\n\n \n\nQuadro 1 - Estados L\u00edderes em Vendas e Consumo de Agrot\u00f3xicos no Brasil \n\nTotal das Vendas de  Agrot\u00f3xicos e Afins nas Regi\u00f5es e Estados Brasileiros \n\u2013 2017 \n\n \n\nUnidade de medida = toneladas de ingrediente ativo (IA) \n\nEstado Quantidade \n(Toneladas) \n\nPorcentagem \nNacional \n\nTotal Nacional \n(Toneladas) \n\nMato Grosso (MT) 100.638,47 18,64% 539.944,95 \n\nS\u00e3o Paulo (SP) 77.232,66 14,30% 539.944,95 \n\nRio Grande do Sul \n(RS) \n\n70.143,64 13,00% 539.944,95 \n\nParan\u00e1 (PR)  61.130,01 11,32% 539.944,95 \n\nTotal 309.144,78 57,26% 539.944,95 \n\nFonte: IBAMA (2017), adaptado pelo autor \n\nO estado do Mato Grosso lidera com 18,64%, sendo este estado o l\u00edder nacional \n\nna produ\u00e7\u00e3o de soja, cultura esta que depende intensamente do uso de herbicidas. \n\nEm seguida o estado de S\u00e3o Paulo com 14,30%, l\u00edder na produ\u00e7\u00e3o de cana de a\u00e7\u00facar. \n\nO estado do Rio Grande do Sul que vem aumentando o cultivo de soja representa \n\n13,00% da comercializa\u00e7\u00e3o e o Paran\u00e1 com 11,32% ocupa o terceiro lugar em \n\nprodu\u00e7\u00e3o de cana e segundo lugar em produ\u00e7\u00e3o de soja, \u00e9 o quarto estado em uso \n\nde agrot\u00f3xicos, juntos os quatro estados representam 57,26% da comercializa\u00e7\u00e3o \n\nnacional de defensivos agr\u00edcolas. Tal consumo destes agrot\u00f3xicos, vem causando \n\numa s\u00e9rie de preju\u00edzos sociais, ambientais e econ\u00f4micos, como a contamina\u00e7\u00e3o do \n\nsolo e len\u00e7\u00f3is fre\u00e1ticos, morte ou intoxica\u00e7\u00e3o de trabalhadores rurais e ainda aumento \n\ndos custos de produ\u00e7\u00e3o das culturas agr\u00edcolas devido a resist\u00eancia das plantas \n\ndaninhas.  \n\n Nos itens seguintes, informa\u00e7\u00f5es sobre as plantas daninhas, a resist\u00eancia \n\ndessas plantas a herbicidas e quais esp\u00e9cies apresentam maior resist\u00eancia e \n\npreju\u00edzos ao agricultor s\u00e3o abordados, assim como as 5 esp\u00e9cies de plantas daninhas \n\nque s\u00e3o foco deste estudo. \n\n \n\n \n\n \n\n \n\n18\n\n\n\n \n\n2.2 Plantas Daninhas \n\nSegundo dados da EMBRAPA (Empresa Brasileira de Pesquisa Agropecu\u00e1ria) \n\nas plantas daninhas aparecem como um dos fatores que mais afetam a produtividade. \n\nPrejudicando a cultura plantada seja pela competi\u00e7\u00e3o por recursos como sol, \u00e1gua e \n\nnutrientes, ou pela alopatia, que \u00e9 a capacidade das plantas produzirem subst\u00e2ncias \n\nprejudiciais para outras plantas e consequentemente causando a perda de rendimento \n\nda produ\u00e7\u00e3o. Causam tamb\u00e9m efeitos indiretos, como o aumento do custo, a piora na \n\ncolheita e deprecia\u00e7\u00e3o da qualidade do produto. Em casos em que n\u00e3o \u00e9 feito controle \n\nalgum de plantas daninhas, as perdas podem chegar a mais de 90% da produ\u00e7\u00e3o, \n\ncom o controle tem-se em m\u00e9dia de 13 a 15% de perda na produ\u00e7\u00e3o de gr\u00e3os. \n\nA primeira ideia do conceito de plantas daninhas surgiu ainda nos tempos \n\nb\u00edblicos, no momento em que o homem deu in\u00edcio \u00e0s atividades agr\u00edcolas \n\nselecionando plantas consideradas \u00fateis (cultivadas) daquelas consideradas in\u00fateis \n\n(invasoras). Nos dias de hoje, plantas daninhas englobam todas as plantas que \n\ninterferem no crescimento das cultivadas, mostrando-se persistentes, e que atuam de \n\nforma negativa nas atividades humanas, sendo consideradas como plantas \n\nindesejadas. Este tipo de planta costuma crescer em condi\u00e7\u00f5es adversas, como \n\nambientes secos ou \u00famidos, com temperaturas baixas ou elevadas e variados tipos \n\nde solos. Estas plantas apresentam capacidade de produzir sementes vi\u00e1veis em \n\nabund\u00e2ncia, com variadas formas de dispers\u00e3o, al\u00e9m de apresentarem resist\u00eancia ao \n\nataque de pragas e doen\u00e7as(EMBRAPA, 2018). \n\nDas 350.000 esp\u00e9cies conhecidas de plantas, apenas 3.000 s\u00e3o cultivadas; e \n\naproximadamente 250 s\u00e3o universalmente consideradas plantas daninhas, das quais \n\ncerca de 40% pertencem a apenas duas fam\u00edlias: Poaceae (gram\u00edneas) e Asteraceae \n\n(compostas). Por causa do seu car\u00e1ter competitivo, as plantas daninhas garantem sua \n\nperpetua\u00e7\u00e3o por meio de dorm\u00eancia e germina\u00e7\u00e3o desuniforme das sementes. Estas \n\nhabilidades conferem um dif\u00edcil controle das esp\u00e9cies invasoras pelo fato de n\u00e3o \n\ngerminarem todas ao mesmo tempo, mesmo em condi\u00e7\u00f5es ideais de temperatura, \n\numidade e luz (EMBRAPA, 2018). \n\nO desenvolvimento das plantas invasoras \u00e9 r\u00e1pido, sendo capaz de atingir sua \n\nmaturidade em pouco tempo. A produ\u00e7\u00e3o de sementes \u00e9 elevada (produzem em \n\ngrandes quantidades), por\u00e9m, este n\u00e3o \u00e9 o \u00fanico meio de reprodu\u00e7\u00e3o destas \n\n19\n\n\n\n \n\ninvasoras; algumas esp\u00e9cies apresentam capacidade reprodutiva tamb\u00e9m atrav\u00e9s de \n\nbulbos, tub\u00e9rculos, rizomas(caules) e enraizamento. \n\n2.2.1 Plantas Daninhas do Brasil Resistentes ao Herbicida Glifosato \n\nAs plantas daninhas, evolutivamente, j\u00e1 apresentam uma variabilidade \n\ngen\u00e9tica natural. Ou seja, dentro de uma mesma esp\u00e9cie de planta daninha h\u00e1 \n\ndiferen\u00e7as gen\u00e9ticas entre cada indiv\u00edduo. Os herbicidas selecionam aqueles \n\nindiv\u00edduos que apresentam genes que resultam em resist\u00eancia. A Resist\u00eancia de uma \n\nplanta daninha a um herbicida \u00e9 a capacidade adquirida por uma planta em sobreviver \n\ne se reproduzir mesmo com a aplica\u00e7\u00e3o de um herbicida na dose registrada (dose \n\nindicada na bula) em condi\u00e7\u00f5es normais e adequadas de aplica\u00e7\u00e3o. \n\nLavouras de soja com plantas daninhas resistentes ao glifosato, principal \n\nherbicida comercial utilizado no Brasil, possuem custos de 22 a 42% maiores, \n\nsegundo dados da EMBRAPA. As aplica\u00e7\u00f5es de herbicidas n\u00e3o criam uma planta \n\nresistente, apenas selecionam. A sele\u00e7\u00e3o de bi\u00f3tipos resistentes ocorre atrav\u00e9s da \n\naplica\u00e7\u00e3o de um mesmo herbicida repetidas vezes na mesma \u00e1rea. Como, por \n\nexemplo, nas culturas transg\u00eanicas resistentes ao glifosato, onde muitos produtores \n\naplicam esse herbicida para toda a cultura plantada, por um per\u00edodo cont\u00ednuo de \n\ndiversas safras. A figura 3 ilustra o processo de sele\u00e7\u00e3o de plantas resistentes que \n\nocorre ao longo dos anos de aplica\u00e7\u00e3o de um mesmo herbicida. \n\nFigura 3 - Aumento de Resist\u00eancia de Plantas Daninhas ao Mesmo Herbicida \n\n \nFonte: Christoffoleti (2008). \n \n\nConforme ilustrado na figura 4 a cada ano que o mesmo herbicida \u00e9 aplicado, \n\nmenos eficiente ele se torna na elimina\u00e7\u00e3o de plantas daninhas. No Brasil, foram \n\n20\n\n\n\n \n\nidentificados bi\u00f3tipos resistentes ao glifosato das seguintes esp\u00e9cies apresentadas \n\npelo quadro 2 e na figura 4: \n\nQuadro 2 - Esp\u00e9cies de Plantas Daninhas Selecionadas para este Trabalho \n\nNome Popular da Esp\u00e9cie Nome Cient\u00edfico da Esp\u00e9cie \n\nBuva Conyza Bonariensis \n\nCapim-Azev\u00e9m Lolium Multiflorum \n\nCapim-Amargoso Digitaria Insularis \n\nCapim-P\u00e9-de-Galinha Eleusine Indica \n\nCaruru Plameri Amaranthus Palmeri \n\nFonte: Elabora\u00e7\u00e3o do p?oprio  Autor \n\nFigura 4 - Esp\u00e9cies de Plantas Daninhas Resistentes ao Glifosato,(a) Buva,(b) \nCapim Amargoso,(c) Capim-Azev\u00e9m, (d) Capim-P\u00e9-de-Galinha,(e) Caruru \n\n \n      (a)     (b)    (c) \n\n \n(d)            (e) \n\nFonte: Elabora\u00e7\u00e3o do p?oprio  Autor \n \n\nAs esp\u00e9cies contidas na figura 4 e descritas no quadro 2 foram  selecionadas \n\npara a cria\u00e7\u00e3o do banco de imagens e treinamento das arquiteturas de redes neurais \n\nprofundas. Cada esp\u00e9cie tem uma apar\u00eancia singular se comparadas entre si, por\u00e9m \n\npodem ser facilmente camufladas quando em meio a vegeta\u00e7\u00e3o. Sendo que este ser\u00e1 \n\num dos pontos testados neste trabalho, que \u00e9 a capacidade da rede convolucional \n\n21\n\n\n\n \n\nclassificar corretamente a esp\u00e9cie, mesmo quando n\u00e3o segmentada do plano de \n\nfundo.  \n\nSegundo dados da Syngenta, empresa l\u00edder, na produ\u00e7\u00e3o de semente e \n\ndefensivos agricolas,  do total da \u00e1rea de soja plantada no Brasil, 60% tem esp\u00e9cies \n\nde plantas daninhas resistentes. O custo devido a resist\u00eancia das plantas daninhas \n\nchega a R$ 9 bilh\u00f5es, podendo chegar a perda de 70% de produtividade caso n\u00e3o \n\nhaja nenhum tipo de controle . Na safra 2016/2017 teve-se uma incid\u00eancia de 50% de \n\nBuva e 40% de Capim-Amargoso, estima-se que at\u00e9 2022 esse n\u00famero aumente \n\ntendo-se 55% de incid\u00eancia de Buva e 80% de Capim-Amargoso (Syngenta, 2017). \n\n Uma vez definidas as esp\u00e9cies de plantas daninhas que ser\u00e3o utilizadas neste \n\ntrabalho, os cap\u00edtulos seguintes abordam o hist\u00f3rico e a evolu\u00e7\u00e3o das redes neurais, \n\nat\u00e9 sua aplica\u00e7\u00e3o na agricultura de precis\u00e3o. \n\n2.3 Redes Neurais Artificiais \n\nCom base no  c\u00e9rebro do ser humano pesquisadores tentaram simular este \n\nfuncionamento, principalmente o aprendizado por experi\u00eancia para criar sistemas \n\ninteligentes capazes de realizar tarefas como classifica\u00e7\u00e3o, reconhecimento de \n\npadr\u00f5es, processamento de imagens, entre outras atividades. Como resultado dessas \n\npesquisas surgiu o modelo do neur\u00f4nio artificial e posteriormente um sistema com \n\nv\u00e1rios neur\u00f4nios interconectados, a chamada Rede Neural. Na figura 5 tem-se uma \n\nrepresenta\u00e7\u00e3o do neur\u00f4nio biol\u00f3gico.  \n\nFigura 5 - Representa\u00e7\u00e3o do Neur\u00f4nio Biol\u00f3gico \n\n \n\nFonte Silva (2017), adaptado pelo autor \n\n22\n\n\n\n \n\nO neur\u00f4nio biol\u00f3gico pode ser dividido entre os dendritos que s\u00e3o os terminais \n\nde recep\u00e7\u00e3o, que recebem os impulsos nervosos (entradas), o corpo celular, que \n\nprocessa os sinais das entradas, e o ax\u00f4nio que s\u00e3o os terminais de transmiss\u00e3o \n\n(sa\u00edda) do impulso nervoso. Os primeiros neur\u00f4nios matem\u00e1ticos artificiais datam de \n\n1943, quando o neurofisiologista Warren McCulloch e o matem\u00e1tico Walter Pitts \n\nescreveram um artigo sobre como os neur\u00f4nios poderiam funcionar e para isso, eles \n\nmodelaram uma rede neural simples usando circuitos el\u00e9tricos (W, S. McCulloch, \n\n1943).  Com base no neur\u00f4nio McCulloch e Pitts, uma representa\u00e7\u00e3o do neur\u00f4nio \n\nmatem\u00e1tico artificial, assim como seus componentes s\u00e3o apresentados na figura 6.  \n\nFigura 6 - Representa\u00e7\u00e3o do Neur\u00f4nio Matem\u00e1tico \n\n \n\nFonte: Deep Learning Book, adaptado pelo autor \n\n O neur\u00f4nio artificial pode ser dividido nos seguintes itens: \n\n? Sinais de Entrada: Semelhante aos dendritos recebem os sinais externos, \n\nneste caso { X1, X2, \u2026.,Xn}, s\u00e3o os dados que alimentam o neur\u00f4nio artificial. \n\n? Pesos Sin\u00e1pticos: Representados por { W1, W2, \u2026.,Wn}, s\u00e3o valores que \n\nponderam cada entrada da rede, sendo estes valores aprendidos ao longo do \n\ntreinamento (aprendizagem por experi\u00eancia).   \n\n? Combinador Linear {?}: Une os sinais de entrada ponderados pelos pesos \n\nsin\u00e1pticos com o intuito de gerar um  pot\u00eancia de ativa\u00e7\u00e3o. \n\n? Limiar de Ativa\u00e7\u00e3o {?}: Determina o valor apropriado de resultado para o \n\ncombinador linear, para dessa maneira gerar um disparo de ativa\u00e7\u00e3o. \n\n23\n\n\n\n \n\n? Potencial de Ativa\u00e7\u00e3o {u}: Valor determinado pela diferen\u00e7a entre o combinador \n\nlinear a o limiar de ativa\u00e7\u00e3o. Caso o valor seja positivo u ? 0, o neur\u00f4nio produz \n\num potencial excitat\u00f3rio, caso contr\u00e1rio, o potencial ser\u00e1 inibit\u00f3rio. \n\n? Fun\u00e7\u00e3o de ativa\u00e7\u00e3o {g}: \u00c9 respons\u00e1vel por limitar a sa\u00edda de um neur\u00f4nio em \n\num intervalo valores. \n\n? Sinal de sa\u00edda {y}: Semelhante ao que ocorre no ax\u00f4nio, o valor de sa\u00edda pode \n\nser usado como entrada de outros neur\u00f4nios que est\u00e3o interligados. \n\nSintetizando, a arquitetura b\u00e1sica uma rede neural \u00e9 dividida entre camada de \n\nentrada, uma ou mais camadas ocultas e camada de sa\u00edda que apresenta o resultado \n\np\u00f3s treinamento da rede. \n\nEm 1956 Frank Rosenblatt um neurologista come\u00e7ou a trabalhar no Perceptron. O \n\nperceptron, pode ser compreendido como um procedimento de aprendizado que \n\nexamina os valores antes da mudan\u00e7a dos respectivos pesos sin\u00e1pticos. Ao longo dos \n\nanos foram sendo inseridas mais camadas ao perceptron, dessa maneira criando o \n\nperceptron multicamadas (Multilayer Perceptron)(Rosenblatt, F., 1957). \n\n Em 1959 foi desenvolvido o ADALINE (Adaptive Linear Element) ou Elemento \n\nLinear Adaptativo e o MADALINE (Many ADALINE) ou muitos Adalines pelos \n\ncientistas Bernard Widrow e Marcian Hoff, de Stanford. O modelo foi desenvolvido \n\npara reconhecer padr\u00f5es bin\u00e1rios de modo que, se ele estivesse lendo bits de \n\ntransmiss\u00e3o de uma linha telef\u00f4nica, poderia prever o pr\u00f3ximo bit, j\u00e1 o MADALINE foi \n\na primeira rede neural aplicada a um problema do mundo real, usando um filtro \n\nadaptativo que elimina ecos nas linhas telef\u00f4nicas. Apesar do tempo, os sistemas \n\nainda est\u00e3o em uso comercial (W, Bernard et al., 1960). \n\n Com maior difus\u00e3o do perceptron multicamadas, Em 1982 David Rumelhart, \n\npropuseram o Backpropagation, neste algoritmo os pesos s\u00e3o retro propagados para \n\nas camadas anteriores da rede, dessa maneira calculando e atualizando os pesos \n\nsin\u00e1pticos com base no erro que foi gerado. A cria\u00e7\u00e3o do algoritmo de retropropaga\u00e7\u00e3o \n\n(Backpropagation) foi um passo importante para o desenvolvimento das redes \n\nprofundas e posteriormente das redes neurais convolucionais. Na figura 7 \u00e9 \n\napresentado tanto uma arquitetura de rede neural tradicional de propaga\u00e7\u00e3o para \n\nfrente (Forward Propagation) quanto uma arquitetura com a retropropaga\u00e7\u00e3o \n\n24\n\n\n\n \n\n(Backpropagation) de seus pesos sin\u00e1pticos com base no sinal de erro (Rumelhart, D. \n\nE., 1986) \n\nFigura 7 - Arquitetura de Rede Neural com Propaga\u00e7\u00e3o para Frente e Retro \npropaga\u00e7\u00e3o \n\n \n\nFonte: Fagundes (2018) \n\n Nas arquiteturas de rede de propaga\u00e7\u00e3o para frente (Forward Propagation), n\u00e3o h\u00e1 \n\na corre\u00e7\u00e3o dos pesos sin\u00e1pticos de acordo com o valor de erro, que caracteriza o \n\naprendizado supervisionado. J\u00e1 nas arquiteturas de rede por retro propaga\u00e7\u00e3o \n\n(Backpropagation), existe a verifica\u00e7\u00e3o do valor que se tem ao final da rede neural, e \n\no valor de erro gerado, enquanto o valor de erro n\u00e3o diminuir a um patamar \n\ndeterminado aceit\u00e1vel haver\u00e1 uma atualiza\u00e7\u00e3o cont\u00ednua dos pesos sin\u00e1pticos. \n\n Em 1982, John Hopfield da Caltech apresentou um documento \u00e0 Academia \n\nNacional de Ci\u00eancias (National Science Foundation), onde sua abordagem n\u00e3o \n\nbuscava apenas modelar c\u00e9rebros, mas criar dispositivos \u00fateis. J\u00e1 em 1985, o Instituto \n\nAmericano de F\u00edsica (American Institute of Physics) come\u00e7ou o que se tornou uma \n\nreuni\u00e3o anual de Redes Neurais para Computa\u00e7\u00e3o.  \n\n Ap\u00f3s esse per\u00edodo, a pesquisa em redes neurais passou por um hiato, uma vez \n\nque muitos dos resultados esperados n\u00e3o poderiam ser atingidos com a capacidade \n\nde processamento computacional da \u00e9poca, felizmente muitos cientistas continuaram \n\nsuas pesquisas em redes neurais mesmo com a redu\u00e7\u00e3o do investimento e interesse \n\nem pesquisa nesta \u00e1rea. \n\n25\n\n\n\n \n\n2.4 Aprendizado Profundo \n\n  O Aprendizado Profundo (Deep Learning), \u00e9 uma sub\u00e1rea da Intelig\u00eancia Artificial \n\n(Artificial Intelligence). Na intelig\u00eancia artificial tem-se tamb\u00e9m o aprendizado de \n\nM\u00e1quina (Machine Learning), que faz uso de modelos estat\u00edsticos para a interpreta\u00e7\u00e3o \n\nde dados. J\u00e1 o aprendizado profundo emprega algoritmos para processar dados e \n\nimitar o processamento feito pelo c\u00e9rebro humano (Deep Learning Book, Cap 3, \n\n2018). Na figura 8 tem-se uma ilustra\u00e7\u00e3o de como estas t\u00e9cnicas est\u00e3o relacionadas \n\nno campo da  de intelig\u00eancia artificial. \n\nFigura 8 - Representa\u00e7\u00e3o da Intelig\u00eancia Artificial e suas sub\u00e1reas \n\n  \n\nFonte: Elabora\u00e7\u00e3o do p?oprio  autor \n\n No aprendizado profundo usa-se camadas de neur\u00f4nios matem\u00e1ticos para \n\nprocessar dados, compreender a fala humana e reconhecer objetos visualmente. \n\nNesta t\u00e9cnica os dados passam por todas as camadas que est\u00e3o interligadas. \n\nSemelhantemente as arquiteturas de redes neurais primeira camada representa as \n\nentradas, enquanto a \u00faltima representa sa\u00edda. Cada camada cont\u00e9m normalmente um \n\nalgoritmo simples e uniforme contendo um tipo de fun\u00e7\u00e3o de ativa\u00e7\u00e3o. \n\n \n\n \n\n \n\nIntelig\u00eancia \nArtificial \n\nAprendizado de \n\nAprendiza\ndo \n\nProfundo\n\n26\n\n\n\n \n\n Uma vez definido o conceito de aprendizado profundo, as redes neurais \n\nconvolucionais, que s\u00e3o uma das t\u00e9cnicas de aprendizado profundo e foco deste \n\ntrabalho s\u00e3o apresentadas a seguir.  \n\n2.5 Redes Neurais Convolucionais \n\n O primeiro modelo de rede neural convolutiva foi usado por Kunihiko Fukushima \n\nem 1979. Fukushima desenvolveu uma rede denominada Neocognitron, que usava \n\num design hier\u00e1rquico e multicamadas. Este design permite ao computador \u201caprender\u201d \n\na reconhecer padr\u00f5es visuais. As redes se assemelhavam a vers\u00f5es modernas de \n\nredes convolucionais, por\u00e9m, foram treinadas com uma estrat\u00e9gia de refor\u00e7o de \n\nativa\u00e7\u00e3o recorrente em m\u00faltiplas camadas. A arquitetura de rede feita por Fukushima \n\npermitiu que os recursos importantes fossem ajustados manualmente aumentando o \n\n\u201cpeso\u201d de certas conex\u00f5es (Deep Learning Book, Cap 3, 2018). \n\n Muitos dos conceitos de Neocognitron continuam a ser utilizados. O uso de \n\nconex\u00f5es de cima para baixo e novos m\u00e9todos de aprendizagem permitiram a \n\nrealiza\u00e7\u00e3o de uma variedade de redes neurais. Um Neocognitron moderno n\u00e3o s\u00f3 \n\npode identificar padr\u00f5es com informa\u00e7\u00f5es faltantes (por exemplo, um n\u00famero 5 \n\ndesenhado de maneira incompleta), mas tamb\u00e9m pode completar a imagem \n\nadicionando as informa\u00e7\u00f5es que faltam. Isso pode ser descrito como \u201cinfer\u00eancia\u201d. \n\nUma das mais importantes pesquisas demonstrando a aplicabilidade das Redes \n\nNeurais Convolucionais foi realizado por Yann Lecun, ap\u00f3s se unir aos laborat\u00f3rios \n\nAT&amp;T Bell Labs em 1988. Em seu experimento o autor fez uso das redes neurais \n\nconvolucionais para reconhecimento de caracteres escritos \u00e0 m\u00e3o, assim, validando \n\na hip\u00f3tese proposta que as redes convolucionais poderiam ser utilizadas em \n\naplica\u00e7\u00f5es pr\u00e1ticas (Lecun, Y. et al., 1988) \n\nA figura 9 apresenta estrutura da rede convolucional introduzida por Yann Lecun. \n\nComo entrada tem-se uma imagem em preto e branco de 32x32 pixels, nesta imagem \n\naplica-se um filtro convolucional, onde uma convolu\u00e7\u00e3o pode ser interpretada como \n\numa opera\u00e7\u00e3o matem\u00e1tica passando por um sistema ou filtro linear invariante ao \n\ntempo. Ap\u00f3s o filtro convolucional, \u00e9 aplicada uma t\u00e9cnica denominada \n\nsubamostragem (subsampling), que reduz a tamanho da imagem, essa rotina se \n\nrepete at\u00e9 a \u00faltima camada que \u00e9 uma rede neural totalmente conectada, com o \n\nrespectivo n\u00famero de sa\u00eddas de acordo com o n\u00famero de classes. \n\n27\n\n\n\n \n\nFigura 9 -  Rede Neural Convolucional proposta por Lecun, Y \n\n \n\nFonte: Lecun (1988) \n\nEntretanto, uma limita\u00e7\u00e3o que existia era a capacidade de processamento dos \n\ncomputadores, por se tratar de processamento de imagens.  \n\nO pr\u00f3ximo passo evolutivo significativo para o Deep Learning ocorreu em 1999, \n\nquando os computadores come\u00e7aram a se tornar mais r\u00e1pidos no processamento de \n\ndados e Unidades de Processamento de Gr\u00e1fico ou GPUs (Graphics Processing \n\nUnits) foram desenvolvidas. O uso de GPUs significou um salto no tempo de \n\nprocessamento, resultando em um aumento das velocidades computacionais em 1000 \n\nvezes ao longo de um per\u00edodo de 10 anos. Durante esse per\u00edodo, as redes neurais \n\ncome\u00e7aram a competir com m\u00e1quinas de vetor de suporte. Enquanto que num est\u00e1gio \n\ninicial uma rede neural poderia ser lenta em compara\u00e7\u00e3o com uma m\u00e1quina de vetor \n\nde suporte, \u00e0s redes neurais ofereciam melhores resultados usando os mesmos \n\ndados. As redes neurais tamb\u00e9m t\u00eam a vantagem de continuar a melhorar \u00e0 medida \n\nque mais dados de treinamento s\u00e3o adicionados. \n\n Uma vez apresentadas as redes neurais, seu hist\u00f3rico e princ\u00edpio de \n\nfuncionamento. A seguir trabalhos de destaque nacional e internacional utilizando \n\nredes neurais convolucionais para a detec\u00e7\u00e3o de plantas daninhas ser\u00e3o \n\napresentados. \n\n2.6 Redes Neurais Convolucionais na Agroind\u00fastria \n\nAp\u00f3s o aprimoramento gen\u00e9tico das culturas agr\u00edcolas, o uso de Internet das \n\nCoisas e outras ferramentas de vanguarda da Tecnologia da informa\u00e7\u00e3o, como \n\naprendizado de m\u00e1quina e grandes dados (Big Data) s\u00e3o consideradas a pr\u00f3xima \n\ngrande revolu\u00e7\u00e3o agroindustrial. Neste campo f\u00e9rtil de pesquisas e tecnologias, \n\ndiversas t\u00eam sido a aplica\u00e7\u00e3o de redes neurais convolucionais na agricultura de \n\nprecis\u00e3o. Como este trabalho tem o foco na detec\u00e7\u00e3o de plantas daninhas usando \n\n28\n\n\n\n \n\nredes convolucionais, 3 trabalhos de destaque nacional e internacional s\u00e3o \n\napresentados, por sua originalidade e contribui\u00e7\u00e3o para esta \u00e1rea de pesquisa. \n\n2.6.1 Detec\u00e7\u00e3o Autom\u00e1tica e Classifica\u00e7\u00e3o de Plantas Daninhas sob Condi\u00e7\u00f5es \n\nNaturais de Ilumina\u00e7\u00e3o \n\nDyrmann (2017) em seu trabalho Detec\u00e7\u00e3o Autom\u00e1tica e Classifica\u00e7\u00e3o de Plantas \n\nDaninhas sob Condi\u00e7\u00f5es Naturais de Ilumina\u00e7\u00e3o (Automatic Detection and \n\nClassification of Weed Seedlings under Natural Light Conditions) apresenta-se como \n\num dos precursores no uso de redes convolucionais para a detec\u00e7\u00e3o de plantas \n\ndaninhas, em sua tese de Doutorado, em 2017 o autor fez a detec\u00e7\u00e3o de diversas \n\nesp\u00e9cies de plantas daninhas sob diferentes n\u00edveis de ilumina\u00e7\u00e3o e sobreposi\u00e7\u00e3o de \n\nfolhas, ainda propondo uma arquitetura pr\u00f3pria de rede neural convolucional. A \n\nestrutura do algoritmo utilizado pelo autor e apresentado na figura 10. \n\nFigura 10 -  Arquitetura de Aprendizagem Profunda Para Classifica\u00e7\u00e3o de Plantas \nDaninhas \n\n  \n\nFonte: Dyrmann (2017) \n\nA arquitetura inicia-se com a aquisi\u00e7\u00e3o das imagens para a constru\u00e7\u00e3o do banco \n\nde dados,  essas imagens em padr\u00e3o RGB (Red, Green, Blue), tamb\u00e9m s\u00e3o as \n\nimagens de entrada da rede neural (Input Image), neste trabalho o autor coletou estas \n\nimagens manualmente utilizando um celular Nokia Lumia 1020 e por meio de um \n\nVe\u00edculo Terrestre ATV (All Terrain Vehicle), com um computador integrado (Nvidia \n\nTX1), para a coleta autom\u00e1tica das imagens. No total foram coletadas 13.976 \n\nimagens, 4.537 foram anotadas ao longo de 63 dias e tr\u00eas esta\u00e7\u00f5es de crescimento. \n\nEssas 4.537 imagens foram utilizadas como entrada da rede neural.  \n\n29\n\n\n\n \n\nAp\u00f3s a etapa de coleta e cria\u00e7\u00e3o do banco de dados, filtros convolucionais foram \n\naplicados nas imagens Por exemplo, uma imagem de dimens\u00f5es 224x224x3, tendo \n\nrespectivamente 224 pixels de altura, 224 pixels de largura e 3 pixels de profundidade \n\nrepresentando na camada de profundidade as cores vermelho, verde e azul, feita a \n\naplica\u00e7\u00e3o de um filtro convolucional ter\u00e1 as dimens\u00f5es 224x224x64, destacando as \n\ncaracter\u00edsticas principais em sua profundidade e assim gerando mapas de \n\ncaracter\u00edsticas.  \n\nUma vez gerados os mapas de caracter\u00edsticas \u00e9 aplicada uma t\u00e9cnica de \n\ncompress\u00e3o (Pooling), assim, uma imagem com dimens\u00f5es  224x224x64 ap\u00f3s a \n\ncompress\u00e3o, ter\u00e1 as dimens\u00f5es 112x112x128, dessa maneira, comprimindo a altura, \n\nlargura e aumentando a profundidade, sendo etapa essencial para a extra\u00e7\u00e3o e \n\naquisi\u00e7\u00e3o de caracter\u00edsticas relevantes das imagens.  \n\nAp\u00f3s a etapa de compress\u00e3o uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o e aplicada para inserir n\u00e3o \n\nlinearidade a rede, tornando-a mais eficiente na extra\u00e7\u00e3o de informa\u00e7\u00f5es, as principais \n\nfun\u00e7\u00f5es de ativa\u00e7\u00e3o utilizadas s\u00e3o a ReLu (Rectified linear unit ) ou Retificador de \n\nUnidade Linear e a fun\u00e7\u00e3o Sigmoid. As etapas anteriores s\u00e3o repetidas at\u00e9 a imagem \n\nser utilizada como entrada de uma rede neural completamente conectada (Fully \n\nConnected Neural Network) para a classifica\u00e7\u00e3o das plantas daninhas. \n\n Ao final do trabalho o autor conclui que realizou a classifica\u00e7\u00e3o de 17 esp\u00e9cies de \n\nplantas daninhas mais frequentes com uma exatid\u00e3o total de 87% de maneira \n\nautom\u00e1tica, o autor destaca tamb\u00e9m que o m\u00e9todo representa uma economia \n\nimportante para os agricultores em rela\u00e7\u00e3o ao uso de herbicidas, uma vez que pode-\n\nse classificar plantas daninhas sob condi\u00e7\u00f5es de ilumina\u00e7\u00e3o natural. \n\n2.6.2 Detec\u00e7\u00e3o de Plantas Daninhas em Planta\u00e7\u00f5es de Soja usando Redes \n\nConvolucionais \n\n Dos Santos Ferreira, A. (2017), em sua disserta\u00e7\u00e3o de mestrado que \n\nposteriormente resultou no artigo Detec\u00e7\u00e3o de Plantas Daninhas em Planta\u00e7\u00f5es de \n\nSoja usando Redes Convolucionais (Weed detection in soybean crops using \n\nConvNets), fez uso de das redes neurais convolucionais para a classifica\u00e7\u00e3o entre \n\nsoja, solo, plantas daninhas de folha larga e folha estreita.  Neste trabalho o autor \n\ncriou um banco de imagens com mais de 15.000 imagens, al\u00e9m de ter realizado a \n\n30\n\n\n\n \n\ncoleta com um VANT (Veiculo Aereo Nao tripulado) DJI Phanton3 a altura m\u00e9dia de 2 \n\nmetros do solo. O trabalho destaca-se principalmente por ser uma das primeiras \n\npesquisas usando redes convolucionais para a detec\u00e7\u00e3o e classifica\u00e7\u00e3o de plantas \n\ndaninhas no Brasil. \n\nO autor tamb\u00e9m realizou a segmenta\u00e7\u00e3o das imagens do plano de fundo utilizando \n\no algoritmo SLIC (Simple Linear Iterative Clustering) Superpixels ou Simples \n\nClusteriza\u00e7\u00e3o Linear Iterativa de Superpixels. A figura 11 apresenta algumas das \n\nimagens j\u00e1 segmentadas utilizadas no algoritmo. \n\nFigura 11 - Imagens Segmentadas Utilizadas para Classifica\u00e7\u00e3o de Plantas \nDaninhas \n\n \n\nFonte: Dos Santos Ferreira, A. (2017) \n\n Para a etapa de valida\u00e7\u00e3o o autor utilizou 1000 imagens divididas respectivamente \n\nem 250 imagens de solo, 250 de soja, 250 plantas daninhas de folha larga, 250 plantas \n\ndaninhas de folha estreita. O melhor resultado obtido foi para o solo com 100% de \n\nacerto e o pior plantas daninhas de folha estreita com 98,4% de exatid\u00e3o e 246 \n\nimagens classificadas corretas. \n\n O trabalho destaca-se por ser um dos precursores no Brasil em rela\u00e7\u00e3o ao uso de \n\naprendizado profundo para a determina\u00e7\u00e3o e classifica\u00e7\u00e3o de plantas daninhas. \n\n2.6.3 Segmenta\u00e7\u00e3o Sem\u00e2ntica em Tempo Real de Planta\u00e7\u00f5es e Plantas \n\nDaninhas para Rob\u00f4s de Agricultura de Precis\u00e3o com o Conhecimento de Plano \n\nde Fundo em CNNs \n\n Milioto em (2018) em seu trabalho Segmenta\u00e7\u00e3o Sem\u00e2ntica em Tempo Real de \n\nPlanta\u00e7\u00f5es e Plantas Daninhas para Rob\u00f4s de agricultura de Precis\u00e3o com o \n\nConhecimento de Plano de Fundo em CNNs (Real-time Semantic Segmentation of \n\nCrop and Weed for Precision Agriculture Robots Leveraging Background Knowledge \n\nin CNNs) aplicou as redes neurais convolucionais visando a automa\u00e7\u00e3o na detec\u00e7\u00e3o \n\ne elimina\u00e7\u00e3o de plantas daninhas, neste trabalho o autor fez a classifica\u00e7\u00e3o entre a \n\n31\n\n\n\n \n\nbeterraba-sacarina em est\u00e1gio de crescimento inicial, plantas daninhas e plano de \n\nfundo, para isso o autor  fez uso de uma t\u00e9cnica denominada segmenta\u00e7\u00e3o sem\u00e2ntica \n\n(Semantic Segmentation) o que permitiu a detec\u00e7\u00e3o em tempo real das plantas \n\ndaninhas a uma frequ\u00eancia de 20 frames por segundo. \n\nO autor utilizou 15.197 de tr\u00eas locais distintos, as imagens foram coletadas em RGB \n\ne pelo \u00cdndice de Vegeta\u00e7\u00e3o por Diferen\u00e7a Normalizada NDVI (Normalized Difference \n\nVegetation Index), al\u00e9m de 14 \u00edndices usados como entradas da rede neural. Para \n\nefetuar o treinamento e detec\u00e7\u00e3o em tempo real foi utilizado um computador com \n\nprocessador  Intel i7 e placa gr\u00e1fica NVIDIA GTX1080Ti, o computador foi embarcado \n\nem uma plataforma denomina Bonirob (Bosch). Na figura 12 pode-se ver tanto a \n\nplataforma Bonirob quanto as imagens que s\u00e3o processadas. \n\nFigura 12 - Plataforma de Coleta de Imagens Bonirob e Plantas Analisadas \n\nFigura 12 - Plataforma de Coleta de Imagens Bonirob e Plantas Analisadas \n\n \n\nFonte: Milioto et al (2018)  \n\nO trabalho faz parte de uma iniciativa da Uni\u00e3o Europ\u00e9ia denominada Flourish, \n\nprovando-se eficiente na detec\u00e7\u00e3o e elimina\u00e7\u00e3o de plantas daninhas em tempo real, \n\numa vez que o Bonirob conta com pist\u00f5es pneum\u00e1ticos que eliminam mecanicamente \n\nas plantas daninhas, atualmente o sistema trabalha com a frequ\u00eancia de 20hz. \n\n Os 3 tr\u00eas trabalhos trouxeram importantes contribui\u00e7\u00f5es sobre a aplica\u00e7\u00e3o das \n\nredes neurais convolucionais na detec\u00e7\u00e3o e classifica\u00e7\u00e3o de plantas daninhas, o \n\ntrabalho de Dyrmann destaca-se pela classifica\u00e7\u00e3o das plantas daninhas sob \n\ncondi\u00e7\u00f5es naturais de ilumina\u00e7\u00e3o e sobreposi\u00e7\u00e3o de folhas, demonstrando como as \n\nredes neurais profundas s\u00e3o confi\u00e1veis, mesmo em cen\u00e1rios dificultosos. \n\n32\n\n\n\n \n\nO trabalho de Dos Santos Ferreira destaca-se por ser um  dos precursores no \n\nuso de aprendizado profundo para a classifica\u00e7\u00e3o de plantas daninhas no Brasil, al\u00e9m \n\nde ter obtido resultados consistentes na classifica\u00e7\u00e3o no banco de imagens criado, o \n\nautor demonstra o potencial das redes neurais convolucionais para a classifica\u00e7\u00e3o de \n\nplantas daninhas  em planta\u00e7\u00f5es de soja, que atualmente \u00e9 a cultura mais atingida, e \n\ntamb\u00e9m \u00e9 a que consome mais herbicidas no Brasil. \n\n No trabalho de Milioto pode-se ver como ser\u00e1 uma plataforma rob\u00f3tica que \n\nutiliza as redes neurais convolucionais para a classifica\u00e7\u00e3o entre a cultura plantada, \n\nplantas daninhas e solo e que concomitantemente realiza a elimina\u00e7\u00e3o das plantas \n\ndaninhas. Assim, este trabalho engloba tanto o desenvolvimento quanto a aplica\u00e7\u00e3o \n\ndo algoritmo no campo para a correta elimina\u00e7\u00e3o de plantas daninhas. \n\nNo seguinte cap\u00edtulo os materiais, como softwares, o banco de imagens e \n\nm\u00e9todos como as arquiteturas de redes neurais convolucionais s\u00e3o apresentados.   \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n33\n\n\n\n \n\n3 MATERIAIS E M\u00c9TODOS \n\n Para a execu\u00e7\u00e3o das arquiteturas de aprendizado profundo e gera\u00e7\u00e3o de \n\nresultados, seguiu-se uma ordem de etapas, iniciando com o download das imagens, \n\nsele\u00e7\u00e3o e inser\u00e7\u00e3o de ru\u00eddos, cria\u00e7\u00e3o e divis\u00e3o do banco de imagens para treinamento \n\ne valida\u00e7\u00e3o, e finalizando com a cria\u00e7\u00e3o das matrizes de confus\u00e3o e determina\u00e7\u00e3o da \n\nexatid\u00e3o local e total de cada arquitetura. A figura 13 exemplifica o fluxo deste \n\nprocesso descrito.  \n\nFigura 13 - Fluxo metodol\u00f3gico de etapas para a aplica\u00e7\u00e3o das arquiteturas de \naprendizado profundo e gera\u00e7\u00e3o dos resultados \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n Nos itens a seguir todos os elementos principais utilizados para a aplica\u00e7\u00e3o \n\ndos algoritmos de aprendizado profundo, parametriza\u00e7\u00e3o das arquiteturas e cria\u00e7\u00e3o \n\ndo banco de imagens s\u00e3o descritos em maiores detalhes. \n\n3.1 Biblioteca Tensorflow \n\nCriado pela equipe do Google Brain, a biblioteca TensorFlow  \u00e9 uma biblioteca \n\nde c\u00f3digo aberto para computa\u00e7\u00e3o num\u00e9rica e aprendizado de m\u00e1quina em larga \n\nescala. O TensorFlow re\u00fane v\u00e1rios modelos e algoritmos de aprendizado de m\u00e1quina \n\ne aprendizagem profunda (tamb\u00e9m conhecido como redes neurais). \u00c9 feito uso da \n\nlinguagem de programa\u00e7\u00e3o Python para fornecer uma API (Application Programming \n\nInterface) ou Interface de Programa\u00e7\u00e3o de Aplicativos conveniente para construir \n\naplicativos com a estrutura, enquanto executa-se esses aplicativos em C ++ para alto \n\ndesempenho (Abadi, M. et al., 2015) \n\nPor meio do TensorFlow pode-se treinar e executar redes neurais profundas \n\npara classifica\u00e7\u00e3o manuscrita de d\u00edgitos, reconhecimento de imagens, incorpora\u00e7\u00e3o \n\nde palavras, redes neurais recorrentes, processamento de linguagem natural e \n\nsimula\u00e7\u00f5es baseadas em equa\u00e7\u00f5es diferenciais parciais. O TensorFlow tamb\u00e9m \n\n34\n\n\n\n \n\nsuporta previs\u00e3o de produ\u00e7\u00e3o em escala, com os mesmos modelos usados para \n\ntreinamento. \n\nPossibilita a cria\u00e7\u00e3o de gr\u00e1ficos de fluxo de dados, que descrevem como os \n\ndados se movem atrav\u00e9s de um gr\u00e1fico ou uma s\u00e9rie de n\u00f3s de processamento. Cada \n\nn\u00f3 no gr\u00e1fico representa uma opera\u00e7\u00e3o matem\u00e1tica e cada conex\u00e3o ou borda entre \n\nos n\u00f3s \u00e9 uma matriz de dados ou um tensor multidimensional. Isso \u00e9 fornecido por \n\nmeio da linguagem Python. O Python fornece maneiras convenientes de expressar \n\nabstra\u00e7\u00f5es de alto n\u00edvel podem ser acopladas.  \n\nAs opera\u00e7\u00f5es matem\u00e1ticas reais, no entanto, n\u00e3o s\u00e3o executadas no Python. \n\nAs bibliotecas de transforma\u00e7\u00f5es que est\u00e3o dispon\u00edveis atrav\u00e9s do TensorFlow s\u00e3o \n\nescritas como bin\u00e1rios C++ de alto desempenho. O Python apenas direciona o tr\u00e1fego \n\nentre as partes e fornece abstra\u00e7\u00f5es de programa\u00e7\u00e3o de alto n\u00edvel para conect\u00e1-las. \n\nOs modelos resultantes criados pelo TensorFlow, podem ser implantados na maioria \n\ndos dispositivos em que ser\u00e3o usados para exibir previs\u00f5es. Para este trabalho, foram \n\ntreinados modelos contidos no Keras e o TensorFlow foi utilizado como plano de \n\nfundo. \n\n3.2 Biblioteca Keras \n\n Keras \u00e9 definida como uma API (Application Programming Interface) ou Interface \n\nde Programa\u00e7\u00e3o de Aplicativos de alto n\u00edvel para redes neurais, capaz de executar \n\nbibliotecas como o TensorFlow, que \u00e9 uma biblioteca aberta para computa\u00e7\u00e3o \n\nnum\u00e9rica de alta performance. Diferente do TensorFlow onde a modelagem da rede \n\nneural \u00e9 feita graficamente, o Keras possibilita a constru\u00e7\u00e3o modular, linear e \n\nsequencial das redes neurais convolucionais (Chollet., 2015).  \n\nO Keras propriamente n\u00e3o faz suas pr\u00f3prias opera\u00e7\u00f5es de baixo n\u00edvel, como \n\nprodutos tensoriais e convolu\u00e7\u00f5es; Ele depende de um mecanismo de plano de fundo \n\npara isso, seu plano de fundo prim\u00e1rio (e padr\u00e3o) \u00e9 o TensorFlow, a API Keras vem \n\nempacotada no TensorFlow como tf.keras. \n\n A API foi criado para ser modular, f\u00e1cil de entender e trabalhar com o Python. \n\nCamadas neurais, fun\u00e7\u00f5es de custo, otimizadores, esquemas de inicializa\u00e7\u00e3o, \n\nfun\u00e7\u00f5es de ativa\u00e7\u00e3o e esquemas de regulariza\u00e7\u00e3o s\u00e3o todos m\u00f3dulos independentes \n\n35\n\n\n\n \n\nque podem ser combinados para criar novos modelos. Os modelos s\u00e3o definidos em \n\nc\u00f3digo Python, n\u00e3o em arquivos de configura\u00e7\u00e3o de modelo separados. \n\n Al\u00e9m da simplicidade estrutural, atrav\u00e9s do Keras \u00e9 poss\u00edvel importar arquiteturas \n\nde redes neurais convolucionais pr\u00e9-treinadas, dessa maneira aprimorando a etapa \n\nde treinamento da rede, assim obtendo-se maior precis\u00e3o com menor tempo de \n\nprocessamento. O quadro 3 apresenta as arquiteturas de redes neurais \n\nconvolucionais pr\u00e9-treinadas que foram utilizadas para este trabalho. Originalmente \n\nestas arquiteturas foram treinadas com o banco de imagens do  ILSVRC (Imagenet \n\nLarge Scale Visual Recognition Challenge) ou Imagenet Desafio de Reconhecimento \n\nVisual em Larga Escala (Deng, J. et al ., 2009). \n\nQuadro 3 - Arquiteturas de Redes Neurais Profundas Armazenadas no Keras e \nusadas neste trabalho \n\nModelo Tamanho Exatid\u00e3o Top-5 Par\u00e2metros Profundidade \n\nVGG16 528 MB 0,901 138.357.544 23 \n\nResNet50 99 MB 0,921 25.636.712 168 \n\nInceptionV3 92 MB 0,937 23.851.784 159 \n\nInceptionResnetV2 215 MB 0,953 55.873.736 572 \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\nQuatro arquiteturas de redes neurais convolucionais foram treinadas no banco \n\nde imagens criado para este trabalho, seguindo a ordem cronol\u00f3gica do lan\u00e7amento \n\ndas arquiteturas treinou-se: VGG16, ResNet50, InceptionV3 e InceptionResnetV2. \n\nA rede VGG16 possui mais par\u00e2metros e maior tamanho,  assim tendo um \n\ntempo de treinamento maior que as demais redes. Em contrapartida a rede \n\nInceptionV3 \u00e9 a rede com menos par\u00e2metros e menor tamanho tamb\u00e9m, tendo seu \n\ntempo de treinamento reduzido. A rede mais profunda \u00e9 a InceptionResnetV2 com 572 \n\ncamadas e menos da metade do tamanho e n\u00famero de par\u00e2metros da rede VGG16. \n\nO item exatid\u00e3o do Top-5 significa que qualquer uma das 5 respostas de \n\nprobabilidade mais alta da arquitetura deve corresponder \u00e0 resposta esperada. Estes \n\nresultados foram obtidos aplicando essas arquiteturas no banco de imagens do \n\nILSVRC, sendo que a arquitetura InceptionResnetV2 apresenta a melhor performance \n\ncom 0,953 ou 95,3% seguida pela InceptionV3 com 93,7%, ResNet50 92,1% e VGG16 \n\n36\n\n\n\n \n\ncom 90,1%. espera-se portanto obter resultados e uma ordem semelhantes aos \n\nobtidos pelas arquiteturas anteriormente.  \n\nNos itens a seguir detalhes estruturais das arquiteturas pr\u00e9-treinadas no \n\nILSVRC, que foram utilizadas neste trabalho, assim como sua performance, \n\nconstru\u00e7\u00e3o e  aplica\u00e7\u00e3o s\u00e3o descritos. \n\n3.3 Arquiteturas de Redes Neurais Convolucionais \n\nDesde os resultados obtidos por  Krizhevsky em 2012, diversas foram as \n\narquiteturas de redes neurais convolucionais desenvolvidas (Krizhevsky., 2012). Para \n\neste trabalho foram selecionadas 4 arquiteturas de rede, cujos pesos foram treinados \n\ncom base no (ILSVRC)  (Deng, J. et al ., 2009) e encontram-se dispon\u00edveis por meio \n\nda aplica\u00e7\u00e3o Keras, s\u00e3o estas redes: \n\n3.3.1 VGG16: Redes Convolucionais Muito Profundas para Reconhecimento de \n\nImagem em Grande Escala  \n\n Em   2015 (Simonyan, K;  Zisserman, A. 2015),  em seu trabalho (VGG16: Very \n\nDeep Convolutional Networks for Large-Scale Image Recognition) ou VGG16: Redes \n\nConvolucionais Muito Profundas para Reconhecimento de Imagem em Grande \n\nEscala, propuseram uma arquitetura de rede neural convolucional profunda, tendo 2 \n\nvariantes VGG16 e VGG19.  A figura 14 representa a constru\u00e7\u00e3o e estrutura desta \n\narquitetura. \n\nFigura 14 - Arquitetura VGG16 \n\n \n\nFonte: K. Simonyan;  A. Zisserman ( 2015)  \n\n37\n\n\n\n \n\n A arquitetura tem como entradas imagens em RGB tendo dimens\u00f5es de 224x224x3 \n\npixels, que representa respectivamente altura, largura e profundidade da imagem. \n\nLogo em de in\u00edcio \u00e9 aplicado um filtro Convolucional e uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o de um \n\nRetificador de Unidade Linear (Convolutional+ReLu), tendo-se como resultado uma \n\nimagem de dimens\u00f5es 224x224x64 pixels, aumentando a profundidade da imagem \n\nque est\u00e1 sendo analisada. Em seguida \u00e9 aplicado um filtro para a redu\u00e7\u00e3o de \n\ndimensionalidade da imagem, no exemplo acima ap\u00f3s a aplica\u00e7\u00e3o do filtro de m\u00e1ximo \n\nlocal  (max pooling) a  entrada tem suas dimens\u00f5es reduzidas de 224x224x64 para \n\n112x112x128, pode-se perceber que h\u00e1 uma diminui\u00e7\u00e3o na altura e comprimento da \n\nimagem, por\u00e9m n\u00e3o na profundidade, pelo contr\u00e1rio ao longo dos filtros convolucionais \n\ne os filtros de m\u00e1ximo local a imagem torna-se mais profunda. Isso deve-se pelo fato \n\nde que as caracter\u00edsticas ou informa\u00e7\u00f5es mais importantes sobre as imagem est\u00e3o \n\nem sua profundidade e ainda visa evitar o sobreajuste,que \u00e9 um termo usado em \n\nestat\u00edstica para descrever quando um modelo estat\u00edstico se ajusta muito bem ao \n\nconjunto de dados anteriormente observado, mas se mostra ineficaz para prever \n\nnovos resultados.  \n\n Ap\u00f3s repetidos filtros convolucionais, fun\u00e7\u00f5es de ativa\u00e7\u00e3o de retificador de unidade \n\nlinear chega-se a pen\u00faltima etapa que \u00e9 uma rede neural completamente conectada \n\ne em sua sa\u00edda  um retificador de unidade linear nesta pen\u00faltima etapa a imagem tem \n\ndimens\u00f5es 1x1x4096. Por fim \u00e9 aplicada uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o softmax, na \n\nmatem\u00e1tica, a fun\u00e7\u00e3o softmax usa um vetor n\u00e3o normalizado e a normaliza em uma \n\ndistribui\u00e7\u00e3o de probabilidade, em aprendizado profundo a fun\u00e7\u00e3o softmax \u00e9 usada \n\npara classifica\u00e7\u00e3o. No caso das imagens do ILSVRC significa classificar as imagens \n\nentre as 1000 classes do desafio. \n\n Para este trabalho foram usadas as dimens\u00f5es recomendadas para a arquitetura \n\nVGG16, ent\u00e3o todas as imagens de entrada desta rede tinham dimens\u00f5es 224x224x3 \n\npixels. \n\n3.3.2 ResNet-50: Aprendizagem Residual Profunda para Reconhecimento de \n\nImagem \n\n Em 2016 (He. et al., 2016), propuseram uma arquitetura de rede neural profunda \n\nresidual (Resnet) em seu artigo (Deep Residual Learning for Image Recognition) ou \n\nAprendizagem Residual Profunda para Reconhecimento de Imagem. Nesta \n\n38\n\n\n\n \n\narquitetura, os blocos residuais foram propostos com o prop\u00f3sito de resolver o \n\nseguinte problema, com o aumento da profundidade da rede, a exatid\u00e3o fica saturada \n\ne depois se degrada rapidamente. Assim como a rede VGG16 a rede Resnet tem \n\ncomo entradas imagens de dimens\u00e3o 224x224x3 pixels. A ideia central da ResNet \u00e9 \n\nintroduzir a chamada \u201cconex\u00e3o de atalho de identidade\u201d que pula uma ou mais \n\ncamadas, como mostra a figura 15. \n\nFigura 15 -  Bloco Residual Rede Resnet \n\n \n\nFonte: He, K. et al. (2016) \n\n Os atalhos de identidade podem ser usados diretamente quando a entrada e sa\u00edda \n\ns\u00e3o das mesmas dimens\u00f5es. Quando as dimens\u00f5es mudam: A) O atalho ainda \n\nexecuta o mapeamento de identidade, com entradas extras de zero preenchidas com \n\na dimens\u00e3o aumentada; B) O atalho de proje\u00e7\u00e3o \u00e9 usado para coincidir com a \n\ndimens\u00e3o.  \n\n Cada bloco do ResNet tem 2 camadas de profundidade (usado em redes pequenas \n\ncomo ResNet 18, 34) ou 3 camadas de profundidade (ResNet 50, 101, 152). Os \n\ndemais componentes da arquitetura mant\u00e9m-se como os descritos anteriormente na \n\narquitetura VGG16, tendo ao final uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o softmax para realizar a \n\nclassifica\u00e7\u00e3o. \n\n3.3.3 InceptionV3: Repensando a Arquitetura Inception para Vis\u00e3o \n\nComputacional \n\n A InceptionV3 (Szegedy, C. et al., 2016) \u00e9 um modelo de reconhecimento de \n\nimagem amplamente utilizado, o modelo \u00e9 o culminar de muitas ideias desenvolvidas \n\npor v\u00e1rios pesquisadores ao longo dos anos. \u00c9 baseado no artigo original: \n\n\u201cRepensando a Arquitetura Inception para Vis\u00e3o Computacional (Rethinking the \n\nInception Architecture for Computer Vision)\u201d de Szegedy, et. al. A figura 16 ilustra a \n\narquitetura da rede. \n\n39\n\nhttps://arxiv.org/abs/1512.00567\nhttps://arxiv.org/abs/1512.00567\n\n\n \n\nFigura 16 - Arquitetura InceptionV3 \n\n \n\nFonte Szegedy, C. et al. (2015) \n\n Nesta arquitetura tem-se como entrada imagens de dimens\u00f5es 299x299x3 pixels, \n\no principal diferencial desta arquitetura foi a introdu\u00e7\u00e3o de m\u00f3dulos Inception, que \n\nintroduziu a concatena\u00e7\u00e3o de diferentes filtros convolucionais (Concat) de distintos \n\ntamanhos, por exemplo, filtros 5x5, 3x3 ou at\u00e9 1x1, fazendo parte de um m\u00f3dulo \n\nInception. Foi introduzido tamb\u00e9m o m\u00f3dulo abandono (Dropout), o abandono \u00e9 uma \n\nt\u00e9cnica de regulariza\u00e7\u00e3o patenteada pelo Google para reduzir o sobreajuste \n\n(overfitting) em redes neurais, evitando co-adapta\u00e7\u00f5es complexas nos dados de \n\ntreinamento. \u00c9 uma maneira muito eficiente de executar a m\u00e9dia do modelo com redes \n\nneurais. Tamb\u00e9m usa-se agrupamento m\u00e9dio (AvgPool). O agrupamento m\u00e1ximo \n\n(MaxPool) extrai os caracter\u00edsticas mais importantes da imagem, como as bordas, \n\nenquanto o agrupamento m\u00e9dio (AvgPool) extrai os recursos de maneira mais suave. \n\nA figura 17 mostra a diferen\u00e7a entre o agrupamento m\u00e1ximo e m\u00e9dio. \n\nFigura 17 - Diferen\u00e7a entre Agrupamento M\u00e9dio e M\u00e1ximo \n\n \n\nFonte: Rahman, N (2017) \n\n40\n\n\n\n \n\n3.2.4 Inception-v4, Inception-ResNet e o Impacto das Conex\u00f5es Residuais no \n\nAprendizado \n\n Inception-ResNet-v2 \u00e9 baseada no artigo Inception-v4, Inception-ResNet e o \n\nImpacto das Conex\u00f5es Residuais no Aprendizado ou (Inception-v4, Inception-ResNet \n\nand the Impact of Residual Connections on Learning) (Szegedy, C; Ioffe, S;  \n\nVanhoucke, V., 2016)  \u00e9 uma varia\u00e7\u00e3o do  modelo InceptionV3, com alguns dos itens \n\nda rede ResNet. Como a rede InceptionV3 as imagens de entrada possuem dimens\u00e3o \n\n299x299x3, na figura 18 \u00e9 exemplifica a constru\u00e7\u00e3o da rede. \n\nFigura 18 - Arquitetura InceptionResnetV2 \n\n \n\nFonte: Szegedy, C; Ioffe, S;  Vanhoucke, V.  (2016) \n\n Como principal inova\u00e7\u00e3o desta arquitetura em compara\u00e7\u00e3o com a InceptionV3 \n\ndestaca-se a inclus\u00e3o de um m\u00f3dulo residual, realizando a opera\u00e7\u00e3o descrita \n\nanteriormente na arquitetura ResNet50.Todos os modelos citados acima podem ser \n\nencontrados no Keras, assim possibilitando o uso de redes pr\u00e9-treinadas.  \n\n3.4 Obten\u00e7\u00e3o do Banco de Imagens \n\nPara a aplica\u00e7\u00e3o das arquiteturas de redes neurais descritas, criou-se um \n\nbanco de imagens. Como \u00e9 inten\u00e7\u00e3o deste trabalho analisar a capacidade das redes \n\nde extrair informa\u00e7\u00f5es relevantes e efetuar a classifica\u00e7\u00e3o entre as 5 esp\u00e9cies de \n\nplantas daninhas de maneira eficaz, foi realizada a obten\u00e7\u00e3o de diferentes imagens \n\ndas 5 esp\u00e9cies. Para realizar esta descarga(download) aplicou-se um algoritmo para \n\n41\n\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\nhttp://arxiv.org/abs/1602.07261\n\n\n \n\nque automaticamente descarrega-se as imagens baixadas com base em seu nome \n\ncient\u00edfico que foi pesquisado no buscador Google Imagens. \n\nO banco de dados inicial continha centenas de imagens de cada esp\u00e9cie, foi \n\nrealizada uma filtragem manual destas imagens uma vez que muitas das imagens \n\nbaixadas n\u00e3o tinham conex\u00e3o com as esp\u00e9cies de planta daninhas que ser\u00e3o \n\nclassificadas. A?os as etapas de descarga e filtragem das imagens foi inserido ru\u00eddos \n\nartificialmente nas imagens. \n\n3.5 Inser\u00e7\u00e3o de Ru\u00eddos \n\n Foi inserido artificialmente ru\u00eddos nas imagens baixadas por dois motivos, o \n\nprimeiro \u00e9 de fornecer mais dados (imagens) para o treinamento. A t\u00e9cnica do \n\naumento dos dados atrav\u00e9s da inser\u00e7\u00e3o de ru\u00eddo \u00e9 conhecida como aumento de dados \n\nou (data augmentation). Essa t\u00e9cnica possibilita o aumento do banco de imagens, \n\nassim, permitindo que a arquitetura obtenha resultados mais consistentes tanto em \n\nseu treinamento quanto na valida\u00e7\u00e3o. \n\n O segundo motivo trata-se de treinar as arquiteturas aqui estudadas sob \n\nimagens em diferentes condi\u00e7\u00f5es, como rota\u00e7\u00e3o, embasamento, diferentes planos de \n\nfundo, ilumina\u00e7\u00e3o e diferentes dimens\u00f5es de objetos. Assim tornando as redes \n\ninvariantes quanto \u00e0s mudan\u00e7as de ilumina\u00e7\u00e3o, rota\u00e7\u00e3o e dimensionamento, \n\ntornando-as menos propensas a falhas na classifica\u00e7\u00e3o ou at\u00e9 mesmo sobreajuste. \n\nExemplos de imagens com e sem ru\u00eddos s\u00e3o  apresentados nas figuras 19 e 20. \n\nFigura 19 - Compara\u00e7\u00e3o entre uma imagem da esp\u00e9cie Capim Azev\u00e9m sem ru\u00eddo \n(a), e com ru\u00eddo inserido (b) \n\n \n  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n   (a)     (b) \nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n \n\n \n\n42\n\n\n\n \n\nFigura 20 - Compara\u00e7\u00e3o entre uma imagem da esp\u00e9cie Capim P\u00e9 de Galinha sem \nru\u00eddo (a), e com ru\u00eddo inserido (b) \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n(a)                   (b) \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nNa figura 19 tem-se uma imagem da esp\u00e9cie Capim Azev\u00e9m sem ru\u00eddos (a) e \n\nao seu lado outra imagem com inser\u00e7\u00e3o de ru\u00eddos (b), percebe-se tamb\u00e9m na figura \n\n20 uma planta daninha da esp\u00e9cie Capim p\u00e9 de Galinha sem inser\u00e7\u00e3o de ru\u00eddos (a) e \n\ncom inser\u00e7\u00e3o de ru\u00eddos (b). Cabe salientar que n\u00e3o foi aplicada t\u00e9cnica alguma de \n\nsegmenta\u00e7\u00e3o (separa\u00e7\u00e3o do objeto de interesse do plano de fundo), ou seja as \n\narquiteturas de rede s\u00e3o alimentados com os dados brutos, o que torna a tarefa mais \n\ndificultosa para a classifica\u00e7\u00e3o, uma vez que cabe a rede distinguir entre a esp\u00e9cie de \n\nplanta daninha e todo o plano de fundo. Ao total foram utilizadas 3.500 imagens \n\ndivididas entre as 5 classes (Buva, Capim Amargoso, Capim Azev\u00e9m, Capim p\u00e9 de \n\nGalinha e Caruru) para o treinamento e 1.500 imagens divididas entre as 5 classes \n\npara a valida\u00e7\u00e3o, totalizando 5.000 do banco de imagens criado. \n\n  \n\n3.6 Parametriza\u00e7\u00e3o do Algoritmo \n\nPara a aplica\u00e7\u00e3o das 4 arquiteturas de redes no banco de imagens descrito \n\nanteriormente, foi implementado um algoritmo de aprendizado profundo com o intuito \n\nde atingir-se os objetivos propostos. O algoritmo foi desenvolvido e implementado na \n\nlinguagem de programa\u00e7\u00e3o Python, sendo que \u00e9 constitu\u00eddo de diversos itens, tendo \n\ndestaque: \n\n1) N\u00famero de \u00c9pocas: Inicialmente todas as arquiteturas foram treinadas com 20 \n\n\u00e9pocas. Cada \u00e9poca representa um ciclo de treinamento, desta maneira as \n\n3.500 imagens selecionadas para o treinamento foram treinadas uma vez, \n\nassim, esse processo repetiu-se 20 vezes. \n\n43\n\n\n\n \n\n2) Taxa de Aprendizado: Foi utilizada uma taxa de aprendizado constante de \n\n0,0001. A taxa de aprendizado determina a velocidade do treinamento da rede \n\nneural, uma taxa muito alta, pode causar a perda de dados substanciais. \n\nenquanto uma taxa muito baixa poder\u00e1 deixar o aprendizado da rede \n\nestagnado. \n\n3) Comprimento e Largura das imagens: Nesta se\u00e7\u00e3o determina-se o \n\ncomprimento e altura das imagens, como foi visto anteriormente as imagens \n\nvariam de 224x224 a 299x299 de comprimento e altura de acordo com a \n\narquitetura que foi utilizada. \n\n4) Taxa de Ajuste Fino: O ajuste fino \u00e9 um conceito de aprendizagem de \n\ntransfer\u00eancia. Aprendizagem de transfer\u00eancia \u00e9 uma t\u00e9cnica de aprendizado \n\nde m\u00e1quina, em que o ganho de conhecimento durante o treinamento em um \n\ntipo de problema \u00e9 usado para treinar em outra tarefa ou dom\u00ednio relacionado. \n\nNesse caso possibilitando que as arquiteturas utilizem pesos pr\u00e9-treinados, \n\ncom base nos resultados obtidos anteriormente na competi\u00e7\u00e3o ILSVRC. \n\n5) Perda de Entropia Cruzada Categ\u00f3rica: Para determinar a perda foi utilizado a \n\nPerda de Entropia Cruzada Categ\u00f3rica, que \u00e9 pr\u00f3pria para a determina\u00e7\u00e3o de \n\nperda em classifica\u00e7\u00e3o de v\u00e1rias classes (Esp\u00e9cies de Plantas Daninhas).  \n\nTamb\u00e9m conhecida como perda de log usa-se essa m\u00e9trica para medir o \n\ndesempenho de um modelo de classifica\u00e7\u00e3o. A perda de entropia cruzada \n\naumenta \u00e0 medida que  a classe prevista diverge do r\u00f3tulo real. \n\n6) Matriz de confus\u00e3o: Ao final do treinamento e valida\u00e7\u00e3o uma matriz de confus\u00e3o \n\n\u00e9 gerada para cada arquitetura, possibilitando principalmente verificar a \n\nexatid\u00e3o de cada arquitetura. \n\nA seguir s\u00e3o apresentadas tanto as matrizes de confus\u00e3o geradas para cada \n\narquitetura, quanto a exatid\u00e3o, perda e os resultados gerais das arquiteturas \n\nanalisadas. \n\n \n\n \n\n \n\n \n\n44\n\n\n\n \n\n4 RESULTADOS E DISCUSS\u00d5ES \n\n Para a obten\u00e7\u00e3o dos resultados, 4 arquiteturas de redes neurais convolucionais \n\nforam treinadas em  3.500 imagens. A seguir os gr\u00e1ficos criados a partir dos dados \n\ndas 20 \u00e9pocas de treinamento s\u00e3o apresentados. Estes gr\u00e1ficos foram criados com \n\nbase nas informa\u00e7\u00f5es que cada arquitetura exibe a cada \u00e9poca, os dados s\u00e3o perda \n\ne exatid\u00e3o referente a etapa de treinamento com 3.500 imagens, perda de valida\u00e7\u00e3o \n\ne exatid\u00e3o de valida\u00e7\u00e3o referente a valida\u00e7\u00e3o das 1.500 imagens distribu\u00eddas entre as \n\n5 esp\u00e9cies de plantas daninhas. \n\n A primeira m\u00e9trica computada e apresentada para as 4 arquiteturas \u00e9 a perda \n\nna etapa de treinamento. Para o c\u00e1lculo da perda utilizou-se a fun\u00e7\u00e3o de Perda de \n\nEntropia Cruzada Categ\u00f3rica, que \u00e9 pr\u00f3pria para a determina\u00e7\u00e3o de perda em \n\nclassifica\u00e7\u00e3o de v\u00e1rias classes. Esta fun\u00e7\u00e3o pode ser definida de acordo com a \n\nseguinte equa\u00e7\u00e3o. \n\n? =  ?\n1\n\n?\n ? [? ln ? + (1 ? ?) ln (1 ? ?)\n\n?\n \n\n Nessa equa\u00e7\u00e3o a sa\u00edda do neur\u00f4nio \u00e9 a = ?(z), onde z = ?jwjxj + b \u00e9 a soma \n\nponderada das entradas.  O n\u00famero total de itens de dados de treinamento \u00e9 \n\nrepresentado por n, sendo somadas todas as entradas de treinamento da rede neural \n\nem x, tendo y como sa\u00edda desejada. A fun\u00e7\u00e3o de perda por entropia cruzada \n\ncateg\u00f3rica comporta-se da seguinte maneira, \u00e0 medida que a arquitetura de \n\naprendizado profundo se torna mais exata na determina\u00e7\u00e3o das distintas classes de \n\nplantas daninhas a perda de entropia cruzada categ\u00f3rica diminui tendendo ao zero. \n\nPor outro lado caso a arquitetura passe a falhar na classifica\u00e7\u00e3o a perda de entropia \n\ncruzada categ\u00f3rica aumentar\u00e1, isso ocorre devido a uma penaliza\u00e7\u00e3o desta fun\u00e7\u00e3o de \n\nperda. Caso fosse necess\u00e1rio fazer a classifica\u00e7\u00e3o entre 2 classes poderia-se utilizar \n\numa fun\u00e7\u00e3o de Perda de Entropia Cruzada Bin\u00e1ria. \n\nPara todas as arquiteturas a perda de entropia cruzada categ\u00f3rica inicia-se com \n\nvalores acima de 1. Esta m\u00e9trica \u00e9 importante principalmente para compreender como \n\na rede se comportou na classifica\u00e7\u00e3o, uma vez que uma perda alta, ou seja igual ou \n\nacima de 1 resultar\u00e1 em uma classifica\u00e7\u00e3o falha.  Na figura 21 s\u00e3o apresentados os \n\nvalores de perda ao longo das 20 \u00e9pocas de treinamento para as arquiteturas. \n\n \n\n45\n\n\n\n \n\nFigura 21 - Gr\u00e1fico de Perda de Treinamento das Arquiteturas \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nOs resultados de perda distinguem-se entre as arquiteturas, enquanto que para \n\na arquitetura VGG16 o m\u00ednimo de perda foi de 0,7088, na arquitetura ResNet50 teve-\n\nse um m\u00ednimo de perda de 0,1407. Foi percebido tamb\u00e9m uma queda dr\u00e1stica no valor \n\nde perda na rede ResNet50, na primeira e?oca apresentava perda de 1,1040 e na \n\nsegunda \u00e9poca 0,6134. As arquiteturas InceptionV3 e InceptionResNetV2 \n\napresentaram uma diminui\u00e7\u00e3o no valor de perda gradual, tendo como m\u00ednimo de \n\nperda respectivamente 0,3350 e 0,4460. \n\nNa figura 22 a exatid\u00e3o de treinamento das arquiteturas de redes neurais \n\nconvolucionais \u00e9  apresentada. \n\nFigura 22 - Gr\u00e1fico de Exatid\u00e3o de Treinamento das Arquiteturas \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n46\n\n\n\n \n\n \n\nDas arquiteturas analisadas a ResNet50 teve maior exatid\u00e3o com 95,29% ao \n\nfinal, seguida pela InceptionV3 com 87,81%, InceptionResNetV2 com 83,53% e por \n\n\u00faltimo VGG16 com 73,14%. \u00c9 importante mencionar que n\u00e3o necessariamente uma \n\nperformance muito boa na etapa de treinamento resultar\u00e1 em uma alta exatid\u00e3o na \n\netapa de valida\u00e7\u00e3o. Isso se deve pelo fen\u00f4meno denominado sobreajuste (overfitting). \n\nQuando o sobreajuste ocorre, representa que a arquitetura de rede neural testada \n\ndecorou o conjunto de dados propostos para o treinamento, mas n\u00e3o aprendeu a \n\ndetectar caracter\u00edsticas importantes do conjunto de dados que levaria a arquitetura \n\nobter bons resultados na etapa de valida\u00e7\u00e3o. Outra  informa\u00e7\u00e3o  que pode ser melhor \n\nverificado nas matrizes de confus\u00e3o \u00e9 a performance das arquiteturas para classificar \n\ncada esp\u00e9cie de planta daninha, uma vez que a arquitetura pode ter uma performance \n\nalta para classifica\u00e7\u00e3o de uma ou mais esp\u00e9cies, entretanto ter uma performance \n\ninferior para a classifica\u00e7\u00e3o das demais esp\u00e9cies. Na figura 23 \u00e9 apresentada a perda \n\nde valida\u00e7\u00e3o, possibilitando verificar o desempenho das rede na etapa de valida\u00e7\u00e3o. \n\nFigura 23 - Gr\u00e1fico de Perda de Valida\u00e7\u00e3o das Arquiteturas \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n Conforme mencionado anteriormente, somente a exatid\u00e3o de treinamento, n\u00e3o \n\n\u00e9 o suficiente para determinar a performance de uma arquitetura. Nesse caso de \n\nacordo com as informa\u00e7\u00f5es contidas na figura 23, a perda de valida\u00e7\u00e3o na rede \n\nResNet50 aumentou, o que significa que a rede n\u00e3o aprendeu, pelo contr\u00e1rio \u201cdecorou \n\nas informa\u00e7\u00f5es\u201d. Em rela\u00e7\u00e3o \u00e0s outras arquiteturas rede InceptionV3 teve melhor \n\nperformance com 0,6253 de perda de valida\u00e7\u00e3o, seguida pela rede VGG16 com \n\n47\n\n\n\n \n\n0,6425 e InceptionResNetV2 com 0,8120.Nos gr\u00e1ficos a seguir pode-se verificar a \n\nexatid\u00e3o de valida\u00e7\u00e3o de cada arquitetura, assim, podendo-se inferir, quais destas \n\nobtiveram melhor performance. Na figura 24 a exatid\u00e3o de verifica\u00e7\u00e3o das arquiteturas \n\n\u00e9 apresentado, entretanto com base nos dados da perda de valida\u00e7\u00e3o pode-se \n\nconcluir que a arquitetura ResNet50 ter\u00e1 uma performance ruim e a arquitetura \n\nInceptionV3 ter\u00e1 uma performance boa. \n\nFigura 24 - Gr\u00e1fico de Exatid\u00e3o de Valida\u00e7\u00e3o das Arquiteturas \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nAs arquiteturas VGG16, InceptionV3 e InceptionResnetV2, apresentaram um \n\ncrescimento na exatid\u00e3o de valida\u00e7\u00e3o, cada uma com respectivamente 76,68% na \n\nd\u00e9cima oitava \u00e9poca, 79,37 na d\u00e9cima oitava \u00e9poca e 77,89% na decima sexa \u00e9poca. \n\nEsse dado \u00e9 importante, pois atrav\u00e9s dele j\u00e1 se pode detectar que n\u00e3o houve \n\nsobreajuste nessas arquiteturas. Uma vez que se atinja seja o valor m\u00ednimo de perda \n\nou m\u00e1ximo de exatid\u00e3o esse valor mant\u00e9m-se, mesmo nas pr\u00f3ximas \u00e9pocas de \n\ntreinamento o valor seja superior no caso da perda ou inferior no caso da exatid\u00e3o. A \n\narquitetura ResNet50 apresentou uma estagna\u00e7\u00e3o mantendo-se nos 20,03% de \n\nexatid\u00e3o de valida\u00e7\u00e3o. Para sumarizar a performance das redes, nas figuras 25 e 26 \n\ns\u00e3o apresentados os valores de perda e exatid\u00e3o para a arquitetura VGG16, tanto na \n\netapa de treinamento quanto de valida\u00e7\u00e3o, nas figuras 27 e 28 para a arquitetura \n\nResNet50, nas figuras 29 e 30 para a arquitetura InceptionV3 e nas figuras 31 e 32 \n\npara a arquitetura InceptionResNetV2. \n\n48\n\n\n\n \n\nFigura 25 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o VGG16 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 26 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o VGG16 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n49\n\n\n\n \n\nFigura 27- Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o ResNet50 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 28 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o ResNet50 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n50\n\n\n\n \n\nFigura 29 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o InceptionV3 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 30 - Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o InceptionV3 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n51\n\n\n\n \n\nFigura 31 - Gr\u00e1fico de Perda de Treinamento e Valida\u00e7\u00e3o InceptionResNetV2 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 32- Gr\u00e1fico de Exatid\u00e3o de Treinamento e Valida\u00e7\u00e3o InceptionResNetV2 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nDas quatro arquiteturas a rede VGG16 tem o desempenho mais linear \n\nconforme as figuras 25 e 26, esse desempenho deve-se principalmente pelo numero \n\nde parametros que a rede tem 138.357.544, conforme apresentado no quadro 3, o \n\nn\u00famero elevado de par\u00e2metros torna o tempo de treinamento mais longo para esta \n\nrede, em contrapartida tanto os resultados de treinamento e valida\u00e7\u00e3o s\u00e3o lineares, \n\nestando pr\u00f3ximos ao longo das 20 \u00e9pocas de treinamento e valida\u00e7\u00e3o. \n\nNo caso da arquitetura ResNet50 o que pode-se verificar nas figuras 27 e 28 \u00e9 \n\no contraste dos resultados nas etapas de treinamento e valida\u00e7\u00e3o. Se por um lado a \n\n52\n\n\n\n \n\nperda e exatid\u00e3o de treinamento da ResNet50 s\u00e3o os melhores entre as quatro \n\narquiteturas, a perda e exatid\u00e3o de valida\u00e7\u00e3o s\u00e3o os piores, demonstrando o \n\nsobreajuste desta rede. \n\nDe acordo com os gr\u00e1ficos 29 e 30, a rede InceptionV3 apresenta um \n\ndesempenho linear nas etapas de perda e exatid\u00e3o de treinamento, por\u00e9m estes \n\nresultados oscilam mais na etapa de perda e exatid\u00e3o de valida\u00e7\u00e3o. Essa performance \n\ndeve-se pelo n\u00famero de par\u00e2metros da rede, 23.851.784, tornando \u00e0 rede mais r\u00e1pida \n\nem tempo de treinamento, por\u00e9m que necessita de mais dados. Por exemplo na etapa \n\nde treinamento usou-se 3.500 imagens e na etapa de valida\u00e7\u00e3o 1.500, ou seja, um \n\nconjunto menor de dados para uma rede com menos par\u00e2metros. \n\nOs resultados da arquitetura InceptionResNetV2 s\u00e3o apresentados nas figuras \n\n31 e 32, por ter sua constru\u00e7\u00e3o semelhante a rede InceptionV3 os resultados obtidos \n\nforam semelhantes ao da arquitetura. Entretanto a rede InceptionV3 foi superior a  \n\nInceptionResNetV2 se comparados principalmente os gr\u00e1ficos de perda de valida\u00e7\u00e3o \n\ndas figuras 29 para InceptionV3 e  31 para InceptionResNetV2.  \n\nDas arquiteturas analisadas a InceptionV3 apresenta melhor performance, \n\nseguida pela InceptionResNetV2. Ambas possuem desempenho similar, sendo que \n\nesses resultados pr\u00f3ximos devem-se principalmente por compartilharem muitos \n\npontos estruturais em comum. Al\u00e9m dos gr\u00e1ficos que foram criados com base nas \n\ninforma\u00e7\u00f5es que cada arquitetura gera ao longo das 20 \u00e9pocas de treinamento, \n\nmatrizes de confus\u00e3o foram plotadas para uma melhor verifica\u00e7\u00e3o da performance das \n\narquiteturas, principalmente na classifica\u00e7\u00e3o individual de cada esp\u00e9cie. \n\n Ap\u00f3s o treinamento na etapa de valida\u00e7\u00e3o das 1.500 imagens o algoritmo \n\ndesenvolvido cria matrizes de confus\u00e3o para cada arquitetura, podendo-se averiguar \n\na performance de cada arquitetura na classifica\u00e7\u00e3o das diferentes classes de plantas \n\ndaninhas. Com base nas informa\u00e7\u00f5es das matrizes de confus\u00e3o foram criados \n\nquadros que sintetizam a exatid\u00e3o local e total de cada arquitetura estuda. \n\n A seguir as matrizes de confus\u00e3o para cada arquitetura s\u00e3o descritas. As matrizes de \n\nconfus\u00e3o foram criadas com base no banco de imagens de valida\u00e7\u00e3o que cont\u00e9m \n\n1.500 imagens divididas em 5 esp\u00e9cies de plantas daninhas, tendo assim 300 \n\nimagens para cada esp\u00e9cie. A matrizes s\u00e3o divididas em 2 r\u00f3tulos o r\u00f3tulo verdadeiro, \n\nou seja, quantas esp\u00e9cies realmente pertencem a cada classe e o r\u00f3tulo previsto, ou \n\n53\n\n\n\n \n\nseja, quantas das esp\u00e9cies classificadas est\u00e3o de acordo com o valor do r\u00f3tulo \n\nverdadeiro.  Os resultados podem ser verificados nas figuras 33, 34, 35 e 36. \n\nFigura 33 - Matriz de Confus\u00e3o  Arquitetura VGG16 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 34 - Matriz de Confus\u00e3o  Arquitetura ResNet50 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n54\n\n\n\n \n\nFigura 35 - Matriz de Confus\u00e3o  Arquitetura InceptionV3 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\nFigura 36 - Matriz de Confus\u00e3o  Arquitetura InceptionResNetV2 \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n55\n\n\n\n \n\nDe todas as arquiteturas a rede InceptionV3 teve a melhor performance, tendo \n\nseu melhor resultado para classificar a esp\u00e9cie capim p\u00e9 de galinha acertando 284 \n\nimagens de 300, e pior resultado na classifica\u00e7\u00e3o de caruru com 212 imagens de 300. \n\nEm segundo lugar est\u00e1 a rede InceptionResNetV2 com 82,86% de exatid\u00e3o \n\ntotal, levando em conta as cinco esp\u00e9cies e em terceiro lugar a rede VGG16 com \n\n80,6% de exatid\u00e3o total. \n\nO pior resultado foi da arquitetura ResNet50 acertando 100% na categoria \n\ncapim amargoso, por\u00e8m errando todas as outras especies, mais uma vez \n\ncomprovando que a rede n\u00e3o \u201caprendeu\u201d e sim decorou. \n\nNo quadro 4 s\u00e3o apresentados os valores de exatid\u00e3o local, referente a \n\nexatid\u00e3o na classifica\u00e7\u00e3o de cada esp\u00e9cie de planta daninha com base nos dados das \n\nmatrizes de confus\u00e3o de cada arquitetura, e a exatid\u00e3o total, que representa qual foi \n\na porcentagem total de imagens classificadas corretamente em cada arquitetura de \n\nacordo com o total das 1.500 imagens usadas para a valida\u00e7\u00e3o. \n\nQuadro 4 - Performance das Arquiteturas com base nas matrizes de confus\u00e3o \n\n VGG16 ResNet50 InceptionV3 InceptionResNet \n\nEsp\u00e9cie de Planta \nDaninha \n\nExatid\u00e3o \nLocal(%) \n\nExatid\u00e3o \nLocal(%) \n\nExatid\u00e3o \nLocal(%) \n\nExatid\u00e3o \nLocal(%) \n\nBuva 83,67 0,00 83,67 79,67 \n\nCapim-Amargoso 68,00 100,00 83,33 88,67 \n\nCapim-Azev\u00e9m 60,00 0,00 91,33 74,00 \n\nCapim-p\u00e9-de-Galinha 98,33 0,00 94,67 91,67 \n\nCaruru 93,00 0,00 70,66 80,33 \n\nExatid\u00e3o Total(%) 80.60 20,00 84,73 82,87 \n\n \nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\nCom 84,73% de exatid\u00e3o total e 1.271 imagens classificadas corretamente das \n\n1.500 usadas na valida\u00e7\u00e3o, a arquitetura InceptionV3 obteve os melhores resultados. \n\nAp\u00f3s a identifica\u00e7\u00e3o da melhor arquitetura, a rede InceptionV3 foi treinada pelo \n\nper\u00edodo de 40 \u00e9pocas. Na figura 37  matriz de confus\u00e3o resultante deste treinamento \n\n\u00e9 apresentada. \n\n \n\n56\n\n\n\n \n\nFigura 37 -  Matriz de Confus\u00e3o  Arquitetura InceptionV3 40 \u00c9pocas \n\nFigura 37 -  Matriz de Confus\u00e3o  Arquitetura InceptionV3 40 \u00c9pocas \n\n \n\nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\nAp\u00f3s o treinamento com 40 \u00e9pocas a exatid\u00e3o total paras as 5 esp\u00e9cies \n\nanalisadas aumentou de 84,73% para 88,6%. Foi identificado tamb\u00e9m uma melhora \n\nna classifica\u00e7\u00e3o da esp\u00e9cie caruru, que aumentou a assertividade de 212 imagem \n\npara 274 de um total de 300. No quadro 5 \u00e9 apresentada a performance da rede \n\nInceptionV3 com 40 \u00e9pocas de treinamento para a classifica\u00e7\u00e3o das 5 esp\u00e9cies. \n\nQuadro 5 - Performance da Arquitetura InceptionV3 com 40 \u00e9pocas de treinamento, \ncom base nas matrizes de confus\u00e3o \n\nEsp\u00e9cie Exatid\u00e3o Local(%) \n\nBuva 87,00 \n\nCapim-Amargoso 87,67 \n\nCapim-Azev\u00e9m 83,33 \n\nCapim-p\u00e9-de galinha 94,33 \n\nCaruru 90,67 \n\nExatid\u00e3o Total(%) 88,60 \n\n \nFonte: Elabora\u00e7\u00e3o do pr\u00f3prio autor \n\n \n\n57\n\n\n\n \n\n5 CONCLUS\u00d5ES \n\n \n\nDe acordo com os objetivos propostos e resultados obtidos conclui-se que \n\ndentre as quatro arquiteturas de redes neurais profundas estudadas, ou seja, as \n\narquiteturas VGG16, InceptionV3; InceptionResNetV2 e ResNet50, as tr\u00eas primeiras \n\nobtiveram resultados promissores na classifica\u00e7\u00e3o de imagens de cinco esp\u00e9cies de \n\nplantas daninhas com ru\u00eddos artificialmente adicionados, excetuando-se a arquitetura \n\nResNet50 que teve desempenho bastante pobre. \n\nConforme observado no cap\u00edtulo anterior, observa-se que a exatid\u00e3o total das \n\narquiteturas na classifica\u00e7\u00e3o das 1.500 imagens utilizadas na valida\u00e7\u00e3o foram \n\nrespectivamente VGG16 80,6%, ResNet50 20,0%, InceptionV3 84,73% e \n\nInceptionResNetV2 82,86%. A arquitetura InceptionV3, que apresentou o melhor \n\nresultado, foi treinada novamente com 40 \u00e9pocas, obtendo uma exatid\u00e3o total de \n\n88,6%. \n\nNesse contexto, as arquiteturas que apresentaram os melhores resultados \n\npodem ser empregadas na identifica\u00e7\u00e3o e classifica\u00e7\u00e3o de plantas daninhas \n\nauxiliando na erradica\u00e7\u00e3o das mesmas, seja com a aplica\u00e7\u00e3o localizada de defensivos \n\nou com a elimina\u00e7\u00e3o mec\u00e2nica reduzindo o risco de contamina\u00e7\u00e3o ao meio ambiente \n\ne a cultura. \n\nEntretanto, existem fatores limitantes para a utiliza\u00e7\u00e3o dessas abordagens, \n\nvisto que o tempo necess\u00e1rio para a identifica\u00e7\u00e3o e classifica\u00e7\u00e3o ainda \u00e9 \n\ndemasiadamente longo quando empregado em computadores comerciais, limitando \n\naplica\u00e7\u00e3o em campo. \n\nComo trabalho futuro, prop\u00f5e-se ampliar os ensaios para um maior n\u00famero de \n\n\u00e9pocas, arquiteturas de redes e pretende-se aprimorar a resposta destas arquiteturas \n\naplicando segmenta\u00e7\u00e3o no banco de imagens, visando otimizar o emprego destas \n\nredes em sistemas de tempo real, bem como estender o banco de imagens para \n\noutras esp\u00e9cies miscigenado com culturas de soja, milho dentre outras. \n\n \n\n \n\n58\n\n\n\n \n\nREFER\u00caNCIAS \n\n \n\nABADI, M. et al. TensorFlow: Large-scale machine learning on heterogeneous \nsystems. 2015. Dispon\u00edvel em:&lt;http://tensorflow.org/> . Acesso 12/12/2018. \n \n\nBADRINARAYANAN, V et al. SegNet: A Deep Convolutional Encoder-Decoder \nArchitecture for Image Segmentation. IEEE Transactions on Pattern Analysis &amp; \nMachine Intelligence, v. 39, p. 2481 - 2495, 2017. \n\n \n\nBRASIL. Minist\u00e9rio da Sa\u00fade. Secretaria de Vigil\u00e2ncia em Sa\u00fade. Departamento de \n\nVigil\u00e2ncia em Sa\u00fade Ambiental e Sa\u00fade do Trabalhador. Relat\u00f3rio Nacional de \n\nVigil\u00e2ncia em Sa\u00fade de Popula\u00e7\u00f5es Expostas a Agrot\u00f3xicos. \u2013 v. 1. t.  \u2013 Bras\u00edlia \n\n(DF), 2018. Dispon\u00edvel em: \n\n<http://bvsms.saude.gov.br/bvs/publicacoes/relatorio_nacional_vigilancia_populacoe\n\ns_expostas_agrotoxicos.pdf> Acesso em 15/12/2018. \n\n \n\nCHOLLET, F.  Keras. 2015. Disponivel em: \n<https://github.com/fchollet/keras>.Acesso em 10 jan.2019.  \n \nCHRISTOFFOLETI, P.J. (Coord.). Aspectos de resist\u00eancia de plantas daninhas a \nherbicidas. 3.ed. Campinas, 2008. \n \n\nDENG, J. et al. ImageNet: A Large-Scale Hierarchical Image Database.  IEEE \nConference on Computer Vision and Pattern Recognition, Miami, v. 1, p.  248 - \n255, 2009. \n \n\nDEEP LEARNING BOOK. Cap\u00edtulo 3 - O Que S\u00e3o As Redes Neurais Artificiais \nProfundas Ou Deep Learning? Dispon\u00edvel em: \n<http://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/> \n. Acesso 05 jan.2019. \n \nDEEP LEARNING BOOK. Cap\u00edtulo 4 \u2013 O Neur\u00f4nio, Biol\u00f3gico e Matem\u00e1tico. \n\n2018. Dispon\u00edvel em:&lt;http://deeplearningbook.com.br/o-neuronio-biologico-e-\n\nmatematico/> Acesso em : 10/01/2019. \n\nDOS SANTOS FERREIRA, A. et al.  Weed detection in soybean crops using \nConvNets. Computers and Electronics in Agriculture. v. 143, p. 314\u2013324, Mato \nGrosso, MT, dez. 2017. \n \nDYRMANN, M. Automatic Detection and Classification of Weed Seedlings under \nNatural Light. Phd Thesis, University of Southern Denmark. jun. 2017. \n \nEMBRAPA - EMPRESA BRASILEIRA DE PESQUISA AGROPECU\u00c1RIA. Plantas \nDaninhas.  Bras\u00edlia (DF). Dispon\u00edvel em:  \n\n59\n\nhttps://ieeexplore.ieee.org/author/37547792900\nhttps://ieeexplore.ieee.org/author/37547792900\nhttps://ieeexplore.ieee.org/author/37547792900\nhttp://bvsms.saude.gov.br/bvs/publicacoes/relatorio_nacional_vigilancia_populacoes_expostas_agrotoxicos.pdf\nhttp://bvsms.saude.gov.br/bvs/publicacoes/relatorio_nacional_vigilancia_populacoes_expostas_agrotoxicos.pdf\nhttps://github.com/fchollet/keras\nhttps://ieeexplore.ieee.org/xpl/conhome/5191365/proceeding\nhttps://ieeexplore.ieee.org/xpl/conhome/5191365/proceeding\nhttp://deeplearningbook.com.br/o-que-sao-redes-neurais-artificiais-profundas/\nhttp://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/\nhttp://deeplearningbook.com.br/o-neuronio-biologico-e-matematico/\n\n\n \n\n<https://www.embrapa.br/tema-plantas-daninhas/sobre-o-tema>. Acesso \n10/12/2018. \n \n\nESTADOS UNIDOS. Assembleia Geral das Na\u00e7\u00f5es Unidas. Conselho de Direitos \n\nHumanos. Report of the Special Rapporteur on the right to food. Nova York (NY), \n\n2017. Dispon\u00edvel em:<https://documents-dds-\n\nny.un.org/doc/UNDOC/GEN/G17/017/85/PDF/G1701785.pdf?OpenElement> \n\n15/12/2018. \n\n \nFAGUNDES, E. Uso de Redes Neurais Artificiais para aumentar a confiabilidade \ndos sistemas el\u00e9tricos. 2018. Dispon\u00edvel em:<https://efagundes.com/blog/uso-de-\nredes-neurais-artificiais-para-aumentar-a-confiabilidade-dos-sistemas-eletricos/>  \nAcesso 12/12/2018. \n \n\nGIRSHICK, R  et al. Rich Feature Hierarchies for Accurate Object Detection and \nSemantic Segmentation. IEEE Conference on Computer Vision and Pattern \nRecognition, Columbus, v. 1, p. 580 - 587, 2014. \n\n \n\nGIRSHICK, R. Fast R-CNN.  IEEE International Conference on Computer Vision, \nSantiago, v.1,  p. 580 - 1440 - 1448 , 2015. \n\n \n\nHE, K. et al. Deep Residual Learning for Image Recognition. IEEE Conference on \nComputer Vision and Pattern Recognition, Las Vegas,v. 1, p. 770-778, 2016. \n\n \n\nHE, K et al. Mask R-CNN. IEEE International Conference on Computer Vision, \n\nVeneza, 2017. \n\nIBAMA - INSTITUTO BRASILEIRO DE MEIO AMBIENTE E DOS RECURSOS \nNATURAIS RENOV\u00c1VEIS. Relat\u00f3rios de Comercializa\u00e7\u00e3o de Agrot\u00f3xicos. \nBras\u00edlia (DF), 2013. \n \nIBAMA - INSTITUTO BRASILEIRO DE MEIO AMBIENTE E DOS RECURSOS \nNATURAIS RENOV\u00c1VEIS. Relat\u00f3rios de Comercializa\u00e7\u00e3o de Agrot\u00f3xicos. \nBras\u00edlia (DF), 2017. \n \nKRIZHEVSKY, A; Sutskever, I;  Hinton, G. E. ImageNet Classification with Deep \nConvolutional Neural Networks. Advances in Neural Information Processing \nSystems, San Diego, v. 25, p. 1097-1105, 2012. \n \n \nLECUN, Y. et al. Gradient-based learning applied to document recognition. \nProceedings of the IEEE, v. 88, p. 2278\u20132324, nov. 1998. \n \nMILIOTO, A. et al. Real-time Semantic Segmentation of Crop and Weed for Precision \nAgriculture Robots Leveraging Background Knowledge in CNNs.  IEEE  \n\n60\n\nhttps://www.embrapa.br/tema-plantas-daninhas/sobre-o-tema\nhttps://documents-dds-ny.un.org/doc/UNDOC/GEN/G17/017/85/PDF/G1701785.pdf?OpenElement\nhttps://documents-dds-ny.un.org/doc/UNDOC/GEN/G17/017/85/PDF/G1701785.pdf?OpenElement\nhttps://efagundes.com/blog/uso-de-redes-neurais-artificiais-para-aumentar-a-confiabilidade-dos-sistemas-eletricos/\nhttps://efagundes.com/blog/uso-de-redes-neurais-artificiais-para-aumentar-a-confiabilidade-dos-sistemas-eletricos/\nhttps://dl.acm.org/author_page.cfm?id=81467645792&amp;coll=DL&amp;dl=ACM&amp;trk=0\nhttps://dl.acm.org/author_page.cfm?id=81467645792&amp;coll=DL&amp;dl=ACM&amp;trk=0\n\n\n \n\nInternational Conference on Robotics &amp; Automation, Austria, v. 1, p. 2229 - 2235, \n2018. \n \nRAHMAN, N. What is the benefit of using average pooling rather than max \npooling?. 2017. Dispon\u00edvel em:<https://www.quora.com/What-is-the-benefit-of-\nusing-average-pooling-rather-than-max-pooling>Acesso 12/12/2018. \n \n\nREDMON, J et al. You Only Look Once: Unified, Real-Time Object Detection. IEEE \nConference on Computer Vision and Pattern Recognition, Las Vegas,v. 1, p. 779 \n- 788, 2016. \n\n \nROSENBLATT, F. The perceptron, perceiving and recognizing automaton Project \nPara. Cornell Aeronautical Laboratory, v. 85, Buffalo, NY, 1957. \n \nRUMELHART, D. E.; Hinton, G. E.;  Williams, R. J. Learning representations by \nback-propagating errors. Nature, v. 323, p. 533\u2013536, out.1986. \n \n\nSERMANET, P  et al. Overfeat: Integrated recognition, localization and detection \nusing convolutional networks. International Conference on Learning \nRepresentations, Banff, 2014. \n\n \n\nSILVA, S. L. A .; Junior, S. B. V. Aplica\u00e7\u00f5es E Benef\u00edcios Obtidos Atrav\u00e9s Das Redes \n\nNeurais Artificiais (RNA). Revista Facima Digital Gest\u00e3o, Macei\u00f3, v. 2, p. 29 - 43, \n\n2017. \n\nSIMONYAN, K;  Zisserman, A. Very deep convolutional networks for Large-Scale \nimage Recognition. International Conference on Learning Representations, San \nDiego, 2015. \n \n\nSINDIVEG - SINDICATO NACIONAL DA IND\u00daSTRIA DE PRODUTOS PARA A \n\nDEFESA VEGETAL. O Que Voc\u00ea Precisa Saber Sobre Defensivos Agr\u00edcolas. \n\nS\u00e3o Paulo, 2017. Dispon\u00edvel em  &lt;http://sindiveg.org.br/wp-\n\ncontent/uploads/2018/08/oquevoceprecisasabersobredefensivosagricolas.pdf>  \n\nAcesso em : 12/12/2018. \n\nSYNGENTA. Tecnologia \u00e9 aliada no controle de daninhas, como buva e \namargoso. 2017. Dispon\u00edvel em: \n<https://www.portalsyngenta.com.br/noticiasdocampo/tecnologia-e-aliada-no-\ncontrole-de-daninhas-como-buva-e-amargoso> . Acesso em : 12/12/2018. \n\n \nSZEGEDY, C. et al. Going Deeper with Convolutions. IEEE Conference on \nComputer Vision and Pattern Recognition, Boston, v. 1, p. 1 - 9,  2015. \n \n\nSZEGEDY, C. et al. Rethinking the Inception Architecture for Computer Vision. IEEE \nConference on Computer Vision and Pattern Recognition, Las Vegas,v. 1, \np.2818 - 2826, 2016. \n\n \n\n61\n\nhttps://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling\nhttps://www.quora.com/What-is-the-benefit-of-using-average-pooling-rather-than-max-pooling\nhttp://sindiveg.org.br/wp-content/uploads/2018/08/oquevoceprecisasabersobredefensivosagricolas.pdf\nhttp://sindiveg.org.br/wp-content/uploads/2018/08/oquevoceprecisasabersobredefensivosagricolas.pdf\nhttps://www.portalsyngenta.com.br/noticiasdocampo/tecnologia-e-aliada-no-controle-de-daninhas-como-buva-e-amargoso\nhttps://www.portalsyngenta.com.br/noticiasdocampo/tecnologia-e-aliada-no-controle-de-daninhas-como-buva-e-amargoso\n\n\n \n\nSZEGEDY, C; Ioffe, S;  Vanhoucke, V. Inceptionv4, inception-resnet and the impact \nof residual connections on learning.International Conference on Learning \nRepresentations, San Juan, 2016. \n CoRR, abs/1602.07261. \n \n\nUIJLINGS, J. R. R.  et al. Selective Search for Object Recognition. International \nJournal of Computer Vision, v. 104, p. 154 - 171, 2013. \n\n \nVASCONCELOS, Yuri. Agrot\u00f3xicos na Berlinda. Pesquisa FAPESP. S\u00e3o Paulo, \n\nEdi\u00e7\u00e3o 271, set. 2018. Dispon\u00edvel em: \n\n<http://revistapesquisa.fapesp.br/2018/09/18/agrotoxicos-na-berlinda/>. Acesso em: \n\n15/12/2018. \n\n \n\nW, S. MCCULLOCH.; Pitts, W. A logical calculus of the ideas immanent in nervous \nactivity. The bulletin of mathematical biophysics, v. 5, p.115\u2013133, dez. 1943. \n \n\nW, BERNARD. et al. Adaptive \u201dAdaline\u201d neuron using chemical \u201dmemistors\u201d. \nStanford Electron. Labs, Technical Report 1553-2, Stanford, CA, out. 1960. \n \n \n \n\n62\n\nhttps://revistapesquisa.fapesp.br/revista/ver-edicao-editorias/?e_id=387\nhttp://revistapesquisa.fapesp.br/2018/09/18/agrotoxicos-na-berlinda/"}]}}}