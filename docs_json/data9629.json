{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.12974"}, {"@name": "filename", "#text": "18787_arquivo981_1.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\n\nSUPPORT VECTOR MACHINES AND\nPARTICLE SWARM OPTIMIZATION\n\nAPPLIED TO RELIABILITY PREDICTION\n\nISIS DIDIER LINS\n\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\n\nRecife, 2009\n\n\n\nUNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\n\nSUPPORT VECTOR MACHINES AND\nPARTICLE SWARM OPTIMIZATION\n\nAPPLIED TO RELIABILITY PREDICTION\n\nA DISSERTATION\n\nBY\n\nISIS DIDIER LINS\n\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\n\nRecife, November/2009\n\n\n\nUNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\n\nSUPPORT VECTOR MACHINES AND\nPARTICLE SWARM OPTIMIZATION\n\nAPPLIED TO RELIABILITY PREDICTION\n\nA DISSERTATION PRESENTED TO THE UNIVERSIDADE\nFEDERAL DE PERNAMBUCO IN PARTIAL FULFILLMENT\nOF THE REQUIREMENTS FOR THE DEGREE OF MESTRE\n\nBY\n\nISIS DIDIER LINS\n\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\n\nRecife, November/2009\n\n\n\nL759s Lins, Isis Didier.\nSupport vector machines and particle swarm optimization applied\n\nto reliability prediction / Isis Didier Lins. \u2013 Recife: O Autor, 2009.\nxv, 77 folhas, il : figs., tabs.\n\nDisserta\u00e7\u00e3o (Mestrado) \u2013 Universidade Federal de Pernambuco.\nCTG. Programa de P\u00f3s-Gradua\u00e7\u00e3o em Engenharia de Produ\u00e7\u00e3o,\n2009.\n\nInclui refer\u00eancias.\n\n1. Engenharia de Produ\u00e7\u00e3o. 2. Support Vector Machines.\n3. Confiabilidade. 4. Otimiza\u00e7\u00e3o. I. T\u00edtulo.\n\nUFPE\n\n658.5 CDD (22.ed.) BCTG/2009-227\n\n\n\n\n\nACKNOWLEDGEMENTS\n\nI would like to give my deep and sincere THANK YOU to...\n\n\u2022 My parents Bernardete and S\u00f3stenes, for the education they provided me with and for the\ncontinuous support in all aspects of my life. My brother Lauro, who lives miles away\nfrom us but always gives his constructive opinions and encourages me to learn different\ncomputer tools.\n\n\u2022 Vicente, who is very special for me, for the kind support and comprehension.\n\n\u2022 My big family, which makes me feel happy: vov\u00f3 Lourdes, vov\u00f4 Luiz, vov\u00f3 Myriam,\nvov\u00f4 Lauro, Nadja, Niedja, J\u00fanior, tia Adelaide, tia L\u00f3, tia Dulce, tia Eneida, Vanessa,\nLula, Pedrinho, Arthur, Fernandinha, Mari, Adriana, Joana and Jo\u00e3o.\n\n\u2022 My friends Sofia, Juju, Ayana, Felipe and Yuri, for the enjoyable moments we passed\ntogether.\n\n\u2022 Professor Enrique L\u00f3pez, for the oportunity of being part of CEERMA, for the attention,\ndedication and motivational words in the most critical phases of the research.\n\n\u2022 Professor Leandro Chaves and Professor Paulo Frutuoso, for the valuable comments,\nwhich enhanced the quality of the work.\n\n\u2022 M\u00e1rcio, for the worthy discussions about the dissertation topics and also for the detailed\nreviews of the document. Paulo, for helping me with statistical concepts. Thiago, Evand-\nson, Rhogi, Ricardo and Romero, for the comments in the early stages of the work.\n\n\u2022 Ana and Juliane, who were always available to help me with the bureaucratic issues.\n\n\u2022 CNPq, for the financial support.\n\niii\n\n\n\nABSTRACT\n\nReliability is a critical metric for organizations since it directly influences their performance\nin face of the market competition, as well as is essential in maintaining their production systems\navailable. The prediction of such quantitaive metric is then of great interest, as it may anticipate\nthe knowledge about system failures and let organizations avoid and/or overcome such unde-\nsirable situations. Systems\u2019 reliability depends on the inherent aging factors as well as on the\noperational conditions the system is subjected to. This may render the reliability modelling very\ncomplex and then traditional stochastic processes fail to accurately predict its behavior in time.\nIn this context, learning methods such as Support Vector Machines (SVMs) emerge as alterna-\ntive to tackle these shortcomings. One of the main advantages of using SVMs is the fact that\nthey do not require previous knowledge about the function or process that maps input variables\ninto output. However, their performances are affected by a set of parameters that appear in the\nrelated learning problems. This gives rise to the SVM model selection problem, which consists\nin choosing the most suitable values for these parameters. In this work, this problem is solved\nby means of Particle Swarm Optimization, a probabilistic approach based on the behavior of\nbiological organisms that move in groups. Moreover, a PSO+SVM methodology is proposed\nto handle reliability prediction problems, which is validated by the resolution of examples from\nliterature based on time series data. The obtained results, compared to the ones provided by\nother prediction tools such as Neural Networks (NNs), indicate that the proposed methodology\nis able to provide competitive or even more accurate reliability predictions. Also, the proposed\nPSO+SVM is applied to an example application involving data collected from oil production\nwells.\n\nKeywords: Support Vector Machines, Particle Swarm Optimization, Reliability Prediction.\n\niv\n\n\n\nRESUMO\n\nConfiabilidade \u00e9 uma m\u00e9trica cr\u00edtica para as organiza\u00e7\u00f5es, uma vez que ela influencia di-\nretamente seus desempenhos face \u00e0 concorr\u00eancia e \u00e9 essencial para a manuten\u00e7\u00e3o da disponi-\nbilidade de seus sistemas produtivos. A previs\u00e3o dessa m\u00e9trica quantitativa \u00e9 ent\u00e3o de grande\ninterresse, pois ela pode antecipar o conhecimento de falhas do sistema e permitir que as or-\nganiza\u00e7\u00f5es possam evitar ou superar essas situa\u00e7\u00f5es indesejadas. A confiabilidade de sistemas\ndepende tanto dos efeitos inerentes da idade assim como das condi\u00e7\u00f5es operacionais a que\no sistema \u00e9 submetido. Isso pode tornar a modelagem da confiabilidade muito complexa de\nforma que processos estoc\u00e1sticos tradicionais falhem em prever de forma acurada o seu com-\nportamento ao longo do tempo. Nesse contexto, m\u00e9todos de aprendizado como Support Vector\nMachines surgem como alternativa para superar essa quest\u00e3o. Uma das principais vantagens de\nse utilizar SVMs \u00e9 o fato de n\u00e3o ser necess\u00e1rio supor ou conhecer previamente a fun\u00e7\u00e3o ou o\nprocesso que mapeia as vari\u00e1veis de entrada (input) em sa\u00edda (output). No entanto, seu desem-\npenho est\u00e1 associado a um conjunto de par\u00e2metros que aparecem no problema de aprendizado.\nIsso d\u00e1 origem ao problema de sele\u00e7\u00e3o de modelo para SVM, que consiste basicamente em\nescolher os valores apropriados para esses par\u00e2metros. Nesse trabalho, tal problema \u00e9 resolvido\npor meio de Otimiza\u00e7\u00e3o via Nuvens de Part\u00edculas (Particle Swarm Optimization - PSO), uma\nabordagem probabil\u00edstica que \u00e9 inspirada no comportamento de organismos biol\u00f3gicos que se\nmovem em grupos. Al\u00e9m disso, \u00e9 proposta uma metodologia PSO+SVM para resolver prob-\nlemas de previs\u00e3o de confiabilidade, que \u00e9 validada por meio da resolu\u00e7\u00e3o de exemplos da\nliteratura baseados em dados de s\u00e9ries temporais. As solu\u00e7\u00f5es encontradas, comparadas \u00e0s\nprovenientes de outras ferramentas de previs\u00e3o como Redes Neurais (Neural Networks - NNs),\nindicam que a metodologia proposta \u00e9 capaz de fornecer previs\u00f5es de confiabilidade competiti-\nvas ou at\u00e9 mesmo mais acuradas. Al\u00e9m disso, a metodologia proposta \u00e9 utilizada para resolver\num exemplo de aplica\u00e7\u00e3o envolvendo dados de po\u00e7os de produ\u00e7\u00e3o de petr\u00f3leo.\n\nPalavras-chave: Support Vector Machines, Otimiza\u00e7\u00e3o via Nuvens de Part\u00edculas, Previs\u00e3o de\nConfiabilidade.\n\nv\n\n\n\nCONTENTS\n\n1 INTRODUCTION 1\n\n1.1 Opening Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Previous works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.3 Justification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n1.4.1 Main Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.4.2 Specific Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n1.5 Dissertation Layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n2 THEORETICAL BACKGROUND 8\n\n2.1 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1.1 Linear Maximal Margin Classifier for Linearly Separable Data . . . . . 9\n2.1.2 Linear Soft Margin Classifier for Non-Linearly Separable Data . . . . . 14\n2.1.3 Non-Linear Classifier of Maximal Margin . . . . . . . . . . . . . . . . 16\n2.1.4 Multi-classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.1.5 Support Vector Regression . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n2.1.5.1 Linear Regression Function . . . . . . . . . . . . . . . . . . 21\n2.1.5.2 Non-Linear Regression Function . . . . . . . . . . . . . . . 24\n2.1.5.3 Time Series Prediction . . . . . . . . . . . . . . . . . . . . . 25\n\n2.1.6 Optimization Techniques and Available Support Vector Machines Libra-\nries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n2.2 Model Selection Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.3 Particle Swarm Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\n3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION 36\n\n3.1 Steps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.1.1 Read of Data and Definition of Variables\u2019 Bounds . . . . . . . . . . . . 36\n3.1.2 Particle Swarm Initialization . . . . . . . . . . . . . . . . . . . . . . . 37\n3.1.3 Definition of Particles\u2019 Neighborhoods . . . . . . . . . . . . . . . . . 38\n3.1.4 Fitness Evaluation: Coupling of Particle Swarm Optimization and Sup-\n\nport Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.1.5 Update of Particles\u2019 Velocities and Positions . . . . . . . . . . . . . . 39\n3.1.6 Update of Global Best and Particles\u2019 Best Neighbors . . . . . . . . . . 40\n3.1.7 Stop Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n3.2 Proposed Methodology Pseudocode and Flow Chart . . . . . . . . . . . . . . . 40\n\n4 RELIABILITY PREDICTION BY PSO AND SVM 43\n\n4.1 Example 1: Failure Times of a Submarine Diesel Engine . . . . . . . . . . . . 44\n4.2 Example 2: Turbochargers in Diesel Engines . . . . . . . . . . . . . . . . . . 49\n\n4.2.1 Example 2.1: Reliability Forecast . . . . . . . . . . . . . . . . . . . . 50\n4.2.2 Example 2.2: Failure Times Forecast . . . . . . . . . . . . . . . . . . 53\n\n4.3 Example 3: Miles to Failure of a Car Engine . . . . . . . . . . . . . . . . . . . 54\n4.4 Example 4: Time Between Failures of Oil Production Wells . . . . . . . . . . 61\n\nvi\n\n\n\n4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.5.1 Performance Comparison Between lbest and gbest Models . . . . . . . 68\n\n5 CONCLUDING REMARKS 70\n\n5.1 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n5.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n5.3 Ongoing Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\nREFERENCES 73\n\nvii\n\n\n\nLIST OF FIGURES\n\n2.1 Relation between model capacity and error . . . . . . . . . . . . . . . . . . . . 9\n2.2 Underfitting (up) and overfitting (down) the data in the case of binary classifi-\n\ncation of linearly separable data. Adapted from Kecman (2005), p. 8 . . . . . . . . . 10\n2.3 Binary classification. Adapted from Kecman (2001), p. 154 . . . . . . . . . . . . . . 11\n2.4 Binary classification for non-linearly separable data. Adapted from Kecman (2005),\n\np. 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.5 Non-linear binary classification . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.6 Vapnik\u2019s ?-insensitive loss function . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.7 Non-linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.8 Influence of parameters in SVR. Adapted from Ito &amp; Nakano (2003), p. 2078 . . . . . 28\n2.9 Different swarm communication networks. Adapted from Bratton &amp; Kennedy (2007) . 33\n\n3.1 Flow chart of the proposed PSO+SVM methodology . . . . . . . . . . . . . . 42\n\n4.1 Swarm evolution during PSO, Example 1 . . . . . . . . . . . . . . . . . . . . 46\n4.2 Validation NRMSE convergence, Example 1 . . . . . . . . . . . . . . . . . . . 46\n4.3 SVR results, Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.4 Swarm evolution during PSO, Example 2.1 . . . . . . . . . . . . . . . . . . . 52\n4.5 Validation NRMSE convergence, Example 2.1 . . . . . . . . . . . . . . . . . . 52\n4.6 SVR results, Example 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.7 Swarm evolution during PSO, Example 2.2 . . . . . . . . . . . . . . . . . . . 55\n4.8 Validation NRMSE convergence, Example 2.2 . . . . . . . . . . . . . . . . . . 55\n4.9 SVR results, Example 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.10 Swarm evolution during PSO, Example 3 . . . . . . . . . . . . . . . . . . . . 59\n4.11 Validation NRMSE convergence, Example 3 . . . . . . . . . . . . . . . . . . . 59\n4.12 SVR results, Example 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.13 Swarm evolution during PSO, Example 4 . . . . . . . . . . . . . . . . . . . . 66\n4.14 Validation NRMSE convergence, Example 4 . . . . . . . . . . . . . . . . . . . 66\n4.15 SVR training results, Example 4 . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.16 SVR validation and test results, Example 4 . . . . . . . . . . . . . . . . . . . . 67\n\nviii\n\n\n\nLIST OF TABLES\n\n1.1 Number of SVM time series prediction papers by application. Adapted from\nSapankevych &amp; Sankar (2009), p. 26 . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n2.1 Common kernel functions. Adapted from Kecman (2001), p. 171 . . . . . . . . . . . 17\n2.2 Construction of data pairs for time series prediction . . . . . . . . . . . . . . . 26\n\n3.1 Examples of particles\u2019 neighborhoods . . . . . . . . . . . . . . . . . . . . . . 38\n\n4.1 Engine age (\u00d7 1000 hours) at time of unscheduled maintenance actions. Adapted\nfrom Ascher &amp; Feingold (1984), p. 75 . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n4.2 PSO required parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4.3 PSO variables\u2019 intervals and initial maximum velocities, Example 1 . . . . . . 45\n4.4 Descriptive statistics of parameters and error functions, stop criteria frequency\n\nand performance for 30 PSO+SVM runs, lbest, Example 1 . . . . . . . . . . . 45\n4.5 Real failure time (engine age) and predictions by SVR, Example 1 . . . . . . . 47\n4.6 Support vectors\u2019 details, Example 1 . . . . . . . . . . . . . . . . . . . . . . . 47\n4.7 Test NRMSE from different forecast models, Example 1. Adapted from Hong &amp;\n\nPai (2006), p. 160 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.8 Turbochargers failure times (\u00d7 1000 hours) and reliability data. Adapted from Xu\n\net al. (2003), p. 259 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n4.9 Descriptive statistics of parameters and error functions, stop criteria frequency\n\nand performance for 30 PSO+SVM runs, lbest, Example 2.1 . . . . . . . . . . 51\n4.10 Real failure time and predictions by SVR, Example 2.1 . . . . . . . . . . . . . 51\n4.11 Support vectors\u2019 details, Example 2.1 . . . . . . . . . . . . . . . . . . . . . . 51\n4.12 Test NRMSE from different forecast models, Example 2.1. Updated from Xu et al.\n\n(2003), p. 260, and Chen (2007), p.430 . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.13 Descriptive statistics of parameters and error functions, stop criteria frequency\n\nand performance for 30 PSO+SVM runs, lbest, Example 2.2 . . . . . . . . . . 54\n4.14 Real failure time and predictions by SVR, Example 2.2 . . . . . . . . . . . . . 56\n4.15 Support vectors\u2019 details, Example 2.2 . . . . . . . . . . . . . . . . . . . . . . 56\n4.16 Miles to failure (\u00d7 1000 hours) of a car engine. Adapted from Xu et al. (2003), p.\n\n262-263 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.17 Descriptive statistics of parameters and error functions, stop criteria frequency\n\nand performance for 30 PSO+SVM runs, lbest, Example 3 . . . . . . . . . . . 58\n4.18 Real MTF and predictions by SVR (\u00d7 1000 hours), Example 3 . . . . . . . . . 58\n4.19 Support vectors\u2019 details, Example 3 . . . . . . . . . . . . . . . . . . . . . . . 60\n4.20 Test NRMSE from different forecast models, Example 3. Updated from Xu et al.\n\n(2003), p. 264, Zio et al. (2008) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.21 Selected variables that influence the TBF . . . . . . . . . . . . . . . . . . . . . 63\n4.22 Transformation of categorical variables x6 and x7 into indicator variables . . . . 63\n4.23 Descriptive statistics of parameters and error functions, stop criteria frequency\n\nand performance for 30 PSO+SVM runs, lbest, Example 4 . . . . . . . . . . . 65\n4.24 Validation and test errors from the \u201cmachine\u201d with the smallest test NRMSE,\n\nExample 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.25 Mean test NRMSE values and Wilcoxon-Mann-Whitney test results lbest \u00d7 gbest 68\n\nix\n\n\n\n4.26 Mean time per run (minutes) and Wilcoxon-Mann-Whitney test results lbest \u00d7\ngbest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n4.27 Mean number of predictions per run and Wilcoxon-Mann-Whitney test results\nlbest \u00d7 gbest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\nx\n\n\n\nLIST OF ACRONYMS\n\nARIMA Autoregressive Integrated Moving Average.\n\nBN Bayesian Network.\n\nCBM Condition Based Maintenance.\n\nDAG Directed Acyclic Graph.\n\nERM Empirical Risk Minimization.\n\nGA Genetic Algorithm.\n\nGRNN General Regression Neural Network.\n\nGRP Generalized Renewal Process.\n\nHPP Homogeneous Poisson Process.\n\nIIR-LRNN Infinite Impulse Response Locally Recurrent Neural Network.\n\nKKT Karush-Kuhn-Tucker.\n\nMAPE Mean Absolute Percentage Error.\n\nMLP-NN Multilayer Perceptron Neural Network.\n\nMP Mechanical Pumping.\n\nMSE Mean Square Error.\n\nMTBF Mean Time Between Failures.\n\nMTF Miles To Failure.\n\nNFN Neural Fuzzy Network.\n\nNHPP Non-Homogeneous Poisson Process.\n\nNN Neural Network.\n\nNRMSE Normalized Root Mean Square Error.\n\nPCP Progressive Cavities Pumping.\n\nPSO Particle Swarm Optimization.\n\nRBF Radial Basis Function.\n\nRBF-NN Radial Basis Function Neural Network.\n\nxi\n\n\n\nRP Renewal Process.\n\nSA Simulated Annealing.\n\nSLT Statistical Learning Theory.\n\nSMDP Semi-Markov Decision Process.\n\nSMO Sequential Minimal Optimization.\n\nSRM Structural Risk Minimization.\n\nSVM Support Vector Machine.\n\nSVR Support Vector Regression.\n\nTBF Time Between Failures.\n\nTTR Time To Repair.\n\nxii\n\n\n\nLIST OF SYMBOLS\n\n???, ????, ???, ???\n?\n\n`-dimensional vectors of Lagrange multipliers.\n\n? Parameter involving the RBF kernel parameter ?.\n\n? Tolerance for consecutive global best fitness values.\n\n? \u201cRadius\u201d of the \u201ctube\u201d defined in the Vapnik\u2019s ?-insensitive loss function.\n\n? Reliability growth factor.\n\n? Number of points in the validation set.\n\n? Number of points in the test set.\n\n???, ???\n?\n\n`-dimensional vectors of slack variabies.\n\n? Angle between xSV,?1 and w.\n\n? RBF kernel parameter\n\n? Mapping Rn ? F .\n\n? Sum of PSO parameters c1 and c2.\n\n? Constriction factor\n\n? Angle between xSV,+1 and w.\n\nb Linear coefficient of H.\n\nbIter PSO iteration in which the best particle was found.\n\nC Trade-off between training error and machine capacity.\n\nc1, c2 PSO parameters.\n\nD Training data set.\n\nd Degree of a polynomial\n\nd(x) Decision function.\n\nF Feature space.\n\nf (x) Mapping or function of x.\n\nfi Validation fitness value associated with the ith particle.\n\nftest Test fitness value associated with the best particle.\n\ng(x) Non-zero function that satisfies\nR\n\ng2(x)dx &lt;?.\n\nH Separating hyperplane.\n\nxiii\n\n\n\nh Test point index.\n\nH?, H+ Lower and upper hyperplanes that define the margin.\n\ni Training point index; particle index.\n\nj Auxiliar index; dimension index of a vector.\n\nK Kernel function.\n\nk Category index of a multi-classification problem; number of steps ahead minus one in a\ntime series prediction; number of subsets in cross-validation.\n\nL Lagrangian function.\n\nLd Dual Lagrangian function.\n\nL Loss function.\n\n` Number of training points (xi, yi).\n\nM Margin.\n\nm Number of categories of a multi-classification problem.\n\nn Dimension of x; number of variables.\n\nnF SV Number of free support vectors.\n\nnIter Maximum number of PSO iterations.\n\nnLF SV, nU F SV Number of free support vectors lying on H? and on H+ (regression case).\n\nnNeigh Number of particles\u2019 neihgbors.\n\nnPart Number of particles.\n\nnSV Number of support vectors.\n\np Number of lagged variables in a time series, dimension of xt .\n\np The best individual position a particle has found so far.\n\npg Best position in the neighborhood of a particle.\n\nR Set of real numbers.\n\nr Number of categories associated with a categorical variable.\n\ns Support vector index.\n\nT Total operational time of an equipment or matrix transpose operation.\n\nt Time index.\n\nu1, u2 Random numbers generated by a uniform distribution defined in the interval [0,1].\n\nxiv\n\n\n\nvmax Maximum velocity.\n\nv Velocity vector.\n\nvmax Maximum velocity a particle can have associated with the jth dimension.\n\nw Inertia weight.\n\nw Normal vector in relation to H.\n\nxind Indicator variable.\n\nxmin, j, xmax, j Minimum and maximum training values of the jth dimension of xi.\n\nx Multidimensional input vector; current particle position\n\nxmin, xmax n-dimensional vectors of variables\u2019 bounds.\n\nxSV,?1, xSV,+1 Support vector from the negative and positive classes.\n\ny Output value.\n\ny? Predicted output value.\n\nymin, ymax Minimum and maximum training values of the output y.\n\nxv\n\n\n\n1 INTRODUCTION\n\n1.1 Opening Remarks\n\nReliability can be understood as the probability of a system to properly perform the tasks\nit was designed for, under certain conditions, during a predefined time length (RAUSAND &amp;\nHOYLAND, 2004). These systems can be either a specific component or an entire system. With\na slightly different interpretation, the absence of reliability means frequent system breakdowns\nand thus loss of productivity and increased costs, which may be associated with maintenance\nactions, legal penalties and also with the (bad) image of the organization in face of their cos-\ntumers. It is also a critical issue in systems that entails environmental and human risks, such\nas oil refineries and nuclear power plants, given that their failures may incur in catastrophic\nevents. Therefore, reliability is a key factor to production systems, since it is directly related to\nthe competitive performance of organizations in the market share they are inserted.\n\nThe systems\u2019 reliability varies during their lifetime since it is influenced by the environ-\nmental and load conditions under which they operate. In this way, it is of great interest to\nidentify this quantitative indicator of systems\u2019 performance in order to control it by means of\nappropriate maintenance actions, so as to guarantee the desired level of production and safety.\nAccording to Zio et al. (2008), reliability prediction modeling of an item may be conducted\nduring various phases of its life-cycle, including the concept validation and definition, the de-\nsign, operation and maintenance phases. At any stage, the reliability predictions obtained serve\nthe purpose of anticipating the evolution of the reliability of the component so as to allow for\ntaking the proper actions for its maintaining and, possibly, improvement. For systems design\nconsidering reliability and cost metrics, see for example Lins &amp; Droguett (2008) and Lins &amp;\nDroguett (2009).\n\nThe evaluation of the reliability behavior in time has been accomplished by means of\nstochastic methods, which usually involves simplifying assumptions so as to allow for the an-\nalytical treatment. The usual stochastic processes to model the reliability evolution are the\nRenewal Process (RP) and Non-Homogeneous Poisson Process (NHPP). If RP is chosen, the\ntimes between failures are independent and identically distributed with an arbitrary probability\ndistribution. Besides, one assumes that the component, after a failure, is subjected to a perfect\nrepair and returns into operation with a condition it presented when new (\u201cas good as new\u201d). A\nspecial case of the RP is the Homogeneous Poisson Process (HPP), in which the times between\nfailures are modeled by identical and independent Exponential distributions and has the under-\nlying supposition that the probability of occurrence of a failure in any time interval depends\nonly on the length of that interval. This assumption may be true for some electronic compo-\nnents or in a short period of time (ROSS, 2000). On the other hand, using NHPP, the times\nbetween failures are neither independent nor identically distributed. In addition, it is supposed\n\n1\n\n\n\nChapter 1 INTRODUCTION\n\nthat the maintenance crew makes a minimal repair in the failed component, that is, it is returned\nto an operational state with the same condition it had just before the failure occurrence (\u201cas bad\nas old\u201d).\n\nHowever, the hypothesis of minimal or perfect repairs required to utilize either NHPP or\nRP, respectively, are often not realistic. In practical situations, corrective maintenance actions\nare likely to be imperfect repairs, i.e., they are intermediate actions between minimal and perfect\nrepairs and the equipment returns into operation with a condition better than old and worse than\nnew. In this context, Generalized Renewal Process (GRP) can be used to model failure-repair\nprocesses of components subject to imperfect repairs. In GRP, a parameter q (rejuvenation\nparameter) is introduced in the model and the value it assumes is related to the maintenance\naction effectiveness. However, this value is often considered as a constant for all interventions\nwithout taking into consideration the current state of the system. For further details in RP,\nHPP, NHPP and GRP, the interested reader may consult Rigdon &amp; Basu (2000) and Rausand\n&amp; Hoyland (2004).\n\nIn reality, the reliability of a system is affected by a set of time-dependent, external (opera-\ntional and environmental) variables which are dependent among them. As an outcome, reliabil-\nity prediction may demand sophisticated probabilistic models so as to realistically capture the\ncomplexities of the systems and components reliability behavior, which may result in burden-\nsome mathematical formulations that in the end, may not provide the required accuracy of the\nreliability estimates (MOURA &amp; DROGUETT, 2009).\n\nIn this context, learning methodologies based on data emerge and Support Vector Machine\n(SVM) is the one selected to be studied and applied to reliability problems in this dissertation.\nSVM has been developed since the years 1960\u2019s and was first introduced by Vapnik and Cher-\nvonenkis (see Vapnik (2000)). Loosely speaking, SVM is a learning method whose theory is\nbased on statistical concepts. It incorporates the idea of learning about the phenomenon under\nanalysis from real observations about it. The main idea is to train a \u201cmachine\u201d with real pairs\nof inputs and outputs so as to allow for the prediction of future outputs based on observed in-\nputs. The training algorithm is a quadratic optimization problem, where the objective function\nessentially entails a generalization error, which comprises the training error as well as the error\nrelated to the machine ability in handling unseen data. The learning problem can be either of\nclassification, in which the outputs are discrete values representing categories, or of regression,\nwhen the outputs can assume any real value.\n\nCompetitive models to SVM are the artificial Neural Network (NN) (HAYKIN, 1999) and\nBayesian Network (BN) (KORB &amp; NICHOLSON, 2003). It is interesting to notice that, in\naccordance with Kecman (2005), SVM has been developed in the reverse order to the devel-\nopment of NN. SVM evolved from the theory to the implementation and experiments, while\nNN followed a more \u201cheuristic\u201d path, from applications to theory. The strong theoretical back-\nground of SVM did not make it widely appreciated at first. It was believed that, despite its\ntheoretical foundations, SVM was neither suitable nor relevant for practical purposes. How-\n\n2\n\n\n\nChapter 1 INTRODUCTION\n\never, afterwards, the use of SVM in learning benchmark problems involving for example digit\nrecognition provided excellent results and then such tool was finally taken seriously.\n\nHaykin (1999) asserts that an NN is designed to model the way in which the brain per-\nforms a particular task or function of interest and is usually simulated in software on a digital\ncomputer. To achieve good performance, NN employs a massive interconnection of simple\ncomputing cells referred to as \u201cneurons\u201d or \u201cprocessing units\u201d, which are basically formed by\n(i) a set of connecting links, each one of them characterized by a weight; (ii) an adder for sum-\nming the input signals, weighted by the respective connecting links; (iii) an activation function\nfor limiting the amplitude of its output.\n\nNN involves the Empirical Risk Minimization (ERM), which measures only the errors from\nthe training step and is appropriate when there is a large quantity of training examples (VAP-\nNIK, 2000). On the other hand, the training phase of SVM entails a convex quadratic optimiza-\ntion problem, whose objective function embodies the principle of Structural Risk Minimization\n(SRM). The general idea of the ERM is to minimize the error during the training phase, while\nthe SRM aims at minimizing the upper bound on the generalization error. Additionally, the char-\nacteristics of the SVM training optimization problem enable the Karush-Kuhn-Tucker (KKT)\nconditions to be necessary and sufficient to provide a global optimum, differently from NN that\nmay be trapped on local minima (SCH\u00d6LKOPF &amp; SMOLA, 2002).\n\nAccording to Korb &amp; Nicholson (2003), BNs are graphical models for reasoning under\nuncertainty, represented by acyclic graphs (BONDY &amp; MURTY, 2008) whose nodes denote\nvariables and arcs represent causal connections between the related variables. Also, a BN can\nmodel the quantitative strength of the connections between variables, allowing probabilistic\nbeliefs about them to be updated automatically as information becomes available.\n\nFor example, Ramesh et al. (2003) use a hybridism of BN and SVM in order to predict\nthe axis positioning errors in machine tools. Such errors depend on the machine temperature\nprofile and also on the specific operating condition the machine is subjected to. Firstly, a BN\napproach is used to classify the error into categories associated with the different operating\nconditions. After that, a knowledge base of errors due to specific machine conditions is formed\nand combined with classification results as input to an SVM regression model for mapping\nthe temperature profiles with the measured errors. The authors provide the following reasons\nfor using a BN: paucity of data, expert knowledge can be incorporated when data availability\nis sparse and it permits the learning of causal relationships between variables. For the use of\nSVM, the authors note that it does not require previous knowledge of the relationship between\ninput and output variables.\n\nIn the context of reliability prediction from time series data, in addition to the previous\nmethods, there is also the Autoregressive Integrated Moving Average (ARIMA) and the Duane\nmodels as alternatives for SVM. Morettin &amp; Toloi (2004) state that the ARIMA model has\nbeen one of the most popular approaches in time series forecasting and assume that predicted\nvalues are a linear combination of the previous values and errors. The Duane model, in turn,\n\n3\n\n\n\nChapter 1 INTRODUCTION\n\nare frequently applied on the analysis of reliability during the early stages of product design\nand development, which may involve reliability growth modeling. It assumes an empirical\nrelationship whereby the improvement in Mean Time Between Failures (MTBF) is proportional\nto T ?, where T is the equipment\u2019s total operational time and ? is the reliability growth factor\n(LEWIS, 1987; SMITH, 2001).\n\nTherefore, the advantages of SVM in relation to the other methods are: (i) no requirements\nof previous knowledge of or suppositions about the relation between inputs and outputs; (ii) no\nneed of a large quantity of data; (iii) incorporation of the SRM principle which provides it with\na better generalization ability and (iv) the resolution of a convex optimization problem in the\ntraining stage.\n\nAlthough SVM has been introduced in the sixties, its first applications in prediction based\non data series occurred in the end of the years 1990\u2019s, for example see M\u00fcller et al. (1999).\nIndeed, Sapankevych &amp; Sankar (2009) provide a literature survey of the works using SVM in\ntime series predictions. They make a count on the number of works regarding the knowledge\nareas in which they are inserted (Table 1.1). It can be noticed that only three works were classi-\nfied into the reliability prediction context, making it as the last field in number of related works.\nFrom this fact, it can be inferred that SVM is actually in the early stages of its applicability in\nthe reliability prediction problems based on time series data, if compared for example with the\neconomic field.\n\nTable 1.1: Number of SVM time series prediction papers by application. Adapted from Sapankevych\n&amp; Sankar (2009), p. 26\n\nApplication Number of papers\n\nFinancial market prediction 21\nElectric utility forecasting 17\nControl systems and signal processing 8\nMiscellaneous applications 8\nGeneral business applications 5\nEnvironmental parameter estimation 4\nMachine reliability forecasting 3\n\nNevertheless, the performance of SVM is influenced by a set of parameters that appear in\nthe training problem. This fact gives rise to the model selection problem that consists in choos-\ning suitable values for these parameters. They are actually very difficult to be manually tuned\nand systematic procedures may be required to perform this task. In this way, methods such as\nParticle Swarm Optimization (PSO) (BRATTON &amp; KENNEDY, 2007) can be used to tackle\nthe model selection problem. PSO is an optimization probabilistic heuristic, well-suited to deal\nwith real variables, based on the behavior of biological organisms that move in groups such as\nbirds. The nature concepts of cognition and socialization are translated in mathematical formu-\nlations for updating particles velocities and positions throughout the search space towards an\n\n4\n\n\n\nChapter 1 INTRODUCTION\n\noptimum position. There are basically two models of communication networks among parti-\ncles: in one of them, the most simple, all particles are connected to each other (gbest) and in the\nother, they are able to communicate only with some of them (lbest).\n\nBesides PSO, there are other probabilistic approaches to solve the model selection problem\nsuch as Genetic Algorithm (GA) (GOLDBERG, 1989). GA is a computational method usually\nused for optimization tasks and attempts to mimic the natural evolution process. It is based\non several genetic operators such as crossover and mutation and is often computationally more\nexpensive if compared to PSO. Other possible alternative to tackle the SVM model selection\nproblem is the grid search method (MOMMA &amp; BENNETT, 2002). The latter assumes the\nparameters as discrete values within a range and all possible combinations of them are assessed.\nIts main drawbacks consist in the discretization of the search space as well as in the great number\nof possibilities to be evaluated when there are several parameters to adjust.\n\nIn the following Section, some previous works related to SVM as well as to SVM model\nselection problem are presented.\n\n1.2 Previous works\n\nFor classification tasks, Rocco &amp; Moreno (2002) have used SVM to classify a component as\noperational or faulty in order to evaluate system overall reliability. The authors take advantage\nof the SVM velocity, which is greater than the one from the traditional discrete event simulation\napproach of Monte Carlo (BANKS et al., 2001). Then, they couple Monte Carlo simulation\nwith SVM. Rocco &amp; Zio (2007), in turn, use a multi-classification SVM to categorize anoma-\nlies in components. Widodo &amp; Yang (2007) make a review of the SVM applied to condition\nmonitoring and fault diagnosis.\n\nFor the prediction of reliability related measures based on time series, Hong &amp; Pai (2006)\nuse SVM coupled with an iterative method for selecting the associated SVM parameters. They\nforecast time failures of an engine. Additionally, Pai (2006) and Chen (2007) propose a GA+\nSVM approach to predict reliability values of an engine. GA is used as optimization tool to\nobtain the most suitable parameter values. No work was found relating the use of SVM with\nsystem characteristics, like temperature or the number of installed components, to predict con-\ntinuous reliability metrics (e.g. Time Between Failures (TBF), Time To Repair (TTR)).\n\nThe methodology of PSO+SVM is presented in some works. Lin et al. (2008) use such\napproach to the model selection problem but also to the choice of the most relevant input en-\ntries (problem known as feature selection). They apply the methodology on freely available\ngeneral data sets in Internet repositories. Dissolved gases contents on power transformers\u2019 oil\nare predicted in the work of Fei et al. (2009) with PSO+SVM approach. Also in the electricity\ncontext, Hong (2009) predicts the electric load by means of a PSO combined with a regression\nSVM. Samanta &amp; Nataraj (2009) apply PSO with a classification SVM in the context of fault\ndetection.\n\n5\n\n\n\nChapter 1 INTRODUCTION\n\nIn this way, so far, no PSO+SVM approach has been proposed to tackle reliability prediction\ntasks based either on time series data or on specific metrics of the system under consideration.\nAdditionally, all of them involved the most simple communication networks among particles,\nthe gbest model.\n\n1.3 Justification\n\nAs already mentioned, reliability of components and systems is a key element of production\nsystems due to its direct relation with productivity and costs, and thus with the performance of\norganizations within market. Thus, reliability prediction is a subject of great interest that can be\nreverted in economic and competitive gains to organizations.\n\nBesides this first motivation and reason, as can be concluded by the survey of Sapankevych\n&amp; Sankar (2009), there are few works of reliability prediction based on time series using SVM\nas forecast tool. In addition, from the literature review, no work was found either concerning\nSVM regression to predict reliability metrics based on system features or relating PSO with\nSVM to tackle the model selection problem in this specific context. Also, all works that involved\nPSO+SVM used a gbest communication network among particles.\n\nAlso, the Condition Based Maintenance (CBM), which is a maintenance program that rec-\nommends maintenance actions based on information collected via equipment condition moni-\ntoring, may incorporate SVM. CBM consists in three main steps (JARDINE et al., 2006):\n\n\u2022 Data acquisition: data regarding equipment status is collected.\n\n\u2022 Data processing: the acquired data is processed, analyzed and interpreted.\n\n\u2022 Maintenance decision-making: after data interpretation, efficient maintenance policies\nare recommended.\n\nSVM may take place in the second step of CBM. If the CBM approach is well established\nand implemented, it can provide the decrease of maintenance costs. For example, the work of\nMoura et al. (2009) assumes that the considered system is continuously monitored and that its\ncurrent state is available. Then, the authors propose maintenance policies based simultaneously\non availability and cost metrics of systems by means of Semi-Markov Decision Process (SMDP)\nand GA. Hence, they tackle the last step of a CBM program and the inclusion of SVM as a\nprevious step would result in a more comprehensive approach of the considered problem.\n\n1.4 Objectives\n\n1.4.1 Main Objective\n\nThis dissertation proposes a PSO algorithm to solve the SVM model selection problem. The\nresulted PSO+SVM methodology is then applied to the reliability context specifically involving\n\n6\n\n\n\nChapter 1 INTRODUCTION\n\nregression problems such as reliability prediction problems based on time series data and/or on\nsystem specific features.\n\n1.4.2 Specific Objectives\n\nIn order to attain the main objective, the following specific goals are established:\n\n\u2022 Literature review of the most used methods to solve the SVM model selection problem.\n\n\u2022 Implementation of a PSO algorithm linked with a SVM library to tackle the SVM model\nselection problem, resulting in a PSO+SVM approach.\n\n\u2022 Application of the proposed PSO+SVM to reliability prediction problems based on time\nseries data and also based on data collected from a real system.\n\n\u2022 Performance comparison between the proposed PSO+SVM and other methodologies such\nas GA+SVM, as well as other time series methods such as NN and ARIMA.\n\n\u2022 Performance comparison between gbest and lbest models for the application examples\ntaken into account.\n\n1.5 Dissertation Layout\n\nBesides this introductory chapter, this dissertation comprises four more chapters, whose\ncontents are described as follows:\n\n\u2022 Chapter 2: the theoretical background is presented. Initially, the SVM methods for\nclassification and regression tasks are detailed. Then, the related model selection problem\nas well as a survey of the methods that have been applied to tackle it are described. This\nchapter also contains the general ideas underlying PSO algorithms.\n\n\u2022 Chapter 3: the PSO methodology proposed in this work to tackle the SVM model selec-\ntion problem is detailed. Also, the PSO+SVM combination is commented.\n\n\u2022 Chapter 4: three application examples from literature are presented and resolved by\nmeans of the proposed PSO+SVM methodology. One of them is used in two different\nways, which yields four examples. The outcomes are compared to results from other\ntools available in literature (NN, ARIMA, among others). Also, an application example\ninvolving data collected from oil production wells is solved. Then, a comparison between\nthe gbest and lbest PSO and a discussion about the obtained results take place.\n\n\u2022 Chapter 5: a summary of the main contributions of this dissertation is provided along\nwith some comments about its limitations. In addition, this chapter presents some topics\nassociated with the ongoing research and suggestions for future works.\n\n7\n\n\n\n2 THEORETICAL BACKGROUND\n\n2.1 Support Vector Machines\n\nSVM is a learning method widely used in pattern recognition and regression problems. The\napplications of SVM belong to different domains, from computational biology (BEN-HUR et\nal., 2008) to financial series forecasting (GESTEL et al., 2001). In the reliability context, SVM\nhas been used for example in CBM and fault detection (WIDODO &amp; YANG, 2007), to classify\nanomalies in components (ROCCO &amp; ZIO, 2007), to forecast equipments\u2019 reliability (HONG\n&amp; PAI, 2006; CHEN, 2007), among others.\n\nIn its classical formulation, SVM is a supervised learning method, since it is based on\n(input, output) examples. SVM stems from the Statistical Learning Theory (SLT) and it is\nparticularly useful when the process in which inputs are mapped into outputs is not known.\nAccording to Kecman (2005), the learning problem is as follows: there is an unknown nonlinear\ndependence (mapping, function) y = f (x) between a multidimensional input vector x and an\noutput y. The only information available is a data set D ={(x1, y1), (x2, y2), . . . , (x`, y`)} , where\n` is the number of examples in D. The data set D is called training set due to its purposeful use\nin training the learning machine.\n\nDepending on the type of output y, different learning problems are defined:\n\n\u2022 Classification problems: y assumes discrete values that represent categories. If only two\ncategories are considered (e.g. y = ?1 or y = +1), the problem at hand is of binary\nclassification. Otherwise, if three or more categories are taken into account, it is the case\nof a multi-classification problem.\n\n\u2022 Regression problems: y is real-valued and its relation with the input vector x is given by\na function.\n\nThe solution of the learning problem is a decision function or a regression (target) function\nwhen, respectively, a classification or a regression is considered. For some learning machines\nlike NN, the underlying idea to obtain the answer of the learning problem is the principle of\nERM, which measures only the errors from the training step and is suitable for situations where\nthere is a large quantity of training examples (VAPNIK, 2000). On the other hand, obtaining the\nlearning problem solution via SVM involves a convex quadratic optimization problem, whose\nobjective function embodies the principle of SRM. This principle entails the minimization of the\nupper bound of the generalization error, which is formed by two parts: one of them is associated\nwith the machine ability to classify or predict unseen data (i.e., examples that are not in D), and\nthe other regards the training errors. In this way, there is a trade-off between model\u2019s capacity\nand training accuracy. Machines with low capacity have high training and generalization errors,\nwhich characterizes the situation of underfitting the data. On the other hand, increasing too\n\n8\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nmuch the machine capacity yields small training error but the generalization error grows due to\nits bad performance in classifying or predicting unseen data. This latter is the case of overfitting\nthe data, that is, the machine is too specialized in the training set that it is unable to work well\non data not in D. The behavior of those errors in relation to the machine capacity is illustrated\nin Figure 2.1 and the examples of underfitting and overfitting in classifying linearly separable\ndata are depicted in Figure 2.2. According to Kecman (2005), the SRM principle was proved\n\nCapacity\n\nUnderfitting Overfitting\n\nE\nrr\n\no\nr\n\nGeneralization error\n\nTraining error\n\nFigure 2.1: Relation between model capacity and error\n\nto be useful when dealing with small samples and its main idea consists in finding a model with\nadequate capacity to describe the given training data set. For further details in ERM and SRM,\nsee Vapnik (2000), Kecman (2001) and Kecman (2005).\n\nAn important advantage of SVM is that the training phase is accomplished by the resolu-\ntion of a convex quadratic optimization problem with a unique local optimum that is also global\n(BOYD &amp; VANDENBERGHE, 2004). Such problem involves well-known and established op-\ntimization theory and techniques to solve it, for example the concept of Lagrangian multipliers\nand KKT conditions. Besides that, the desired decision or regression function is always linear\nand presented by a hyperplane. Even if the relation in the input space is not linear, SVM uses\nkernel functions (see Sch\u00f6lkopf &amp; Smola (2002)) to map the input data x into a feature space,\noften of higher dimension, in which such relation is linear. In this feature space the training\nprocedure is executed. The following subsections introduce the main SVM classifiers as well\nas regression via SVM.\n\n2.1.1 Linear Maximal Margin Classifier for Linearly Separable Data\n\nThe binary classification of linear separable data is the most simple learning problem. Due\nto its simplicity, it is often not applicable in practical situations. Despite that, it presents the\nfundamental aspects of SVM and it is useful to understand the more complex and realistic\nSVM approaches.\n\nLet D = {(x1, y1), (x2, y2), . . . , (x`, y`)} be the training set with xi ? Rn, yi ?{?1, 1} and\n\n9\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nx1\n\nx2\n\nx1\n\nx2\n\nx1\n\nx2\n\nOverfitting, perfect\nclassification\n\nhigh capacity model,\n\nPerfect classification by low capacity model\n\nUnderfitting, low capacity model\n\nWrong classifications due to underfitting\n\nPerfect classification by high capacity model\n\nWrong classifications due to overfitting\n\nOverfitting, high capacity model\n\nPerfect classification by low capacity model\n\nFigure 2.2: Underfitting (up) and overfitting (down) the data in the case of binary classification\nof linearly separable data. Adapted from Kecman (2005), p. 8\n\ni = 1, 2, . . . , ` denoting the ith training example. Suppose that the data is linearly separable, that\nis, they can be perfectly separated by hyperplanes. The best of those hyperplanes is the one\nwith maximal margin, i.e., the one which maximizes the minimum distance between examples\nfrom distinct classes. If this optimal hyperplane is defined, one gets the decision function. The\nhyperplane equation in matrix form is given by:\n\nH = wT x + b = 0 (2.1)\n\nwhere w is the vector normal to the hyperplane, T indicates the matrix transpose operation, x is\nthe input vector and b is the linear coefficient of the hyperplane.\n\nSince the data is separable, in order to correctly classify a given example (xi, yi), the deci-\nsion function should satisfy the constraints:\n\nwT xi + b ? 1, if yi = 1 (2.2)\n\n10\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nwT xi + b ??1, if yi = ?1 (2.3)\n\nthat can be expressed by a single inequality:\n\nyi \u00b7(wT xi + b) ? 1 (2.4)\n\nWhen (2.2) and (2.3) are active, i.e., equalities hold, two hyperplanes H+ and H? are respec-\ntively defined and the distance between them is the margin (M). The vectors x from the training\nset which satisfy either H+ or H? are the so-called support vectors. As an illustration, Figure 2.3\ndepicts a simple two dimensional case of binary classification in which the margin, hyperplanes\nand support vectors are indicated.\n\nw\n\nx1\n\nx\nSV,-1\n\nx\nSV,+1\n\ny = -1\n\ny = +1\n\nx2\n\nH -= b =w x\nT\n\n+ -1\n\nH+ = b = +w x\nT\n\n+ 1\n\nH = b =w x\nT\n\n+ 0\n\nw\nr\n\nm\nar\n\ngi\nn\n\nSupport vectors from positive class\n\nSupport vectors from negative class\n\nFigure 2.3: Binary classification. Adapted from Kecman (2001), p. 154\n\nThe margin is, indeed, defined by the distance between support vectors from distinct classes\n(e.g. xSV,?1 and xSV,+1) projected onto the same direction of the hyperplanes\u2019 perpendicular\nvector w. That is,\n\nM = (xSV,+1 ?xSV,?1)w = ||xSV,+1||cos(?)?||xSV,?1||cos(?) (2.5)\n\nin which ? and ? are respectively the angles between xSV,+1 and w and between xSV,?1 and w\n(see Figure 2.3). These angles are given by:\n\ncos(?) =\nwT xSV,+1\n\n||w||\u00b7||xSV,+1||\ncos(?) =\n\nwT xSV,?1\n||w||\u00b7||xSV,?1||\n\n(2.6)\n\nBy replacing (2.6) in (2.5) and considering that xSV,+1 and xSV,?1 belong to H+ and H?, respec-\ntively, it is obtained:\n\nM =\nwT xSV,+1 ?wT xSV,?1\n\n||w||\n=\n?b + 1?(?b?1)\n\n||w||\n=\n\n2\n||w||\n\n(2.7)\n\n11\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nwhere ||w|| =\n?\n\nwT w =\n?\n\nw21 + w\n2\n2 +\u00b7\u00b7\u00b7+ w2n is the `2-norm of vector w. For further details in\n\nmargin definition consult Kecman (2001). Also, one can find information about the underlying\nanalytic geometry concepts in Reis &amp; Silva (1996).\n\nIt is noticeable that minimizing ||w|| is equivalent to maximize M. Moreover, to minimize?\nwT w is similar to minimize wT w. In this way, the convex optimization problem to be resolved\n\nduring the training step is:\n\nmin\nw,b\n\n1\n2\n\nwT w (2.8)\n\nsubject to yi \u00b7(wT xi + b) ? 1, i = 1, . . . , `\n\nwhere the constant 12 is a numerical convenience and does not change the solution. Notice that in\nthe case of linearly separable data, there is no training error and wT w is associated with machine\ncapacity (VAPNIK, 2000; KECMAN, 2005). To solve (2.8), a classic quadratic programming\nproblem, Lagrange multipliers along with KKT conditions are used. Due to its convex nature\n(objective function is convex and its constraints result in a convex feasible region), the KKT\nconditions are necessary and sufficient for an optimum (BOYD &amp; VANDENBERGHE, 2004).\nFirstly, the Lagrangian function is structured as follows:\n\nL(w, b, ???) =\n1\n2\n\nwT w?\n`\n\n?\ni=1\n\n?i \u00b7[yi \u00b7(wT xi + b)?1] (2.9)\n\nin which ??? is the `-dimensional vector of Lagrange multipliers. It is necessary to find the saddle\npoint (w0, b0, ???0) of L , since (2.9) has to be minimized with respect to the primal variables w\nand b and minimized with respect to the dual variables ?1, ?2, . . . , ?`, which should be non-\nnegative, i.e., ?i ? 0 for all i. This problem can be resolved either in the primal or in the dual\nspace. However, the latter approach has been adopted by a number of works due to the insight-\nful results it provides (for example, see Vapnik (2000), Sch\u00f6lkopf &amp; Smola (2002), Kecman\n(2005)).\n\nIn order to find the saddle point of L , the KKT conditions are then stated:\n\n? L(w0, b0, ???0)\n? w\n\n= 0, w0 =\n`\n\n?\ni=1\n\n?0i yi xi (2.10)\n\n? L(w0, b0, ???0)\n? b\n\n= 0,\n`\n\n?\ni=1\n\n?0i yi = 0 (2.11)\n\nyi \u00b7(wT0 xi + b0)?1 ? 0, i = 1, 2, . . . , ` (2.12)\n\n?0i ? 0, ?i (2.13)\n\n?0i \u00b7[yi \u00b7(wT0 xi + b0)?1] = 0, ?i (2.14)\n\nwhere the first two equalities in (2.10) and (2.11) result directly by the derivatives of L with\n\n12\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nrespect to the primal variables being null when evaluated at the optimum (w0, b0, ???0); the in-\nequalities in (2.12) and (2.13) require primal and dual feasibility of the solution, respectively;\nthe last equations (2.14) are the KKT complementarity conditions which states that the product\nof dual variables and primal constraints must vanish at the optimum. A useful insight stems\nfrom equations (2.14): support vectors lie on either H+ or H? and then nullify the second\nterm of the KKT complementarity conditions, which along with condition (2.13) implies non-\nnegative Lagrange multipliers. According to Nocedal &amp; Wright (2006), the constraints can be\nclassified as:\n\n\u2022 Inactive: if the constraint strictly satisfies the inequality and the related Lagrange mul-\ntiplier is exactly 0. Then, the optimal solution as well as the optimal objective function\nvalue are indifferent to whether such constraint is present or not.\n\n\u2022 Strongly active: if the constraint satistfies the equality and the respective Lagrange multi-\nplier is srictly positive. Thus, a perturbation in the constraint has an impact on the optimal\nobjective value with magnitude proportional to the Lagrange multiplier.\n\n\u2022 Weakly active: if the constraint satisfies the equality and the associated Lagrange multi-\nplier is exactly 0. In these cases, small perturbations in such constraint in some directions\nhardly affects the optimal objective value and solution.\n\nIn this way, for practical purposes, weakly active constraints can be treated as inactive. Hence,\nsupport vectors are identifiable by strictly positive Lagrange multipliers. It is important to\nemphasize that solving the SVM training problem is equivalent to solve the system defined by\nthe KKT conditions, given they are necessary and sufficient for a convex optimization problem.\nBy substituting the equalities (2.10) and (2.11) into L , it is obtained an expression involving\nonly the dual variables which has to be maximized:\n\nmax\n???\n\nLd (???) =\n`\n\n?\ni=1\n\n?i ?\n1\n2\n\n`\n\n?\ni=1\n\n`\n\n?\nj=1\n\nyiy j?i? jxTi x j (2.15)\n\nsubject to ?i ? 0, i = 1, 2, . . . , ` (2.16)\n`\n\n?\ni=1\n\n?i yi = 0 (2.17)\n\nThe dual problem resolution yields ` non-negative values for the Lagrange multipliers. By\nreplacing these values in equation (2.10), the optimal normal vector w0 is directly defined. Still,\nnotice that the summation over all ` is exactly the same as over only the support vectors, since\n?i = 0 if i is not a support vector. Differently from w0, b0 is implicitly determined by KKT\ncomplementarity conditions for any chosen support vector s (s = 1, 2, . . . , nSV , where nSV is\nthe number of support vectors). However, due to numerical instabilities, it is better to set b0\nas the mean over all values resulted from nSV calculations of (2.14), that is (BURGES, 1998;\n\n13\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nKECMAN, 2005):\n\nb0 =\n1\n\nnSV\n\nnSV\n\n?\ns=1\n\n(\n1\nys\n?wT0 xs\n\n)\n(2.18)\n\nOnce the optimal solution has been found, the decision function is obtained from the sepa-\nrating hyperplane H:\n\nd(x) = wT0 x + b0 =\n`\n\n?\ni=1\n\n?0iyixTi x + b0 (2.19)\n\nIf d(x) &lt;0, then x is categorized into the negative class (y = ?1). Otherwise, if d(x) > 0, x is\nclassified as being in the positive class (y = +1).\n\n2.1.2 Linear Soft Margin Classifier for Non-Linearly Separable Data\n\nIn some situations, the linear classifier still is a good option to separate overlapping data.\nThe training optimization problem can then be adapted to support the training step for linearly\nnon-separable data. Differently from the previous classifier, it is now necessary to allow for\ntraining errors, since constraints (2.2) and (2.3) can be violated. Because of this, the margin\nbecomes soft and examples within or beyond it (either in the correct side of the separating\nhyperplane or in the wrong one) are permitted, see Figure 2.4.\n\nw\n\nx1\n\ny = -1\n\ny = +1\n\nx2\n\nH-\n\nH+\n\nH\n\nm\nar\n\ngi\nn\n\nFree support vectors\n\nBounded support vectors\n\nA, D - Missclassified support vectors\n\nB, C - Correctly classified support vectors\n\nA\n\nB\n\nCD\n\nxA\n>1 x\n\nD\n>1\n\n0&lt;<1xB\n\n0&lt;<1xC\n\nCorrectly classified examples\n\nFigure 2.4: Binary classification for non-linearly separable data. Adapted from Kecman (2005), p. 20\n\nIn order to tackle this situation, new non-negative slack variables (?i, i = 1, 2, . . . , `) along\nwith a penalization factor for wrongly classified examples (C) are introduced to the training\nproblem, which becomes:\n\nmin\nw,b,???\n\n1\n2\n\nwT w + C \u00b7\n`\n\n?\ni=1\n\n?i (2.20)\n\nsubject to yi \u00b7(wT xi + b) ? 1??i, i = 1, 2, . . . , ` (2.21)\n\n?i ? 0, ?i (2.22)\n\n14\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nThe ith slack variable concerns the distance from point i to its corresponding margin bound. If\nit crosses such bound and is at its \u201cwrong\u201d side, ?i > 0. Otherwise, ?i = 0. In order to clarify\nthe role of these slack variables, one may see Figure 2.4. The factor C, in turn, is related to\nthe trade-off between training error and machine capacity. Actually, the problem (2.20)-(2.22)\nis a generalization for the training problem for linearly separable data. In the same way, the\nLagrangian function is formulated and the KKT conditions are used to solve it. The Lagrangian\nis:\n\nL(w, b, ???, ???, ???) =\n1\n2\n\nwT w + C \u00b7\n`\n\n?\ni=1\n\n?i ?\n`\n\n?\ni=1\n\n?i \u00b7[yi \u00b7(wT xi + b)?1 + ?i]?\n`\n\n?\ni=1\n\n?i?i (2.23)\n\nin which ??? and ??? are `-dimensional vectors of Lagrange multipliers. Once more, it is necessary\nto find the saddle point (w0, b0, ???0, ???0, ???0) of (2.23). The KKT conditions are:\n\n? L(w0, b0, ???0, ???0, ???0)\n? w\n\n= 0, w0 =\n`\n\n?\ni=1\n\n?0i yi xi (2.24)\n\n? L(w0, b0, ???0, ???0, ???0)\n? b\n\n= 0,\n`\n\n?\ni=1\n\n?0i yi = 0 (2.25)\n\n? L(w0, b0, ???0, ???0, ???0)\n? ?i\n\n= 0, ?0i + ?0i = C, i = 1, 2, . . . , ` (2.26)\n\nyi \u00b7(wT0 xi + b0)?1 + ?0i ? 0, ?i (2.27)\n\n?0i ? 0, ?i (2.28)\n\n?0i ? 0, ?i (2.29)\n\n?0i ? 0, ?i (2.30)\n\n?0i \u00b7[yi \u00b7(wT0 xi + b0)?1 + ?0i] = 0, ?i (2.31)\n\n?0i?0i = 0, (C??0i)\u00b7?0i = 0, ?i (2.32)\n\nIf equations (2.24), (2.25) and (2.32) are replaced in (2.23), it is obtained a Lagrangian in\nfunction only of the dual variables ?i, i = 1, 2, . . . , `. The dual problem is exactly the same as\nthe one presented for the classifier of linearly separable data, except the fact that the Lagrange\nmultipliers ?i have now parameter C as upper bound so as to respect the non-negativity of the\nLagrange multipliers ?i (see (2.32)). The problem is as follows:\n\nmax\n???\n\nLd (???) =\n`\n\n?\ni=1\n\n?i ?\n1\n2\n\n`\n\n?\ni=1\n\n`\n\n?\nj=1\n\nyiy j?i? jxTi x j\n\nsubject to 0 ? ?i ?C, i = 1, 2, . . . , ` (2.33)\n`\n\n?\ni=1\n\n?i yi = 0\n\nThe resulting decision function is also given by (2.19) and its signal defines the classi-\n\n15\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nfication of an input x either in the positive or negative class as is presented in the previous\nsubsection. The Lagrange multipliers, in turn, due to KKT complementarity conditions (2.31)\nand (2.32) have the following possible solutions:\n\n\u2022 ?0i = 0, ?0i = 0 and the input xi is correctly classified.\n\n\u2022 0 &lt;?0i &lt;C, then yi \u00b7(wT0 xi + b0)?1 + ?0i = 0 and ?0i = 0. Therefore, yi \u00b7(w\nT\n0 xi + b0) = 1\n\nand the example (xi, yi) is a support vector. The support vectors with associated Lagrange\nmultipliers satisfying the condition 0 &lt;?0i &lt;C are named free support vectors and permit\nthe calculation of b0 as follows:\n\nb0 =\n1\n\nnF SV\n\nnF SV\n\n?\ns=1\n\n(\n1\nys\n?wT0 xs\n\n)\n(2.34)\n\nwhere nF SV is the number of free support vectors. Again, it is recommended to set b0 as\nan average value.\n\n\u2022 ?0i = C, then yi \u00b7(wT0 xi + b0)?1 + ?0i = 0 and ?0i ? 0. Hence, (xi, yi) is a bounded\nsupport vector, given that its related Lagrange multiplier achieves the upper bound C.\nNote that the slack variable is not precisely defined and because of this, bounded support\nvectors do not participate in the calculation of b0. A bounded support vector is placed\nat the wrong side of either H+ or H? depending on the label yi. For 0 &lt;?0i &lt;1, xi\nis correctly classified. Otherwise, if ?0i ? 1, xi is wrongly classified. For the sake of\nillustration, in Figure 2.4, A\u2019s actual label is y = ?1, but it is at the right side of H and\nis then misclassified. On the other hand, B has label y = 1, but it is at the left side of\nH+. However, because B does not pass the separating hyperplane H, it is still correctly\nclassified. The same analysis can be done for the bounded support vectors C and D to\nconclude they are correctly and wrongly classified, in this order.\n\n2.1.3 Non-Linear Classifier of Maximal Margin\n\nWhen the decision function is not linear in input space, the linear classifiers previously\ndescribed can not be used to separate data. For example, in Figure 2.5-a, the hyperplane has a\npoor performance in classifying the examples into their actual categories in the input space. On\nthe other hand, the non-linear function perfectly accomplishes such task.\n\nIn these situations, SVM\u2019s main idea is to map the input vectors xi ? Rn into vectors ?(xi)\nof a space of higher dimension named feature space (F ). The notation ? represents the mapping\nRn ? F . In F , SVM obtains a linear separation hyperplane of maximal margin so as a linear\nclassification is still executed but in a different space, see Figure 2.5-b.\n\nNotice that the dual formulation (2.15) as well as the decision function (2.19), which hold\nfor both the linearly separable and overlapping data, present input data only in form of dot\n\n16\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nF\n\nInput space Feature space\n\n(a) (b)\n\nx1\n\nx2\n\ny = -1\n\ny = +1\n\nFigure 2.5: Non-linear binary classification\n\nproducts xTi x j, i, j = 1, 2, . . . , `. Then, when the mapping ? is applied, the training algorithm\nbecomes dependent on products of the form ?T (xi)?(x j). The choice of an appropriate map-\nping may be difficult, and besides that, the explicit calculation of the dot products ?T (xi)?(x j)\nmay be computationally burdensome if the dimension of F is very large. The latter problem\nis related to the phenomenon known as curse of dimensionality and can be avoided by intro-\nducing kernel functions K(xi, x j) = ?T (xi)?(x j). In this way, dot products in feature space are\ndirectly calculated by computing K(xi, x j), which in turn is defined on input space. Therefore,\nonly the kernel function may be used in the training algorithm, a possibly high dimension of\nF is bypassed and explicit knowledge of the mapping ? is not even required (BURGES, 1998;\nKECMAN, 2005). Some kernel functions are presented in Table 2.1.\n\nTable 2.1: Common kernel functions. Adapted from Kecman (2001), p. 171\n\nKernel Function Type\n\nK(xi, x j) = [(xTi x j) + 1]\nd Complete polynomial of degree d\n\nK(xi, x j) = exp\n[\n?(xi?x j )\n\n2\n\n2?2\n\n]\nGaussian Radial Basis Function (RBF)\n\nK(xi, x j) = tanh[(xTi x j) + b]\n? Multilayer perceptron\n\n?Only for particular values of b\n\nKernel functions should satisfy the Mercer\u2019s conditions, which state that to describe a dot\nproduct in some F , K(xi, x j) must be symmetric and fulfill the inequalityZ Z\n\nK(xi, x j)g(xi)g(x j)dxidx j > 0 (2.35)\n\nfor all function g(\u00b7) 6= 0 satisfying Z\ng2(x)dx &lt;? (2.36)\n\nFor further details on kernel functions and Mercer\u2019s conditions, the interested reader may con-\nsult Cristiniani &amp; Shawe-Taylor (2000), Vapnik (2000), Kecman (2001) and Sch\u00f6lkopf &amp; Smola\n\n17\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\n(2002).\nThe generalized dual Lagrangian adapted to non-linear classifiers then becomes:\n\nLd (???) =\n`\n\n?\ni=1\n\n?i ?\n1\n2\n\n`\n\n?\ni=1\n\n`\n\n?\nj=1\n\nyiy j?i? jK(xi, x j) (2.37)\n\nIf the data is separable, constraints (2.16) and (2.17) should be taken into account, whereas if\nthe data is non-separable, the Lagrange multipliers must be upper bounded by parameter C.\nTherefore, in the latter case, constraint (2.33) must be considered along with equality (2.17).\nThe decision function (2.19) is slightly modified so as to incorporate the kernel function and the\ndecision once again relies on its signal:\n\nd(x) = wT0 ?(x) + b0 =\n`\n\n?\ni=1\n\n?0iyiK(xi, x) + b0 (2.38)\n\nTo compute b0 it is necessary to replace the dot products by the kernel function where they\nnaturally emerge in (2.18) or (2.34), depending on the nature of training data (separable or not).\nNevertheless, given that the training algorithm for overlapping data is in fact a generalization of\nthe separable case, SVM non-linear classifiers embody the more general approach.\n\n2.1.4 Multi-classification\n\nThe multi-classification problem is a generalization of the binary classification which en-\ntails three or more categories (m ? 3). A usual technique to tackle multi-classification is to\ncombine several binary classifiers (HSU &amp; LIN, 2002).\n\nOne of the implementations of multi-class classifiers is the one-against-all approach. It\nconstructs m decision functions by means of the resolution of m training problems of the form:\n\nmin\nwk,bk,???k\n\n1\n2\n\nwTk wk + C \u00b7\n`\n\n?\ni=1\n\n?ki (2.39)\n\nsubject to wTk ?(xi) + bk ? 1??ki, if yi = k, i = 1, 2, . . . , ` (2.40)\n\nwTk ?(xi) + bk ??1 + ?ki, if yi 6= k, ?i (2.41)\n\n?ki ? 0, ?i (2.42)\n\nwhere k = 1, 2, . . . , m is the category index. Note that (2.39)-(2.42) is a general primal formula-\ntion of the training problem, since it permits errors during training step and also input vectors xi\nare mapped into a feature space through ?. However, transforming such problem into its dual\ncounterpart, once more the dot products ?T (xi)?(x j) naturally appear and instead of them, a\nkernel function K(xi, x j) may be used. In this way, the explicit knowledge of the mapping ?\nis not necessary. This same argument is valid for the decision functions resulted from training\n\n18\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nproblems.\nGiven an unseen input x (i.e., not from the training set D), one has the task of selecting its\n\ncategory. For that, the m decision functions are calculated for the considered x. The chosen\ncategory is the one associated with the decision function that has returned the greatest value. In\nsummary:\n\nClass(x) ? arg max\nk\n\n[wT0k?(x) + b0k] (2.43)\n\nOther approach for multi-classification using binary classifiers is the one-against-one me-\nthod. Its main idea consists in the construction of m(m?1)2 classifiers involving only two cat-\negories each. For instance, to train the data from classes j and k, the following optimization\nproblem has to be solved:\n\nmin\nw jk,b jk,??? jk\n\n1\n2\n\nwTjkw jk + C \u00b7\n`\n\n?\ni=1\n\n?( jk)i (2.44)\n\nsubject to wTjk?(xi) + b jk ? 1??( jk)i, if yi = j, i = 1, 2, . . . , ` (2.45)\n\nwTjk?(xi) + b jk ??1 + ?( jk)i, if yi = k, ?i (2.46)\n\n?( jk)i ? 0, ?i (2.47)\n\nThe dual formulation of such problem may be constructed and then solved as in the binary\nclassifiers training step described in previous subsections. If in average each class has `m training\nexamples, m(m?1)2 problems with\n\n2`\nm decision variables are resolved.\n\nIn order to classify an unseen input x, each one of the resulting m(m?1)2 decision functions\nare calculated for x. If the sign of [wT0 jk?(x) + b0 jk] is positive, than one vote is computed for\nclass j. Otherwise, if the sign is negative, then class k gains an additional vote. Following this\nreasoning, the class that has obtained the greatest number of votes, is the one to be associated\nto x. This strategy of class choice is named max-wins strategy.\n\nPlatt et al. (2000) consider a similar training algorithm as the one-against-one method. The\nselection of class for unseen inputs (x), however, is made by a different strategy. It is based on\na Directed Acyclic Graph (DAG) whose nodes represent the binary decision functions. Then,\nbefore predicting the class of x it is necessary to go through a path along the DAG.\n\nBesides the combination of binary classifiers approach to solve multi-classification pro-\nblems, there are methods which consider all classes simultaneously. According to Sch\u00f6lkopf &amp;\nSmola (2002), in terms of accuracy, the results obtained from these methods are comparable to\nthose obtained from the methods previously described. However, the optimization problem has\nto deal with all support vectors at the same time and the binary classifiers, in turn, usually have\nsmaller support vector sets, which has positive effects on training time. In addition, the authors\ncontend that probably there is no multi-class approach that generally outperforms the others.\nIn this way, the multi-classification method selection depends on the nature of the problem at\nhand, on the required accuracy and also on the available time for training. For further details in\n\n19\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nmulti-classification strategies, see Hsu &amp; Lin (2002) and Sch\u00f6lkopf &amp; Smola (2002).\nNotice that both classification and multi-classification cases have the parameter C and the\n\nkernel parameter (e.g. ?, d) in their training problems. The values for these parameters must be\ndefined beforehand by the user or by a systematic procedure. Indeed, this issue is detailed in\nSection 2.2, where the influence of these parameters in SVM performance is further discussed.\n\n2.1.5 Support Vector Regression\n\nIn a regression, it is necessary to estimate the functional dependence between an output\nvariable y ? R and an input variable x ? Rn. The main difference between classification and\nthe regression problems is that, in the former, only discrete numbers associated with categories\nmay be attributed to y, whilst in the latter, y assume real values, since y = f (x) and f : Rn ? R.\nSimilar to classification, the function estimate is based on the training of a SVM model using\nexamples of the form (input, output). The training phase of an SVM for regression resembles\nthe training phase of an SVM for classification purposes, given that both involve the resolution\nof a convex quadratic optimization problem. Nevertheless, Support Vector Regression (SVR)\ndual training problem entails 2` decision variables, instead of only ` as in classification training\nstep (SCH\u00d6LKOPF &amp; SMOLA, 2002; KECMAN, 2005).\n\nAdditionally, for the regression case, an analog of the soft margin is constructed in the space\nof the target values y by using Vapnik\u2019s linear ?-insensitive loss function (see Figure 2.6), which\nis defined as:\n\nL(x, y, f ) = max(0,|y? f (x)|??) (2.48)\n\nthat is, the loss (cost) is zero if the difference between the predicted f (x) and the observed y\nvalues is less than ?. Otherwise, the loss is given by the absolute difference between these two\nvalues. The Vapnik linear ?-insensitive loss function defines a \u201ctube\u201d of \u201cradius\u201d ? (Figure 2.6-\na). If the observed value is within the tube, there is no computed loss, whilst for values outside\nthe tube the cost is positive. It follows that:\n\n|y? f (x)|?? = ?, for data \u201cabove\u201d the ?-tube (2.49)\n\n|y? f (x)|?? = ??, for data \u201cbelow\u201d the ?-tube (2.50)\n\nwhere ? and ?? are the slack variables for the mutually exclusive situations presented.\nBesides the ?-insensitive loss function, there are other loss functions which can be incor-\n\nporated by SVR as long as they are convex in order to ensure the convexity of the training\noptimization problem and then the existence (and uniqueness for strict convexity) of a mini-\nmum. For example, the quadratic ?-insensitive (2.51) loss function leads to a training problem\nwith the same conveniences provided by the linear ?-insensitive loss function. In this work,\nhowever, only the latter loss function is considered. For more information about loss functions\n\n20\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nH-\n\nH+\n\nx\n\ny\n\ny f x- ( )\n\n0\n\n0\n+e\n\n-e\n\nx\n\nx\n\nx\n*\n\nx\n*\n\n(a)\n\nSupport vectors\n\n(b)\n\n+e-e\n\nFigure 2.6: Vapnik\u2019s ?-insensitive loss function\n\nin SVR context, consult Vapnik (2000) and Sch\u00f6lkopf &amp; Smola (2002).\n\nL(x, y, f ) = max(0,|y? f (x)|2 ??) (2.51)\n\nThis subsection presents the basic concepts of SVR for the case of linear and non-linear\napproximation functions as well as some insights of SVR applied to time series prediction.\n\n2.1.5.1 Linear Regression Function\n\nAs in classification, the only information available for a SVR is the training data set D =\n{(x1, y1), (x2, y2), . . . , (x`, y`)}, xi ? Rn, y ? R. It is supposed that a linear function is a good\nregression alternative. Then, it is necessary to find the regression hyperplane which describes\nbest the training data, so as to allow for the use of such hyperplane to effectively regress unseen\ninput vectors. The equation of the regression hyperplane is:\n\nf (x) = wT x + b (2.52)\n\nIn order to find the optimal regression hyperplane, besides the training error measured by\nthe ??insensitive loss function, as well as in classification, it is necessary to minimize the term\nwT w related to machine capacity. A small wT w corresponds to a linear function that is flat\n(SCH\u00d6LKOPF &amp; SMOLA, 2002; SMOLA &amp; SCH\u00d6LKOPF, 2004). The primal optimization\nproblem is then defined:\n\nmin\nw,b,???,????\n\n1\n2\n\nwT w + C \u00b7\n`\n\n?\ni=1\n\n?i + ?\n?\ni (2.53)\n\nsubject to yi ?wT xi ?b ? ? + ?i, i = 1, 2, . . . , ` (2.54)\n\nwT xi + b?yi ? ? + ??i , ?i (2.55)\n\n21\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\n?i ? 0, ?i (2.56)\n\n?\n?\ni ? 0, ?i (2.57)\n\nThe corresponding Lagrangian is:\n\nL(w, b, ???, ????, ???, ????, ???, ????) =\n1\n2\n\nwT w + C \u00b7\n`\n\n?\ni=1\n\n(?i + ?\n?\ni )?\n\n`\n\n?\ni=1\n\n?i \u00b7[wT xi + b?yi + ? + ?i]\n\n?\n`\n\n?\ni=1\n\n?\n?\ni \u00b7[yi ?w\n\nT xi ?b + ? + ??i ]?\n`\n\n?\ni=1\n\n(?i?i + ?\n?\ni ?\n?\ni )\n\n(2.58)\n\nin which ???, ????, ???, ???? are the `-dimensional vectors of Lagrange multipliers associated to con-\nstraints (2.54)-(2.57) respectively. Notice that that ?i and ??i can not be strictly positive si-\nmultaneously, given that there is no point satisfying both (2.54) and (2.55) at the same time.\nHence, ?i??i = 0. The Lagrangian in (2.58) must be minimized with respect to primal variables\nw, b, ???, ???? and maximized with respect to dual variables ???, ????, ???, ????. Then the saddle point\n(w0, b0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0) has to be found. The KKT conditions are:\n\n? L(w0, b0, ???0, ???\n?\n0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0)\n\n? w\n= 0, w0 =\n\n`\n\n?\ni=1\n\n(?0i ???0i)xi (2.59)\n\n? L(w0, b0, ???0, ???\n?\n0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0)\n\n? b\n= 0,\n\n`\n\n?\ni=1\n\n(?0i ???0i) = 0 (2.60)\n\n? L(w0, b0, ???0, ???\n?\n0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0)\n\n? ?i\n= 0, C??0i = ?0i, i = 1, 2, . . . , ` (2.61)\n\n? L(w0, b0, ???0, ???\n?\n0, ???0, ???\n\n?\n0, ???0, ???\n\n?\n0)\n\n? ?\n?\ni\n\n= 0, C???0i = ?\n?\n0i, ?i (2.62)\n\nwT0 xi + b0 ?yi + ? + ?0i ? 0, ?i (2.63)\n\nyi ?wT0 xi ?b0 + ? + ?\n?\n0i ? 0, ?i (2.64)\n\n?0i ? 0, ?i (2.65)\n\n?\n?\n0i ? 0, ?i (2.66)\n\n?0i ? 0, ?i (2.67)\n\n?\n?\n0i ? 0, ?i (2.68)\n\n?0i ? 0, ?i (2.69)\n\n?\n?\n0i ? 0, ?i (2.70)\n\n?0i \u00b7[wT0 xi + b0 ?yi + ? + ?0i] = 0, ?i (2.71)\n\n?\n?\n0i \u00b7[yi ?w\n\nT\n0 xi ?b0 + ? + ?\n\n?\n0i] = 0, ?i (2.72)\n\n?0i?0i = 0, (C??0i)\u00b7?0i = 0, ?i (2.73)\n\n?\n?\n0i?\n?\n0i = 0, (C??\n\n?\n0i)\u00b7?\n\n?\n0i = 0, ?i (2.74)\n\n22\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nBy replacing equalities (2.59)-(2.60) in (2.58), a Lagrangian in function only of the dual vectors\n??? and ???? is obtained. As in classification, the dual optimization problem may be solved:\n\nmax\n???,????\n\nLd (???, ????) = ?\n1\n2\n\n`\n\n?\ni=1\n\n`\n\n?\nj=1\n\n(?i ???i )(? j ??\n?\nj )x\n\nT\ni x j ?\n\n`\n\n?\ni=1\n\n[?(?i + ?\n?\ni ) + yi(?i ??\n\n?\ni )] (2.75)\n\nsubject to\n`\n\n?\ni=1\n\n(?i ???i ) = 0 (2.76)\n\n0 ? ?i ?C, i = 1, 2, . . . , ` (2.77)\n\n0 ? ??i ?C, ?i (2.78)\n\nAs a result from the dual problem, non-negative values for the Lagrange multipliers ?i\nand ??i for all i are obtained. From them, the optimal normal vector w0 is directly defined by\n(2.59). By the possible values for ?0i and ??0i as well as considering the KKT complementarity\nconditions (2.71)-(2.74), one may conclude:\n\n\u2022 If 0 &lt;?0i &lt;C, then ?0i = 0 is true. Besides that, the equality wT0 xi + b?yi = ?? is\nobtained and then the example (xi, yi) intercepts the parallel hyperplane that is ? above\nthe regression hyperplane (H+). Similarly, if 0 &lt;??0i &lt;C, from ?\n\n?\n0i = 0 holds and conse-\n\nquently the equality yi ?wT0 xi ?b = ?? is valid. Hence, the point (xi, yi) intercepts the\nhyperplane that is ? below the regression hyperplane (H?). When either 0 &lt;?0i &lt;C or\n0 &lt;??0i &lt;C is true, the related example (xi, yi) is named free support vector, which allow\nfor the calculation of the linear coefficient b0:\n\nb0 =\n1\n\nnF SV\n\n[\nnU F SV\n\n?\ns=1\n\n(ys ?wT0 xs ??) +\nnLF SV\n\n?\ns=1\n\n(ys ?wT0 xs + ?)\n\n]\n(2.79)\n\nwhere nU F SV and nLF SV are respectively the number of free support vectors lying in\nH+ and H?.\n\n\u2022 For data \u201cabove\u201d the ?-tube, i.e. ?0i > 0, the equality ?0i = C holds. On the other hand, if\ndata is \u201cbelow\u201d the ?-tube, then ??0i > 0, which implies ?\n\n?\n0i = C. Training data that satisfy\n\nthese conditions are called bounded support vectors. As in the case of soft-margin clas-\nsifier, these support vectors do not construct the value of b0, since it can not be uniquely\ndetermined due to positive but not exactly known slack variables values.\n\n\u2022 For training examples lying \u201cwithin\u201d the ?-tube, |yi ?wT0 xi ?b| &lt;? is valid and as a\nconsequence ?0i = ??0i = 0. Such points are not support vectors and also do not construct\nthe regression hyperplane, which is defined as follows:\n\nf (x) = wT0 x + b0 =\n`\n\n?\ni=1\n\n(?i ???i )x\nT\ni x + b0 (2.80)\n\n23\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\n2.1.5.2 Non-Linear Regression Function\n\nThe generalization from linear to non-linear regression functions is possible by the use of\nkernel functions in the same way linear classifiers evolve to non-linear ones. In this way, even\nif the regression function is non-linear in input space, a regression hyperplane can be found in\na feature space.\n\nx\n\ny\n\n0\n\n+e\n\n-e\n\nF(x)\n\nH-\n\nH+\n\ny f x= ( ))F(\n\nF\n\nInput space\n\n(a) (b)\n\nFeature space\n\nSupport vectors\n\nFigure 2.7: Non-linear regression\n\nOnce more notice that the dual training problem in subsection 2.1.5.1 presents input data\nin the form of dot products xTi x j. Analogous to non-linear classifiers, such dot products are\nreplaced by a kernel function K(xi, x j) = ?T (xi)?(x j) set a priori. The normal vector w0\nmay not be directly obtained, since its expression (2.81) becomes function of the mapping ?,\nwhich often involves a high dimension or it is even not known. Both cases render the explicit\ncomputation of w0 impractical.\n\nw0 =\n`\n\n?\ni=1\n\n(?0i ???0i)?(xi) (2.81)\n\nThe linear coefficient b0, in turn, can be calculated by either the KKT complementarity\nconditions (2.71) or (2.72). After replacing w0 in these equations, the dot product ?T (xi)?(x j)\nnaturally appears and may be substituted by K(xi, x j). Again, it is better to set b0 as the average\nover all calculated expressions (KECMAN, 2005).\n\nTherefore, one may have the non-linear regression function:\n\nf (x) = wT0 ?(x) + b0 =\n`\n\n?\ni=1\n\n(?0i ???0i)?\nT (xi)?(x) + b0 =\n\n`\n\n?\ni=1\n\n(?0i ???0i)K(xi, x) + b0 (2.82)\n\nSVR and traditional statistical (non)-linear regression models are different approaches to\nsolve regression problems. Firstly, traditional models are classified as (non)-linear with respect\nto the parameters of the regression function. SVR, however, is classified as (non)-linear de-\n\n24\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\npending on the (non)-linearity on the input variables (x). One advantage of SVR over statistical\nlinear regression models is the fact that the errors do not need to be drawn from a Normal\ndistribution with zero mean and constant variance. In fact, SVR does not require assumptions\nover the errors. Additionally, differently from SVR, to apply a statistical non-linear regression\nmodel, a previous knowledge about the relationship between the response and input variables\nis often required. Also, due to the use of kernels, SVR always involve a linear regression func-\ntion, either in the input or in the feature space. For more in traditional statistical (non)-linear\nregression methods, see Montgomery et al. (2001).\n\n2.1.5.3 Time Series Prediction\n\nAccording to Fuller (1996), a real valued time series can be considered as a collection of\nrandom variables indexed in time. For a specific event under analysis (e.g. system failure), the\nrealizations of such random variables generate a set of observations ordered in time. Some of\nthe most common purposes of studying time series are to learn about the underlying mecha-\nnism generating the data, to predict future values of the phenomenon under analysis and/or to\noptimally control a system. For instance, financial series of stock prices may provide investors\nwith information whether or not they may invest in the next future. Also, a series of reliability\nvalues of a critical machine gives insights of its \u201chealth\u201d, which is very useful for maintenance\nplanning.\n\nGenerally, a time series is not drawn independently. Conversely, the statistical learning\nmodel underlying SVR assumes independent and identically distributed samples. Despite this\nfundamental difference mentioned by Sch\u00f6lkopf &amp; Smola (2002), SVR has been widely applied\nto the problem of time series prediction and has provided excellent results for them, compara-\nble to or even better than the ones originated from other approaches such as NN. This empirical\nevidence can be verified in the work by M\u00fcller et al. (1999) that shows the superior perfor-\nmance of SVR in a benchmark set from the Santa Fe Time Series Competition (WEIGEND &amp;\nGERSHENFELD, 1994) as well as in the reliability related papers from Hong &amp; Pai (2006)\nand Chen (2007). Moreover, a survey of works using SVM for time series prediction applied to\ndiverse fields is presented by Sapankevych &amp; Sankar (2009).\n\nThe general time series prediction model can be represented as follows:\n\nyt = f (yt?1, yt?2, . . . , yt?p) (2.83)\n\nwhere yt is the observation at time t which is function of the p past observations. In other words,\nthe input vector xt = (yt?1, yt?2, . . . , yt?p), where p denotes its dimension, is directly related to\nthe future value yt . Additionally, if t data observations compose the time series, one may have\nto construct the examples in the form (xi, yi), i = 1, 2, . . .t ? p as shown in Table 2.2. In this\nway, the training set is comprised by t ? p training examples.\n\n25\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nTable 2.2: Construction of data pairs for time series prediction\n\ni Input xi Output yi\n\n1 y1 y2 \u00b7\u00b7\u00b7 yp yp+1+k\n2 y2 y3 \u00b7\u00b7\u00b7 yp+1 yp+2+k\n...\n\n...\n... \u00b7\u00b7\u00b7\n\n...\n...\n\nt ? p yt?p yt?p+1 \u00b7\u00b7\u00b7 yt?1 yt+k\n\nBesides that, the future outcomes may be predicted for different steps in time. In Table\n2.2, k is the number of steps ahead minus one. For example, if the (i?1)th values are inputs\nto forecast the ith one, it is then performed a single-step-ahead prediction (k = 0). If the same\ninputs are used to predict the (i + 1)th outcomes, a two-step-ahead forecast takes place (k = 1).\nGenerally, predicting two or more steps ahead with (i?1)th input values is said to be a multi-\nstep-ahead forecast.\n\nOnce the training examples are formed, the application of SVR follows the same reason-\ning shown in subsections 2.1.5.1 and 2.1.5.2. An advantage of using SVR approach in dealing\nwith time series prediction is the fact that it is model-independent and can tackle non-linear\nand non-stationary series, which may not be handled by traditional methods if simplifying as-\nsumptions or alternative techniques to render them stationary are not considered. Basically, a\nnon-stationary time series randomly varies along time around a constant mean, reflecting a form\nof equilibrium (MORETTIN &amp; TOLOI, 2004).\n\nAs in the (multi-)classification problems, SVR also depends on a set of parameters that\nhas to be defined a priori. Besides the penalization for errors C and the kernel parameter\n(e.g. ? in RBF case), SVR also demands the definition of ? from the Vapnik\u2019s ?-insensitive loss\nfunction. In both classification and regression tasks, the choice of these parameters is often very\ndifficult. This issue gives rise to the model selection problem, which is discussed in Section 2.2.\nThe following Subsection introduces the most used optimization techniques to solve the SVM\ntraining problem.\n\n2.1.6 Optimization Techniques and Available Support Vector Machines Libra-\n\nries\n\nDifferent optimization techniques can be applied to the SVM learning problem. One of\nthem is the Interior Point Method (NOCEDAL &amp; WRIGHT, 2006), that is indicated for small to\nmoderately sized data sets, up to 104 (SCH\u00d6LKOPF &amp; SMOLA, 2002). Alternatively, the faster\nSequential Minimal Optimization (SMO) approach can be adopted. Loosely speaking, SMO\ndecomposes the quadratic training problem into the smallest possible quadratic subproblem,\nwhich involves only two examples (two Lagrange multipliers) so as to allow for the analytic\ntreatment of them instead of numerical. At every step, SMO chooses two Lagrange multipliers\n\n26\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nto jointly optimize, finds the optimal values for these multipliers, and updates the SVM to reflect\nthe new optimal values. Since the storage of large matrices is not required, large data sets can\nbe handled and problems with numerical precision are avoided (PLATT, 1998).\n\nThere are several free SVM solvers available in the Internet, which are periodically im-\nproved by the developers. Two of them are the SVMlight and the LIBSVM. SVMlight is an\nimplementation of SVM learner that solves the SVM dual problem by means of a decomposi-\ntion strategy that generates a series of smaller optimization problems to be resolved. The dual\ndecision variables (???) are divided in two groups: the first one comprises the variables that can\nvary their values during the optimization process and forms the so-called working set; the sec-\nond one is formed by variables with fixed values. The selection of which variables will be in the\nworking set is made through the choice of the steepest descent feasible direction, which in turn\nwill make much progress towards the minimum of the objective function. For further details,\nsee Joachims (1999).\n\nLIBSVM is an integrated software for support vector classification, multi-classification and\nregression tasks. It can be easily linked with other programs through several programming\nlanguages such as MATLAB. Additionally, it implements a SMO algorithm for solving the\ntraining problem (CHANG &amp; LIN, 2001; FAN et al., 2005). In this dissertation, LIBSVM is the\nused library for training of SVM, as well as for prediction via a particular SVM already trained.\n\nSVM performance is influenced by the values of the parameter C, ? (regression case) and\nkernel parameter (e.g. ?, d). The problem of selecting the most suitable set of parameters is\ndetailed in next section.\n\n2.2 Model Selection Problem\n\nThe performance of SVM strongly depends on the chosen set of parameters. The task of\ntuning the SVM parameters so as to obtain the most suitable set of values for them is known\nas the model selection problem. In classification problems, the trade-off between model capac-\nity and training error represented by C and the kernel parameter (e.g. the RBF width ? or the\npolynomial degree d) are the user defined parameters. For instance, a large C forces the SVM\nclassification algorithm to reduce the training errors, which in turn can be accomplished by\nincreasing the machine capacity (by means of wT w) and as a consequence may reduce the mar-\ngin. This is contrary to the main objective of margin maximization and also does not guarantee\na good generalization performance of the classifier.\n\nRegression problems, along with C and the kernel parameter, present the \u201cradius\u201d ? of the\ntube. For a small C, penalty on errors gets negligible and regression function becomes flat,\nwhile for a large C a penalty gets more important and the resulting regression function tries to\nfit the data. A small ? inclines the SVR function to fit the data, since a very thin ?-tube may\nbe not sufficiently wide to include even few data points. A large ?-tube, however, may be wide\nenough, which renders the SVR function flat and, as a consequence, it may not describe well\n\n27\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nthe training set. If a RBF kernel is considered, a small ? means the kernel is more localized.\nThus, the SVR function has a tendency to overfit, while a large ? makes the ?-tube less flexible\n(ITO &amp; NAKANO, 2003). In Figure 2.8, it is illustrated the effects of small and large values of\nthe parameters for the regression case.\n\nx\n\ny\n\nx\n\ny\n\nSmall C Large C\n\nx\n\ny\n\nSmall e\n\nx\n\ny\n\nLarge e\n\nx\n\ny\n\nSmall s\n\nx\n\ny\n\nLarge s\n\nFigure 2.8: Influence of parameters in SVR. Adapted from Ito &amp; Nakano (2003), p. 2078\n\nGiven the influence of parameters on SVM performance, it is necessary to select them\nas accurately as possible. The most naive attempt to set these parameters is the trial and error\nmethod, which is time consuming and does not ensure a useful set will be found. More educated\nways to tune these parameters are then listed:\n\n\u2022 Grid search: ranges of parameters are defined and then made discrete so as to allow for the\n\n28\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\ntest of all possible combinations. This procedure may be suitable if only few parameters\nare to be tuned, but otherwise may be time consuming and may become impractical if\nthere are several parameters as well as many possible values for each one of them. Hsu\n&amp; Lin (2002) use this method for adjusting the SVM parameters in the context of multi-\nclassification problems.\n\n\u2022 Pattern search: is a direct search method, suitable to optimize a wide range of objective\nfunctions, given that it does not require derivatives (LEWIS et al., 2000). The number of\nevaluated combinations is smaller than the quantity assessed in the grid-search method. It\nwas applied to SVR model selection in the context of drug designs by Momma &amp; Bennett\n(2002). The authors also make a comparison between the pattern search and the grid\nsearch.\n\n\u2022 Gradient-based search: requires continuous and differentiable error functions. Vapnik &amp;\nChapelle (2000) derive some generalization error bounds for classification and in the later\nwork by Chapelle et al. (2002) a gradient descent method is applied to obtain the set of\nparameters that minimize those bounds. Chang &amp; Lin (2005) present some error bounds\nfor regression and use a quasi-Newton method to get the parameters minimizing them.\nAs stated by Ito &amp; Nakano (2003) the linear ? loss function does not produce a smooth\nerror surface, which burden gradient methods. Alternatively, the authors incorporate the\nquadratic ? loss function to SVR and then present the derivatives of the error function with\nrespect to the parameters. For details in gradient-based methods see Nocedal &amp; Wright\n(2006).\n\n\u2022 Bayesian evidence framework: its main idea is to maximize the posterior probability of\nthe parameter distribution to get the optimal parameter. The evidence framework was\nadapted to SVM model selection by Kwok (2000). The author describes the methodol-\nogy for classification problems. Later, Gestel et al. (2001) and Yan et al. (2004) apply\nevidence framework to regression situations. The first work is related to financial series\nforecasting. The second one is associated to the refinery oil industry and aims at esti-\nmating the freezing point of light diesel oil in a fractionator by means of process data\nrecords.\n\n\u2022 Probabilistic search heuristics: are flexible optimization techniques that do not require\nderivatives and are often based on nature aspects. For example, GA and PSO are widely\napplied to optimization problems from different contexts. Specifically in SVM field, Pai\n(2006) and Chen (2007) apply GA to select the SVR parameters to forecast series related\nto reliability of engineered components. Additionally, in the electricity management field,\nPSO is used by Fei et al. (2009) in a SVR to predict the quantity of gases dissolved in\npower transformer oil based on previous observed values and by Hong (2009) in select-\ning the parameter of a SVR to forecast electric load. In the domain of fault detection\n\n29\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nSamanta &amp; Nataraj (2009) apply PSO to choose the kernel parameter of the SVM clas-\nsifier. Simulated Annealing (SA) may also be part of this group and is applied to SVR\nforecast electricity load and software reliability by respectively Pai &amp; Hong (2005) and\nPai &amp; Hong (2006).\n\nOther systematic manners to find the parameter values so as to allow for generalization\nerror minimization are available in literature. The interested reader may consult for instance\nFr\u00f6hlich &amp; Zell (2005) for SVM classification and regression parameters selection and Wu &amp;\nWang (2009) for the classification case only. In this work, the PSO approach is adopted to\nselect the parameter values of SVM, given it is well-suited to optimize real-valued functions as\nwell as does not require derivatives, which avoids problems with non-smooth error surfaces. In\naddition, if compared to GA, for example, PSO requires less computational effort.\n\nIn practice, the data set is divided in two parts, one for actual training (training set formed\nby ` elements) and the other for posterior test (test set comprised by ? examples). The test\nset plays the role of unseen data and are not used during SVM training. Instead, it is used to\nevaluate an error function usually involving the real output values (yh) and the predicted ones\n(y?h) resulted from the trained SVM model, where h = 1, 2, . . . , ? is the index of an example from\nthe test set. Two error functions commonly applied for SVM testing are the Normalized Root\nMean Square Error (NRMSE) and the Mean Absolute Percentage Error (MAPE). Eventually\nthe Mean Square Error (MSE) is also used. These error functions are defined as follows:\n\nNRMSE =\n\n??????h=1(yh ? y?h)2\n?\n\n?\n\nh=1 y\n2\nh\n\n(2.84)\n\nMAPE =\n1\n?\n\n?\n\n?\nh=1\n\n????yh ? y?hyh\n????\u00b7100% (2.85)\n\nMSE =\n1\n?\n\n?\n\n?\nh=1\n\n(yh ? y?h)2 (2.86)\n\nIn order to select the most suitable parameters, the mentioned error functions may be itera-\ntively evaluated on the test set so as to guide the quest for improved parameter values. Neverthe-\nless, a unique test set may mislead to an error estimate far from the real generalization error. In\nthis way, specially the grid, pattern and probabilistic search methodologies incorporates during\nthe training phase a sort of model validation, in which the training set is split into even smaller\nsubsets that participate in either actual training or model validation in an alternated manner.\nOne of the following training strategies may be adopted:\n\n\u2022 Validation sets: the training set is divided into two subsets, one for actual training and a\nsecond one to guide the search for optimal parameter values (validation set). Once the\nparameters are found, the machine\u2019s generalization performance can be evaluated on a\n\n30\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\ntest set. It is expected a test error greater than the validation error, since the machine\nwould be specialized in the validation set. However, with this procedure, one may have\ninsights about the machine ability to generalize. Hong &amp; Pai (2006) and Pai (2006) apply\nthis strategy. Also, instead of one single validation set, Chen et al. (2004), in their work\nrelated to electricity load prediction, make use of two validation sets and the quest for the\nSVR parameters are based on the mean validation error resulted from both of them.\n\n\u2022 Cross-validation: the training set is split randomly into several subsets, say k. A k-fold\ncross-validation consists in training the SVM model with k?1 subsets and validate it on\nthe remaining subset by means of an error function. This is made k times so as each sub-\nset participate in the validation phase once. The error is averaged over the k validations\n(equations (2.84) and (2.85) may be multiplied by 1k ). This procedure is more time con-\nsuming than the single test set approach but it gives a better estimate of the generalization\nerror.\n\n\u2022 Leave-one-out: this is the extreme of the cross-validation strategy. If the training set is\nformed by ` examples, ` trainings are performed. In each one of them `?1 examples\nactually participate in the training phase and the remaining one is used for model vali-\ndation. Hence, every example participate in validation once. This approach is the most\ntime-consuming but, according to Sch\u00f6lkopf &amp; Smola (2002) it provides an almost un-\nbiased estimate of the error. Additionally, Lee et al. (2004) present a manner to enhance\nthe efficiency of this procedure for Gaussian kernel-based SVM.\n\nIt is important to emphasize that the described strategies occur during the training phase,\nwhich can be didactically divided in actual training and validation. After that phase, the set of\nparameters which return the minor error value may be adopted and are then often used to train\nthe entire training set (i.e. all ` examples at once). This procedure has some theoretical issues\nspecially associated with cross-validation that are described by Sch\u00f6lkopf &amp; Smola (2002). For\nexample, given that the retraining with all examples makes use of the same data which guided\nthe search for parameters, it can lead to overfitting. Also the optimal parameter settings for data\nsets of size (k?1) `k and ` do not usually coincide. Nevertheless, applications not considering\nthese potential problems are frequently performed with promising results. Finally, after setting\nthe parameters, the SVM model is tested on test set.\n\nAdditionally, it is noteworthy that when handling time series data it is important to realize\nthat the entries are inherently indexed on time. This is crucial specially when dealing with non-\nstationary time series, where the mixing of data may lead to completely different realizations\nof the underlying phenomenon at hand. In this way, choosing random subsets to apply cross-\nvalidation for parameter selection is not an indicated technique for these cases, even if those\nsubsets could be internally sorted or divided in approximate equal time intervals. Given that a\nsubset is extracted from the training in order to validate the model, the machine would not be\n\n31\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nable to catch the existent correlation from the original data. As an alternative, the validation\nsets approach respecting the time series order can be adopted by dividing data set into training,\nvalidation and test sets considering the original order, so as to allow for performance evalu-\nations of the machine ability to predict future outcomes. The leave-one-out procedure could\nalso be applied since only one data entry at a time is excluded from training with possibly no\nserious influence on the natural relationship among data. However, this latter approach has the\ncost of high computational effort, which depending on the parameter search tool used may be\nprohibitive.\n\nThe majority of the application examples considered in this work are related to reliability\ntime series (Chapter 4). Thus, along with PSO, this work makes use of the validation sets ap-\nproach. Even for the example concerning real data from oil production wells the validation sets\nprocedure is adopted, since the cross-validation and leave-one-out techniques are time consum-\ning. In next section, the main ideas and characteristics of PSO are described.\n\n2.3 Particle Swarm Optimization\n\nThe PSO algorithm was introduced by Kennedy &amp; Eberhart (1995) and it is based on the\nsocial behavior of biological organisms that move in groups such as birds and fishes. It was\noriginally developed to solve non-linear optimization problems. PSO also has some ties to evo-\nlutionary algorithms such as GA, since it is population-based (swarm). However, a fundamental\ndifference between these paradigms is the fact that evolutionary algorithms are based on natu-\nral evolution concepts, which embody a competitive philosophy denoted by the idea that only\nthe fittest individuals tends to survive. Conversely, PSO incorporates a cooperative approach to\nsolve a problem, given that all individuals (particles) \u2013 which are allowed to survive \u2013 change\nthemselves over time and one particle\u2019s successful adaptation is shared and reflected in the\nperformance of its neighbors (KENNEDY et al., 2001).\n\nThe basic element of PSO is a particle, which can fly throughout search space towards an\noptimum by using its own information as well as the information provided by other particles\ncomprising its neighborhood. The performance of a particle is determined by its fitness, which\nis assessed by calculating a predefined objective function related to the problem to be solved at\nits current position.\n\nIn PSO, a particle\u2019s neighborhood is the subset of particles with which it is able to commu-\nnicate (BRATTON &amp; KENNEDY, 2007). Depending on how the neighborhood is determined,\nthe PSO algorithm may embody the gbest model, where each particle is connected to every\nother particle in the swarm so as it can obtain information from them. In other words, the\nneighborhood of a particle is the entire swarm. Alternatively, in the lbest model a particle is\nnot able to communicate with all other particles but only with some of them. The simplest\nlbest model, also known as ring model, connects each particle to only two other particles in the\nswarm. It is important to notice that the neighborhood concept gives rise to a communication\n\n32\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nnetwork among particles which does not necessarily depends on distances. Indeed, it is better\nto think about the swarm communication network as a graph where vertices represent particles\nand edges indicate the connections among them without any associated weight. Then, the gbest\nmodel is related to a complete graph in which all vertices (i.e. particles) are connected to each\nother, whereas the ring model forms a cycle with length equal to the number of particles. For\nexample, the left side of Figure 2.9 depicts a cycle of length 12. Other types of swarm com-\nmunication networks are also shown in Figure (2.9). For more on graphs, see Bondy &amp; Murty\n(2008).\n\nlbest ring lbest with four neighbors gbest\n\nFigure 2.9: Different swarm communication networks. Adapted from Bratton &amp; Kennedy (2007)\n\nIf Euclidean distances are used to define particles\u2019 neighbors, the communication networks\u2019\ninherent cycles as depicted on Figure 2.9 are not guaranteed. This may eventually lead to a lack\nof exploration ability, which is not interesting.\n\nAccording to Bratton &amp; Kennedy (2007), the gbest model usually converges more rapidly\nthan the lbest approach. Sometimes this characteristic may be actually a drawback of the for-\nmer model, since it can eventually result in premature convergence of the algorithm to a inferior\nlocal in the case of multi-modal functions. However, in some cases the gbest model can de-\nliver competitive performance even on complex multi-modal problems. Additionally, the gbest\nmodel has the advantage that it often requires less function evaluations, which is very useful\nwhen such assessments are computationally costly. In fact, as the number of particles\u2019 neigh-\nbors increases, one may get a mix of both advantages and shortcomings of lbest and gbest\napproaches (EBERHART &amp; KENNEDY, 1995).\n\nA particle i is formed by three vectors:\n\n\u2022 Its current position in search space xi = (xi1, xi2, . . . , xin).\n\n\u2022 The best individual position it has found so far pi = (pi1, pi2, . . . , pin).\n\n\u2022 Its velocity vi = (vi1, vi2, . . . , vin).\n\nTraditionally the current positions xi and velocities vi are initialized respectively by means\nof a uniform distribution parametrized by the search space and by the maximum velocity vmax.\n\n33\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nThe particles then move throughout the search space by the following set of recursive update\nequations:\n\nvi j(t + 1) = vi j(t) + c1 u1 \u00b7[pi j(t)?xi j(t)] + c2 u2 \u00b7[pg j(t)?xi j(t)], j = 1, 2, . . . , n (2.87)\n\nxi j(t + 1) = xi j(t) + vi j(t + 1), ? j (2.88)\n\nwhere c1 and c2 are constants, u1 and u2 are independent uniform random numbers from the\ninterval [0,1] generated at every update for each individual dimension j = 1, 2, . . . , n and pg(t)\nis the n-dimensional vector formed by the best position encountered so far by any neighbor\nof particle i. Note that velocities and positions at time t + 1 are influenced by the distances\nof the particle\u2019s current position from its own best experience pi(t) and the neighborhood\u2019s\nbest experience pg(t). The second part of (2.87) represents the \u201ccognition\u201d of the particle,\nwhich is its private \u201cthinking\u201d. The last part of (2.87), in turn, is associated with the particle\u2019s\n\u201csocial\u201d ability, which represents the collaboration among particles. Notice also that velocities\nand positions are part of the same equation, even though units of both being different (length\nper time unit and length, respectively). One may interpret the future position as the previous\none plus the velocity multiplied by a time unit. In this way, there is no problem involving units\nin equations (2.87) and (2.88), as it would be thought at first glance.\n\nDuring the iterations, if velocities are not constrained to an upper bound (vmax), the PSO\nalgorithm is prone to enter a state of explosion, since the random weighting of u1 and u2 causes\nvelocities and thus particle\u2019s positions to increase rapidly. In this way, the vmax approach, illus-\ntrated in the following pseudocode, can be used for every dimension j and particle i. According\nto Bratton &amp; Kennedy (2007), however, a single value vmax is not necessarily applicable to all\nsizes of problem spaces and finding its appropriate value is critical to the PSO performance and\nit may be a difficult task.\n\nprocedure CONSTRAINVELOCITY(vmax)\nif vi j(t + 1) > vmax then\n\nvi j(t + 1) = vmax\nelse\n\nif vi j(t + 1) &lt;?vmax then\nvi j(t + 1) = ?vmax\n\nend procedure\n\nAlternatively, the inertia weight w may replace vmax to adjust the influence of previous\nvelocity of the particle during the optimization process and then to balance the trade-off between\nglobal and local search. By adjusting w, the swarm has a tendency to eventually constrict itself\nto the region containing the best fitness and exploit this region (SHI &amp; EBERHART, 1998;\nBRATTON &amp; KENNEDY, 2007). The function for velocity update (2.87) becomes:\n\nvi j(t + 1) = w vi j(t) + c1 u1 \u00b7[pi j(t)?xi j(t)] + c2 u2 \u00b7[pg j(t)?xi j(t)], j = 1, 2, . . . , n (2.89)\n\n34\n\n\n\nChapter 2 THEORETICAL BACKGROUND\n\nIt is also possible to vary the value of w during PSO iteration, which may be greater at the\nbeginning so as to allow for exploration (coverage of the entire search space) and gradually\nsmaller while the algorithm evolves to encourage exploitation (fine adjustments in a specific\narea) on the most promising regions found during exploration. If w is a constant, Shi &amp; Eberhart\n(1998) recommend to pick a value for it from the interval [0.9, 1.2]. On the other hand, if it is\nchanged dynamically, typically it varies from 0.9 to 0.4 throughout PSO iterations (KENNEDY\net al., 2001).\n\nAnother manner to avoid the velocity explosion during PSO iterations is to use a constric-\ntion factor ? multiplying all parts of the velocity update equation:\n\nvi j(t + 1) = ?\n{\n\nvi j(t) + c1 u1 \u00b7[pi j(t)?xi j(t)] + c2 u2 \u00b7[pg j(t)?xi j(t)]\n}\n\n, j = 1, 2, . . . , n\n(2.90)\n\nwhere\n\n? =\n2???2?????2 ?4????, ? = c1 + c2 (2.91)\n\nNotice that ? has no real values when ? &lt;4. The idea of the constriction factor is that the\namplitude of particles\u2019 oscillations decreases as they focus on a previous best point from their\nrespective neighborhoods. In this way, as ? increases, ? decreases and such amplitudes become\neven smaller. However, if a member of a neighborhood finds a better point, the other particles\ncan perfectly explore the new region. Hence, the constriction factor does not forbid particles\nswitching from exploratory to exploitative mode and vice versa (KENNEDY et al., 2001). Brat-\nton &amp; Kennedy (2007) affirms that for simplicity most implementations of constricted PSO set\n? = 4.1, which assures convergence and yields ? ? 7.2984 \u00b710?1, c1 = c2 = 2.05.\n\nAlthough the constriction factor does not require a limit on particles\u2019 velocities, empirical\nexperiments have shown that taking the variables\u2019 ranges as bounds for velocities can provide\nbetter results. Limiting velocities this way does not confine particles to feasible search space,\nbut in general does not allow them to go far beyond the region of interest (KENNEDY et al.,\n2001). Actually, artificially constraining particles\u2019 positions when they reach the boundary of\nsearch space is not recommended since it can affect the performance of PSO. In this way, in-\nfeasible particles may emerge and then the most straightforward method for handling them is to\nleave their velocities and infeasible positions unaltered. Besides that, the fitness evaluation step\nmay be skipped in order to avoid infeasible positions becoming particles\u2019 best and/or neigh-\nborhood best positions. With this procedure, called let particles fly, infeasible particles may be\ndrawn back to feasible search space by the influence of their personal and neighborhood bests.\n\n35\n\n\n\n3 PROPOSED PARTICLE SWARM OPTIMIZATION AND SUP-\nPORT VECTOR MACHINE METHODOLOGY FOR RELIABILITY\nPREDICTION\n\nIn this work, PSO is considered to tackle the model selection problem for SVR tasks. As\nthe Gaussian RBF kernel is broadly used in reliability related SVM works, it is the one taken\ninto account. In this way, the parameters C, ? and the Gaussian RBF width ? must be defined.\nThese three SVR parameters become decision variables to the PSO algorithm and they form\na 3-dimensional search space. In fact, instead of using ?, it is considered ? = 0.5\n\n?2\n, which can\n\nbe noticed on the expression of the RBF kernel in Table 2.1. This is necessary due to LIBSVM\nrequirements, since it works with gamma values and not directly with the width ?. Thus, the ith\n\nparticle is described by the vectors xi = (xi1, xi2, xi3), pi = (pi1, pi2, pi3) and vi = (vi1, vi2, vi3),\nwhere the first, second and third dimensions are respectively related to C, ? and ?.\n\nBratton &amp; Kennedy (2007) contends that a standard PSO algorithm includes a lbest model,\nthe usage of the constriction factor for velocities\u2019 and thus positions\u2019 updates, the number of\nparticles set as 50, a non-uniform swarm initialization and the procedure of skipping fitness\nevaluation when particles exit the feasible search space.\n\nIn this work, similarly to the suggested PSO, it is used the constriction factor, infeasible\nparticles are allowed but their fitness values are not assessed and a lbest model is implemented.\nDifferently from the standard algorithm, the traditional random uniform swarm initialization is\nperformed and the number of particles is set to 30. Indeed, Bratton &amp; Kennedy (2007) states\nthat 20-100 particles had produced quite similar empirical results.\n\nAdditionally, a gbest PSO is also implemented in order to have its performance compared to\nthe lbest one in the specific context of reliability prediction. Also, in the PSO fitness evaluation\nphase, instead of only evaluating an ordinary objective function, the coupling with LIBSVM\ntakes place. Next Section details the steps involved in the implemented PSO combined to\nLIBSVM.\n\n3.1 Steps\n\n3.1.1 Read of Data and Definition of Variables\u2019 Bounds\n\nBefore the initialization of PSO swarm, it is necessary to read the available data of inputs\nand outputs from a file in text format. Such file is organized as follows: the first column is\ncomprised of the output values (y1, y2, . . . , y`+?+?) and the following ones are each filled with\na dimension of the input vector x. This data set, for example, may have been originated from a\ncondition monitoring procedure.\n\nAfter reading all data, the entire set is subdivided in training, validation and test sets, whose\n\n36\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\nsizes (`, ?, ?, respectively) are defined by the user. Usually, the majority of the entries are\nreserved for the training step, and the remaining ones are approximately equally divided to form\nthe validation and test sets. The implemented PSO+SVM entails the validation sets approach\nin which the order of observations are respected, since cross-validation and leave-one-out are\ncomputational costly approaches and problems concerning reliability prediction based on time\nseries are also considered.\n\nThe maximum and minimum values of the variables (xminj , x\nmax\nj , j = 1, 2, 3) define intervals\n\nwith different magnitudes. According to Kecman (2005), the \u201cradius\u201d of the SVR tube ? can be\ndefined as a percentage of the mean of the training outputs (yi, i = 1, 2, . . . , `). Following this\nidea, in this work, ? has its lower and upper bounds defined respectively as 0.001\n\n` ?\n`\ni=1 yi and\n\n0.15\n` ?\n\n`\ni=1 yi. This way of defining the boundary values of ? is adapted to the data under analysis.\n\nHowever, the minimum and maximum values of C and ? are not defined using the available\nobservations and are determined rather in an arbitrary way. As a consequence, their ranges are\ngreater than the one defined for ?.\n\n3.1.2 Particle Swarm Initialization\n\nIn this work, it is implemented the traditional random uniform swarm initialization and the\nparticles\u2019 initial positions are randomly selected from their respective intervals of definition.\nThe positions pi are initially set equal to xi for each particle i.\n\nGiven that the variables\u2019 ranges are very different, the velocities are initialized in a special\nmanner. The maximum velocity of each dimension vmaxj is set to 10% of the range where the\nspecific variable is defined:\n\nvmaxj =\n1\n\n10\n(xmaxj ?x\n\nmin\nj ), j = 1, 2, 3 (3.1)\n\nin which xmaxj and x\nmin\nj are the maximum and minimum values the related variable can assume.\n\nAfter that, velocities are randomly chosen from the interval [?vmaxj , v\nmax\nj ], j = 1, 2, 3, for all\n\nparticles. Only 10% of the range was chosen to initially set small velocities in an attempt to\navoid the exit of particles to infeasible areas in early stages of the PSO algorithm.\n\nThe initialization procedure for an arbitrary particle i is summarized in the following pseu-\ndocode. The notation n indicates the number of dimensions or variables taken into account.\nFor the SVR model selection problem considering RBF kernels, n = 3. The notation RAND (\u00b7),\nin turn, is the function that returns a real number randomly selected in the interval passed as\nargument, according to a uniform distribution.\n\n37\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\nprocedure INITIALIZEPARTICLE (xmin1 , x\nmax\n1 , x\n\nmin\n2 , x\n\nmax\n2 , . . . , x\n\nmin\nn , x\n\nmax\nn )\n\nfor j = 1, 2, . . . , n do\nxi j ? RAND (xminj , x\n\nmax\nj )\n\npi j ? xi j\nvi j ? RAND (?vmaxj , v\n\nmax\nj ) . v\n\nmax\nj defined by Equation (3.1)\n\nend for\nreturn particle i\n\nend procedure\n\n3.1.3 Definition of Particles\u2019 Neighborhoods\n\nThis step is required only when the lbest model is adopted. If the gbest one is considered,\nthe neighborhood is equal to the entire swarm and there is no necessity to explicitly define\nparticles\u2019 neighbors.\n\nIn the lbest approach, particles\u2019 neighbors are arbitrarily defined considering the particles\u2019\ngeneration order and not taking into account any sort of distance metrics. For example, in\nthe case of lbest ring model, particle i has i?1 and i + 1 as neighbors. If i = 1, then the \u201cleft\u201d\nneighbor is the last particle and, conversely, if the last particle is considered, its \u201cright\u201d neighbor\nis the first particle. For the sake of illustration, consider a swarm with 10 particles. Table 3.1\npresents the neighborhood of each one of them when it is formed by 2 or 4 other particles. The\nlist of particles concerns the order of generation in the initialization step.\n\nTable 3.1: Examples of particles\u2019 neighborhoods\n\nParticle 2 neighbors 4 neighbors\n\n1 10, 2 9, 10, 2, 3\n2 1, 3 10, 1, 3, 4\n3 2, 4 1, 2, 4, 5\n4 3, 5 2, 3, 5, 6\n5 4, 6 3, 4, 6, 7\n6 5, 7 4, 5, 7, 8\n7 6, 8 5, 6, 8, 9\n8 7, 9 6, 7, 9, 10\n9 8, 10 7, 8, 10, 1\n10 9, 1 8, 9, 1, 2\n\n3.1.4 Fitness Evaluation: Coupling of Particle Swarm Optimization and Sup-\n\nport Vector Machine\n\nThe objective function denoting the fitness of particles, in this work, is the NRMSE (2.84).\nAt the fitness evaluation step, the coupling between PSO and SVM takes place. The validation\n\n38\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\nsets approach is adopted so as to guide the search of optimal parameter values by PSO. In this\nway, given a specific particle, whose current position (x) defines a set of parameters, C, ?, ?,\nalong with the training and validation sets at hand, LIBSVM is able to perform the SVR. Firstly,\nit solves the training problem taking into account the training set. It then provides the support\nvectors (both bounded and free), their respective Lagrange multipliers values as well as the\nvalue of the linear coefficient b0. With these results it is possible to calculate the regression\nfunction. Secondly, these outcomes are used to feed the prediction portion of LIBSVM. Thus,\nthe trained \u201cmachine\u201d is used to predict the outputs from the input values of the validation set.\nWith the predicted and the available real values it is possible to calculate the validation NRMSE.\n\nAlso, the fitness evaluation phase entails the update of particles\u2019 best positions (p). If\na particle\u2019s current position x results in a smaller validation NRMSE, then its best position\nbecomes x and the calculated fitness value is stored. That is, p is made equal to x and a particle\nfitness is always related to its best position p. Otherwise, nothing changes.\n\nThe fitness evaluation of all particles takes place immediately after the initialization and\ndefinition of neighborhoods (if lbest is adopted) steps so as to start the search procedure by\nPSO. It is also performed after the update of particles\u2019 velocities and positions and is followed\nby the update of the global best particle and the particles\u2019 best neighbors (when lbest model is\nconsidered).\n\n3.1.5 Update of Particles\u2019 Velocities and Positions\n\nSimilarly to the standard algorithm, the developed PSO use the constriction factor in its up-\ndating rules as well as the permission of infeasible particles existence without fitness evaluation\nin such circumstance. In fact, the impacts of the let particles fly strategy could be empirically\nobserved by some experiments performed in early stages of this work. When the particles ex-\nited the feasible search space in terms of one or more variables, their positions were set as\neither the lower or upper bound of the related dimensions, depending on the limit they have\nsurpassed. This procedure negatively influenced the PSO performance, given that the particles\nwere inclined to occupy the boundary regions of the feasible search space, which prejudiced its\nexploration. The implementation of let particles fly gave noticeable improvements to the PSO\nperformance.\n\nThe update of particles\u2019 velocities and positions is accomplished by computing Equations\n(2.90) and (2.88), respectively. Additionally, along with the constriction factor, velocities are\nbounded to the definition ranges of variables so as to avoid particles going to far from the\nfeasible search space. Hence, after initialization step, vmaxj = x\n\nmax\nj ?x\n\nmin\nj , j = 1, 2, 3.\n\n39\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\n3.1.6 Update of Global Best and Particles\u2019 Best Neighbors\n\nThe update of global best and particles\u2019 best neighbors always takes place after the fitness\nevaluation step, given that it is based on particles\u2019 fitness values. The global best updating\nconsists in identifying and storing the particle that have led to the smallest validation NRMSE\noverall particles of the swarm up to the present iteration. Not only the global best particle with\nits associated positions and fitness is stored, but also the number of the iteration in which it was\nfound (bIter). This number may be used in the assessment of the stop criteria.\n\nIf the lbest model is adopted, then it is necessary to verify the best particles\u2019 neighbors\nat each PSO iteration, since they play the role of guiding search along with particles\u2019 best\npositions. In a particle\u2019s neighborhood, including itself, the one with the smallest validation\nNRMSE becomes the best neighbor in the current iteration.\n\n3.1.7 Stop Criteria\n\nThese steps are repeated until a stop criterion is reached. The proposed PSO involves three\nof them:\n\n1. Maximum number of iterations (nIter).\n\n2. The global best particle (and thus the fitness value) is the same in 10% of the maximum\nnumber of iterations.\n\n3. The global best fitness value in consecutive iterations are different but such difference is\nless than a tolerance ?.\n\nThe update of particles\u2019 velocities and positions, the fitness evaluation and the update of\nglobal best and best particles\u2019 neighbors are repeated until one of the considered stop criteria\nis met. As a result, the PSO+SVM procedure provides the \u201cmachine\u201d with the most suitable\nparameter values C, ?, ?. This PSO-optimized SVM is then used to predict the outputs from the\ninput values of the test set so as to have at least an idea of its generalization ability, which is\ngiven by the test NRMSE. Such evaluation, as in the fitness assessment step, is made by means\nof the prediction portion of LIBSVM.\n\nIn order to summarize and to provide the reader with the essence of what has been just\nexplained the PSO+SVM algorithm is given in the forms of pseudocode and flow chart (Figure\n3.1).\n\n3.2 Proposed Methodology Pseudocode and Flow Chart\n\nIn the pseudocode, nPart and nNeigh are respectively the number of particles and of par-\nticles\u2019 neighbors. Besides that, svm is a \u201cmachine\u201d returned from LIBSVM, fi is the the fitness\n\n40\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\nvalue (validation NRMSE) of particle i and ftest is the test NRMSE returned by the best particle\nfound in all PSO iterations. The symbol ? means optimal in relation to the validation NRMSE.\n\nprocedure PARTICLESWARMOPTIMIZATION (n, xmin, xmax, nPart, nNeigh, c1, c2, ?, nIter, ?, D, `, ?, ?)\n. read data from a text file\n. define variables\u2019 bounds\n. particle swarm initialization and first fitness evaluation\nfor i = 1, . . . , nPart do\n\nINITIALIZEPARTICLE(xmin, xmax)\nsvm ? trainLIBSVM (pi, D1, D2, . . . , D`) . train\n(y?1, y?2, . . . , y??) ? predLIBSVM(svm, D`+1, D`+2, . . . , D`+?) . predict validation outputs\nfi ? NRMSE (y?1, y?2, . . . , y??) . particle fitness, Equation (2.84)\n\nend for\n. find best global particle\nf? ? mini ( fi), i = 1, 2, . . . , nPart; b ? i . update best global fitness\np? ? pb . update best global positon\nbIter ? 0 . update best iteration\n. define particles\u2019 neighborhood and best neihgbor\n. perform PSO\nfor k = 1, 2, . . . , nIter do\n\nfor i = 1, 2, . . . , nPart do\nvi ? min(Equation (2.90), xmax ?xmin) . update particle velocity\nxi ? Equation (2.88) . update particle current position\nif xi is feasible then\n\n. evaluate fitness\nsvm ? trainLIBSVM (xi, D1, D2, . . . , D`)\n(y?1, y?2, . . . , y??) ? predLIBSVM(svm, D`+1, D`+2, . . . , D`+?)\nf ? NRMSE (y?1, y?2, . . . , y??)\nif f &lt;fi then\n\nfi ? f . update particle fitness\npi ? xi . update particle best position\n\nend for\n. update best global particle ( f?, p?, bIter)\n. update best particles\u2019 neighbors\n. verify whether stop criterion 2 or 3 is met\nif 0 &lt;| f?k ? f\n\n?\nk?1| &lt;? or (k?bIter = 10%\u00b7nIter and f\n\n?\nk = f\n\n?\nbIter ) then\n\nbreak\nend for\n(y?`+?+1, y?`+?+2, . . . , y?`+?+?) ? predLIBSVM (svm?, D`+?+1, D`+?+2, . . . , D`+?+?) . predict test outputs\nftest ? NRMSE (y?`+?+1, y?`+?+2, . . . , y?`+?+?) . Equation (2.84)\nreturn f?, p?, ftest\n\nend procedure . end of procedure\n\n41\n\n\n\nChapter 3 PROPOSED PSO AND SVM METHODOLOGY FOR RELIABILITY PREDICTION\n\nStart\n\nEnd\n\nRead training,\nvalidation and\n\ntest sets\n\nRandomly initialize\nparticles velocities\n\nand positions\n\nDefine particles\u2019\nneighborhood\n\nUpdate particle\u2019s\nbest position\n\nUpdate particles\u2019\nvelocities and\n\npositions\n\nUpdate global best\nand particles\u2019\nbest neighbors\n\nCalculate NRMSE\n\nFitness evaluation\n\nIs particle\nfeasible?\n\nIs a stop\ncriterion met?\n\nYes\n\nNo i i= +1\n\ni i= +1\n\nYes. Given the best SVM\n\nIf it is test NRMSE\n\nFor each particle\n\ni nPart= 1, 2, ...,\n\nFor each particle\n\ni nPart= 1, 2, ...,\n\nAfter nPart\n\nevaluations\n\nNo\n\nLIBSVM\n\nPSO + SVM\n\nTrain SVM\n\nPredict by SVM\n\nFigure 3.1: Flow chart of the proposed PSO+SVM methodology\n\n42\n\n\n\n4 RELIABILITY PREDICTION BY PARTICLE SWARM OPTIMIZA-\nTION AND SUPPORT VECTOR MACHINES\n\nThis chapter presents three problems from literature related to reliability prediction based\non time series data and one application example with data collected from oil production wells.\nThey are solved by means of the PSO+SVM algorithm described in Chapter 3. As recom-\nmended by Bratton &amp; Kennedy (2007), the lbest model is adopted. The first two literature\nexamples are related to the forecast of failure times of engineered components and the third one\nconcerns the prediction of miles to failure of a car engine. It is worth to forecast these aspects\nassociated with component failures so as to capture non-linear trends they may present and thus\nprovide support to reliability-based maintenance decisions.\n\nXu et al. (2003) contends that in practice the short-term (single-step) forecasts are more\nuseful since they provide timely information for preventive and corrective maintenance plans,\neven though the multi-step predictions may catch some of system dynamics. In this way, single-\nstep-ahead forecasts are taken into account. In addition, one-dimensional input vectors are\nconsidered, that is, p = 1 and xi = xi = yi?1 and consequently the data sets are reduced by one\nentry.\n\nThe last example is related to the prediction of TBF of oil wells by means of different\ncharacteristics of the system, such as the number of installed rods. This example illustrates the\napplication of the proposed methodology in a real situation. Also, it entails numerical as well\nas categorical variables, which may be handled in different manners before they are used by the\nPSO+SVM. In addition, differently from the time series based examples, the last one involves\na multi-dimensional input vector.\n\nGiven that PSO is a stochastic tool, 30 runs of the algorithm are performed in order to ana-\nlyze its behavior. Although the NRMSE was the only error function which guided the search for\nparameters by PSO, the MAPE and the MSE related to particles were also computed. Addition-\nally, in the forthcoming Sections 4.1, 4.2, 4.3 and 4.4, apart from the Tables regarding descrip-\ntive statistics, all other Tables along with the Figures are associated with the lbest PSO+SVM\nrun that resulted in the smallest test NRMSE value. Even though such \u201cmachines\u201d may not\ngive the most suitable validation NRMSE, they show the best generalization performance. For\ncomparison purposes, all examples were also solved by means of a gbest PSO model combined\nwith SVM (Section 4.5.1).\n\nThe PSO algorithm was implemented in MATLAB 7.8 and linked with LIBSVM. All exper-\niments were run in a computer with 2GHz, 2.9Gb of RAM and Linux Ubuntu 9.04 operational\nsystem.\n\n43\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n4.1 Example 1: Failure Times of a Submarine Diesel Engine\n\nIn this first example, the time series regards the times of unscheduled maintenance ac-\ntions for a submarine diesel engine under deterioration process and is extracted from Ascher &amp;\nFeingold (1984). The data is presented in Table 4.1. Since a single-step ahead forecast with\none-dimensional input vectors is performed, the data set is reduced from 71 to 70 in order to\nelaborate a time series in the same reasoning described in Section 2.1.5.3. Then the first 44 data\npoints are used for training, the following 12 for validation and the last 14 for test purposes.\n\nTable 4.1: Engine age (\u00d7 1000 hours) at time of unscheduled maintenance actions. Adapted from\nAscher &amp; Feingold (1984), p. 75\n\nAction Age Action Age Action Age Action Age Action Age\n\n1 1.382 16 17.632 30 21.061 44 21.888 58 23.491\n2 2.990 17 18.122 31 21.309 45 21.930 59 23.526\n3 4.124 18 19.067 32 21.310 46 21.943 60 23.774\n4 6.827 19 19.172 33 21.378 47 21.946 61 23.791\n5 7.472 20 19.299 34 21.391 48 22.181 62 23.822\n6 7.567 21 19.360 35 21.456 49 22.311 63 24.006\n7 8.845 22 19.686 36 21.461 50 22.634 64 24.286\n8 9.450 23 19.940 37 21.603 51 22.635 65 25.000\n9 9.794 24 19.944 38 21.658 52 22.669 66 25.010\n10 10.848 25 20.121 39 21.688 53 22.691 67 25.048\n11 11.993 26 20.132 40 21.750 54 22.846 68 25.268\n12 12.300 27 20.431 41 21.815 55 22.947 69 25.400\n13 15.413 28 20.525 42 21.820 56 23.149 70 25.500\n14 16.497 29 21.057 43 21.822 57 23.305 71 25.518\n15 17.352\n\nThe lbest model involves 4 neighbors and the underlying swarm communication network\ncan be visualized in the middle graph of Figure 2.9. The PSO required parameters are listed in\nTable 4.2 and are also valid for the subsequent examples.\n\nTable 4.2: PSO required parameters\n\nParameter Value\n\nNumber of particles 30\nNumber of neighbors 4\nc1 = c2 2.05\nConstriction factor (?) 7.2984\u00b710?1\nMaximum number of iterations 6000\nMaximum number of iterations\nwith equal best fitness value\n\n600\n\nTolerance (?) 1 \u00b710?12\n\nIn addition, the definition intervals of the PSO variables (C, ?, ?) as well as the initial values\nof vmaxj , j = 1, 2, 3 (i.e. 10% of variables ranges) are shown in Table 4.3. Notice that after the\n\n44\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nswarm initialization, these maximum velocities become equal to the definition range of each\nvariable. Additionally, ? interval is determined as described in Chapter 3, that is, from 0.1% to\n15% of the mean of training outputs.\n\nTable 4.3: PSO variables\u2019 intervals and initial maximum velocities, Example 1\n\nVariable Interval Initial vmaxj\n\nC [1\u00b710?1, 2000] 199.9900\n? [1.7302\u00b710?2, 2.5953] 2.5780\u00b710?1\n? [1\u00b710?6, 50] 4.9999\n\nThe descriptive statistics of the obtained results from the 30 PSO+SVM runs are in Table\n4.4. The parameters related to the machine which provided the smallest test NRMSE are C =\n263.6966, ? = 1.3701 \u00b710?1 and ? = 6.7315\u00b710?3.\n\nTable 4.4: Descriptive statistics of parameters and error functions, stop criteria frequency and\nperformance for 30 PSO+SVM runs, lbest, Example 1\n\nMinimum Maximum Median Mean Std. dev.?\n\nParameters\nC 190.4848 1930.2743 732.5368 822.3439 560.3221\n? 3.4194 \u00b710?2 2.3899 \u00b710?1 1.9672\u00b710?1 1.5857\u00b710?1 6.0396\u00b710?2\n? 4.4980 \u00b710?3 49.9974 6.6326\u00b710?3 1.6729 9.1270\n\nValidation\nNRMSE 4.4060 \u00b710?3 2.9305 \u00b710?1 4.4123\u00b710?3 1.4035\u00b710?2 5.2700\u00b710?2\n\nerror\nMAPE (%) 3.7297 \u00b710?1 26.2757 3.7501\u00b710?1 1.2384 4.7288\nMSE 9.9229 \u00b710?3 43.8973 9.9515\u00b710?3 1.6979 8.6070\n\nTest error\nNRMSE 7.1126 \u00b710?3 3.8265\u00b710?1 7.4624\u00b710?3 2.0307 \u00b710?2 6.8440\u00b710?2\nMAPE (%) 4.7571 \u00b710?1 38.1006 5.2994\u00b710?1 1.8181 6.8533\nMSE 3.0472 \u00b710?2 88.1988 3.3680\u00b710?2 3.4281 17.2899\n\nAbsolute (relative, %) frequency\n\nStop criteria\nMaximum number of iterations (6000) 13 (43.3333)\nEqual best fitness for 600 iterations 13 (43.3333)\nTolerance ? = 1\u00b710?12 4 (13.3334)\n\nMetric value\n\nPerformance\nMean time per run (minutes) 10.8342\nMean number of trainings 116016.1667\nMean number of predictions 116016.1667\n\n?Standard deviation\n\nFigure 4.1 provides a snapshot of the swarm evolution during the optimization process.\nNotice that at the 50th iteration there are particles outside the feasible search space, but the final\nswarm is comprised only by feasible particles. Observe also that given the influence of the\nlbest model and of the validation NRMSE, particles were inclined to form clusters in the search\nspace. Figure 4.2, in turn, shows the NRMSE convergence along PSO. This specific PSO run\nstopped at iteration 703, in which the tolerance ? = 1\u00b710?12 was attained.\n\n45\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n500\n\n1000\n\n1500\n\n2000\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nC\n\nInitial swarm\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n?500\n0\n\n500\n1000\n\n1500\n2000\n\n2500\n3000\n\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\n?60\n\n?40\n\n?20\n\n0\n\n20\n\n40\n\n60\n\n80\n\nC\n\nParticle swarm at iteration 50\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n200\n400\n\n600\n800\n\n1000\n1200\n\n1400\n1600\n\n0.08\n\n0.1\n\n0.12\n\n0.14\n\n0.16\n\n0.18\n6.6\n\n6.7\n\n6.8\n\n6.9\n\n7\n\n7.1\n\n7.2\n\n7.3\n\n7.4\n\n7.5\n\nx 10\n?3\n\nC\n\nParticle swarm at final iteration, 703\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\nFigure 4.1: Swarm evolution during PSO, Example 1\n\n0 1000 2000 3000 4000 5000 6000\n0\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.14\n\nPSO iteration\n\nN\nR\n\nM\nS\n\nE\n\nNRMSE convergence\n\nFigure 4.2: Validation NRMSE convergence, Example 1\n\n46\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.5 presents the real output values as well as validation and prediction results from\nthe selected SVR. Table 4.6, in turn, shows the support vectors, their respective Lagrange mul-\ntipliers values and the classification as free or bounded support vector. Notice that only 22\nfrom the 44 training points were chosen to be support vectors and from these, only 5 are free.\nSubstituting the values from Table 4.6 in equations (2.79) and (2.82), one may obtain the linear\ncoefficient b0 and the regression function f (x), in this order. The real outputs, the validation\nand test prediction values as well as the support vectors are depicted in Figure 4.3.\n\nTable 4.5: Real failure time (engine age) and predictions by SVR, Example 1\n\nValidation Test\n\nAction Real age Predicted age Action Real age Predicted age\n\n46 21.9430 22.0328 58 23.4910 23.4414\n47 21.9460 22.0459 59 23.5260 23.6327\n48 22.1810 22.0489 60 23.7740 23.6686\n49 22.3110 22.2870 61 23.7910 23.9224\n50 22.6340 22.4195 62 23.8220 23.9398\n51 22.6350 22.7503 63 24.0060 23.9714\n52 22.6690 22.7513 64 24.2860 24.1582\n53 22.6910 22.7863 65 25.0000 24.4398\n54 22.8460 22.8089 66 25.0100 25.1356\n55 22.9470 22.9684 67 25.0480 25.1450\n56 23.1490 23.0725 68 25.2680 25.1809\n57 23.3050 23.2807 69 25.4000 25.3857\n\n70 25.5000 25.5062\n71 25.5180 25.5962\n\nNRMSE 4.4142\u00b710?3 NRMSE 7.1126 \u00b710?3\nMAPE(%) 3.7486 \u00b710?1 MAPE (%) 4.7767\u00b710?1\nMSE 9.9597\u00b710?3 MSE 3.0472 \u00b710?2\n\nTable 4.6: Support vectors\u2019 details, Example 1\n\n(x, y) ? ?? Type (x, y) ? ?? Type\n\n(1.382, 2.990) 0 135.2668 free (16.497, 17.352) 122.0212 0 free\n(4.124, 6.827) 263.6966 0 bounded (17.352, 17.632) 0 263.6966 bounded\n(7.472, 7.567) 0 263.6966 bounded (18.122, 19.067) 263.6966 0 bounded\n(7.567 , 8.845) 263.6966 0 bounded (19.067, 19.172) 0 263.6966 bounded\n(8.845 , 9.450) 0 263.6966 bounded (19.172, 19.299) 0 44.2857 free\n(9.450 , 9.794) 0 263.6966 bounded (19.299, 19.360) 0 263.6966 bounded\n(9.794 , 10.848) 263.6966 0 bounded (19.940, 19.944) 0 263.6966 bounded\n(10.848, 11.993) 57.7336 0 free (20.121, 20.132) 0 263.6966 bounded\n(11.993, 12.300) 0 263.6966 bounded (20.132, 20.431) 263.4942 0 free\n(12.300, 15.413) 263.6966 0 bounded (20.525, 21.057) 263.6966 0 bounded\n(15.413, 16.497) 263.6966 0 bounded (21.061, 21.309) 263.6966 0 bounded\n\nIndeed, Hong &amp; Pai (2006) solve this same example application by means of SVM coupled\nwith a method to find the parameters C, ?, ? as well as with other forecast tools, namely the\n\n47\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n0 10 20 30 40 50 60 70\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nExample number\n\nO\nu\n\ntp\nu\n\nt \n(y\n\n)\n\nSupport vector regression results\n\n \n\n \n\nTraining data\nSupport vectors\nValidation data\nValidation SVR\nTest data\nTest SVR\n\nFigure 4.3: SVR results, Example 1\n\nDuane model, the ARIMA and the General Regression Neural Network (GRNN). The search\nmethod for SVR parameters entails the following idea: (i) fix the values of two parameters (C, ?)\nand find the optimal value of the remaining one (?0); (ii) given ?0 and ?, obtain the optimal value\nC0; (iii) find ?0 based on ?0 and C0. This procedure is guided by the evaluation of NRMSE and\nthe authors consider 45, 12 and 14 points for training, validation and test, respectively.\n\nNevertheless, the optimal parameters\u2019 values presented along with the information provided\nare not sufficient to reproduce the NRMSE value of 6.4500\u00b710?3 reported. This data set with the\nmentioned division as well as the presented optimal parameters were used to train and predict\nthe validation and test outputs by means of LIBSVM, but the NRMSE found was much greater\nthan 6.4500\u00b710?3 and also the tendency of both validation and test predictions was the opposite\nof the real one. Despite that, the test NRMSE results from Hong &amp; Pai (2006) are presented\nin Table 4.7 with the additional entry corresponding to the best test NRMSE obtained by the\nPSO+SVM approach from this work. It can be observed that it is competitive with all other\nvalues provided by the different tools.\n\nTable 4.7: Test NRMSE from different forecast models, Example 1. Adapted from Hong &amp; Pai\n(2006), p. 160\n\nMethod Test NRMSE\n\nPSO+SVM 7.1126 \u00b710?3\nSVM 6.4500\u00b710?3\nDuane 1.0590 \u00b710?2\nGRNN 9.7300\u00b710?3\nARIMA 3.3660 \u00b710?2\n\n48\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n4.2 Example 2: Turbochargers in Diesel Engines\n\nThe second application example is extracted from Xu et al. (2003) and is related to tur-\nbochargers in diesel engines. As stated by the authors, the reliability is the one of the most\nimportant considerations for diesel engine systems. In this way, an accurate prediction of its re-\nliability provides a good assessment of its performance. Table 4.8 presents the failure times of\n40 turbochargers of the same type as well as the non-parametric estimation of their reliabilities\ncalculated by:\n\nR(Ti) = 1?\ni?0.3\nn + 0.4\n\n(4.1)\n\nwhere i is the failure index and n is the data sample size.\n\nTable 4.8: Turbochargers failure times (\u00d7 1000 hours) and reliability data. Adapted from Xu et al.\n(2003), p. 259\n\ni Ti R(Ti) i Ti R(Ti) i Ti R(Ti) i Ti R(Ti)\n\n1 1.6 0.9930 11 5.1 0.8934 21 6.5 0.7938 31 7.9 0.6942\n2 2.0 0.9831 12 5.3 0.8835 22 6.7 0.7839 32 8.0 0.6843\n3 2.6 0.9731 13 5.4 0.8735 23 7.0 0.7739 33 8.1 0.6743\n4 3.0 0.9631 14 5.6 0.8635 24 7.1 0.7639 34 8.3 0.6643\n5 3.5 0.9532 15 5.8 0.8536 25 7.3 0.7540 35 8.4 0.6544\n6 3.9 0.9432 16 6.0 0.8436 26 7.3 0.7440 36 8.4 0.6444\n7 4.5 0.9333 17 6.0 0.8337 27 7.3 0.7341 37 8.5 0.6345\n8 4.6 0.9233 18 6.1 0.8237 28 7.7 0.7241 38 8.7 0.6245\n9 4.8 0.9133 19 6.3 0.8137 29 7.7 0.7141 39 8.8 0.6145\n10 5.0 0.9034 20 6.5 0.8038 30 7.8 0.7042 40 9.0 0.6046\n\nXu et al. (2003) made two experiments with these data, both regarding reliability forecasts.\nIn a first situation they considered previous reliability values and also the failure times as input\ndata. The second experiment, in turn, had their inputs comprised only by past reliability values.\nThe authors used several sorts of NN as forecast tool and observed that better results were\nobtained in the latter experiment. Additionally, they compared various types of NN (Multilayer\nPerceptron Neural Network (MLP-NN) with logistic and Gaussian activations and Radial Basis\nFunction Neural Network (RBF-NN) with Gaussian activation) and ARIMA performances.\n\nAlso, Chen (2007) makes use of the same data set to train a SVM and then predict reliability\nusing it. The authors consider 4-dimensional input vectors x (i.e. p = 4) and the SVR parameters\nare obtained by GA. Also, despite the time series characteristics, they adopt a cross-validation\ntechnique embedded in GA so as to guide the search for optimal C, ?, ?. Once this set is found,\nthey retrain all training data and then perform prediction tasks on the test set. The author\ncompared GA+SVM results with the ones obtained by GRNN, MLP-NN, RBF-NN, Neural\nFuzzy Network (NFN) and ARIMA.\n\nIn this work, besides the turbocharger reliability prediction it is also performed the forecast\nof its failure times. The input vectors for the reliability experiment are made only by a single\n\n49\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\npast reliability value and do not consider failure times at all. For failure times case, similarly,\ninput vectors are one-dimensional and formed by the immediately previous failure time.\n\n4.2.1 Example 2.1: Reliability Forecast\n\nActually, this example is the simplest among the ones presented in this work, given that the\nreliability values does not depend on previous values of reliability or on failure times, but only\non the failure index (see Equation (4.1)). In this way, the time series pairs (yi?1, yi), i = 1, 2,\u00b7\u00b7\u00b7n\nform a straight line. One may question why such an \u201ceasy\u201d problem may need sophisticated\ntools like NN, ARIMA and SVM to be solved. However, this particular example is widely\nused in literature works, as can be inferred from the comments in the beginning of this Section,\nmaybe to show the effectiveness of these methods in also tackling simple problems.\n\nFor this case the first 30 points are used for SVR training, the following 4 entries guide the\nsearch for optimal parameters C, ? and ? by PSO and the last 5 examples form the test set. The\nonly difference in parameters\u2019 intervals shown in Table 4.3 regards the lower and upper bounds\nof ?, whose definition depends on the data at hand. For the reliability prediction case, they are\nrespectively 8.5358\u00b710?4 and 1.2804\u00b710?1.\n\nThe descriptive statistics of the model selection results for the 30 PSO+SVM runs are pre-\nsented in Table 4.9. Observe that due to problem simplicity, standard deviations of all param-\neters and errors were small if compared to the ones from the other examples. Additionally,\nnone of the runs made use of the maximum number of iterations as stop criterion and the mean\nelapsed time was very low, only about one minute per run.\n\nThe parameter values associated with the PSO+SVM run that resulted in the smallest test\nNRMSE are C = 1923.6203, ? = 8.3873\u00b710?4, ? = 8.9796\u00b710?4. Figure 4.4 depicts the particle\nswarm in the initial, 50th and 1822th (final) iterations. At this particular run, the best global val-\nidation NRMSE was found in iteration 1222 and remained the same for the next 600 iterations.\nThe validation NRMSE evolution can be seen in Figure 4.5.\n\nThe real and predicted values for validation and test outputs were very near from each\nother and are listed in Table 4.10. The support vectors features are shown in Table 4.11, from\nwhich can be noted that only two bounded support vectors were required by the SVR model.\nFigure 4.6 presents the two support vectors in addition to the validation and reliability forecasts.\nInterestingly, the chosen support vectors were the first and last training points.\n\nXu et al. (2003) and Chen (2007) present the test NRMSE values for this example from\ndifferent time series models. In order to update the list of such values with the results of the\nproposed methodology in this work, Table 4.12 is provided. Notice that among all methods,\nPSO+SVM was able to give the smallest test NRMSE. This fact indicates the ability of PSO in\nhandling the model selection problem related to SVR as well as the great capacity of SVR itself\nto tackle reliability forecast problems. It is important to emphasize that this better result was\nattained even with a smaller training set size. Xu et al. (2003) mention only the training and\n\n50\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.9: Descriptive statistics of parameters and error functions, stop criteria frequency and\nperformance for 30 PSO+SVM runs, lbest, Example 2.1\n\nMinimum Maximum Median Mean Std. dev.?\n\nParameters\nC 1900.9063 1946.8917 1917.5483 1919.3760 10.8901\n? 8.3864 \u00b710?4 8.4078 \u00b710?4 8.3869\u00b710?4 8.3906\u00b710?4 6.7860\u00b710?7\n? 8.8721 \u00b710?4 9.0869 \u00b710?4 9.0078\u00b710?4 9.0041\u00b710?4 4.6488\u00b710?6\n\nValidation\nNRMSE 8.4451 \u00b710?5 4.1839\u00b710?4 8.4487 \u00b710?5 1.0678 \u00b710?4 8.4700\u00b710?5\n\nerror\nMAPE (%) 7.4023 \u00b710?3 4.1137\u00b710?2 7.4037\u00b710?3 9.6523\u00b710?3 8.5579 \u00b710?3\nMSE 3.1960 \u00b710?9 7.8443\u00b710?8 3.1987\u00b710?9 8.2172\u00b710?9 1.9088\u00b710?8\n\nTest error\nNRMSE 2.0304\u00b710?4 5.8311\u00b710?4 2.0444\u00b710?4 2.2999\u00b710?4 9.5987\u00b710?5\nMAPE (%) 1.8678 \u00b710?2 5.7884\u00b710?2 1.8831 \u00b710?2 2.1465\u00b710?2 9.8995\u00b710?3\nMSE 1.6087 \u00b710?8 1.3267\u00b710?7 1.6308\u00b710?8 2.4114\u00b710?8 2.9507 \u00b710?8\n\nAbsolute (relative, %) frequency\n\nStop criteria\nMaximum number of iterations (6000) 0 (0)\nEqual best fitness for 600 iterations 25 (83.3333)\nTolerance ? = 1\u00b710?12 5 (16.6667)\n\nMetric value\n\nPerformance\nMean time per run (minutes) 1.0066\nMean number of trainings 67989.7000\nMean number of predictions 67989.7000\n\n?Standard deviation\n\nTable 4.10: Real failure time and predictions by SVR, Example 2.1\n\nValidation Test\n\ni R(Ti) Predicted R(Ti) i R(Ti) Predicted R(Ti)\n\n32 0.6843 0.6842 36 0.6444 0.6445\n33 0.6743 0.6743 37 0.6345 0.6345\n34 0.6643 0.6644 38 0.6245 0.6247\n35 0.6544 0.6544 39 0.6145 0.6147\n\n40 0.6046 0.6047\n\nNRMSE 8.4525\u00b710?5 NRMSE 2.0305\u00b710?4\nMAPE (%) 7.4037 \u00b710?3 MAPE (%) 1.8678 \u00b710?2\nMSE 3.2016\u00b710?9 MSE 1.6087\u00b710?8\n\nTable 4.11: Support vectors\u2019 details, Example 2.1\n\n(x, y) ? ?? Type\n\n(0.9930, 0.9831) 1923.6203 0 bounded\n(0.7042, 0.6942) 0 1923.6203 bounded\n\nvalidation sets. Thus, one may infer that the validation set actually played the role of the test\nset. Chen (2007), in turn, makes use of the cross-validation approach and after parameters have\nbeen found, all training set was retrained by SVR. Hence, in both cases, the training sets were\n\n51\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n500\n\n1000\n\n1500\n\n2000\n\n0.02\n0.04\n\n0.06\n0.08\n\n0.1\n0.12\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nC\n\nInitial swarm\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n?500\n0\n\n500\n1000\n\n1500\n2000\n\n2500\n\n?0.03\n\n?0.02\n\n?0.01\n\n0\n\n0.01\n?20\n\n?10\n\n0\n\n10\n\n20\n\n30\n\n40\n\nC\n\nParticle swarm at iteration 50\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n1900\n1920\n\n1940\n1960\n\n1980\n2000\n\n8.25\n\n8.3\n\n8.35\n\n8.4\n\n8.45\n\nx 10\n?4\n\n8.7\n\n8.75\n\n8.8\n\n8.85\n\n8.9\n\n8.95\n\n9\n\nx 10\n?4\n\nC\n\nParticle swarm at final iteration, 1822\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\nFigure 4.4: Swarm evolution during PSO, Example 2.1\n\n0 1000 2000 3000 4000 5000 6000\n0\n\n0.005\n\n0.01\n\n0.015\n\n0.02\n\n0.025\n\n0.03\n\n0.035\n\nPSO iteration\n\nN\nR\n\nM\nS\n\nE\n\nNRMSE convergence\n\nFigure 4.5: Validation NRMSE convergence, Example 2.1\n\nindeed greater than the one used in this work.\n\n52\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n0 5 10 15 20 25 30 35 40\n\n0.65\n\n0.7\n\n0.75\n\n0.8\n\n0.85\n\n0.9\n\n0.95\n\n1\n\nExample number\n\nO\nu\ntp\n\nu\nt \n(y\n\n)\n\nSupport vector regression results\n\n \n\n \nTraining data\nSupport vectors\nValidation data\nValidation SVR\nTest data\nTest SVR\n\nFigure 4.6: SVR results, Example 2.1\n\nTable 4.12: Test NRMSE from different forecast models, Example 2.1. Updated from Xu et al.\n(2003), p. 260, and Chen (2007), p.430\n\nMethod Test NRMSE\n\nPSO+SVM 2.0305\u00b710?4\nGA+SVM 4.0000\u00b710?4\nNFN 3.6900\u00b710?3\nRBF-NN (Gaussian activation) 3.9100\u00b710?3\nMLP-NN (Gaussian activation) 2.4970\u00b710?2\nMLP-NN (logistic activation) 3.9700\u00b710?2\nGRNN 1.0850\u00b710?2\nARIMA 1.9900\u00b710?2\n\n4.2.2 Example 2.2: Failure Times Forecast\n\nDifferently from the reliability prediction problem, in the failure times forecast the data set\nis divided approximately with the same proportions of the first example, that is, 63%, 17% and\n20% from the data dedicated to training, validation and test purposes, in this order. This yields\nrespectively 24, 7 and 8 points. Also, ? ? [5.2750\u00b710?3, 7.9125 \u00b710?1].\n\nDescriptive statistics of parameters and error functions as well as some performance metrics\nfor lbest model are listed on Table 4.13. Observe that in the majority of the runs, the best global\nfitness value remained the same in 600 consecutive iterations and the maximum number of\niterations was not attained in none of them. This indicates that the PSO is able to find good\nsolutions without requiring the maximum number of iterations, which positively influence the\nalgorithm elapsed time.\n\nThe parameter values associated with the \u201cmachine\u201d which provided the smallest lbest test\n\n53\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.13: Descriptive statistics of parameters and error functions, stop criteria frequency and\nperformance for 30 PSO+SVM runs, lbest, Example 2.2\n\nMinimum Maximum Median Mean Std. dev.?\n\nParameters\nC 34.3824 1997.3652 1650.7974 1276.5337 778.9501\n? 1.1999 \u00b710?2 1.5487 \u00b710?1 5.2240\u00b710?2 8.8812\u00b710?2 6.3591\u00b710?2\n? 4.6374 \u00b710?3 5.2242 \u00b710?1 1.9170\u00b710?2 1.5319\u00b710?1 2.2412\u00b710?2\n\nValidation\nNRMSE 1.6620 \u00b710?2 1.6862\u00b710?2 1.6823 \u00b710?2 1.6777 \u00b710?2 9.3827\u00b710?5\n\nerror\nMAPE (%) 1.2262 1.3132 1.2422 1.2539 2.8847\u00b710?2\nMSE 1.6274 \u00b710?2 1.6751\u00b710?2 1.6674\u00b710?2 1.6583\u00b710?2 1.8497\u00b710?4\n\nTest error\nNRMSE 1.3412 \u00b710?2 7.1785\u00b710?2 1.7837\u00b710?2 3.1674\u00b710?2 2.4001\u00b710?2\nMAPE (%) 1.1299 5.0759 1.4722 2.4071 1.6025\nMSE 1.3087 \u00b710?2 3.7489\u00b710?1 2.3149 \u00b710?2 1.1353\u00b710?1 1.5178\u00b710?1\n\nAbsolute (relative, %) frequency\n\nStop criteria\nMaximum number of iterations (6000) 0 (0)\nEqual best fitness for 600 iterations 29 (96.6667)\nTolerance ? = 1\u00b710?12 1 (3.3333)\n\nMetric value\n\nPerformance\nMean time per run (minutes) 10.5648\nMean number of trainings 68977.6667\nMean number of predictions 68977.6667\n\n?Standard deviation\n\nNRMSE are C = 1936.7744, ? = 1.5474\u00b710?1, ? = 4.6374\u00b710?3 and that specific run stopped at\niteration 2836 since the global best particle reached the associated best position 600 iterations\nearlier. Figure 4.7 show the evolution of the particle swarm in three different phases of the\nalgorithm and Figure 4.8 depicts the NRMSE values versus the PSO iterations. Tables 4.14 and\n4.15 present, respectively, the real and predicted failure times for validation and test sets and\nthe support vectors\u2019 details. Notice that for this example the test errors are smaller than the\nvalidation ones and that only 6 from the 24 training examples are support vectors (50% of them\nare free). Lastly, the SVR results are summarized in Figure 4.9.\n\n4.3 Example 3: Miles to Failure of a Car Engine\n\nThis example is associated with the prediction of Miles To Failure (MTF) of a car engine\nand it also comes from Xu et al. (2003), who collected data from 100 units of a specific car\nengine (Table 4.16).\n\nThe objective is to predict future MTF of car engines based on past failure evidence. Once\nmore, it is performed a single-step-ahead forecast with one-dimensional input vectors, which re-\nsults in a data set with 99 entries. From these 80, 9 and 10 points are used for training, validation\nand test purposes, respecting their natural order. In this example ? ? [3.7590 \u00b710?2, 5.6385].\n\nTable 4.17 presents the descriptive statistics related to parameters and error functions in 30\nPSO+SVM runs. Only in this example the mean number of predictions were different from\n\n54\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n500\n\n1000\n\n1500\n\n2000\n\n0.1\n0.2\n\n0.3\n0.4\n\n0.5\n0.6\n\n0.7\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nC\n\nInitial swarm\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n?1500\n?1000\n\n?500\n0\n\n500\n1000\n\n1500\n2000\n\n?0.4\n\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n?2\n\n0\n\n2\n\n4\n\n6\n\n8\n\nC\n\nParticle swarm at iteration 50\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n1650\n1700\n\n1750\n1800\n\n1850\n1900\n\n1950\n2000\n\n?0.4\n\n?0.2\n\n0\n\n0.2\n\n0.4\n?0.02\n\n?0.01\n\n0\n\n0.01\n\n0.02\n\n0.03\n\n0.04\n\n0.05\n\n0.06\n\nC\n\nParticle swarm at final iteration, 2836\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\nFigure 4.7: Swarm evolution during PSO, Example 2.2\n\n0 1000 2000 3000 4000 5000 6000\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\nPSO iteration\n\nN\nR\n\nM\nS\n\nE\n\nNRMSE convergence\n\nFigure 4.8: Validation NRMSE convergence, Example 2.2\n\nthe number of trainings. This is possible because, for some values of ?, all points may lay\n\n55\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.14: Real failure time and predictions by SVR, Example 2.2\n\nValidation Test\n\ni Ti Predicted Ti i Ti Predicted Ti\n\n26 7.3 7.4145 33 8.1 8.0696\n27 7.3 7.4145 34 8.3 8.1622\n28 7.7 7.4145 35 8.4 8.3464\n29 7.7 7.7902 36 8.4 8.4381\n30 7.8 7.7902 37 8.5 8.4381\n31 7.9 7.8836 38 8.7 8.5294\n32 8.0 7.9767 39 8.8 8.7109\n\n40 9.0 8.8011\n\nNRMSE 1.6827\u00b710?2 NRMSE 1.3412\u00b710?2\nMAPE (%) 1.2344 MAPE (%) 1.1299\nMSE 1.6682 \u00b710?2 MSE 1.3087\u00b710?2\n\nTable 4.15: Support vectors\u2019 details, Example 2.2\n\n(x, y) ? ?? Type\n\n(1.6, 2.0) 0 390.8909 free\n(3.9, 4.5) 1936.7744 0 bounded\n(4.5, 4.6) 0 975.3477 free\n(6.0, 6.0) 0 1936.7744 bounded\n(6.5, 6.5) 0 570.5358 free\n(6.7, 7.0) 1936.7744 0 bounded\n\n0 5 10 15 20 25 30 35 40\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nExample number\n\nO\nu\ntp\n\nu\nt \n(y\n\n)\n\nSupport vector regression results\n\n \n\n \n\nTraining data\nSupport vectors\nValidation data\nValidation SVR\nTest data\nTest SVR\n\nFigure 4.9: SVR results, Example 2.2\n\n56\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.16: Miles to failure (\u00d7 1000 hours) of a car engine. Adapted from Xu et al. (2003), p. 262-263\n\nNumber MTF Number MTF Number MTF Number MTF\n\n1 37.1429 26 35.8095 51 37.1429 76 36.3810\n2 37.4286 27 36.9524 52 37.8095 77 38.0000\n3 37.6190 28 37.6190 53 38.0952 78 38.1905\n4 38.5714 29 37.8095 54 38.6667 79 38.6667\n5 40.0000 30 38.0952 55 40.0619 80 38.6667\n6 35.8095 31 36.8571 56 36.1905 81 37.1429\n7 36.2857 32 38.0952 57 36.3810 82 37.6190\n8 36.2857 33 38.0952 58 37.0476 83 37.6190\n9 36.4762 34 38.3810 59 37.2381 84 38.0952\n10 38.1905 35 39.0476 60 38.0000 85 39.0476\n11 36.1905 36 37.2381 61 35.7143 86 36.2857\n12 36.8571 37 37.3333 62 36.4762 87 37.1429\n13 37.6190 38 37.5238 63 37.3333 88 37.5238\n14 37.8095 39 37.8095 64 37.6190 89 37.8095\n15 38.7619 40 38.5714 65 38.4762 90 38.0000\n16 35.9048 41 37.1429 66 36.8571 91 36.8571\n17 36.4762 42 37.2381 67 37.1429 92 37.0476\n18 36.8571 43 37.6190 68 37.9048 93 37.9048\n19 37.1429 44 38.1905 69 38.0952 94 38.1905\n20 37.4286 45 38.5714 70 38.8571 95 39.5238\n21 37.4286 46 36.0952 71 37.1429 96 35.4286\n22 37.6190 47 37.2381 72 37.6190 97 36.0000\n23 38.3810 48 37.4286 73 37.6190 98 37.7143\n24 38.5714 49 37.5238 74 37.8095 99 38.0952\n25 39.4286 50 39.0476 75 38.3810 100 38.5714\n\nwithin the ?-tube, resulting in a model without support vectors. In this way, the first part of\nthe regression function vanish. Then, when LIBSVM predicts based on a \u201cmachine\u201d with these\nfeatures, it returns only a constant value equal to b0. This situation is not desirable, thus when\nthe outcome of a training is an SVM without support vectors, LIBSVM is not allowed to predict\nand the fitness values associated with the particle under consideration remains unaltered.\n\nThe run which resulted in the smallest test NRMSE is related to the following parameter\nvalues: C = 18.2629, ? = 7.9411\u00b710?2 and ? = 6.8979\u00b710?1. In this particular run, PSO+SVM\nsteps continued up to the 4225th iteration and, like all other runs, stopped after 600 iterations\nwith the same best global validation NRMSE.\n\nFigure 4.10 depicts the particle swarm in three different moments. The final swarm, as in\nExample 1, forms some clusters. However it can be noticed from the axes ranges that the pa-\nrameter values are rather concentrated in those ranges if compared with their respective original\nintervals. The validation NRMSE convergence can be visualized in Figure 4.11.\n\nTable 4.18 shows the real and predicted values by the SVM which provided the smallest\ntest NRMSE. Table 4.19, in turn, lists the support vectors, the associated Lagrange multipliers\nand also their classification as free or bounded. From the 80 training points, 68 were selected\nas support vectors and only 5 of them were free. This high number of support vectors can be\n\n57\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.17: Descriptive statistics of parameters and error functions, stop criteria frequency and\nperformance for 30 PSO+SVM runs, lbest, Example 3\n\nMinimum Maximum Median Mean Std. dev.?\n\nParameters\nC 16.3248 30.1075 21.7998 22.5316 5.4798\n? 7.9411 \u00b710?2 2.3345 \u00b710?1 1.6092\u00b710?1 1.5861\u00b710?1 7.6099\u00b710?2\n? 4.9354 \u00b710?1 7.2213 \u00b710?1 5.9863\u00b710?1 6.0241\u00b710?1 1.0515\u00b710?1\n\nValidation\nNRMSE 8.5041 \u00b710?3 9.1959\u00b710?3 8.8512 \u00b710?3 8.8506 \u00b710?3 3.5098\u00b710?4\n\nerror\nMAPE (%) 6.2633 \u00b710?1 7.0907\u00b710?1 6.7005\u00b710?1 6.6897\u00b710?1 4.0743 \u00b710?2\nMSE 1.0273 \u00b710?1 1.2012\u00b710?1 1.1145\u00b710?1 1.1144\u00b710?1 8.8249\u00b710?3\n\nTest error\nNRMSE 1.8969\u00b710?2 1.9468\u00b710?2 1.9265\u00b710?2 1.9242\u00b710?2 2.2394\u00b710?4\nMAPE (%) 1.4338 1.4978 1.4697 1.4679 2.9455\u00b710?2\nMSE 5.0739 \u00b710?1 5.3441\u00b710?1 5.2338 \u00b710?1 5.2217\u00b710?1 1.2150\u00b710?2\n\nAbsolute (relative, %) frequency\n\nStop criteria\nMaximum number of iterations (6000) 0 (0)\nEqual best fitness for 600 iterations 30 (100)\nTolerance ? = 1\u00b710?12 0 (0)\n\nMetric value\n\nPerformance\nMean time per run (minutes) 4.9337\nMean number of trainings 66653.6333\nMean number of predictions 66563.8000\n\n?Standard deviation\n\njustified by the small ? (near 0.2% of the mean of output training values), which results in a thin\n?-tube and also by the complex behavior of the time series, as can be visualized in Figure 4.12.\n\nTable 4.18: Real MTF and predictions by SVR (\u00d7 1000 hours), Example 3\n\nValidation Test\n\nNumber MTF Predicted MTF Number MTF Predicted MTF\n\n82 37.6190 37.5078 91 36.8570 38.3673\n83 37.6190 37.8891 92 37.0480 37.4383\n84 38.0960 37.8891 93 37.9050 37.4813\n85 39.0470 38.4474 94 38.1900 38.2621\n86 36.2860 36.9183 95 39.5240 38.4914\n87 37.1430 37.0125 96 35.4290 35.7357\n88 37.5240 37.5082 97 36.0000 36.3744\n89 37.8090 37.7766 98 37.7140 36.6496\n90 38.0000 38.1398 99 38.0950 38.0127\n\n100 38.5714 38.4468\n\nNRMSE 8.5074\u00b710?3 NRMSE 1.8969\u00b710?2\nMAPE (%) 6.3127 \u00b710?1 MAPE (%) 1.4338\nMSE 1.0281\u00b710?1 MSE 5.0739\u00b710?1\n\nZio et al. (2008) applied an Infinite Impulse Response Locally Recurrent Neural Network\n(IIR-LRNN) to solve this example and updated the results presented in Xu et al. (2003). These\nresults are repeated in Table 4.20 along with the best test NRMSE provided by the PSO+SVM\n\n58\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n500\n\n1000\n\n1500\n\n2000\n\n1\n\n2\n\n3\n\n4\n\n5\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nC\n\nInitial swarm\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n0\n500\n\n1000\n1500\n\n2000\n2500\n\n?1\n\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\n?50\n\n0\n\n50\n\nC\n\nParticle swarm at iteration 50\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n18.27\n18.28\n\n18.29\n18.3\n\n18.31\n18.32\n\n18.33\n18.34\n\n0.0791\n\n0.0792\n\n0.0793\n\n0.0794\n\n0.0795\n0.6885\n\n0.689\n\n0.6895\n\n0.69\n\n0.6905\n\n0.691\n\nC\n\nParticle swarm at final iteration, 4225\n\nepsilon\n\ng\na\nm\n\nm\na\n\nFigure 4.10: Swarm evolution during PSO, Example 3\n\n0 1000 2000 3000 4000 5000 6000\n0.008\n\n0.009\n\n0.01\n\n0.011\n\n0.012\n\n0.013\n\n0.014\n\nPSO iteration\n\nN\nR\n\nM\nS\n\nE\n\nNRMSE convergence\n\nFigure 4.11: Validation NRMSE convergence, Example 3\n\nmethodology. The latter value occupies the third position in the rank. One possible reason for\nthat is the smaller number of training points, due to the validation phase adopted. In order to\n\n59\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.19: Support vectors\u2019 details, Example 3\n\n(x, y) ? ?? Type (x, y) ? ?? Type\n\n(37.1429, 37.4286) 0 7.3029 free (37.1420, 37.2390) 8.9927 0 free\n(37.4286, 37.6190) 18.2629 0 bounded (37.6190, 38.1900) 0 18.2629 bounded\n(37.6190, 38.5714) 18.2629 0 bounded (38.1900, 38.5710) 18.2629 0 bounded\n(38.5714, 40.0000) 0 15.1115 free (38.5710, 36.0960) 0 18.2629 bounded\n(40.0000, 35.8095) 0 18.2629 bounded (36.0960, 37.2380) 0 18.2629 bounded\n(35.8095, 36.2857) 0 18.2629 bounded (37.2380, 37.4280) 18.2629 0 bounded\n(36.2857, 36.2857) 0 18.2629 bounded (37.4280, 37.5240) 18.2629 0 bounded\n(36.2857, 36.4762) 18.2629 0 bounded (39.0480, 37.1430) 18.2629 0 bounded\n(36.4762, 38.1905) 0 18.2629 bounded (37.1430, 37.8090) 18.2629 0 bounded\n(38.1905, 36.1905) 18.2629 0 bounded (38.0950, 38.6670) 18.2629 0 bounded\n(36.8571, 37.6190) 0 18.2629 bounded (38.6670, 40.0620) 18.2629 0 bounded\n(37.6190, 37.8095) 18.2629 0 bounded (40.0620, 36.1900) 0 18.2629 bounded\n(37.8095, 38.7619) 0 18.2629 bounded (36.1900, 36.3810) 0 18.2629 bounded\n(38.7619, 35.9048) 0 18.2629 bounded (36.3810, 37.0480) 18.2629 0 bounded\n(36.4762, 36.8571) 0 18.2629 bounded (37.0480, 37.2380) 0 18.2629 bounded\n(36.8571, 37.1429) 0 18.2629 bounded (37.2380, 38.0000) 1.0729 0 free\n(37.6190, 38.3810) 18.2629 0 bounded (38.0000, 35.7140) 18.2629 0 bounded\n(38.3810, 38.5714) 18.2629 0 bounded (35.7140, 36.4770) 18.2629 0 bounded\n(38.5714, 39.4286) 18.2629 0 bounded (36.4770, 37.3330) 0 18.2629 bounded\n(39.4286, 35.8095) 0 18.2629 bounded (37.6190, 38.4760) 0 18.2629 bounded\n(35.8095, 36.9524) 18.2629 0 bounded (38.4760, 36.8570) 18.2629 0 bounded\n(36.9524, 37.6190) 18.2629 0 bounded (36.8570, 37.1430) 0 18.2629 bounded\n(37.6190, 37.8095) 0 18.2629 bounded (37.1430, 37.9050) 18.2629 0 bounded\n(38.0952, 36.8571) 0 18.2629 bounded (37.9050, 38.0950) 0 18.2629 bounded\n(36.8571, 38.0960) 18.2629 0 bounded (38.8570, 37.1430) 18.2629 0 bounded\n(38.0960, 38.0950) 0 18.2629 bounded (37.1430, 37.6190) 0 18.2629 bounded\n(38.0950, 38.3810) 18.2629 0 bounded (37.6190, 37.6190) 0 5.9140 free\n(38.3810, 39.0470) 18.2629 0 bounded (37.6190, 37.8100) 18.2629 0 bounded\n(39.0470, 37.2390) 0 18.2629 bounded (37.8100, 38.3810) 0 18.2629 bounded\n(37.2390, 37.3330) 0 18.2629 bounded (38.3810, 36.3810) 18.2629 0 bounded\n(37.3330, 37.5240) 18.2629 0 bounded (38.0000, 38.1900) 0 18.2629 bounded\n(37.5240, 37.8090) 0 18.2629 bounded (38.1900, 38.6670) 18.2629 0 bounded\n(37.8090, 38.5720) 0 18.2629 bounded (38.6670, 38.6670) 18.2629 0 bounded\n(38.5720, 37.1420) 18.2629 0 bounded (38.6670, 37.1420) 0 18.2629 bounded\n\ninvestigate this issue, 30 runs of PSO+SVM with the same parameters and without a validation\nset were executed. In this way, the number of training points increased to 89 and the number of\ntest entries remained the same (10). The best test NRMSE result was 1.2536\u00b710?2 with associ-\nated parameters quite different from the ones obtained with the validation and test procedures:\nC = 1144.3099, ? = 5.7140 \u00b710?3, ? = 7.8020. In addition, 37 of the 89 training points became\nsupport vectors, and 20 from these were free. Such test NRMSE would shift the PSO+SVM\napproach to a second position, only loosing for MLP-NN (Gaussian activation), but for a small\namount, in the order of 10?4.\n\n60\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n0 20 40 60 80 100\n35\n\n36\n\n37\n\n38\n\n39\n\n40\n\n41\n\nExample number\n\nO\nu\ntp\n\nu\nt \n(y\n\n)\n\nSupport vector regression results\n\n \n\n \nTraining data\nSupport vectors\nValidation data\nValidation SVR\nTest data\nTest SVR\n\nFigure 4.12: SVR results, Example 3\n\nTable 4.20: Test NRMSE from different forecast models, Example 3. Updated from Xu et al. (2003),\np. 264, Zio et al. (2008)\n\nMethod Test NRMSE\n\nPSO+SVM 1.8969 \u00b710?2?; 1.2536\u00b710?2??\nRBF-NN (Gaussian activation) 2.1100 \u00b710?2\nMLP-NN (Gaussian activation) 1.2200\u00b710?2\nMLP-NN (logistic activation) 1.5600\u00b710?2\nIIR-LRNN 1.5800\u00b710?2\nARIMA 4.2200\u00b710?2\n? With validation; ?? Without validation\n\n4.4 Example 4: Time Between Failures of Oil Production Wells\n\nIn this example, differently from the previous ones, it is resolved a regression problem\ninvolving real data related to features (numerical and categorical) of the system under analysis.\n\nThe systems of interest are oil production wells. The reliability metric considered is the\nTBF, which is believed to be influenced by specific features of the wells. The failure of wells\nrepresent the interruption of oil production and, as a consequence, economical losses. In this\nway, the prediction of the TBF of these systems may permit preventive actions so as to reduce\nor even avoid the effects of the very next failure.\n\nThis example is based on a database that was presented by Barros Jr. (2006). It contains\nrecords of TBF, TTR and related factors of different onshore wells from 1983 to 2006. The\nauthor makes a comprehensive analysis of the variables of the database and proposes the use of\nBNs integrated with Markov chains to estimate the availability of oil wells. The database incor-\n\n61\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nporates the concept of socket (ASCHER &amp; FEINGOLD, 1984), which loosely speaking means\nthat the records are associated with the equipments installed in the wells and are not related to\nthe equipments themselves. For example, it is expected that the behaviors of pumps consecu-\ntively installed in a specific place of a well are approximately the same, since the environmental\nand operational conditions to which they are subjected have not changed.\n\nAccording to Barros Jr. (2006), in the considered context, it has been observed that the\nmost critical components of an oil production well are the pump, the rods and the columns.\nThese equipments are related to the artificial elevation of oil to the surface. The two considered\nartificial elevation methods are the mechanical and the one via progressive cavities. For both\ntypes of elevation methods the columns have the same role of permitting the passage of the rods\nand also of isolating the well boundaries. In the mechanical elevation, the rotating energy of an\nengine is transformed in an alternating motion that is transmitted to the rods and the pump is\nresponsible to transmit the energy to the fluid, which is brought to the surface. In the elevation\nby progressive cavities, in turn, the rotating energy of an engine on the surface is transmitted to\nthe rods that also rotates. The rotating rods transmit energy to the pump, which is within the\nwell and whose components\u2019 disposition permits the passage of the oil.\n\nIn this example, it is considered the wells\u2019 failures due to failures on their installed rods.\nThe elevation method type, the kind of installed filter and the concentration of water and solids\nwithin the well are factors that influence the rods\u2019 performance. These factors, along with the\nnumber of installed rods, are the variables considered to predict the wells\u2019 TBF. Hence, only a\nsubset of the entire database is used. Despite the great number of entries in this subset (more\nthan 10.000), there are many empty cells or cases that present inconsistent information. Also,\nthe database involves essentially non-homogeneous data, given that the records concern various\nwells located in different places and consequently subject to diverse environmental factors.\n\nAs an attempt to reduce the effects of the data non-homogeneity, it was selected a specific\ngroup of wells that are located essentially in the same geographical area with similar charac-\nteristics. The cases that presented any empty cell associated with a variable of interest were\neliminated. After pre-processing the database, a data set with 214 examples was obtained and\ndivided in a training set with the first 170 points, a validation set with the following 20 entries\nand a test set formed by the last 24 examples.\n\nThe description of the input variables selected from the database are presented in Table 4.21\nalong with the characteristics of the TBF itself. Each one of the input variables reflects a specific\nfeature of the wells taken into account. Basically, it is considered the percentage of water and\nsolids within the wells; the number of installed rods of different lengths (3/4, 5/8, 7/8, 1, in\ninches); the absence (N) or presence of a filter (if present, its type C, S or F is recorded and the\nrelated quality increases from C to F); the way the oil is pumped upwards (Progressive Cavities\nPumping (PCP) or Mechanical Pumping (MP)).\n\nNotice that x6 and x7 are categorical variables. The former has an ordinal scale, that is, the\nassociated categories have an underlying order, but can not be quantified. The latter, in turn, has\n\n62\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.21: Selected variables that influence the TBF\n\nName Variable Type Range/Categories\n\nx1 Percentage of water and solids in the well Numerical [0, 98.3]\nx2 Number of 3/4\u201d rods installed Numerical [0, 104]\nx3 Number of 5/8\u201d rods installed Numerical [0, 101]\nx4 Number of 7/8\u201d rods installed Numerical [0, 96]\nx5 Number of 1\u201d rods installed Numerical [0, 95]\nx6 Type of installed filter Categorical N?, C, S, F\nx7 Type of elevation method Categorical PCP, MP\n\ny Time Between Failures (TBF, in days) Numerical [2, 3469]\n?No filter installed\n\na nominal scale and then its categories only denotes the sort of elevation method, which forbids\nany sort of ordering or arithmetical operations. Nevertheless, the SVM training problem only\naccepts numerical values, thus the categorical variables have to be treated before being used by\nthe PSO+SVM algorithm. Traditional statistical regression methods often handle categorical\nvariables by transforming them into indicator or dummy variables (MONTGOMERY et al.,\n2001). That is, if a variable has two associated categories, for example the type of pump used,\nthe indicator variable xind is either 0 to denote that a PCP is used or 1 to indicate that a MP\nis installed. In general, if a categorical variable has r related categories, then r ?1 indicator\nvariables are necessary. For SVM, Hsu et al. (2009) also recommend the use of indicator\nvariables to handle categorical variables. The transformation of the categorical variables x6\nand x7 are shown in Table 4.22.\n\nTable 4.22: Transformation of categorical variables x6 and x7 into indicator variables\n\nx6 xind6,1 x\nind\n6,2 x\n\nind\n6,3 x7 x\n\nind\n7\n\nN 0 0 0 PCP 0\nC 0 0 1 MP 1\nS 0 1 0\nF 1 0 0\n\nIt can be observed from Table 4.21 that the TBF\u2019 interval is quite different from the ranges\nof the numerical input variables. In this way, in order to obtain better results, it is necessary to\nscale the data. The usual scaling range is [0,1], however, a 0 value for the output y gives rise\nto a division by 0 in the computation of MAPE. Thus, instead of using [0,1], the data is scaled\nwithin [1,2]. In addition, each variable is scaled by using its proper minimum and maximum\ntraining values (scaling factors), which results in 8 different scales, 7 for inputs and 1 for the\nTBF. In other words, each dimension of the input vector x as well as the output variable y are\nscaled on their own. Also, as the validation and test sets play the role of unseen data, they are\nscaled using the scaling factors from the training set. The formula is as follows:\n\n63\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nScaled xi j =\n(xi j ?xmin, j)\n\n(xmax, j ?xmin, j)\n\u00b7(up?low) + low (4.2)\n\nwhere i is the example index, j is the dimension of xi under consideration, low and up are the\nboundaries of the scale interval, xmin, j and xmax, j are respectively the minimum and maximum\ntraining values of the jth dimension of xi for i = 1, 2, . . . , `. When validation and test points are\nconsidered (i > `), the same scaling factors xmin, j and xmax, j are used. Moreover, for the present\nexample, low = 1 and up = 2. Following the same reasoning from Equation (4.2), one obtains\na scaling expression for the output variable y:\n\nScaled yi =\n(yi ?ymin)\n\n(ymax ?ymin)\n\u00b7(up?low) + low (4.3)\n\nin which ymin and ymax are the minimum and maximum training values of y, that is, ymin =\nmini(y1, y2, . . . , yi, . . . , y`) and ymax = maxi(y1, y2, . . . , yi, . . . , y`). Again, when validation and\ntest values of y are considered (i > `), ymin and ymax are also used.\n\nMontgomery et al. (2001) assert that, although indicator variables with 0-1 values are often\na best choice, any two distinct values (e.g. 1 and 2) for an indicator variable would be satisfac-\ntory. Hence, in order to follow the same scale of the numerical variables, the categorical ones\nare transformed in 1-2 indicator variables. In Table 4.22, 0 and 1 values are then substituted by\n1 and 2, in this order.\n\nAfter applying the necessary transformations and scales, the proposed PSO+SVM method-\nology can be used. The PSO parameters as well as the bounds for C and ? were the same as the\nones adopted in the previous time series based examples. However, ? ? [1.1106 \u00b710?3, 1.6659 \u00b7\n10?1].\n\nDescriptive statistics of the 30 PSO+SVM runs are presented in Table 4.23. All runs at-\ntained the stop criterion related to equal best fitness values for 600 consecutive iterations.\nThe parameter values related to the \u201cmachine\u201d that provided the smallest test NRMSE are\nC = 4.4422, ? = 1.1774 \u00b710?2 and ? = 2.1226 \u00b710?1. The related errors are listed in Table\n4.24.\n\nIn Figure 4.13, the evolution of the particle swarm can be visualized in three different\nmoments. Notice that in iteration 50 there were infeasible particles, but at the 5276th and last\niteration all particles were within the feasible search space. The validation NRMSE convergence\nis shown in Figure 4.14.\n\nFor this example, the SVR results are presented in separate pictures, given that a unique\ngraphic would be very dense due to the number of data entries involved and would render its\nanalysis difficult. In this way, Figure 4.15 presents the SVR training results, whilst Figure 4.16\ndepicts the SVR validation and test outcomes. From the former figure, it can be observed that\nthe majority of the training examples became support vectors. Indeed, from the 170 training\npoints, 141 were selected as support vectors (120 bounded and 21 free). From the latter figure,\n\n64\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nTable 4.23: Descriptive statistics of parameters and error functions, stop criteria frequency and\nperformance for 30 PSO+SVM runs, lbest, Example 4\n\nMinimum Maximum Median Mean Std. dev.?\n\nParameters\nC 2.6099 11.5072 2.8707 3.7511 2.6397\n? 1.1774 \u00b710?2 2.1867 \u00b710?2 1.5028\u00b710?2 1.5799\u00b710?2 2.1879\u00b710?3\n? 1.1226 \u00b710?1 7.5687 \u00b710?1 6.7570\u00b710?1 6.5571\u00b710?1 1.0901\u00b710?1\n\nValidation\nNRMSE 3.1551 \u00b710?2 3.1852\u00b710?2 3.1561 \u00b710?2 3.1595 \u00b710?2 9.3203\u00b710?5\n\nerror\nMAPE (%) 2.2365 2.4069 2.2412 2.2602 5.0494\u00b710?2\nMSE 1.0697 \u00b710?3 1.0903\u00b710?3 1.0704\u00b710?3 1.0728\u00b710?3 6.3498\u00b710?6\n\nTest error\nNRMSE 4.2354 \u00b710?2 4.8617\u00b710?2 4.7869\u00b710?2 4.7234\u00b710?2 1.8450\u00b710?3\nMAPE (%) 3.4024 4.1029 3.9496 3.9067 1.7208\u00b710?1\nMSE 1.9059 \u00b710?3 2.5112\u00b710?3 2.4346 \u00b710?3 2.3739\u00b710?3 1.7768\u00b710?4\n\nAbsolute (relative, %) frequency\n\nStop criteria\nMaximum number of iterations (6000) 0 (0)\nEqual best fitness for 600 iterations 30 (100)\nTolerance ? = 1\u00b710?12 0 (0)\n\nMetric value\n\nPerformance\nMean time per run (minutes) 14.6685\nMean number of trainings 67929.3000\nMean number of predictions 67929.3000\n\n?Standard deviation\n\nTable 4.24: Validation and test errors from the \u201cmachine\u201d with the smallest test NRMSE, Ex-\nample 4\n\nError function Validation Test\n\nNRMSE 3.1747 \u00b710?2 4.2354 \u00b710?2\nMAPE (%) 2.4069 3.4024\nMSE 1.0831\u00b710?3 1.9059 \u00b710?3\n\nnotice that despite the low quantity of precise predictions, the machine attempts to catch the\ntrend of the validation and test data. Additionally, both figures have the scaled output as vertical\naxis and the example number as the horizontal one, provided that the input vectors are multi-\ndimensional and it is not possible to draw a graphic involving all input vectors with the output\nTBF values.\n\n4.5 Discussion\n\nIn all presented examples, the validation NRMSE dropped to values very near the best\none found in early stages of the PSO algorithm, which suggests the ability of PSO in finding\ngood solutions even with a few number of iterations. Additionally, it can be inferred that the\nSVM, with an appropriate set of parameters, is able to provide excellent results to reliability\n\n65\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n500\n\n1000\n\n1500\n\n2000\n\n0.05\n\n0.1\n\n0.15\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nC\n\nInitial swarm\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n?500\n0\n\n500\n1000\n\n1500\n2000\n\n0.09\n\n0.095\n\n0.1\n\n0.105\n\n0.11\n\n0.115\n48\n\n48.5\n\n49\n\n49.5\n\n50\n\n50.5\n\n51\n\n51.5\n\nC\n\nParticle swarm at iteration 50\n\nepsilon\n\ng\na\n\nm\nm\n\na\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\n0.0114\n\n0.0116\n\n0.0118\n\n0.012\n\n0.0122\n\n0.0124\n0.1\n\n0.105\n\n0.11\n\n0.115\n\n0.12\n\n0.125\n\n0.13\n\n0.135\n\n0.14\n\n0.145\n\nC\n\nParticle swarm at final iteration, 5276\n\nepsilon\n\ng\na\nm\n\nm\na\n\nFigure 4.13: Swarm evolution during PSO, Example 4\n\n0 1000 2000 3000 4000 5000 6000\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.14\n\n0.16\n\n0.18\n\nPSO iteration\n\nN\nR\n\nM\nS\n\nE\n\nNRMSE convergence\n\nFigure 4.14: Validation NRMSE convergence, Example 4\n\n66\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\n0 20 40 60 80 100 120 140 160 180\n\n1\n\n1.1\n\n1.2\n\n1.3\n\n1.4\n\n1.5\n\n1.6\n\n1.7\n\n1.8\n\n1.9\n\n2\n\nExample number\n\nS\nca\n\nle\nd\n o\n\nu\ntp\n\nu\nt\n\nSupport vector regression results\n\n \n\n \nTraining data\nSupport vectors\n\nFigure 4.15: SVR training results, Example 4\n\n170 175 180 185 190 195 200 205 210 215\n0.98\n\n1\n\n1.02\n\n1.04\n\n1.06\n\n1.08\n\n1.1\n\n1.12\n\n1.14\n\n1.16\n\nExample number\n\nS\nca\n\nle\nd\n o\n\nu\ntp\n\nu\nt\n\nSupport vector regression results\n\n \n\n \nValidation data\nValidation SVR\nTest data\nTest SVR\n\nFigure 4.16: SVR validation and test results, Example 4\n\nprediction based on time series, better than or comparable to its competitive models such as NN\nand ARIMA.\n\nConsidering Example 4, the quality of the obtained results certainly is related to the qual-\nity of the data set used. Firstly, even considering wells from the same geographical area, the\noriginal database subset presented, essentially, non-homogeneous records and many empty cells\nor cases with contradictory information. Moreover, the use of categorical variables influences\nthe SVM performance, since it involves a quadratic programming problem in its training step,\nwhich treat all variables as if they were numerical. Hence, the use of categorical variables are\n\n67\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nindicated for the cases when there is not a quantitative manner to measure the factor of interest.\nFor example, transform a variable that is naturally numerical into a categorical one is usually\nnot recommended.\n\nEven with these shortcomings, the PSO+SVM was able to provide small NRMSE test val-\nues in Example 4. In this way, its performance would be certainly enhanced if the data set were\noriginated from a database without errors. Additionally, other manners to handle categorical\nvariables have to be analyzed.\n\nIn the majority of the examples, the descriptive statistics showed a high variance for the\nparameter representing the trade-off between training error and machine capacity (C). This fact\nindicates the difficulty in tuning this parameter and also in using techniques that assign only\ndiscrete values for it, such as the grid search model. If the inherent trade-off of SVM could\nbe explicitly treated, C could be omitted, remaining only the other two parameters ? and ? to\nadjust. Indeed, Mierswa (2007) proposes a multi-objective approach of SVM, in which the\nminimization of training errors is one objective and the margin maximization is the other one.\nIn this situation, C is no longer needed.\n\n4.5.1 Performance Comparison Between lbest and gbest Models\n\nAll examples were solved also by the gbest model, in the same conditions of lbest, i.e.,\nusing the same PSO parameters and SVM data set division among training, validation and test\npoints. The test NRMSE is the metric of greatest interest because it provides an idea of the\ngeneralization ability of the \u201cmachine\u201d under consideration. By taking the 30 test NRMSE\nvalues resulted from each PSO model as independent samples, a Wilcoxon-Mann-Whitney test\n(WACKERLY et al., 2002) can be performed for each example so as to assess the chance of\nobtaining greater values for the test NRMSE values with the gbest model than with the lbest\napproach. In Table 4.25, the p-value of the one-sided test is presented for every case. Notice\nthat the two examples concerning the reliability and failure times of turbochargers as well as\nExample 4 yielded a non-significant p-value for the level of significance of 5%.\n\nTable 4.25: Mean test NRMSE values and Wilcoxon-Mann-Whitney test results lbest \u00d7 gbest\n\nExample\nMean test NRMSE\n\np-value\nlbest gbest\n\n1 2.0307 \u00b710?2 3.3670\u00b710?2 9.3340\u00b710?5\n2.1 2.2999\u00b710?4 5.5485\u00b710?4 1.1490\u00b710?1\n2.2 3.1674\u00b710?2 3.3395\u00b710?2 9.1130\u00b710?2\n3 1.9242\u00b710?2 2.0282\u00b710?2 2.0160\u00b710?3\n4 4.7234\u00b710?2 5.0871 \u00b710?2 6.1820 \u00b710?1\n\nAn interesting point of the turbochargers failure times (Example 2.2) is that not all PSO runs\ncould catch their increasing trend. Only 20% from the gbest runs were not able to predict output\n\n68\n\n\n\nChapter 4 RELIABILITY PREDICTION BY PSO AND SVM\n\nvalues from the test set with the correct increasing trend against 26.6667% of the lbest runs.\nNevertheless, the Wilcoxon-Mann-Whitney test was non-significant, i.e., there is no evidence\nto affirm that the gbest provides better test NRMSE values than the lbest does, considering the\nlevel of significance of 5%.\n\nBratton &amp; Kennedy (2007) asserts that usually the gbest approach presents a faster conver-\ngence if compared to lbest. However, for the presented examples, all lbest runs had a smaller\nmean time per run in absolute terms (Table 4.26). Also, apart from Example 1, the mean num-\nber of SVM predictions (fitness evaluations) was smaller for the lbest approach for all examples\n(Table 4.27). In order to statistically compare the required times by each model and also the\nnumber of predictions, Wilcoxon-Mann-Whitney tests may be performed for each case.\n\nFor the computational time case, Table 4.26 presents the p-values from the related statistical\ntests. Notice that, for a level of significance of 10%, all results were statistically significant.\nTable 4.27, in turn, provides the p-values resulted from the statistical tests associated with the\nmean number of predictions. Taking into account a level of significance of 10%, only the test\nconcerning Example 1 was non-significant. Therefore, the lbest model is prone to require less\ntime as well as less fitness evaluations than the gbest model does.\n\nTable 4.26: Mean time per run (minutes) and Wilcoxon-Mann-Whitney test results lbest \u00d7 gbest\n\nExample\nMean time per run (minutes)\n\np-value\nlbest gbest\n\n1 10.8342 13.2886 6.3310\u00b710?2\n2.1 1.0066 1.3956 1.1710\u00b710?3\n2.2 10.5648 15.3496 6.9060\u00b710?2\n3 4.9337 9.4805 2.6620\u00b710?2\n4 14.6685 16.9978 8.4030\u00b710?2\n\nTable 4.27: Mean number of predictions per run and Wilcoxon-Mann-Whitney test results lbest\n\u00d7 gbest\n\nExample\nMean number of predictions per run\n\np-value\nlbest gbest\n\n1 116016.1667 106998.6333 6.8340 \u00b710?1\n2.1 67989.7000 85499.2000 3.7350\u00b710?2\n2.2 68977.6667 82603.7000 8.8710 \u00b710?2\n3 66563.8000 89619.9333 1.0790\u00b710?2\n4 67929.3000 89402.3000 1.8560 \u00b710?2\n\nIn this way, for the resolved examples, one can infer that the lbest approach is inclined to\nprovide smaller values of test NRMSE than the gbest model does, or at least comparable ones.\nMoreover, for the considered examples, the lbest PSO has a tendency to converge more rapidly\nthan the gbest approach.\n\n69\n\n\n\n5 CONCLUDING REMARKS\n\nThis chapter provides some concluding remarks. In addition, limitations as well as a de-\nscription of the ongoing research along with some suggestions for future works are presented.\n\n5.1 Conclusions\n\nThis work proposed a PSO+SVM methodology for solving reliability prediction problems.\nPSO+SVM combined with a validation set approach to guide the search for appropriate SVR\nparameters\u2019 values was validated with examples from literature concerning reliability prediction\nbased on time series data. The results showed that PSO+SVM can achieve outcomes compa-\nrable to or better than the ones provided by other time series prediction tools, such as NN and\nARIMA.\n\nMoreover, PSO+SVM was applied to an example involving data from oil production wells.\nThe TBF was predicted by considering specific characteristics of the system, differently from\nthe other examples, which were all from literature and based on time series data. The input vari-\nables were both numerical and categorical. The latter were transformed into indicator variables,\nso as to be used by the SVR algorithm. In addition, the numerical input variables as well as the\noutput variable were scaled in [1,2] in order to avoid scale problems and then to get improved\nsolutions. Although the original database presented non-homogeneous data and some problems\nrelated to data gathering, the proposed PSO+SVM methodology was able to provide quite small\nerror values.\n\nPSO was adopted to tackle the SVM problem in the specific context of reliability predic-\ntion. The implemented PSO involved an empirical manner to avoid particles from exiting the\nfeasible search space in early iterations of the algorithm by initially setting a small value for\ntheir maximum velocities. Also, both lbest and gbest communication network among particles\nwere incorporated.\n\nFor all examples, apart from 13 runs in Example 1, the PSO runs converged before reaching\nthe maximum number of iterations. This reflects the ability of the PSO algorithm to find good\nsolutions in early steps of the algorithm. Also, for every example, the validation NRMSE, which\nguided the search for the parameters (fitness), did not present considerable differences along the\n30 runs, which can be observed from the related standard deviations.\n\nFurthermore, a comparison between lbest and gbest PSO models was performed. The re-\nsults for the specific examples considered in this dissertation indicated that the lbest was faster\nand also provided test NRMSE values statistically comparable to or better than the ones yielded\nfrom the gbest approach.\n\nTherefore, given the obtained results, the coupling of PSO with SVM is a promising me-\nthodology to tackle reliability prediction problems based on time series or on data related to\n\n70\n\n\n\nChapter 5 CONCLUDING REMARKS\n\nspecific features of the system, for example, obtained by a condition monitoring procedure.\n\n5.2 Limitations\n\nAlthough the implemented PSO linked with LIBSVM is quite general and can be easily\nadapted to other application domains, the results achieved in this dissertation are limited to\nregression problems in the specific context of reliability prediction from time series data sets\nor from system features. Thus, they can not be generalized to other applications without a\nprevious investigation of the behavior of the proposed methodology applied to the problem\nunder consideration.\n\nAdditionally, PSO is a heuristic search procedure based on probabilities, then it does not\nguarantee the convergence to optimum point. However, it is a useful tool for complex objective\nfunctions that, for example, have no defined derivatives in their domains or whose search for\ntheir optimum is very burdensome. It is rather a mechanism to go in the \u201cright\u201d direction so as\nto obtain good solutions. In many contexts these good solutions are indeed valuable.\n\nFurthermore, the generalization capability in all provided examples was assessed by the test\nNRMSE. This is an indication of such ability but by no means guarantees that the corresponding\ntrained machine will have a good performance when predicting outputs from inputs not in the\ntraining, validation or test sets. If some time after the prediction of an outcome the real value\ncould be observed, this new observation may be incorporated in the data set. With this proce-\ndure, the SVM can be periodically retrained and thus improve its performance in predicting the\nphenomenon under analysis.\n\nThis work considered that failure times and TBF predictions were related to systems subject\nto a single failure mode. The modeling of various failure modes can be tackled analytically by\nmeans of a competitive risks framework (COOKE, 1996), in which the different failure modes\nrepresent risks that compete for leading the system to a failed state. To handle different failure\nmodes with an SVM approach, it would be necessary to associate an SVM to each one of them.\nIn this way, the estimate of the system\u2019s very next failure time or TBF would be determined by\nthe most critical failure mode at the moment, that is, the one with the smallest failure time or\nTBF returned by the associated SVM.\n\n5.3 Ongoing Research\n\nThe following items can be cited as topics of ongoing or future research effort:\n\n\u2022 The application of the proposed methodology in the context of fault diagnostic and prog-\nnostic. The former usually demands a SVM classification or a multi-classification task\nand the latter, often requires a SVR based on metrics obtained via condition monitoring.\n\n\u2022 The combination of PSO+SVM with SMDP so as to allow for the development of a more\n\n71\n\n\n\nChapter 5 CONCLUDING REMARKS\n\ncomprehensive tool to support maintenance decisions.\n\n\u2022 A further investigation of the multi-objective approach for SVM.\n\n\u2022 The SVM training problem may involve huge matrices, which can render the optimization\nprocedure burdensome. The paralellization of the SVM code can be an alternative to this\nissue, by dividing the entire problem in smaller ones to be solved in parallel.\n\n\u2022 The handling of different failure modes considering a PSO+SVM modeling.\n\n72\n\n\n\nREFERENCES\n\nASCHER, H.; FEINGOLD, H. Repairable Systems Reliability: modeling, inference,\nmisconceptions and their causes. New York: Marcel Dekker, 1984.\n\nBANKS, J.; CARSON, J. S.; NELSON, B. L.; NICOL, D. M. Discrete event system simulation.\n3ed. Upper Saddle River: Prentice Hall, 2001.\n\nBARROS JR., P. F. do R.. Uma metodologia para an\u00e1lise de disponibilidade de sistemas\ncomplexos via hibridismo de redes Bayesianas e processos Markovianos. Disserta\u00e7\u00e3o\n(Mestrado) - Universidade Federal de Pernambuco, Recife, August 2006.\n\nBEN-HUR, A.; ONG, C. S.; SONNENBURG, S.; SCH\u00d6LKOPF, B.; R\u00c4TSCH, G. Support\nvector machines and kernels for computational biology. PLoS Computational Biology, vol. 4,\n2008.\n\nBONDY, J. A.; MURTY, U. S. R. Graph theory. New York: Springer, 2008.\n\nBOYD, S.; VANDENBERGHE, L. Convex optimization. Cambridge: Cambridge University\nPress, 2004. Available at:&lt;http://www.stanford.edu/?boyd/cvxbook/>.\n\nBRATTON, D.; KENNEDY, J. Defining a standard for particle swarm optimization. In:\nProceedings of the IEEE Swarm Intelligence Symposium. Honolulu, United States: 2007.\n\nBURGES, C. J. C. A tutorial on support vector machines for pattern recognition. Data Mining\nand Knowledge Discovery, v. 2, p. 121\u2013167, 1998.\n\nCHANG, C.-C.; LIN, C.-J. LIBSVM: a library for support vector machines. 2001. Available\nat:&lt;http://www.csie.ntu.edu.tw/?cjlin/libsvm>.\n\nCHANG, M.-W.; LIN, C.-J. Leave-one-out bounds for support vector regression model\nselection. Neural Computation, vol. 17, n. 5, p. 1188\u20131222, 2005.\n\nCHAPELLE, O.; VAPNIK, V.; BOUSQUET, O.; MUKHERJEE, S. Choosing multiple\nparameters for support vector machines. Machine Learning, vol. 46, p. 131\u2013159, 2002.\n\nCHEN, B.-J.; CHANG, M.-W.; LIN, C.-J. Load forecasting using support vector machines:\na study on eunite competition 2001. IEEE Transactions on Power Systems, vol. 19, n. 4, p.\n1821\u20131830, 2004.\n\nCHEN, K.-Y. Forecasting systems reliability based on support vector regression with genetic\nalgorithms. Reliability Engineering and System Safety, vol. 92, p. 423\u2013432, 2007.\n\nCOOKE, R. M. The design of reliability data bases, part II: competing risk and data\ncompression. Reliability Engineering &amp; System Safety, vol. 51, p. 209\u2013223, 1996.\n\nCRISTINIANI, N.; SHAWE-TAYLOR, J. An introduction to support vetor machines and other\nkernel-based learning methods. Cambridge: Cambridge Universty Press, 2000.\n\nEBERHART, R.; KENNEDY, J. A new optimizer using particle swarm theory. In: Proceedings\nofthe Sixth International Symposium on Micro Machine and Human Science. Nagoya, Japan:\n1995. p. 39\u201343.\n\n73\n\n\n\nReferences\n\nFAN, R.-E.; CHEN, P.-H.; LIN, C.-J. Working set selection using second order information for\ntraining svm. Journal of Machine Learning Research, vol. 6, p. 1889\u20131918, 2005.\n\nFEI, S.-W.; WANG, M.-J.; MIAO, Y.-B.; TU, J.; LIU, C.-L. Particle swarm optimization-based\nsupport vector machine for forecasting dissolved gases content in power transformer oil.\nEnergy Conversion and Management, vol. 50, p. 1604\u20131609, 2009.\n\nFR\u00d6HLICH, H.; ZELL, A. Efficient parameter selection for supportvector machines in\nclassification and regression via model-based global optimization. In: Proceedings of the\nInternational Conference on Neural Networks. Montreal, Canada: 2005.\n\nFULLER, W. A. Introduction to statistical time series. 2ed. New York: John Wiley &amp; Sons,\n1996.\n\nGESTEL, T. V.; SUYKENS, J.; BAESTAENS, D.-E.; LAMBRECHTS, A.; LANCKRIET, G.;\nVANDAELE, B.; MOOR, B.; VANDEWALLE, J. Financial time series prediction using least\nsquares support vector machines within the evidence framework. IEEE Transactions on Neural\nNetworks, vol. 12, n. 4, p. 809\u2013821, 2001.\n\nGOLDBERG, D. E. Genetic algorithms in search, optimization and machine learning.\nAddison-Wesley, 1989.\n\nHAYKIN, S. Neural networks. 2ed. Upper Saddle River: Prentice Hall, 1999.\n\nHONG, W.-C. Chaotic particle swarm optimization algorithm in a support vector regression\nelectric load forecasting model. Energy Conversion and Management, vol. 50, p. 105\u2013117,\n2009.\n\nHONG, W.-C.; PAI, P.-G. Predicting engine reliability by support vector machines.\nInternational Journal of Advaned Manufacturing Technology, vol. 28, p. 154\u2013161, 2006.\n\nHSU, C.-W.; CHANG, C.-C.; LIN, C.-J. A practical guide to support vector classification.\n2009. Available at:&lt;http://www.csie.ntu.edu.tw/?cjlin/papers/guide/guide.pdf>.\n\nHSU, C.-W.; LIN, S.-J. A comparison of methods for multiclass support vector machines.\nIEEE Transactions on Neural Networks, vol. 13, p. 415\u2013425, 2002.\n\nITO, K.; NAKANO, R. Optimizing support vector regression hyperparameters based on\ncross-valiation. In: Proceedings of the International Joint Conference on Neural Networks.\nPortland, United States: 2003.\n\nJARDINE, A. K. S.; LIN, D.; BANJEVIC, D. A review on machinery diagnostics and\nprognostics implementing condition-based maintenance. Mechanical Systems and Signal\nProcessing, vol. 20, p. 1483\u20131510, 2006.\n\nJOACHIMS, T. Making large-scale svm learning practical. In: SCH\u00d6LKOPF, B.; BURGES,\nC.; SMOLA, A. J. (Ed.). Advances in kernel methods: support vector learning. Cambridge:\nThe MIT Press, 1999. p. 169\u2013184.\n\nKECMAN, V. Learning and soft computing: support vector machines, neural networks and\nfuzzy logic models. Cambridge: The MIT Press, 2001.\n\n74\n\n\n\nReferences\n\nKECMAN, V. Support vector machines: an introduction. In: WANG, L. (Ed.). Support vector\nmachines: theory and applications. Berlin Heidelberg: Springer-Verlag, 2005, (Studies in\nFuzziness and Soft Computing, v. 177). p. 1\u201347.\n\nKENNEDY, J.; EBERHART, R. Particle swarm optimization. In: Proceedings of the IEEE\nInternational Conference on Neural Networks. Perth, Australia: 1995.\n\nKENNEDY, J.; EBERHART, R.; SHI, Y. Swarm intelligence. San Francisco: Morgan\nKaufmann, 2001.\n\nKORB, K. B.; NICHOLSON, A. E. Bayesian artificial intelligence. Florida: Chapman &amp;\nHall/CRC, 2003.\n\nKWOK, J. T.-Y. The evidence framework applied to support vector machines. IEEE\nTransactions on Neural Networks, vol. 11, n. 5, p. 1162\u20131173, 2000.\n\nLEE, M. M. S.; KEERTHI, S. S.; ONG, C. J.; DECOSTE, D. An efficient method for\ncomputing leave-one-out error in support vector machines with gaussian kernels. IEEE\nTransactions on Neural Networks, vol. 15, n. 3, p. 750\u2013757, 2004.\n\nLEWIS, E. E. Introduction to reliability engineering. Singapore: John Wiley &amp; Sons, 1987.\n\nLEWIS, R. M.; TORCZON, V.; TROSSET, M. W. Direct search methods: then and now.\nJournal of Computational and Applied Mathematics, vol. 124, p. 191\u2013207, 2000.\n\nLIN, S.-W.; YING, K.-C.; CHEN, S.-C.; LEE, Z.-J. Particle swarm optimization for\nparameter determination and feature selection of support vector machines. Expert Systems with\nApplications, vol. 35, p. 1817\u20131824, 2008.\n\nLINS, I. D.; DROGUETT, E. L. Multiobjective optimization of redundancy allocaion\nproblems in systemas with imperfect repairs via ant colony and discrete event simulation. In:\nProceedings of the European Safety &amp; Reliability Conference (ESREL). Valencia, Spain: 2008.\n\nLINS, I. D.; DROGUETT, E. L. Multiobjective optimization of availability and cost\nin repairable systemas via genetic algorithms and discrete event simulation. Pesquisa\nOperacional, vol. 29, p. 43\u201366, 2009.\n\nMICHALEWICZ, Z. Genetic algorithms + data structures. 3ed. Berlin: Springer-Verlag,\n1996.\n\nMIERSWA, I. Controlling overfitting with multi-objective support vector machines. In:\nProceedings of the 2007 Genetic and Evolutionary Computation Conference (GECCO\u201907).\nLondon, England: 2007.\n\nMOMMA, M.; BENNETT, K. P. A pattern search method for model selection of support vector\nregression. In: Proceedings of the 2002 SIAM International Conference on Data Mining. 2002.\np. 261\u2013274.\n\nMONTGOMERY, D. C.; PECK, E. A.; VINING, G. G. Introduction to Linear Regression\nAnalysis. 3ed. New York: John Wiley &amp; Sons, 2001.\n\nMORETTIN, P. A.; TOLOI, C. M. C. An\u00e1lise de S\u00e9ries Temporais. S\u00e3o Paulo: Edgar Bl\u00fccher,\n2004.\n\n75\n\n\n\nReferences\n\nMOURA, M. J. das C.; DROGUETT, E. A. L. Mathematical formulation and numerical\ntreatment based on transition frequency densities and quadrature methods for non-\nhomogeneous semi-Markov processes. Reliability Engineering &amp; System Safety, vol. 94, p.\n342\u2013349, 2009.\n\nMOURA, M. J. das C.; LINS, I. D.; FIRMINO, P. R. A.; DROGUETT, E. L.; JACINTO, C. M.\nSemi-Markov decision processes for determining multiobjective optimal condition-based\nreplacement policies. In: Proceedings of the European Safety &amp; Reliability Conference\n(ESREL). Prague, Czech Republic: 2009.\n\nM\u00dcLLER, K.-R.; SMOLA, A. J.; R\u00c4TSCH, G.; SCH\u00d6LKOPF, B.; KOHLMORGEN, J.;\nVAPNIK, V. Using support vector machines for time series prediction. In: SCH\u00d6LKOPF, B.;\nBURGES, C. J. C.; SMOLA, A. (Ed.). Advances in kernel methods: support vector learning.\nCambridge: The MIT Press, 1999. p. 243\u2013253.\n\nNOCEDAL, J.; WRIGHT, S. J. Numerical optimization. 2ed. New York: 2006.\n\nPAI, P.-F. System reliability forecasting by support vector machines with genetic algorithms.\nMathematical and Computer Modelling, vol. 43, p. 262\u2013274, 2006.\n\nPAI, P.-F.; HONG, W.-C. Support vector machines with simulated annealing algorithms in\nelectricity load forecasting. Energy Conversion &amp; Management, vol. 46, p. 2669\u20132688, 2005.\n\nPAI, P.-F.; HONG, W.-C. Software reliability forecasting by support vector machines with\nsimulated annealing algorithms. The Journal of Systems and Software, vol. 79, p. 747\u2013755,\n2006.\n\nPLATT, J. C. Fast training of support vector machines using sequential minimal optimization.\nIn: SCH\u00d6LKOPF, B.; BURGES, C. J. C.; SMOLA, A. (Ed.). Advances in kernel methods:\nsupport vector machines. Cambridge: The MIT Press, 1998.\n\nPLATT, J. C.; CRISTINIANI, N.; SHAWE-TAYLOR, J. Large margin DAG\u2019s for multiclass\nclassification. Advances in Neural Information Processing Systems, vol. 12, p. 547\u2013553, 2000.\n\nRAMESH, R.; MANNAN, M. A.; POO, A. N.; KEERTHI, S. S. Thermal error measurement\nand modelling in machine tools. part ii. hybrid bayesian network \u2013 support vector machine\nmodel. International Journal of Machine Tools &amp; Manufacture: Design, Research and\nApplication, vol. 43, p. 405\u2013419, 2003.\n\nRAUSAND, M.; HOYLAND, A. System reliability theory: models and statistical methods.\n2ed. New York: John Wiley &amp; Sons, 2004.\n\nREIS, G. L. dos; SILVA, V. V. da. Geometria anal\u00edtica. 2ed. Rio de Janeiro: Livros T\u00e9cnicos\n&amp; Cient\u00edficos, 1996.\n\nRIGDON, S. E.; BASU, A. P. Statistical methods for the reliability of repairable systems. New\nYork: John Wiley &amp; Sons, 2000.\n\nROCCO, C. M.; MORENO, J. A. Fast monte carlo reliability evaluation using support vector\nmachines. Rliability Engineering &amp; System Safety, vol. 76, p. 237\u2013243, 2002.\n\n76\n\n\n\nReferences\n\nROCCO, C. M.; ZIO, E. A support vector machine integrated system for the classification\nof operation anomalies in nuclear components and systems. Rliability Engineering &amp; System\nSafety, vol. 92, p. 593\u2013600, 2007.\n\nROSS, S. M. Introduction to probability models. 7. ed. San Diego: Academic Press, 2000.\n\nSAMANTA, B.; NATARAJ, C. Use of particle swarm optimization for machinery fault\ndetection. Engineerinhg Applications of Artificial Intelligence, vol. 22, p. 308\u2013316, 2009.\n\nSAPANKEVYCH, N.; SANKAR, R. Times series prediction using support vector machines: a\nsurvey. IEEE Computational Intelligence Magazine, vol. 4, p. 24\u201338, 2009.\n\nSCH\u00d6LKOPF, B.; SMOLA, A. J. Learning with kernels: support vector machines,\nregularization, optimization and beyond. Cambridge: The MIT Press, 2002.\n\nSHI, Y.; EBERHART, R. A modified particle swarm optimizer. In: Proceedings of the IEEE\nInternational Conference on Evolutionary Computation. 1998. p. 69\u201373.\n\nSMITH, D. J. Reliability, maintainability and risk. 6ed. Burlington: Elsevier, 2001.\n\nSMOLA, A. J.; SCH\u00d6LKOPF, B. A tutorial on support vector regression. Statistics and\nComputing, vol. 14, p. 199\u2013222, 2004.\n\nVAPNIK, V. N. The nature of statistical learning theory. 2ed. New York: Springer-Verlag,\n2000.\n\nVAPNIK, V. N.; CHAPELLE, O. Bounds on error expectation for support vector machines.\nNeural Computation, v. 12, n. 9, p. 2013\u20132036, 2000.\n\nWACKERLY, D. D.; III, W. M.; SCHEAFFER, R. L. Mathematical statistics with applications.\nPacific Grove: Thomson Learning, 2002.\n\nWEIGEND, A. S.; GERSHENFELD, N. A. (Ed.). Time series prediction: forecasting the\nfuture and understanding the past. Reading: Addison-Wesley, 1994.\n\nWIDODO, A.; YANG, B.-S. Support vector machine in machine condition monitoring and\nfault diagnosis. Mechanical Systems and Signal Processing, vol. 21, p. 2560\u20132574, 2007.\n\nWU, K.-P.; WANG, S.-D. Choosing the kernel parameters for support vector machines by the\ninter-cluster distance in the feature space. Pattern Recognition, vol. 42, p. 710\u2013717, 2009.\n\nXU, K.; XIE, M.; TANG, L. C.; HO, S. L. Application of neural networks in forecasting engine\nsystems reliability. Applied Soft Computing, vol. 2, p. 255\u2013268, 2003.\n\nYAN, W.; SHAO, H.; WANG, X. Soft sensing modeling based on support vector machines and\nbayesian model selection. Computers and Chemical Engineering, vol. 28, p. 1489\u20131498, 2004.\n\nZIO, E.; BROGGI, M.; GOLEA, L.; PEDRONI, N. Failure and reliability predictions by\nInfinite Response Locally Recurrent Neural Networks. In: Proceedings of 5th European\nCongress on Computational Methods in Applied Science and Engineering (ECCOMAS).\nVenice, Italy: 2008.\n\n77"}]}}}