{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.19311"}, {"@name": "filename", "#text": "26474_001065456.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nINSTITUTO DE INFORM\u00c1TICA\n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM COMPUTA\u00c7\u00c3O\n\nVINICIUS MEDEIROS GRACIOLLI\n\nA Novel Classification Method Applied to\nWell Log Data Calibrated by\n\nOntology-based Core Descriptions\n\nThesis presented in partial fulfillment\nof the requirements for the degree of\nMaster of Computer Science\n\nProfa. Dra. Mara Abel\nAdvisor\n\nPorto Alegre, February 2018\n\n\n\nCIP \u2013 CATALOGING-IN-PUBLICATION\n\nGraciolli, Vinicius Medeiros\n\nA Novel Classification Method Applied to Well Log Data Cal-\nibrated by Ontology-based Core Descriptions / Vinicius Medeiros\nGraciolli. \u2013 Porto Alegre: PPGC da UFRGS, 2018.\n\n64 f.: il.\n\nThesis (Master) \u2013 Universidade Federal do Rio Grande do Sul.\nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o, Porto Alegre, BR\u2013\nRS, 2018. Advisor: Mara Abel.\n\nI. Abel, Mara. II. T\u00edtulo.\n\nUNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nReitor: Profa. Rui Vicente Oppermann\nPr\u00f3-Reitora de Coordena\u00e7\u00e3o Acad\u00eamica: Profa. Jane Fraga Tutikian\nPr\u00f3-Reitor de P\u00f3s-Gradua\u00e7\u00e3o: Prof. Celso Giannetti Loureiro Chaves\nDiretora do Instituto de Inform\u00e1tica: Profa. Carla Maria Dal Sasso Freitas\nCoordenador do PPGC: Prof. Jo\u00e3o Luiz Dihl Comba\nBibliotec\u00e1ria-chefe do Instituto de Inform\u00e1tica: Beatriz Regina Bastos Haro\n\n\n\n\u201cWelcome to the show,\nthe great finale is finally here.\u201d\n\n\u2014 BLACKIE LAWLESS\n\n\n\nACKNOWLEDGMENTS\n\nI would like to thank first and foremost my advisor Mara Abel for all her support prior\nand during this work. I would also like to thank my colleagues Eduardo, Lucas, J\u00falia,\nRenata and Elias who helped me process the data cordially ceded by ANP and Petrobras.\nThis work would also not be possible without the help of Luiz Fernando De Ros who\nprovided support with the geological part of this work.\n\nI would also like to thank Jean-Fran\u00e7ois Rainaud and his wife Marie-Laure for being\nsuch gracious hosts and providing me with the opportunity to spend a wonderful time in\nFrance.\n\n\n\nCONTENTS\n\nLIST OF ABBREVIATIONS AND ACRONYMS . . . . . . . . . . . . . . . . 7\n\nLIST OF FIGURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n\nLIST OF TABLES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\nABSTRACT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\nRESUMO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n\n1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n2 STATE OF THE ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n3 PREVIOUS WORK IN THIS PROJECT . . . . . . . . . . . . . . . . . . 23\n3.1 Automatic Bedding Discriminator . . . . . . . . . . . . . . . . . . . . . 23\n3.2 Enhancements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n\n4 DOMAIN ONTOLOGY OF SEDIMENTARY FACIES . . . . . . . . . . . 27\n4.1 The Strataledge R\u00a91 Ontology . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n5 WIRELINE LOGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.1 The LAS File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.2 Correlation Logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.2.1 Spontaneous Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n5.2.2 Gamma Ray . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n5.2.3 Caliper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n5.3 Porosity Logs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n5.3.1 Sonic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.3.2 Density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.3.3 Neutron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.4 Resistivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.4.1 Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.4.2 Laterolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n5.4.3 Microresistivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n6 DESCRIPTION OF THE TESTING DATA . . . . . . . . . . . . . . . . . 34\n6.1 Testing Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n1STRATALEDGE is a trademark of ENDEEPER Co.\n\n\n\n7 METHODOLOGY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.1 Detecting Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.2 Determining Lithology . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n\n8 RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n\n9 CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n\nREFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\nANNEX 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n\nANNEX 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n\n\n\nLIST OF ABBREVIATIONS AND ACRONYMS\n\nKNN K-Nearest Neighbor\n\nNN Neural Network\n\nBNN Bayesian Neural Network\n\nHMM Hidden Markov Model\n\nSVM Support Vector Machine\n\nLDA Linear Discriminant Analysis\n\nCRF Conditional Random Field\n\nNB Na\u00efve Bayes\n\nABD Automatic Bedding Discriminator\n\nDT Sonic interval transit time\n\nGR Gamma Ray\n\nNPHI Neutron porosity\n\nRHOB Bulk density\n\nDRHO Density correction\n\nILD Induction resistivity\n\nLAS Log ASCII Standard\n\n\n\nLIST OF FIGURES\n\n1.1 Example of a well log report. Each line represents a different geo-\nphysical measurement, as outlined in the image header. The verti-\ncal axis measures the depths at which the measurements were taken.\nSource (SENANYAKE, 2016. Available at http://sanuja.com/blog/what-\nis-a-well-log. Accessed Mar 22 2018) . . . . . . . . . . . . . . . . . 16\n\n1.2 Example of a box containing a sequence of core samples labeled by\ndepth. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n1.3 Example of a non-standardized core description made with freehand\ndrawings and text. The labels show column attributes (2A), area be-\ning digitized (2B - outlined green box), attached symbol menu (2C),\nheader information (such as latitude, longitude, and elevation) (2D\n- dashed green box), and locations of first four digitized points: top\nand bottom of core, and left and right side of symbol menu (2E).\nColumns are recognized by their distance from origin (x-value), and\nvertical dimension (y) is either core length or depth of core pene-\ntration relative to a datum. Source: (USGS, 2010. Available at\nhttps://pubs.usgs.gov/ds/542/. Accessed Mar 22 2018.) . . . . . . . . 18\n\n3.1 Result of moving means being used to detect change points in an\narbitrary dataset. Filter 1 is obtained by applying a small moving\nwindow to the original data, Filter 2 by applying a larger one. The\nfinal result displayed is obtained by averaging all values between each\nintersection. Source (MACDOUGALL; NANDI, 1997) . . . . . . . . 24\n\n3.2 Example showing the integration of break point results. Each vertical\nblack line represents a log from the same well, where break points\nwere detected at the red markers. The numbers on top of each log\ncorresponds to the weight of that log. The orange sections on the logs\nrepresent the depths encompassed by the agreement window ws. If\nwe consider a weight of 3 to declare breaks, a break will be recorded\nin the final assessment on the depth corresponding to the mean depth\nof the three breaks crossed by the green line, as the sum of the three\nlogs with breaks within that depth\u2019s tolerance window is greater or\nequal than 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n4.1 Representation model of sedimentary facies built by a propositional\nterm and a pictorial icon. The icon resembles the visual aspect of\nthe facies. Source: (ONTOLOGICAL PRIMITIVES FOR VISUAL\nKNOWLEDGE, 2010) . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n\n\n4.2 Examples of the rock facies domain ontology, showing the dual rep-\nresentation of geological features. Source: (ONTOLOGICAL PRIM-\nITIVES FOR VISUAL KNOWLEDGE, 2010) . . . . . . . . . . . . 29\n\n6.1 Box plot of the log values for each of the lithologies present in the\ntesting data. The logs present in this data are Gamma Ray (GR),\nSonic (DT), Density (RHOB, DRHO) and Neutron (NPHI). The litholo-\ngies are color coded based on the group to which they were assigned.\nThe sand/clay heterolite belongs to group 2 (pink), and contains only\none sample. While the other two lithologies in group 2 seem to have\ncontrasting signatures, this is due to the low number of samples on\nthe Sand/silt heterolite not being representative enough to create an\naccurate model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n6.2 Box plot of the log values for each of the lithologies present in the\ntesting data. Notice the contrast in distributions between groups 3\nand 4 (sandstone and conglomerates) and the other lithologies. . . . . 38\n\n6.3 Plot of the samples contained in Well 1. Each square presents a scat-\nterplot of two logs, the diagonal shows histograms of the distribution\nof each of the logs. . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n6.4 Plot of the samples contained in Well 2. Each square presents a scat-\nterplot of two logs, the diagonal shows histograms of the distribution\nof each of the logs. . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n6.5 Plot of the samples contained in Well 3. Each square presents a scat-\nterplot of two logs, the diagonal shows histograms of the distribution\nof each of the logs. . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n6.6 Plot of the samples contained in Well 1 grouped in the way described\nin this chapter. Each square presents a scatterplot of two logs, the\ndiagonal shows histograms of the distribution of each of the logs. . . 42\n\n6.7 Plot of the samples contained in Well 2 grouped in the way described\nin this chapter. Each square presents a scatterplot of two logs, the\ndiagonal shows histograms of the distribution of each of the logs. . . 43\n\n6.8 Plot of the samples contained in Well 3 grouped in the way described\nin this chapter. Each square presents a scatterplot of two logs, the\ndiagonal shows histograms of the distribution of each of the logs. . . 44\n\n6.9 Plot of the samples contained in the first synthetic dataset. Each\nsquare presents a scatterplot of two artificial logs, the diagonal shows\nhistograms of the distribution of each of the logs. . . . . . . . . . . . 45\n\n6.10 Plot of the samples contained in the second synthetic dataset. Each\nsquare presents a scatterplot of two artificial logs, the diagonal shows\nhistograms of the distribution of each of the logs. . . . . . . . . . . . 45\n\n6.11 Plot of the samples contained in the third synthetic dataset. Each\nsquare presents a scatterplot of two artificial logs, the diagonal shows\nhistograms of the distribution of each of the logs. . . . . . . . . . . . 45\n\n7.1 This framework shows an overall view of our method for lithology\ndetection, the processes involved and what data artifacts are used and\ngenerated. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\n9.1 Sedimentary Facies and its attributes. . . . . . . . . . . . . . . . . . 61\n\n\n\n9.2 Plot of data from Well 1 . . . . . . . . . . . . . . . . . . . . . . . . 62\n9.3 Plot of data from Well 2 . . . . . . . . . . . . . . . . . . . . . . . . 63\n9.4 Plot of data from Well 3 . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n\n\nLIST OF TABLES\n\n6.1 This table shows the number of samples for each lithology in the three\nwells chosen for the test case. . . . . . . . . . . . . . . . . . . . . . 35\n\n8.1 Classification accuracy achieved on the synthetic dataset with dif-\nferent variations of the method developed, compared to the method\ndescribed in (BOSCH; LEDO; QUERALT, 2013a). . . . . . . . . . . 50\n\n8.2 Classification accuracy achieved on the real dataset with different\nvariations of the method developed, compared to the method described\nin (BOSCH; LEDO; QUERALT, 2013a). . . . . . . . . . . . . . . . 51\n\n8.3 Classification accuracy in each well using the method developed on a\ncomplete split test with and without the use of the Bayesian prior. . . 51\n\n8.4 Classification accuracy in each well using a KNN classifier, the num-\nber after KNN specifies how many neighbors the classifier takes into\naccount. Using more than 40 neighbors makes little sense consider-\ning the number of samples in the training data. The NN line stands\nfor the classification accuracy of the neural network. . . . . . . . . . 52\n\n8.5 Results from the application of the multi-agent system assembled on\nthe three real-data test cases . . . . . . . . . . . . . . . . . . . . . . 53\n\n\n\nABSTRACT\n\nA method for the automatic detection of lithological types and layer contacts was\ndeveloped through the combined statistical analysis of a suite of conventional wireline\nlogs, calibrated by the systematic description of cores.\n\nThe intent of this project is to allow the integration of rock data into reservoir mod-\nels. The cores are described with support of an ontology-based nomenclature system\nthat extensively formalizes a large set of attributes of the rocks, including lithology, tex-\nture, primary and diagenetic composition and depositional, diagenetic and deformational\nstructures. The descriptions are stored in a relational database along with the records of\nconventional wireline logs (gamma ray, resistivity, density, neutrons, sonic) of each an-\nalyzed well. This structure allows defining prototypes of combined log values for each\nlithology recognized, by calculating the mean and the variance-covariance values mea-\nsured by each log tool for each of the lithologies described in the cores. The statistical\nalgorithm is able to learn with each addition of described and logged core interval, in\norder to progressively refine the automatic lithological identification.\n\nThe detection of lithological contacts is performed through the smoothing of each of\nthe logs by the application of two moving means with different window sizes. The results\nof each pair of smoothed logs are compared, and the places where the lines cross define\nthe locations where there are abrupt shifts in the values of each log, therefore potentially\nindicating a change of lithology. The results from applying this method to each log are\nthen unified in a single assessment of lithological boundaries.\n\nThe mean and variance-covariance data derived from the core samples is then used\nto build an n-dimensional gaussian distribution for each of the lithologies recognized. At\nthis point, Bayesian priors are also calculated for each lithology. These distributions are\nchecked against each of the previously detected lithological intervals by means of a prob-\nability density function, evaluating how close the interval is to each lithology prototype\nand allowing the assignment of a lithological type to each interval.\n\nThe developed method was tested in a set of wells in the Sergipe-Alagoas basin and\nthe prediction accuracy achieved during testing is superior to classic pattern recognition\nmethods such as neural networks and KNN classifiers. The method was then combined\nwith neural networks and KNN classifiers into a multi-agent system. The results show\nsignificant potential for effective operational application to the construction of geological\nmodels for the exploration and development of areas with large volume of conventional\nwireline log data and representative cored intervals.\n\nKeywords: Core-log integration, geophysical log, core description, lithology interpreta-\ntion.\n\n\n\nRESUMO\n\nUm m\u00e9todo para a detec\u00e7\u00e3o autom\u00e1tica de tipos litol\u00f3gicos e contato entre camadas foi\ndesenvolvido atrav\u00e9s de uma combina\u00e7\u00e3o de an\u00e1lise estat\u00edstica de um conjunto de perfis\ngeof\u00edsicos de po\u00e7os convencionais, calibrado por descri\u00e7\u00f5es sistem\u00e1ticas de testemunhos.\n\nO objetivo deste projeto \u00e9 permitir a integra\u00e7\u00e3o de dados de rocha em modelos de\nreservat\u00f3rio. Os testemunhos s\u00e3o descritos com o suporte de um sistema de nomencla-\ntura baseado em ontologias que formaliza extensamente uma grande gama de atributos\nde rocha. As descri\u00e7\u00f5es s\u00e3o armazenadas em um banco de dados relacional junto com\ndados de perfis de po\u00e7o convencionais de cada po\u00e7o analisado. Esta estrutura permite\ndefinir prot\u00f3tipos de valores de perfil combinados para cada litologia reconhecida atrav\u00e9s\ndo c\u00e1lculo de m\u00e9dia e dos valores de vari\u00e2ncia e covari\u00e2ncia dos valores medidos por\ncada ferramenta de perfilagem para cada litologia descrita nos testemunhos. O algoritmo\nestat\u00edstico \u00e9 capaz de aprender com cada novo testemunho e valor de log adicionado ao\nbanco de dados, refinando progressivamente a identifica\u00e7\u00e3o litol\u00f3gica.\n\nA detec\u00e7\u00e3o de contatos litol\u00f3gicos \u00e9 realizada atrav\u00e9s da suaviza\u00e7\u00e3o de cada um dos\nperfis atrav\u00e9s da aplica\u00e7\u00e3o de duas m\u00e9dias m\u00f3veis de diferentes tamanhos em cada um dos\nperfis. Os resultados de cada par de perfis suavizados s\u00e3o comparados, e as posi\u00e7\u00f5es onde\nas linhas se cruzam definem profundidades onde ocorrem mudan\u00e7as bruscas no valor do\nperfil, indicando uma potencial mudan\u00e7a de litologia. Os resultados da aplica\u00e7\u00e3o desse\nm\u00e9todo em cada um dos perfis s\u00e3o ent\u00e3o unificados em uma \u00fanica avalia\u00e7\u00e3o de limites\nlitol\u00f3gicos.\n\nOs valores de m\u00e9dia e vari\u00e2ncia-covari\u00e2ncia derivados da correla\u00e7\u00e3o entre testemu-\nnhos e perfis s\u00e3o ent\u00e3o utilizados na constru\u00e7\u00e3o de uma distribui\u00e7\u00e3o gaussiana n-dimensional\npara cada uma das litologias reconhecidas. Neste ponto, probabilidades a priori tamb\u00e9m\ns\u00e3o calculadas para cada litologia. Estas distribui\u00e7\u00f5es s\u00e3o comparadas contra cada um\ndos intervalos litol\u00f3gicos previamente detectados por meio de uma fun\u00e7\u00e3o densidade de\nprobabilidade, avaliando o qu\u00e3o perto o intervalo est\u00e1 de cada litologia e permitindo a\natribui\u00e7\u00e3o de um tipo litol\u00f3gico para cada intervalo.\n\nO m\u00e9todo desenvolvido foi testado em um grupo de po\u00e7os da bacia de Sergipe-\nAlagoas, e a precis\u00e3o da predi\u00e7\u00e3o atingida durante os testes mostra-se superior a algorit-\nmos cl\u00e1ssicos de reconhecimento de padr\u00f5es como redes neurais e classificadores KNN.\nO m\u00e9todo desenvolvido foi ent\u00e3o combinado com estes m\u00e9todos cl\u00e1ssicos em um sistema\nmulti-agentes. Os resultados mostram um potencial significante para aplica\u00e7\u00e3o operacio-\nnal efetiva na constru\u00e7\u00e3o de modelos geol\u00f3gicos para a explora\u00e7\u00e3o e desenvolvimento de\n\u00e1reas com grande volume de dados de perfil e intervalos testemunhados.\n\nPalavras-chave: integra\u00e7\u00e3o perfil-testemunho, perfil geof\u00edsico, descri\u00e7\u00e3o de testemunho,\ninterpreta\u00e7\u00e3o litol\u00f3gica.\n\n\n\n\n\n15\n\n1 INTRODUCTION\n\nThis work details the development of a novel technique for data classification applied\nto geological data. This classification consists of associating numerical data samples with\nclasses representing specific rock types. The framework developed attempts to emulate\nthe behavior of the geologist commonly assigned to this task by segmenting data prior to\nthe use of a classification algorithm; as opposed to classifying the samples independently,\nas it is commonly done in applications that attempt to solve this task automatically.\n\nIn the process of petroleum exploration and development, massive amounts of data are\ngenerated with the goal of locating possible oil reservoirs and determining their viability.\nThis data can span scales ranging from the continental to the microscopic. When a rock\nformation containing oil reserves is found, this data is used to build 3D reservoir models\nthat can be used to simulate the production potential of the reservoir. The goal of this\nwork is to ultimately enrich these models by providing a tool that can be used to infer\nadditional information from the available data, which can then be added to the model in\nlocations where this information is missing.\n\nThe main datasets used to create these models come from seismic imaging and well\ndata. Seismic imaging consists of taking images of the subsurface of the selected lo-\ncation that are produced using seismic echography; by laying acoustic sensors on the\nground at regular intervals and detonating explosive charges these subsurface images can\nbe constructed by measuring the time it takes for the acoustic waves to penetrate the rock\nformation and be reflected back to the sensors. These sensors can be spread across as\nseveral square kilometers, and with these images, which reflect the density and shape of\nthe underlying rock formations.\n\nWell data includes geophysical information and rock samples. This geophysical in-\nformation is presented as wireline logs, which are numerical measurements taken from\ntools lowered down the well during or after perforation. These tools can gauge many dif-\nferent properties of the rock formation such as resistivity, conductivity, sonic transit time\nor gamma ray emissions, producing reports such as the one depicted in figure 1.1 that\nshows a single well with several geophysical measures aligned by depth. With these logs,\nproperties such as porosity, grain size or the presence of fluids can be inferred based on\nthe response of the log. When there are several available wells in the same sedimentary\nbasin, they can be integrated to support the generation of a 3D model of the reservoir.\n\nThe recovery of rock samples allows direct access to rocks located many hundreds of\nmeters underground. There are many types of rock samples that can be recovered and\nanalyzed; detritus that leaves the borehole during drilling for instance, can inform the\ntype of rock present in the drilled area within a reasonably precise depth. More detailed\nsamples are also recovered from the well during and after drilling, such as sidewall cores,\nwhich are samples taken from wall of the borehole via mechanical or percussion drilling\n\n\n\n16\n\nFigure 1.1: Example of a well log report. Each line represents a different geophysical\nmeasurement, as outlined in the image header. The vertical axis measures the depths\nat which the measurements were taken. Source (SENANYAKE, 2016. Available at\nhttp://sanuja.com/blog/what-is-a-well-log. Accessed Mar 22 2018)\n\nat specific depths. Objective measurements can also be taken by submitting a rock sample\nto laboratorial analysis, to calculate attributes such as density, microporosity and perme-\nability.\n\nThe most valuable sample that can be extracted from the well however, is the core\nsample, which is a whole cylindrical section of the borehole which is extracted during\ndrilling without being destroyed in the process. Figure 1.2 shows a box containing core\nsamples collected from a well for further description. The recovery of a core sample is\nan expensive process, since it requires the drilling to stop, and the regular drilling head to\nbe replaced by a special drill with a hollow center to allow for the recovery of the core.\nFor that reason, a borehole spanning several kilometers may only have a couple hundred\nmeters of cores recovered. As the core is retrieved relatively intact, it can offer important\ninformation about the structure of the rock formation, as well as allow the analysis of\nthe contacts between each layer of rock. These samples can be analyzed by a geologist\nwhich generates a qualitative description of the rock formation based on the perceived\ncharacteristics of the rock sample. The qualitative description is a hand-made task that\nproduces reports similar to those presented in Fig XX\n\nThis work focuses on the correlating lithology data, which is a qualitative assessment\nof the rock characteristics recorded in the core descriptions, with readings taken from\nwireline logs. The goal is to be able to extrapolate these rock characteristics based on\nthe much more common wireline log data, which can span most of the depth of the well.\nThis is a pattern recognition problem, where a series of numerical measurements taken at\na specific depth in the borehole are labeled as a lithology, based on the readings taken at\ndepths with previously known lithologies. This work uses a novel approach where prior\nto being labeled, the samples are grouped based on their context within the borehole; by\ndetecting sudden changes in the log values, likely depths at which a lithological change\nmay occur set the boundaries between these groups. This grouping process is shown to\nincrease the prediction accuracy when used in tandem with the method developed to label\nthe samples analyzed.\n\n\n\n17\n\nFigure 1.2: Example of a box containing a sequence of core samples labeled by depth.\n\n\n\n18\n\nFigure 1.3: Example of a non-standardized core description made with freehand drawings\nand text. The labels show column attributes (2A), area being digitized (2B - outlined\ngreen box), attached symbol menu (2C), header information (such as latitude, longitude,\nand elevation) (2D - dashed green box), and locations of first four digitized points: top\nand bottom of core, and left and right side of symbol menu (2E). Columns are recognized\nby their distance from origin (x-value), and vertical dimension (y) is either core length\nor depth of core penetration relative to a datum. Source: (USGS, 2010. Available at\nhttps://pubs.usgs.gov/ds/542/. Accessed Mar 22 2018.)\n\n\n\n19\n\nThis work relies on three hypotheses. First, that we can increase the classification\naccuracy on lithology prediction by separating the data in segments corresponding to\nhomogeneous rock sections. Second, that the boundaries between these rock sections\nshould be detectable by a sudden change in log value. And third, that each lithology has\na distinct log signature.\n\nThe method used for classification is a gaussian classifier based on supervised learn-\ning, where the inference process is calibrated by a training set composed of a set of wire-\nline logs and core descriptions which provide the lithology labels for sampled depths in\nthe wireline logs. The framework developed was trained and tested with real well data\nfrom the Sergipe-Alagoas basin, and the classification accuracy was tested against other\nclassic machine learning methods for pattern recognition such as fuzzy classifiers, neural\nnetworks and k-nearest neighbors classifiers. The results show that the method developed\nachieves a classification accuracy comparable to or greater than these other algorithms.\n\nBy inferring this lithology data in well scale, this information can later be extrapolated\nto a larger scale by integrating this information into reservoir models built from seismic\nimaging in uncored sections, where there is no lithology information.\n\nThis work is organized in the following structure. First, the current methods for lithol-\nogy identification methods are analyzed and their shortcomings are identified. Then we\npresent a summary my previous work and how it relates to the method developed. Later,\nthe datatypes which are used in this work are introduced and explained. Then, the data\nused for testing is presented and analyzed. Finally, the framework for lithology interpre-\ntation developed is presented, tested and compared to other classification methods, from\nwhich we draw our final conclusions.\n\n\n\n20\n\n2 STATE OF THE ART\n\nPredicting lithology from wireline logs is not a novel idea; much work has been done\nin this area attempting to predict lithotypes from log values. Where most of these works\nfall short is when it comes to the testing data. Synthetic datasets are often used, and they\ncan show a vastly different reality from the average borehole. Even when done with real\ndata, samples may be classified not by actual inspection of the rock formation, but by\ngrouping the samples through clustering algorithms.\n\nNeural networks are a popular solution for pattern recognition problems such as this,\nwhich boils down to recognizing a pattern of log signatures and assigning a lithology\nvalue based on these measurements. Neural networks work as a series of nodes passing\nalong numerical values through weighted connections. By training the network with a\nset of data containing inputs and expected outputs, these weights are adjusted by means\nof a backpropagation algorithm until the output of the neural network is in line with the\nexpected output.\n\nReid (REID; LINSEY; FROSTICK, 1989) describes an approach that he has called\nthe Automatic Bedding Discriminator, a method to detect boundaries between lithologies\nbased on gamma ray logs. This is done by using moving means to detect sudden changes\nin the log data, which characterize a change in lithology. The method can then discrimi-\nnate between rocks with larger or smaller grain sizes based on the deflection of the log at\nthese change points, this is possible due to the gamma ray log used being highly sensitive\nto grain size. This method forms the basis of the first part of this work, which allows the\nseparation of the logged interval into discrete sections.\n\nCoudert (COUDERT; FRAPPA; ARIAS, 1994) uses gaussian distributions to build\nprototypes in a similar way to the method developed in this work. However, Coudert\u2019s\ngaussian distributions are all one-dimensional, as opposed to the multivariate distributions\nused in this work. Once the prototypes have been calculated by correlating log values and\nrock samples, they are compared to the prototypes through a probability density function.\nTo increase prediction accuracy, Coudert also uses Bayesian priors and rules based on\ngeological principles to determine lithology on uncertain situations. Due to the use of\nthese rules, Coudert\u2019s method is not as flexible as this work when it comes to adaptation\nto different rock characteristics or other problems.\n\nBrereton (BRERETON; GALLOIS; WHITTAKER, 2001) uses a clustering method\nthat distributes sampled points in a color space based on the readings of the wireline logs,\nand then assigns a lithological significance to these points based on the area of the color\nspace that was assigned to them. While it discerns the most contrasting changes in lithol-\nogy with reasonable accuracy, it also detects a large number of lithological changes not\ndescribed in the validation data. While the article claims that these are subtle variations\nnot easily detected by the geologist doing the core description, it also means the results\n\n\n\n21\n\ncan only be truly validated against the rock sample themselves, not against descriptions\nthat can not be revisited.\n\nLi (LI; ANDERSON-SPRECHER, 2006) compared a naive Bayes classifier with lin-\near discriminant analysis (LDA) and found both methods to perform adequately on a set\nof data from three well consisting of gamma-ray (GR), neutron porosity (NPHI), forma-\ntion density (RHOB), and deep resistivity (LLD) logs and core descriptions from which\nfive distinct facies, which were not entirely based on lithology were identified. The top\nprediction accuracy of Li\u2019s work reached 81.2% with the linear discriminant analysis ap-\nproach.\n\nAl-Anazi (AL-ANAZI; GATES, 2010) shows an approach based on a support vector\nmachine (SVM). Support vector machines can generate mapping functions through super-\nvised learning which allows the samples to be separated by a hyperplane in n-dimensional\nspace. Al-Anazi\u2019s work focuses on predicting permeability, and suffers from the lack of\nhard rock data, as validation is made through comparisons with known electrofacies.\n\nGifford (GIFFORD; AGAH, 2010) uses neural networks, along with other learning\nalgorithms such as k-nearest neighbors (KNN) classifiers in a multi-agent system. In this\napproach, the problem is solved by multiple independent modules, each using a different\nmethod. These results are then integrated into a final output, resulting in a higher accuracy\nthan any individual method used. The complete system presented in the article achieves\na top accuracy of 84.3%.\n\nThe problem with neural network based approaches is that the training is expensive\nin terms of processing power, requiring a large dataset to avoid overfitting. While it can\nproduce good results, the series of calculations learned and performed by a neural network\nare often seen and treated as a black box, for peering inside it reveals a series of low level\nprocesses that are difficult to be understood by a human reader.\n\nBosch in (BOSCH; LEDO; QUERALT, 2013b) describes a fuzzy logic based method\nthat was implemented as a MATLAB routine for the task of facies classification. In this\nmethod, membership functions are calculated from the training data in order to classify\na validation set by measuring the degree of membership of sample to each lithology in a\nmanner similar to the method developed in this work. In Bosch\u2019s work, his method was\ntested using synthetic data. Since the implementation of Bosch\u2019s work was made public,\nit has been tested with the data used in this work for the purposes of comparison with the\nmethod developed.\n\nOjha (OJHA; MAITI, 2013) presents a Bayesian Neural Network (BNN) based ap-\nproach that optimizes the starting weights of his neural network, decreasing the training\ntime needed until the neural network starts to produce reasonable results. The network\nis then trained using a data set derived from clustering and statistical analysis of wireline\nlog data. In the presence of 10% red noise, the method presented in the article achieves\nan average accuracy of 67.38%.\n\nJeong (JEONG et al., 2014) uses a Hidden Markov Model (HMM) and a Conditional\nRandom Field (CRF) based approach to tackle lithology prediction. A HMM can be seen\nas a sequential version of a naive Bayes (NB) classifier, which learns how to classify data\nthrough a joint distribution of the training data. On the other hand, a CRF can be seen\nas a sequential version of a logistic reversion classifier, which learns from conditional\ndistribution. While Jeong\u2019s work managed up to 82% prediction accuracy, it was tested\nusing synthetic data.\n\nSome limitations are show to be very prevalent in the methods presented in the litera-\nture, namely the usage of low quality or synthetic data, and the fact that all these methods\n\n\n\n22\n\ntreat each data sample individually, ignoring the context in which they are inserted. The\nmethod described in this work introduces a new approach where the data is segmented into\nfacies prior to classification. This segmentation is done using a moving-means based al-\ngorithm derived from Reid\u2019s work, which was adapted to work with multiple logs instead\nof relying solely on the gamma ray readings. These segments are then labeled by compar-\ning the samples contained within against multivariate gaussian distributions derived from\ncorrelation between well logs and core descriptions in a training set. This approach has\nthe benefit of increasing classification accuracy by analyzing the data samples within the\ncontext of a contiguous body of rock, instead of isolated datapoints.\n\n\n\n23\n\n3 PREVIOUS WORK IN THIS PROJECT\n\nThe framework described in this work bases its first step in the research previously\npresented in (GRACIOLLI, 2014), where wireline logs are used to determine lithological\nboundaries which can then be correlated to descriptions of core samples in order to correct\nany possible offsets between the core and log depths resulting from faulty data acquisition.\n\nThe method developed uses the boundary information obtained from application of\nthat previous work as a way to enhance the accuracy of pattern recognition methods when\napplied to the task of lithological identification in a novel way which is not explored by\nthe methods currently used in this task.\n\nThis segmentation method is based on the work done by Reid (REID; LINSEY; FRO-\nSTICK, 1989) which is briefly mentioned in the previous chapter, but expanded in order\nto accommodate working with multiple wireline logs, instead of being restricted to the\ngamma ray log. Reid\u2019s work, as well as the enhancements developed in (GRACIOLLI,\n2014) are described in the next section.\n\n3.1 Automatic Bedding Discriminator\n\nReid\u2019s boundary detection algorithm is dubbed the Automatic Bedding Discriminator.\nHis method starts from the assumption that a lithological change can be characterized in\nthe log reading by a sudden change in log values. Reid\u2019s work uses exclusively gamma\nray logs to build a boundary assessment; which is one of the most common logs taken\nfrom a borehole. The Gamma Ray log responds to the organic matter content of the\nrock and has a high correlation to grain size, which makes it useful to detect intercalated\nsandstone-shale layers.\n\nThe first step in detecting these sudden changes is to deal with signal noise. Log\nreadings can be affected by a wide number of variables such as borehole size or the com-\nposition of the drilling mud used during perforation. Even small scale changes in lithol-\nogy that are not detected by the geologist on a core sample or do not characterize a clear\nchange in lithology can be picked up by the logging tools and appear as slight changes in\nlog value.\n\nThis noise is dealt with by applying a centered moving mean to the log data. The\nvalue of a centered moving mean at the data point dp with a window of size n is defined\nas the average value of the n closest data points to point dp (including dp). This results\nin a loss of data, so care must be taken to not use a window that is too large, which will\ndiscard meaningful log features; nor a window that is too small, which won\u2019t effectively\nfilter out variations induced by noise. Reid\u2019s work suggests a window of around 1m for\nthis step, since information is rapidly lost with windows larger than 2m.\n\nNext, a moving mean with a much larger window is applied to the original log, with\n\n\n\n24\n\nthe aim of deriving a curve that shows the general trend of the log; for this, Reid suggests\na window of approximately 10m. By comparing both filtered logs, the points where there\nare sudden changes in log values can be determined by checking at which positions both\nlogs intersect; in essence, where the log increases or decreases more than it\u2019s general\ntrend. This is done simply by checking at each data point if the status quo of which filtered\nlog is higher than the other is either maintained or inverted, if the previously lower-valued\nlog turns becomes the higher-valued log, that means the logs have intersected. A visual\nrepresentation of the result of this process can be seen in figure 3.1.\n\nFigure 3.1: Result of moving means being used to detect change points in an arbitrary\ndataset. Filter 1 is obtained by applying a small moving window to the original data,\nFilter 2 by applying a larger one. The final result displayed is obtained by averaging all\nvalues between each intersection. Source (MACDOUGALL; NANDI, 1997)\n\nAnother consideration taken in Reid\u2019s Automatic Bedding Discriminator is with sec-\ntions of the log where there is little change across a long section of measured depths; in\nthese cases the values of both smoothed logs may be very similar, and very small alter-\nations in the log value may register as an intersection between the smoother logs. In order\nto deal with this situation and cull the resulting false positives from the assessment, Reid\nestablishes a threshold of 4 API1 and disregards detected changes in lithology resulting\nfrom a change in log value lower than this threshold.\n\n1API is a measurement originated from the petroleum industry, and is the standard unit of measurement\nfor gamma ray logs\n\n\n\n25\n\nReid\u2019s method then attempts to classify the facies found in the previous step of his\nalgorithm by analyzing the deflection on the log curve at the points where a bedding\ncontact was detected. By assessing the sign and magnitude of the change in log value,\nit is possible to estimate the increase or decrease in grain size which can differentiate\nbetween shale and sandstone.\n\n3.2 Enhancements\n\nReid\u2019s Automatic Bedding Discriminator results in an assessment of break points for a\nsingle gamma ray log, but the method can be expanded in order to take into account other\nlogs that may also have important lithological significance, such as porosity, density and\nresistivity logs (OJHA; MAITI, 2013); by applying it to multiple logs and then integrating\nthe results in an unified assessment. Before this can be done, however, some issues must\nbe taken into consideration: first, wireline logs have varying degrees of representativeness\nfor lithological assessment (KRYGOWSKI, 2003); second, the same bedding contact is\nnot likely to be detected at the exact same depth across multiple logs.\n\nThe first problem can be solved by assigning a weight w to each log, which is checked\nagainst a user defined threshold t when declaring break points: if the sum of the weights\nof all logs accusing a break at depth d is equal or greater than the threshold t, we say there\nis a break at depth d. The ideal weights can be affected by the type of sedimentary terrain.\nConsidering that the field available for validation in this work is mostly composed by sili-\nciclastic rocks with minor presence of carbonates, the following guidelines for assigning\nweights were proposed by the inquired geologists:\n\n\u2022 Gamma Ray logs are the most representative for lithological changes, and thus\nshould have the highest weight.\n\n\u2022 Density and porosity related logs are also highly representative and thus should\nhave weights close or equal to highest weight.\n\n\u2022 Resistivity logs should have weights around half of the highest value.\n\n\u2022 The remaining logs are not representative enough for lithological assessment, and\nthus, should have weights equal to zero.\n\nThe second problem can be addressed by defining a window of size ws around the\nbreaks detected by each log, and then checking not which logs are accusing a break at a\ngiven depth d, but which logs have a window overlapping depth d. If enough logs have\nbreaks sufficiently near a given depth, as defined by their overlapping windows, and the\nsum of weights w of these logs are equal or greater than t, a break point is declared on\nthe depth defined by the average of the depths of the logs involved, weighted by their\nrespective w weights. An example of this procedure can be seen in figure 3.2\n\nOnce the individual results are compared, and depths that pass the threshold test are\ndeclared as bedding contacts, we have an automated unified assessment of heterogeneities\nin the rock formation. This assessment is more accurate than one derived exclusively from\nthe gamma ray log as it takes into account log characteristics other than organic matter\ncontent and grain size which are expressed in the gamma ray log.\n\n\n\n26\n\nFigure 3.2: Example showing the integration of break point results. Each vertical black\nline represents a log from the same well, where break points were detected at the red\nmarkers. The numbers on top of each log corresponds to the weight of that log. The\norange sections on the logs represent the depths encompassed by the agreement window\nws. If we consider a weight of 3 to declare breaks, a break will be recorded in the final\nassessment on the depth corresponding to the mean depth of the three breaks crossed\nby the green line, as the sum of the three logs with breaks within that depth\u2019s tolerance\nwindow is greater or equal than 3.\n\n\n\n27\n\n4 DOMAIN ONTOLOGY OF SEDIMENTARY FACIES\n\nThe branch of philosophy known as ontology, which is sometimes equated to meta-\nphysics, is the field of study which deals with the nature and structure of reality. Aristo-\ntle defined ontology as the study of attributes intrinsic to things (GUARINO; OBERLE;\nSTAAB, 2009). As such, an ontological study is not concerned with modeling reality\nunder a perspective constrained by data and experiments, but with providing a description\nof the things present in the domain of interest. This means it is completely valid to study\nthe ontology of dragons for example, even though dragons are fictitious beasts, they can\nbe described in terms of concepts and relations.\n\nIn the context of computer science and software engineering, an ontology can be seen\nas a data artifact which specifies the concepts and relations that exist within the universe\navailable for a given information system. In an ontology describing dragons for example,\nthe concepts needed to describe our universe would include dragon, wings, scales, ability\nto breathe fire, hoard and hero, along with the required relations to link these concepts,\nsuch as possesses, guards and fights. These concepts and relations are organized into\na hierarchical taxonomy, with drake for example being a subclass or specialization of\ndragon. These general concepts can then be instantiated to refer to specific actors.\n\nA computational ontology has been defined as \"a formal, explicit specification of a\nshared conceptualization\" (STUDER; BENJAMINS; FENSEL, 1998). To fulfill these\nrequirements this means the ontology should be written in a language that is machine\nreadable, so it is formal. This allows the ontology to be queried and parsed by an appli-\ncation, which allows the system to answer questions such as \"does an instance of dragon\npossesses a hoard?\" by analyzing the actors involved and concepts that link them. Many\nlanguages exist today to encode these ontologies, such as OWL, KIF and OntoUML.\n\nThis definition also requires that our concepts and relations should interpreted cor-\nrectly and consistently so our specification can be explicit. The effective way to ensure\nthis is to constrain the interpretation of the language used by the means logical axioms\nthat allow the possible states of the universe in our specification to be modeled while\nalso minimizing the possibility of modeling unintended, illegal states. For example, the\nrelations fights and possesses can be differentiated by specifying the relation fights as\nirreflexive, intransitive and symmetrical and the relation possesses as irreflexive, intransi-\ntive and asymmetrical. This is reflected in the languages described earlier, which tend to\nbe based on predicates and first-order logic.\n\nThe last point made in the definition presented is that the ontology should be shared.\nThis means the concepts and relations specified in the model should express a consensus\ninstead of an individual view; since as a collection of structured knowledge, an ontol-\nogy is only useful if information it models is agreed upon by all the users. Many top-level\nontologies and domain ontologies have been created with this purpose of knowledge shar-\n\n\n\n28\n\ning. Top-level ontologies such as DOLCE and UFO deal with describing the most basic\nconcepts needed to represent and categorize various entities; they define for example, the\ndifference between countable and uncountable subjects, concrete things versus abstract\nthings.\n\nA domain ontology can be defined as a collection of concepts and relations pertaining\nto a specific domain. Domain ontologies are usually extended from a known top-level\nontology and deal with describing the concepts relevant to the domain of interest. We\nhave for example the National Cancer Institute Thesaurus (NCIT) ontology, which defines\nover one hundred thousand terms related to the medical sciences. The core description\ndata used in this work is backed by another domain ontology focused on the field of geol-\nogy, this allows for standardized and unambiguous descriptions of the rock characteristics\napparent in the samples.\n\nWhen making core descriptions, geologists rely on drawings for expressing what they\nobserve in the rock, since the available vocabulary for describing outcrops or rock sam-\nples is in many cases incomplete or ambiguous. In a previous project, it was studied\nhow to deal with this visual knowledge in order to provide the best support for captur-\ning sedimentary facies descriptions for stratigraphic interpretation, keeping in mind that\ncomputers require propositional information for processing.\n\n4.1 The Strataledge R\u00a91 Ontology\n\nLorenzatti in (ONTOLOGICAL PRIMITIVES FOR VISUAL KNOWLEDGE, 2010)\nproposed a hybrid representation approach for ontologies, which was demonstrated in a\ndomain ontology for macroscopic description of sedimentary facies as a pair composed\nby an icon that visually resembles the visual aspect of sedimentary feature and a proposi-\ntional descriptor, as can be seen in figure 4.1.\n\nFigure 4.1: Representation model of sedimentary facies built by a propositional term and\na pictorial icon. The icon resembles the visual aspect of the facies. Source: (ONTOLOG-\nICAL PRIMITIVES FOR VISUAL KNOWLEDGE, 2010)\n\nLater on, Endeeper (STRATALEDGE: CORE DESCRIPTION SYSTEM, 2012) took\nadvantage of this proposal and formalized an extensive domain ontology for facies de-\nscription of all types of rocks covering more than 750 geological features and 300 icons.\n\n1STRATALEDGE is a trademark of ENDEEPER Co.\n\n\n\n29\n\nThe ontology was developed following the principle of foundational ontologies (GUIZ-\nZARDI, 2005) and it covers all the textural, structural, palentological and lithological\naspects of igneous, metamorphic and sedimentary rocks, including the metassomatic, cat-\naclastic and chemical less common types. The descriptive capability of the formal vo-\ncabulary provides the needed semantic content for the geologist to capture the aspects of\nthe rock for stratigraphic interpretation. We can see in figure 4.2 a small example of the\nknowledge model of sedimentary features.\n\nFigure 4.2: Examples of the rock facies domain ontology, showing the dual represen-\ntation of geological features. Source: (ONTOLOGICAL PRIMITIVES FOR VISUAL\nKNOWLEDGE, 2010)\n\nThe ontology originally proposed by Lorenzatti was further extended and refined by\nCarbonera in (CARBONERA, 2012) to support automatic interpretation of depositional\nprocesses. The author describes the features that define a sedimentary facies which\nare further used to discriminate the sedimentary units. These features were used by\nStrataledge R\u00a9 in this work to segment and describe the core samples. A brief overview of\nthe sedimentary facies concept and its attributes as defined by the ontology is presented\nin Annex 1.\n\nThis extensive controlled vocabulary is embedded in an application for description of\ncores and columnar outcrops. The Strataledge R\u00a9 system produces standardize descrip-\ntions that are stored as records in a database, eliminating ambiguities and reducing sub-\njectivity of the description process. This capability allows computer algorithms to process\nthe information extracting automatic geological interpretation like those described in this\nwork.\n\nThe Strataledge descriptions can then be exported in a XML format that allows for\neasy data processing, or as an SVG profile image file that can be shared for human in-\nspection. The core sample descriptions used in this work are expressed in the Strataledge\nformat, which allows for easy correlation between the wireline logs and the core samples.\n\n\n\n30\n\n5 WIRELINE LOGS\n\nWireline logs are measurements taken from boreholes by using tools that may be low-\nered in one at a time or as a series of sequentially connected tools. These measurements\ncan be either taken during drilling by using logging while drilling (LWD) methods where\nthe logging tools are integrated into the drilling head, or more commonly; after the drilling\nis done and the tools are lowered into the well one at a time or as a series of connected\ntools.\n\nWhen done after drilling, the logging is commonly done while the well is still uncased,\nwhich means it has not yet been cemented and the pipe has not yet been inserted. This\ngives the tool access to the bare rock, which results in less obstructions and more precise\nreadings. While not as common, logs made after the borehole is cased are still possible\nand are sometimes performed.\n\nThis chapter describes the most commonly used well logs (KRYGOWSKI, 2003) and\nthe structure of the file used to record the data resulting from the logging process.\n\n5.1 The LAS File\n\nThe Log ASCII Standard (LAS) file, developed by the Canadian Well Logging Soci-\nety(CRANGLE, 2007) is currently the industry standard for storage of wireline log data\nand is organized as such:\n\n1. A header informing the version of the LAS file.\n\n2. A section containing metadata, such as the identity of well that is logged, its geo-\ngraphical coordinates, elevation, logged depth, among others.\n\n3. A section listing which logs are present in the file, each entry is composed of a\nmnemonic and an optional description.\n\n4. A section containing the data itself, in the form of a list of space separated numerical\nvalues pertaining to each of the logs listed in the previous section at each of the\nlogged depths.\n\n5.2 Correlation Logs\n\nThe logs described in this section are usually used for correlation with other logs, as\nwell as to differentiate between reservoir and non-reservoir formations.\n\n\n\n31\n\n5.2.1 Spontaneous Potential\n\nAlso known as SP, the spontaneous potential log measures the voltage from electrical\ncurrents resulting from the difference in salinities between water in the rock formation\nand the drilling mud in the well. As such, its values are can vary highly from well to well\nbased on the drilling mud used. It can only be run in uncased wells and in the presence of\nwater or water-based drilling mud.\n\nThis log can expresses the presence of a reservoir as a sharp change in value (either\npositive or negative) from an arbitrary yet stable baseline value. This log can also show\nthe presence (but not the magnitude) of permeability in the rock formation. Depositional\nenvironment can also be inferred from the shape of the log curve. The presence of hydro-\ncarbons or shale content in the rock formation will also cause a small deflection in the log\nvalue.\n\nThe mnemonic most often used for this log is SP, and the measurements are usually\ntaken in millivolts (mV).\n\n5.2.2 Gamma Ray\n\nOne of the most common logs, this tool measures the emission of gamma rays from\nnaturally occurring thorium, potassium and uranium present in the rock formation. The\ntool may either take a single measurement from all these three elements, or discrete mea-\nsurements from each one of them (in which case, it is referred to as a spectral gamma ray\nlog). These tools have no restriction on cased or uncased boreholes, or the type of fluid\npresent in the well.\n\nGamma ray measurements correlate to the amount of organic matter present in the\nrock formation. High values can therefore indicate a source rock, which is rich in organic\nmatter, or a fracture where soluble uranium compounds have been deposited. Gamma\nray readings are also highly correlated with shale content, and therefore, can be a good\nindicative of grain size.\n\nThe mnemonic used for this log is usually GR or some variation thereof. Spectral\ngamma ray logs may be divided in 3 logs, identified as THOR, URAN, POTA, or TH, U,\nK, based on the element being tracked. Measurements are taken in API or ppm.\n\n5.2.3 Caliper\n\nThe caliper measures the diameter of the borehole, most commonly through the use\nof arms that extend from the tool. As with the gamma ray log, it is a very common log\nthat has no operational constraints.\n\nThis log in particular has no direct correlation to rock type, and it is used mostly as\ninput for environmental corrections on other logs.\n\nThe most common mnemonics used for the caliper log are CAL or CALI, measure-\nments are taken in centimeters or inches.\n\n5.3 Porosity Logs\n\nThe logs used in this section are mostly used to estimate the porosity of a given rock\nformation. It is important to note that none of these logs measure porosity directly, the\nestimation of porosity is usually obtained through the interpretation of the combination of\ntwo or three of these logs.\n\n\n\n32\n\n5.3.1 Sonic\n\nThis tool consists of a transmitter that emits sonic pulses that are then received by two\nor more receivers located on the same tool. The time differential between each receiver\ndetecting the sonic pulse is called the transit time, or ?T . This tool can only be run in\nuncased boreholes containing a non-gaseous medium.\n\nThe sonic log is used in conjunction with neutron and density data as an estimator\nof lithology, it can also be a good indicator of the mechanical properties of the borehole,\nsuch as formation strength, permeability and porosity.\n\nThe usual mnemonic used for sonic logs is DT, and the measurements are taken in\n\u00b5sec/ft or \u00b5sec/m.\n\n5.3.2 Density\n\nThe tools in this category emit gamma rays from a chemical source towards the rock\nformation, two detectors in the tool count the number of returning gamma rays, which are\nrelated to the density of electrons in the rock formation.\n\nThrough combination with the neutron log, these density logs can be used to estimate\nlithology, gas presence, clay content and formation mechanical properties.\n\nDensity logs can refer to a variety of different log curves, such as bulk density (RHOB,\nDEN or ZDEN, measured in g/cm3 or kg/m3), density porosity (DPHI, PHID or DPOR,\nmeasured in % or v/v decimal), density correction (DRHO, measured in g/cm3 or kg/m3)\nor photoelectric effect (PE, Pe, PEF, measured in b/e).\n\n5.3.3 Neutron\n\nThe tool emits high energy neutrons from a chemical source which are slowed down\nby the nuclei in the rock formation. Two detectors in the tool count either the number\nof returning neutrons or gamma rays, which are inversely proportional to the amount of\nhydrogen residing in the rock formation. Since this hydrogen resides inside the pores of\nthe formation, this measurement is related to the porosity of the rock.\n\nThis log can estimate porosity taking a specific lithology such as limestone as a base-\nline, corrections should be made to estimate porosity for other lithologies through the use\nof charts or other algorithms. By combining this log with density and sonic logs, it is\npossible to estimate lithology, presence of gas and clay content.\n\nThe common mnemonics for the neutron porosity log are NPHI, PHIN and NPOR,\nand the measurements are expressed as % or v/v decimal.\n\n5.4 Resistivity\n\nThese logs measure the electrical resistivity of the rock formation, and can give an\nindicator of the fluid saturation in the rock formation.\n\n5.4.1 Induction\n\nThe tool contains transmitter coils which induce an alternating current in the rock\nformation, the response is then sensed in both magnitude and phase by receiver coils built\ninto the tool. This response is the conductivity of the rock formation, which is the inverse\nof the resistivity. This tool can only be run in an uncased borehole.\n\nInduction logs can be used to calculate formation restivity, fluid saturation, diameter\nof invasion and geopressure.\n\n\n\n33\n\nMnemonics associated to the curves generated by induction logging tools include ILD,\nRILD, ILM, RILM, LLR, SGRD and SFL, which are all measured in ohm.meter.\n\n5.4.2 Laterolog\n\nThis tool creates a horizontal disk-shaped current around the borehole by focusing a\nlow frequency current through the use of an electrode array. Resistivity can be measured\nby monitoring the current passing through the tool. This tool can only be run in an uncased\nborehole filled with water or water-based mud.\n\nAs with the induction logs, laterolog measurements can be used to calculate formation\nresistivity, fluid saturation, diameter of invasion and geopressure.\n\nMnemonics associated with laterologs are DLL, LLD, RLLD, SLL, LLS, RLLS and\nRxo, which are all measured in ohm.meter.\n\n5.4.3 Microresistivity\n\nAlso know ans Rxo, this tool forces an electrical current into the rock formation using\nelectrodes mounted on pads which are pressed against the borehole wall. Some microre-\nsitivity tools focus the current using electrodes similar to the ones used in laterolog tools.\nThis tool can only by run in an uncased borehole filled with water or water-based mud.\n\nRapid curve movement in this log can be an indicator of fractures. The relationship\nbetween microresistivity and other resistivity logs can be in indication of permeability.\nMicroresistivity measurements can also be used to calculate flushed zone formation resis-\ntivity and water saturation. This log is also useful to identify very thin beds.\n\nMicroresistivity curve mnemonics include MNOR, MINV, MSFL and MLL, which\nare all measured in ohm.meter.\n\n\n\n34\n\n6 DESCRIPTION OF THE TESTING DATA\n\nThe greatest hurdle to overcome during this work was the obtention of quality testing\ndata. Well data is highly confidential and therefore, going through the proper channels\nand obtaining the required clearance to work with the data takes time. Additionally, once\nthe data is made available it is often useless for the process described in this work due to\nthe datasets containing few logs, short or inexisting cored sections or simply due to the\nlow quality of the data itself. Frequently the datasets acquired presented one extremely\ndominant lithology, usually sandstone, being over 80% of the cored section. Such bi-\nased datasets make it difficult to create reliable lithology prototypes, as the less prevalent\nlithologies can present a number of samples that are not representative enough for reliable\nstatistical analysis.\n\nAnother issue arises from certain lithotypes being highly variable when it comes to\nlog responses due to their intrinsic properties. Heterolites, which are rock types charac-\nterized by the successive intercalation of thin layers of high and low grain size deposits\nare a good example of this. A section of rock may be characterized as a sand/clay hetero-\nlite and the log readings on this rock might be more similar to clay or sand depending on\nthe ratio of clay to sand in this rock, which might lead to miscategorization. Evaporites\nsuch as anhydrite are also very troublesome due to interaction with water changing the\nchemical composition of the rock and drastically altering the resulting log readings. Oc-\ncurrences of anhydrite can therefore have highly unique log signatures depending on the\nenvironment on which they are situated. All these variations are common in real data and\nbring additional difficulties for the automatic recognition of rock types.\n\nThe data which is provided for scientific research also tends to be older, and while\nStrataledge provides a rich and well structured vocabulary with which to make the rock\ndescriptions, the data captured in the field with direct access to the rock samples available\nfor these experiments was not originally described with the semantic richness offered by\nStrataledge. Therefore, the Strataledge descriptions available for testing are translations\nof descriptions made using other less precise tools, which means the Strataledge toolkit\nis not being used to its full effect. Needless to say this confidentiality makes it extremely\ndifficult to replicate the results of similar works since there is generally no access to their\ndata.\n\nDespite these issues, a suitable test case was found in a sedimentary environment\nwhere the cored section of the wells involved presented enough variability in lithotypes\nwith enough samples to derive reasonable prototypes from once these lithologies were\ngrouped.\n\nThe studied sedimentary succession was deposited in the Sergipe-Alagoas basin in\nnortheastern Brazil. The wells used in this work belong to the Carmopolis field, where\nthe depositional environment is interpreted as an alternation between fan deltaic systems\n\n\n\n35\n\nTable 6.1: This table shows the number of samples for each lithology in the three wells\nchosen for the test case.\n\nWell 1 Well 2 Well 3\n\nGroup 1\nClay/silt heterolite 2 8 19\nShale 27 36 41\nSiltstone 33 24 44\n\nGroup 2\nSand/silt/clay heterolite 0 41 61\nSand/clay heterolite 0 1 0\nSand/silt heterolite 16 0 0\n\nGroup 3 Sandstone 164 158 300\nGroup 4 Conglomerate 71 55 151\n\nand associated alluvial fans and braid deltas prograding into lakes under arid/semiarid\nclimate and increasing marine influence (AZAMBUJA FILHO et al., 1980) (CANDIDO;\nWARDLAW, 1985). In this environment, conglomerates, sandstones and mudrocks were\ndeposited in fining-upward cycles. The reservoirs in this sedimentary unit are constituted\nof conglomerates and sandstones that occur at the base of the depositional cycles. Fining-\nupward amalgamated sequences of these cycles are interbedded with shales, marls, and\ncalcilutites containing anhydrite nodules and stromatolitic laminites replaced by dolomite\nand anhydrite.\n\n6.1 Testing Data Analysis\n\nThe method proposed was tested using real data from three exploration wells in the\nSergipe-Alagoas basin. Each well had geophysical logs available measuring sonic transit\ntime (DT), gamma ray (GR), resistivity (ILD), neutron porosity (NPHI), bulk density\n(RHOB) and density correction (DRHO); as well as core descriptions converted from\nhand-made descriptions into the Strataledge format. We can see this data plotted as scatter\nplots in figures 6.3 6.4 6.5. In these graphs, each box shows the samples present in\neach well plotted in a two-dimensional plane where each axis corresponds to one of the\nlogs. In these graphs it is possible to see the high degree of correlation between certain\nlogs, as well as how samples of the same lithology tend to be grouped together, this is\nespecially noticeable in the conglomerates and sandstones in wells 2 and 3. A breakdown\nof the samples present in this data can be seen in table 6.1, it shows a clear dominance of\nsandstones and conglomerates among every well.\n\nThe difference in log signatures between certain lithotypes was determined to not\nbe significant enough for reliable discrimination, so these lithotypes were grouped in four\ndifferent groups by a geologist based on grain size and lithological similarity. The method\nthen classifies each sampled depth as one of these groups, not as a specific lithology. The\nlithotypes present in the testing data were grouped as follows:\n\n1. Group 1 - Claystone-Siltstone: Clay/silt heterolite, Shale, Siltstone\n\n2. Group 2 - Siltstone-Sandstone: Sand/clay heterolite, Sand/silt heterolite, Sand/silt/clay\nheterolite\n\n3. Group 3: Sandstone\n\n4. Group 4: Conglomerate\n\n\n\n36\n\nThe grouped data can be seen plotted in figures 6.6 6.7 6.8. Box plots showing the\ndifferences of the log readings for each lithology can be seen in figure 6.1, box plots for\nthe lithology groups can be seen in figure 6.2. These box plots show how the sampled\nvalues for each log is distributed in each lithology. The line in the middle of each box\nindicates the median value of that log for that lithology; the box represents the range in\nwhich 50% of samples are located; the \"whiskers\" extend to the value of the highest and\nlowest data points that are not outliers; outliers are marked by + sign and correspond to\nreadings that are at least three scaled median absolute deviations away from the median. In\nthese figures we can clearly see how the conglomerates have a very distinctive distribution\ncompared to the other lithologies.\n\nAdditionally, a synthetic dataset was gracefully granted by the authors of (JEONG\net al., 2014). This synthetic dataset presents three cases with 1600 samples each belonging\nto one of three artificial lithologies with two associated artificial log values. This data can\nbe seen plotted in figures 6.9, 6.10 and 6.11. These cases come from scenario 3 described\nin (JEONG et al., 2014), which is the most complete scenario presented, where noise has\nbeen added to the data to better simulate the environmental conditions from a real well.\nEven in their most true-to-reality form, these graphs show readings with a much lower\ncorrelation, and much more well-defined groups than the real data presented.\n\n\n\n37\n\nFigure 6.1: Box plot of the log values for each of the lithologies present in the testing data.\nThe logs present in this data are Gamma Ray (GR), Sonic (DT), Density (RHOB, DRHO)\nand Neutron (NPHI). The lithologies are color coded based on the group to which they\nwere assigned. The sand/clay heterolite belongs to group 2 (pink), and contains only one\nsample. While the other two lithologies in group 2 seem to have contrasting signatures,\nthis is due to the low number of samples on the Sand/silt heterolite not being representative\nenough to create an accurate model\n\n\n\n38\n\nFigure 6.2: Box plot of the log values for each of the lithologies present in the testing data.\nNotice the contrast in distributions between groups 3 and 4 (sandstone and conglomerates)\nand the other lithologies.\n\n\n\n39\n\nFigure 6.3: Plot of the samples contained in Well 1. Each square presents a scatterplot of\ntwo logs, the diagonal shows histograms of the distribution of each of the logs.\n\n\n\n40\n\nFigure 6.4: Plot of the samples contained in Well 2. Each square presents a scatterplot of\ntwo logs, the diagonal shows histograms of the distribution of each of the logs.\n\n\n\n41\n\nFigure 6.5: Plot of the samples contained in Well 3. Each square presents a scatterplot of\ntwo logs, the diagonal shows histograms of the distribution of each of the logs.\n\n\n\n42\n\nFigure 6.6: Plot of the samples contained in Well 1 grouped in the way described in this\nchapter. Each square presents a scatterplot of two logs, the diagonal shows histograms of\nthe distribution of each of the logs.\n\n\n\n43\n\nFigure 6.7: Plot of the samples contained in Well 2 grouped in the way described in this\nchapter. Each square presents a scatterplot of two logs, the diagonal shows histograms of\nthe distribution of each of the logs.\n\n\n\n44\n\nFigure 6.8: Plot of the samples contained in Well 3 grouped in the way described in this\nchapter. Each square presents a scatterplot of two logs, the diagonal shows histograms of\nthe distribution of each of the logs.\n\n\n\n45\n\nFigure 6.9: Plot of the samples contained in the first synthetic dataset. Each square\npresents a scatterplot of two artificial logs, the diagonal shows histograms of the dis-\ntribution of each of the logs.\n\nFigure 6.10: Plot of the samples contained in the second synthetic dataset. Each square\npresents a scatterplot of two artificial logs, the diagonal shows histograms of the distribu-\ntion of each of the logs.\n\nFigure 6.11: Plot of the samples contained in the third synthetic dataset. Each square\npresents a scatterplot of two artificial logs, the diagonal shows histograms of the distribu-\ntion of each of the logs.\n\n\n\n46\n\n7 METHODOLOGY\n\nHaving the standardized core description and the corresponding geophysical logs, our\nmethod is divided into three steps: segmenting the log data into lithology intervals, build-\ning lithological prototypes by matching core and log data, and finally assigning lithologies\nto the segmented intervals by comparing them to the lithology prototypes. The method\nattempts to extrapolate rock data taken from cores ()which is rare and expensive), from\nlog data (which is relatively cheap and abundant). A workflow of this process can be seen\nin figure 7.1, the steps taken in this workflow are detailed later.\n\nFigure 7.1: This framework shows an overall view of our method for lithology detection,\nthe processes involved and what data artifacts are used and generated.\n\n7.1 Detecting Intervals\n\nIn order to extract lithological significance from wireline logs, the most intuitive way\nof doing this is to first segment the log into regions where the log readings stay within\nroughly the same value. This is the way geologists work while trying to identify rock for-\nmations from wireline logs, and our work attempts to emulate this process with automatic\ndetection of these change points.\n\nStarting from the assumption that a change in lithology can be characterized in the log\nrecord by a sudden change in the measured value, we detect these changes in lithology\n\n\n\n47\n\nusing the work previously described in (GRACIOLLI, 2014) and presented in chapter 3.\nThe assessment generated from this procedure can then be edited or not by the user\n\nusing the program interface and the intervals defined by these break points are passed to\nthe second part of the method, which will estimate the likely lithology of each interval\nbased on the log values encompassed by each of them.\n\n7.2 Determining Lithology\n\nOnce the intervals of interest have been determined, the log values measured at those\ndepths can be analyzed and a lithological significance to the interval in question can be\nassigned. But first, a representation of each lithotype must be created and expressed in\nterms of numerical values, which can then be compared with the log readings taken at\neach interval.\n\nWith the standardized Strataledge core descriptions, the rock characteristics at any\ndepth of a cored interval can be checked easily, this allows the parsing large amounts\nof data automatically. By checking the depth column in the log files, the rest of the\ncore readings can be associated with a corresponding lithology if that depth at that well\ncorrelates to a cored interval, a prototypical log signature for each lithology can then be\ndefined based on the readings taken at depths corresponding to that lithology. The set of\nlogs and core descriptions used to create these prototypes is referred in this work as the\ntraining set.\n\nThe prototypes are created by calculating the average and covariance values between\nall available logs for each lithology. With these values, a log signature corresponding to a\ncertain lithology can be expressed as a multivariate Gaussian distribution defined by the\naverage vector \u00b5 and the covariance matrix ?. Considering a matrix Dm\u00d7n where each of\nthe m rows corresponds to a sampled depth in the wireline log data, with n measured log\nvalues, the average vector \u00b5 can be defined as expressed in the equation 7.1, and each of\nthe values of the covariance matrix ?n\u00d7n can be calculated as defined in equation 7.2.\n\nDuring this step, the Bayesian prior for each lithology is also calculated. The prior of\na lithology L is expressed in 7.4, where n is the number of samples of lithology L, and m\nis the total number of samples in the training data. This prior represents the probability\nfor a data point belonging to a given lithology before being analyzed. This prior is useful\nto give more weight to lithologies that are common in the testing data.\n\n\u00b5 = [\nm?\ni=1\n\nDi,1\nm\n\n,\nm?\ni=1\n\nDi,2\nm\n\n,\nm?\ni=1\n\nDi,3\nm\n\n,...,\nm?\ni=1\n\nDi,n\nm\n\n] (7.1)\n\n?j,k =\nm?\ni=1\n\n(Di,j ?\u00b5j)(Di,k ?\u00b5k)\nm\n\n(7.2)\n\npdf(x,\u00b5, ?) =\n1?\n\n(2?)k|?|\nexp(?\n\n1\n\n2\n(x?\u00b5)T ??1(x?\u00b5)) (7.3)\n\nP(L) =\nn\n\nm\n(7.4)\n\nupdf(x,\u00b5, ?) =\nm?\ni=1\n\npdf(xi,\u00b5i, ?(i,i))\n\nm\n(7.5)\n\nEach interval defined in the first step can then be checked to determine which lithology\nis the best fit for each interval. This is done by checking every point inside the interval\n\n\n\n48\n\nagainst each lithology by means of a probability density function, defined in equation 7.3.\nThe equation receives as input the point x that is being checked, represented as a vector\nof all log values at that depth, and the average vector \u00b5 and covariance matrix ? of the\nlithology that is being checked. The result of this function measures how close the point is\nto that lithology, this result is then multiplied by the prior of the corresponding lithology\nto obtain the final score indicating the likelihood of that point belonging to that lithology.\nOnce every point inside the interval is compared to every lithology encountered in the\ntraining data, the interval\u2019s lithology is declared to be the one that scored higher across\nevery point in the interval.\n\nA univariate variation of this method was also implemented, where the covariance\nbetween the logs is disregarded during the calculation, this can be done simply by calcu-\nlating the average of the probability density function for each individual value of x, \u00b5 and\nthe diagonal of ?. This univariate variation of the pdf is defined in 7.5.\n\nThe original method developed was tested against other pattern matching algorithms\non the same datasets. These algorithms are:\n\n1. A KNN classifier, an implementation of which was used as one of the agents de-\nscribed in (GIFFORD; AGAH, 2010). A K-Nearest Neighbours classifier works\nby assigning a label to a data point based on the distance of that point to the K\nnearest points in the training set. By verifying which labels are the most prevalent\nin the surrounding points, a label to the point analyzed can be assigned based on\nthis assessment. There are many ways to calculate this distance and weigh labels of\nthe neighbors in this estimate. The version implemented in this work uses euclidean\ndistance and equal weights to all neighbors.\n\n2. An artificial neural network composed of 500 nodes on the hidden layer and trained\nusing scaled conjugate gradient. Different neural network implementations have\nbeen widely used in this task, such as in (GIFFORD; AGAH, 2010) (OJHA; MAITI,\n2013). The most common neural network implementations consist of a series of\nnodes (also called neurons) organized in three layers, where each node feeds its\noutput to every node in the next layer. The first layer is the input layer, each node\nin the input layer corresponds to one value of the input data, in our case, each log\nreading at the depth to be analyzed. The second layer is the hidden layer, where\nthe number of neurons is chosen based on the task. A hidden layer size of 500\nwas chosen as a compromise between prediction accuracy and training time. The\nlast layer is the output layer, with one node for each possible prediction label, in\nour case, one node for each lithology. The neural network is trained by feeding\nit a training set consisting of pre-labeled data points adjusting the weights of the\nconnections and the bias of each node until the output of the network classifies\nthe data points with the expected label at a reasonable degree of accuracy. Neural\nnetworks have been successfully used in many classification problems similar to\nthe one presented in this work. As the starting values for the weights of the network\nare randomized at the start of each training session, multiple training sessions may\nresult in varying degrees of categorization accuracy.\n\n3. A fuzzy logic based approach outlined in (BOSCH; LEDO; QUERALT, 2013a). In\nthis method, membership functions are calculated from the training data in order to\nclassify a validation set by measuring the degree of membership of sample to each\nlithology in a manner similar to the method developed in this work.\n\n\n\n49\n\nThe method developed was then combined with the KNN classifier and the neural\nnetwork in a multi-agent system. In a multi-agent system, the problem is solved multiple\ntimes by independent methods, the results are then compared to reach a final, unified\nassessment. Each sample is labeled with the lithology that labeled by the most agents.\nFor example, if 3 agents label sample S as lithology A and 2 agents label sample S as\nlithology B, we label sample S as lithology A. In the case of a tie, the sample is labeled\nas the lithology detected by the first agent to be queried between all the agents involved\nin the tie. These agents are always queried in the same order.\n\nThis methodology aims to verify the classification accuracy of the method developed\nby comparing how it performs against other algorithms, subjecting them to tests using\nboth real and synthetic data. The goal is to validate the hypothesis that the segmentation\nof the log data leads to an increase in classification accuracy.\n\n\n\n50\n\n8 RESULTS\n\nThe methodology previously described has been applied to the both the synthetic data\nand the real data previously mentioned in this work.\n\nAs the method relies on separating the log data in intervals, three different methods of\nmaking this assessment were defined and dubbed as follows:\n\n1. Complete split: each sampled depth is considered an independent facies, this is the\nsame as skipping the segmentation step of the method, and it is useful to assess the\nimpact of this step in the final result.\n\n2. Perfect split: this is a facies assessment created by checking the facies limits in the\ntesting data, and represents a best case scenario where the facies segmentation is\ndone with 100% accuracy.\n\n3. Auto split: this is the moving means method derived from (REID; LINSEY; FRO-\nSTICK, 1989) and developed in (GRACIOLLI, 2014) as described in this work.\n\nFirst, the method was tested using the synthetic dataset provided by the authors of\n(JEONG et al., 2014). Three test cases were constructed from the three datasets in the sce-\nnario analyzed, where each test case would use one of the datasets as the validation case,\nand the remaining two datasets as the training set. These cases were tested against both\nthe univariate and multivariate versions of the algorithm, assisted by the three types of\nsegmentations previously described, as well as the method described in (BOSCH; LEDO;\nQUERALT, 2013a). The results can be seen in table 8.1\n\nThese results show that the premise that the segmentation of the dataset leads to in-\ncreased accuracy holds, as the perfect split outperformed the complete split in every test\ncase and achieved an accuracy in line with the results obtained in (JEONG et al., 2014)\nwhere this dataset was originally used. However, the segmentation method proved to\n\nTable 8.1: Classification accuracy achieved on the synthetic dataset with different varia-\ntions of the method developed, compared to the method described in (BOSCH; LEDO;\nQUERALT, 2013a).\n\nFuzzy Logic\nBased Approach\n\nUnivariate Multivariate\nComplete Auto Perfect Complete Auto Perfect\n\nDataset 1 0.5244 0.7644 0.5669 0.8313 0.7769 0.5356 0.8275\nDataset 2 0.5463 0.8006 0.5738 0.8525 0.8325 0.5444 0.8625\nDataset 3 0.6200 0.7850 0.5676 0.8306 0.8750 0.5381 0.8900\n\n\n\n51\n\nTable 8.2: Classification accuracy achieved on the real dataset with different variations of\nthe method developed, compared to the method described in (BOSCH; LEDO; QUER-\nALT, 2013a).\n\nFuzzy Logic\nBased Approach\n\nUnivariate Multivariate\nComplete Auto Perfect Complete Auto Perfect\n\nWell 1 0.4984 0.5304 0.5240 0.5240 0.4185 0.7029 0.7157\nWell 2 0.3901 0.5077 0.4906 0.4892 0.5015 0.6074 0.6409\nWell 3 0.4042 0.4954 0.4876 0.4892 0.6378 0.6532 0.6749\n\nTable 8.3: Classification accuracy in each well using the method developed on a complete\nsplit test with and without the use of the Bayesian prior.\n\nWithout Bias With Bias\nWell 1 0.2971 0.4185\nWell 2 0.4211 0.5015\nWell 3 0.4367 0.6378\n\nbe ill-suited for this particular synthetic dataset. Since the data is generated from ran-\ndomly seeded samples, it does not behave like readings from a real borehole; there are\nno smooth, small variances within readings from the same facies contrasting with sharp\nand sudden increases or decreases when the rock type changes. For this reason, the best\nsegmentations achieved through exhaustive testing still produced results with an accuracy\nwell under the other types of splits. In this particular case, the segmenting step of the\nalgorithm works best when using windows which are close in value, which result in a\nlarger number of segments in the limited number of logs available in this dataset.\n\nNext, the method was tested against the data extracted from real boreholes. With three\nwells in the testing data, three different test cases were built, each one using one of the\nwells as validation data, and the other two as training data. Due to the close geologi-\ncal distance between these wells, they present similar characteristics and can be used to\ncreate prototypes which are valid between them. These were tested against the method\ndeveloped and the method outlined in (BOSCH; LEDO; QUERALT, 2013a). These tests\nwere using the complete and perfect splits, and both the regular multivariate version of\nthe method and the univariate variation. The results can be seen in 8.2. The impact of the\nBayesian prior was also analyzed by performing complete split tests with and without the\nbias on each test case, the results of this test can be seen in 8.3\n\nThe results show that the multivariate version of the algorithm produces a more accu-\nrate prediction than both the univariate version and Bosch\u2019s approach when supported by\na good segmentation of the log data. It also shows that the univariate version is actually\nnegatively impacted by the segmentation. Since it disregards the correlation between the\nlog values, the univariate version has a lower variance on the degree of certainty it assigns\nlabels. This is further exacerbated when grouping multiple samples, which causes the\nhigher degree of misclassification since it is harder for the correct lithology to emerge as\na clear winner once the probabilities are summed up.\n\nWell 1 as the validation set presented the best results when supported by a quality\nfacies segmentation; this is due to the higher number of outliers and distribution overlap\npresent in well 1 creating less reliable prototypes when inserted into the training data;\nwhen these samples are used only as validation this is not an issue: it is easier to classify\n\n\n\n52\n\nTable 8.4: Classification accuracy in each well using a KNN classifier, the number after\nKNN specifies how many neighbors the classifier takes into account. Using more than 40\nneighbors makes little sense considering the number of samples in the training data. The\nNN line stands for the classification accuracy of the neural network.\n\nKNN5 KNN10 KNN20 KNN40 NN\nWell 1 0.3801 0.4281 0.4600 0.4792 0.5320\nWell 2 0.4427 0.4303 0.4861 0.5108 0.5273\nWell 3 0.4513 0.4643 0.4513 0.4562 0.4894\n\noutliers using reliable prototypes than classify quality data using unreliable prototypes.\nThe wells were then tested using auto split, the parameters used in the segmentation\ngreatly affect the final result. After extensive experimentation involving multiple wells,\nit was ascertained that the most accurate segmentations are obtained by using large win-\ndows for both moving means filters, and a break threshold lower than any of the weights\nascribed to any of the logs. In essence, trusting breaks detected in any logs completely,\nbut requiring large changes in the logs to determine a change in lithology. This goes\nagainst the guidelines suggested by Reid (REID; LINSEY; FROSTICK, 1989), however,\nsince multiple logs are being used instead of just the gamma ray, the use larger windows\nis feasible since the loss of detail incurred by a more extensive smoothing in a single log\nwill be offset by the detections made in other logs. By using these guidelines and exper-\nimenting with the segmentation parameters, an accuracy of 70.29% on well 1 could be\nachieved with moving mean windows of sizes 100 and 300, a result which is very close\nto the one obtained using the perfect split. Windows used on well 2 were 150/300 and on\nwell 3 130/280.\n\nThe other two methods were then tested in each well, without the use of segmentation,\nand the results can be seen in 8.4. Neural network results vary with the random initial\nvalue of the nodes during training, but the average accuracy after 10 independent training\nsessions can be seen in the table. The neural network outperformed the KNN classifier\nin all test cases, but both were still inferior to the method developed in the presence of a\nreliable assessment of break points.\n\nNext, a multi-agent system was built using the methods previously tested. In this im-\nplementation, each agent classifies the samples in the test data as one of the four litholog-\nical groups based on the same training data. The label for each sample is then determined\nto be the one that was selected the most between all the agents.\n\nThe multi-agent system that presented the most consistent results was composed of\nthe following agents:\n\n\u2022 Two agents implementing the method developed in this work.\n\n\u2022 One agent implementing the neural network previously tested.\n\n\u2022 One agent implementing a cascade forward version of the neural network previ-\nously tested. In this version of the neural network, the input nodes are also linked\nto nodes in the output layer in addition to the nodes in the hidden layer.\n\n\u2022 One agent implementing a KNN40 classifier as previously tested.\n\nSince the method developed obtained the best results, it is implemented twice to act\nas a tie-breaker in case of indecision between an even number of agents. The results\n\n\n\n53\n\nTable 8.5: Results from the application of the multi-agent system assembled on the three\nreal-data test cases\n\nUnivariate Multivariate\nComplete Split Perfect Split Complete Split Perfect Split\n\nWell 1 0.5335 0.5240 0.5240 0.6645\nWell 2 0.5356 0.5170 0.5759 0.6254\nWell 3 0.4659 0.4594 0.4627 0.4753\n\nfrom this multi-agent system can be seen in table 8.5. These results are in line with the\nones previously obtained on this data set. Wells 1 and 2 presented increases in prediction\naccuracy on the complete split cases, but a decrease on the perfect split cases due to the\nother methods disregarding the log segmentation.\n\nThe results show that the method developed benefits greatly from the segmentation of\nwell data. Despite the difficulty in finding effective parameters for the automatic segmen-\ntation method, the tests done with the perfect split show a top increase in accuracy of up\nto 71% in one of the test cases.\n\n\n\n54\n\n9 CONCLUSION\n\nThe objective of this work is to is to present a method for lithological interpretation\nthrough the calibration of wireline logs via correlation with core sample descriptions; with\nthe goal of enriching reservoir models by integrating this data in a larger scale.\n\nTo this end, a novel method was developed that proposes the segmentation of the log\ndata prior to the use of a classification algorithm. By segmenting the log data into regions\nof interest using a moving-means based algorithm, these segments can then be labeled by a\ngaussian classifier that takes into account a whole section of the rock formation, instead of\na single sample. This segmentation was shown to improve the final classification accuracy\nof the algorithm significantly.\n\nThe results show that the method developed can achieve an accuracy comparable or\nbetter than current methods used for lithology classification. This work shows that the\nlog segmentation performed previous to the classification proved to increase the accuracy\nof the method significantly, and while it is dependent on a large number of parameters, it\ncan be assisted by a geologist determining the break points manually or semi-manually,\nimproving the final results of the classification. By utilizing signal processing algorithms\nprior to the classic pattern recognition methods we can increase the prediction accuracy in\nsituations such as the one described in this work, where the context of the data matters; the\nsamples do not exist in a vacuum, as they are measurements from a contiguous section of\nrock, and therefore should be treated as such as most of the current methods for lithology\nprediction propose.\n\nAnother strength of the method proposed is its modularity. The framework presented\ncan easily accommodate changes to each individual process without changing the overall\nbehavior of the method. This leaves the method open to improvements in the way that\nnew methods can be developed to segment the logs, build prototypes or classify samples\nwhich can then be easily plugged in the framework. This was already demonstrated to an\nextent in this work in the way the multi-agent system was implemented by adding parallel\nmethods that are later integrated in the categorization section of the framework.\n\nIt is also worth noting that the method itself is completely divorced from any notion\nof lithology, the only thing determining that the program classifies lithologies are the\nnumerical models fed into it. We can just as easily create statistics based on other features\nwhose description is supported by domain ontology, such as hydrocarbon levels or create\nmodels that reflect other ways to categorize rock formations, such as petrofacies. The\ntechniques developed in this work are also not tied to the field of geology, and can be\nused in any other similar problem that attempts to classify sections of continuous data.\n\nLimitations arise from the high number of parameters required by the segmentation\nalgorithm. Log readings can also vary wildly between different basins, reservoirs and\nwells; which makes hinder the construction of more universal lithology prototypes. The\n\n\n\n55\n\nmethod must use a training set with data recovered from locations near the testing well,\nthis can be a problem if the sampling in the region is low-quality or low-volume. Biased\nsampling when recovering the core samples can also lead to misclassification on depths\nwhere the rock data was not reflected in the core samples.\n\nA different segmentation proposal or a method to infer the best possible parameters for\nsegmentation more efficiently can make the method more reliable and result in a product\nwith easier interaction with the end user. The method can also be explored to identify\ndifferent rock characteristics or to be applied to a different field altogether in the task\nof classifying sequential numerical data. Testing different mathematical models for the\nprototypes could also prove to be beneficial.\n\nFinally, this work should also be revisited at a later date when more quality data be-\ncomes available. More reliable core descriptions captured with the support of Strataledge\ninstead of merely translated into Strataledge could be used to create more accurate pro-\ntotypes and eliminate issues with more ambiguous lithologies where there is a weaker\nconsensus between geologists describing them.\n\n\n\n56\n\nREFERENCES\n\nAL-ANAZI, A.; GATES, I. Support vector regression for porosity prediction in a hetero-\ngeneous reservoir: a comparative study. Computers &amp; Geosciences, [S.l.], v.36, n.12,\np.1494\u20131503, 2010.\n\nAZAMBUJA FILHO, N.; ABREU, C.; HORSCHUTZ, P.; CANDIDO, A.; RAMOS,\nE. Estudo sedimentol\u00f3gico, faciol\u00f3gico e diagen\u00e9tico dos conglomerados do campo\npetrol\u00edfero de Carm\u00f3polis: xxxi congresso brasileiro de geologia. Anais, [S.l.], v.1,\np.240\u2013253, 1980.\n\nBOSCH, D.; LEDO, J.; QUERALT, P. Fuzzy logic determination of lithologies from well\nlog data: application to the ktb project data set (germany). Surveys in Geophysics, [S.l.],\nv.34, n.4, p.413\u2013439, 2013.\n\nBOSCH, D.; LEDO, J.; QUERALT, P. Fuzzy logic determination of lithologies from well\nlog data: application to the ktb project data set (germany). Surveys in Geophysics, [S.l.],\nv.34, n.4, p.413\u2013439, 2013.\n\nBRERETON, N.; GALLOIS, R.; WHITTAKER, A. Enhanced lithological description of\na Jurassic mudrock sequence using geophysical wireline logs. Petroleum Geoscience,\n[S.l.], v.7, n.3, p.315\u2013320, 2001.\n\nCANDIDO, A.; WARDLAW, N. Reservoir geology of the Carmopolis oil field, Brazil.\nBulletin of Canadian Petroleum Geology, [S.l.], v.33, n.4, p.379\u2013395, 1985.\n\nCARBONERA, J. L. Racioc\u00ednio sobre conhecimento visual: um estudo em estratigrafia\nsedimentar. , [S.l.], 2012.\n\nCOUDERT, L.; FRAPPA, M.; ARIAS, R. A statistical method for litho-facies identifica-\ntion. Journal of applied geophysics, [S.l.], v.32, n.2-3, p.257\u2013267, 1994.\n\nCRANGLE, R. D. Log ASCII Standard(LAS) Files for Geophysical Wireline Well Logs\nand Their Application to Geologic Cross Sections through the Central Appalachian Basin.\nOpen-file Report. U. S. Geological Survey, [S.l.], p.14, 2007.\n\nGIFFORD, C. M.; AGAH, A. Collaborative multi-agent rock facies classification from\nwireline well log data. Engineering Applications of Artificial Intelligence, [S.l.], v.23,\nn.7, p.1158\u20131172, 2010.\n\nGRACIOLLI, V. M. Bedding contact detection: a moving mean-based approach. , [S.l.],\n2014.\n\n\n\n57\n\nGUARINO, N.; OBERLE, D.; STAAB, S. What is an ontology? In: Handbook on\nontologies. [S.l.]: Springer, 2009. p.1\u201317.\n\nGUIZZARDI, G. Ontological foundations for structural conceptual models. [S.l.]:\nCTIT, Centre for Telematics and Information Technology, 2005.\n\nJEONG, J.; PARK, E.; HAN, W. S.; KIM, K.-Y. A novel data assimilation methodology\nfor predicting lithology based on sequence labeling algorithms. Journal of Geophysical\nResearch: Solid Earth, [S.l.], v.119, n.10, p.7503\u20137520, 2014.\n\nKRYGOWSKI, D. A. Guide to petrophysical Interpretation. Austin Texas USA, [S.l.],\n2003.\n\nLI, Y.; ANDERSON-SPRECHER, R. Facies identification from well logs: a comparison\nof discriminant analysis and na\u00efve bayes classifier. Journal of Petroleum Science and\nEngineering, [S.l.], v.53, n.3-4, p.149\u2013157, 2006.\n\nMACDOUGALL, S.; NANDI, A. K. Hybrid Bayesian procedures for automatic detection\nof change-points. Journal of the Franklin Institute, [S.l.], v.334, n.4, p.575\u2013597, 1997.\n\nOJHA, M.; MAITI, S. Sediment classification using neural networks: an example from\nthe site-u1344a of {IODP} expedition 323 in the bering sea. Deep Sea Research Part II:\nTopical Studies in Oceanography, [S.l.], n.0, p.\u2013, 2013.\n\nROCHA COSTA, A. C. da; VICARI, R. M.; TONIDANDEL, F. (Ed.). Ontological\nPrimitives for Visual Knowledge. Berlin, Heidelberg: Springer Berlin Heidelberg, 2010.\np.1\u201310.\n\nREID, I.; LINSEY, T.; FROSTICK, L. E. Automatic bedding discriminator for use with\ndigital wireline logs. Marine and petroleum geology, [S.l.], v.6, n.4, p.364\u2013369, 1989.\n\nSENANYAKE, S. What is a Well Log? 2016. Available at http://sanuja.com/blog/what-\nis-a-well-log. Accessed Mar 22 2018.\n\nSTRATALEDGE: core description system. Porto Alegre, Brazil: Endeeper, 2012.\n\nSTUDER, R.; BENJAMINS, V. R.; FENSEL, D. Knowledge engineering: principles and\nmethods. Data &amp; knowledge engineering, [S.l.], v.25, n.1-2, p.161\u2013197, 1998.\n\nUSGS. U.S. Geological Survey Data Series 542. 2010. Available at\nhttps://pubs.usgs.gov/ds/542/. Accessed Mar 22 2018.\n\n\n\n58\n\nANNEX 1\n\nThe following text is an excerpt translated from Carbonera\u2019s master dissertation as\nmentioned in chapter 4, which details the concepts and structure of a section of the domain\nontology used by Strataledge to create the core descriptions used in this work.\n\n8.1 Domain ontology presentation\n\nThe fundamental concept of the domain is that of Sedimentary Facies. This concept\noffers the identity criteria to all its instances, once geologists are capable of distinguishing\none instance from the other through observation of its visual characteristics. This concept\nalso offers the unity criteria to its instances. This criteria involves the observation of\ndiscontinuities on the visual characteristics of the facies and allows for the geologist to\ndiscriminate between the different units of the Sedimentary Facies. Furthermore, this\nconcept is rigid, once Sedimentary Facies instances can\u2019t stop being Sedimentary Facies\ninstances, unless they cease to exist. In this ontology, a Sedimentary Facies is considered\na Kind, according to UFO.\n\nA Sedimentary Facies is characterized through sixteen distinct qualities, which are\nconsidered Quality Universals, according to UFO. Each of these Quality Universals is\nassociated with a quality structure which represents the space of possible values. Each of\nthese qualities, along with the respective associated value space, is detailed as follows:\n\n\u2022 Mode: Defies the most frequent grain size present in the rock. This is a textural\nproperty of extreme importance, since it characterizes distinct sedimentary envi-\nronments and provides information regarding depositional processes. Its quality\nstructure contains: Silt, Clay, Very fine sand, Fine sand, Medium sand, Coarse\nsand, Very coarse sand, Gravel, Granule, Pebble, Block and Boulder.\n\n\u2022 Larger grain size: Measurement of the largest grain size observed in a sedimentary\nfacies. Its quality structure contains: Silt, Clay, Very fine sand, Fine sand, Medium\nsand, Coarse sand, Very coarse sand, Gravel, Granule, Pebble, Block and Boulder.\n\n\u2022 Smaller grain size: Measurement of the smallest grain size observed in a sedi-\nmentary facies. Its quality structure contains: Silt, Clay, Very fine sand, Fine sand,\nMedium sand, Coarse sand, Very coarse sand, Gravel, Granule, Pebble, Block and\nBoulder.\n\n\u2022 Base grain size: Mode of the size measurement of the particles observed at the\nbase of a sedimentary facies. Its quality structure contains: Silt, Clay, Very fine\nsand, Fine sand, Medium sand, Coarse sand, Very coarse sand, Gravel, Granule,\nPebble, Block and Boulder.\n\n\n\n59\n\n\u2022 Top grain size: Mode of the size measurement of the particles observed at the top\nof a sedimentary facies. Its quality structure contains: Silt, Clay, Very fine sand,\nFine sand, Medium sand, Coarse sand, Very coarse sand, Gravel, Granule, Pebble,\nBlock and Boulder.\n\n\u2022 Sorting: Refers to the degree of uniformity between the size of the particles com-\nposing the facies. Sediments whose grains possess a predominantly uniform size\nare classified as well sorted. Sediments which contain grains with different diam-\neters are considered poorly sorted. Its quality structure contains: Very well sorted,\nWell sorted, Moderately sorted, Poorly sorted and Very poorly sorted.\n\n\u2022 Sphericity: Indicates the degree of approximation between a sedimentary particle\nand a sphere. Low sphericity particles are elongated and are indicative of the source\narea. Its quality structure contains: High, Medium, Low.\n\n\u2022 Roundness: Corresponds to an attribute independent from sphericity, which in-\ndicates the degree of angularity of the corners of the sedimentary particles. Well\nrounded particles are commonly indicative of prolonged mechanical abrasion. Its\nquality structure contains: Very angular, Angular, Subangular, Subrounded, Rounded\nand Well-rounded.\n\n\u2022 Color: Mainly indicates the composition of the minerals in a rock and consequently\nin a facies. Isolated however, this attribute is not a sufficient property to discrim-\ninate between facies. Its quality structure contains 119 colors defined nominally\naccording to the standard rock table NBS/ISCC RC.\n\n\u2022 Lithology: Corresponds to classification of the rock that composes the facies, as a\nresult of combination of chemical, mineralogic and textural aspects of the facies.\nIts quality structure contains 141 different lithotypes.\n\n\u2022 Bed gradation: Indicates the gradation mode of the grain sizes along a facies,\nfrom base to top. Normal gradation indicates a reduction on granulometry from the\nbase to the top of the rock layer. Inverse gradation indicates a gradation that is the\nopposite of the normal, with higher diameter grains on the top, gradating to lower\ndiameter grains on the base. The facies can also present inexistent gradation, when\nit shows homogeneity between the grain sizes from the base to the top. Its quality\nstructure contains: Inexistent, Normal, Inverse.\n\n\u2022 Laminae gradation: Indicates an aspect analogous to bed gradation, considering\nhowever, the laminae that form the structures. In essence, it indicates the grain size\ngradation between two laminae of the structure that compose the facies. The mea-\nsurement of this gradation is taken considering the direction of the force responsible\nfor the creation of the laminae, which can be identified through observation of the\nsmallest angle between the laminae and the base of the structure. Its quality struc-\nture contains: Inexistent, Normal, Inverse.\n\n\u2022 Specific grains: Indicates the presence of certain types of specific grains, impor-\ntant for future predictions. Specific grains can be intraclasts, when they are re-\nworked fragments of poorly consolidated sediments inside the same depositional\nbasin, through the action of currents, landslides, etc; or extraclasts, when they are\nsedimentary fragments whose origins are external to the sedimentary environment.\nIts quality structure contains: Extraclasts, Intraclasts.\n\n\n\n60\n\n\u2022 Fabric orientation: Indicates the orientation of the grain that form the facies.\nIts quality structure contains: Chaotic, Imbricated, Heterogeneous, Homogeneous,\nParallel, Sub-parallel, Without orientation.\n\n\u2022 Fabric support: indicates the support of the sediment in the facies. Its quality\nstructure contains: Crystalline fabric, Bioconstruction undifferentiated fabric, Bio-\nconstruction baffle fabric, Bioconstruction encrusting fabric, Bioconstruction rigid\nframework fabric, Grain-supported, Matrix-supported, Cement-supported, Grain to\ncement-supported, Matrix to cement-supported, Grain to matrix-supported.,\n\n\u2022 Geometry: Refers to the geometrical forms external to the layer which contains the\nfacies, independent of its internal organization, along a determined lateral exten-\nsion, as a result of sediment deposition. Often geometry is hard to reconstruct, due\nto the limitation of exposure. In rock outcrops, the description of geometry is based\nmainly in the observation of the thickness of the facies and their lateral continuity, in\ncore samples however, the reconstruction of geometry is generally not possible. Its\nquality structure contains: Tabular, Lenticular concave-convex, Lenticular convex\ntop plain base, Lenticular plain top convex base, Sigmoidal, Wedge and Irregular.\n\nIn figure 9.1 we present a fragment of the domain ontology representing the concept\nof sedimentary facies and its qualities.\n\n\n\n61\n\nFigure 9.1: Sedimentary Facies and its attributes.\n\n\n\n62\n\nANNEX 2\n\nThe following plots show the log and core data from the real wells described in chapter\n6. The leftmost column shows the facies described by the geologist in terms of lithology\nand grain size. The remaining columns show the log readings along the cored interval.\nThese graphs were generated using the Strataledge R\u00a9core description system.\n\nFigure 9.2: Plot of data from Well 1\n\n\n\n63\n\nFigure 9.3: Plot of data from Well 2\n\n\n\n64\n\nFigure 9.4: Plot of data from Well 3\n\n\n\tList of Abbreviations and Acronyms\n\tList of Figures\n\tList of Tables\n\tAbstract\n\tResumo\n\tIntroduction\n\tState of the Art\n\tPrevious Work in This Project\n\tAutomatic Bedding Discriminator\n\tEnhancements\n\n\tDomain Ontology of Sedimentary Facies\n\tThe Strataledge\u00aeSTRATALEDGE is a trademark of ENDEEPER Co. Ontology\n\n\tWireline Logs\n\tThe LAS File\n\tCorrelation Logs\n\tSpontaneous Potential\n\tGamma Ray\n\tCaliper\n\n\tPorosity Logs\n\tSonic\n\tDensity\n\tNeutron\n\n\tResistivity\n\tInduction\n\tLaterolog\n\tMicroresistivity\n\n\n\tDescription of The Testing Data\n\tTesting Data Analysis\n\n\tMethodology\n\tDetecting Intervals\n\tDetermining Lithology\n\n\tResults\n\tConclusion\n\tReferences\n\tAnnex 1\n\tAnnex 2"}]}}}