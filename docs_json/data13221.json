{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.17800"}, {"@name": "filename", "#text": "24607_Dorini_FabioAntonio_D.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "ii\n\n\n\niii\n\n\n\nInstituto de Matema?tica, Estat??stica e Computac?a?o Cient??fica\n\nUniversidade Estadual de Campinas\n\nMe?todos para Equac?o?es do Transporte\n\ncom Dados Aleato?rios\n\nFabio Antonio Dorini1\n\nDezembro de 2007\n\nBanca Examinadora:\n\n\u2022 Profa. Dra. Maria Cristina de Castro Cunha (Orientadora)\n\n\u2022 Prof. Dr. He?lio Pedro Amaral Souto - IPRJ/UERJ\n\n\u2022 Prof. Dr. Jose? Alberto Cuminato - ICMC/USP\n\n\u2022 Prof. Dr. Marcelo Martins dos Santos - IMECC/UNICAMP\n\n\u2022 Prof. Dr. Lu?cio Tunes dos Santos -IMECC/UNICAMP\n\n1Suporte financeiro de: bolsa do CNPq (processo 140406/2004-2) Mar/2004\u2013Dez/2007;\ne bolsa de Doutorado Sandu??che do CNPq (processo 210132/2006-0) Jun/2007\u2013Nov/2007.\n\nv\n\n\n\nResumo\n\nModelos matema?ticos para processos do mundo real frequ?entemente te?m a forma de sis-\n\ntemas de equac?o?es diferenciais parciais. Estes modelos usualmente envolvem para?metros\n\ncomo, por exemplo, os coeficientes no operador diferencial, e as condic?o?es iniciais e de\n\nfronteira. Tipicamente, assume-se que os para?metros sa?o conhecidos, ou seja, os mode-\n\nlos sa?o considerados determin??sticos. Entretanto, em situac?o?es mais reais esta hipo?tese\n\nfrequ?entemente na?o se verifica dado que a maioria dos para?metros do modelo possui uma\n\ncaracter??stica aleato?ria ou estoca?stica. Modelos avanc?ados costumam levar em consi-\n\nderac?a?o esta natureza estoca?stica dos para?metros. Em vista disso, certos componentes\n\ndo sistema sa?o modelados como varia?veis aleato?rias ou func?o?es aleato?rias. Equac?o?es di-\n\nferenciais com para?metros aleato?rios sa?o chamadas equac?o?es diferenciais aleato?rias (ou\n\nestoca?sticas). Novas metodologias matema?ticas te?m sido desenvolvidas para lidar com\n\nequac?o?es diferenciais aleato?rias, entretanto, este problema continua sendo objeto de estudo\n\nde muitos pesquisadores. Assim sendo, e? importante a busca por novas formas (nume?ricas\n\nou anal??ticas) de tratar equac?o?es diferenciais aleato?rias. Durante a realizac?a?o do curso de\n\ndoutorado, vislumbrando a possibilidade de aplicac?o?es futuras em problemas de fluxo de\n\nfluidos em meios porosos (dispersa?o de poluentes e fluxos bifa?sicos, por exemplo), desen-\n\nvolvemos trabalhos relacionados a? equac?a?o do transporte linear unidimensional aleato?ria\n\ne ao problema de Burgers-Riemann unidimensional aleato?rio. Nesta tese, apresentamos\n\numa nova metodologia, baseada nas ide?ias de Godunov, para tratar a equac?a?o do trans-\n\nporte linear unidimensional aleato?ria e desenvolvemos um eficiente me?todo nume?rico para\n\nos momentos estat??sticos da equac?a?o de Burgers-Riemann unidimensional aleato?ria. Para\n\nfinalizar, apresentamos tambe?m novos resultados para o caso multidimensional: mostra-\n\nmos que algumas metodologias propostas para aproximar a me?dia estat??stica da soluc?a?o\n\nda equac?a?o do transporte linear multidimensional aleato?ria podem ser va?lidas para todos\n\nos momentos estat??sticos da soluc?a?o.\n\nvii\n\n\n\nAbstract\n\nMathematical models for real-world processes often take the form of systems of partial\n\ndifferential equations. Such models usually involve certain parameters, for example, the\n\ncoefficients in the differential operator, and the initial and boundary conditions. Usually,\n\nall the model parameters are assumed to be known exactly. However, in realistic situati-\n\nons many of the parameters may have a random or stochastic character. More advanced\n\nmodels must take this stochastic nature into account. In this case, the components of\n\nthe system are then modeled as random variables or random fields. Differential equations\n\nwith random parameters are called random (or stochastic) differential equations. New\n\nmathematical methods have been developed to deal with this kind of problem, however,\n\nsolving this problem is still the goal of several researchers. Thus, it is important to look\n\nfor new approaches (numerical or analytical) to deal with random differential equations.\n\nThroughout the realization of the doctorate and looking toward future applications in\n\nporous media flow (pollution dispersal and two phase flows, for instance) we developed\n\nworks related to the one-dimensional random linear transport equation and to the one-\n\ndimensional random Burgers-Riemann problem. In this thesis, based on Godunov\u2019s ideas,\n\nwe present a new methodology to deal with the one-dimensional random linear transport\n\nequation, and develop an efficient numerical scheme for the statistical moments of the\n\nsolution of the one-dimensional random Burgers-Riemann problem. Finally, we also pre-\n\nsent new results for the multidimensional case: we have shown that some approaches to\n\napproximate the mean of the solution of the multidimensional random linear transport\n\nequation may be valid for all statistical moments of the solution.\n\nix\n\n\n\nAgradecimentos\n\nA? minha esposa Leyza, por estar sempre ao meu lado, compartilhando momentos alegres\n\ne dif??ceis e, acima de tudo, me incentivando a sempre acreditar em mim mesmo.\n\nA? minha orientadora, Profa. Cristina Cunha, pelo incentivo e apoio prestados.\n\nAo Prof. Fred Furtado, University of Wyoming, pelo apoio e incentivo durante a realizac?a?o\n\ndo Programa de Doutorado Sandu??che no Exterior.\n\nAo Prof. Lu?cio Tunes dos Santos pelo apoio e ajuda durante a realizac?a?o deste trabalho.\n\nAo IMECC e a? UNICAMP, pela estrutura e ambiente.\n\nA todos que de alguma forma contribu??ram para este trabalho: fam??lia, professores, colegas\n\ne funciona?rios do IMECC.\n\nAo CNPq pelo excelente suporte financeiro.\n\nxi\n\n\n\nSuma?rio\n\nResumo vii\n\nAbstract ix\n\nAgradecimentos xi\n\n1 Introduc?a?o 1\n\n1.1 Motivac?a?o e objetivos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n\n1.2 Organizac?a?o da tese . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2.1 Cap??tulo 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2.2 Cap??tulo 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2.3 Cap??tulo 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2.4 Cap??tulo 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n1.2.5 Cap??tulo 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n1.2.6 Cap??tulo 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n2 A note on the Riemann problem for the random transport equation 7\n\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n2.2 The Riemann problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n2.3 Monte Carlo simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n2.4 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n\n3 A finite volume method for the mean of the solution of the random\n\nlinear transport equation 21\n\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n3.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n3.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 27\n\n3.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n3.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\nxiii\n\n\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n4 A numerical scheme for the variance of the solution of the random linear\n\ntransport equation 35\n\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n4.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n4.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 40\n\n4.4 Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n4.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n5 Statistical moments of the random linear transport equation 47\n\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n\n5.2 The numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n\n5.3 Numerical analysis of the scheme . . . . . . . . . . . . . . . . . . . . . . . 52\n\n5.3.1 The Normal case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n\n5.4 The system of partial differential equations for the central moments . . . . 53\n\n5.5 Computational tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n5.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n6 Statistical moments of the solution of the random Burgers-Riemann\n\nproblem 63\n\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n6.2 The random solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n6.3 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n6.4 Computational tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n\n6.5 Concluding remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n\n7 On the evaluation of moments for solute transport by random velocity\n\nfields 79\n\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n7.2 Main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n\n7.3 First application: Gaussian processes . . . . . . . . . . . . . . . . . . . . . 81\n\n7.3.1 The probability density function . . . . . . . . . . . . . . . . . . . . 82\n\n7.4 Second application: Telegraph processes . . . . . . . . . . . . . . . . . . . 83\n\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n\nxiv\n\n\n\n8 Concluso?es e trabalhos futuros 87\n\n8.1 Concluso?es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n8.2 Trabalhos futuros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n8.2.1 Problema 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n8.2.2 Problema 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n8.2.3 Problema 3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n\nRefere?ncias Bibliogra?ficas 91\n\nxv\n\n\n\nLista de Tabelas\n\n6.1 Illustration of the first step of Algorithm 1 . . . . . . . . . . . . . . . . . . 70\n\n6.2 Absolute errors and CPU times; h = 0.01. . . . . . . . . . . . . . . . . . . 73\n\n6.3 Absolute errors and CPU times; h = 0.01 (600 subintervals). . . . . . . . . 74\n\nxvii\n\n\n\nLista de Figuras\n\n2.1 Interval of dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n2.2 ?U (x, T )?, fixed T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3 A is normal, A ? N (1, 0.6), and T = 0.4. . . . . . . . . . . . . . . . . . . . 17\n2.4 A is normal, A ? N (1, 0.6), and T = 0.8. . . . . . . . . . . . . . . . . . . . 17\n2.5 A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.4. . . . . . . . . . 17\n2.6 A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.8. . . . . . . . . . 17\n\n3.1 ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b). . . . . . . . . . . . . . 30\n\n3.2 ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b). . . . . . . . . . . . . . . 30\n\n3.3 ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b). . . . . . . . . . . . . . . . 30\n\n3.4 ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b). . . . . . . . . . . . . . 31\n\n3.5 ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b). . . . . . . . . . . . . . . 32\n\n3.6 ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b). . . . . . . . . . . . . . . . 32\n\n4.1 ?x = 0.02 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n4.2 ?x = 0.02 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n4.3 ?x = 0.01 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n4.4 ?x = 0.01 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n4.5 ?x = 0.02 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n4.6 ?x = 0.02 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n4.7 ?x = 0.01 and T = 0.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n\n4.8 ?x = 0.01 and T = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n\n5.1 Schematic diagram of the algorithm. . . . . . . . . . . . . . . . . . . . . . 49\n\n5.2 A ? N (1.0, 0.8), ?x = 0.01, ?t = 0.000195, and tf = 0.4. . . . . . . . . . . 57\n5.3 A = exp(?), ? ? N (0.5, 0.35), ?x = 0.01, ?t = 0.000312, and tf = 0.4. . . . 58\n5.4 A ? N (?0.5, 0.6), ?x = 0.02, ?t = 0.000138, and tf = 0.4. . . . . . . . . . 59\n\n6.1 Integration regions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n6.2 Discretization scheme of the ?M square. . . . . . . . . . . . . . . . . . . . 69\n\n6.3 Mean at t = 0.4 (left) and t = 0.8 (right). . . . . . . . . . . . . . . . . . . . 72\n\nxix\n\n\n\n6.4 Approximations to the statistical moments using the Monte Carlo method\n\n(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 74\n\n6.5 Approximations to the statistical moments using the Monte Carlo method\n\n(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 75\n\n6.6 Approximations to the statistical moments using the Monte Carlo method\n\n(with 50 000 realizations), and Algorithm 1 (with N=601). . . . . . . . . . 75\n\n7.1 Mean (left), variance (middle), and third central moment (right) of the\n\nsolution to (7.5); ? = 0.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n7.2 Mean (left), variance (middle), and third central moment (right) of the\n\nsolution to (7.5); ? = 1.0. . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\nxx\n\n\n\nCap??tulo 1\n\nIntroduc?a?o\n\n1.1 Motivac?a?o e objetivos\n\nModelos matema?ticos para problemas f??sicos frequ?entemente te?m a forma de sistemas de\n\nequac?o?es diferenciais parciais. Estes modelos usualmente envolvem para?metros como, por\n\nexemplo, os coeficientes no operador diferencial, e as condic?o?es iniciais e de fronteira.\n\nTipicamente, assume-se que os para?metros sa?o conhecidos, ou seja, os modelos sa?o consi-\n\nderados determin??sticos. Entretanto, em situac?o?es mais reais esta hipo?tese frequ?entemente\n\nna?o se verifica dado que a maioria dos para?metros do modelo possui uma caracter??stica\n\naleato?ria ou estoca?stica. Modelos mais avanc?ados costumam levar em considerac?a?o esta\n\nnatureza estoca?stica dos para?metros. Em vista disso, certos componentes do sistema sa?o\n\nmodelados como varia?veis aleato?rias ou func?o?es aleato?rias. Equac?o?es diferenciais com\n\npara?metros aleato?rios sa?o chamadas equac?o?es diferenciais aleato?rias (ou estoca?sticas).\n\nNovas metodologias matema?ticas te?m sido desenvolvidas para lidar com equac?o?es diferen-\n\nciais aleato?rias (veja [13, 17, 18, 22, 28, 30, 44], por exemplo); entretanto, este problema\n\ncontinua sendo objeto de estudo de muitos pesquisadores.\n\nDentre as va?rias metodologias para tratar equac?o?es diferenciais aleato?rias podemos\n\ncitar os me?todos para equac?o?es dos momentos (veja [28, 44], por exemplo). Nestes\n\nme?todos o objetivo e? obter equac?o?es diferenciais determin??sticas que governem os mo-\n\nmentos estat??sticos da soluc?a?o do problema aleato?rio. A mais relevante destas equac?o?es e?\n\na equac?a?o diferencial para a me?dia (esperanc?a matema?tica) da soluc?a?o, chamada equac?a?o\n\nefetiva. Estas equac?o?es diferenciais determin??sticas sa?o enta?o resolvidas numericamente\n\nou analiticamente. Conve?m ressaltar que tal estrate?gia na?o e? simples de ser aplicada e\n\nos me?todos oriundos desta metodologia possuem inu?meras restric?o?es de validade, con-\n\nsequ?e?ncias de va?rias aproximac?o?es que sa?o necessa?rias durante o processo (veja [44], por\n\nexemplo).\n\nNa maioria dos casos as soluc?o?es nume?ricas de equac?o?es diferenciais parciais com co-\n\n1\n\n\n\n2 Cap??tulo 1\n\neficientes aleato?rios sa?o calculadas (aproximadas) usando o conhecido me?todo de Monte\n\nCarlo (veja [10, 35], por exemplo). Neste caso, os para?metros aleato?rios do modelo sa?o\n\namostrados repetidamente reduzindo o problema a? soluc?a?o de uma equac?a?o diferencial de-\n\ntermin??stica para cada amostra. As propriedades estoca?sticas da soluc?a?o sa?o subsequ?ente-\n\nmente determinadas pela ana?lise estat??stica do conjunto de soluc?o?es obtidas. De um modo\n\ngeral, este me?todo demanda geradores de nu?meros aleato?rios e um nu?mero excessivo de\n\nsimulac?o?es nume?ricas, ou seja, o custo computacional e? alto.\n\nAssim sendo, e? importante a busca por novas te?cnicas (nume?ricas ou anal??ticas) de\n\ntratar equac?o?es diferenciais aleato?rias. Durante a realizac?a?o do curso de doutorado e\n\nvislumbrando a possibilidade de futuras aplicac?o?es em problemas de fluxo de fluidos em\n\nmeios porosos (dispersa?o de poluentes e fluxos bifa?sicos, por exemplo), desenvolvemos\n\ntrabalhos relacionados aos seguintes problemas:\n\n\u2022 a equac?a?o do transporte linear unidimensional aleato?ria:\n\n?\n\n?t\nQ(x, t) + A(t, x)\n\n?\n\n?x\nQ(x, t) = 0, t > 0, x ? R,\n\nQ(x, 0) = Q0(x), (1.1)\n\nonde a velocidade A(t, x) e a condic?a?o inicial sa?o func?o?es aleato?rias. Esta equac?a?o\n\ne? frequ?ente, por exemplo, em problemas de dispersa?o de poluentes no qual a va-\n\nriabilidade da permeabilidade do meio poroso tem como consequ?e?ncia um campo\n\naleato?rio de velocidades, utilizado para calcular a concentrac?a?o do poluente.\n\n\u2022 o problema de Burgers-Riemann unidimensional aleato?rio:\n\n?\n\n?t\nQ(x, t) +\n\n1\n\n2\n\n?\n\n?x\nQ2(x, t) = 0, t > 0, x ? R,\n\nQ(x, 0) =\n\n{\nQL, if x &lt;0,\n\nQR, if x > 0,\n(1.2)\n\nonde QL and QR sa?o varia?veis aleato?rias. Aqui a aleatoriedade aparece somente\n\nna condic?a?o inicial. A versa?o determin??stica de (1.2) foi introduzida por Burgers\n\n[1] como um modelo simplificado para capturar caracter??sticas ba?sicas em dina?mica\n\ndos gases. Mas, em vez de modelar um processo f??sico, a Equac?a?o de Burgers\n\ninv??scida tem sido usada no desenvolvimento de me?todos nume?ricos e teo?ricos para\n\nequac?o?es hiperbo?licas determin??sticas. A investigac?a?o dos momentos estat??sticos da\n\nsoluc?a?o deste problema foi nosso primeiro passo na direc?a?o do tratamento de leis de\n\nconservac?a?o na?o-lineares com para?metros aleato?rios.\n\n\n\n1.2. Organizac?a?o da tese 3\n\n1.2 Organizac?a?o da tese\n\nO texto desta tese esta? organizado de modo a agrupar (cronologicamente) os principais\n\nartigos, publicados e/ou submetidos para publicac?a?o, que foram resultados da pesquisa\n\nrealizada. A seguir apresentamos uma breve relato sobre o conteu?do de cada cap??tulo.\n\n1.2.1 Cap??tulo 2\n\nA note on the Riemann problem for the random transport equation\n\n(Ref. [4]; publicado no journal: Computational &amp; Applied Mathematics).\n\nApresentamos uma expressa?o expl??cita para a soluc?a?o do problema (1.1), com A(t, x) = A\n\numa varia?vel aleato?ria, e com condic?a?o inicial dada por\n\nQ(x, 0) =\n\n{\nQ+0 , x > 0,\n\nQ?0 , x &lt;0,\n\nonde os estados iniciais, Q?0 e Q\n+\n0 , sa?o varia?veis aleato?rias. Este problema e? conhecido\n\ncomo problema de Riemann aleato?rio. Sua soluc?a?o e? fundamental no desenvolvimento\n\nde esquemas nume?ricos com condic?a?o inicial mais geral, via me?todo de Godunov [14, 27]\n\nou me?todo de Glimm [12] (random choice method). Mostramos que esta soluc?a?o (func?a?o\n\naleato?ria) e? de similaridade e, admitindo independe?ncia estat??stica entre a velocidade, A,\n\ne os estados iniciais, Q?0 e Q\n+\n0 , obtemos uma expressa?o para os momentos estat??sticos.\n\n1.2.2 Cap??tulo 3\n\nA finite volume method for the mean of the solution of the random linear\n\ntransport equation (Ref. [8]; publicado no Journal of Applied Mathematics and Com-\n\nputation).\n\nUtilizando as ide?ias do trabalho [4] e o me?todo de Godunov [14], constru??mos um es-\n\nquema nume?rico expl??cito para a me?dia da soluc?a?o da Equac?a?o (1.1), onde A(t, x) = A\n\ne? uma varia?vel aleato?ria e a condic?a?o inicial, Q(x, 0) = Q0(x), e? uma func?a?o aleato?ria.\n\nSob algumas hipo?teses na discretizac?a?o obtemos condic?o?es de estabilidade do me?todo e\n\nmostramos sua consiste?ncia com uma equac?a?o do tipo advecc?a?o-difusa?o determin??stica.\n\nVa?rios exemplos computacionais mostram uma boa concorda?ncia dos resultados quando\n\ncomparados com o me?todo de Monte Carlo.\n\n1.2.3 Cap??tulo 4\n\nA numerical scheme for the variance of the solution of the random linear\n\ntransport equation (Ref. [5]; publicado no Journal of Applied Mathematics and Com-\n\nputation).\n\n\n\n4 Cap??tulo 1\n\nAvanc?ando no conhecimento dos momentos estat??sticos da soluc?a?o de (1.1), com A(t, x) =\n\nA uma varia?vel aleato?ria e Q0(x) uma func?a?o aleato?ria, propomos um esquema nume?rico\n\nexpl??cito para a varia?ncia da soluc?a?o. Obtemos as condic?o?es de estabilidade do me?todo\n\nproposto e mostramos sua consiste?ncia com um sistema determin??stico (para a me?dia e\n\na varia?ncia) de equac?o?es do tipo advecc?a?o-difusa?o na?o-homoge?neo desacoplado. Testes\n\ncomputacionais sa?o apresentados para avaliar a proposta.\n\n1.2.4 Cap??tulo 5\n\nStatistical moments of the random linear transport equation\n\n(Ref. [6]; submetido para o Journal of Computational Physics).\n\nNeste trabalho generalizamos as ide?ias de [5, 8] e apresentamos um esquema nume?rico\n\npara os momentos da soluc?a?o da equac?a?o do transporte linear unidimensional aleato?ria\n\n(1.1), com A(t, x) = A uma varia?vel aleato?ria e Q0(x) uma func?a?o aleato?ria. O esquema\n\ne? baseado na soluc?a?o de problemas de Riemann e no me?todo de Godunov. Mostramos\n\nque o esquema e? consistente e esta?vel com uma equac?a?o do tipo advecc?a?o-difusa?o. Ale?m\n\ndisso, no caso em que a velocidade e? normalmente distribu??da, obtemos um sistema de\n\nequac?o?es diferenciais parciais para os momentos e momentos centrais da soluc?a?o. Testes\n\ncomputacionais sa?o apresentados para avaliar a proposta.\n\n1.2.5 Cap??tulo 6\n\nStatistical moments of the solution of the random Burgers-Riemann problem\n\n(Ref. [7]; submetido para o Journal of Mathematics and Computers in Simulation ).\n\nNeste trabalho apresentamos uma expressa?o para a soluc?a?o da Equac?a?o de Burgers aleato?ria\n\n(1.2). A soluc?a?o aleato?ria permite expresso?es integrais para os momentos estat??sticos da\n\nsoluc?a?o. Usando ide?ias de integrac?a?o nume?rica, propomos um algoritmo eficiente para\n\ncalcular os momentos estat??sticos da soluc?a?o. Testes computacionais sa?o apresentados\n\npara validar a proposta.\n\n1.2.6 Cap??tulo 7\n\nOn the evaluation of moments for solute transport by random velocity fields\n\n(Ref. [9]; submetido para o Journal of Applied Numerical Mathematics).\n\nApresentamos um u?til resultado para a equac?a?o do transporte linear aleato?ria multidi-\n\nmensional. Basicamente, provamos que algumas metodologias baseadas em averaging\n\napproach para aproximar a me?dia estat??stica da soluc?a?o da equac?a?o do transporte linear\n\naleato?ria (Equac?a?o 1.1, multidimensional) podem ser va?lidas para todos os momentos\n\n\n\n1.2. Organizac?a?o da tese 5\n\nestat??sticos da soluc?a?o. Com este resultado podemos obter mais informac?o?es estat??sticas\n\nsobre a soluc?a?o aleato?ria, como ilustrado em dois exemplos particulares.\n\n\n\nCap??tulo 2\n\nA note on the Riemann problem for\n\nthe random transport equation\n\nAbstract\n\nWe present an explicit expression to the solution of the random Riemann problem for the\n\none-dimensional random linear transport equation. We show that the random solution is a\n\nsimilarity solution and the statistical moments have very simple expressions. Furthermore,\n\nwe verify that the mean, variance, and 3rd central moment agree quite well with the Monte\n\nCarlo method. We point out that our approach could be useful in designing numerical\n\nmethods for more general random transport problems.\n\nKeyword: random linear transport equation, Riemann problem, statistical moments.\n\n2.1 Introduction\n\nConservation laws are differential equations arising from physical principles of the con-\n\nservation of mass, energy or momentum. The simplest of these equations is the one-\n\ndimensional advective equation and its solution plays a role in more complex problems\n\nsuch as the numerical solution of nonlinear conservation laws [6]. This linear initial value\n\nproblem can, for instance, model the concentration, or density, of a chemical substance\n\ntransported by a one dimensional fluid that flows with a known velocity. The deterministic\n\nproblem is to find u(x, t) such that\n{\n\nut + a(x)ux = 0, t > 0, x ? R,\nu(x, 0) = u0(x).\n\n(2.1)\n\nIt is well known that the solution to (2.1) is the initial condition transported along the\n\ncharacteristic curves. The characteristic system associated to (2.1) is defined by ordinary\n\n7\n\n\n\n8 Cap??tulo 2\n\ndifferential equations:\n?\n???\n???\n\ndx\n\ndt\n= a(x), x(0) = x0,\n\nd[u(x(t), t)]\n\ndt\n= 0, u(x, 0) = u0(x),\n\n(2.2)\n\nwhere the last equation is along the characteristic curve, x(t), given by the first equation.\n\nIf a is constant, the characteristics are straight lines and the analytic solution is u(x, t) =\n\nu0(x ? at).\nThe complexity of natural phenomena compels us to study partial differential equa-\n\ntions with random data. For example, (2.1) may model the flux of a two phase equal\n\nviscosity miscible fluid in a porous media. The total velocity is obtained from Darcy\u2019s law\n\nand it depends on the geology of the porous media. Thus, the external velocity is defined\n\nby a given statistics. Also, the prediction of the initial state of the process is obtained\n\nfrom data acquired with a few number of exploratory wells using geological methods.\n\nOur aim in this paper is to study the random Riemann problem:\n?\n??\n??\n\nUt + AUx = 0, t > 0, x ? R,\n\nU (x, 0) = U0(x) =\n\n{\nU +0 , x > 0,\n\nU?0 , x &lt;0,\n\n(2.3)\n\nwhere A, U?0 and U\n+\n0 are random variables.\n\nSeveral numerical methods which were developed to solve the deterministic problem\n\n(2.1) use solutions of Riemann problems. For instance, the Random Choice Method,\n\ndeveloped by Glimm [2], and the Godunov\u2019s method [4, 6]. These methods suggest that\n\nthe random Riemann solutions can be used for designing numerical methods to random\n\ntransport equations, where the velocity and the initial condition are random fields. Our\n\npreliminary results in this direction, i.e., using Godunov\u2019s method with random Riemann\n\nsolutions in the averaging step, are promising.\n\nBesides the well-developed theoretical methods such as Ito integrals, Martingales and\n\nWiener measure [5, 7, 9, 10] to deal with stochastic differential equations, two types of\n\nmethods are normally used in the construction of solutions for random partial differential\n\nequations. The first is based on the Monte Carlo method which, in general, demands\n\nmassive numerical simulations (see [8], for example), and the second is based on effective\n\nequations (see [3], for example), deterministic differential equations whose solutions are\n\nthe statistical means of (2.3).\n\nIt is well known that, for each realization A(?) and U0(x, ?), of A and U0(x), re-\n\nspectively, one has a deterministic problem that can be solved analytically using the\n\ncharacteristics\u2019 method. Therefore, if the probability of the realizations is known then\n\nthe random solution U (x, t, ?), and its probability, can be found analytically.\n\n\n\n2.2. The Riemann problem 9\n\nOn the other hand, if we have precise information about the velocity we may consider\n\na mixed deterministic-random version for (2.2):\n\n?\n???\n???\n\ndx\n\ndt\n= a, x(0) = x0,\n\nd[U (x(t), t)]\n\ndt\n= 0, U (x, 0) = U0(x).\n\n(2.4)\n\nThis mixed formulation gives the characteristic straight lines x(t) = x0 + at and a\n\nrandom ordinary differential equation along these straight lines. The formulation (2.4) is\n\nconvenient to our future arguments because, for each realization U0(x, ?) of U0(x), the\n\nrandom function (x, t) 7? U (x, t, ?) = U0(x ? at, ?) solves (2.4). This means that for\nprecise values of the velocity the random initial conditions are \u201ctransported\u201d along the\n\nstraight lines.\n\nIn this paper, we use (2.4) to find the random Riemann solution to (2.3). The pro-\n\ncedure and the theoretical consequences are presented in Section 2.2. In Section 2.3 we\n\nassess our results by comparing them with the Monte Carlo method.\n\n2.2 The Riemann problem\n\nIn this section we study the random Riemann initial value problem:\n\n?\n????\n????\n\ndX\n\ndt\n= A, X(0) = x0,\n\nd[U (X, t)]\n\ndt\n= 0, U (x, 0) =\n\n{\nU +0 , x > 0,\n\nU?0 , x &lt;0,\n\n(2.5)\n\nwhere A, U?0 and U\n+\n0 are random variables. We assume the statistical independence of\n\nA and both U?0 and U\n+\n0 , and that their cumulative probability functions, FA and FU?U + ,\n\nare known.\n\nIn our approach we focus on partial realizations in (2.5), i.e., we consider only A(?),\n\nletting the data U?0 and U\n+\n0 out of the realizations. This kind of decoupling of the system\n\n(2.5) allows us to use the solution of (2.4). To simplify, let us consider A continuously\n\nvarying in some interval [am, aM ], am &lt;aM .\n\nWe recall that each realization A(?) yields the random function (x, t) 7? U0(x?A(?)t),\ni.e., the initial condition at x0 = x ? A(?)t. Also, as illustrated in Figure 2.1, for a fixed\n(x?, t? ) we have x? ? aM t? ? x0 ? x? ? amt?. Hence, the solution at (x?, t? ) depends upon\nthe initial data in the interval [ x? ? aM t?, x? ? amt? ]. As shown in Figure 2.1, this interval\nis determined by two characteristics x ? aM t = constant and x ? amt = constant, both\n\n\n\n10 Cap??tulo 2\n\nx ? aM t = constant\n\nx? ? amt? xx? ? aM t?\n0\n\n1\n\n?\n1\n\naM\n\n1\n\nam\n\n(x?, t?)\n\nx ? amt = constant\n\nFigure 2.1: Interval of dependence\n\npassing through (x?, t? ). From now on the interval [ x? ? aM t?, x? ? amt? ] will be referred to\nas the interval of dependence of the point (x?, t? ).\n\nTo separate the contributions of the left state, U?0 , and right state, U\n+\n0 , to the solution\n\nat (x?, t? ), we shall call ? = x?/t? and define the following disjoint subsets of [am, aM ]:\n\nM? = {a; xa = x? ? at? &lt;0} and M + = {a; xa = x? ? at? > 0} .\n\nComparing the slopes of the characteristics (see Figure 2.1), we can rewrite these sets\n\nas\n\nM? =\n{\n\na;\n1\n\naM\n? 1\n\na\n<\n\n1\n\n?\n\n}\n= {a; ? &lt;a ? aM}\n\nand\n\nM + =\n\n{\na;\n\n1\n\n?\n<\n\n1\n\na\n? 1\n\nam\n\n}\n= {a; am ? a &lt;?} .\n\nThus, the probability of occurrence of the sets M + and M? can be calculated using\nthe cumulative probability function of the velocity:\n\nP (M +) = FA(?) = ? and P (M\n?) = 1 ? FA(?) = 1 ? ?. (2.6)\n\nThe solution to (2.5) is given by the following result:\n\nProposition 2.1. Let (x?, t? ), t? > 0, be an arbitrary point and ? = x?/t?. The solution to\n\n(2.5) at (x?, t? ) is the random variable\n\nU (x?, t? ) = (1 ? X)U?0 + XU +0 = U?0 + X\n(\nU +0 ? U?0\n\n)\n, (2.7)\n\nwhere X is the Bernoulli random variable with P (X = 0) = 1 ? FA(?) and P (X = 1) =\nFA(?).\n\n\n\n2.2. The Riemann problem 11\n\nProof. To prove this proposition we use the characteristics x ? amt = 0 and x ? aM t =\n0 to divide the semi-plane t ? 0 into three regions, R1 = {(x, t); x &lt;amt}, R2 =\n{(x, t); amt ? x ? aM t}, and R3 = {(x, t); x > aM t}, and we demonstrate (2.7) for each\none of this regions.\n\nIf (x?, t? ) ? R2, we may divide the interval of dependence into two sub-intervals: I? =\n[ x? ? aM t?, 0 ) and I+ = [ 0, x? ? amt? ]. In a realization such that x0 = x? ? A(?)t? ? I?,\nonly the left state will contribute to the solution. On the other hand, we also conclude\n\nthat x0 = x? ? A(?)t? ? I? if and only if A(?) ? M?, and therefore the probability\nof occurrence of I? is equal to the probability of occurrence of M?. Thus, from (2.6)\nit follows that P (I?) = P (M?) = 1 ? FA(?). Otherwise, in a realization such that\nx0 = x? ? A(?)t? ? I+, the contribution will be due only to the right state and we use\nthe same arguments above to conclude that P (I+) = P (M +) = FA(?). Finally, taking in\n\naccount the probability of occurrence of U?0 and U\n+\n0 , the solution is obtained \u201cweighting\u201d\n\ntheir respective probabilities, i.e., U (x?, t? ) = (1 ? X)U?0 + XU +0 , where X is the Bernoulli\nrandom variable with P (X = 1) = FA(?) and P (X = 0) = 1 ? FA(?).\nIf (x?, t? ) ? R1 then x? ? amt? &lt;0 and all the points of the interval of dependence,\n[ x? ? aM t?, x? ? amt? ], are negatives. Therefore, only the left state contributes to the so-\nlution, i.e., U (x?, t? ) = U?0 with probability one. In this case the solution is (2.7) with\nFA(?) = 0. On the other hand, if (x?, t? ) ? R3 only the right state contributes to the\nsolution and we have U (x?, t? ) = U +0 with probability one, i.e., (2.7) with FA(?) = 1.\n\nCorollary 2.1. The solution of (2.5) is constant along the rays x/t = constant, i.e., the\n\nrandom solution is a similarity function.\n\nProof. This result follows directly from (2.7) since if x/t = constant then FA (x/t) =\n\nconstant.\n\nProposition 2.2. If (x, t) is fixed, n ? N, n ? 1, and we assume the statistical indepen-\ndence of A and both U?0 and U\n\n+\n0 , then the nth moment of the random solution (2.7) is\n\ngiven by:\n\n?U n(x, t)? =\n?\n(U?0 )\n\nn\n?\n\n+ FA\n\n(x\nt\n\n) {?\n(U +0 )\n\nn\n?\n?\n\n?\n(U?0 )\n\nn\n?}\n\n. (2.8)\n\nProof. From Proposition 2.1,\n\nU (x, t) = U?0 + X\n(\nU +0 ? U?0\n\n)\n= (1 ? X)U?0 + XU +0 ,\n\n\n\n12 Cap??tulo 2\n\nwhere X = X(x, t) is the Bernoulli random variable:\n\nX =\n\n{\n1, P (X = 1) = FA\n\n(\nx\nt\n\n)\n= ?\n\n0, P (X = 0) = 1 ? FA\n(\n\nx\nt\n\n)\n= 1 ? ?.\n\nIt is easy to see that ?Xj? = FA\n(\n\nx\nt\n\n)\n= ?, for all j = 1, 2, 3, ....\n\nTo prove (2.8) we first need the following results:\n\n\u2022 For all n ? 1,\nn?\n\nj=0\n\n(?1)j\n(\n\nn\n\nj\n\n)\n= 0, (2.9)\n\nwhere\n\n(\nn\n\nj\n\n)\nis the binomial coefficient.\n\n\u2022 For n ? 1 and 1 ? j ? n ? 1,\n\n?\n(1 ? X)n?j Xj\n\n?\n= (2.10)\n\n=\n\n?\nXj\n\nn?j?\nm=0\n\n(?1)m\n(\n\nn ? j\nm\n\n)\nXm\n\n?\n=\n\n?\nn?j?\nm=0\n\n(?1)m\n(\n\nn ? j\nm\n\n)\nXm+j\n\n?\n=\n\n=\n\nn?j?\nm=0\n\n(?1)m\n(\n\nn ? j\nm\n\n) ?\nXm+j\n\n?\n? ?? ?\n\n?\n\n= ?\n\nn?j?\nm=0\n\n(?1)m\n(\n\nn ? j\nm\n\n)\n\n? ?? ?\nzero by (2.9)\n\n= 0.\n\n\u2022 For n ? 1,\n\n?(1 ? X)n? =\n?\n\nn?\nj=0\n\n(?1)j\n(\n\nn\n\nj\n\n)\nXj\n\n?\n= (2.11)\n\n= 1 +\nn?\n\nj=1\n\n(?1)j\n(\n\nn\n\nj\n\n) ?\nXj\n\n?\n? ?? ?\n\n?\n\n= 1 + ?\nn?\n\nj=1\n\n(?1)j\n(\n\nn\n\nj\n\n)\n\n? ?? ?\n?1 by (2.9)\n\n= 1 ? ?.\n\n\n\n2.2. The Riemann problem 13\n\nNow, assuming the independence of A and both U?0 and U\n+\n0 , we have:\n\n?U n(x, t)? =\n?[\n\n(1 ? X)U?0 + XU +0\n]n?\n\n=\n\n=\n\n?\nn?\n\nj=0\n\n(\nn\n\nj\n\n)\n(1 ? X)n?jXj\n\n(\nU?0\n\n)n?j (\nU +0\n\n)j\n?\n\n=\n\n= ?(1 ? X)n?? ?? ?\n1?? by (2.11)\n\n?(\nU?0\n\n)n?\n+ ?Xn?? ?? ?\n\n?\n\n?(\nU +0\n\n)n?\n+\n\n+\nn?1?\nj=1\n\n(\nn\n\nj\n\n) ?\n(1 ? X)n?jXj\n\n?\n? ?? ?\n\nzero by (2.10)\n\n?(\nU?0\n\n)n?j (\nU +0\n\n)j?\n=\n\n= (1 ? ?)\n?(\n\nU?0\n)n?\n\n+ ?\n?(\n\nU +0\n)n?\n\n.\n\nCorollary 2.2. For a fixed (x, t), ? = FA (x/t), and considering the independence of A\n\nand both U?0 and U\n+\n0 , the mean of the solution (2.5) is\n\n?U (x, t)? = (1 ? ?)?U?0 ? + ??U +0 ? = ?U?0 ? + ?\n[\n?U +0 ? ? ?U?0 ?\n\n]\n. (2.12)\n\nProof. The expression (2.12) follows from (2.8) with n = 1.\n\nProposition 2.3. Let (x1, t1) and (x2, t2) be fixed. Define ?j = xj/tj, ?j = FA(?j), and\n\nH(?j) = U (xj, tj) as in (2.7) (j = 1, 2). Also, consider the statistical independence of A\n\nand both U?0 and U\n+\n0 . If ?1 6= ?2 then the covariance between H(?1) and H(?2) is\n\nCov[H(?1), H(?2)] = (1 ? ?1)(1 ? ?2)V ar[U?0 ] + ?1?2V ar[U +0 ]+\n+ {?1(1 ? ?2) + ?2(1 ? ?1)} Cov[U?0 , U +0 ]. (2.13)\n\nOn the other hand, if ?1 = ?2 then the variance is given by\n\nV ar[H(?1)] = V ar[U (x1, t1)] = V ar[U\n?\n0 ] + ?1\n\n{\nV ar[U +0 ] ? V ar[U?0 ]\n\n}\n+\n\n+ ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2. (2.14)\n\nProof. At first, from (2.7) and (2.12) we can observe that\n\n?H(?1)??H(?2)? =\n{\n?U?0 ? + ?1[?U +0 ? ? ?U?0 ?]\n\n} {\n?U?0 ? + ?2[?U +0 ? ? ?U?0 ?]\n\n}\n=\n\n= ?U?0 ?2 + (?1 + ?2)?U?0 ?[?U +0 ? ? ?U?0 ?] + ?1?2[?U +0 ? ? ?U?0 ?]2, (2.15)\n\n\n\n14 Cap??tulo 2\n\nand\n\n?H(?1)H(?2)? =\n?{\n\nU?0 + X(?1)[U\n+\n0 ? U?0 ]\n\n} {\nU?0 + X(?2)[U\n\n+\n0 ? U?0 ]\n\n}?\n=\n\n= ?(U?0 )2? +\n?\n{X(?1) + X(?2)}\n\n{\nU?0 [U\n\n+\n0 ? U?0 ]\n\n}?\n+\n\n+ ?X(?1)X(?2)??[U +0 ? U?0 ]2?,\nwhere X(?j) (j=1,2) is the Bernoulli random variable defined in (2.7).\n\nSince\n\nV ar[U +0 ? U?0 ] = V ar[U?0 ] + V ar[U +0 ] ? 2Cov[U?0 , U +0 ] =\n= ?[U +0 ? U?0 ]2? ? [?U +0 ? ? ?U?0 ?]2,\n\nwe have\n\n?H(?1)H(?2)? = ?(U?0 )2? + (?1 + ?2)?U?0 [U +0 ? U?0 ]? + ?X(?1)X(?2)?[?U +0 ? ? ?U?0 ?]2+\n+ ?X(?1)X(?2)?\n\n{\nV ar[U?0 ] + V ar[U\n\n+\n0 ] ? 2Cov[U?0 , U +0 ]\n\n}\n. (2.16)\n\nFrom (2.15) and (2.16) it follows that\n\nCov[H(?1), H(?2)] = ?H(?1)H(?2)? ? ?H(?1)??H(?2)? =\n= V ar[U?0 ] + (?1 + ?2)\n\n{\n?U?0 [U +0 ? U?0 ]? ? ?U?0 ?[?U +0 ? ? ?U?0 ?]\n\n}\n+\n\n+ ?X(?1)X(?2)?\n{\nV ar[U?0 ] + V ar[U\n\n+\n0 ] ? 2Cov[U?0 , U +0 ]\n\n}\n+\n\n+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2 =\n= V ar[U?0 ] + (?1 + ?2)\n\n{\nCov[U?0 , U\n\n+\n0 ] ? V ar[U?0 ]\n\n}\n+\n\n+ ?X(?1)X(?2)?\n{\nV ar[U?0 ] + V ar[U\n\n+\n0 ] ? 2Cov[U?0 , U +0 ]\n\n}\n+\n\n+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2,\nor, equivalently,\n\nCov[H(?1), H(?2)] = {1 ? ?1 ? ?2 + ?X(?1)X(?2)?} V ar[U?0 ] + ?X(?1)X(?2)?V ar[U +0 ]+\n+ {?1 + ?2 ? 2?X(?1)X(?2)?} Cov[U?0 , U +0 ]+\n+ {?X(?1)X(?2)? ? ?1?2} [?U +0 ? ? ?U?0 ?]2. (2.17)\n\nNow we can observe that\n\n?X(?1)X(?2)? =\n{\n\n?1?2 if ?1 6= ?2,\n?1 if ?1 = ?2.\n\n(2.18)\n\nTherefore, if ?1 6= ?2 we have from (2.17) and (2.18):\nCov[H(?1), H(?2)] = {1 ? ?1 ? ?2 + ?1?2?} V ar[U?0 ] + ?1?2V ar[U +0 ]+\n\n+ {?1 + ?2 ? 2?1?2} Cov[U?0 , U +0 ] =\n= (1 ? ?1)(1 ? ?2)V ar[U?0 ] + ?1?2V ar[U +0 ]+\n+ {?1(1 ? ?2) + ?2(1 ? ?1)} Cov[U?0 , U +0 ].\n\n\n\n2.2. The Riemann problem 15\n\nOtherwise, if ?1 = ?2 it follows that\n\nV ar[H(?1)] = [1 ? ?1]V ar[U?0 ] + ?1V ar[U +0 ] + ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2\n= V ar[U?0 ] + ?1\n\n{\nV ar[U +0 ] ? V ar[U?0 ]\n\n}\n+ ?1(1 ? ?1)[?U +0 ? ? ?U?0 ?]2.\n\nAs an illustration we plot in Figure 2.2 the mean of the solution at t = T , ?U (x, T )?,\nusing (2.12). We can observe a diffusive behavior in the interval [amT, aM T ], called by\n\nsome authors the mixing zone. In this mixing zone, ?U (x, T )? is the mean of the left state\nadded to the product of the cumulative probability function of the velocity and the jump\n\nbetween the means of right and left states. This illustration also shows that the shape of\n\nthe cumulative probability function of A controls the mixing zone: only symmetric density\n\nfunctions will produce antisymmetrical mixing zones. Our computational tests will make\n\nclear this remark (see Figures 2.3-2.4 (symmetric) and Figures 2.5-2.6 (nonsymmetric)).\n\n?U\n?\n0\n?\n\namT 0 aM T x\n\n?U\n+\n\n0\n?\n\nFigure 2.2: ?U (x, T )?, fixed T\n\nThe length of this mixing zone is studied by some authors (see [1, 3, 11, 12], for\n\nexample) using the effective equation methodology. For instance, the effective equation\n\nfor the linear transport with random velocity, ?(x), is\n\n??c?\n?t\n\n+ ??(x)? ??c?\n?x\n\n? D(t) ?\n2?c?\n?x2\n\n= 0,\n\nwith the dissipation coefficient given by\n\nD(t) =\n\n? t\n0\n\n???(x ? st)??(x)?ds.\n\nIf the random velocity is constant then\n\nD(t) =\n\n? t\n0\n\n???2?ds = ?2t,\n\n\n\n16 Cap??tulo 2\n\nwhere ? is the standard deviation of ?.\n\nWe shall compare a particular solution of the effective equation methodology with our\n\nexpression for the mean, (2.12). If we take the initial condition\n\n?c(x, 0)? = U0(x) =\n{\n\n1, x &lt;0,\n\n0, x > 0,\n\nfor both the effective equation and problem (2.5), we can show that the analytical expres-\n\nsions for the mean are:\n\n(i) using the effective equation:\n\n?c(x, t)? = 1\n2\n\n{\n1 ? 2?\n\n?\n\n? x????t\nl(t)\n\n0\n\ne??\n2\n\nd?\n\n}\n,\n\nwhere l(t) = 2\n\n[? t\n0\n\nD(?) d?\n\n] 1\n2\n\nis the mixing length;\n\n(ii) using (2.12) with a normally distributed random velocity, A ? N (???, ?):\n\n?U (x, t)? = 1\n2\n\n{\n1 ? 2?\n\n?\n\n? x????t?\n2?t\n\n0\n\ne??\n2\n\nd?\n\n}\n.\n\nComparing these expressions, they will be equal only if the mixing length satisfies\n\nl(t) =\n?\n\n2?t or, equivalently, if the diffusion coefficient of the effective equation is D(t) =\n\n?2t, i.e., the same dissipation coefficient for the constant velocity case.\n\n2.3 Monte Carlo simulations\n\nTo assess our results we compare the expressions for the mean, variance and 3rd central\n\nmoment with the Monte Carlo method. We use suites of realizations of A, U?0 and U\n+\n0\n\nconsidering: the independence of A and both U?0 and U\n+\n0 ; U\n\n?\n0 and U\n\n+\n0 have a bivariate\n\nnormal distribution with ?U?0 ? = 1, ?U +0 ? = 0, V ar\n[\nU?0\n\n]\n= 0.16, V ar\n\n[\nU +0\n\n]\n= 0.25 and\n\nCov\n(\nU?0 , U\n\n+\n0\n\n)\n= 0.12. We plot the results in T = 0.4 and T = 0.8. In order to investigate\n\nthe influence of the velocity randomness we use two models: (i) A is normally distributed,\n\nA ? N (1, 0.6), in Figures 2.3 and 2.4; (ii) A is lognormally distributed, A = exp (?),\n? ? N (0.5, 0.15), in Figures 2.5 and 2.6. All Monte Carlo simulations were performed\nwith 1 500 realizations and recalling that the solution to (2.5), at (x, t), for a single\n\nrealization\n(\nA(?), U?0 (?), U\n\n+\n0 (?)\n\n)\nof\n\n(\nA, U?0 , U\n\n+\n0\n\n)\n, is\n\nU (x, t) = U0(x ? A(?)t) =\n{\n\nU?0 (?), x ? A(?)t &lt;0,\nU +0 (?), x ? A(?)t > 0.\n\nAll the numerical experiments presented in this section were computed in double precision\n\nwith some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of memory.\n\n\n\n2.3. Monte Carlo simulations 17\n\n?2 0 2 4 6\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\nmean of solution\n\nMonte Carlo\nproposed\n\n?2 0 2 4 6\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\nvariance of solution\n\n?2 0 2 4 6\n?0.2\n\n?0.1\n\n0\n\n0.1\n\n0.2\n3rd central moment of solution\n\nFigure 2.3: A is normal, A ? N (1, 0.6), and T = 0.4.\n\n?2 0 2 4 6\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\nmean of solution\n\nMonte Carlo\nproposed\n\n?2 0 2 4 6\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\nvariance of solution\n\n?2 0 2 4 6\n?0.2\n\n?0.1\n\n0\n\n0.1\n\n0.2\n3rd central moment of solution\n\nFigure 2.4: A is normal, A ? N (1, 0.6), and T = 0.8.\n\n?2 0 2 4 6\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\nmean of solution\n\nMonte Carlo\nproposed\n\n?2 0 2 4 6\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\nvariance of solution\n\n?2 0 2 4 6\n?0.2\n\n?0.1\n\n0\n\n0.1\n\n0.2\n3rd central moment of solution\n\nFigure 2.5: A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.4.\n\n?2 0 2 4 6\n?0.5\n\n0\n\n0.5\n\n1\n\n1.5\nmean of solution\n\nMonte Carlo\nproposed\n\n?2 0 2 4 6\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\nvariance of solution\n\n?2 0 2 4 6\n?0.2\n\n?0.1\n\n0\n\n0.1\n\n0.2\n3rd central moment of solution\n\nFigure 2.6: A is lognormal, A = exp (?), ? ? N (0.5, 0.15), and T = 0.8.\n\n\n\n18 References\n\n2.4 Concluding remarks\n\nIn this article we present an expression to the solution of the random Riemann problem\n\nfor the linear transport equation with random velocity. As far as we know, this approach\n\ndoes not appear in the literature and we believe that it can be useful in the development of\n\nnumerical procedures for more general random partial differential equations. Expression\n\n(2.7) shows us that if the statistics of the velocity is known then the local behavior of the\n\nsolution is independent of the physical mechanisms governing the process. The procedure\n\nalso shows agreement with the effective equation methodology when the velocity is a\n\nnormal random variable; however, it seems to us that the random expression to the\n\nsolution yields more information about the process.\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) through the grants 5551463/02-3 and 140406/2004-2.\n\nReferences\n\n[1] F. Furtado, F. Pereira, Scaling analysis for two-phase immiscible flow in heteroge-\n\nneous porous media. Computational and Applied Mathematics, 17(3):237\u2013263 (1998).\n\n[2] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.\n\nPure Appl. Math., 18:695\u2013715 (1965).\n\n[3] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03\u201344, Providence, 1998.\n\n[4] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntion of the equations of hydrodynamics. Mat. Sb., 47:271\u2013306 (1959).\n\n[5] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.\n\nSpringer, New York, 1999.\n\n[6] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n\n\nReferences 19\n\n[7] B. Oksendal, Stochastic Differential Equations: an introduction with applications.\n\nSpringer, New York, 2000.\n\n[8] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-\n\ndiretional stochastic transport equation. SIAM Journal on Scientific Computing,\n\n19(3):799\u2013812 (1998).\n\n[9] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New\n\nYork, 1985.\n\n[10] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic\n\nPress, New York, 1973.\n\n[11] Q. Zhang, The asymptotic scaling behavior of mixing induced by a random velocity\n\nfield. Adv. Appl. Math., 16:23\u201358 (1995).\n\n[12] Q. Zhang, The transient behavior of mixing induced by a random velocity field.\n\nWater Research Resources, 31:577\u2013591 (1995).\n\n\n\nCap??tulo 3\n\nA finite volume method for the mean\n\nof the solution of the random linear\n\ntransport equation\n\nAbstract\n\nWe present a numerical scheme, based on Godunov\u2019s method (REA algorithm), for the\n\nstatistical mean of the solution of the one-dimensional random linear transport equation,\n\nwith homogeneous random velocity and random initial condition. Numerical examples\n\nare considered to validate our method.\n\nKeyword: random linear transport equation, finite volume schemes, Riemann problem,\n\nstatistical mean, Godunov\u2019s method (REA algorithm).\n\n3.1 Introduction\n\nConservation laws are differential equations arising from physical principles of the con-\n\nservation of mass, energy or momentum. The simplest of these equations is the one-\n\ndimensional advective equation and its solution plays a role in more complex problems\n\nsuch as the numerical solution for nonlinear conservation law. This linear initial value\n\nproblem can, for instance, model the concentration, or density, of a chemical substance\n\ntransported by a one-dimensional fluid that flows with a known velocity. In the determi-\n\nnistic case, we want to find q(x, t) such that:\n\n{\nqt + a(x)qx = 0, t > 0, x ? R,\nq(x, 0) = q0(x).\n\n(3.1)\n\n21\n\n\n\n22 Cap??tulo 3\n\nIt is well known that the solution to (3.1) is the initial condition transported along\n\nthe characteristic curves.\n\nThe complexity of natural phenomena compels us to study partial differential equa-\n\ntions with random data. For example, (3.1) may model the flux of a two phase equal\n\nviscosity miscible fluid in a porous media. The total velocity is obtained from Darcy\u2019s law\n\nand it depends on the geology of the porous media. Thus, the external velocity is defined\n\nby given statistics. Also, the prediction of the initial state of the process is obtained from\n\ndata acquired from a small number of exploratory wells using geological methods.\n\nIn this work, we are concerned with the numerical solution of the random version of\n\nthe problem (3.1), i.e., the stochastic transport equation,\n\nQt(x, t) + AQx(x, t) = 0, t > 0, x ? R, (3.2)\n\nwith a homogeneous random transport velocity, A, and stochastic initial condition, Q(x, 0) =\n\nQ0(x).\n\nA mathematical basis for the solution of stochastic, or random, partial differential\n\nequations has not been complete yet. Besides the well-developed theoretical methods\n\nsuch as Ito integrals, Martingales and Wiener measure [7, 8], two types of methods are\n\nnormally used in the construction of solutions for random partial differential equations.\n\nThe first method is based on the Monte Carlo method, which in general demands massive\n\nnumerical simulations using high resolution methods (see [6]), and the second is based on\n\nthe effective equations (see [2]), the deterministic differential equations whose solutions\n\nare the statistical means of (3.2).\n\nThe solution of (3.2) is a random function. For a particular case when the initial\n\ncondition is given by\n\nQ(x, 0) =\n\n{\nQ+0 , x > 0,\n\nQ?0 , x &lt;0,\n(3.3)\n\nwith Q?0 and Q\n+\n0 random variables, we have shown in [1] that the solution of the Riemann\n\nproblem (3.2)-(3.3) is\n\nQ(x, t) = Q?0 + X\n(\nQ+0 ? Q?0\n\n)\n, (3.4)\n\nwhere X is the Bernoulli random variable with P (X = 0) = 1?FA(x/t) and P (X = 1) =\nFA(x/t); here FA(x) is the cumulative probability function of the random variable A.\n\nAlso, according to [1], considering the independence of A and both Q?0 and Q\n+\n0 , the\n\nstatistical mean of the solution of the Riemann problem (3.2)-(3.3) for a fixed (x, t) is\n\n?Q(x, t)? = ?Q?0 ? + FA\n( x\n\nt\n\n) [\n?Q+0 ? ? ?Q?0 ?\n\n]\n. (3.5)\n\nBesides the formal verification of the explicit expression (3.4), in [1] we compare (3.5)\n\nwith the mean given by an effective equation to (3.2) and also show that the Monte Carlo\n\n\n\n3.2. The numerical scheme 23\n\nmethod agree quite well with (3.5). We can observe that (3.5) gives the mean, ?Q(x, t)?,\nwithout considering either the effective equation or the Monte Carlo method.\n\nIn this paper, we use these results to design a numerical scheme to approximate the\n\nstatistical mean for (3.2) with a more general initial condition. The method is based on\n\nthe Riemann problems solution, Godunov\u2019s ideas, and finite volume methods \u2013 widely\n\nused in high-resolution methods for deterministic conservation laws (see [5], Ch. 4).\n\nThe outline of this paper is as follows. In Section 3.2 we deduce the explicit numerical\n\nscheme using the ideas of Godunov\u2019s reconstruct-evolve-average algorithm. The analysis of\n\nstability and convergence of the method is presented in Section 3.3. Finally, in Section 3.4\n\nwe present and compare some numerical examples.\n\n3.2 The numerical scheme\n\nIn this section, we present the numerical method for the mean of the solution of (3.2).\n\nInitially, we discretize both space and time assuming uniform mesh spacing with ?x\n\nand ?t, respectively. We denote the spatial and time grid points by xj = j?x and\n\ntn = n?t, respectively. In a context of finite volume methods, denoting the jth grid cell\n\nby Cj = (xj?1/2, xj+1/2), where xj\u00b11/2 = xj\u00b1?x/2, the value denoted by Qnj approximates\nthe average value of the random function Q(x, tn) over the jth grid cell:\n\nQnj ?\n1\n\n?x\n\n?\n\nCj\n\nQ(x, tn)dx =\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\nQ(x, tn)dx. (3.6)\n\nWe follow the basic ideas of REA algorithm, Reconstruct-Evolve-Average, a finite\n\nvolume algorithm originally proposed by [3] as a method for solving the nonlinear Euler\n\nequations of gas dynamics.\n\nAssuming that the cell averages at time tn, Q\nn\nj , are known, we summarize the REA\n\nalgorithm (see [5], Ch. 4) in three steps:\n\nStep 1. Reconstruct a piecewise polynomial function Q?(x, tn), defined for all x, from the\n\ncell averages Qnj . In our case we use the piecewise constant function with Q\nn\nj in the\n\njth grid cell, i.e., Q?(x, tn) = Q\nn\nj , for x ? Cj.\n\nStep 2. Evolve the equation exactly, or approximately, with this initial data to obtain\n\nQ?(x, tn+1) a time ?t later. In our case we can evolve exactly using (3.4).\n\nStep 3. Average Q?(x, tn+1) over each grid cell to obtain the new cell averages, i.e.,\n\nQ n+1j =\n1\n\n?x\n\n?\n\nCj\n\nQ?(x, tn+1)dx.\n\n\n\n24 Cap??tulo 3\n\nThe piecewise constant function, step 1, defines a set of Riemann problems in each\n\nx = xj?1/2: differential equation (3.2) with the initial condition\n\nQ(x, tn) =\n\n{\nQnj?1, x &lt;xj?1/2,\nQnj , x > xj?1/2.\n\n(3.7)\n\nTherefore, we may use (3.4) to solve each Riemann problem:\n\nQ(x, tn+1/2) = Q\nn\nj?1 + X\n\n(\nx ? xj?1/2\n\n?t/2\n\n) [\nQnj ? Qnj?1\n\n]\n, (3.8)\n\nwhere, for x fixed, X(x) is the Bernoulli random variable:\n\nX(x) =\n\n{\n1, P (X(x) = 1) = FA(x),\n\n0, P (X(x) = 0) = 1 ? FA(x).\n(3.9)\n\nAs in the deterministic case the solution at tn+1/2, Q?(x, tn+1/2), can be constructed by\n\npiecing together the Riemann solutions, provided that the half time step ?t/2 is short\n\nenough such that adjacent Riemann problems have not started to interact yet. This\n\nrequires that ?x and ?t must be chosen satisfying:\n\nQ(xj?1, tn+1/2) ? Qnj?1 and Q(xj, tn+1/2) ? Qnj ,\n\nwhere the symbol \u201c ? \u201d means \u201csufficiently near to\u201d.\nSubstituting the above conditions into (3.8) we must have X (??x/?t) = 0 and\n\nX (?x/?t) = 1 both with probability sufficiently near to 1. From (3.9) this means the\n\nfollowing conditions:\n\nFA\n\n(\n??x\n\n?t\n\n)\n? 0 and FA\n\n(\n?x\n\n?t\n\n)\n? 1. (3.10)\n\nRemark 3.1. We may regard (3.10) as a kind of CFL condition for the method: the inter-\n\nval [??x/?t, ?x/?t] must contain the effective support of the density probability function\nof A. The word effective support means that outside [??x/?t, ?x/?t] the probability of\nA is sufficiently near to zero, i.e., it can be disregarded. The existence of the effective\n\nsupport is ensured by Chebyshev\u2019s inequality: for any k > 0, P{|A??A?| ? k?A} ? 1/k2,\nwhere ?A is the standard variation of A.\n\nUnder hypothesis (3.10) we may finish the step 2 taking\n\nQ?(x, tn+1/2) =\n?\n\nj\n\nQ(x, tn+1/2) 1[xj?1, xj ], (3.11)\n\nwhere 1[xj?1, xj ] is the characteristic function of [xj?1, xj].\n\n\n\n3.2. The numerical scheme 25\n\nIn step 3 of REA algorithm we use (3.11) to calculate Q\nn+1/2\nj?1/2 :\n\nQ\nn+1/2\nj?1/2 =\n\n1\n\n?x\n\n? xj\nxj?1\n\nQ?(x, tn+1/2)dx\n\n=\n1\n\n?x\n\n? xj\nxj?1\n\n{\nQnj?1 + X\n\n(\nx ? xj?1/2\n\n?t/2\n\n) [\nQnj ? Qnj?1\n\n]}\ndx\n\n= Qnj?1 +\n1\n\n?x\n\n{? xj\nxj?1\n\nX\n\n(\nx ? xj?1/2\n\n?t/2\n\n)\ndx\n\n}\n[\nQnj ? Qnj?1\n\n]\n\n= Qnj?1 +\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nX(x) dx\n\n}\n[\nQnj ? Qnj?1\n\n]\n. (3.12)\n\nThe cell averages, Q\nn+1/2\nj?1/2 , define new Riemann problems at xj. We repeat the proce-\n\ndure above to obtain the solution in Cj at tn+1:\n\nQn+1j =\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\n{\nQ\n\nn+1/2\nj?1/2 + X\n\n(\nx ? xj\n?t/2\n\n) [\nQ\n\nn+1/2\nj+1/2\n\n? Qn+1/2\nj?1/2\n\n]}\ndx =\n\n= Q\nn+1/2\nj?1/2 +\n\n1\n\n?x\n\n{? xj+1/2\nxj?1/2\n\nX\n\n(\nx ? xj\n?t/2\n\n)\ndx\n\n} [\nQ\n\nn+1/2\nj+1/2\n\n? Qn+1/2\nj?1/2\n\n]\n=\n\n= Q\nn+1/2\nj?1/2 +\n\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nX(x)dx\n\n} [\nQ\n\nn+1/2\nj+1/2\n\n? Qn+1/2\nj?1/2\n\n]\n. (3.13)\n\nLemma 3.1. Let Y =\n\n? ?\n??\n\nX(x)dx be a random variable with ? > 0 and X(x) the random\n\nfield defined in (3.9). Then P{Y = ?Y ?} = 1.\n\nProof. Since ?Y ? =\n?? ?\n\n??\nX(x)dx\n\n?\n=\n\n? ?\n??\n?X(x)?dx =\n\n? ?\n??\n\nFA(x)dx, we have\n\n?Y 2? =\n?[? ?\n\n??\nX(x)dx\n\n]2?\n=\n\n?? ?\n??\n\n? ?\n??\n\nX(x1)X(x2)dx1dx2\n\n?\n\n=\n\n? ?\n??\n\n? ?\n??\n?X(x1)X(x2)?dx1dx2 =\n\n? ?\n??\n\n? ?\n??\n?X(x1)??X(x2)?dx1dx2\n\n=\n\n? ?\n??\n\n? ?\n??\n\nFA(x1)FA(x2)dx1dx2 =\n\n[? ?\n??\n\nFA(x)dx\n\n]2\n= ?Y ?2.\n\nTherefore, V ar(Y ) = ?Y 2? ? ?Y ?2 = 0 and thus P{Y = ?Y ?} = 1.\n\n\n\n26 Cap??tulo 3\n\nFrom this result we can conclude that\n? ?x\n\n?t\n\n??x\n?t\n\nX(x)dx =\n\n?? ?x\n?t\n\n??x\n?t\n\nX(x)dx\n\n?\n=\n\n? ?x\n?t\n\n??x\n?t\n\n?X(x)?dx =\n? ?x\n\n?t\n\n??x\n?t\n\nFA(x)dx,\n\nand thus rewrite (3.12)\u2013(3.13) as\n\nQ\nn+1/2\nj?1/2 = Q\n\nn\nj?1 +\n\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nFA(x) dx\n\n}\n[\nQnj ? Qnj?1\n\n]\n(3.14)\n\nand\n\nQn+1j = Q\nn+1/2\nj?1/2 +\n\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nFA(x)dx\n\n} [\nQ\n\nn+1/2\nj+1/2\n\n? Qn+1/2\nj?1/2\n\n]\n. (3.15)\n\nLemma 3.2. Let A be a random variable and [??, ?] an effective support of the density\nprobability function, fA, of A. Then\n\n? ?\n??\n\nFA (x) dx ? ? ? ?A?. (3.16)\n\nProof. Using the hypothesis and integration by parts in the definition of the statistical\n\nmean of A we have:\n\n?A? =\n? ?\n??\n\nx fA (x) dx ?\n? ?\n??\n\nx fA (x) dx = x FA(x)|??? ?\n? ?\n??\n\nFA (x) dx.\n\nSince FA(??) ? 0 and FA(?) ? 1 we obtain the result.\n\nUsing (3.16) as an approximation of the integral in (3.14) and (3.15), and denoting\n\n? = (?t?A?)/?x, we define the two-step numerical scheme:\n\nQ\nn+1/2\nj?1/2 =\n\n1\n\n2\n\n[\nQnj?1 + Q\n\nn\nj\n\n]\n? ?\n\n2\n\n[\nQnj ? Qnj?1\n\n]\n\nand\n\nQn+1j =\n1\n\n2\n\n[\nQ\n\nn+1/2\nj?1/2 + Q\n\nn+1/2\nj+1/2\n\n]\n? ?\n\n2\n\n[\nQ\n\nn+1/2\nj+1/2\n\n? Qn+1/2\nj?1/2\n\n]\n.\n\nJoining these expressions we can summarize the two-step scheme above in the explicit\n\nmethod:\n\nQn+1j = Q\nn\nj ?\n\n?\n\n2\n\n[\nQnj+1 ? Qnj?1\n\n]\n+\n\n1\n\n4\n\n(\n1 + ?2\n\n) [\nQnj+1 ? 2Qnj + Qnj?1\n\n]\n. (3.17)\n\nTaking the statistical mean in (3.17) we obtain the explicit scheme for the mean of\n\nthe solution to (3.2):\n\n?Qn+1j ? = ?Qnj ? ?\n?\n\n2\n\n[\n?Qnj+1? ? ?Qnj?1?\n\n]\n+\n\n1\n\n4\n\n(\n1 + ?2\n\n) [\n?Qnj+1? ? 2?Qnj ? + ?Qnj?1?\n\n]\n, (3.18)\n\nwhere ? = (?t?A?)/?x.\n\n\n\n3.3. Numerical analysis of the scheme 27\n\nRemark 3.2. The numerical method (3.18) is conservative, in the sense that it can be\n\nrewritten as\n\n?Qn+1j ? = ?Qnj ? ?\n?t\n\n?x\n\n[\nF nj+1/2 ? F nj?1/2\n\n]\n,\n\nwhere F nj?1/2 = (1/2)?A?[?Qnj?1? + ?Qnj ?] ? (1/4)?A? (1? + ?) [?Qnj ???Qnj?1?] is an appro-\nximation to the average flux at x = xj?1/2.\n\n3.3 Numerical analysis of the scheme\n\nIn this section, we analyze the convergence of the method (3.18) and show its stability\n\nand consistency with an advective-diffusive equation.\n\nProposition 3.1. For (?x2/?t) = ? fixed the numerical scheme (3.18) is an O(?x2)\napproximation for u(x, t), solution of the deterministic differential equation\n\nut + ?A?ux =\n?\n\n4\nuxx. (3.19)\n\nProof. Let u(x, t) be a smooth function such that u(xj, tn) = ?Qnj ?. Thus, by (3.18) we\nhave\n\nu(x, t + ?t) = u(x, t) ? ?t\n2?x\n\n?A? [u(x + ?x, t) ? u(x ? ?x, t)] +\n\n+\n1\n\n4\n\n[\n1 +\n\n(\n?t\n\n?x\n?A?\n\n)2]\n[u(x + ?x, t) ? 2u(x, t) + u(x ? ?x, t)] ,\n\nand using Taylor\u2019s expansion it follows\n{\n\nut +\n?t\n\n2\nutt +\n\n?t2\n\n6\nuttt + ...\n\n}\n+ ?A?\n\n{\nux +\n\n?x2\n\n6\nuxxx + ...\n\n}\n=\n\n=\n1\n\n4\n\n(\n?x2\n\n?t\n+ ?t?A?2\n\n) {\nuxx +\n\n?x2\n\n2\nuxxxx + ...\n\n}\n.\n\nSince (?x2/?t) = ? is fixed, we have ?t = (?x2/?) = O(?x2). Thus, grouping the\nterms of the same order we arrive at the expression:\n\nut + ?A?ux =\n?\n\n4\nuxx + O(?x2).\n\nProposition 3.2. The numerical method (3.18) is stable under the conditions (3.10) and\n\n?t\n\n?x\n|?A?| ? 1. (3.20)\n\n\n\n28 Cap??tulo 3\n\nProof. Using the von Neumann analysis (see [9]) it follows that the amplification factor\n\nassociated to (3.18) is\n\ng(?) = 1 ? ?\n2\n\n(\nei? ? e?i?\n\n)\n+\n\n1\n\n4\n(1 + ?2)\n\n(\nei? ? 2 + e?i?\n\n)\n\n= 1 +\n1\n\n2\n(1 + ?2)(cos ? ? 1) ? i ? sin ?\n\n= 1 ? (1 + ?2) sin2\n(\n\n?\n\n2\n\n)\n? i 2? sin\n\n(\n?\n\n2\n\n)\ncos\n\n(\n?\n\n2\n\n)\n,\n\nfor ? ? [??, ?].\nThe magnitude of the amplification factor g(?) is given by,\n\n|g(?)|2 =\n{\n\n1 ? (1 + ?2) sin2\n(\n\n?\n\n2\n\n)}2\n+ 4?2 sin2\n\n(\n?\n\n2\n\n)\ncos2\n\n(\n?\n\n2\n\n)\n\n= 1 ?\n[\n2(1 + ?2) ? 4?2\n\n]\nsin2\n\n(\n?\n\n2\n\n)\n+\n\n[\n(1 + ?2)2 ? 4?2\n\n]\nsin4\n\n(\n?\n\n2\n\n)\n\n= 1 ? 2(1 ? ?2) sin2\n(\n\n?\n\n2\n\n)\n+ (1 ? ?2)2 sin4\n\n(\n?\n\n2\n\n)\n\n=\n\n[\n1 ? (1 ? ?2) sin2\n\n(\n?\n\n2\n\n)]2\n, ? ? [??, ?].\n\nTherefore, if |?| ? 1 we have |g(?)| ? 1, for all ? ? [??, ?].\n\nRemark 3.3. We can show that the conditions in (3.10) are sufficient for (3.20). In\n\nfact, using Lemma 3.2:\n\n0 ?\n? ?x\n\n?t\n\n??x\n?t\n\nFA(x)dx ?\n?x\n\n?t\n? ?A? ? 2?x\n\n?t\n.\n\nThus ??x/?t ? ?A? ? ?x/?t or |?A?| ? ?x/?t, i.e., ?t |?A?| /?x ? 1. With this\nremark we conclude that the conditions (3.10) ensure the stability of the proposed scheme.\n\nRemark 3.4. Under the stability conditions (3.10) and the consistency (Proposition 3.1)\n\nwe have the convergence of the means calculated by (3.18) to the solution of equation\n\n(3.19).\n\nProposition 3.3. Under the conditions (3.10), the numerical scheme (3.18) is total va-\n\nriation diminishing (TVD), i.e., T V (Qn+1) ? T V (Qn).\n\n\n\n3.4. Numerical examples 29\n\nProof. We observe that (3.18) can be rewritten as\n\n?Qn+1j ? = ?Qnj ? ?\n(1 + ?)2\n\n4? ?? ?\n?\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]\n+\n\n(1 ? ?)2\n4? ?? ?\n?\n\n[\n?Qn+1j ? ? ?Qnj ?\n\n]\n.\n\nAccording to Harten\u2019s theorem [4] the sufficient conditions to ensure the TVD property\n\nof a method are: ? ? 0, ? ? 0 and ? + ? ? 1. From Remark 3.3 we have |?| ? 1. Thus,\nthese three conditions are satisfied under hypothesis (3.10).\n\n3.4 Numerical examples\n\nTo assess our method for the mean of the linear advective equation with random data we\n\npresent two numerical examples. In Example 3.1 we solve a Riemann problem with ran-\n\ndom velocity and deterministic initial condition; in this case the exact solution, ?Q(x, t)?,\nis known. In Example 3.2 we apply our method in a problem with random velocity and\n\ninitial condition being a correlated random field. In both examples we use A normally,\n\nlognormally, and uniformly distributed, respectively, to compare the effects of the velocity\n\ndistribution.\n\nExample 3.1.\n\nLet us consider the PDE (3.2) with the deterministic initial condition\n\nQ(x, 0) =\n\n{\n1, x &lt;0,\n\n0, x ? 0.\n\nIn Figures 3.1 \u2013 3.3 we compare the approximations of the mean calculated using\n\n(3.18) with the exact values given by (3.5): ?Q(x, t)? = 1 ? FA(x/t). We plot the results\nat T = 0.1 and T = 0.3 (figures (a) and (b), respectively). To observe the influence of the\n\nvelocity variation we use three models: [i] A is normally distributed, A ? N (1.0, 0.8), in\nFigure 3.1; [ii] A is lognormally distributed, A = exp (?), ? ? N (0.5, 0.25), in Figure 3.2;\n[iii] A is uniformly distributed in [0.75, 1.25], in Figure 3.3. The values of ?t and ?x are\n\npresented in the captions of the figures. The figures in this example, especially Figure\n\n3.3, also help us in the verification of the \u201chigh-resolution\u201d of the proposed method in the\n\nsense that the numerical dispersion of the method does not give a false appearance to the\n\nmixing zone derived from the variability of the velocity.\n\n\n\n30 Cap??tulo 3\n\n?0.5 0 0.5 1 1.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\n?0.5 0 0.5 1 1.5\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\nFigure 3.1: ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b).\n\n?0.5 0 0.5 1 1.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\n?0.5 0 0.5 1 1.5\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\nFigure 3.2: ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b).\n\n?0.5 0 0.5 1 1.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\n?0.5 0 0.5 1 1.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed Method\nExact Solution\n\nFigure 3.3: ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b).\n\n\n\n3.4. Numerical examples 31\n\nExample 3.2.\n\nHere we consider the PDE (3.2) with a normal random initial condition, Q0(x). The\n\nmean of Q0(x) is\n\n?Q0(x)? =\n{\n\n1, x ? (1.4, 2.2),\ne?20(x?0.25)\n\n2\n\n, otherwise,\n(3.21)\n\nand the covariance is defined by Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2\nis constant and ? > 0 governs the decay rate of the spatial correlation. In the tests we use\n\n? = 40 and ?2 = 0.2. The numerical results are compared with the Monte Carlo method\n\nusing suites of realizations of A and Q0(x), with A and Q0(x) statistically independents.\n\nThe solution of (3.2)\u2013(3.21) for a single realization A(?) and Q0(x, ?), of A and Q0(x),\n\nrespectively, is given by Q(x, t, ?) = Q0(x ? A(?)t, ?). The realizations of the correlated\nrandom field Q0(x) are generated using the matriz decomposition method, a direct method\n\nfor generating correlated random fields (see [10], Ch. 3, for example). We use the Monte\n\nCarlo method with 1500 realizations, and plot the results at T = 0.1 and T = 0.3 (figures\n\n(a) and (b), respectively). Again, we use three models of velocity: [i] A is normally\n\ndistributed, A ? N (1.0, 0.8), in Figure 3.4; [ii] A is lognormally distributed, A = exp (?),\n? ? N (0.5, 0.25), in Figure 3.5; [iii] A is uniformly distributed in [0.75, 1.25], in Figure 3.6.\nThe values of ?t and ?x are the same used in Example 3.1. In fact, the known solution\n\nof the Riemann problem allows to choose good values for ?t and ?x. Once these values\n\nwere calibrated, they are used in the general initial condition problem with success, as\n\nshow the results presented here. The numerical tests have shown that a good choice for\n\n? in (3.19) is ? = 2Var[A]T .\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\nMean of solution\n\nMean of Initial Condition\nProposed Method\nMonte Carlo Simulations\n\nFigure 3.4: ?x = 0.016, ?t = 0.002 (a), and ?t = 0.00065 (b).\n\n\n\n32 Cap??tulo 3\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\nMean of Initial Condition\nProposed Method\nMonte Carlo Simulations\n\nFigure 3.5: ?x = 0.016, ?t = 0.005 (a), and ?t = 0.0022 (b).\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\n0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\nMean of Initial Condition\nProposed Method\nMonte Carlo Simulations\n\nFigure 3.6: ?x = 0.004, ?t = 0.003 (a), and ?t = 0.001 (b).\n\n3.5 Concluding remarks\n\nIn this article, we present a numerical scheme for the statistical mean of the solution of the\n\nrandom linear transport equation. The random data are the velocity (random variable)\n\nand the initial condition (random function). To design the method we use the basic\n\nideas of Godunov\u2019s method (REA algorithm) with a known expression for the random\n\nRiemann problem solution. We obtain the stability condition of the method and show its\n\nconsistency with a deterministic advective-diffusive equation, which means convergence\n\nof the method. The examples show good agreement of the results with the Monte Carlo\n\nmethod.\n\nAs far as we know, this methodology has not been studied yet. The advantages of\n\nthe algorithm are: it does not require an effective equation and it does not demand the\n\ngreat number of realizations necessary in the Monte Carlo method. We believe that\n\nthis methodology can also be applied to solve more general problems, and also to obtain\n\ninformation about other statistical moments of the solution.\n\n\n\nReferences 33\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) through the grants 5551463/02-3 and 140406/2004-2 (doctoral scholarship).\n\nReferences\n\n[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random\n\ntransport equation. Computational and Applied Mathematics 26(3):323\u2013335 (2007).\n\n[2] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03\u201344, Providence, 1998.\n\n[3] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntions of the equations of hydrodynamics. Mat. Sb. 47:271\u2013306 (1959).\n\n[4] A. Harten, High resolution schemes for hyperbolic conservation laws. J. Comput.\n\nPhys. 49(2):357\u2013393 (1983).\n\n[5] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n[6] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-\n\ndiretional stochastic transport equation. SIAM Journal on Scientific Computing\n\n19(3):799\u2013812 (1998).\n\n[7] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New\n\nYork, 1985.\n\n[8] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic\n\nPress, New York, 1973.\n\n[9] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.\n\nWadsworth &amp; Brooks/Cole, California, 1989.\n\n[10] D. Zhang, Stochastic Methods for Flows in Porous Media - Coping with Uncertainties.\n\nAcademic Press, 2002.\n\n\n\nCap??tulo 4\n\nA numerical scheme for the variance\n\nof the solution of the random linear\n\ntransport equation\n\nAbstract\n\nWe present a numerical scheme, based on Godunov\u2019s method (REA algorithm), for the\n\nvariance of the solution of the one-dimensional random linear transport equation with\n\nhomogeneous random velocity and stochastic initial condition. We obtain the stability\n\nconditions of the method and we also show its consistency with a deterministic nonho-\n\nmogeneous advective-diffusive equation, which means convergence. Numerical results are\n\nconsidered to validate our scheme.\n\nKeyword: random linear transport equation, finite volume schemes, Godunov\u2019s method.\n\n4.1 Introduction\n\nIn this work, we are concerned with the variance of the solution of the random transport\n\nequation, {\nQt(x, t) + AQx(x, t) = 0, t > 0, x ? R,\nQ(x, 0) = Q0(x),\n\n(4.1)\n\nwith a homogeneous random transport velocity, A, and stochastic initial condition, Q0(x).\n\nThe solution, Q(x, t), is a random function. For the particular case, Riemann problem\n\n(4.1) with\n\nQ(x, 0) =\n\n{\nQ?0 if x &lt;0,\nQ+0 if x > 0,\n\n(4.2)\n\n35\n\n\n\n36 Cap??tulo 4\n\nwhere Q?0 and Q\n+\n0 are random variables, we presented in [1] the expression for its solution:\n\nQR(x, t) = Q\n?\n0 + X\n\n(\nQ+0 ? Q?0\n\n)\n, (4.3)\n\nwhere X is the Bernoulli random variable with P (X = 0) = 1?FA(x/t) and P (X = 1) =\nFA(x/t); here FA(x) is the cumulative probability function of the random variable A.\n\nAlso, according to [1], and considering the statistical independence between A and\n\nboth Q?0 , Q\n+\n0 , the mean and variance of the solution are given by\n\n?QR(x, t)? = ?Q?0 ? + FA\n(x\n\nt\n\n) [\n?Q+0 ? ? ?Q?0 ?\n\n]\n(4.4)\n\nand\n\nV ar[QR(x, t)] =V ar[Q\n?\n0 ] + FA\n\n(x\nt\n\n) [\nV ar[Q+0 ] ? V ar[Q?0 ]\n\n]\n+\n\nFA\n\n(x\nt\n\n) [\n1 ? FA\n\n(x\nt\n\n)] [\n?Q+0 ? ? ?Q?0 ?\n\n]2\n, (4.5)\n\nrespectively.\n\nIn our point of view, the special attraction of (4.3), (4.4), and (4.5) is their utilization\n\nin discretizations of stochastic equations, like (4.1). In [2] we present an explicit method\n\nto calculate the mean of Q(x, t), the solution of (4.1) with Q(x, 0) = Q0(x) a random\n\nfunction. In that report, we show that Godunov\u2019s method provides a numerical scheme\n\nfor the statistical mean which is, under certain assumptions on the discretization, stable\n\nand consistent with an advective-diffusive equation. Therefore, besides the scheme itself,\n\nthe numerical approach also gives an effective equation compatible with one published in\n\nthe literature.\n\nThe aim of this paper is to improve the knowledge of the random solution of (4.1).\n\nBasically, we present a numerical method to calculate the variance of Q(x, t), which is\n\nthe quantity most commonly used to specify the dispersion of the distribution around its\n\nmean.\n\nThe outline of this paper is as follows. In Section 4.2 we deduce the explicit numerical\n\nscheme using Godunov\u2019s ideas. Consistency, stability and convergence are analyzed in\n\nSection 4.3. Finally, in Section 4.4 we present some numerical examples.\n\n4.2 The numerical scheme\n\nIn this section, we present the numerical scheme for the variance of the solution of (4.1).\n\nWe denote the spatial and time grid points by xj = j?x and tn = n?t, respectively, and\n\nthe jth grid cell is Cj = (xj?1/2, xj+1/2), xj\u00b11/2 = xj \u00b1?x/2. Let Qnj be an approximation\n\n\n\n4.2. The numerical scheme 37\n\nof the cell average of Q(x, tn):\n\nQnj '\n1\n\n?x\n\n?\n\nCj\nQ(x, tn)dx =\n\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\nQ(x, tn)dx. (4.6)\n\nAssuming that the cell averages at time tn, Q\nn\nj , are known, we summarize the REA,\n\nReconstruct-Evolve-Average, algorithm [4, 5] in three steps:\n\nStep 1. Reconstruct a piecewise polynomial function, Q?(x, tn), from the cell averages\n\nQnj . In our case we use the piecewise constant function with Q\nn\nj in the jth cell, i.e.,\n\nQ?(x, tn) = Q\nn\nj for all x ? Cj.\n\nStep 2. Evolve the equation exactly, or approximately, with this initial data to obtain\n\nQ?(x, tn+1) a time ?t later.\n\nStep 3. Average Q?(x, tn+1) over each grid cell to obtain the new cell averages, i.e.,\n\nQ n+1j =\n1\n\n?x\n\n?\n\nCj\nQ?(x, tn+1)dx.\n\nAt a time tn, the piecewise constant function, step 1, defines a set of Riemann problems\n\nin each x = xj?1/2: the differential equation (4.1) with the initial condition\n\nQ(x, tn) =\n\n{\nQnj?1 if x &lt;xj?1/2\nQnj if x > xj?1/2.\n\n(4.7)\n\nWe may use (4.3) to find a local solution to each Riemann problem at a time ?t/2\n\nlater:\n\nQ(x, tn+1/2) = Q\nn\nj?1 + X\n\n(\nx ? xj?1/2\n\n?t/2\n\n) [\nQnj ? Qnj?1\n\n]\n, (4.8)\n\nwhere, for x sufficiently close to xj?1/2, X(x) is the Bernoulli random variable:\n\nX(x) =\n\n{\n1, P (X(x) = 1) = FA(x)\n\n0, P (X(x) = 0) = 1 ? FA(x).\n(4.9)\n\nAlso, according to (4.4) and (4.5), and denoting ?j?1/2(x) = FA\n(\n\nx?xj?1/2\n?t/2\n\n)\n, we have:\n\n?Q(x, tn+1/2)? = ?Qnj?1? + ?j?1/2(x)\n[\n?Qnj ? ? ?Qnj?1?\n\n]\n(4.10)\n\nand\n\nV ar[Q(x, tn+1/2)] =V ar[Q\nn\nj?1] + ?j?1/2(x)\n\n[\nV ar[Qnj ] ? V ar[Qnj?1]\n\n]\n+\n\n?j?1/2(x)\n(\n1 ? ?j?1/2(x)\n\n) [\n?Qnj ? ? ?Qnj?1?\n\n]2\n. (4.11)\n\n\n\n38 Cap??tulo 4\n\nTherefore, the variance of the solution at tn+1/2, V ar[Q?(x, tn+1/2)], can be constructed\n\nby piecing together the local values of the variance, (4.11), provided that the half time\n\nstep ?t/2 is short enough such that adjacent Riemann problems do not interact between\n\nthemselves. This requires that ?x and ?t must be chosen satisfying:\n\nV ar[Q(xj?1, tn+1/2)] ? V ar[Qnj?1] and V ar[Q(xj, tn+1/2)] ? V ar[Qnj ],\n\nwhere the symbol \u201c ? \u201d means \u201csufficiently near to\u201d. Substituting these conditions in\n(4.11), the following conditions must be satisfied:\n\nFA\n\n(\n??x\n\n?t\n\n)\n? 0 and FA\n\n(\n?x\n\n?t\n\n)\n? 1. (4.12)\n\nRemark 4.1. We may regard (4.12) as a kind of CFL condition for the method. The\n\ninterval [??x/?t, ?x/?t] must contain the \u201ceffective support\u201d of the density probabil-\nity function of A. This means that outside [??x/?t, ?x/?t] the probability of A is\nsufficiently near to zero, i.e., it can be disregarded. The existence of an effective sup-\n\nport is ensured by Chebyshev\u2019s inequality: P{|A ? ?A?| ? k?A} ? 1/k2, for all k > 0,\nwhere ?A is the standard variation of A. Therefore, if we take 1/k\n\n2 sufficiently close to\n\nzero, to escape from the interaction between solutions of Riemann problems we must take\n\n(|?A?| + k?A) ?t/?x ? 1.\n\nUnder the hypothesis (4.12), the expression (4.11) defines V ar[Q?(x, tn+1/2)], x ?\n[xj?1, xj]; its cell average will be denoted by\n\nV\nn+1/2\nj?1/2 =\n\n1\n\n?x\n\n? xj\nxj?1\n\nV ar[Q?(x, tn+1/2)] dx.\n\nTherefore, using (4.11) we have the cell average of the variance in [xj?1, xj] at t =\ntn+1/2:\n\nV\nn+1/2\nj?1/2 =\n\n1\n\n?x\n\n? xj\nxj?1\n\n{\nV nj?1 + ?j?1/2(x)\n\n[\nV nj ? V nj?1\n\n]}\ndx +\n\n1\n\n?x\n\n? xj\nxj?1\n\n?j?1/2(x)\n(\n1 ? ?j?1/2(x)\n\n)\ndx\n\n? ?? ?\n?\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]2\n.\n\nPreliminary computational tests have shown that ? reduces excessively the contribu-\n\ntion of\n[\n?Qnj ? ? ?Qnj?1?\n\n]2\nto V\n\nn+1/2\nj?1/2 . The following approximation provides better results:\n\n? =\n1\n\n?x\n\n? xj\nxj?1\n\n?j?1/2(x)\n[\n1 ? ?j?1/2(x)\n\n]\ndx ' ?j?1/2(?)\n\n[\n1 ? ?j?1/2(?)\n\n]\n,\n\n\n\n4.2. The numerical scheme 39\n\nwhere ? ? [xj?1, xj] is such that\n\n?j?1/2(?)\n[\n1 ? ?j?1/2(?)\n\n]\n= max\n\nx?[xj?1, xj ]\n?j?1/2(x)\n\n[\n1 ? ?j?1/2(x)\n\n]\n.\n\nIt is straightforward to show that ? must satisfy ?j?1/2(?) = 1/2.\nThus, ?j?1/2(?)\n\n[\n1 ? ?j?1/2(?)\n\n]\n= 1/4 and, changing variables in the other integral,\n\nwe obtain\n\nV\nn+1/2\nj?1/2 = V\n\nn\nj?1 +\n\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nFA(x) dx\n\n}\n[\nV nj ? V nj?1\n\n]\n+\n\n1\n\n4\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]2\n. (4.13)\n\nLemma 4.1. Let A be a random variable and [??, ?] an effective support of the density\nprobability function, fA, of A, i.e., FA(??) ? 0 and FA(?) ? 1. Then\n\n? ?\n??\n\nFA (x) dx ? ? ? ?A?. (4.14)\n\nProof. See [2].\n\nSubstituting (4.14) in (4.13), and denoting ? = ?t?A?/?x, we have:\n\nV\nn+1/2\nj?1/2 =\n\n1\n\n2\n\n[\nV nj + V\n\nn\nj?1\n\n]\n? ?\n\n2\n\n[\nV nj ? V nj?1\n\n]\n+\n\n1\n\n4\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]2\n. (4.15)\n\nNow we can repeat the same procedure to obtain approximations of the solution in\n\n[xj?1/2, xj+1/2] at tn+1:\n\nV n+1j =\n1\n\n2\n\n[\nV\n\nn+1/2\nj+1/2\n\n+ V\nn+1/2\nj?1/2\n\n]\n? ?\n\n2\n\n[\nV\n\nn+1/2\nj+1/2\n\n? V n+1/2\nj?1/2\n\n]\n+\n\n1\n\n4\n\n[\n?Qn+1/2\n\nj+1/2\n? ? ?Qn?1/2\n\nj?1/2 ?\n]2\n\n. (4.16)\n\nThe ideas of the Godunov method were also used in [2] to design a scheme for appro-\n\nximations of the statistical means of (4.1):\n\n?Qn+1/2\nj?1/2 ? =\n\n1\n\n2\n\n[\n?Qnj ? + ?Qnj?1?\n\n]\n? ?\n\n2\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]\n(4.17)\n\nand\n\n?Qn+1j ? =\n1\n\n2\n\n[\n?Qn+1/2\n\nj+1/2\n? + ?Qn+1/2\n\nj?1/2 ?\n]\n? ?\n\n2\n\n[\n?Qn+1/2\n\nj+1/2\n? ? ?Qn+1/2\n\nj?1/2 ?\n]\n, (4.18)\n\nor, joining these expressions,\n\n?Qn+1j ? = ?Qnj ? ?\n?\n\n2\n\n[\n?Qnj+1? ? ?Qnj?1?\n\n]\n+\n\n1\n\n4\n\n(\n1 + ?2\n\n) [\n?Qnj+1? ? 2?Qnj ? + ?Qnj?1?\n\n]\n. (4.19)\n\n\n\n40 Cap??tulo 4\n\nUsing (4.15) and (4.17) in (4.16), we can summarize the two-step scheme for the\n\nvariance in the explicit form:\n\nV n+1j =V\nn\nj ?\n\n?\n\n2\n\n[\nV nj+1 ? V nj?1\n\n]\n+\n\n1\n\n4\n\n(\n1 + ?2\n\n) [\nV nj+1 ? 2V nj + V nj?1\n\n]\n+\n\n1\n\n8\n(1 ? ?)\n\n[\n?Qnj+1? ? ?Qnj ?\n\n]2\n+\n\n1\n\n8\n(1 + ?)\n\n[\n?Qnj ? ? ?Qnj?1?\n\n]2\n+ (4.20)\n\n1\n\n16\n\n{[\n?Qnj+1? ? ?Qnj?1?\n\n]\n? ?\n\n[\n?Qnj+1? ? 2?Qnj ? + ?Qnj?1?\n\n]}2\n,\n\nwhere ? = ?t?A?/?x.\n\n4.3 Numerical analysis of the scheme\n\nIn this section, we analyze some numerical aspects of the method (4.19)\u2013(4.20), for the\n\nmean and variance of the solution of (4.1). We obtain the stability conditions of the\n\nscheme and we also show its consistency with a deterministic nonhomogeneous advective-\n\ndiffusive system.\n\nProposition 4.1. For ?x2/?t = ? fixed, the numerical scheme (4.19)-(4.20) is an\n\nO(?x2) approximation for u(x, t) and v(x, t), solutions of the deterministic system of\npartial differential equations:\n\n{\nut + ?A?ux = ?4 uxx\nvt + ?A?vx = ?4 vxx + ?2 u2x.\n\n(4.21)\n\nProof. Let v(x, t) and u(x, t) be smooth functions such that v(xj, tn) = V\nn\nj and u(xj, tn) =\n\n?Qnj ?. We have shown in [2] that if ?x2/?t = ? is fixed then the numerical scheme\n(4.19) gives an O(?x2) approximation for u(x, t), solution of the differential equation\nut + ?A?ux = (?/4)uxx.\nAlso, using Taylor\u2019s expansion in (4.20) we have\n\n[\nvt +\n\n?t\n\n2\nvtt + O(?t2)\n\n]\n+ ?A?\n\n[\nvx + O(?x2)\n\n]\n=\n\n?\n\n4\n\n(\n1 + ?2\n\n) [\nvxx + O(?x2)\n\n]\n+\n\n?\n\n8\n(1 ? ?)\n\n[\nux +\n\n?x\n\n2\nuxx + O(?x2)\n\n]2\n+\n\n?\n\n8\n(1 + ?)\n\n[\nux ?\n\n?x\n\n2\nuxx + O(?x2)\n\n]2\n+\n\n?\n\n4\n\n{[\nux + O(?x2)\n\n]\n? ?\n\n2\n\n[\nuxx + O(?x2)\n\n]}2\n.\n\nSince ?x2/?t = ? is fixed, we have ? = ?t?A?/?x = ?x?A?/? = O(?x) and ?t =\nO(?x2).\n\n\n\n4.4. Numerical examples 41\n\nThus, grouping the terms of the same order, we obtain\n\nvt + ?A?vx =\n?\n\n4\nvxx +\n\n?\n\n2\nu2x + O(?x2).\n\nRemark 4.2. Although the consistency result is for any ?, computational tests have shown\n\nthat a good choice for ? = ?x2/?t, in (4.21), to calculate ?Q(x, T )? and Var(Q(x, T )),\nis ? = 2Var[A]T .\n\nRemark 4.3. The modified equations in (4.21) constitute a decoupled deterministic non-\n\nhomogeneous advective-diffusive system whose transport terms are the mean of the velocity,\n\nand the diffusive terms are the same. The source term in the second equation involves the\n\nspatial derivative of the mean, given by the first equation.\n\nRemark 4.4. In [2] we have shown that the stability condition of (4.19) is (4.12). On\n\nthe other hand, since the terms corresponding to the mean can be considered source terms,\n\nthe method for the variance, (4.20), has the same stability conditions, i.e., (4.12). As a\n\nlinear problem, we have convergence.\n\n4.4 Numerical examples\n\nTo assess our method for the variance we present two numerical examples. In Example 4.1\n\nwe solve a Riemann problem in which the exact values of ?Q(x, t)? and Var[Q(x, t)] are\nknown. In Example 4.2 we apply the method in a problem with random velocity and a\n\ncorrelated random field as the initial condition. In both examples we use A normally and\n\nlognormally distributed. The value of ?x is presented in the caption of the figures. The\n\nvalue of ?t was chosen based on Remark 4.2, i.e., we used ? = ?x2/?t = 2Var[A]T . All\n\nthe numerical experiments presented in this section were computed in double precision\n\nwith some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of memory.\n\nExample 4.1.\n\nLet us consider the problem (4.1)\u2013(4.2) where the random variables U?0 and U\n+\n0 have\n\na bivariate normal distribution defined by: ?U?0 ? = 1 (mean of UL); ?U +0 ? = 0 (mean\nof UR); V ar[U\n\n?\n0 ] = 0.16 (variance of U\n\n?\n0 ); V ar[U\n\n+\n0 ] = 0.25 (variance of U\n\n+\n0 ); and ? =\n\n0 (correlation coefficient between U?0 and U\n+\n0 ). In Figures 4.1 \u2013 4.4 we compare the\n\napproximations of the mean and variance calculated using (4.19) and (4.20), respectively,\n\nwith the exact values given by (4.4) and (4.5). We plot the results at T = 0.3 and T = 0.5.\n\nTo observe the influence of the velocity variation we use two models: [i] A is normally\n\ndistributed, A ? N (1.0, 0.6), in Figures 4.1 and 4.2; [ii] A is lognormally distributed,\nA = exp (?), ? ? N (0.5, 0.25), in Figures 4.3 and 4.4.\n\n\n\n42 Cap??tulo 4\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed method\nExact solution\n\n?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nVariance of solution\n\nFigure 4.1: ?x = 0.02 and T = 0.3.\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed method\nExact solution\n\n?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nVariance of solution\n\nFigure 4.2: ?x = 0.02 and T = 0.5.\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed method\nExact solution\n\n?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nVariance of solution\n\nFigure 4.3: ?x = 0.01 and T = 0.3.\n\n\n\n4.5. Concluding remarks 43\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\n\nProposed method\nExact solution\n\n?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nVariance of solution\n\nFigure 4.4: ?x = 0.01 and T = 0.5.\n\nExample 4.2.\n\nIn this example we consider the random partial differential equation (4.1) with initial\n\ncondition being the normal random field with mean\n\n?Q0(x)? =\n{\n\n1, x ? (1.4, 2.2),\ne?20(x?0.25)\n\n2\n\n, otherwise,\n(4.22)\n\nand covariance Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2 is constant. The\nparameter ? > 0 governs the decay rate of the spatial correlation. In the tests we use\n\n? = 40 and ?2 = 0.12. The numerical results are compared with the Monte Carlo method\n\nusing suites of realizations of A and Q0(x), with A and Q0(x) statistically independent.\n\nThe analytical solution of each realization A(?) and Q0(x, ?) is given by Q(x, t, ?) =\n\nQ0(x?A(?)t, ?). The 2000 realizations of the correlated random field Q0(x) are generated\nusing the matriz decomposition method, a direct method for generating correlated random\n\nfields (see [9], Ch. 3, for example). As in the previous example, we use two models of\n\nvelocity: [i] A is normally distributed, A ? N (?0.5, 0.6), in Figures 4.5 and 4.6; [ii] A is\nlognormally distributed, A = exp (?), ? ? N (0.15, 0.25), in Figures 4.7 and 4.8.\n\n4.5 Concluding remarks\n\nIn this paper, we extend the ideas presented in our previous work [2] to obtain more\n\ninformation about the statistical properties of the solution to the one-dimensional random\n\nlinear transport equation. We show that the ideas of the Godunov method can also be used\n\nto design a numerical scheme to calculate the variance of the solution: (4.19) and (4.20).\n\nWe also present the stability conditions and the consistency of the numerical scheme with\n\nthe decoupled system of advective-diffusive equations (4.21). Computational results are\n\n\n\n44 References\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution\nMean of initial condition\nProposed method\nMonte Carlo simulations\n\n?1 0 1 2 3\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nVariance of solution\n\nFigure 4.5: ?x = 0.02 and T = 0.3.\n\n?1 0 1 2 3\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nMean of solution Mean of initial condition\nProposed method\nMonte Carlo simulations\n\n?1 0 1 2 3\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\nVariance of solution\n\nFigure 4.6: ?x = 0.02 and T = 0.5.\n\ncompared with the exact solution, in the Riemann problem, and with the Monte Carlo\n\nmethod in a more general situation. As far as we know, this kind of methodology has not\n\nbeen used to deal with differential equations with uncertainties in the parameters.\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) through the grants 5551463/02-3 and 140406/2004-2 (doctoral scholarship).\n\nReferences\n\n[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random\n\ntransport equation. Computational and Applied Mathematics 26(3):323\u2013335 (2007).\n\n\n\nReferences 45\n\n?0.5 0 0.5 1 1.5 2 2.5 3 3.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution Mean of initial condition\nProposed method\nMonte Carlo simulations\n\n?0.5 0 0.5 1 1.5 2 2.5 3 3.5\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\nVariance of solution\n\nFigure 4.7: ?x = 0.01 and T = 0.3.\n\n?0.5 0 0.5 1 1.5 2 2.5 3 3.5\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nMean of solution Mean of initial condition\nProposed method\nMonte Carlo simulations\n\n?0.5 0 0.5 1 1.5 2 2.5 3 3.5\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\nVariance of solution\n\nFigure 4.8: ?x = 0.01 and T = 0.5.\n\n[2] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution\n\nof the random transport equation. Appl. Math. and Comput. 187(2):912\u2013921 (2007).\n\n[3] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03\u201344, Providence, 1998.\n\n[4] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntions of the equations of hydrodynamics. Mat. Sb. 47:271\u2013306 (1959).\n\n[5] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n[6] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-\n\ndiretional stochastic transport equation. SIAM Journal on Scientific Computing\n\n19(3):799\u2013812 (1998).\n\n\n\n46 References\n\n[7] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.\n\nWadsworth &amp; Brooks/Cole, California, 1989.\n\n[8] J. W. Thomas, Numerical Partial Differential Equations: finite difference methods.\n\nSpringer-Verlag, New York, 1995.\n\n[9] D. Zhang, Stochastic Methods for Flows in Porous Media - Coping with Uncertainties.\n\nAcademic Press, 2002.\n\n\n\nCap??tulo 5\n\nStatistical moments of the random\n\nlinear transport equation\n\nAbstract\n\nThis paper deals with a numerical scheme to approximate the mth moment of the solution\n\nof the one-dimensional random linear transport equation. The initial condition is assumed\n\nto be a random function and the transport velocity is a random variable. The scheme\n\nis based on local Riemann problem solutions and Godunov\u2019s method. We show that the\n\nscheme is consistent and stable with an advective-diffusive equation. Furthermore, in the\n\ncase where the velocity is normally distributed we obtain partial differential equations for\n\nthe moments and the central moments. Numerical examples are added to illustrate our\n\napproach.\n\nKeyword: random linear transport equation, Riemann problem, statistical moments, Go-\n\ndunov\u2019s method, numerical methods for random partial differential equations.\n\n5.1 Introduction\n\nPartial differential equations have been important models during the last centuries, mainly\n\nbecause they have the fundamental support of differential calculus, numerical methods,\n\nand computers. However, the formulation of a physical process as a partial differential\n\nequation demands experiments to measure the data, for example, the diffusion coefficient,\n\npermeability of a porous media, initial conditions, boundary conditions and so on. This\n\nmeans that the interpretation of the data as random variables is more realistic in some\n\npractical situations. Differential equations with random parameters are called Random\n\nDifferential Equations; new mathematical methods have been developed to deal with this\n\nkind of problems (see [6, 8, 12, 15], for example).\n\n47\n\n\n\n48 Cap??tulo 5\n\nWe are interested in the solution of random linear transport equations given by\n{\n\nQt(x, t) + AQx(x, t) = 0, t > 0, x ? R,\nQ(x, 0) = Q0(x),\n\n(5.1)\n\nwhere A is a random variable and Q0(x) is a random function.\n\nThe solution for the random Riemann problem to (5.1),\n\nQ0(x) =\n\n{\nQL, if x &lt;0,\n\nQR, if x > 0,\n(5.2)\n\nwith QL and QR being random variables is given by [1]:\n\nQ(x, t) = QL + X\n(x\n\nt\n\n)\n(QR ? QL) . (5.3)\n\nIn (5.3) X is the Bernoulli random variable with P {X(?) = 1} = FA(?), the cumulative\nprobability function of the random variable A. Furthermore, in case of independence\n\nbetween A and both QL and QR, the mth moment of Q(x, t), ?Qm(x, t)?, m ? N, m ? 1,\nis given by\n\n?Qm(x, t)? = ?QmL ? + FA\n(x\n\nt\n\n)\n[?QmR? ? ?QmL ?] . (5.4)\n\nThe closed solution (5.3) and Godunov\u2019s ideas [7, 9, 10] are used in [4] and [2] to\n\ndesign numerical methods to compute the mean and the variance of the solution to (5.1).\n\nThe methods are explicit and neither demand generation of random numbers (as does\n\nthe Monte Carlo method [5, 11, 14, 17]), nor require differential equations governing the\n\nstatistical moments (as in the effective equations methodology [6, 17]). Moreover, the\n\nschemes are stable and consistent with an advective-diffusive equation which agrees with\n\nthe effective equation to the expectation presented in the literature (see [6], for example).\n\nIn [3] we use the idea of collecting deterministic realizations through their probability\n\nfunctions to solve the nonlinear random Riemann-Burgers equation.\n\nIn this paper, we deal with the general moments of the solution to (5.1). The outline of\n\nthis paper is as follows. In Section 5.2 we use (5.3) and (5.4) to design a numerical method\n\nto the mth statistical moment of the solution to the general problem (5.1). We present\n\nthe CFL condition under which the local solutions do not interact between themselves.\n\nIn Section 5.3 we show the stability of the numerical scheme and its consistency with an\n\nadvective-diffusive equation. We show that the diffusion coefficient is related with the\n\nprobability density function of the velocity by Eq. (5.18), which has a simple solution\n\nin the normal velocity case. In Section 5.4 we present a decoupled system of partial\n\ndifferential equations to be satisfied by the central moments of the random solution. All\n\nthe partial differential equations in this paper are linear. In fact, denoting by L(u) =\nut + ?A?ux ? ?uxx, the equations are of the form: L(u) = 0, for the moments, and\nL(u) = f , for the central moments. Computational experiments and comparisons with\nthe Monte Carlo method are presented in Section 5.5.\n\n\n\n5.2. The numerical scheme 49\n\n5.2 The numerical scheme\n\nIn this section, we present the numerical method for the mth statistical moment of the\n\nsolution to (5.1). The method is based on the juxtaposition of Riemann problems whose\n\nsolutions are given by (5.3). We discretize both space and time assuming a uniform mesh\n\nspacing: xj = j?x, xj\u00b11/2 = xj \u00b1(?x/2), tn = n?t, tn\u00b11/2 = tn \u00b1(?t/2), for ?x, ?t > 0.\nIn Figure 5.1 we present a schematic diagram of the algorithm. Let us assume that the\n\nrandom variables Qnj and the mth moments ?Qm,nj ? = ?Qm(xj, tn)? are known at t = tn.\n\nxj+3/2\n\nQnj?1 Q\nn\nj Q\n\nn\nj+1\n\nQ\nn+1/2\n\nj?1/2\nQ\n\nn+1/2\n\nj+1/2\n\nQ\nn+1\nj\n\nxj?3/2 xj?1 xj?1/2 xj xj+1/2 xj+1\n\ntn\n\ntn+1/2\n\ntn+1\n\nFigure 5.1: Schematic diagram of the algorithm.\n\nIn the following we use the ideas of REA, Reconstruct-Evolve-Average, algorithm\n\n[7, 10] to approximate ?Qm,n+1j ? = ?Qm(xj, tn+1)?.\nStep 1. We reconstruct the piecewise random constant function Q?(x, tn) from Q\n\nn\nj ,\n\ni.e, Q?(x, tn) = Q\nn\nj for x ? [xj?1/2, xj+1/2].\n\nThe piecewise constant random function Q?(x, tn) defines a set of local random Riemann\n\nproblems, each one centered at x = xj?1/2,\n\nQt(x, t) + AQx(x, t) = 0, t > tn, x ? R,\n\nQ(x, tn) =\n\n{\nQnj?1, if x &lt;xj?1/2,\nQnj , if x > xj?1/2.\n\n(5.5)\n\nStep 2. From (5.3) and (5.4), the local solutions of (5.5) and the respective statistical\n\nmoments are given by\n\nGj?1/2(x, tn+1/2) = Q\nn\nj?1 + X\n\n(\nx ? xj?1/2\n\n?t/2\n\n) [\nQnj ? Qnj?1\n\n]\n(5.6)\n\n\n\n50 Cap??tulo 5\n\nand\n\n?Gmj?1/2(x, tn+1/2)? = ?Qm,nj?1? + FA\n(\n\nx ? xj?1/2\n?t/2\n\n) [\n?Qm,nj ? ? ?Qm,nj?1?\n\n]\n. (5.7)\n\nThe global solution at t = tn+1/2, Q?(x, tn+1/2), can be constructed by piecing together\n\nthe local random Riemann solutions (5.6), provided that ?t/2 is sufficiently small such\n\nthat adjacent local random Riemann solutions do not interact. Therefore, taking into\n\naccount the similarity property of the random Riemann solutions, ?x and ?t must be\n\nchosen such that:\n\nGj?1/2(x, tn+1/2)\n??\nx=xj?1\n\n? Qnj?1, Gj?1/2(x, tn+1/2)\n??\nx=xj\n\n? Qnj ,\n\nwhere the symbol \u201c ? \u201d means \u201csufficiently near to\u201d. By substituting these conditions in\n(5.6) we must have\n\nFA\n\n(\n??x\n\n?t\n\n)\n? 0 and FA\n\n(\n?x\n\n?t\n\n)\n? 1. (5.8)\n\nRemark 5.1. We may regard (5.8) as the CFL condition for the method: the interval\n\n[??x/?t, ?x/?t] must contain an effective support of the density probability function\nof A. This means that the probability of A outside of the interval [??x/?t, ?x/?t]\nis sufficiently near to zero, and then may be disregarded. The existence of an effective\n\nsupport is ensured by Chebyshev\u2019s inequality: P{|A ? ?A?| ? k?A} ? 1/k2, for all k > 0,\nwhere ?A is the standard variation of A. Therefore, if we take 1/k\n\n2 sufficiently close to\n\nzero, to escape from the interaction between solutions of Riemann problems we must take\n\n(|?A?| + k?A) ?t/?x ? 1.\n\nUnder condition (5.8), we conclude Step 2 by taking\n\nQ?(x, tn+1/2) =\n?\n\nj?1/2\nGj?1/2(x, tn+1/2) 1[xj?1,xj ]\n\nwhere 1[a,b] denotes the characteristic function of the interval [a, b]. From (5.7) it follows\n\nthat\n\n?Q?m(x, tn+1/2)? =\n?\n\nj?1/2\n?Gmj?1/2(x, tn+1/2)? 1[xj?1,xj ]. (5.9)\n\nIn a similar way, using the values at t = tn+1/2, we obtain\n\n?Q?m(x, tn+1)? =\n?\n\nj\n\n?Gmj (x, tn+1)? 1[xj?1/2,xj+1/2]. (5.10)\n\n\n\n5.2. The numerical scheme 51\n\nStep 3. We use (5.10) to approximate ?Qm,n+1j ? as the average value of ?Q?m(x, tn+1)?\nover the interval [xj?1/2, xj+1/2]:\n\n?Qm,n+1j ? '\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\n?Q?m(x, tn+1)?dx =\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\n?Gmj (x, tn+1)?dx\n\n=\n1\n\n?x\n\n? xj+1/2\nxj?1/2\n\n{\n?Qm,n+1/2\n\nj?1/2 ? + FA\n(\n\nx ? xj\n?t/2\n\n) [\n?Qm,n+1/2\n\nj+1/2\n? ? ?Qm,n+1/2\n\nj?1/2 ?\n]}\n\ndx\n\n= ?Qm,n+1/2\nj?1/2 ? +\n\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nFA(x)dx\n\n} [\n?Qm,n+1/2\n\nj+1/2\n? ? ?Qm,n+1/2\n\nj?1/2 ?\n]\n. (5.11)\n\nLikewise, we use (5.9) to approximate ?Qm,n+1/2\nj?1/2 ?:\n\n?Qm,n+1/2\nj?1/2 ? '\n\n1\n\n?x\n\n? xj\nxj?1\n\n?Q?m(x, tn+1/2)?dx =\n1\n\n?x\n\n? xj\nxj?1\n\n?Gmj?1/2(x, tn+1/2)?dx\n\n=\n1\n\n?x\n\n? xj\nxj?1\n\n{\n?Qm,nj?1? + FA\n\n(\nx ? xj?1/2\n\n?t/2\n\n) [\n?Qm,nj ? ? ?Qm,nj?1?\n\n]}\ndx\n\n= ?Qm,nj?1? +\n?t\n\n2?x\n\n{? ?x\n?t\n\n??x\n?t\n\nFA(x)dx\n\n}\n[\n?Qm,nj ? ? ?Qm,nj?1?\n\n]\n. (5.12)\n\nThe following result is proved in [4]:\n\nLemma 5.1. Let Y be a random variable and [??, ?] an effective support of the density\nprobability function, fY , of Y , i.e., FY (??) ? 0 and FY (?) ? 1. Then\n\n? ?\n??\n\nFY (x)dx ? ? ? ?Y ?. (5.13)\n\nInserting (5.13) in (5.11) and (5.12), and denoting ? = ?t?A?/?x, gives\n\n?Qm,n+1j ? =\n1\n\n2\n\n[\n?Qm,n+1/2\n\nj?1/2 ? + ?Q\nm,n+1/2\nj+1/2\n\n?\n]\n? ?\n\n2\n\n[\n?Qm,n+1/2\n\nj+1/2\n? ? ?Qm,n+1/2\n\nj?1/2 ?\n]\n\n(5.14)\n\nand\n\n?Qm,n+1/2\nj?1/2 ? =\n\n1\n\n2\n\n[\n?Qm,nj?1? + ?Qm,nj ?\n\n]\n? ?\n\n2\n\n[\n?Qm,nj ? ? ?Qm,nj?1?\n\n]\n. (5.15)\n\nGrouping these expressions we summarize the two-step scheme (5.14)\u2013(5.15) in the\n\none-step explicit method:\n\n?Qm,n+1j ? = ?Qm,nj ? ?\n?\n\n2\n\n[\n?Qm,nj+1? ? ?Qm,nj?1?\n\n]\n+\n\n+\n1\n\n4\n\n(\n1 + ?2\n\n) [\n?Qm,nj+1? ? 2?Qm,nj ? + ?Qm,nj?1?\n\n]\n. (5.16)\n\n\n\n52 Cap??tulo 5\n\nRemark 5.2. The numerical scheme (5.16) is conservative, i.e., it can be rewritten as\n\n?Qm,n+1j ? = ?Qm,nj ? ?\n?t\n\n?x\n\n[\nF\n\nm,n\nj+1/2\n\n? F m,n\nj?1/2\n\n]\n,\n\nwhere F\nm,n\nj?1/2 = (1/2)?A?[?Q\n\nm,n\nj?1? + ?Qm,nj ?] ? (1/4)?A? (1/? + ?) [?Qm,nj ? ? ?Qm,nj?1?] is an\n\napproximation to the average flux at x = xj?1/2.\n\n5.3 Numerical analysis of the scheme\n\nThe scheme (5.16) is a generalization of a previously studied scheme to the mean (m = 1)\n\nof the solution to (5.1). Therefore, we can use the same arguments used in [4] to show\n\n\u2022 consistency: if ? = ?x2/(4?t) is fixed then the numerical scheme (5.16) yields an\nO(?x2) approximation for the solution of the partial differential equation\n\nut + ?A?ux = ?uxx; (5.17)\n\n\u2022 stability: the numerical method (5.16) is stable under the CFL condition (5.8).\n\nAs a linear problem, the convergence of (5.16) to the differential equation (5.17) is a\n\nconsequence of the Lax Equivalence Theorem, no matter what ? = ?x2/(4?t) is. The\n\nfollowing proposition gives an additional information about the diffusion associated with\n\nthe random velocity, A.\n\nProposition 5.1. The diffusion coefficient in (5.17) must satisfy\n\n?f ?A\n( x\n\nt\n\n)\n?(x, t) = fA\n\n(x\nt\n\n)\n(x ? ?A?t) , (5.18)\n\nwhere fA(?) = d[FA(?)]/d? is the density probability function of A.\n\nProof. As a general differential equation, (5.17) must be satisfied by a particular solu-\n\ntion. The random Riemann problem (5.1)\u2013(5.2) is a particular case of (5.1) with known\n\nmoments given by (5.4):\n\n?Qm(x, t)? = ?QmL ? + FA\n(x\n\nt\n\n)\n[?QmR? ? ?QmL ?] .\n\nDirect derivations and substitution of this solution in (5.17) gives (5.18), a necessary\n\ncondition to ?(x, t).\n\n\n\n5.4. The system of partial differential equations for the central moments 53\n\n5.3.1 The Normal case\n\nLet A ? N (?A?, ?A). Using the normal probability density function in (5.18) we obtain\n? = ?2At. In this case, the differential equation (5.17) turns to be\n\nut + ?A?ux = (?2At)uxx, t > 0, (5.19)\nwhich agrees with the effective equation for the statistical mean presented by some authors\n\n(see [6], for example). We emphasize that our convergence results show that the differential\n\nequation which describes the evolution of all the moments is the same. Using (5.18) we\n\nmay also show that if ?(x, t) depends only on t then A is normally distributed.\n\nNow let t = tf be fixed, and select ?t and ?x such that\n\n?x2\n\n4?t\n= ? =\n\n1\n\n2\n(?2Atf ). (5.20)\n\nThe convergence results show that our method converges to the solution of the differential\n\nequation\n\nut + ?A?ux =\n1\n\n2\n(?2Atf )uxx. (5.21)\n\nThe solutions of (5.19) and (5.21), u1(x, t) and u2(x, t), respectively, both with u(x, 0) =\n\ng(x), are equal at t = tf . Indeed, according to [13] we have\n\nu1(x, tf ) =\n1?\n\n??1(tf )\n\n? +?\n??\n\nexp\n\n[\n?\n\n(\nx ? ?A?tf ? ?\n\n?1(tf )\n\n)2]\ng(?) d?, (5.22)\n\nwhere\n\n?1(tf ) = 2\n\n[? tf\n0\n\n(?2As) ds\n\n]1/2\n=\n?\n\n2?Atf .\n\nOn the other hand, the solution to (5.21) is also given by (5.22) with\n\n?2(tf ) = 2\n\n[? tf\n0\n\n[(?2Atf )/2] ds\n\n]1/2\n\ninstead of ?1(tf ). Since ?1(tf ) = ?2(tf ) then u1(x, tf ) = u2(x, tf ).\n\nThe condition (5.20) can be rewritten as ?x/?t = 2?2Atf /?x. Thus, the CFL condi-\n\ntion (5.8) may be satisfied for ?x sufficiently small.\n\n5.4 The system of partial differential equations for\n\nthe central moments\n\nCentral moments of a given random function Q(x, t) are deterministic functions defined\n\nby \u00b5m = ?(Q ? ?Q?)m?, m ? N, m ? 2. The most used central moment is the variance,\n\n\n\n54 Cap??tulo 5\n\nm = 2, which was introduced by K. F. Gauss (1777-1855) as a measure of dispersion of\n\nthe distribution of Q(x, t). But high order central moments are also useful information\n\nconcerning random variables [12, 15].\n\nIn the following we show that the central moment \u00b5m(x, t), if sufficiently smooth,\n\nsatisfies an advective-diffusive equation with the source term defined by the expectation\n\nand the central moments \u00b5m?1(x, t) and \u00b5m?2(x, t). Here, we may extend the definition\nof central moments for m ? 0 since \u00b50 = 1 and \u00b51 = 0.\n\nWe may use algebraic manipulations to show that\n\n(i) If k ? m ? 2 then\n(\n\nm\n\nk + 2\n\n)\n(k + 1)(k + 2) =\n\n(\nm\n\nk\n\n)\n(m ? k)(m ? k ? 1). (5.23)\n\n(ii) If k ? m ? 1 then (\nm\n\nk + 1\n\n)\n(k + 1) =\n\n(\nm\n\nk\n\n)\n(m ? k). (5.24)\n\n(iii)\n\n\u00b5m = ?Qm? ?\nm?1?\n\nk=2\n\n(\nm\n\nk\n\n)\n\u00b5k?Q?m?k ? ?Q?m. (5.25)\n\nProposition 5.2. Let Z(x,t) be a random function whose statistical moments satisfy\n\n(5.17), i.e., the advective-diffusive equations:\n\n?Zm?t + ?A??Zm?x = ??Zm?xx, (5.26)\n\nm ? N, m ? 1. Then the central moments, \u00b5m(x, t) = ?[Z ? ?Z?]m?, m ? N, m ? 2,\nsatisfy the advective-diffusive equations with source term:\n\n\u00b5m,t + ?A?\u00b5m,x ? ?\u00b5m,xx = 2m?\u00b5m?1,x ?Z?x + m(m ? 1)?\u00b5m?2?Z?2x, (5.27)\n\nwhere \u00b50 = 1 and \u00b51 = 0.\n\nProof. The proof is based on the induction principle. Since \u00b52(x, t) = ?Z2(x, t)? ?\n?Z(x, t)?2, \u00b51(x, t) = 0 and \u00b50(x, t) = 1, direct substitution and derivations show (5.27)\nfor k = 2. As the induction hypothesis we assume that (5.27) is true for k = 3 : (m ? 1),\nand our task is to prove that (5.27) is true for k = m.\n\nFrom (5.25) we have \u00b5m(x, t) = ?Zm? ?\nm?1?\n\nk=2\n\n(\nm\n\nk\n\n)\n\u00b5k?Z?m?k ? ?Z?m. By differentiating\n\nthis expression with respect to t and x, grouping conveniently the terms, and using (5.26)\n\n\n\n5.4. The system of partial differential equations for the central moments 55\n\nwe arrive at\n\n\u00b5m,t + ?A?\u00b5m,x ? ?\u00b5m,xx =\n\n?\nm?1?\n\nk=2\n\n(\nm\n\nk\n\n)\n?Z?m?k {\u00b5k,t + ?A?\u00b5k,x ? ?\u00b5k,xx} +\n\n+ 2 ?\nm?1?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) \u00b5k,x ?Z?m?k?1 ?Z?x+\n\n+ ?\nm?2?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) (m ? k ? 1) \u00b5k ?Z?m?k?2 (?Z?x)2+\n\n+ ? m (m ? 1) ?Z?m?2?Z?2x. (5.28)\nUsing the induction hypothesis in the first sum in (5.28), and separating the last term of\n\nthe second and third sums, we obtain\n\n\u00b5m,t + ?A?\u00b5m,x ? ?\u00b5m,xx =\n\n?\nm?1?\n\nk=2\n\n(\nm\n\nk\n\n)\n?Z?m?k\n\n{\n2k?\u00b5k?1,x?Z?x + k(k ? 1)?\u00b5k?2(?Z?x)2\n\n}\n+\n\n+ 2 ?\nm?2?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) \u00b5k,x ?Z?m?k?1 ?Z?x+\n\n+ ?\nm?3?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) (m ? k ? 1) \u00b5k ?Z?m?k?2 ?Z?2x+\n\n+ 2 m ? \u00b5m?1,x ?Z?x + m (m ? 1) ? \u00b5m?2 ?Z?2x+\n+ ? m (m ? 1) ?Z?m?2?Z?2x? ?? ?\n\nequal the first sum with k=2.\n\n,\n\nor, equivalently,\n\n\u00b5m,t + ?A?\u00b5m,x ? ?\u00b5m,xx = 2m?\u00b5m?1,x?Z?x + m(m ? 1)?\u00b5m?2?Z?2x?\n\n? ?\nm?1?\n\nk=3\n\n(\nm\n\nk\n\n)\n?Z?m?k\n\n{\n2k\u00b5k?1,x?Z?x + k(k ? 1)\u00b5k?2?Z?2x\n\n}\n+\n\n+ ?\nm?2?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) 2 \u00b5k,x ?Z?m?k?1 ?Z?x+\n\n+ ?\nm?3?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k) (m ? k ? 1) \u00b5k ?Z?m?k?2 ?Z?2x. (5.29)\n\n\n\n56 Cap??tulo 5\n\nTo show that the three sums on the right side of (5.29) are zero, we open the first one of\n\nthem:\n\nm?1?\n\nk=3\n\n(\nm\n\nk\n\n)\n?Z?m?k\n\n{\n2 k \u00b5k?1,x?Z?x + k(k ? 1)\u00b5k?2?Z?2x\n\n}\n=????\n\n\u00b51=0\n\n=\nm?1?\n\nk=3\n\n(\nm\n\nk\n\n)\n?Z?m?k 2k\u00b5k?1,x?Z?x +\n\nm?1?\n\nk=4\n\n(\nm\n\nk\n\n)\n?Z?m?k k(k ? 1)\u00b5k?2?Z?2x =\n\n= 2\nm?2?\n\nk=2\n\n(\nm\n\nk + 1\n\n)\n(k + 1)?Z?m?k?1\u00b5k,x?Z?x+\n\n+\nm?3?\n\nk=2\n\n(\nm\n\nk + 2\n\n)\n(k + 1)(k + 2)?Z?m?k?2\u00b5k?Z?2x =????\n\nusing (5.23) and (5.24)\n\n= 2\nm?2?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k)\u00b5k,x?Z?m?k?1?Z?x+\n\n+\nm?3?\n\nk=2\n\n(\nm\n\nk\n\n)\n(m ? k)(m ? k ? 1)\u00b5k?Z?m?k?2?Z?2x.\n\nTherefore, from (5.29) we arrive at (5.27).\n\nRemark 5.3. In Section 5.3 we have shown that the numerical method (5.16), for the\n\nmoments, is stable and consistent with (5.17). Since we have used the same method (5.16)\n\nto compute the central moments, we conclude that the method for the central moments is\n\nstable and consistent with (5.27), equation (5.17) with a source term.\n\n5.5 Computational tests\n\nIn this section, we present some examples to assess our approach. In Examples 5.1\n\nand 5.2 the initial condition allows exact statistical moments of the solution. We use\n\nRiemann initial conditions defined by bivariate normal distributions; in this case the\n\nsolutions for the moments are given by (5.4). In order to investigate the influence of the\n\nrandomness we use two models: in Example 5.1 the velocity, A, is normally distributed,\n\nand in Example 5.2 the velocity is lognormally distributed. In both cases we compare the\n\nexact solutions, given by (5.4), with the solutions yielded by the numerical scheme (5.16)\n\nfor some statistical moments. In Example 5.3 we apply our method in the problem (5.1)\n\nwhere the initial condition is a normal random function and the transport velocity is a\n\nnormal random variable. The numerical experiments presented in this section were done\n\n\n\n5.5. Computational tests 57\n\nin double precision with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of\n\nmemory.\n\nExample 5.1.\n\nLet us consider the random Riemann problem (5.1)\u2013(5.2) where the random velocity\n\nis normally distributed, A ? N (1.0, 0.8), and the random variables QL and QR have a\nbivariate normal distribution defined by: ?QL? = 1.0 (mean of QL); ?QR? = 0.0 (mean\nof QR); ?L = 0.4 (standard deviation of QL); ?R = 0.5 (standard deviation of QR); and\n\n? = 0.4 (correlation coefficient between QL and QR). In Figure 5.2 we compare the exact\n\nvalues for the mean, variance, 3rd central moment, and 4th central moment with the\n\ncomputations using (5.16) at tf = 0.4, and ?t and ?x satisfying (5.20).\n\n?2 ?1 0 1 2 3 4\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nmean\n\nnumerical scheme\nexact solution\n\n?2 ?1 0 1 2 3 4\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nvariance\n\n?2 ?1 0 1 2 3 4\n\n?0.15\n\n?0.1\n\n?0.05\n\n0\n\n0.05\n\n0.1\n\n3rd central moment\n\n?2 ?1 0 1 2 3 4\n\n?0.1\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n4th central moment\n\nFigure 5.2: A ? N (1.0, 0.8), ?x = 0.01, ?t = 0.000195, and tf = 0.4.\n\nExample 5.2.\n\nTo check the influence of the velocity distribution we consider the random Riemann\n\nproblem (5.1)\u2013(5.2) in which the random velocity is lognormally distributed, A = exp(?),\n\n\n\n58 Cap??tulo 5\n\n? ? N (0.5, 0.35). The initial condition (QL, QR) has a bivariate normal distribution\ndefined by: ?QL? = 1.0; ?QR? = 0.15; ?L = 0.36; ?R = 0.25; and ? = 0.4 . Taking the\nlognormal distribution, A = exp(?), ? ? N (\u00b5?, ??), in (5.18) we obtain\n\n?(x, t) =\n?2?\n\n(\nx\nt\n\n) (\nx\nt\n? ?A?t\n\n)\n\n(?2? ? \u00b5?) + ln\n(\n\nx\nt\n\n). (5.30)\n\nThis mean that it is not possible to find constants ?x and ?t such that (?x2)/(4?t) = ?,\n\nthe consistency condition. Moreover, the diffusion coefficient given by (5.30) may be\n\nphysically inappropriate since it can assume negative values. If we use (5.20) as in the\n\nprevious example the results loose quality as shown in Figure 5.3.\n\n?1 0 1 2 3 4\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nmean\n\nnumerical scheme\nexact solution\n\n?1 0 1 2 3 4\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\n0.55\n\nvariance\n\n?1 0 1 2 3 4\n?0.04\n\n?0.02\n\n0\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.14\n\n3rd central moment\n\n?1 0 1 2 3 4\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n4th central moment\n\nFigure 5.3: A = exp(?), ? ? N (0.5, 0.35), ?x = 0.01, ?t = 0.000312, and tf = 0.4.\n\nExample 5.3.\n\nIn this example we test our method for the random partial differential equation (5.1)\n\nin which A is normal, A ? N (?0.5, 0.6), and Q0(x) is a normal random function with\n\n\n\n5.5. Computational tests 59\n\nmean\n\n?Q0(x)? =\n{\n\n1, x ? (1.4, 2.2),\ne?20(x?0.25)\n\n2\n\n, otherwise,\n(5.31)\n\nand covariance Cov(x, x?) = ?2 exp (??|x ? x?|), where Var[Q0(x)] = ?2 is constant and\n? > 0 governs the decay rate of the spatial correlation. We use ? = 0.3 and ?2 =\n\n0.16. The numerical results are compared with Monte Carlo simulations using suites of\n\nrealizations of A and Q0(x), where A and Q0(x) are statistically independents. Observe\n\nthat each realization A(?) and Q0(x, ?) implies analytical solution given by Q(x, t, ?) =\n\nQ0(x ? A(?)t, ?). To generate the realizations required by Monte Carlo simulations we\nuse random numbers generator of MATLAB. Comparisons with the Monte Carlo method,\n\nwith 30 000 realizations, are plotted in Figure 5.4.\n\n?2 ?1 0 1 2 3\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\nmean\n\nnumerical scheme\nMonte Carlo\nmean of init. cond.\n\n?2 ?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\nvariance\n\n?2 ?1 0 1 2 3\n\n?0.1\n\n?0.05\n\n0\n\n0.05\n\n0.1\n\n3rd central moment\n\n?2 ?1 0 1 2 3\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n4th central moment\n\nFigure 5.4: A ? N (?0.5, 0.6), ?x = 0.02, ?t = 0.000138, and tf = 0.4.\n\n\n\n60 References\n\n5.6 Conclusions\n\nIn this paper, we have used the Godunov ideas to obtain a numerical scheme for the\n\nstatistical moments of the solution of the one-dimensional random linear transport equa-\n\ntion. We consider the velocity as a random variable and the initial condition as a random\n\nfunction. We have used an explicit solution of the random Riemann problem to evolve\n\nin the REA algorithm. Moreover, we have shown that the scheme is stable and consis-\n\ntent with an advective-diffusive equation. A particular Riemann problem solution is used\n\nto find the diffusion coefficient of the differential equations for the statistical moments.\n\nAlso, we have obtained the differential equations for the central moments of the solution.\n\nComputational tests have illustrated our theoretical results.\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) through the grant 140406/2004-2.\n\nReferences\n\n[1] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random\n\ntransport equation. Computational and Applied Mathematics 26(3):323\u2013335 (2007).\n\n[2] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution\n\nof the random transport equation. Appl. Math. Comput. 190(1):362\u2013369 (2007).\n\n[3] M. C. C. Cunha, F. A. Dorini, Statistical moments of the solution of the random\n\nBurgers-Riemann problem. Technical Report 11/07, Imecc, Unicamp, Campinas,\n\nBrazil, (2007).\n\nhttp://www.ime.unicamp.br/rel pesq/2007/rp11-07.html.\n\n[4] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution\n\nof the random transport equation. Appl. Math. and Comput. 187(2):912\u2013921 (2007).\n\n[5] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer-Verlag,\n\nNew York, 1996.\n\n[6] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03-44, Providence, 1998.\n\n\n\nReferences 61\n\n[7] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntions of the equations of hydrodynamics. Mat. Sb. 47:271\u2013306 (1959).\n\n[8] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.\n\nSpringer, New York, 1999.\n\n[9] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.\n\n[10] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n[11] P. O\u2019Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-\n\ntardation: numerical results. Computer Methods in Water Resources XII 1:255\u2013261\n\n(1998).\n\n[12] B. Oksendal, Stochastic Differential Equations: an introduction with applications.\n\nSpringer, New York, 2000.\n\n[13] A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and\n\nScientists. Chapman &amp; Hall/CRC, New York, 2002.\n\n[14] G. I. Schue?ller, A state-of-the-art report on computational stochastic mechanics.\n\nProb. Engrg. Mech. 12(4):197\u2013322 (1997).\n\n[15] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic\n\nPress, New York, 1973.\n\n[16] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.\n\nWadsworth &amp; Brooks/Cole, California, 1989.\n\n[17] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.\n\nAcademic Press, San Diego, 2002.\n\n\n\nCap??tulo 6\n\nStatistical moments of the solution\n\nof the random Burgers-Riemann\n\nproblem\n\nAbstract\n\nWe solve Burgers\u2019 equation with random Riemann initial conditions. The closed solution\n\nallows simple expressions for its statistical moments. Using these ideas we design an\n\nefficient algorithm to calculate the statistical moments of the solution. Our methodology\n\nis an alternative to the Monte Carlo method. The present approach does not demand a\n\nrandom numbers generator as does the Monte Carlo method. Computational tests are\n\nadded to validate our approach.\n\nKeyword: random Burgers\u2019 equation, Monte Carlo method, Riemann problem, statistical\n\nmoments, numerical methods for random partial differential equations.\n\n6.1 Introduction\n\nWhen the data of a differential equation, the coefficients or the initial conditions, are\n\nrandom variables its solution is a random function; this kind of mathematical problem\n\nhas been called a random differential equation. A great number of practical processes\n\nunder current investigations falls on the stochastic modeling; we may quote the models\n\nin control, communications, economic systems, chemical kinetics, biosciences, statistical\n\nmechanics and spatial areas and so on. The methodology to understand and solve diffe-\n\nrential equations with uncertainties has stimulated studies under several points of view.\n\nSince the solution is a random function, one particular solution corresponding to a spe-\n\n63\n\n\n\n64 Cap??tulo 6\n\ncific realization is not of concern: it is important to know the statistical properties of the\n\nsolution such as its mean, variance, or other statistical moments.\n\nSome methods for random differential equations are categorized as moment equations\n\nmethods. In these methods the purpose is to obtain differential equations governing the\n\nstatistical moments. The most important of these equations is the differential equation\n\nfor the expectation (mean), which is called for some authors as effective equation. As far\n\nas we know, no effective equation is known for the nonlinear problem discussed in this\n\npaper.\n\nThe Monte Carlo method is an alternative in solving random differential equations.\n\nPartial differential equations and the Monte Carlo method have been related for more\n\nthan a century, since the works developed by Lord Rayleigh (1899), Courant et al (1928),\n\nand Kolmogorov (1931). For instance, Courant et al showed that a particular finite\n\ndifference equation for the two dimensional Dirichlet boundary value problem and a two\n\ndimensional random walk produce the same results. In modern terms the Monte Carlo\n\nmethod originated from Los Alamos and the atomic bomb project. Now it is being used\n\nin many scientific fields [6, 20]. The basic idea is to solve a large number of deterministic\n\ndifferential equations choosing particular values for the random variables according to their\n\nassumed probabilistic distribution. The statistical information of the random solution is\n\nestimated using these realizations. The Monte Carlo method can be used in either linear\n\nor nonlinear random differential equations. However, the exceptionally large volume of\n\ncalculations, and the difficulty for generating random numbers limit the significance of\n\nthis method.\n\nIn a different direction we have been studying numerical methods for the random\n\ntransport equation. In the linear case our ideas were inspired by Godunov\u2019s method\n\n[9, 15] for the deterministic transport equation. In [3] we present an explicit expression for\n\nthe random solution to one-dimensional random advective equations where the constant\n\nvelocity and the Riemann initial condition are random variables. This closed solution\n\nyields simple expressions for its statistical moments, and computational experiments show\n\ngood agreement between our expressions and the Monte Carlo method for the first three\n\nmoments. The closed solution for random Riemann problems and Godunov\u2019s ideas are\n\nused in [5] and [4] to design numerical methods to calculate the mean and variance of the\n\nsolution to transport equations with more general initial condition (random fields). Our\n\nmethods are explicit and do not demand differential equations governing the statistical\n\nmoments, the effective equations. Furthermore, our scheme is consistent and stable with\n\nthe diffusive effective equation presented in the literature [8]. Computational experiments\n\nhave shown good agreements with the Monte Carlo method.\n\nIn this paper, we generalize our previous ideas to solve the random Riemann problem\n\n\n\n6.1. Introduction 65\n\nfor Burgers\u2019 equation\n\n?\n\n?t\nU (x, t) +\n\n1\n\n2\n\n?\n\n?x\nU 2(x, t) = 0, t > 0, x ? R,\n\nU (x, 0) =\n\n{\nUL, if x &lt;0,\n\nUR, if x > 0,\n(6.1)\n\nwhere UL and UR are random variables. Here the randomness appears only because of the\n\ninitial condition. The deterministic version of (6.1) was introduced by Burgers [1] as the\n\nsimplest model that captures some key features of gas dynamics, the nonlinear hyperbolic\n\nterm. But, rather than modeling a physical process, the inviscid Burgers equation has\n\nbeen widely used for developing both theoretical and numerical methods in the literature\n\nof deterministic hyperbolic equations.\n\nTaking into account that several numerical methods to deal with deterministic con-\n\nservation laws use solutions of Riemann problems (Random Choice Method developed by\n\nGlimm [7], and Godunov\u2019s method [9, 15], for example), we believe that the results of the\n\ncurrent paper may be useful in developing numerical methods for more general random\n\nconservation laws. Moreover, since the mathematical theory of methods to random par-\n\ntial differential equations are difficult and not complete yet (see [13, 16, 19], for example),\n\nnumerical methods can be a good alternative to deal with random differential equations.\n\nKim (2006) presents a scheme to calculate the statistical moments of the random\n\nBurgers\u2019equation in [11]. Nevertheless, the author considers the simple case where the\n\nrandom initial condition is an explicit function of the spatial variable, and of the normal\n\nrandom variable with zero mean and unit variance. The author uses Wiener chaos expan-\n\nsion to separate random and deterministic effects, and utilizes the Lax-Wendroff method\n\nto discretize the deterministic system of partial differential equations that governs the\n\npropagation of randomness.\n\nIn this paper, we use two basic ideas to construct the solution, and its moments, to\n\n(6.1): (i) the realizations of the probabilistic problem are nonlinear transport equations\n\nwhose analytical solutions are known (shock and rarefaction waves); and (ii) the random\n\nsolution and its statistical moments, as functions of the initial condition and its joint\n\ndensity function, are found using geometrical partitions of the phase plane (UL, UR).\n\nIntegrations on these sets are the shock and rarefaction averaging process.\n\nThe outline of this paper is as follows. In Section 2 we deduce an explicit solution to\n\nproblem (6.1). We also show the similarity of the solution as well as present an expression\n\nfor its statistical moments. Based on bidimensional midpoint quadrature formula, in Sec-\n\ntion 3 we suggest an efficient algorithm to approximate the statistical moments. Finally,\n\nwe present some computational tests and conclusions.\n\n\n\n66 Cap??tulo 6\n\n6.2 The random solution\n\nIn this section, we construct the solution to (6.1), the one-dimensional Burgers\u2019 equation\n\nwith random Riemann initial condition. We assume that the random initial states, UL and\n\nUR, and their joint probability density function, fULUR , are known. For a single realization,\n\nUL(?) and UR(?), of UL and UR, respectively, we have the deterministic Burgers-Riemann\n\nproblem:\n\n?\n\n?t\nu(x, t, ?) +\n\n1\n\n2\n\n?\n\n?x\nu2(x, t, ?) = 0, t > 0, x ? R,\n\nu(x, 0, ?) =\n\n{\nUL(?), if x &lt;0,\n\nUR(?), if x > 0.\n(6.2)\n\nPhysically correct solutions to (6.2), i.e., entropy solutions, are the rarefaction or shock\n\nwaves (see [14, 15], for example):\n\n(a) If UL(?) &lt;UR(?) then the solution is the rarefaction wave emanating from\n\n(x, t) = (0, 0)\n\nu(x, t, ?) =\n\n?\n?\n?\n\nUL(?), if\nx\nt\n\n&lt;UL(?),\nx\nt\n, if UL(?) ? xt ? UR(?),\n\nUR(?), if\nx\nt\n\n> UR(?).\n\n(6.3)\n\n(b) If UL(?) > UR(?) then the solution is the shock wave\n\nu(x, t, ?) =\n\n{\nUL(?), if\n\nx\nt\n\n&lt;s(?),\n\nUR(?), if\nx\nt\n\n> s(?),\n(6.4)\n\nwith the shock velocity, s(?) = (1/2) [UL(?) + UR(?)], given by the Rankine-Hugoniot\n\njump condition.\n\nThus, holding (x, t) fixed and considering the rarefaction and shock solutions together,\n\nwe can join (6.3)-(6.4) to express u(x, t, ?) as\n\nu(x, t, ?) =\n\n?\n??????\n??????\n\nUL(?), if ? &lt;UL(?) and UL(?) &lt;UR(?),\n\n?, if UL(?) ? ? ? UR(?) and UL(?) &lt;UR(?),\nUR(?), if ? > UR(?) and UL(?) &lt;UR(?),\n\nUL(?), if ?&lt;\n1\n2\n[UL(?) + UR(?)] and UL(?) > UR(?),\n\nUR(?), if ? >\n1\n2\n[UL(?) + UR(?)] and UL(?) > UR(?),\n\n(6.5)\n\nwhere ? = x/t.\n\nTo simplify (6.5) we define the following mutually exclusive subsets of the phase plane\n\n\n\n6.2. The random solution 67\n\n(UL, UR):\n\nR?r = {(UL, UR) such that UL &lt;UR and ? &lt;UL} ;\nR0r = {(UL, UR) such that UL &lt;UR and UL ? ? ? UR} ;\nR+r = {(UL, UR) such that UL &lt;UR and ? > UR} ;\nR?s =\n\n{\n(UL, UR) such that UL > UR and ?&lt;\n\n1\n2\n[UL + UR]\n\n}\n;\n\nR+s =\n{\n(UL, UR) such that UL > UR and ? >\n\n1\n2\n[UL + UR]\n\n}\n.\n\n(6.6)\n\nIn this way, for a fixed ? = x/t, we can rewrite the solution (6.5) as follows:\n\nu(x, t, ?) =\n\n?\n?\n?\n\nUL(?), if (UL(?), UR(?)) ? R?r\n? R?s = R?(?),\n\n?, if (UL(?), UR(?)) ? R0r = R0(?),\nUR(?), if (UL(?), UR(?)) ? R+r\n\n? R+s = R+(?).\n(6.7)\n\nIn Figure 6.1 we illustrate the phase plane as R?(?) ? R0(?) ? R+(?); as we can see this\npartition of the phase plane depends exclusively of ? = x/t.\n\nR\n\n?(?)P\n\nUR\n\nUL? 2?\n\n?\n\nR\n\n+(?)\n\nR\n\n0(?)\n\nFigure 6.1: Integration regions.\n\nLet XA be the characteristic function of A, a set in (UL, UR) plane:\n\nXA =\n{\n\n1, if (UL, UR) ? A,\n0, otherwise.\n\nUsing XA in (6.7), the arguments so far summarized prove the proposition:\n\n\n\n68 Cap??tulo 6\n\nProposition 6.1. The solution to the random Burgers-Riemann problem (6.1), in a fixed\n\n(x, t), is the random function\n\nU (x, t) = ULXR? + ?XR0 + URXR+ , (6.8)\nwhere ? = x/t, and XR?, XR0, and XR+ are the characteristic functions of the mutually\nexclusive sets defined in (6.7).\n\nRemark 6.1. Expression (6.8) is the same for all (x, t) such that x/t = ?. Therefore,\n\nU (x, t) is a similarity function.\n\nIn the following corollary, the expression (6.8) and the joint probability density func-\n\ntion of UL and UR are used to calculate the statistical properties of the random solution.\n\nCorollary 6.1. The mth statistical moment of U (x, t), for a fixed (x, t), ? = x/t, is\n\n?U m(x, t)? =\n? ?\n\nR?\numL fULUR (uL, uR)duLduR+\n\n+?m\n? ?\n\nR0\nfULUR (uL, uR)duLduR +\n\n? ?\n\nR+\numR fULUR (uL, uR)duLduR.\n\n(6.9)\n\nProof. From (6.8), since R?(?), R0(?) and R+(?) are mutually exclusive sets, we have\n\n?U m(x, t)? =\n? ?\n\nR\u00d7R\n[uLXR? + ?XR0 + uRXR+ ]m fULUR (uL, uR)duLduR =\n\n=\n\n? ?\n\nR\u00d7R\n[umL XR? + ?mXR0 + umRXR+ ] fULUR (uL, uR)duLduR =\n\n=\n\n? ?\n\nR?\numL fULUR (uL, uR)duLduR + ?\n\nm\n\n? ?\n\nR0\nfULUR (uL, uR)duLduR+\n\n+\n\n? ?\n\nR+\numR fULUR (uL, uR)duLduR.\n\nEffective values of the moments (6.9) require the calculations of three double integrals\n\nfor each value of ?. In some particular cases we can calculate these integrals exactly. For\n\ninstance, if UL and UR are independent random variables and uniformly distributed in\n\nthe interval [?a, a], some calculations show that the mean of the solution to (6.1) is given\nby\n\n?U (x, t)? =\n{ ? ?\n\n4a2\n[sign(?)? ? a]2 , if ? a ? ? ? a,\n\n0, otherwise,\n(6.10)\n\nwhere ? = x/t. We will use this solution in Example 6.1 as a test problem to assess the\n\nperformances between the Monte Carlo method and our algorithm. However, in general\n\nwe must use numerical integration to calculate (6.9). In the following section we describe\n\na useful way to do that.\n\n\n\n6.3. The algorithm 69\n\n6.3 The algorithm\n\nTo design an efficient algorithm to calculate the statistical moments using (6.9), we take\n\na square in the phase plane (UL, UR), ?M = [?M, M ] \u00d7 [?M, M ], which contains the\neffective support of fULUR ; this means that outside ?M the values of the density probability\n\nfunction, fULUR , are sufficiently near to zero.\n\nAs shown in Figure 6.1, the point P = (?, ?), ? = x/t, is critical to define R?,\nR0 and R+ regions. This point moves in northeast (southwest) direction as ? increases\n(decreases). Without loss of generality, we will take t = 1 and use the similarity property\n\n(Remark 6.1) to obtain the solution for t > 0. Therefore, we may identify xj = ?j and\n\ntake the same discretization grid for ?, UL, and UR, as illustrated in Figure 6.2.\n\n? = x2\n\nuLx1 x2 x3 x4 x5 xNx7\n\n0\n\nx2\n\nx3\n\nx1\n\nx4\n\nx5\n\nxN\n\nx8\n\nx7\n\nuR ? = x3\n\nuLx1 x2 x3 x4 x5 xNx7\n\n0\n\nx2\n\nx3\n\nx1\n\nx4\n\nx5\n\nxN\n\nx8\n\nx7\n\nuR\n\n(a) ? = x2 (b) ? = x3\n\nFigure 6.2: Discretization scheme of the ?M square.\n\nNotation:\n\n\u2022 {xj = ?M + jh; (j = 1 : N )} is the ?-mesh with ?? = h > 0; x1 = ?M ; xN = M ;\nxj+1/2 = xj + h/2 (j = 1 : N ? 1); N is an odd number;\n\n\u2022 Il ?\n? ?\n\nR?\numL fULUR (uL, uR)duLduR;\n\n\u2022 I0 ?\n? ?\n\nR0\nfULUR (uL, uR)duLduR;\n\n\u2022 Ir ?\n? ?\n\nR+\numR fULUR (uL, uR)duLduR.\n\n\n\n70 Cap??tulo 6\n\nWe initiate the calculations taking ? = ?1 = x1. In this case ?M ? R?(?1) and,\nconsequently, Il = ?U mL ?, I0 = 0 and Ir = 0; these values are used to initiate the algorithm.\nTo save memory, the temporary calculations to update Il, I0 and Ir in ?j-step, xj =\n\nxj?1 + h (j = 2, 3, . . . , N ), are done in the \u201cS\nh\nj -strip\u201d: S\n\nh\nj = ?M\n\n? {R?(?j) \\ R?(?j?1)}.\nThis strip is a collection of squares (and half squares) with edges h and\n\n?\n2h (see Figure\n\n6.2). The integration is performed using the bidimensional midpoint quadrature formula\n\n(see [10]). To clarify the ideas, in Table 6.1 we summarized the first step of our algorithm.\n\nStep 1\n? ? x2;\n\nIl ? Il ? h2xm1+1/2\n{\n\nN?2?\ni=1\n\nf\n(\nx1+1/2, xN?i+1/2\n\n)\n+\n\n1\n\n2\nf\n\n(\nx1+1/2, x1+1/2\n\n)\n}\n\n?h2xm2 f (x2, x1);\n\nI0 ? I0 + h2\nN?2?\ni=1\n\nf\n(\nx1+1/2, xN?i+1/2\n\n)\n;\n\nIr ? Ir + 12 h2xm1+1/2f\n(\nx1+1/2, x1+1/2\n\n)\n+ h2xm1 f (x4, x1);\n\n?U (x2, 1)m? ? Il + ?mI0 + Ir.\n\nTable 6.1: Illustration of the first step of Algorithm 1\n\nRepeating this idea in the next ?j-steps, we formulate Algorithm 1.\n\nRemark 6.2. Observe that the ?M -discretization scheme has the recursive advantage:\n\nthe solution at ?j can be calculated just by updating the solution at ?j?1.\n\nRemark 6.3. The main advantage of Algorithm 1 is that it does not require a random\n\nnumbers generator (massive simulation of data with a known probability distribution) as\n\ndoes the Monte Carlo method. Furthermore, as we will see in Examples 6.1 and 6.4, its\n\nconvergence is faster than the Monte Carlo method.\n\n\n\n6.3. The algorithm 71\n\nAlgorithm 1\n\nN is an odd number;\n\nIl = ?U mL ?; I0 = 0; Ir = 0;\nfor k = 1 : N?1\n\n2\ndo\n\n? ? xk+1;\n\nIl ? Il ? h2xmk+1/2\n{\n\nN?2?\n\ni=k\n\nf\n(\nxk+1/2, xN +k?i?1+1/2\n\n)\n+\n\n1\n\n2\nf\n\n(\nxk+1/2, xk+1/2\n\n)\n}\n\n?2h2\n2k?1?\n\ni=k+1\n\nxmi f (xi, x2k?i+1) ? h2xm2kf (x2k, x1);\n\nI0 ? I0 + h2\n{\n\nN?2?\n\ni=k\n\nf\n(\nxk+1/2, xN +k?i?1+1/2\n\n)\n?\n\nk?1?\ni=1\n\nf\n(\nxi+1/2, xk+1/2\n\n)\n}\n\n;\n\nIr ? Ir + h2xmk+1/2\n{\n\nk?1?\ni=1\n\nf\n(\nxi+1/2, xk+1/2\n\n)\n+\n\n1\n\n2\nf\n\n(\nxk+1/2, xk+1/2\n\n)\n}\n\n+2h2\n2k?1?\n\ni=k+1\n\nxm2k?i+1f (xi, x2k?i+1) + h\n2xm1 f (x2k, x1);\n\n?U (xk+1, 1)m? ? Il + ?mI0 + Ir;\nend for\n\nfor k = N +1\n2\n\n: (N ? 1) do\n? ? xk+1;\n\nIl ? Il ? h2xmk+1/2\n{\n\nN?2?\n\ni=k\n\nf\n(\nxk+1/2, xN +k?i?1+1/2\n\n)\n+\n\n1\n\n2\nf\n\n(\nxk+1/2, xk+1/2\n\n)\n}\n\n?2h2\nN?1?\n\ni=k+1\n\nxmi f (xi, x2k?i+1) ? h2xmN f (xN , x2k?N +1);\n\nI0 ? I0 + h2\n{\n\nN?2?\n\ni=k\n\nf\n(\nxk+1/2, xN +k?i?1+1/2\n\n)\n?\n\nk?1?\ni=1\n\nf\n(\nxi+1/2, xk+1/2\n\n)\n}\n\n;\n\nIr ? Ir + h2xmk+1/2\n{\n\nk?1?\ni=1\n\nf\n(\nxi+1/2, xk+1/2\n\n)\n+\n\n1\n\n2\nf\n\n(\nxk+1/2, xk+1/2\n\n)\n}\n\n+2h2\nN?1?\n\ni=k+1\n\nxm2k?i+1f (xi, x2k?i+1) + h\n2xm2k?N +1f (xN , x2k?N +1);\n\n?U (xk+1, 1)m? ? Il + ?mI0 + Ir;\nend for\n\n\n\n72 Cap??tulo 6\n\n6.4 Computational tests\n\nIn this section, we present some examples to assess and illustrate our approach. In\n\nExample 6.1 we take an initial condition that allows exact calculations for the mean.\n\nIn the following examples the initial condition has a bivariate normal distribution. In\n\nthese examples the mean, variance, 3rd central moment, and 4th central moment of the\n\nsolution are obtained by Algorithm 1 and compared with the Monte Carlo method. To\n\ngenerate the realizations (UL(?), UR(?)), required by the Monte Carlo method, we use\n\nrandom numbers generators of MATLAB. The analytical solution for each realization,\n\n(UL(?), UR(?)), is given by (6.3) or (6.4). We compare the performances of the methods.\n\nWe also plot the solution of the deterministic problem where the initial condition is the\n\nstatistical mean of the random data. Some authors ([17], for example) use the name\n\n\u201cnaive\u201d for this solution. The numerical experiments presented in this section were done\n\nin double precision with some MATLAB codes on a 3.0Ghz Pentium 4 with 512Mb of\n\nmemory.\n\nExample 6.1.\n\nWe use (6.10) to calculate exact values of the mean of the solution to (6.1) with the\n\nRiemann initial condition:\n\nU (x, 0) =\n\n{\nUL, if x &lt;0,\n\nUR, if x > 0,\n\nwhere UL and UR are independent random variables uniformly distributed in the interval\n\n[?1, 1]. The mean, at t = 0.4 and t = 0.8, is plotted in Figure 6.3. Absolute errors of\napproximations given by the Monte Carlo method and Algorithm 1 are compared in Table\n\n6.2, using ?U (x, 1)?, x ? [?1, 1]. The CPU times are also presented in this table.\n\n?1 ?0.5 0 0.5 1\n?0.06\n\n?0.04\n\n?0.02\n\n0\n\n0.02\n\n0.04\n\n0.06\n\n?1 ?0.5 0 0.5 1\n?0.06\n\n?0.04\n\n?0.02\n\n0\n\n0.02\n\n0.04\n\n0.06\n\nFigure 6.3: Mean at t = 0.4 (left) and t = 0.8 (right).\n\n\n\n6.4. Computational tests 73\n\nMonte Carlo method Algorithm 1\nrealizations absolute error CPU time\n\n(Nr) (|| . ||?) (sec)\n1 000 0.0268 0.071\n5 000 0.0124 0.358\n10 000 0.0091 0.718\n30 000 0.0048 2.154\n50 000 0.0037 3.599\n100 000 0.0027 7.223\n\nnumber of absolute error CPU time\npartitions (N) (|| . ||?) (sec)\n\n201 2.49 \u00d7 10?5 0.084\n\nTable 6.2: Absolute errors and CPU times; h = 0.01.\n\nExample 6.2.\n\nLet us consider the problem (6.1) with UL and UR having bivariate normal distribution\n\ndefined by: ?UL? = 0.1 (mean of UL); ?UR? = 0.9 (mean of UR); ?L = 0.3 (standard\ndeviation of UL); ?R = 0.2 (standard deviation of UR); and ? = 0.42 (correlation coefficient\n\nbetween UL and UR). Since the probability density function with these data has the\n\neffective support in the semiplane UL &lt;UR, the rarefaction wave solutions dominate.\n\nFigure 6.4 illustrates the mean (compared with the naive solution), variance, 3rd central\n\nmoment, and 4th central moment calculated at t = 1 for x ? [?3, 3]. As we can see, the\nrandomness of the initial conditions smoothen the edges of the naive solution, as in the\n\nrandom linear transport equations.\n\nExample 6.3.\n\nTo illustrate a shock-dominant case, we changed the data of UL and UR used in the\n\npreviews example: ?UL? = 0.9; ?UR? = 0.1; ?L = 0.3; ?R = 0.2; and ? = 0.42. With these\ndata, the bivariate normal probability density function has the effective support in the\n\nsemiplane UL > UR. In Figure 6.5 we plot the mean, variance, 3rd central moment, and\n\n4th central moment calculated at t = 1 for x ? [?3, 3]. Again, the randomness of the\ninitial conditions smoothen the edges of the naive solution.\n\nExample 6.4.\n\nIn this example, we also consider a bivariate normal distribution with data that mix\n\nrarefaction and shock waves in the realizations: ?UL? = 0.2; ?UR? = 0.4; ?L = 0.2;\n?R = 0.5; and ? = 0.42. In Figure 6.6 we present approximations to the mean, variance,\n\n3rd central moment, and 4th central moment computed using the Monte Carlo method\n\nand Algorithm 1. We also include the naive solution. Since ?UL? &lt;?UR? the naive\nsolution is a rarefaction wave. This example emphasizes the difference between the mean\n\nof the solution and the solution computed using means of the data. Here, the effect of\n\nthe randomness is more than to smoothen edges: as shown in Figure 6.6 the mean of\n\n\n\n74 Cap??tulo 6\n\n?3 ?2 ?1 0 1 2 3\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nmean\n\nMonte Carlo\nnaive solution\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n0\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.14\nvariance\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n?5\n\n0\n\n5\n\n10\n\n15\n\n20\n\nx 10\n?3 3rd central moment\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n0\n\n0.005\n\n0.01\n\n0.015\n\n0.02\n\n0.025\n\n0.03\n\n0.035\n\n0.04\n\n4th central moment\n\nMonte Carlo\nour algorithm\n\nFigure 6.4: Approximations to the statistical moments using the Monte Carlo method\n(with 50 000 realizations), and Algorithm 1 (with N=601).\n\nthe solution is a humped function. In Table 6.3 we compare the performances between\n\nthe Monte Carlo method and Algorithm 1 in calculating ?U (x, 1)?, x ? [?3, 3], taking\ninto account the error estimates of each method and the CPU time. For instance, in the\n\napproximations plotted in Figure 6.6 the Monte Carlo method has taken 8.675 sec while\n\nAlgorithm 1 has taken 0.991sec.\n\nMonte Carlo method Algorithm 1\nrealizations estimate of CPU time\n\n(Nr) error O(1/\n?\n\nNr) (sec)\n1 000 0.0316 0.185\n5 000 0.0141 0.877\n10 000 0.0100 1.744\n30 000 0.0063 5.210\n50 000 0.0044 8.675\n100 000 0.0031 17.294\n\nnumber of estimate of CPU time\npartitions (N) error O(1/N 2) (sec)\n\n601 0.00018 0.991\n\nTable 6.3: Absolute errors and CPU times; h = 0.01 (600 subintervals).\n\n\n\n6.4. Computational tests 75\n\n?3 ?2 ?1 0 1 2 3\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\nmean\n\nMonte Carlo\nour algorithm\nnaive solution\n\n?3 ?2 ?1 0 1 2 3\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nvariance\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n?0.1\n\n?0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n3rd central moment\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n4th central moment\n\nMonte Carlo\nour algorithm\n\nFigure 6.5: Approximations to the statistical moments using the Monte Carlo method\n(with 50 000 realizations), and Algorithm 1 (with N=601).\n\n?3 ?2 ?1 0 1 2 3\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\nmean\n\nMonte Carlo\nour algorithm\nnaive solution\n\n?3 ?2 ?1 0 1 2 3\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\nvariance\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n?0.06\n\n?0.04\n\n?0.02\n\n0\n\n0.02\n\n0.04\n3rd central moment\n\nMonte Carlo\nour algorithm\n\n?3 ?2 ?1 0 1 2 3\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n4th central moment\n\nMonte Carlo\nour algorithm\n\nFigure 6.6: Approximations to the statistical moments using the Monte Carlo method\n(with 50 000 realizations), and Algorithm 1 (with N=601).\n\n\n\n76 References\n\n6.5 Concluding remarks\n\nWe have used the basic solutions to nonlinear conservation laws, the shock and rarefac-\n\ntion waves, to construct the random solution for Burgers\u2019 equation with random Riemann\n\ninitial condition. These basic solutions are grouped to deduce simple expressions to cal-\n\nculate the statistical properties of the random solution by integrations in three mutually\n\nexclusive cones in the phase plane (Figure 6.1). We also design an algorithm to calculate\n\nthe integrals, in case of difficult analytic expressions of the joint density distribution of\n\nthe initial condition. Our approach outperformed the Monte Carlo method in terms of\n\naccuracy and computational cost. We believe that this approach can be also used to solve\n\nmore general problems.\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) through the grant 140406/2004-2, and to the reviewer for the helpful comments.\n\nReferences\n\n[1] J. M. Burgers, A mathematical model illustrating the theory of turbulance. Ad. Appl.\n\nMech. 1:171\u2013179 (1948).\n\n[2] R. Courant, K. Friedrichs, H. Lewy, Uber die partiellen differenzengleichungen der\n\nmatematischen physik. Math. Annalen. 100:32-74 (1928); translated into English by\n\nP. Fox (1956), Institute of Mathematical Sciences, New York University.\n\n[3] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random\n\ntransport equation. Computational and Applied Mathematics 26(3):323\u2013335 (2007).\n\n[4] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution\n\nof the random transport equation. Appl. Math. Comput. 190(1):362\u2013369 (2007).\n\n[5] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution\n\nof the random transport equation. Appl. Math. and Comput. 187(2):912\u2013921 (2007).\n\n[6] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer, 1996.\n\n[7] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.\n\nPure Appl. Math. 18:695\u2013715 (1965).\n\n\n\nReferences 77\n\n[8] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03-44, Providence, 1998.\n\n[9] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntions the equations of hydrodynamics. Mat. Sb. 47:271\u2013306 (1959).\n\n[10] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.\n\n[11] H. Kim, An efficient computational method for statistical moments of Burger\u2019s equa-\n\ntion with random initial conditions. Mathematical Problems in Engineering. 2006:1\u2013\n\n21, doi:10.1155/MPE/2006/17406 (2006).\n\n[12] A. Kolmogorov, Uber die analytischen methoden in der wahrscheinlichkeitsrech-nung.\n\nMath. Annalen. 104:415\u2013458 (1931).\n\n[13] P. E. Kloeden and E. Platen, Numerical Solution of Stochastic Differential Equations.\n\nSpringer, New York, 1999.\n\n[14] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.\n\n[15] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n[16] B. Oksendal, Stochastic Differential Equations: an introduction with applications.\n\nSpringer, New York, 2000.\n\n[17] P. O\u2019Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic retar-\n\ndation: numerical results. Computer Methods in Water Resources XII. 1:255\u2013261\n\n(1998).\n\n[18] L. J. W. S. Rayleigh, On James Bernoulli\u2019s theorem in probabilities. Phil. Mag.\n\n47:246\u2013251 (1899).\n\n[19] T. T. Soong, Random Differential Equations in Science and Engineering. Academic\n\nPress, New York, 1973.\n\n[20] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.\n\nAcademic Press, San Diego, 2002.\n\n\n\nCap??tulo 7\n\nOn the evaluation of moments for\n\nsolute transport by random velocity\n\nfields\n\nAbstract\n\nIn this note, we consider the random linear transport equation. We indicate that standard\n\naveraging approaches to obtain an equation for the evolution of the statistical mean of the\n\nsolution may also be valid for all the statistical moments of the solution. With this result\n\nwe can obtain more statistical information about the random solution, as illustrated in\n\ntwo particular examples.\n\nKeyword: random linear transport equation, random velocity field, averaging approach,\n\nstatistical moments, Gaussian process, Telegraph process.\n\n7.1 Introduction\n\nIn this note, we consider the transport of a passive scalar by an incompressible random\n\nvelocity field as described by the equation\n\nUt(x, t) + ?.[V(x, t)U (x, t)] = 0, U (x, t0) = g(x), (7.1)\nwhere U is the density of a passively advected agent (concentration of a chemical species,\n\ntemperature, etc.), V is a random velocity field, and g(x) is the deterministic initial\n\ndistribution of the scalar. The subscript t in Ut(x, t) denotes the partial derivative with\n\nrespect to this variable. Taking into account the incompressibility of V, i.e., ? \u00b7 V = 0,\nwe rewrite equation (7.1) as\n\nUt(x, t) + Vi(x, t)Uxi (x, t) = 0, U (x, t0) = g(x), (7.2)\n\n79\n\n\n\n80 Cap??tulo 7\n\nwhere repeated indices indicate summation.\n\nStandard approaches (see, e.g., [2, 6, 7, 9, 10, 12]) to derive an equation for the mean\n\nof U use the Reynolds decomposition\n\nVi(x, t) = ?Vi(x, t)? + V ?i (x, t)\n(angle brackets denote ensemble averaging) in (7.2) to obtain the following non-closed\n\naveraged equation\n\n?U?t + ?Vi(x, t)??U?xi + ?V ?i (x, t)Uxi? = 0. (7.3)\nThe basic difficulty with such approaches lies in the necessity to approximate (model)\n\nthe unknown correlation moment between the random velocity fluctuations and U (x, t),\n\nthe term ?V ?i (x, t)Uxi (x, t)? in (7.3). Moreover, the knowledge of the mean, ?U (x, t)?, is\nnot enough to provide a detailed understanding of the random transport process. One\n\nmust at least examine higher moments of U (x, t). With that in mind, it is our purpose to\n\nshow that for the linear transport equation (7.1), some approaches used to approximate\n\n?V ?i (x, t)Uxi (x, t)? in (7.3) may be also used to approximate all the moments of the solution.\nIn Section 7.2 we present this result, and in Sections 7.3\u20137.4 we illustrate the approach\n\nwith two examples.\n\n7.2 Main result\n\nProposition 7.1. Let Vi(x, t) = ?Vi(x, t)? + V ?i (x, t) in (7.2). Then\n?U m(x, t)?t + ?Vi(x, t)??U m(x, t)?xi + ?V ?i (x, t)U mxi (x, t)? = 0, (7.4)\n\nwhere ?U m(x, t)?, m ? Z, m ? 1, is the mth moment of the solution to (7.2).\nProof. Notice that U m(x, t), m ? Z, m ? 1, satisfies an equation like (7.2). Indeed,\ndifferentiating U m(x, t) with respect to t and x, and using (7.2) we obtain\n\n(U m)t + Vi(x, t)(U\nm)xi = m U\n\n(m?1)[?Ut(x, t) + Vi(x, t)Uxi (x, t)] = 0.\n\nAveraging this expression and using the Reynolds decomposition of the velocity, Vi(x, t) =\n\n?Vi(x, t)? + V ?i (x, t), yields (7.4).\n\nRemark 7.1. In [1] we have shown that in one-dimensional transport problems with a\n\nconstant random velocity, V , if the partial differential equation for the moments is an\n\nadvection-diffusion equation with diffusion coefficient ?, then ? must satisfy the equation\n\n?f ?V (x/t)? = fV (x/t)(x??V ?t), where fV (?) is the probability density function of V . For\nsuch problems equation (7.3) is closed with ??V ?Ux? = ??U?xx.\n\n\n\n7.3. First application: Gaussian processes 81\n\n7.3 First application: Gaussian processes\n\nConsider the following one-dimensional version of problem (7.1):\n\nUt(x, t) + V (t)Ux(x, t) = 0, U (x, 0) = H(?x), (7.5)\nwhere H(x) is the Heaviside function, and the random velocity, V (t), is Gaussian with\n\n?V (t)? = V constant and an exponentially decaying covariance function, CovV (t, ? ) =\n?2V exp (?|t ? ?|/?). The covariance function is parameterized by the variance, Var[V (t)] =\n?2V (which is assumed to be constant), and by the correlation length, ? > 0, which governs\n\nthe decay rate of the time correlation.\n\nAccording to [6, 9, 10], the correlation moment between the random flow-velocity and\n\nthe random concentration U can be written in the form\n\n?V ?(t)Ux(x, t)? = ?\n(? t\n\n0\n\nCovV (t, ? ) d?\n\n)\n?U (x, t)?xx. (7.6)\n\nThus, the mean concentration is exactly governed by\n\n?U (x, t)?t + V ?U (x, t)?x =\n(? t\n\n0\n\nCovV (t, ? ) d?\n\n)\n?U (x, t)?xx,\n\n?U (x, 0)? = H(?x).\n(7.7)\n\nIn view of Proposition 7.1 we can use (7.6) to calculate all the moments, i.e., the mth\n\nmoment satisfies the following equation:\n\n?U m(x, t)?t + V ?U m(x, t)?x =\n(? t\n\n0\n\nCovV (t, ? ) d?\n\n)\n?U m(x, t)?xx,\n\n?U m(x, 0)? = [H(?x)]m = H(?x).\n(7.8)\n\nThe solution to (7.8) is\n\n?U m(x, t)? = 1\n2\nerfc\n\n(\nx ? V t\n?(t)\n\n)\n, (7.9)\n\nwhere erfc(x) is the complementary error function and\n\n?(t) = 2\n\n[? t\n0\n\n? ?\n0\n\nCovV (?, ? )d? d?\n\n]1/2\n.\n\nWe now compare the moments (7.9) with those yielded by the Monte Carlo method.\n\nTo generate the realizations V (t, ?) required by the Monte Carlo method, we use the\n\nsubroutine [mvnrnd.m] of MATLAB. The analytical solution for each realization is\n\nU (x, t, ?) = U\n\n(\nx ?\n\n? t\n0\n\nV (s, ?) ds, 0\n\n)\n= H\n\n(? t\n0\n\nV (s, ?) ds ? x\n)\n\n.\n\n\n\n82 Cap??tulo 7\n\nIn our numerical experiments the integration of V (t, ?) is performed using the Simpson\u2019s\n\nquadrature rule (see [3], for example). Figures 7.1 and 7.2 illustrate the mean, variance,\n\nand third central moment of the solution to (7.5) computed using the averaging approach\n\nand the Monte Carlo method (with 50 000 realizations). The plots correspond to the\n\nfollowing data: ?V (t)? = V = ?0.2; ?2V = 0.4; t = 0.6; ?t = 0.001; and ?x = 0.0005. In\nFigure 7.1 we use ? = 0.1 and in Figure 7.2 we use ? = 1.0, i.e, a more correlated field.\n\nAll the numerical experiments were done in double precision with some MATLAB codes\n\non a 1.73Ghz Intel Core Duo 2 with 2Gb of memory.\n\n?2 ?1 0 1 2\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\ninit. cond.\nMonte Carlo\nAveraging\n\n?2 ?1 0 1 2\n?0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\nMonte Carlo\nAveraging\n\n?2 ?1 0 1 2\n\n?0.1\n\n?0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\nMonte Carlo\nAveraging\n\nFigure 7.1: Mean (left), variance (middle), and third central moment (right) of the solution\nto (7.5); ? = 0.1.\n\n?2 ?1 0 1 2\n?0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\ninit. cond.\nMonte Carlo\nAveraging\n\n?2 ?1 0 1 2\n?0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\nMonte Carlo\nAveraging\n\n?2 ?1 0 1 2\n\n?0.1\n\n?0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\nMonte Carlo\nAveraging\n\nFigure 7.2: Mean (left), variance (middle), and third central moment (right) of the solution\nto (7.5); ? = 1.0.\n\n7.3.1 The probability density function\n\nFor this particular example, we have shown that each moment of the solution to (7.5),\n\n?U m(x, t)? (even in the case of a more general random initial condition, G(x)) satisfies\n\n\n\n7.4. Second application: Telegraph processes 83\n\nthe following advection-diffusion equation:\n\n?t(x, t) + V ?x(x, t) = ?(t) ?xx(x, t),\n\n?(x, 0) = ?G(x)m?, (7.10)\n\nwhere ?(t) =\n\n? t\n0\n\nCovV (t, ? ) d? . As a consequence, the probability density function for\n\nthe random solution U (x, t), fU (u; x, t), also satisfies an initial value problem for the\n\nadvection-diffusion equation (7.10), i.e.,\n\n(fU )t + V (fU )x = ?(t) (fU )xx,\n\nfU (u; x, 0) = fG(u; x).\n(7.11)\n\nIndeed, the Fourier transform of fU (u; x, t), under the assumption that the probability\n\ndensity function is uniquely determined by its moments (see, e.g., [5] for conditions for\n\nuniqueness in the problems of moments), is\n\nf?U (?; x, t) =\n??\n\nj=0\n\n(i?)j\n\nj!\n?U m(x, t)?, (7.12)\n\nwhere ??U m(x, t)?t + V ?U m(x, t)?x = ?(t)?U m(x, t)?xx. Taking the derivative with respect\nto t and x in (7.12), we arrive at\n\n(f?U )t + V (f?U )x = ?(t) (f?U )xx. (7.13)\n\nSince the variable ? does not appear in the derivatives, we can go back to the variable\n\nu and find (7.11). The respective initial condition follows from the probability density\n\nfunction of G(x). This result for the density probability of U (x, t), fU (u; x, t) agrees with\n\nthat presented in [11] on page 247 using a different methodology.\n\n7.4 Second application: Telegraph processes\n\nIn this section, we consider the one-dimensional transport with the random telegraph\n\nprocess (see [4, 8], for example) as a model for the velocity, V (t). According to [10], this\n\nis a convenient model of a function that has finite jumps in random times. The random\n\ntelegraph process is a stochastic process V (t) defined by\n\nV (t) = V + ?(?1)N (t), (7.14)\n\nwhere the state space of V (t) is {V ? ?0, V + ?0}, the times at which the process changes\nthe values (V ? ?0) and (V + ?0) are distributed according to a Poisson process N (t)\n\n\n\n84 References\n\nwith intensity rate ?, and ? is a random variable independent of N (t) and such that\n\nP{? = ?0} = 1/2 = P{? = ??0}. This process is stationary (see [4], for more details)\nwith mean ?V (t)? = V and covariance CovV (t, ? ) = ?20 exp (?2?|t ? ?|).\n\nAccording to [6, 10], the correlation moment between V ?(t) and U (x, t) is exactly given\nby\n\n?V ?(t)U (x, t)? = ?\n? t\n\n0\n\nCovV (t, ? )\n?\n\n?x\n?U (x ? V (t ? ? ), ? )? d?. (7.15)\n\nUsing (7.15) in (7.3) we obtain the differential equation for the mean concentration,\n\n?U (x, t)?t + V ?U (x, t)?x =\n?\n\n?x\n\n? t\n0\n\nCovV (t, ? )\n?\n\n?x\n?U (x ? V (t ? ? ), ? )? d?. (7.16)\n\nProposition 7.1 asserts that Equation (7.16) is the same for all statistical moments,\n\ni.e., the mth moment satisfies the equation\n\n?U m(x, t)?t + V ?U m(x, t)?x =\n?\n\n?x\n\n? t\n0\n\nCovV (t, ? )\n?\n\n?x\n?U m(x ? V )(t ? ? ), ? )? d?.\n\nThe analysis of the exact solution to (7.16) is presented in [10].\n\nAcknowledgments\n\nOur acknowledgments to the Brazilian Council for Development of Science and Technology\n\n(CNPq) for support through grants 140406/2004?2 and 210132/2006?0. We thank Prof.\nLu?cio Tunes dos Santos, IMECC, UNICAMP, for his helpful suggestions used in Section\n\n(7.3.1).\n\nReferences\n\n[1] F. A. Dorini, M. C. C. Cunha, Statistical moments of the random linear trans-\n\nport equation. Technical Report 15/07, Imecc, Unicamp, Campinas, Brazil, (2007).\n\nhttp://www.ime.unicamp.br/rel pesq/2007/rp15-07.html.\n\n[2] P. O\u2019Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-\n\ntardation: numerical results. Computer Methods in Water Resources XII 1:255\u2013261\n\n(1998).\n\n[3] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.\n\n[4] D. Kannan, An Introduction to Stochastic Processes. Elsevier North Holland, New\n\nYork, 1979.\n\n\n\nReferences 85\n\n[5] M. G. Kendall, Conditions for uniqueness in the problems of moments. The Annals\n\nof Mathematical Statistics 11:402\u2013409 (1940).\n\n[6] V. Klyatskin, Stochastic Equations and Waves in Randomly Inhomogeneous Media.\n\nNauka, Moscow, 1980.\n\n[7] V. I. Klyatskin, Statistical description of the diffusion of a passive tracer is a random\n\nvelocity field. Physics-Uspekhi 37(5):501\u2013513 (1994).\n\n[8] S. Miller, D. Childers, Probability and Random processes: with applications to signal\n\nprocessing an communications. Elsevier Academic Press, San Diego, California, 2004.\n\n[9] S. M. Rytov, A. Yu, V. I. Tatarskii, Principles of Statistical Radiophysics - Elements\n\nof Random Fields. Springer-Verlag, Berlin, Heidelberg, 1989.\n\n[10] M. Shvidler, K. Karasaki, Exact averaging of stochastic equations for transport in\n\nrandom velocity field. Transport in Porous Media 50:223\u2013241 (2003).\n\n[11] M. Shvidler, K. Karasaki, Probability density functions for solute in random field.\n\nTransport in Porous Media 50:243\u2013266 (2003).\n\n[12] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.\n\nAcademic Press, San Diego, 2002.\n\n\n\nMe?todos para Equac?o?es do Transporte\n\ncom Dados Aleato?rios\n\nEste exemplar corresponde a? redac?a?o final da\n\nTese devidamente corrigida e defendida por\n\nFabio Antonio Dorini e aprovada pela Banca\n\nExaminadora.\n\nCampinas, 17 de dezembro de 2007.\n\nTese apresentada ao Programa de Po?s Gra-\n\nduac?a?o do Instituto de Matema?tica, Estat??stica\n\ne Computac?a?o Cient??fica, unicamp, como re-\n\nquisito parcial para a obtenc?a?o do t??tulo de\n\nDoutor em Matema?tica Aplicada.\n\ni\n\n\n\nCap??tulo 8\n\nConcluso?es e trabalhos futuros\n\n8.1 Concluso?es\n\nA apresentac?a?o desta tese na forma cronolo?gica em que nosso trabalho foi sendo desen-\n\nvolvido e submetido a revistas especializadas reflete como fomos nos aprofundando numa\n\narea bastante fe?rtil.\n\nO desafio inicial de conhecer me?todos para lidar com incertezas em para?metros de\n\nequac?o?es diferenciais como modelos matema?ticos foi motivado pela simulac?a?o de fluxo em\n\nmeios porosos. Historicamente estes modelos foram incorporando os conceitos de permea-\n\nbilidade, permeabilidade relativa, porosidade, pressa?o capilar, ale?m dos mais tradicionais\n\nna meca?nica dos fluidos, como a viscosidade. Experimentos em laborato?rio sa?o usados na\n\navaliac?a?o destas propriedades e como tal esta?o sujeitos a erros de medic?a?o e dificuldade\n\nde obtenc?a?o de amostras compat??veis com os reservato?rios em analise. Assim, deve-se\n\nconsiderar a aleatoriedade dos para?metros que alimentam as va?rias equac?o?es diferenciais\n\nque sa?o usadas como modelo de fluxos em meios porosos.\n\nOs trabalhos apresentados nesta tese representam nossa contribuic?a?o na metodologia\n\naplica?vel as equac?o?es diferenciais com dados aleato?rios.\n\n8.2 Trabalhos futuros\n\nNesta sec?a?o resumimos tre?s problemas que esta?o em estudo e nos quais usamos algumas\n\nide?ias desenvolvidas nesta tese.\n\n87\n\n\n\n88 Cap??tulo 8\n\n8.2.1 Problema 1.\n\nAs equac?o?es diferenciais parciais que foram objeto de estudo nesta tese sa?o casos particu-\n\nlares da lei de conservac?a?o mais geral:\n\n?\n\n?t\nQ(x, t) +\n\n?\n\n?x\nf (Q(x, t)) = 0, t > 0, x ? R,\n\nQ(x, 0) = Q0(x). (8.1)\n\nonde Q e? a quantidade conservada e f (Q) e? a func?a?o fluxo. Aplicac?o?es desta equac?a?o\n\naparecem em problemas de recuperac?a?o de petro?leo, dispersa?o de poluentes, fluxo de\n\ngases, fluxo de tra?fego, dentre outros. Visando a dar continuidade aos nossos estudos,\n\ne baseados nas ide?ias apresentadas em [6, 17], vamos investir na busca das propriedades\n\nestat??sticas da soluc?a?o da Equac?a?o de Buckley-Leverett unidimensional aleato?ria, que\n\nmodela fluxos bifa?sicos imisc??veis e incompress??veis em um meio poroso,\n\n?\n\n?t\nS(x, t) +\n\n?\n\n?x\nf (S(x, t)) = 0, t > 0, x ? 0,\n\nS(0, t) = S?, t > 0, (8.2)\n\nS(x, 0) = S+, x > 0,\n\nonde S(x, t) representa a saturac?a?o de a?gua no meio poroso e a func?a?o fluxo, determinada\n\nusando a Lei de Darcy e a incompressibilidade das duas fases dos fluidos, e? dada por:\n\nf (S) =\nS?\n\nS? + ?(1 ? S)? . (8.3)\n\nDe acordo com a literatura especializada [2, 17, 18], as principais fontes de incerteza\n\n(aleatoriedade) em (8.2)\u2013(8.3) aparecem em ? (raza?o entre as viscosidades dos fluidos),\n\n?, e nos estados S? e S+. O expoente ? e? oriundo da hipo?tese que a permeabilidade\nrelativa e? governada por uma lei de pote?ncia; isto e?, assume-se que a permeabilidade (que\n\ne? func?a?o da saturac?a?o S) e? proporcional a S?, onde o expoente ? e? obtido a partir de\n\nresultados experimentais via ajuste de curvas.\n\n8.2.2 Problema 2.\n\nA concentrac?a?o de um soluto na?o-reagente S(x, t) na regia?o ?x? = ?(x1, x2, ..., xN )? &lt;?,\nt ? t0 e? descrita pelas equac?o?es\n\n?St(x, t) + ?.[V(x, t)S(x, t)] = 0, S(x, t0) = g(x), (8.4)\n\nonde ? e? a porosidade e V (x, t) e? a velocidade aleato?ria de Darcy. Assumimos que g(x)\n\ne? a concentrac?a?o inicial (na?o aleato?ria) e ? e? uma constante (na?o aleato?ria). Levando em\n\n\n\n8.2. Trabalhos futuros 89\n\nconsiderac?a?o a condic?a?o de incompressibilidade do fluido em (8.4), isto e?, div[V(x, t)] = 0,\n\ntemos\n\n?St(x, t) + Vi(x, t)Sxi (x, t) = 0, S(x, t0) = g(x). (8.5)\n\nOutra simplificac?a?o em (8.4) seria considerar a velocidade aleato?ria uma func?a?o apenas\n\ndo tempo \u2013 essa hipo?tese certamente simplifica o problema, pore?m o mesmo continua su-\n\nficientemente interessante e complicado. Assim, nosso problema de interesse e? o seguinte:\n\n?St(x, t) + Vi(t)Sxi (x, t) = 0, S(x, t0) = g(x). (8.6)\n\nE? fato que para cada realizac?a?o V(t, ?) da velocidade aleato?ria V(t), a soluc?a?o,\n\nS(x, t, ?), e? constante ao longo das curvas caracter??sticas\n\n?\n\n?t\nX(t) = V(t, ?), X(0) = X0, (8.7)\n\nou, equivalentemente, S(X(t), t, ?) = g(X0), onde X(t) = X0 +\n\n? t\n0\n\nV(?, ?)d? .\n\nDeste modo, a soluc?a?o para (8.6) pode ser expressa como\n\nS(x, t) = g\n\n(\nx ?\n\n? t\n0\n\nV(? )d?\n\n)\n. (8.8)\n\nAgora, denotando A(t) (vetor aleato?rio N-dimensional) como\n\nA(t) =\n\n? t\n0\n\nV(? )d?, (8.9)\n\nsegue que S(x, t) = g (x ? A(t)).\nOs resultados ate? o momento apresentados provam o seguinte resultado:\n\nProposition 8.1. O m-e?simo momento estat??stico, m ? Z, m ? 1, da soluc?a?o para (8.6)\ne? dado por\n\n?Sm(x, t)? =\n?\n\nRN\ng(x ? a)mfA(t)(a) da, (8.10)\n\nonde fA(t)(a) e? a func?a?o de densidade de probabilidade conjunta do vetor aleato?rio A(t).\n\nRemark 8.1. De acordo com (8.10), se g(x) = ?(x) e? uma func?a?o Delta enta?o\n\n?Sm(x, t)? =\n?\n\nRN\n?(x ? a)mfA(t)(a) da = fA(t)(x). (8.11)\n\nEm vista destes resultados, seria interessante entender as propriedades estat??sticas do\n\nvetor aleato?rio A(t) como func?a?o das propriedades estat??sticas de V(t). Por exemplo, a\n\npartir de [19], pa?gina 162, segue que:\n\n\n\n90 Cap??tulo 8\n\nProposition 8.2. Se V(t) e? um processo aleato?rio Gaussiano enta?o A(t) e? tambe?m um\n\nprocesso Gaussiano.\n\nNeste caso, as propriedades estat??sticas (me?dia e covaria?ncia) de A(t) sa?o facilmente\n\nobtidas via integrac?a?o das propriedades estat??sticas do processo Gaussiano V(t). Tendo\n\nem vista o exposto, nosso interesse futuro e? entender o processo aleato?rio A(t) a partir\n\ndas informac?o?es estat??sticas do processo V(t).\n\n8.2.3 Problema 3.\n\nOutro problema de interesse e? investigar a possibilidade de aplicac?a?o da metodologia\n\napresentada no Cap??tulo 4 a fim propor esquemas nume?ricos para\n\n\u2022 a equac?a?o de advecc?a?o unidimensional aleato?ria:\n?\n\n?t\nQ(x, t) + A(x)\n\n?\n\n?x\nQ(x, t) = 0, t > 0, x ? R,\n\nQ(x, 0) = Q0(x), (8.12)\n\nonde a velocidade de transporte A(x) e a condic?a?o inicial sa?o func?o?es aleato?rias.\n\n\u2022 a equac?a?o de advecc?a?o bidimensional aleato?ria:\n{\n\nQt(x, y, t) + A(x, y)Qx(x, y, t) + B(x, y)Qy(x, y, t) = 0, t > 0, x, y ? R,\nQ(x, y, 0) = Q0(x, y),\n\n(8.13)\n\nonde as velocidades de transporte, A(x, y) e B(x, y), e a condic?a?o inicial, Q0(x, y),\n\nsa?o campos aleato?rios. Me?todos do tipo direc?o?es alternadas (ADI) [27, 40], que te?m\n\nsido usados com sucesso para tratar equac?o?es diferenciais parciais multidimensionais\n\ndetermin??sticas, seriam utilizados para dividir o problema bidimensional em dois\n\nproblemas unidimensionais.\n\n\n\nRefere?ncias Bibliogra?ficas\n\n[1] J. M. Burgers, A mathematical model illustrating the theory of turbulance. Ad. Appl.\n\nMech. 1:171\u2013179 (1948).\n\n[2] Z. Chen, G. Huan, Y. Ma, Computational Methods for Multiphase Flows in Porous\n\nMedia. SIAM - Computational Science and Engineering, Philadelphia, 2006.\n\n[3] R. Courant, K. Friedrichs, H. Lewy, Uber die partiellen differenzengleichungen der\n\nmatematischen physik. Math. Annalen. 100:32\u201374 (1928); translated into English by\n\nP. Fox (1956), Institute of Mathematical Sciences, New York University.\n\n[4] M. C. C. Cunha, F. A. Dorini, A note on the Riemann problem for the random\n\ntransport equation. Computational and Applied Mathematics 26(3):323\u2013335 (2007).\n\n[5] M. C. C. Cunha, F. A. Dorini, A numerical scheme for the variance of the solution\n\nof the random transport equation. Appl. Math. and Comput. 190(1):362\u2013369 (2007).\n\n[6] M. C. C. Cunha, F. A. Dorini, Statistical moments of the solution of the random\n\nBurgers-Riemann problem. Technical Report 11/07, Imecc, Unicamp, Campinas,\n\nBrazil, (2007). http://www.ime.unicamp.br/rel pesq/2007/rp11-07.html.\n\n[7] F. A. Dorini, M. C. C. Cunha, Statistical moments of the random linear trans-\n\nport equation. Technical Report 15/07, Imecc, Unicamp, Campinas, Brazil, (2007).\n\nhttp://www.ime.unicamp.br/rel pesq/2007/rp15-07.html.\n\n[8] F. A. Dorini, M. C. C. Cunha, A finite volume method for the mean of the solution\n\nof the random transport equation. Appl. Math. and Comput. 187(2):912\u2013921 (2007).\n\n[9] F. A. Dorini, F. Furtado, M. C. C. Cunha, On the evaluation of moments for so-\n\nlute transport by random velocity fields. Technical Report 31/07, Imecc, Unicamp,\n\nCampinas, Brazil, (2007). http://www.ime.unicamp.br/rel pesq/2007/rp31-07.html.\n\n[10] G. S. Fishman, Monte Carlo: concepts, algorithms and applications. Springer-Verlag,\n\nNew York, 1996.\n\n91\n\n\n\n92 Refere?ncias Bibliogra?ficas\n\n[11] F. Furtado, F. Pereira, Scaling analysis for two-phase immiscible flow in heteroge-\n\nneous porous media. Computational and Applied Mathematics 17(3):237\u2013263 (1998).\n\n[12] J. Glimm, Solutions in the large for nonlinear hyperbolic systems of equations. Comm.\n\nPure Appl. Math. 18:695\u2013715 (1965).\n\n[13] J. Glimm, D. Sharp, Stochastic partial differential equations: Selected applications\n\nin continuum physics, in Stochastic Partial Differential Equations: Six Perspecti-\n\nves, (Edited by R. A. Carmona and B. L. Rozovskii), Mathematical Surveys and\n\nMonographs, American Mathematical Society, No. 64, p.03\u201344, Providence, 1998.\n\n[14] S. K. Godunov, A difference method for numerical calculation of discontinuous solu-\n\ntions of the equations of hydrodynamics. Mat. Sb. 47:271\u2013306 (1959).\n\n[15] G. Hammerlin, K. H. Hoffmann, Numerical Mathematics. Springer, 1991.\n\n[16] A. Harten, High resolution schemes for hyperbolic conservation laws. J. Comput.\n\nPhys. 49(2):357\u2013393 (1983).\n\n[17] H. Holden, N. H. Risebro, Stochastic properties of the scalar Buckley-Leverett equa-\n\ntion. SIAM Journal on Applied Mathematics 51(5):1472\u20131488 (1991).\n\n[18] L. Holden, The Buckley\u2013Leverett equation with spatially stochastic flux function.\n\nSIAM Journal on Applied Mathematics 57(5):1443\u20131454 (1997).\n\n[19] D. Kannan, An Introduction to Stochastic Processes. Elsevier North Holland, New\n\nYork, 1979.\n\n[20] M. G. Kendall, Conditions for uniqueness in the problems of moments. The Annals\n\nof Mathematical Statistics 11:402\u2013409 (1940).\n\n[21] H. Kim, An efficient computational method for statistical moments of Burger\u2019s equa-\n\ntion with random initial conditions. Mathematical Problems in Engineering. 2006:1\u2013\n\n21, doi:10.1155/MPE/2006/17406 (2006).\n\n[22] P. E. Kloeden, E. Platen, Numerical Solution of Stochastic Differential Equations.\n\nSpringer, New York, 1999.\n\n[23] V. Klyatskin, Stochastic Equations and Waves in Randomly Inhomogeneous Media.\n\nNauka, Moscow, 1980.\n\n[24] V. I. Klyatskin, Statistical description of the diffusion of a passive tracer is a random\n\nvelocity field. Physics-Uspekhi 37(5):501\u2013513 (1994).\n\n\n\nRefere?ncias Bibliogra?ficas 93\n\n[25] A. Kolmogorov, Uber die analytischen methoden in der wahrscheinlichkeitsrech-nung.\n\nMath. Annalen. 104:415\u2013458 (1931).\n\n[26] R. J. LeVeque, Numerical Methods for Conservation Laws. Birkha?user, Berlin, 1992.\n\n[27] R. J. LeVeque, Finite Volume Methods for Hyperbolic Problems. Cambridge Univer-\n\nsity Press, Cambridge, 2002.\n\n[28] P. O\u2019Leary, M. B. Allen, F. Furtado, Groundwater transport with stochastic re-\n\ntardation: numerical results. Computer Methods in Water Resources XII 1:255\u2013261\n\n(1998).\n\n[29] S. Miller, D. Childers, Probability and Random processes: with applications to signal\n\nprocessing an communications. Elsevier Academic Press, San Diego, California, 2004.\n\n[30] B. Oksendal, Stochastic Differential Equations: an introduction with applications.\n\nSpringer, New York, 2000.\n\n[31] H. Osnes, H. P. Langtangen, A study of some finite difference schemes for a uni-\n\ndiretional stochastic transport equation. SIAM Journal on Scientific Computing\n\n19(3):799\u2013812 (1998).\n\n[32] A. D. Polyanin, Handbook of Linear Partial Differential Equations for Engineers and\n\nScientists. Chapman &amp; Hall/CRC, New York, 2002.\n\n[33] L. J. W. S. Rayleigh, On James Bernoulli\u2019s theorem in probabilities. Phil. Mag.\n\n47:246\u2013251 (1899).\n\n[34] S. M. Rytov, A. Yu, V. I. Tatarskii, Principles of Statistical Radiophysics - Elements\n\nof Random Fields. Springer-Verlag, Berlin, Heidelberg, 1989.\n\n[35] G. I. Schue?ller, A state-of-the-art report on computational stochastic mechanics.\n\nProb. Engrg. Mech. 12(4):197\u2013322 (1997).\n\n[36] M. Shvidler, K. Karasaki, Exact averaging of stochastic equations for transport in\n\nrandom velocity field. Transport in Porous Media 50:223\u2013241 (2003).\n\n[37] M. Shvidler, K. Karasaki, Probability density functions for solute in random field.\n\nTransport in Porous Media 50:243\u2013266 (2003).\n\n[38] K. Sobczyk, Stochastic Wave Propagation. Elsevier-PWN Polish Scientific Pub., New\n\nYork, 1985.\n\n\n\n94 Refere?ncias Bibliogra?ficas\n\n[39] T. T. Soong, Random Differential Equations in Sciences and Engineering. Academic\n\nPress, New York, 1973.\n\n[40] J. C. Strikwerda, Finite Difference Schemes and Partial Differential Equations.\n\nWadsworth &amp; Brooks/Cole, California, 1989.\n\n[41] J. W. Thomas, Numerical Partial Differential Equations: finite difference methods.\n\nSpringer-Verlag, New York, 1995.\n\n[42] Q. Zhang, The asymptotic scaling behavior of mixing induced by a random velocity\n\nfield. Adv. Appl. Math. 16:23\u201358 (1995).\n\n[43] Q. Zhang, The transient behavior of mixing induced by a random velocity field.\n\nWater Research Resources 31:577\u2013591 (1995).\n\n[44] D. Zhang, Stochastic Methods for Flow in Porous Media - Coping with Uncertainties.\n\nAcademic Press, San Diego, 2002."}]}}}