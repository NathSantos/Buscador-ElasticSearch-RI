{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.07116"}, {"@name": "filename", "#text": "11756_Mesquita_MarcosEduardoRibeirodoValle_M.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "Um Estudo Comparativo em Mem\u00f3rias Associativas com Enfase em Memorias Associativas Morfologicas\nAutor: Marcos Eduardo Ribeiro do Valle Mesquita\nOrientador: Prof. Dr. Peter Sussner\nCampinas, SP\nAgosto/2005\nEste exemplar corresponde \u00e0 reda\u00e7\u00e3o final da disserta\u00e7\u00e3o devidamente corrigida e defendida por Marcos Eduardo Ribeiro do Valle Mesquita e aprovada pela comissao comissao julgadora.\nCampinas, 24 de Agosto de 2005\nProf. Dr. Peter Sussner\nBanca Examinadora\n1.\tAlvaro Rodolfo De Pierro, Dr. ....\n2.\tEmanuel Pimentel Barbosa, Dr. ...\n3.\tFernando Gomide, Dr. ......\n4.\tPeter Sussner, Dr. ........\n.... DMA/IMECC/Unicamp\n....DE/IMECC/Unicamp\n.... DCA/FEEC/Unicamp\n.... DMA/IMECC/Unicamp\nDissertacao apresentada ao Instituto de Matematica, Estat\u00edstica e Computacao Cient\u00edfica, UNICAMP, como requisito parcial para obtencao do T\u00edtulo de Mestre em Matematica Aplicada.\nFICHA CATALOGR\u00c1FICA ELABORADA PELA BIBLIOTECA DO IMECC DA UNICAMP\nBibliotec\u00e1rio: Maria J\u00falia Milani Rodrigues - CRB8a / 2116\nMesquita, Marcos Eduardo Ribeiro do Valle\nM562e Um estudo comparativo em mem\u00f3rias associativas com \u00eanfase em mem\u00f3rias associativas morfol\u00f3gicas / Marcos Eduardo Ribeiro do Valle -- Campinas, [S.P. :s.n.], 2005.\nOrientador : Peter Sussner\nDisserta\u00e7\u00e3o (mestrado) - Universidade Estadual de Campinas, Instituto de Matem\u00e1tica, Estat\u00edstica e Computa\u00e7\u00e3o Cient\u00edfica.\n1. Redes neurais (Computa\u00e7\u00e3o). 2. Morfologia matem\u00e1tica. 3. Sistemas de mem\u00f3ria de computadores. I. Sussner, Peter. II. Universidade Estadual de Campinas. Instituto de Matem\u00e1tica, Estat\u00edstica e Computa\u00e7\u00e3o Cient\u00edfica. III. T\u00edtulo.\nT\u00edtulo em ingl\u00eas: A comparative study on associative memories with emphasis on morphological associative memories\nPalavras-chave em ingl\u00eas (Keywords): 1. Neural networks (Computer science). 2. Mathematical morphology. 3. Computer memory systems.\n\u00c1rea de concentra\u00e7\u00e3o: Matem\u00e1tica Aplicada\nTitula\u00e7\u00e3o: Mestre em Matem\u00e1tica Aplicada\nBanca examinadora: Prof. Dr. Peter Sussner (IMECC-UNICAMP)\nProf. Dr. \u00c1lvaro Rodolfo de Pierro (IMECC-UNICAMP)\nProf. Dr. Emanuel Pimentel Barbosa (IMECC-UNICAMP)\nProf. Dr. Fernando Ant\u00f4nio Campos Gomide (FEEC-UNICAMP)\nData da defesa: 24/08/2005\nDisserta\u00e7\u00e3o de Mestrado defendida em 24 de agosto de 2005 e aprovada\nPela Banca Examinadora composta pelos Profs. Drs.\nProE (a). Dr (a). PETER SUSSNER\nProf. (a). Dr (a). FERNANDO ANTONIO CAMPOS GOMIDE\nResumo\nMemorias associativas neurais s\u00e3o modelos do fen\u00f3meno biol\u00f3gico que permite o armazenamento de padr\u00f5es e a recordacao destes ap\u00f3s a apresentacao de uma versao ruidosa ou incompleta de um padrao armazenado. Existem varios modelos de mem\u00f3rias associativas neurais na literatura, entretanto, existem poucos trabalhos comparando as varias propostas. Nesta dissertacao comparamos sistematicamente o desempenho dos modelos mais influentes de mem\u00f3rias associativas neurais encontrados na literatura. Esta comparacao esta baseada nos seguintes criterios: capacidade de armazenamento, distribuicao da informacao nos pesos sinapticos, raio da bacia de atracao, mem\u00f3rias espurias e esfor\u00e7o computacional. Especial enfase e dado para as mem\u00f3rias associativas morfol\u00f3gicas cuja fundamentacao matematica encontra-se na morfologia matematica e na algebra de imagens.\nPalavras-chave: Redes Neurais, Morfologia Matematica, Sistemas de Mem\u00f3ria de Computadores.\nAbstract\nAssociative neural memories are models of biological phenomena that allow for the storage of pattern associations and the retrieval of the desired output pattern upon presentation of a possibly noisy or incomplete version of an input pattern. There are several models of neural associative memories in the literature, however, there are few works relating them. In this thesis, we present a systematic comparison of the performances of some of the most widely known models of neural associative memories. This comparison is based on the following criteria: storage capacity, distribution of the information over the synaptic weights, basin of attraction, number of spurious memories, and computational effort. The thesis places a special emphasis on morphological associative memories whose mathematical foundations lie in mathematical morphology and image algebra.\nKeywords: Neural Networks, Mathematical Morphology, Computer Memory Systems.\nA Margarida Ribeiro do Valle e Mercedes Mesquita Silva\nAgradecimentos\nAos meus pais, Antonio Marcos de Mesquita Silva e Ana LUcia Ribeiro do Valle Silva, pela vida e educa\u00e7\u00e3o que me deram.\nA minha irma, Ana Elisa do Valle Mesquita, e a minha noiva, Luciana Maria Ricci, pelo apoio durante esta jornada.\nAo meu orientador, Peter Sussner, sou grato pela orientacao.\nA minha fam\u00edlia e a todos os meus colegas da Unicamp e Sao Joao da Boa Vista. Em particular para Marcio, Rangel, Gustavo, Roberto, Jorge, Renata, Andre, David, Ederson, Mateus, Homero, Denilson e Angela. As boas amizades permanecem!\nA todos os professores que contribu\u00edram para a minha formacao na Unicamp.\nA FAPESP, pelo apoio financeiro.\nSum\u00e1rio\n1\tIntrodu\u00e7\u00e3o\t1\n1.1\tContexto Hist\u00f3rico............................................................. 2\n1.1.1\tRedes Neurais Artificias................................................ 2\n1.1.2\tMem\u00f3rias Associativas Neurais........................................... 4\n1.1.3\tMorfologia Matematica e Algebra de Imagens ............................. 5\n1.2\tObjetivos e Organizacao da\tDissertacao......................................... 6\n2\tConceitos B\u00e3sicos de Redes Neurais\t9\n2.1\tIntroducao .................................................................... 9\n2.2\tModelos Neurais................................................................ 9\n2.2.1\tModelo Neural Ciassico................................................. 10\n2.2.2\tModelo Neural Morfol\u00f3gico.............................................. 11\n2.3\tArquiteturas de Redes Neurais................................................. 12\n2.4\tAprendizagem ................................................................. 15\n2.4.1\tAprendizado Supervisionado............................................. 15\n3\tConceitos B\u00e3sicos de Memorias Associativas Neurais\t17\n3.1\tFormulacao Matematica, Armazenamento e Associacao............................. 17\n3.2\tClassificacao das Mem\u00f3rias Associativas Neurais............................... 18\n3.3\tCaracter\u00edsticas para um Bom Desempenho ....................................... 20\n4\tMemorias Associativas Lineares\t21\n4.1\tArmazenamento por Correlacao.................................................. 21\n4.1.1\tArmazenamento por Correlacao Auto-associativo Bipolar.................. 23\n4.2\tArmazenamento por Projecao ................................................... 24\n5\tMemorias Associativas Dinamicas\t29\n5.1\tMem\u00f3ria Associativa de Hopfield Discreta ..................................... 29\n5.1.1\tArquitetura da Rede.................................................... 29\n5.1.2\tAprendizado ........................................................... 30\n5.1.3\tConvergencia........................................................... 30\n5.2\tMem\u00f3ria Associativa Bidirecional.............................................. 33\n5.2.1\tArquitetura ........................................................... 33\n5.2.2\tAprendizado ........................................................... 33\n5.2.3\tConvergencia............................................................ 34\n5.3\tMemoria Associativa de Personnaz............................................... 38\n5.3.1\tArquitetura da Rede..................................................... 38\n5.3.2\tAprendizado ............................................................ 38\n5.3.3\tConvergencia............................................................ 39\n5.4\tMem\u00f3ria Associativa de Kanter-Sompolinsky...................................... 39\n5.4.1\tArquitetura da Rede..................................................... 40\n5.4.2\tAprendizado............................................................. 40\n5.4.3\tConvergencia............................................................ 40\n5.5\tMemoria Associativa Bidirecional Assimetrica .................................. 40\n5.5.1\tArquitetura ............................................................ 41\n5.5.2\tAprendizado ............................................................ 41\n5.6\tMemoria Associativa com Capacidade Exponencial................................. 42\n5.6.1\tArquitetura............................................................. 42\n5.6.2\tAprendizado ............................................................ 43\n5.6.3\tConvergencia............................................................ 44\n5.7\tMemoria Associativa Bidirecional com Capacidade Exponencial.................... 44\n5.7.1\tArquitetura............................................................. 44\n5.7.2\tAprendizado ............................................................ 46\n5.7.3\tConvergencia............................................................ 46\n5.8\tModelo do Estado Cerebral numa Caixa (BSB) .................................... 46\n5.8.1\tArquitetura ............................................................ 46\n5.8.2\tAprendizado ............................................................ 47\n5.8.3\tConvergencia............................................................ 47\n6\tMemorias Associativas Morfol\u00f3gicas\t49\n6.1\tMemorias Associativas Morfologicas Heteroassociativas.......................... 49\n6.1.1\tAprendizado ............................................................ 50\n6.2\tMemorias Auto-Associativas Morfologicas ....................................... 58\n6.3\tMemorias Auto-associativas Morfologicas Binarias............................... 61\n6.4\tMemorias Associativas Morfologicas de Duas Camadas ............................ 63\n6.4.1\tArquitetura............................................................. 63\n6.4.2\tAprendizado ............................................................ 65\n7\tDesempenho das Memorias Associativas Binarias\t67\n7.1\tCapacidade de Armazenamento ................................................... 67\n7.1.1\tMemoria Associativa de Hopfield......................................... 69\n7.1.2\tMemoria Associativa Bidirecional........................................ 70\n7.1.3\tMemoria Associativa de Personnaz ....................................... 70\n7.1.4\tMemoria Associativa de Kanter-Sompolinsky............................... 71\n7.1.5\tMemoria Associativa Bidirecional Assimetrica............................ 71\n7.1.6\tMemoria Associativa com Capacidade Exponencial.......................... 71\n7.1.7\tMemoria Associativa Bidirecional com Capacidade Exponencial............. 71\n7.1.8\tMemorias Associativa Morfologicas....................................... 72\n7.1.9\tMem\u00f3ria Associativa Morfol\u00f3gica de Duas Camadas...................... 72\n7.2\tDistribuicao da Informacao ................................................ 72\n7.3\tRaio de Atracao............................................................ 73\n7.4\tMem\u00f3rias EspUrias.......................................................... 77\n7.5\tEsfor\u00e7o Computacional...................................................... 78\n7.5.1\tNUmero de Operacoes na Fase de Armazenamento......................... 80\n7.5.2\tNUmero de Operacoes por Iteracao na Fase de Recordacao............... 81\n7.5.3\tNUmero de Iteracoes na\tFase de Recordacao ........................... 82\n8\tConclus\u00e3o\t85\nReferencias bibliogr\u00e1ficas\t88\nCap\u00edtulo 1\nIntrodu\u00e7\u00e3o\nA primeira pergunta que surge em nossa mente e, o que e uma memoria? A resposta nao e simples e nao e nosso objetivo discutir este assunto, mas podemos dizer que uma memoria e, simplificada-mente, um sistema que possui tres func\u00f3es ou etapas: 1 - Registro, processo pelo qual armazenamos informacao; 2 - Preserva\u00e7\u00e3o, para garantir que a informacao esta intacta; 3 - Recordac\u00e3o, processo pelo qual uma informacao e recuperada [12].\nExistem varios tipos de mem\u00f3ria. Por exemplo, quando escrevemos o numero de um telefone num papel, estamos usando o papel como memoria. Depois poderemos ler e usar este nilmero.\nSempre que registramos informacoes na mem\u00f3ria, precisamos de uma chave ou algo que permita recuperar o conte\u00fado armazenado. Por exemplo, quando deixamos uma bolsa num guarda-volumes, pegamos um ticket indicando o compartimento onde ela ficara. Aqui, o ticket nao possui nenhuma relacao com o contei\u00fado da bolsa e podemos dizer que o endereco (ticket) e apenas um s\u00edmbolo abstrato da mem\u00f3ria onde a entidade (bolsa) esta, e mais, este endereco nao possui nenhuma relacao com o conteudo armazenado. Este tipo de mem\u00f3ria e muito usado nos computadores digitais (mem\u00f3ria RAM ou ROM). Em muitos casos este tipo de mem\u00f3ria apresenta-se eficiente, entretanto possui uma serie de limitac\u00f3es. Por exemplo, o que acontecera se perdermos o ticket?\nSuponha que perdemos o pequeno ticket. Teremos que usar um procedimento diferente para recuperar nossa bolsa. Evidentemente procuraremos o responsavel e passaremos informacoes parciais, mas suficientes, sobre a bolsa e seu conteudo. E comum encontrarmos bolsas semelhantes, mas o conteudo geralmente e diferente e uma descricao parcial dele e suficiente para que o responsavel possa identifica-la, e provavelmente concordara em devolve-la. Neste exemplo, podemos dizer que o endereco (descricao parcial do que tem na bolsa) e igual ao conteudo e dizemos que esta e uma mem\u00f3ria auto-associativa (tambem conhecida por mem\u00f3ria enderecada por conteudo), um caso particular das mem\u00f3rias associativas.\nUma mem\u00f3ria associativa poderia recuperar um item da mem\u00f3ria a partir de informacoes parciais. Por exemplo, se um item armazenado na mem\u00f3ria e \u201cJ.J. Hopfield &amp; D.W. Tank, Biological Cybernetics 52, 141-152 (1985)\u201d. A entrada \u201c&amp; Tank (1985)\u201dpoderia ser suficiente para recuperarmos a informacao completa. Alem disso, uma mem\u00f3ria associativa ideal poderia trabalhar com ru\u00eddos (ou erros) e recuperar esta referencia mesmo a partir de entradas incorretas como \u201c&amp; Rank, (1985)\u201d. Nos computadores digitais, apenas formas relativamente simples de mem\u00f3ria associativa tem sido implementadas em hardware. A maioria dos recursos para tolerancia a ru\u00eddo no acesso da informacao sao implementados via software [40]. Esta e uma das raz\u00f5es para o estudo das mem\u00f3rias associativas.\nAs mem\u00f3rias associativas encontram aplica\u00e7\u00f5es em v\u00e1rios ramos da ci\u00eancia. Por exemplo, Zhang et. al. utilizaram um modelo de memoria associativa para reconhecimento e classificacao de padroes [111, 112]. A metodologia para classificacao de padroes baseada em memorias associativas tambem foi aplicada em problemas de deteccao de falha em motores [54], seguranca de rede [110] e aprendizado de linguagem natural [29]. Hopfield mostrou que seu modelo de memoria associativa pode ser usado para resolver problemas de otimizacao, como por exemplo, o problema do caixeiro viajante [38]. As memorias associativas morfologicas discutidas nesta dissertacao foram aplicadas em problemas de localizacao de faces, auto-localizacao e analise de imagens hiperespectrais [68, 26]. Recentemente, Valle e Sussner apresentaram uma aplicacao das memorias associativas nebulosas em um modelo de previsao [103, 98, 102].\nO termo memoria associativa veio da psicologia e nao da engenharia. Veio da psicologia porque o cerebro humano pode ser visto como uma memoria associativa. Ele associa o item a ser lembrado com um fragmento da recordacao. Por exemplo, ouvindo um trecho de uma musica podemos lembrar da cancao inteira, ou sentido um certo perfume podemos associar o cheiro a uma pessoa especial. Nao so o cerebro humano, mas moscas de fruta ou lesmas de jardim tambem possuem memorias associativas. Na verdade, qualquer sistema nervoso relativamente simples apresenta uma memoria associativa [12]. Isso sugere que a habilidade de criar associacoes e natural - praticamente espontanea - em qualquer sistema neural. Portanto, uma fonte de inspiracoes para os estudos das memorias associativas encontra-se nos estudos do funcionamento de um sistema nervoso, em particular, do cerebro humano.\nO cerebro humano e composto por bilhoes de neur\u00f4nios interligados formando uma rede. Dizemos que o cerebro e uma rede neural biologica. Em nossos estudos, apresentaremos modelos que descrevem um neur\u00f4nio e chamaremos este modelo de neur\u00f4nio artificial. Chamaremos de rede neural artificial, ou simplesmente rede neural, uma rede formada por neur\u00f4nios artificiais [33]. A teoria das redes neurais e vasta e possui aplicacoes em varias areas, como por exemplo, no reconhecimento de padroes, controle, otimizacao e previsao de mercados financeiros [8, 45, 61]. Neste trabalho voltaremos nossa atencao para a intersecao das redes neurais com as memorias associativas. Especificamente, estudaremos as memorias associativas neurais.\nNeste trabalho tambem discutiremos as memorias associativas morfologicas que sao modelos de memoria associativa onde usamos a morfologia matematica e a algebra de imagens como ferramenta [70, 73, 76]. Este tipo particular de memoria associativa neural sera o enfoque principal do nosso trabalho.\nAntes de introduzirmos os conceitos sobre redes neurais e memorias associativas, apresentaremos brevemente a historia das redes neurais, memorias associativas, morfologia matematica e da algebra de imagens.\n1.1\tContexto Hist\u00f3rico\n1.1.1\tRedes Neurais Artificias\nPodemos dizer que os estudos das redes neurais artificiais iniciaram em 1943 quando o bi\u00f3logo Warren McCulloch e o matematico Walter Pitts apresentaram um modelo matematico de um neur\u00f4nio biol\u00f3gico [55]. Eles assumiram que um neur\u00f4nio seguia uma lei \u201ctudo ou nada\u201d e acreditavam que\ncom um n\u00famero suficiente de neur\u00f4nios com conex\u00f5es sin\u00e1pticas apropriadas operando de forma s\u00edncrona (paralelamente), seria poss\u00edvel, a princ\u00edpio, a computacao de qualquer funcao booleana com-putavel. Este foi um resultado muito significativo e com ele e aceito o surgimento das disciplinas de redes neurais e inteligencia artificial [33].\nEm 1949, o neurofisiologista Donald Hebb publicou o livro \u201cThe Organization of Behavior\u201d[34]. Neste livro foi apresentada pela primeira vez a formulacao de uma regra de aprendizagem. Hebb notou que as conex\u00f5es sinapticas do cerebro sao continuamente modificadas conforme um organismo aprende novas tarefas, criando assim agrupamentos neurais. Ele prop\u00f4s no seu livro o seguinte postulado de aprendizagem: \u201cA eficiencia de uma sinapse e aumentada pela interacao entre dois neur\u00f4nios atraves da sinapse\u201d. Este postulado forma a base do que chamamos hoje de aprendizagem (ou regra) de Hebb. Outras regras de aprendizado foram apresentadas posteriormente e muitas sao generalizares da regra de Hebb [2, 32].\nCerca de 15 anos ap\u00f3s a publicacao do classico artigo de McCulloch e Pitts, uma nova abordagem para o problema de reconhecimento de padroes foi introduzida por Rosenblatt [82] em seu trabalho sobre o Perceptron, um metodo inovador de aprendizagem supervisionada. O coroamento deste trabalho encontra-se no teorema da convergencia do perceptron, cuja primeira demonstracao foi delineada por Rosenblatt em 1960. Este teorema garante que o perceptron sempre converge para os pesos corretos se os pesos sinapticos que resolvem o problema existirem. Na mesma epoca, Wi-drow e Hoff introduziram o algoritmo do m\u00ednimo quadrado m\u00e9dio (LMS, Least Mean-Square) e usaram-no para formular o Adaline (Adaptive Linear Element, Elemento Linear Adaptativo) [107]. A diferenca fundamental entre o perceptron e o Adaline esta no procedimento de aprendizagem. Infelizmente existem limites fundamentais para aquilo que o perceptron de camada unica e o Adaline podem calcular [58]. Rosenblatt e Widrow estavam cientes destas limitacoes e apresentaram redes neurais de miultiplas camadas que poderiam superar tais limitacoes, mas nao conseguiram estender seus algoritmos de aprendizado para estas redes mais complexas. Uma destas redes e o Madaline (Multiple-Adaline), proposta por Widrow e seus estudantes, e uma das primeiras redes neurais em camadas de treinamento com m u ltiplos elementos adaptativos. Tais dificuldades e a falta de recursos tecnologicos nos anos 60 proporcionou uma adormecimento nas redes neurais e poucos pesquisadores como James Anderson, Shunichi Amari, Leon Cooper, Kunihiko Fukushima, Stephen Grossberg e Teuvo Kohonen permaneceram no ramo.\nNos anos 80, a ausencia de recursos tecnologicos foi superada e a pesquisa em redes neurais aumentou drasticamente. Computadores pessoais e estacoes de trabalho, que cresciam em capacidade, tornaram-se vitais para o desenvolvimento da pesquisa em redes neurais artificiais. Ale\u00edm disso, novos conceitos foram introduzidos. Dois deles tiveram grande influencia no meio cient\u00edfico. O primeiro foi o uso da mecanica estat\u00edstica para explicar as operacoes e convergencia de algumas redes neurais recorrentes. Este conceito foi introduzido pelo f\u00edsico John Hopfield em 1982 [40]. A segunda chave para o desenvolvimento na decada de 80 foi o algoritmo de retropropagacao (back-propagation), usado para treinar o perceptrons de miultiplas camadas. Este algoritmo foi descoberto por pesquisadores diferentes em diferentes pontos do mundo. Bryson talvez tenha sido o primeiro a estudar o algoritmo de retropropagacao [13, 83]. Entretanto, a publicacao mais influente foi o livro em dois volumes \u201cParallel Distributed Processing: Explorations in the Microstrutures of Cognition\u201d, editado por Rumelhart e McClelland. Este livro exerceu uma grande influencia na utilizacao da aprendizagem por retropropagacao, que emergiu como o algoritmo de aprendizagem mais popular para o treinamento de perceptrons de miultiplas camadas devido a sua simplicidade computacional e eficiencia.\nA historia das redes neurais segue com muitos cap\u00edtulos desafiantes e interessantes, mas ficaremos por aqui, pois acreditamos que voce, leitor, ja esta bem situado no contexto hist\u00f3rico das redes neurais artificiais1. Agora voltaremos aos anos 50 e falaremos sobre as mem\u00f3rias associativas neurais.\n1.1.2\tMem\u00f3rias Associativas Neurais\nOs estudos sobre mem\u00f3ria associativa iniciaram nos anos 50 por Taylor [100]. Em 1961, Steinbruch introduziu o conceito de matriz de aprendizagem [88]. Nos anos seguintes surgiram as mem\u00f3rias associativas holograficas [17, 22, 105, 106] que nao ser\u00e2o discutidas neste trabalho. Em 1972, Anderson [4], Kohonen [47] e Nakano [60] introduziram, de maneira independente, a ideia de uma mem\u00f3ria por matriz de correlacao, baseada na regra de aprendizagem por produto externo que pode ser interpretada como uma generalizacao do postulado de aprendizagem de Hebb. Nestes artigos, os autores apresentaram um modelo linear para os neur\u00f4nios formando a mem\u00f3ria associativa linear que estudaremos no cap\u00edtulo 4.\nEm 1977 foi publicado o livro \u201cAssociative Memory - A System Theoric Approach\u201d de Kohonen [48]. Este foi um dos primeiros livros sobre mem\u00f3rias associativas e apresenta uma analise detalhada das mem\u00f3rias associativas lineares. No mesmo ano, Anderson et al. introduziram o modelo do estado cerebral numa caixa (BSB) [6, 5], uma das primeiras redes neurais que pode ser vista como uma mem\u00f3ria auto-associativa dinamica. Entretanto, o comportamento deste modelo como uma mem\u00f3ria associativa s\u00f3 foi amplamente estudado ap\u00f3s a publicacao dos artigos de Hopfield. Alem de ter aplicacoes como uma mem\u00f3ria associativa, a BSB tambem pode ser vista como um modelo cognitivo [5].\nEm 1982, Hopfield publicou o classico artigo \u201cNeural Networks and Physical Systems with Emergent Collective Computational Abilities\u201d [40] onde introduz a famosa rede (ou memoria associativa) de Hopfield discreta. Neste artigo, Hopfield sugere que um sistema dinamico pode representar uma mem\u00f3ria associativa dinamica onde cada estado estavel do sistema seria um padr\u00e2o memorizado. Para compreender a computacao executada e mostrar a convergencia da rede, Hopfield introduziu o conceito de funcao energia (funcao de Lyapunov) para uma rede neural, um conceito que foi posteriormente usado para a analise de varias outras mem\u00f3rias associativas dinamicas [23, 24, 41, 42]. Hopfield tambem apresentou alguns resultados sobre a capacidade de armazenamento da mem\u00f3ria auto-associativa de Hopfield. Dois anos depois, Hopfield publicou um novo artigo apresentando a rede (ou memoria auto-associativa) de Hopfield cont\u00ednua, uma extensao do modelo apresentado anteriormente que utiliza uma funcao sigm\u00f3ide como funcao de ativacao [37]. Nos anos seguintes, Hopfield e Tank apresentaram aplicacoes da rede de Hopfield em problemas de otimizacao, como o classico problema do caixeiro viajante [38, 39, 99].\nNa decada de 80 e in\u00edcio da decada de 90, foram apresentadas variacoes do modelo de Hopfield. Entre elas, podemos citar a memoria associativa de Hopfield com armazenamento por projecao. Este modelo foi inicialmente discutido por Personnaz et. al em 1985 [67], e posteriormente por Kanter e Sompolinsky em 1987 [44]. Em 1991, Chiueh e Goodman apresentaram a memoria associativa de capacidade exponencial (ECAM), um modelo que abrange o modelo de Hopfield com armazenamento por correlacao [15]. Em 1987, surgiram modelos de memorias associativas dinamicas para heteroassociacao [51, 62]. Entre estes modelos encontramos a memoria associativa bidirecional\n1Para o leitor interessado em maiores detalhes da historia das redes neurais, recomendamos os livros: [7, 28, 33]\n(BAM), introduzida por Kosko, que pode ser vista como uma uma generaliza\u00e7\u00e3o da mem\u00f3ria associativa de Hopfield [51, 52].\nNa metade dos anos 90 iniciaram-se os estudos sobre as mem\u00f3rias associativas morfol\u00f3gicas (MAM), que usam a morfologia matematica e a algebra de imagens como ferramenta matematica para descrever o neur\u00f4nio. Antes de entrar nos detalhes do contexto hist\u00f3rico das mem\u00f3rias associativas morfol\u00f3gicas, vamos voltar novamente no tempo e apresentar brevemente a hist\u00f3ria da morfologia matematica e da algebra de imagens, que sao as ferramentas matematicas usadas nestas mem\u00f3rias associativas.\n1.1.3\tMorfologia Matem\u00e1tica e Algebra de Imagens\nA morfologia matematica surgiu em 1964 enquanto Matheron e Serra estudavam a geometria de meios porosos e analise de textura. Esta nova ferramenta matematica esta baseada nos trabalhos de Minkowski e Hadwiger sobre teoria de medida geometrica e geometria integral [57,27]. Os primeiros anos, de 1964 a 1968, foram dedicados ao desenvolvimento de um corpo de notacoes te\u00f3ricas e de um prot\u00f3tipo para a analise de textura. Deste per\u00edodo podemos citar o artigo \u201cElements pour une theorie des milieux poreux\u201d, de Matheron, onde aparece a primeira transformacao morfol\u00f3gica para investigar a geometria dos objetos de uma imagem. Podemos citar tambem o trabalho em hardware especializado: \u201cTexture Analyser\u201d de J. Serra e J.-C. Kein. Neste per\u00edodo tambem foi criado o \u201cCentre de Morphologie Mathematique\u201d no campus da Escola de Minas de Paris, em Fontainebleu, Franca. Varios pesquisadores juntaram-se a este grupo e formaram o que chamamos hoje de \u201cEscola de Fontainebleu\u201d. Entre eles, podemos citar: Klein, Lantuejoul, Meyer e Beucher [85, 87].\nNo ano 1975, Matheron publicou o livro \u201cRandom Sets and Integral Geometry\u201d, que contem as primeiras fundamentac\u00f3es te\u00f3ricas da morfologia matematica. Este livro foi bem aceito pelos interessados em geometria estocastica, mas infelizmente levou alguns anos para ser aceito pela comunidade de processamento de imagens. Podemos dizer que a morfologia matematica s\u00f3 passou a fazer parte das ferramentas para processamento de imagens ap\u00f3s a publicacao do classico livro \u201cImage Analysis and Mathematical Morphology\u201d de Jean Serra, publicado em 1982. Segundo Heijmans, este livro pode ser visto como o primeiro tratamento sistematico da morfologia matematica como uma ferramenta para a analise de imagens [35]. Uma breve revisao sobre a morfologia matematica em tons de cinza, incluindo abordagens usando a teoria dos conjuntos nebulosos, pode ser encontrada em [96].\nNa decada seguinte houve uma explosao no m\u00edmero de artigos e pesquisadores trabalhando com a morfologia matematica e seria imposs\u00edvel listar todos eles. No Brasil encontramos os rnlcleos de pesquisa liderados por Barrera, na Universidade de Sao Paulo, e Banon, no Instituto Nacional de Pesquisas Espaciais em Sao Jose do Campos. Uma exposicao detalhada contendo os trabalhos mais influentes da morfologia matematica encontra-se em [84]. Esta explosao nao atingiu apenas a morfologia matema\u00edtica, mas todas as atividades envolvendo processamento de imagens e resultou num excesso de tecnicas, notacoes e operac\u00f3es para o processamento de imagens. Surgiu entao a necessidade de uma estrutura algebrica padr\u00e2o, eficiente e com um certo rigor matematico, designado especificamente para o processamento de imagens. Em resposta a esta situacao, pesquisadores da Universidade da Fl\u00f3rida desenvolveram uma estrutura matematica para analise e processamento de imagens conhecida como \u00c1lgebra de Imagens (Image Algebra). Muitas sao as vantagens da algebra de imagens, mas nao vamos lista-las aqui. Vamos dizer apenas que ela abrange todas as areas de processamento de imagens e visao computacional usando uma linguagem comum. Para o leitor inte-\nressado, recomendamos [69, 79, 81].\nA algebra de imagens nasceu na metade da decada de 80 e os primeiros artigos apareceram em 1987 [72, 78, 80]. Em 1990, Ritter,Wilson e Davidson publicaram o artigo: \u201cImage Algebra: An Overview\u201d[81]. Este artigo descreve as estruturas algebricas da algebra de imagens e teve um papel importante na divulgacao desta teoria matematica. Nos anos seguintes, a algebra de imagens comecou a ganhar aplicac\u00f3es. As redes neurais e as operacoes da morfologia matematica foram descritas e estudadas usando elementos e operacoes da algebra de imagens [20, 71, 69]. Sendo descritas pela mesma estrutura matematica, foi facil combina-las, criando as redes neurais morfol\u00f3gicas, antes conhecidas como redes da algebra de imagens (IA networks).\nUma rede neural morfol\u00f3gica e uma rede neural onde os neur\u00f4nios sao descritos pelas operacoes da morfologia matematica. Estas operacoes podem ser formuladas usando a algebra de imagens, mas esta nao e a ilnica formulacao matematica de uma rede neural morfol\u00f3gica. Uma formulacao diferente, que nao sera discutida aqui, pode ser encontrada em [108]. Entre as redes neurais morfol\u00f3gicas, encontramos as mem\u00f3rias associativas morfol\u00f3gicas (Morphological Associative Memory, MAM). O estudo das mem\u00f3rias associativas morfol\u00f3gicas e recente, a primeira publicacao apareceu em 1996 [73]. Outras publicac\u00f3es vieram depois [74, 76, 91, 92, 93, 94, 97], mas ainda sao poucas e com certeza existem muitos modelos e teorias para serem descobertas e estudadas. Uma nova classe de mem\u00f3rias associativas neurais baseadas na teoria dos conjuntos nebulosos conhecida como \u201cMemorias Associativas Nebulosas Implicativas\u201d (Implicative Fuzzy Associative Memories, IFAM) [103, 98, 102]. As mem\u00f3rias associativas nebulosas implicativas generalizam as mem\u00f3rias associativas morfol\u00f3gicas quando a ultima e aplicada a padr\u00f5es nebulosos. As mem\u00f3rias associativas nebulosas implicativas, por sua vez, podem ser vistas como um caso particular das mem\u00f3rias associativas morfol\u00f3gicas nebulosas que serao discutidas em trabalhos futuros. Neste trabalho nao discutiremos as mem\u00f3rias associativas nebulosas implicativas nem as mem\u00f3rias associativas morfol\u00f3gicas nebulosas.\n1.2\tObjetivos e Organiza\u00e7\u00e3o da Disserta\u00e7\u00e3o\nMuitos modelos de mem\u00f3ria associativa neural foram apresentados nos ultimos anos, entretanto, nao encontramos na literatura um trabalho reunindo e comparando os modelos mais influentes. Esta dissertacao de mestrado tem como objetivo principal discutir os principais modelos de mem\u00f3ria associativa neural, incluindo as mem\u00f3rias associativas morfol\u00f3gicas. Os modelos de mem\u00f3ria associativa neural mais influentes sao essencialmente modelos binarios. Por esta razao, esta dissertacao e dedicada principalmente as mem\u00f3rias associativas neurais binarias.\nTambem nao existe na literatura um criterio comum de comparacao para o desempenho de mem\u00f3rias associativas neurais. Esta dissertacao de mestrado tambem tem como objetivo formalizar conceitos usados empiricamente na literatura para a comparacao de modelos de mem\u00f3rias associativas. Formalizados os conceitos, usamos estes para comparar os modelos mais influentes de mem\u00f3ria associativa binaria.\nResumindo, os objetivos desta dissertacao de mestrado sao:\n1.\tReunir os modelos mais influentes de mem\u00f3ria associativa neural (binarias) incluindo as mem\u00f3rias associativas morfol\u00f3gicas.\n2.\tFormalizar criterios para comparacao do desempenho das mem\u00f3rias associativas neurais binarias.\n3.\tComparar os modelos mais influentes com base nos criterios discutidos no item anterior.\nEsta dissertacao de mestrado esta dividida em 8 cap\u00edtulos. Os cap\u00edtulos 2 e 3 tratam dos conceitos basicos de redes neurais e mem\u00f3rias associativas neurais. Especificamente, no cap\u00edtulo 2, discutimos o conceito de rede neural artificial e como classifica\u00ed-la. Apresentamos alguns modelos neurais e falamos brevemente sobre regras de aprendizado. No cap\u00edtulo 3, apresentamos uma formulacao matematica para o problema das mem\u00f3rias associativas, como classifica-las e apresentamos uma lista contendo as caracter\u00edsticas desejaveis para o bom desempenho de uma mem\u00f3ria associativa neural.\nOs cap\u00edtulos 4, 5, e 6 tem como objetivo apresentar os modelos mais influentes de mem\u00f3ria associativa neural incluindo as mem\u00f3rias associativas neurais morfol\u00f3gicas, isto e, estes tres cap\u00edtulos cobrem o primeiro item dos objetivos desta dissertacao. Precisamente, no cap\u00edtulo 4, discutimos as mem\u00f3rias associativas lineares. Estes sao os modelos mais simples de mem\u00f3ria associativa neural e possuem um papel fundamental: definem as principais regras para armazenamento de padr\u00f5es numa mem\u00f3ria associativa neural. No cap\u00edtulo 5 examinamos varios modelos de mem\u00f3rias associativas dinamicas para auto e heteroassocicao. Introduzimos neste cap\u00edtulo a Mem\u00f3ria Associativa Bidirecional com Capacidade Exponencial que e uma extensao da Mem\u00f3ria Associativa com Capacidade Exponencial para o caso hetero-associativo. No cap\u00edtulo 6 discutimos as mem\u00f3rias associativas morfol\u00f3gicas.\nO cap\u00edtulo 7 cobre os objetivos listados nos itens 2 e 3 acima. Neste cap\u00edtulo formalizamos alguns conceitos para comparacao das mem\u00f3rias associativas neurais binarias. Neste cap\u00edtulo tambem apresentamos resultados te\u00f3ricos e emp\u00edricos para a comparacao dos modelos mais influentes de mem\u00f3ria associativa neural. Terminamos a dissertacao no cap\u00edtulo 8 com a conclusao.\nForam realizados varios experimentos computacionais nesta dissertacao de mestrado. Todos eles foram conduzidos no software MATLAB. As implementac\u00f3es dos modelos de mem\u00f3ria associativa apresentadas nos cap\u00edtulos 5 e 6, bem como as rotinas usadas nos experimentos computacionais do cap\u00edtulo 7, podem ser obtidas com o autor em sua pagina pessoal [104].\nAs imagens usadas nesta dissertacao de mestrado podem ser encontradas na pagina eletr\u00f4nica do grupo CVG (Computer Vision Group) [1]. As imagens originais foram convertidas em imagens menores de dimensao 64 x 64 pixels usando o comando imresize do MATLAB com o metodo de interpolacao padr\u00e2o. Depois transformamos cada imagem num vetor coluna com 4096 componentes usando o comando reshape do MATLAB. As imagens binarias apresentadas na figuras 1.1 e 1.2 serao usadas com frequencia nos exemplos computacionais com imagens binarias. Estas imagens foram obtidas aplicando um threshold nas imagens com tamanho reduzido, antes de serem transformadas num vetor coluna. Os n\u00edveis dos thresholds foram calculados usando o metodo de Otsu que minimiza a variancia entre os valores preto e branco dos pixels [63] e pode ser obtido usando o comando graythresh no MATLAB. Nesta dissertacao, o valor 1 representa o preto (objeto) e o valor 0 representa o branco numa imagem binaria. As imagens apresentadas na figura 1.3 serao usadas nos exemplos computacionais com imagens em tons de cinza.\n1\nx2\nx3\nx4\n5\nx\nx\nFig. 1.1: Padr\u00f5es x1, x2,..., x5 usados nos exemplos computacionais com imagens bin\u00e1rias.\ny1\ny2\ny4\nr\ny\n5\nFig. 1.2: Padr\u00f5es y1, y2,..., y5 usados nos exemplos computacionais com imagens bin\u00e1rias.\nx1\n2\n3\n4\nx\nx\nx\nFig. 1.3: Padroes x1, x2, x3, x4 usados nos exemplos computacionais com imagens em tons de cinza.\nCap\u00edtulo 2\nConceitos Basicos de Redes Neurais\nNeste cap\u00edtulo discutimos os conceitos b\u00e1sicos de redes neurais artificiais; a nota\u00e7\u00e3o e a nomenclatura que sera usada durante toda a dissertacao.\n2.1\tIntrodu\u00e7\u00e3o\nPodemos dizer que os computadores modernos sao retardatarios no mundo da computacao, pois os computadores biol\u00f3gicos - o cerebro e o sistema nervoso animal e humano - existem por milh\u00f5es de anos e sao extremamente eficientes no processamento de informacoes sensoriais e no controle da interacao entre o animal e o meio em que vive. Tarefas como procurar um sandu\u00edche, reconhecer uma face ou relembrar coisas sao operac\u00f3es simples, como somas e multiplicac\u00f3es, para o nosso cerebro.\nO fato dos computadores biol\u00f3gicos serem tao efetivos sugere que possamos extrair caracter\u00edsticas similares a partir de um modelo do sistema neural. O modelo matematico que descreve o sistema nervoso biol\u00f3gico e conhecido como rede neural artificial ou simplesmente rede neural1 [33]. Apesar de nossos modelos serem apenas metaforas do sistema nervoso biol\u00f3gico, eles fornecem um modo elegante e diferente de compreendermos o funcionamento de maquinas computacionais, alem de oferecer informacoes uteis sobre o funcionamento do nosso cerebro.\nUma rede neural e caracterizada por tr\u00eas fatores:\n1.\tModelos (ou caracter\u00edsticas) neurais,\n2.\tArquitetura (ou topologia) da rede,\n3.\tRegra de aprendizado.\nNas pr\u00f3ximas sec\u00f3es discutiremos estes fatores.\n2.2\tModelos Neurais\nOs neur\u00f4nios, ou celulas nervosas, sao os elementos computacionais usados pelo sistema nervoso. Podemos identificar tr\u00eas elementos basicos no modelo neural:\n1 As redes neurais tamb\u00e9m sao referidas na literatura como neurocomputadores, redes conexionistas ou processadores paralelamente distribu\u00eddos.\n1.\tUm conjunto de sinapses (ou elos de conexeos), cada uma caracterizada por um peso. Espec\u00edficamente, um sinal Xj, na entrada da sinapse j do neur\u00f4nio i, interage com o peso sinaptico Wj. Note que o primeiro \u00edndice de w refere-se ao neur\u00f4nio em questao e o segundo refere-se ao terminal de entrada da sinapse.\n2.\tUma regra de propagacao, que define as operacoes usadas para processar os sinais de entrada, ponderados pelas respectivas sinapses. Normalmente estas operacoes constituem um combinador linear. As operacoes usadas no modelo ciassico e a multiplica\u00e7\u00e3o seguida da soma; entretanto, no modelo morfol\u00f3gico do neur\u00f4nio, usaremos a soma e uma operacao de maximo ou m\u00ednimo.\n3.\tUma funcao de ativacao, usada para introduzir nao-linearidade no modelo e/ou restringir a amplitude da sa\u00edda de um neur\u00f4nio. Neste trabalho usaremos basicamente quatro tipos de funcoes de ativacao: identidade, funcao linear por partes, funcao sinal ou limiar, e a funcao exponencial. Deixaremos a funcao de ativacao impl\u00edcita quando usarmos a identidade.\nE comum encontramos um bias nos modelos neurais artificiais. O bias pode ser visto como uma sinapse (wi0) conectada a uma entrada constante (x0). Por efeito de simplicidade, nao acrescentaremos o bias nos nossos modelos neurais.\nEncontramos varios modelos neurais na literatura [55, 73], como por exemplo os modelos neurais nebulosos [65, 103, 98, 102]. Neste trabalho, discutiremos somente o modelo neural classico e o modelo neural morfol\u00f3gico. Apresentaremos a seguir cada um destes modelos.\n2.2.1\tModelo Neural Cl\u00e1ssico\nO modelo neural cl\u00e1ssico foi proposto por McCulloch e Pitts [55]. Neste modelo, um neur\u00f4nio i \u00e9 descrito pelo par de equacoes:\nn\nVi\tXI wijxj, e yi = ^(vi),\nj=i\n(2.1)\nou pela unica equacao\nyi y y wij xj^ ,\n(2.2)\nonde x1, x2,... ,xn s\u00e3o os sinais de entrada, wn, wi2,..., win s\u00e3o os pesos sin\u00e1pticos do neur\u00f4nio i, ^(\u2022) e a func\u00e3o de ativac\u00e3o, vi e a ativac\u00e3o do neur\u00f4nio e yi e o sinal de sa\u00edda. Na figura 2.1 temos uma representac\u00e3o simb\u00f3lica de um neur\u00f4nio cl\u00e3ssico com n entradas.\nUm conjunto com m neur\u00f4nios em paralelo poder ser escrito na forma matricial atraves da equac\u00e3o\ny = 4(W x,\t(2.3)\nonde x e o vetor coluna contendo os sinais de entrada, W G Rmxn representa a matriz de pesos sin\u00e3pticos, e uma func\u00e3o vetorial com componentes ^i : R \u2014> R e y e o vetor coluna contendo a sa\u00edda da rede. Note que cada linha da equac\u00e3o (2.3) representa um neur\u00f4nio descrito por (2.2).\n^(\u2022)\nyi\nFig. 2.1: Representa\u00e7\u00e3o do modelo matem\u00e1tico cl\u00e1ssico de um neur\u00f4nio artificial.\n2.2.2\tModelo Neural Morfol\u00f3gico\nO modelo neural morfol\u00f3gico foi proposto por Davidson e Ritter no in\u00edcio dos anos 90 [20,73,74,76]. Neste modelo, a soma e substitu\u00edda pelo maximo ou pelo m\u00ednimo e a multiplicacao e substitu\u00edda pela soma. Uma motivacao biol\u00f3gica para esta substituicao e fornecida em [77]. Matematicamente, um neur\u00f4nio morfol\u00f3gico i e descrito pela equacao\nn\nVi = (Wil + X1) V (Wi2 + X2) V ... V (Win + Xn) = \\/ (\u2122ij + Xj) ,\t(2.4)\nj=1\nou\nn\nVi = (Wi1 + X1) A (Wi2 + X2) A ... A (Win + Xn) = /\\ (Wj + Xj) ,\t(2.5)\nj=1\nem conjunto com a equacao\nyi = ^(Vi).\t(2.6)\nNote que a equacao (2.6) e identica a segunda equacao de (2.1). Logo, a diferenca entre o modelo classico e o modelo morfol\u00f3gico esta no calculo da ativacao do neur\u00f4nio (vi). Neste trabalho usaremos somente a funcao identidade e a funcao limiar como funcao de ativacao no modelo neural morfolo\u00ed gico.\nO modelo neural descrito pelas equacoes (2.4) e (2.6), e o modelo descrito pelas equacoes (2.5) e\n(2.6)\tsao conhecidos na literatura como modelo neural morfol\u00f3gico porque (2.4) e (2.5) representam as operacoes basicas da morfologia matematica: dilatacao e eros\u00e3o, respectivamente [79, 85, 87].\nUm conjunto com m neur\u00f4nios morfol\u00f3gicos em paralelo tambem pode ser escrito na forma matricial de um modo analogo a equacao (2.3) do modelo classico. Para tanto, precisamos definir um produto matricial em termos das operacoes de maximo ou m\u00ednimo, e da soma. Para uma matriz A E Rmxp, e uma matriz B E Rpxn, o produto matricial C = A IV B, tambem conhecido como produto maximo de A por B, e definido por\np\ncij\t\\J (aik + bkj) .\t(2.7)\nk=1\nO produto m\u00ednimo de A por B e definido de modo semelhante. Especificamente, os elementos de C = A El B sao dados por\np\ncij\t(aik + bkj) \u2022\t(2.8)\nk=1\nUm conjunto com m neur\u00f4nios morfologicos, expresso na forma matricial, e dado por:\ny = (W E x),\t(2.9)\nou\ny = (W 0 x) \u2022\t(2.10)\nAs equacoes (2.9) e (2.10) generalizam o par de equacoes (2.4) e (2.6), e o par (2.5) e (2.6), respectivamente.\nOs neuronios morfologicos descritos pela equacao (2.9) estao baseado em operacoes da estrutura algebria (R, V, +), conhecida como belt [18,19]. Analogamente, os neuronios morfologicos descritos pela equacao (2.10) estao baseados em operacoes no belt (R, l, +). Os belts (R, V, +) e (R, l, +) podem ser combinados formando a estrutura algebrica de grupo ordenado-reticulado (lattice-ordered group) (R, V, l, +). Existe uma elegante dualidade em (R, V, l, +) obtida a partir da equacao r lu = \u2014((\u2014r) V (\u2014u)), valida para os ni\u00edmero reais. Dado r G R, definimos o conjugado aditivo r * como r* = \u2014r. Desta forma, (r*)* = r e r l u = (r* V u*)* para todo r, u G R. Se A G Rmxn, entao a matriz conjugada A* de A e a dada por A* = \u2014 AT. Segue entao que\nA l B = (A* V B*)*,\t(2.11)\ne\nA 0 B = (B* 0 A*)*,\t(2.12)\npara matrizes de tamanho apropriado. Consequentemente, uma rede neural morfologica formulada usando a operacao 0 pode ser reformulada em termos da operacao l, e vice-versa, usando a relacao expressa na equacao (2.12). Alem disso, toda proposicao em (R, V, l, +) induz uma proposicao dual obtida substituindo o s\u00edmbolo l por V e vice-versa, e revertendo as desigualdades.\nNote que o modelo neural morfologico nao envolve operacoes de multiplicacao, mas apenas operacoes como maximo ou m\u00ednimo e soma. Temos entao um modelo neural com computacoes r\u00e1pidas e de r\u00e1cil implementacao em hardware. Problemas de convergencia e longos algoritmos de treinamento praticamente nao existem [75]. Alem disso, as redes neurais morfologicas sao capazes de resolver os problemas computacionais convencionais [73].\n2.3\tArquiteturas de Redes Neurais\nA arquitetura (ou topologia) de uma rede neural consiste na estrutura de interconexao dos seus neuronios que sao geralmente organizados em camadas com um ou varios neuronios. Na literatura encontramos a seguinte classificacao para as camadas de neuronios:\n\u2022 Camada de Entrada: Camada de neuronios que introduz as entradas externas na rede. Os neuronios desta camada sao chamados de neuronios de entrada.\nCamada de\nEntrada\nCamadas Ocultas\nCamada de\nSa\u00edda\nO\nO\nO\nFig. 2.2: Nomenclatura das camadas de uma rede neural. Rede neural progressiva totalmente conexa de multiplas camadas.\n\u2022\tCamada de Sa\u00edda: Camada de neur\u00f4nios que produz a sa\u00edda da rede. Os neur\u00f4nios desta camada sao chamados neur\u00f4nios de sa\u00edda.\n\u2022\tCamada Oculta: Camada de neur\u00f4nios que interage com outras camadas da rede. Os neur\u00f4nios desta camada sao aqueles que nao pertencem nem a camada de entrada nem a camada de sa\u00edda da rede.\nNa figura 2.2 apresentamos uma rede de mi\u00edltiplas camadas com duas camadas ocultas, sendo uma delas composta por um \u00fanico neur\u00f4nio.\nE comum caracterizarmos uma rede pelo numero de camadas. A contagem das camadas e feita considerando apenas as camadas com pesos ajustaveis. Nao contamos a camada de entrada, pois ela nao tem pesos ajustaveis. Uma rede neural que nao possui camadas ocultas e chamada rede de camada \u00fanica, pois encontramos pesos ajustaveis somente na camada de sa\u00edda. Na figura 2.3 apresentamos uma rede de camada unica. Numa rede de m\u00faltiplas camadas encontramos uma ou mais camadas ocultas. A figura 2.4 ilustra como e feita a contagem das camadas.\nUma rede de mi\u00e1ltiplas camadas e dita progressiva2 quando as conex\u00f5es sinapticas avancam para a sa\u00edda da rede, ou seja, quando nao houver laco de realimentacao3 na rede. Apresentamos exemplos de redes progressivas nas figuras 2.2, 2.3 e 2.4. Chamaremos rede recorrente quando houver conexoes entre neuronios de uma mesma camada e/ou conexoes retropropagadas. Apresentamos na figura 2.5 uma rede recorrente com uma camada de neuronios ocultos.\nUma rede de milltiplas camadas e dita totalmente conexa quando cada neur\u00f4nio de uma camada esta conectado a todos os neur\u00f4nios da camada seguinte. As figuras 2.2 e 2.3 apresentam redes totalmente conexas. Se alguns elos de comunicacao (conex\u00f5es sinapticas) estiverem faltando, diremos que a rede e\u00ed parcialmente conexa. Na figura 2.4 encontramos uma rede parcialmente conexa. A mesma nomenclatura vale para uma rede recorrente de camada unica. Na figura 2.6 apresentamos uma rede recorrente de camada unica totalmente conexa. Neste caso, todos os neur\u00f4nios possuem laco de alimentacao de modo que a sa\u00edda da iteracao t e usada como entrada na iteracao t +1.\n2Traducao para o termo ingles \u201cfeedforward\u201d.\n3 Existe realimentacao quando a sa\u00edda de um elemento influencia em parte a entrada aplicada aquele elemento particu\nlar.\nCamada de\nEntrada\nCamada de Sa\u00edda (Primeira Camada)\nFig. 2.3: Rede neural progressiva totalmente conexa de camada ilnica.\nCamada de\tPrimeira\nEntrada\tCamada\nSegunda Terceira Camada Camada\nCamada de Sa\u00edda (Quarta Camada)\nFig. 2.4: Numeracao das camadas de uma rede neural de mUltiplas camadas. Rede neural progressiva parcialmente conexa com multiplas camadas.\nCamada de\tCamada\tCamada de\nEntrada\tOculta\tSa\u00edda\nFig. 2.5: Rede recorrente com uma camada oculta.\nFig. 2.6: Rede recorrente de camada \u00fanica totalmente conexa.\n2.4\tAprendizagem\nA capacidade de aprender e uma das principais caracter\u00edsticas da inteligencia. O aprendizado numa rede neural e realizado ajustando-se os pesos das conex\u00f5es sinapticas. Em outras palavras, aprendizagem e o processo onde os par\u00e2metros livres de uma rede sao modificados. O tipo de aprendizagem e determinado pela maneira pela qual ocorre a modificacao dos par\u00e2metros.\nExistem dois tipos basicos de aprendizagem numa rede neural: aprendizado supervisionado (ou aprendizado com professor) e aprendizado nao-supervisionado (ou aprendizado sem professor). Em ambos os casos precisamos de um conjunto de dados, conhecidos como dados de treinamento.\nO aprendizado supervisionado consiste na apresentacao de exemplos de entrada-sa\u00edda. Durante o processo de aprendizado e feito um ajuste nos pesos de forma a minimizar a diferenca (erro) entre a resposta da rede e a resposta desejada4. Temos assim a acao de um \u201cprofessor\u201d que apresenta a resposta correta indicando a acao \u00f3tima a ser realizada pela rede neural.\nNo aprendizado n\u00e3o supervisionado, apenas os dados de entrada sao fornecidos. Neste caso, o aprendizado esta baseado em agrupamentos de padr\u00f5es. Os pesos sao ajustados de modo que padr\u00f5es semelhantes produzam a mesma sa\u00edda.\nNos estudos das mem\u00f3rias associativas usaremos somente o aprendizado supervisionado, pois sempre teremos os dados de entrada e as respectivas sa\u00eddas desejadas.\n2.4.1\tAprendizado Supervisionado\nUma rede neural articial com n neur\u00f4nios na camada de entrada e m neur\u00f4nios na camada de sa\u00edda pode representar uma funcao G :\t[21]. Por exemplo, uma rede neural progressiva classica\nde camada unica pode ser escrita como\ny = G(x) = $(W x).\n4 A aprendizagem supervisionada pode ser vista como aprendizagem por corre\u00e7\u00e3o de erro.\nLembre-se que a rec\u00edproca tambem e verdadeira visto que as redes neurais classicas sao aproxima-dores universais [33]. Temos que observar, porem, que falta conduzir pesquisas sobre a utilizacao de redes neurais morfol\u00f3gicas como aproximadores de funcoes.\nNo aprendizado supervisionado conhecemos o conjunto de vetores de entrada e suas respectivas sa\u00eddas , para \u00a3 = 1, 2,..., k. Podemos entao interpretar a rede neural como uma funcao dos pesos sinapticos e resolver um problema de otimizacao onde minimizamos o erro cometido pela rede neural. Por exemplo, para encontrar a matriz dos pesos sinapticos de uma rede neural classica progressiva de camada ilnica, resolvemos o problema:\nmin ||y^ \u2014 (Wx^) ||, para todo \u00a3 = 1,..., k.\nExistem varios algoritmos de aprendizado supervisionado que resolvem um problema de otimizacao. O mais conhecido e o algoritmo de retropropaga\u00e7\u00e3o (backpropagation) [21, 33]. Entretanto, devido ao elevado custo computacional, nao utilizaremos nenhum algoritmo de otimizacao complexo para encontrar a matriz dos pesos sinapticos. Neste trabalho utilizaremos somente regras simples, como o armazenamento por correlacao ou o armazenamento por projecao, para treinar nossas redes neurais (veja Cap\u00edtulo 4).\nCap\u00edtulo 3\nConceitos Basicos de Memorias Associativas Neurais\nNeste cap\u00edtulo apresentamos os fundamentos matematicos das memorias associativas neurais e especificamos a notacao e a nomenclatura que sera usada durante toda a dissertacao.\n3.1\tFormula\u00e7\u00e3o Matem\u00e1tica, Armazenamento e Associa\u00e7\u00e3o\nUma mem\u00f3ria associativa (Associative Memory, AM) representa um sistema de entrada-sa\u00edda (inputoutput) que armazena varios pares de padroes (x, y), onde x G e y G Rm. Numa memoria associativa criamos um mapeamento entre a entrada e a sa\u00edda dado por y = G(x), onde G :\t>\ne o mapeamento associativo da memoria. Cada par entrada-sa\u00edda (x, y) armazenado na memoria e dito uma associacao. A entrada do sistema (vetor x) e conhecido como padr\u00e3o-chave (ou memoria-chave) e a sa\u00edda (vetor y) e chamado padrao recordado.\nA formulacao matematica para um problema de memoria associativa pode ser escrita como: Dado um conjunto finito de pares de entrada-sa\u00edda {(x^, y^) : \u00a3 = 1, 2,..., k} a ser armazenado, nossa tarefa e encontrar um mapeamento que recupere cada um destes pares, isto e, determinar uma funcao G tal que G(x^) = y^, para todo \u00a3 = 1,..., k [31]. Alem disso, desejamos que G tenha toler\u00e2ncia a ru\u00eddo (capacidade de correcao de erros). Assim, se x^ e uma versao ruidosa de x^, isto e, se x^ = x^ e d(x^, x) &lt;5 com 5 > 0 pequeno, desejamos que x^ e x^ produzam a mesma sa\u00edda, ou seja, G(x^) = y^ (a funcao G nao e injetora). Nas memorias associativas neurais utilizamos uma rede neural para representar o mapeamento associativo G e a fase de armazenamento reduz-se a determinar a(s) matriz(es) dos pesos sina\u00edpticos.\nNote que, se um par (x^, y^) foi corretamente armazenado numa memoria associativa e se G e um mapeamento \u00edmpar, entao G(\u2014x) = \u2014 G(x) = \u2014y, ou seja, o par (\u2014x^, y;) tambem foi armazenado na memoria. Podemos mostar que uma rede neural classica com funcoes de ativacao \u00edmpares produz um mapeamento G \u00edmpar. Este fato nao vale para as redes neurais morfol\u00f3gicas devido as operacoes de maximo e m\u00ednimo usadas no modelo neural morfologico. Varias memorias associativas neurais classicas utilizam funcoes de ativacao \u00edmpar e, consequentemente, representam mapeamentos \u00edmpares. Como exemplo temos a rede de Hopfield, a BAM e a BSB.\nO conjunto das associacoes {(x^, y^), \u00a3 = 1,..., k} e chamado conjunto das memorias funda-\nmentais. Cada associa\u00e7\u00e3o (x^, ) neste conjunto \u00e9 uma mem\u00f3ria fundamental, cada padr\u00e3o \u00e9 uma chave fundamental e cada padr\u00e2o y^ neste conjunto \u00e9 uma recorda\u00e7\u00e3o fundamental. Quando o conjunto das mem\u00f3rias fundamentais e da forma {(x^, x^), \u00a3 = 1, 2,... , k}, dizemos que esta e uma mem\u00f3ria auto-associativa. Neste caso particular, os termos mem\u00f3ria fundamental, chave fundamental e recordacao fundamental sao sin\u00f4nimos. No caso geral, quando y^ e diferente de x^, temos uma mem\u00f3ria heteroassociativa.\nO processo usado para determinar (ou sintetizar) uma mem\u00f3ria associativa e conhecido como fase de armazenamento. Um dos principais objetivos numa mem\u00f3ria associativa e criar um mapeamento com uma grande capacidade de armazenamento, isto e, uma vasta quantidade de mem\u00f3rias fundamentais podem ser armazenadas [33]. Um dos maiores problemas em uma mem\u00f3ria associativa e a criacao de associates que nao fazem parte do conjunto das mem\u00f3rias fundamentais. Estas associates, armazenadas indevidamente, sao as chamadas memorias esp\u00farias.\nQuando a fase de armazenamento esta completa, inicia-se a fase de recordacao. Aqui, uma mem\u00f3ria pode ser testada para verificar se as mem\u00f3rias fundamentais foram corretamente armazenadas e a capacidade de correcao de erro pode ser medida apresentando as chaves fundamentais corrompidas com varios tipos de ru\u00eddos e observando a sa\u00eddas resultantes, i.e., comparamos a sa\u00edda de uma entrada ruidosa com a sa\u00edda desejada. O conjunto dos pontos x G tais que G(x) = y e chamado regiao de recordacao do padr\u00e2o y.\n3.2\tClassificacao das Memorias Associativas Neurais\nAs mem\u00f3rias associativas neurais podem ser divididas em duas grandes classes: as mem\u00f3rias associativas est\u00e1ticas e as memorias associativas dinamicas (Dynamic Associative Memory, DAM). As mem\u00f3rias associativas neurais estaticas podem ser descritas por uma rede neural progressiva. Uma rede neural recorrente usada como mapeamento associativo produz uma mem\u00f3ria associativa dinamica. Portanto, a arquitetura da rede neural utilizada (progressiva ou recorrente) define a arquitetura da mem\u00f3ria associativa neural (estatica ou dinamica, respectivamente).\nOs padr\u00f5es armazenados numa mem\u00f3ria associativa neural podem ser bipolares ({-1,1}), binarios ({0,1}), discretos (Z) ou cont\u00ednuos (R). Apresentaremos nesta dissertacao modelos de mem\u00f3rias associativas neurais para padr\u00f5es cont\u00ednuos mas daremos enfase as mem\u00f3rias associativas bipolares e binarias.\nA fase de recordacao de uma mem\u00f3ria associativa dinamica pode ser interpretada como um processo temporal que assume valores discretos ou cont\u00ednuos. Deste modo, classificamos as mem\u00f3rias associativas dinamicas como sendo discretas ou cont\u00ednuas no tempo. Nesta dissertacao nao discutiremos as mem\u00f3rias associativas dinamicas cont\u00ednuas no tempo [33, 37, 36]. Na figura 3.1 apresentamos um diagrama com a classificacao das mem\u00f3rias associativas neurais. Os modelos com caixas pontilhadas nao serao discutidos neste trabalho.\nAs mem\u00f3rias associativas dinamicas discretas no tempo podem ser descritas pelas equates\ny(t)\t=\tF (x(t)),\t(3.1)\nx(t + 1)\t=\tH(y(t)),\t(3.2)\nV t\t=\t0,1,...\t(3.3)\nFig. 3.1: Diagrama para classifica\u00e7\u00e3o de uma mem\u00f3ria associativa neural. Os modelos com caixas pontilhadas nao serao discutidos neste trabalho.\nonde F :\te H :\tsao func\u00f3es nao lineares. Dizemos que o par (x, y) e um ponto\nestacion\u00e1rio de uma mem\u00f3ria associativa dinamica se F(x) = y e H (y) = x. Um ponto estacionario e tambem referido como ponto fixo no caso auto-associativo. O conjunto dos pontos x = x(0) para o qual a seq\u00fcencia dos pares {(x(0), y(0)), (x(1), y(1)),...)} gerada atraves das equac\u00f3es (3.1)-(3.3) nao converge e conhecido como regiao de indecis\u00e3o. Note que uma mem\u00f3ria associativa dinamica sempre converge para um ponto estacionario independente do padr\u00e2o-chave se e somente se a regiao de indecisao for vazia. Devemos impor um numero maximo de iterac\u00f3es na ausencia de informac\u00f3es sobre a regiao de indecisao ou quando soubermos que esta e nao vazia.\nA interpretacao das equac\u00f3es (3.1) e (3.2) depende do tipo de atualizacao das componentes dos vetores y(t) e x(t +1). As duas formas de atualizacao mais comum sao: sincronizada (ouparalela) e ass\u00edncrona (ou sequencial). Numa mem\u00f3ria associativa com atualizacao sincronizada, todas as componentes dos vetores y (t) e x(t + 1) sao atualizadas simultaneamente a cada iteracao. Na atualizacao ass\u00edncrona, a cada iteracao t, definimos It como sendo uma permutacao do conjunto {1, 2,..., m}, Jt como sendo uma permutacao de {1, 2,... ,n} e computamos\nyi(t) = Fi(x(t)) , i G It\nxj(t +1) = Hj(y(t)), j G Jt\n(3.4)\n(3.5)\nseguindo a ordem proposta em It e Jt. Em outras palavras, na iteracao t, atualizamos uma ilnica componente de cada vez nos vetores y (t) e x(t + 1) seguindo uma seq\u00fcencia aleat\u00f3ria de \u00edndices ate atualizarmos todas as componentes. Na iteracao seguinte, repetimos o processo escolhendo seq\u00fcencias diferentes de \u00edndices It e Jt. Note que as atualizacao ass\u00edncrona adicionam incerteza na seq\u00fcencia entre o padrao-chave e o padr\u00e2o recordado. Por esta razao, podemos dizer que uma mem\u00f3ria associativa dinamica com atualizacao ass\u00edncrona representa um modelo estocastico.\nO modo de atualizacao das componentes pode afetar drasticamente a fase de recordacao de uma memoria associativa dinamica. Por exemplo, a memoria associativa de Hopfield com atualizacao ass\u00edncrona sempre converge para um ponto fixo se certas condicoes forem satisfeitas. Por outro lado, a mesma memoria associativa com atualizacao sincronizada pode ter uma regiao de indecisao nao vazia. Nesta dissertacao de mestrado, embora usamos com frequencia uma notacao vetorial semelhante as equacoes (3.1) e (3.2) para descrever uma memoria associativa dinamica, consideramos apenas atualizacao ass\u00edncrona.\nO mapeamento associativo G de uma memoria associativa dinamica e definido como segue. Dado um padrao-chave x, tome x(0) = x e compute a seq\u00fcencia finita {(x(0), y(0)),..., (x(tf), y(tf))}, onde tf e, ouo menor t tal que (x(t), y (t)) e um ponto estacionario, ou o ni\u00edmero maximo de iteracoes permitidas. O padrao recordado pela memoria associativa apos a fase de recordacao e dado pela seguinte equacao\ny = G(x) := y(tf).\t(3.6)\nNote que o mapeamento associativo G inclui a dinamica da memoria associativa e considera, indiretamente, o modo de atualizacao das componentes.\n3.3\tCaracter\u00edsticas para um Bom Desempenho\nE de nosso interesse caracterizar o desempenho de uma memoria associativa. Um conjunto de caracter\u00edsticas desejaveis para uma classe de memorias associativas encontra-se em [31, 32, 64].\nUma memoria associativa de baixa performance e aquela incapaz de armazenar todas as memorias fundamentais ou com baixa tolerancia a ru\u00eddo. Ela possui um grande ni\u00edmero de memorias espurias, e estas possuem grandes regioes de recordacao. No caso das memorias associativas dinamicas, uma baixa performance tambem pode ser caracterizada pela presenca de oscilacoes, onde um estado inicial proximo a uma memoria armazenada tem grande probabilidade de convergir para uma memoria espuria ou para um ciclo limite.\nUma memoria associativa dinamica ideal e aquela que possui grande tolerancia a ru\u00eddo, um numero relativamente pequeno de memorias espurias, e cada memoria espuria possui uma pequena regiao de recordacao. No caso das memorias associativas dinamicas, ela deve ser uma memoria associativa estavel no sentido de nao possuir oscilacoes e possuir uma convergencia r\u00e1pida para quaisquer padroes-chaves apresentados a rede. Resumindo, diremos que uma memoria associativa possui um bom desempenho quando possuir as seguintes caracter\u00ed\u00edsticas:\n1.\tGrande capacidade de armazenamento,\n2.\tTolerancia a ru\u00eddo ou entradas incompletas,\n3.\tExistencia de poucas memorias espurias,\n4.\tO armazenamento da informacao deve ser distribu\u00eddo e robusto.\n5.\tRecordacao r\u00e1pida e baixo custo computacional.\nUtilizaremos estas caracter\u00edsticas no cap\u00edtulo 7 para comparar os varios modelos de memorias associativas neurais apresentados nesta dissertacao.\nCap\u00edtulo 4\nMemorias Associativas Lineares\nNeste cap\u00edtulo apresentamos as memorias associativas lineares que foram introduzidas em 1972 independentemente por Anderson, Kohonen e Nakano [4, 47, 60]. Numa mem\u00f3ria associativa linear (Linear Associative Memory, LAM) criamos um mapeamento linear G : Rn \u2014> Rm entre a entrada e a sa\u00edda. Neste caso, o mapemento G pode ser representado por uma matriz, digamos W E Rmxn, e dado um padr\u00e2o-chave x E Rn, encontramos o padrao recordado y E Rm atraves da equacao\ny = Wx.\t(4.1)\nUma mem\u00f3ria associativa linear e descrita por uma rede neural classica progressiva de camada unica (veja Figura 2.3 no Cap\u00edtulo 2) com a funcao identidade como funcao de ativacao. Discutimos dois procedimentos diferentes utilizados para obter a matriz W da equacao (4.1). Precisamente, o armazenamento por correlacao (Correlation Recipe) e o armazenamento por projecao (Projection Recipe) [31, 32]. Estes dois procedimentos definem a base para o aprendizado das mem\u00f3rias associativas discutidas nos pr\u00f3ximos cap\u00edtulos.\n4.1\tArmazenamento por Correla\u00e7\u00e3o\nO armazenamento por correla\u00e7\u00e3o (Correlation Recording), tambem conhecido como aprendizado de Hebb, e um dos procedimentos mais usados para obter a matriz dos pesos sin\u00e3pticos W e est\u00e3 baseado no postulado de aprendizagem de Hebb [34]. O postulado de Hebb afirma que se um neur\u00f4nio A e ativado por um neur\u00f4nio B repetidas vezes, ent\u00e3o o neur\u00f4nio A se tornar\u00e3 mais sens\u00edvel aos est\u00edmulos do neur\u00f4nio B e a conex\u00e3o sin\u00e3ptica entre A e B ser\u00e3 aumentada [21, 33]. Em resumo, o peso sin\u00e3ptico wij sofrer\u00e3 uma variac\u00e3o dada pela correlac\u00e3o entre a entrada Xj e a sa\u00edda yi. Se temos um conjunto finito de pares {(x^, y^) : \u00a3 = 1,..., k} a ser armazenado numa mem\u00f4ria associativa linear, o armazenamento por correlac\u00e3o fornecer\u00e3 uma matriz W G Rmxn onde\nk\nwij = 52 yi xj \u25a0\t(4.2)\n\u00ed=i\nUsando uma notac\u00e3o matricial temos\nk\nW = YXt = 52 y\u00ab (x\u00ab )T,\t(4.3)\n\u00ed=i\nFig. 4.1: Padr\u00f5es recordados pela mem\u00f3ria auto-associativa linear com armazenamento por correla\u00e7\u00e3o quando usamos como entrada as chaves fundamentais x1,..., x5.\nonde X = [x1, x2,..., xk] G Rnxk e a matriz obtida tomando as chaves fundamentais como coluna e Y = [y1, y2,..., yk] G Rmxk e obtida concatenando as recordacoes fundamentais.\nA mem\u00f3ria associativa linear com armazenamento por correlacao pode ser facilmente determinada, mas possui serias limitacoes. Por exemplo, substituindo (4.3) em (4.1) e assumindo que xh e uma chave fundamental, encontramos a seguinte expressao para o padr\u00e2o recordado yh:\nr k\nyh\n\u00a3y4 (x4 )T\nL 4=1\nxh\nk\nx'\u2018||2 yh +\ty4 (x4 )T xh.\n4=h\n(4.4)\nO segundo termo do lado direito de (4.4) \u00e9 um vetor ru\u00eddo e surge devido \u00e0 interfer\u00eancia cruzada (cross-talk) entre o padr\u00e0o xh e as demais chaves fundamentais. Note que este termo ser\u00e0 zero se os vetores x1, x2,..., xk forem ortogonais. O primeiro termo do lado direito de (4.4) e proporcional a recordacao fundamental yh, com constante de proporcionalidade ||xh||2. Para evitar esta constante, podemos impor ||x^||2 = 1 para \u00a3 = 1, 2,... , k. Assim, uma condicao suficiente para recordar uma mem\u00f3ria perfeitamente e ter um conjunto de vetores {x1, x2,..., xk} ortonormal. Dificilmente teremos uma recordacao perfeita das mem\u00f3rias fundamentais se os padr\u00f5es x1, x2,..., xk nao forem ortonormais. Note que nao impormos condicoes sobre as recordacoes fundamentais, portanto, os vetores coluna y1, y2,..., yk podem ser quaisquer.\nExemplo 4.1.1. Considere as imagens com 256 tons de cinza apresentadas na figura 1.3. Estas imagens foram convertidas em padr\u00f5es (vetores coluna) x1,..., x4 G [0,1]4096 e armazenadas na mem\u00f3ria auto-associativa linear com armazenamento por correlacao. Usando as chaves fundamentais como entrada, encontramos como resposta os padr\u00f5es apresentados na figura 4.1, respectivamente. Neste exemplo percebemos claramente o efeito da interferencia curzada discutido anteriormente. O erro quadr\u00e1tico medio normalizado (EQMN) calculado atraves da equacao\n1 k\nE ' =\n4=1\n||Wx4 - y4\nx4\n(4.5)\nfoi aproximadamente 3100.\nExemplo 4.1.2. A memoria associativa linear com armazenamento por correlacao tambem pode ser usada para armazenar padr\u00f5es bipolares. Considere as chaves fundamentais x1, x2,..., x5 G\nFig. 4.2: Padr\u00f5es recordados pela mem\u00f3ria associativa linear com armazenamento por correla\u00e7\u00e3o quando usamos como entrada as chaves fundamentais x1,..., x5.\n{\u20141,1}4096 apresentados na figura 1.1 e as recordacoes fundamentais y\\ y2,..., y5 E {-1,1}4096 apresentados na figura 1.2. Armazenamos estes cinco pares de padr\u00f5es na mem\u00f3ria associativa linear usando o armazenamento por correlacao. Apresentando as chaves fundamentais x1,..., x5 como entrada, encontramos os padr\u00f5es recordados apresentados na figura 4.2, respectivamente. Percebemos novamente o efeito da interferencia curzada neste exemplo. Note que, embora as recordacoes fundamentais sejam padr\u00f5es bipolares, os padr\u00f5es recordados nao sao padr\u00f5es bipolares. De fato, Wx1, Wx2,..., Wx5 E [\u201414354,14354]4096. O erro quadr\u00e1tico medio normalizado (EQMN) foi aproximadamente 10471, um valor grande pois alem da interferencia cruzada temos o valor ||x||2 = 4096 multiplicando o vetor coluna y^ na equacao (4.4).\nO armazenamento por correlacao parece nao ser muito eficiente, visto que dificilmente armazenara o conjunto das mem\u00f3rias fundamentais se existir uma chave fundamental que nao e ortogonal as demais. Todavia, ele possui grandes vantagens. A primeira delas esta na motivacao biol\u00f3gica do postulado de Hebb. A segunda vantagem esta no custo computacional realizado para encontrar a matriz W, pois neste armazenamento realizamos (2k \u2014 1)mn operac\u00f5es1.\n4.1.1\tArmazenamento por Correla\u00e7\u00e3o Auto-associativo Bipolar\nNo caso auto-associativo bipolar, os elementos da diagonal de W s\u00e3o w\u00fc = |=1 (xf) . Estes elementos s\u00e3o sempre positivos e seus valores aumentam indefinidamente quando adicionamos novos padr\u00f5es. Deste modo, quando armazenamos muitos padr\u00f5es, encontramos uma matriz cujos elementos da diagonal sao muito maiores que os demais elementos e se fizessemos uma normalizacao dos elementos da matriz, encontrar\u00edamos uma matriz parecida com uma matriz diagonal. Entretanto, uma matriz diagonal nao possui capacidade de correcao de erro e nao teremos uma mem\u00f3ria associativa eficiente. Este e um problema comum no aprendizado de Hebb e pode ser resolvido impondo w\u00fc = 0. Quando impomos wif = 0, encontramos:\n|P=1 4 xj,\nwij =\nse i = j, se i = j,\npara 1 &lt;i, j &lt;n.\n(4.6)\nUsando uma notacao matricial temos\nW = XXT \u2014 kI,\n(4.7)\nxNeste trabalho +, -, x e representam uma opera\u00e7ao (1 flop).\nonde I e a matriz identidade n x n e k e o n\u00famero de padr\u00f4es armazenados na memoria associativa linear. A regra de aprendizado descrita pelas equacoes 4.6 e 4.7 e conhecido como armazenamento por correlacao com diagonal nula ou aprendizado de Hebb sem auto-conex\u00e3o (ou auto-realimentacao). Este aprendizado sera usado nas memorias auto-associativas dinamicas apresentadas nos cap\u00edtulo 5.\n4.2\tArmazenamento por Proje\u00e7\u00e3o\nO armazenamento por proje\u00e7\u00e3o (Projection Recording) foi proposto por Kohonen e Ruohonen [50] e tem como objetivo resolver o problema\nk\nmin ||y - WXHF = minJ2 lly\u20ac - Wx\u20acIl2>\t(4.8)\ne=i\nonde H \u2022 Hf representa a norma de Frobenius2 e || \u2022 ||2 representa a norma Euclidiana (para vetores). Uma mem\u00f3ria associativa linear treinada usando o armazenamento por projecao e chamada memoria associativa linear \u00f3tima (Optimal Linear Associative Memory, OLAM) pois a matriz dos pesos sinapticos W G Rmxn e a matriz que minimiza o erro entre as recordacoes fundamentais y e os padr\u00f5es recordados W. A solucao de (4.8) e\nW = YX1,\t(4.9)\nonde X1 e a pseudo-inversa (ou inversa generalizada de Moore-Penrose) de X [25, 101]. A matriz W e a matriz de projecao no espaco gerado pelas recordacoes fundamentais y, por isso chamamos este aprendizado de armazenamento por proje\u00e7ao.\nSe o conjunto {x2, \u00a3 = 1, 2,..., k} e linearmente independente, i.e., se X e uma matriz de posto completo, entao a pseudo-inversa de X e dada por\n\tXt = (XTX 1 XT,\t(4.10)\ne\tW = Y (X TX )-1 XT.\t(4.11)\nSe os padr\u00f5es \u00a1 x2, \u00a3 = 1,..\t., k} forem ortonormais, entao XTX\t= I e W = YXT e a matriz\nencontrada usando o armazenamento por correlacao. Logo, o armazenamento por correlacao e um caso particular do armazenamento por projecao se os padr\u00f5es armazenados forem ortonormais.\nExiste uma versao iterativa do procedimento de armazenamento por projecao baseada no teorema de Greville que nao sera discutida aqui mas pode ser encontrada em [49]. Esta versao iterativa pode ser usada para adicionar uma nova associacao na mem\u00f3ria associativa linear \u00f3tima.\nNo caso auto-associativo, Y = X, temos W2 = W (matriz de projecao ) e W = WT (matriz sime\u00edtrica). Estes fatos podem ser obtidos diretamente das propriedades da matriz pseudo-inversa [53, 48, 101]. Note que W = I e solucao do problema minW ||X \u2014 WX \\\\F independente da matriz X. Logo podemos armazenar um m\u00edmero ilimitado de padr\u00f5es na OLAM no caso auto-associativo. No caso heter\u00f3-associativ\u00f3, minW ||Y \u2014 WX||F pode ser maior que zero e nao garantimos sucesso\n2Tambem referida como norma Euclidiana para matrizes.\nna fase de armazenamento. Se X possui posto completo, pela equa\u00e7\u00e3o (4.11) conclu\u00edmos que a matriz W do armazenamento por proje\u00e7ao e tal que WX = Y. Por outro lado, se k > n, entao certamente teremos um conjunto linearmente dependente, X provavelmente nao tera posto completo e dificilmente conseguiremos armazenar o conjunto das mem\u00f3rias fundamentais na OLAM.\nExemplo 4.2.1. Considere o caso auto-associativo onde armazenamos os padr\u00f5es em tons de cinza apresentados na figura 1.3. Vimos no exemplo 4.1 que a mem\u00f3ria associativa linear com armazenamento por correlacao nao e capaz de armazenar o conjunto das mem\u00f3rias fundamentais. Armazenamos estas mem\u00f3rias fundamentais na OLAM e encontramos como sa\u00edda as recordac\u00f3es fundamentais ap\u00f3s apresentar as respectivas chaves fundamentais como entrada. De fato, todas as mem\u00f3rias fundamentais foram armazenadas com sucesso posto que o conjunto {x1,..., x4} e linearmente independente. O EQMN deste experimento foi 2, 7 x 10-15 por causa de erros numericos.\nExemplo 4.2.2. Vamos considerar o caso hetero-associative bipolar. Considere os padr\u00f5es de entrada apresentados na figura 1.1 e os padr\u00f5es de sa\u00edda apresentados na figura 1.2. Primeiramente, notamos que o posto da matriz X e 5. Portanto, a OLAM deve ser capaz de armazenar o conjuto de mem\u00f3rias fundamentais {(xf, yf) : \u00a3 = 1,..., 5}. De fato, armazenando este conjunto e apresentando os padr\u00f5es x1,..., x5 como entrada, encontramos um EQMN de 5, 6 x 10-14. Novamente, o valor encontrado para EQMN e diferente de zero devido a erros de arredondamento.\nTeorema 4.2.1. Sejam X G Rnxk e Y G Rmxk as matrizes concatenando as chaves e recorda\u00e7\u00f5es fundamentais, respectivamente. Considere a decomposicao em valores singulares (SVD)\nr\nX = U= ^2 a Uj\t.\t(4.12)\nj=1\nSeja N = [n1, n2,..., nk] G Rnxk uma matriz gerada aleatoriamente com distribucao gaussiana com m\u00e9dia zero e variancia3 a2, isto \u00e9, E(nf) = 0 e\n4\"2 se 1 = j\u2019\t(4.13)\nI 0\tcaso contr\u00e1rio,\nonde E (X) representa a esperanca da vari\u00e1vel aleatoria X .Se W = YX t \u00e9 a matriz das conexoes sinapticas da OLAM, entao o erro quadr\u00e1tico medio (Mean Square Error, MSE) total das recordacoes da memoria associativa sera\nMSE = E (||Y - W(X + N)||F)\t(4.14)\n= E IIY vy |I2 + ka2 \u00bf \"?'2 ' \u2019\t(4.|5)\nj=r+1\tj=1\tj\nonde vi,..., vk sao os vetores singulares da direita e a1,..., ar sao os valores singulares de X.\nCom base neste teorema conclu\u00edmos que:\n3Note que usamos aj, com \u00edndice, para representar os valores singulares e a2, sem \u00edndice, para representar a vari\u00e2ncia\nde nf.\nE (nf nf)\n1.\tO primeiro termo de (4.15) surge devido a depend\u00eancia linear das chaves fundamentais. Se os vetores x1, x2,..., xk forem linearmente independentes, entao o posto r da matriz X sera k e o primeiro termo de (4.15) sera zero. Se os padr\u00f5es-chave forem linearmente dependente, o erro do primeiro termo sera inevitavel e podemos chamar este termo de erro da depend\u00eancia linear.\n2.\tO segundo termo de (4.15) e obtido devido ao ru\u00eddo nos padr\u00f5es de entrada e podemos chamar este termo de erro do ru\u00eddo. Quando apresentamos como entrada um padr\u00e2o-chave sem ru\u00eddo, entao a2 = 0 e este termo tambem sera zero. Por outro lado, se fornecermos uma entrada ruidosa, necessariamente teremos a interfer\u00eancia do erro do ru\u00eddo na recordacao. Note que, no somat\u00f3rio do erro do ru\u00eddo, se um dos valores singulares for muito pequeno, entao ka\\\\Yvj ||2/aj >> 0 e o erro de recordacao sera grande. Logo, quando uma entrada ruidosa e apresentada a mem\u00f3ria associativa, o erro de recordacao pode ser descrito pelos valores singulares a de X. Observe tambem que, se X tem posto completo, entao r = k e quanto mais elementos sao armazenados, maior sera o erro devido ao termo do ru\u00eddo.\nCorol\u00e1rio. O erro quadratico medio total das recordacoes da OLAM no caso auto-associativo e\nMSE = ka2 r.\n(4.16)\nNote que o MSE da mem\u00f3ria associativa linear \u00f3tima no caso auto-associativo n\u00e3o possui o termo devido ao erro da dependencia linear.\nSabemos que a media da soma total do ru\u00eddo e\n(L\"'\")\nE\n= ka2 n.\n(4.17)\nCom base nas equacoes (4.16) e (4.17), conclu\u00edmos que a medida de correcao de erro desta mem\u00f3ria\nassociativa e:\nka2r\nka2n\nr\nn\n(4.18)\nLogo, se r &lt;n, entao a mem\u00f3ria auto-associativa linear com armazenamento por projecao reduz o ru\u00eddo da entrada. O pior caso ocorre quando r = n onde o ru\u00eddo nao diminui. Note tambem que, quanto menor for r (ou k, pois r &lt;k), melhor sera a correcao de erro esta mem\u00f3ria. Entretanto, nao temos uma restricao quanto ao m\u00edmero de padr\u00f5es armazenados. Podemos armazenar um numero de padr\u00f5es maior que a dimensao dos padr\u00f5es de entrada, isto e, k > n, mas perdemos com isso a capacidade de correcao de erro, pois a matriz W tende para a matriz identidade.\nO teorema 4.2.1 e o corolario deste teorema podem ser encontrados no artigo de Murakami e Aibara [59]. Outros resultados sobre a capacidade de correcao de erro da mem\u00f3ria associativa linear \u00f3tima (usando armazenamento por projecao) foram apresentados por Kohonen [49], Stiles e Denq [89] e Casasent e Telfer [14], entre outros.\nExemplo 4.2.3. Considere os padr\u00f5es em tons de cinza apresentados na figura 1.3. Pelo teorema 4.2.1 e pelo exemplo 4.2.1, sabemos que a mem\u00f3ria auto-associative linear \u00f3tima e capaz de armazenar os padr\u00f5es x1,..., x4. Vamos verificar agora a tolerancia a ru\u00eddo deste modelo. Na figura 4.3 apresentamos vers\u00f5es ruidosas x1,..., x4 das mem\u00f3rias fundamentais geradas com distribuyo gaussiana\nFig. 4.3: Memorias-chave apresentada a OLAM no exemplo 4.2.3.\nFig. 4.4: Padroes recordados pela OLAM no exemplo 4.2.3 quando apresentamos as memorias-chave apresentadas na figura 4.3.\ncom media zero e variancia 0,1. Na figura 4.4 apresentamos os padr\u00f5es recordados apios apresentar os padroes da figura 4.3 como entrada. O erro quadr\u00e1tico medio normalizado calculado ap\u00f5s 1000 simulacoes foi aproximadamente 0, 08.\nExemplo 4.2.4. Considere os padroes bipolares x1,..., x5 E {\u20141, +1}4096 e y1,..., y5 E {\u20141, +1}4096 apresentados nas figuras 1.1 e 1.2, respectivamente. Vimos no exemplo 4.2.2 que a OLAM e capaz de armazenar este conjunto de memorias fundamentais. Armazenamos estes padr\u00f5es na OLAM e usamos como entrada os padr\u00f5es x1,..., x5 apresentados na figura 4.5, respectivamente. Estes padr\u00f5es ruidosos foram gerados a partir das chaves fundamentais revertendo o valor de um pixel seguindo uma distribuicao uniforme com probabilidade 0, 3. Na figura 4.6 apresentamos os respectivos padr\u00f5es recordados. O erro quadr\u00e1tico medio normalizado (EQMN) foi aproximadamente 0, 6. Note que os padr\u00f5es recordados pela OLAM foram imagens em tons de cinza [\u20141, +1]4096 com ru\u00eddo vindo de outras recordac\u00f5es fundamentais mas dominado pelo sa\u00edda desejada (erro do ru\u00eddo). Encontramos as memorias fundamentais y1,..., y5 e um EQMN igual a zero aplicando um threshold com corte no n\u00edvel de cinza fornecido pelo me\u00edtodo de Otsu [63].\nFig. 4.5: Vers\u00f3es corrompidas dos padr\u00f3es x1,..., x5 da figura 1.1. Estes padr\u00f3es foram gerados introduzindo ru\u00eddo uniforme com probabilidade 0, 3 de reverter o valor de um pixel.\nFig. 4.6: Padr\u00f3es recordados pela OLAM no exemplo 4.2.4 quando apresentamos as mem\u00f3rias-chave apresentadas na figura 4.5.\nCap\u00edtulo 5\nMemorias Associativas Dinamicas\nNeste cap\u00edtulo apresentamos os principais modelos de mem\u00f3rias associativas din\u00e2micas para padr\u00f5es bipolares e introduzimos a mem\u00f3ria associativa bidirecional com capacidade exponencial. Para cada modelo fornecemos uma breve introducao, especificamos a arquitetura da rede neural e a regra de aprendizado, apresentamos uma breve analise sobre a convergencia e exemplos computacionais.\n5.1\tMem\u00f3ria Associativa de Hopfield Discreta\nA mem\u00f3ria associativa de Hopfield foi introduzida em 1982 pelo f\u00edsico J.J. Hopfield e \u00e9 o modelo de mem\u00f3ria associativa neural mais conhecido e estudado na literatura [40, 31, 33]. Este modelo forma a base para os demais modelos de mem\u00f3rias associativas discutidas neste cap\u00edtulo.\n5.1.1\tArquitetura da Rede\nA mem\u00f3ria associativa de Hopfield e descrita por uma rede neural classica recorrente de camada ilnica totalmente conexa com funcao sinal como funcao de ativacao [40]. A arquitetura da rede de Hopfield esta apresentada na figura 2.6. O modelo de Hopfield e descrito pela equacao\nxi (t + 1) = sinal\nwij xj (t)j\n= sinal (wTx(t) ,\npara i E In e t = 0,1,...\n(5.1)\nonde wT \u00e9 a i-\u00e9sima linha da matriz W, x(t) \u00e9 a entrada da rede no tempo t e In \u00e9 uma permuta\u00e7\u00e3o do conjunto de \u00edndices {1, 2,..., n}. A fun\u00e7ao sinal usada nas mem\u00f3rias associativas dinami\u00e7as e definida como segue:\nI +1 sinal (wTx(t) = &lt;xi(t)\n1\nse wTx(t) > 0,\nse wT x(t) = 0,\t(5.2)\nse wfx(t) &lt;0.\nPodemos descrever a rede de Hopfield simplificadamente atraves da equacao\nx(t + 1) = sinal(Wx(t)), t = 0,1,...\t(5.3)\nNeste caso, devemos lembrar que a atualizacao das componentes e feita no modo ass\u00edncrono.\nPode-se usar atualizacao sincronizada na rede de Hopfield. Entretanto, a rede com atualizacao sincronizada pode apresentar ciclo limite, isto e, uma regiao de indecisao nao nula [3]. Lembre-se que o tipo de atualizacao altera a trajet\u00f3ria dos pontos x(t), mas nao altera os pontos fixos da memoria associativa dinamica.\n5.1.2\tAprendizado\nNa memoria associativa de Hopfield usamos o armazenamento por correlacao com diagonal nula. Neste caso, a matriz dos pesos sinapticos e computada atrav\u00e9s da equacao 4.6, ou pela equacao 4.7. Note que a matriz dos pesos sinapticos W e simetrica (W = WT) com diagonal nula (wi\u00bf = 0, Vi = l,...,n).\nRepare na semelhanca entre a memoria associativa de Hopfield e a memoria associativa linear com armazenamento por correlacao discutida no cap\u00edtulo 4. A diferenca entre estas duas memorias associativas esta na existencia da funcao sinal, que forca a sa\u00edda a ser +1 ou \u2014 1, e na recursividade presentes na rede de Hopfield. A seguir apresentaremos as propriedades da memoria associativa de Hopfield e veremos como esta simples mudanca (funcao sinal + recursividade) produz melhoras significativas no desempenho da memoria associativa.\n5.1.3\tConvergencia\nO seguinte teorema, introduzido por Hopfield em [40], garante a convergencia da memoria associativa dinamica descrita pelas equacao 5.1 (ou 5.3). Em outras palavras, o seguinte teorema garante que a regiao de indecisao da mem\u00f3ria associativa de Hopfield com atualizacao ass\u00edncrona e vazia. Outros resultados sobre a convergencia desta memoria associativa dinamica podem ser encontrados em [9].\nTeorema 5.1.1 (Teorema da Convergencia de Hopfield). Seja W uma matriz sim\u00e9trica (W = WT) com diagonal nao negativa. (wn > 0, i = 1,..., n). A mem\u00f3ria associativa dinamica descrita pela equacao 5.3 com atualizado ass\u00edncrona converge para um ponto fixo e minimiza a funcao energia\nr> r>\n(5.4)\nO seguinte teorema fornece uma estimativa para o numero maximo de memorias fundamentais que podem ser armazenadas na memoria associativa de Hopfield. Este teorema foi introduzido por McEliece et. al. em [56]. Uma demonstracao simplificada pode ser encontrada em [3].\nTeorema 5.1.2 (Teorema de McEliece et. al.). Sejam x1,..., xk e {\u20141,1}n, com n suficientemente grande, padroes nao-correlacionados gerados aleatoriamente com distribuicao uniforme.\n1.\tUma memoria fundamental x; ter\u00e1 grande probabilidade de ser um ponto fixo da memoria associativa de Hopfield se\nk &lt;n/(2 logn).\n(5.5)\n2.\tTodos os padr\u00f5es x1,..., xk ser\u00e3o pontos fixos da mem\u00f3ria associativa de Hopfield com grande probabilidade se\nk &lt;n/(4logn).\t(5.6)\nO teorema 5.1.2 est\u00e1 baseado na convergencia forte em probabilidade. Neste caso, dada a sequencia de variaveis aleat\u00f3rias X1, X2,... e a variavel aleat\u00f3ria X, dizemos que Xn \u25a0 X quando n \u25a0 x com grande probabilidade se Pr(Xn \u25a0 X quando n \u25a0 1) = 1 [43].\nExemplo 5.1.1. Considere uma rede de Hopfield com 100 neur\u00f4nios. Geramos k padr\u00f5es bipolares aleatoriamente com distribuicao uniforme e armazenamos todos eles na memoria associativa de Hopfield. Depois verificamos se o primeiro padrao e um ponto fixo e se todos os padr\u00f5es sao pontos fixos. Repetimos o experimento 1000 vezes para diferentes valores de k (nilmero de mem\u00f3rias fundamentais) e calculamos a probabilidade emp\u00edrica do primeiro e de todas as recordacoes fundamentais serem pontos fixos da memoria associativa de Hopfield. Na figura 5.1 apresentamos com linha marcada com o a probabilidade emp\u00edrica de uma certa memoria fundamental (no nosso caso x1) ser um ponto fixo e com linha marcada com \u25a1 a probabilidade emp\u00edrica de todas as memorias fundamentas serem pontos fixos. A linha tracejada representa uma estimativa teorica para a capacidade de armazenamento obtida usando a equacao 7.12 que sera introduzida no cap\u00edtulo 7. No eixo horizontal colocamos k, o nilmero de memorias fundamentais. As linhas pontilhadas verticais indicam os valores n/(2 log n) e n/(4 log n). Note que a probabilidade emp\u00edrica de todos os padr\u00f5es serem pontos fixos deixa de ser 1 quando k > n/(n log n). A probabilidade emp\u00edrica de todas as mem\u00f3rias fundamentais serem pontos fixos para k = n/(2 log(n)) foi menor que 0, 8. Entretanto, a probabilidade emp\u00edrica de uma certa memoria fundamental ser ponto fixo foi muito proxima de 1. Estes resultados numericos conferem com o teorema 5.1.\nNote que a matriz dos pesos sinpaticos W = XXT fornecida pelo armazenamento por correlacao com auto-alimentacao tambem satisfaz as condiciOes do teorema 5.1.1. Logo, este procedimento tambem pode ser usado para treinar a memoria associativa descrita pela equacao 5.3. O comportamento desta nova memoria associativa dinamica sera muito parecido com o modelo com diagonal nula porque nao podemos armazenar muitos padroes e, com poucos padroes armazenados, nao temos uma matriz W com elementos na diagonal muito maior, em modulo, que os demais (veja secao 4.1.1 sobre armazenamento por correlacao auto-associativo).\nExemplo 5.1.2. Considere os padroes bipolares apresentados na figura 1.1. Armazenamos estes padroes na memoria associativa de Hopfield usando o armazenamento por correlacao (com diagonal nula) e depois apresentamos as chaves fundamentais como entrada. A memoria associativa de Hopfield encontrou os pontos fixos com no maximo 2 iteracoes. Na figura 5.2 apresentamos passo a passo os padroes recordados pela memoria associativa de Hopfield ate o final da segunda iteracao. Note que k = 5 e muito menor que o valor fornecido pelo teorema 5.1.2, (4096/(4 log(4096)) = 283, 5). Entretanto, nenhuma das memorias fundamentais e um ponto fixo. Isso acontece porque os padroes x1,..., x5 possuem 2173 componentes em comum (no background) e uma correlacao media (1/20)\t&lt;x\u20ac, xn >= 2344. Um resultado similar sera obtido pela memoria associ-\nativa dinamica descrita pela equacao 5.3 treinana com o armazenamento por correlacao com auto-alimentacao.\nFig. 5.1: Probabilidade das mem\u00f3rias fundamentais serem pontos fixos na memoria associativa de Hopfield por k. A linha marcada com o representa a probabilidade de uma dada memoria fundamental ser ponto fixo e a linha marcada com \u25a1 representa a probabilidade de todas as memorias fundamentais serem pontos fixos. A linha tracejada representa a capacidade de armazenamento que sera\u00ed discutida no cap\u00edtulo 7. As linhas pontilhadas verticais representam os valores n/(2 log n) e n/(4 log n).\nPadroes (1) = sinal(W(0)), \u00a3 = 1,..., 5 obtidos no final da primeira iteracao.\n* i i- \u25a0.\t* i i- \u25a0.\t* i i- \u25a0.\nPontos fixos x^(2) = sinal(Wx^(1)), \u00a3 = 1,..., 5 obtidos no final da segunda iteracao.\nFig. 5.2: Padroes recordados pela memoria associativa de Hopfield no exemplo 5.1.2 quando apresentamos as chaves fundamentais como entrada.\nW (Wi)\tsinal(-)\nWT (W2)\tsinal(-)\nxi (t)\nx2 (t)\nx3 (t)\nx\u201e(t)\n0~yi(t)^ 0\n0~ y2(t^ 0\n0 \u2014y\u00ab(t)^ 0\n0\n0\n0\n0\nxi (t + 1)\nx2 (t + 1)\nx3 (t + 1)\nx\u201e(t + 1)\nFig. 5.3: Arquitetura da Mem\u00f3ria Associativa Bidirecional (Assimetrica).\n5.2\tMemori\u00e3 Associativa Bidirecional\nA Mem\u00f3ria Associativa Bidirecional (Bidirectional Associative Memory, BAM), proposta por Kosko em 1987, e uma generalizacao da mem\u00f3ria auto-associativa de Hopfield discreta com armazenamento por correlacao com auto-alimentacao [51, 52].\n5.2.1\tArquitetura\nA BAM e\u00ed descrita por uma rede neural cla\u00edssica recorrente totalmente conexa com duas camadas e funcao sinal como funcao de ativacao, como apresentado na figura 5.3. A matriz dos pesos sinapticos da segunda camada da BAM e a transposta da matriz dos pesos sinapticos da primeira cada. A BAM e descrita pelo par de equac\u00f3es\ny(t) = sinal (Wx(t)),\t(5.7)\nx(t +1) = sinal (WTy(t)) , para t = 0,1, 2,...\t(5.8)\ncom atualizacao ass\u00edncrona.\n5.2.2\tAprendizado\nAs matrizes dos pesos sinapticos da BAM sao obtidas usando o armazenamento por correlacao (aprendizado de Hebb). A matriz dos pesos sinapticos da primeira camada e W = YXT ea matriz da segunda camada e WT = XYT. Note que WT e a matriz das conex\u00f3es sinapticas da mem\u00f3ria associativa linear com armazenamento por correlacao com as mem\u00f3rias fundamentais {(y^, x^), \u00a3 = 1, 2,..., k}, onde y^ G {-1,1}m e a entrada e x^ G {-1,1}n e a sa\u00edda, para \u00a3 = 1,..., k. No caso auto-associativo, isto e, se y^ = x^, para \u00a3 = 1,..., k, teremos a mem\u00f3ria associativa de Hopfield.\n\n\nPadr\u00f5es (0) = sinal(W(0)), \u00a3 = 1,..., 5.\nPadr\u00f5es x^(1) = sinal(Wy^(0)), \u00a3 = 1,..., 5.\n-:\u00b1,-s\t-\u25a0\n\nPadr\u00f5es y^(1) = sinal(Wx^(1)), \u00a3 = 1,..., k.\nFig. 5.4: Padroes recordados pela BAM quando apresentamos as chaves fundamentais x1,..., x5 como entrada.\n5.2.3\tConverg\u00eancia\nA BAM pode ser convertida numa mem\u00f3ria auto-associativa discreta de Hopfield com vetores de estados xT = [xT, yT]T e matriz das conex\u00f5es sinapticas\nW T\n_0 WT\nW0\n(5.9)\nA matriz WT e sim\u00e9trica com diagonal nula. L\u00f5g\u00f5, pel\u00f5 teorema da convergencia de Hopfield, esta mem\u00f5ria associativa sempre converge para um ponto estacionario com atualizacao ass\u00edncrona. A convergencia da BAM, para ambos os casos, tambem pode ser verificada mostrando que a funcao energia\nEnergia(x, y) = \u20141 (xTWy + yTWTx) = \u2014xTWy,\t(5.10)\ne\u00ed minimizada.\nExemplo 5.2.1. Considere os padr\u00f5es x1,..., x5 G {\u20141,1}4096 e y1,..., y5 G {\u20141,1}4096 apresentados nas figuras 1.1 e 1.2. Armazenamos o conjunto das mem\u00f3rias fundamentais {(x^, y^), \u00a3 = 1,..., 5} e verificamos se as mem\u00f3rias fundamentais sao pontos estacionarios da BAM. Comecamos apresentando as chaves fundamentais x1,..., x5 como entrada e encontramos como resposta os padr\u00f5es apresentados na figura 5.4. Os padr\u00f5es x1 (2),..., x5(2) obtidos no final da segunda iteracao\nsao iguais aos padr\u00f3es x1 (1),..., x5(1) entrados no final da primeira iteracao. Neste exemplo a BAM convergiu com 2 iterac\u00f3es, entretanto nenhuma das mem\u00f3rias fundamentais e um ponto estacionario.\nA seguinte conjectura, introduzida por nos nesta dissertacao de mestrado, e uma extensao do teorema de McEliece et. al. para a BAM. Lembre-se que Xn \u25a0 X quando n \u25a0 x com grande probabilidade se P(Xn \u25a0 X quando n \u25a0 1) = 1 [43].\nConjectura 5.2.1. Sejam x1,..., xk G {-1,1}n e y1,..., yk G {-1,1}m, n e m suficientemente grandes, padr\u00f5es nao-correlacionados gerados aleatoriamente com distribuyo uniforme.\n1. Uma mem\u00f3ria fundamental (x^,\t) tera uma grande probabilidade de ser um ponto esta-\ncionario da BAM se\n1 . ( n&lt;-mm -----\n2\tylogm\nm log n\n(5.11)\nk\n2. Todas as mem\u00f3rias fundamentais com grande probabilidade se\n(x1, y1),..., (xk, yk) serao pontos estacionarios da BAM\nk &lt;- min\n_ 4\nnm log m log n\n(5.12)\nAcreditamos que uma demostracao formal do resultado acima pode ser obtida fazendo as devidas modificacoes na demonstracao do teorema principal (The Big Theorem) apresentada em [56]. Nosso\nobjetivo nesta dissertacao e fazer um estudo comparativo em mem\u00f3rias associativas. Por esta razao apresentamos apenas um esboco da demonstracao desta conjectura. Uma demostracao formal requer detalhes que serao omitidos.\nSeja X = [x1,..., xk ] G { \u20141,1}nxk e Y = [y1,..., yk ] G { \u20141,1}mxk onde xf e y? sao variaveis aleat\u00f3rias independentes com probabilidade 1/2 de ser +1 ou \u20141 para todo j = 1,..., n, i = 1,..., m e \u00a3, n = 1,..., k. Sabemos que y? = sinal(wTxn) para n G {1,..., k} se e somente se y?wTxn > 0 para todo i = 1,..., m e n G {1,..., k}. A matriz dos pesos sinapticos da BAM e W = YXT e\nn k\nwTx\tyi\u25a0\u25a04.\nj=1 ?=1\n(5.13)\nLogo,\nk n\tk n\nyn (wTxn)\tJ^y/yfxfx? = n '\ty yfxfxn = n + r\u2019\t(544)\nf=i j=i\tf=n j=i\nonde r representa o ru\u00eddo.\nO termo do ru\u00eddo pode ser positivo ou negativo e sera\u00ed nosso objetivo estimar o seu valor. Para tanto, vamos tomar\nk\nzj\tl\u00ed yi7 yf xf xj-\t(545)\nf=n\nDeste modo, o termo do ru\u00eddo sera\nn\nr = \u00a3 zj \u2022\t(5.16)\nj=i\nSabemos que y?, yj, xj e xj sao variaveis aleat\u00f3rias independentes e identicamente distribu\u00eddas (i.i.d.), portanto y?yjxfxj = \u00b11 com a mesma probabilidade. Sendo assim, podemos seguir a demonstracao rigorosa de McEliece et. al. [56].\nApos aplicar o teorema central do limite, encontramos\nPr [yn = sinal (wTxn)] = 1\n1 + erf\n5\n(5.17)\npara n, k suficientemente grandes com k/n peque\u00f1o. A l'uiicao erf(x) \u00e9 afuncao erro dada por\n2 \u00ed'x 2 erf(x) =\te \u00fa dt.\t(5.18)\nVn Jo\nPara valores grandes no argumento, a funcao erro pode ser aproximada por\n1 2 erf(x) \u00ab 1-----=ex .\t(5.19)\nFinalmente,\nPr [yn = sinal (Wxn)] = Pr [y? = sinal (wTxn) , V i = 1,..., m]\nK1 -\texp (-2k)-\n(5.20)\n(5.21)\npois as componentes de X e Y sao variaveis aleat\u00f3rias i.i.d. Dizemos que f g se f (x)/g(x)\t1\nquando x x. Seguindo o resultado apresentado em [3], teremos yn = sinal (Wxn) com probabilidade proxima de 1 se k &lt;n/(2 log m). Analogamente, deveremos ter k &lt;m/(2 log n) para termos xn = sinal (WTyn) com probabilidade proxima de 1. Logo, a memoria fundamental (xn, yn) sera um ponto estacionario se\n\u00ed n m \\ ylog m\u2019 log n J\n1\n&lt;- min\n\u201c 2\nk\n(5.22)\nSe queremos y^ = sinal (Wx^), para todo \u00a3 = 1,..., k, entao devermos ter\nPr (Y = sinal (WX)) = [Pr (y^ = sinal (wTxn}}]mk .\t(5.23)\nEsta probabilidade sera proxima de 1 se k &lt;n/(4log m). Analogamente, Pr (X = sinal (WTY)) sera proxima de 1 se k &lt;m/(4log n). Finalmente, todos as memorias fundamentais serao pontos fixos da BAM com probabilidade proxima de 1 se\nk &lt;- min\n_ 4\n\u00ed n m \\ \\log m\u2019 logn J\n(5.24)\n\npara n, m, k suficientemente grandes.\nNote que a conjectura 5.2.1 coincide com o teorema 5.1 se m = n.\nFig. 5.5: Probabilidade das mem\u00f3rias fundamentais serem pontos fixos na BAM por k. A linha marcada com o representa a probabilidade emp\u00edrica de uma dada mem\u00f3ria fundamental ser ponto fixo e a linha marcada com \u25a1 representa a probabilidade emp\u00edrica de todas as mem\u00f3rias fundamentais serem pontos fixos. A linha tracejada representa a capacidade de armazenamento obtida usando a equacao 7.15 (probabilidade te\u00f3rica). As linhas pontilhadas verticais representam os valores min(m/ log(n), n/ log(m))/2 e min(m/ log(n), n/ log(m))/4.\nExemplo 5.2.2. Considere o conjunto {(x^, ), G {-1,1}100, G {-1,1}80, \u00a3 = 1,..., k} gerado aleatoriamente com distribui\u00e7\u00e3o uniforme. Armazenamos este conjunto de mem\u00f3rias fundamentais na BAM. Depois verificamos se a associa\u00e7ao (x1, y1) e um ponto estacionario e se todas as mem\u00f3rias fundamentais (x^, y^), \u00a3 = 1,..., k, sao pontos estacionarios. Repetimos o experimento 1000 vezes para diferentes valores de k (m\u00edmero de mem\u00f3rias fundamentais) e calculamos as probabilidades emp\u00edricas de termos pontos estacionarios. Na figura 5.5 apresentamos com linha marcada com o a probabilidade emp\u00edrica de uma certa mem\u00f3ria fundamental ser um ponto estacionario e com linha marcada com \u25a1 a probabilidade emp\u00edrica de todas as mem\u00f3rias fundamentas serem pontos estacionarios. A linha tracejada representa uma estimativa te\u00f3rica para esta probabilidade, que sera chamada capacidade de armazenamento no cap\u00edtulo 7. No eixo horizontal colocamos k, o numero de mem\u00f3rias fundamentais. As linhas pontilhadas verticais indicam os valores min(m/ log(n),n/ log(m))/2 e min(m/ log(n),n/ log(m))/4, respectivamente. Note que a probabilidade emp\u00edrica de todos os padr\u00f5es serem pontos fixos deixa de ser 1 quando k > min(m/ log(n),n/ log(m))/4. A probabilidade emp\u00edrica de todas as mem\u00f3rias fundamentais serem pontos fixos para k = 8, sendo min(m/ log(n),n/ log(m))/2 = 8, 7 foi 0, 83, entretanto, a probabilidade emp\u00edrica de uma certa mem\u00f3ria fundamental ser ponto fixo foi 0, 97. Estes resultados numericos conferem com a conjectura 5.2.1. Lembre-se que n = 100 e m = 80 neste exemplo.\n5.3\tMem\u00f3ria Associativa de Personnaz\nPersonnaz et. al. apresentaram uma variacao da mem\u00f3ria associativa de Hopfield que utiliza o armazenamento por projecao para obter a matriz dos pesos sinapticos [67]. Este modelo e referido na literatura simplesmente como \u201crede de Hopfield com armazenamento por projecao\u201d [3, 31], mas sera referido neste trabalho como Mem\u00f3ria Associativa de Personnaz (Personnaz Associative Memory, Personnaz AM) porque apresenta caracter\u00edticas diferentes do modelo classico de Hopfield introduzido na secao anterior e tambem porque caracterizamos uma rede neural pelo modelo dos neur\u00f4nios, arquitetura e aprendizado (veja Cap\u00edtulo 2). Logo, regras de aprendizado distintas produzirao redes neurais (ou mem\u00f3rias associativas) distintas.\n5.3.1\tArquitetura da Rede\nA mem\u00f3ria associativa de Personnaz e descrita por uma rede neural classica recorrente de camada unica totalmente conexa com a funcao sinal como funcao de ativacao. A arquitetura da mem\u00f3ria associativa de Personnaz esta apresentada na figura 2.6. Este modelo e descrito pela equacao 5.1 (ou equacao 5.3). A diferenca entre a mem\u00f3ria associativa de Personnaz e a mem\u00f3ria associativa de Hopfield esta\u00ed na regra de armazenamento.\n5.3.2\tAprendizado\nA matriz dos pesos sinapticos da mem\u00f3ria associativa de Personnaz e\nW = XXf,\t(5.25)\nonde X = [x1, x2,..., xk] G {-1,1}nxk e a matriz com as mem\u00f3rias fundamentais.\nProposi\u00e7\u00e3o 1. Todas as mem\u00f3rias fundamentais x1,..xk ser ao pontos fixos da mem\u00f3ria associativa de Personnaz.\nDemonstra\u00e7\u00e3o. O problema minW ||X \u2014 WX || = 0 sempre tem solucao (em particular W = I) e sinal(WX) = sinal(X) = X.\t\u25a1\nProposi\u00e7\u00e3o 2. Seja X G { \u2014 1,1}nxk. Se W = XXt entao W = WT e 0 &lt;wii &lt;1, para todo i = 1,... ,n.\nDemonstra\u00e7\u00e3o. Considere a decomposicao em valores singulares (decomposicao SVD)\nr\nX = UXVT\tViUxT,\t(5.26)\ni=1\nonde u1un e v1vn sao os vetores singulares da esquerda e da direita, respectivamente, ai sao os valores singulares e r e o posto da matriz X [101, 25]. Sabemos que\nr\nX1 = VSfUT =\ta-1 VjuT,\t(5.27)\nj=1\ne portanto,\nW = XX1 =\n\n-1\tTI\nvu I\nr r\nV V '' u v V u \u2022\ni=1 j = 1\n(5.28)\nOs vetores vb \u2022 \u2022 \u2022, vn sao ortogonais, logo vf Vj = \u00f4j e\nrr\nW = xx 1 = ^2 a\u00ed a-1ujuf = 52 uj\u2022\n(5.29)\nj=i\nj=1\nLogo, wh\tVj=1 u2j > 0, onde uj e a i-esima componente do vetor uj. Por outro lado, sabemos\nque a matriz U e ortogonal, logo I = UUT e\nn\tr\tn\tn\ni = 52\t= 52 x +52\t= Wii +52\t\u2022\t(5-30)\nj=1\tj=1\tj=r+1\tj=r+1\nLogo,\nn\n0 &lt;Wii = 1 - Y, uij &lt;1\t(5.31)\nj=r+1\nPela Equacao (5.29) conclu\u00edmos tambem que\n\tr WT = (\u00e8 uuT j=1\tTr 1 = \u00e8u uf=w, j=1\t(5.32)\nou seja, W = WT.\t\t\t\u25a1\n5.3.3 Converg\u00eancia\nA Proposicao 2 e o Teorema 5.1.1 mostram que a mem\u00f3ria associativa de Personnaz com atualizacao ass\u00edncrona sempre converge para um ponto fixo.\nExemplo 5.3.1. Considere os padr\u00f5es bipolares apresentados na figura 1.1. Armazenamos estes padr\u00f5es na mem\u00f3ria associativa de Personnaz e verificamos que todas as mem\u00f3rias fundamentais sao pontos fixos conforme a proposicao 1.\nConsidere agora os padr\u00f5es ruidosos apresentados na figura 4.5. Estes padr\u00f5es foram gerados a partir dos padr\u00f5es x1, \u2022 \u2022 \u2022, x5 G {-1,1}4096 da figura 1.1 intoduzindo ruido uniforme com probabilidade 0, 3 de reverter o valor do pixel. A mem\u00f3ria associativa de Personnaz encontrou as mem\u00f3rias fundamentais no final da primeira iteracao.\n5.4 Mem\u00f3ria Associativa de Kanter-Sompolinsky\nKanter e Sompolinsky discutiram outra variacao da mem\u00f3ria associativa de Hopfield que utiliza o armazenamento por projecao, neste caso com diagonal nula, para obter a matriz dos pesos sinapticos [44]. Este modelo, tambem referido na literatura como \u201cmem\u00f3ria associativa de hopfield com armazenamento por projecao\u201d [3, 31], sera referido neste trabalho como Mem\u00f3ria Associativa de Kanter-Sompolinsky (Kanter-Sompolinsky Associative Memory, Kanter-Sompolinsky AM).\n5.4.1\tArquitetura da Rede\nA memoria associativa de Kanter-Sompolinsky e descrita por uma rede neural classica recorrente de camada ilnica totalmente conexa com a funcao sinal como funcao de ativacao. A arquitetura da memoria associativa de Kanter-Sompolinsky esta apresentada na figura 2.6. Este modelo e descrito pela equacao 5.1 (ou pela equacao 5.3).\n5.4.2\tAprendizado\nA matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolinsky e\nM = XXt, com mu = 0 Vi = 1,..., n,\t(5.33)\nonde X = [x1, x2,..., xk] G {-1,1}nxk e a matriz com as mem\u00f3rias fundamentais.\nProposi\u00e7\u00e3o 3. Todas as mem\u00f3rias fundamentais x1,..., xk ser\u00e3o pontos fixos da memoria associativa de Kanter-Sompolinsky.\nDemonstracao. A matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolinsky e M = W \u2014 D, onde D e uma matriz diagonal n x n com 0 &lt;dn &lt;1 e W e obtida resolvendo o problema da equacao 4.8. Note que Mxf = Wxf \u2014 Dxf = (I \u2014 D)xf para toda memoria fundamental xf. Logo, 1 \u2014 dn > 0, para todo i = 1,..., n e pela definicao da funcao sinal temos que\nsinal (mTxf) = sinal ((1 \u2014 di\u00ed)x0 = xf,\t(5.34)\npara todo i = 1,..., n e para todo \u00a3 = 1,..., k.\t\u25a1\nNote que a diferenca entre a memoria associativa de Kanter-Sompolinsky e a memoria associativa de Personnaz esta na diagonal nula da matriz dos pesos sinapticos do primeiro modelo. Por causa da diagonal nula, a memoria associativa de Kanter-Sompolinsky possui uma toler\u00e2ncia a ru\u00eddo melhor que a memoria associativa de Personnaz [44]. Verificamos este fato atraves de experimentos computacionais no cap\u00edtulo 7.\n5.4.3\tConvergencia\nA Proposicao 2 e o Teorema 5.1.1 mostram que a memoria associativa de Kanter-Sompolinsky com atualizacao ass\u00edncrona sempre converge para um ponto fixo.\nExemplo 5.4.1. Repetimos o mesmo experimento realizado no exemplo 5.3.1. Primeiro, verificamos que todas as memorias fundamentais sao pontos fixos da memoria associativa de Kanter-Sompolinsky. Depois apresentamos os padroes da figura 4.5 como entrada e verificamos que as recordacoes fundamentais x1,..., xk foram encontradas no final da primeira iteracao.\n5.5\tMemoria Associativa Bidirecional Assim\u00e9trica\nA Memoria Associativa Bidirecional Assim\u00e9trica (Asymmetric Bidirectional Associative Memories, ABAM), tambem conhecida como Projection HDAM e uma extensao da memoria associativa de Personnaz para o caso heteroassociativo [109, 32].\n5.5.1\tArquitetura\nA mem\u00f3ria associativa bidirecional assimetrica e uma rede neural classica recorrente totalmente conexa descrita pelas equac\u00f3es\ny(t) = sinal (Wix(t)),\nx(t + 1) = sinal (W2y(t)), para t = 0,1, 2,...\n(5.35)\n(5.36)\ncom atualizacao ass\u00edncrona. A arquitetura desta rede esta apresentada na figura 5.3.\n5.5.2\tAprendizado\nConsidere os pares de padr\u00f5es (x^, y^), onde G {-1,1}n e y^ G {-1,1}m, para todo \u00a3 = 1,..., k. Tome X = [x1, x2,..., xk] e Y = [y1, y2,..., yk]. As matrizes dos pesos sinapticos, W e W2, sao encontradas usando o armazenamento por projecao, isto e,\nWi = YX1 e W2 = XY1.\t(5.37)\nTeorema 5.5.1. Sejam X = [x1,..., xk] G { \u20141,1}nxk e Y = [y1,..., yk] G { \u20141,1}mxk as matrizes das mem\u00f3rias fundamentais {(x^, y^), \u00a3 = 1,..., k}. Se X e Y possem ambas posto completo, ent\u00e3o todas as memorias fundamentais ser\u00e3o pontos estacion\u00e1rios.\nDemonstra\u00e7\u00e3o. Se X tem posto completo, entao Xt = (XTX)-1XT e\nsinal (W1X) = sinal (Y(XTX)-1 XTX) = sinal(Y) = Y.\t(5.38)\nAnalogamente, substituindo X por Y e vice-versa, encontramos sinal(W2Y) = X se Y tem posto completo.\t\u25a1\nNao garantimos a convergencia da ABAM porque\n0 W2\n\nWi\t0\n(5.39)\nnao e uma matriz simetrica (W2 = W^). Resultados emp\u00edricos mostraram que este modelo apresenta um comportamento oscilatorio principalmente quando o numero de padroes armazenados esta proximo ou e maior que min(m, n) [30].\nExemplo 5.5.1. Considere os padroes x1,..., x5 G {\u20141,1}4096 e y1,..., y5 G {\u20141,1}4096 apresentados nas figuras 1.1 e 1.2. Neste caso, as matrizes X e Y possuem ambas posto completo. Armazenamos o conjunto das memorias fundamentais {(x^, y^), \u00a3 = 1,..., 5} na ABAM e verificamos que todas as memorias fundamentais sao pontos estacionarios segundo o teorema 5.5.1. Depois apresentamos como entrada os padroes corrompidos x1,..., x5 apresentados na figura 4.5 e verificamos que os padroes recordados pela ABAM no final da primeira iteracao sao exatamente as recordacoes fundamentais.\n5.6\tMem\u00f3ria Associativa com Capacidade Exponencial\nA Memoria Associativa com Capacidade Exponencial (Exponential Correlation Associative Memory, ECAM) pode ser vista como um caso particular das Memorias Associativas Recorrentes por Correlacao (Recurrent Correlation Associative Memories, RCAM) que serao discutidas nesta secao [15, 16, 32]. As mem\u00f3rias associativas recorrentes por correlacao podem ser vistas como generaliza-coes da mem\u00f3ria associativa de Hopfield discreta com auto-alimentacao.\n5.6.1\tArquitetura\nNa mem\u00f3ria associativa de Hopfield discreta com auto-alimentacao, a i-esima componente do vetor de estados, x(t + 1), e calculado atraves da equacao\nx\u00bb(t + 1) = sinal (wTx(t)) ,\t(5.40)\nonde wT e a i-esima linha da matriz W. No armazenamento por correlacao temos\nk\nwT = \u00a3 xf (x? )T.\t(5.41)\n?=i\nPortanto, a i-esima componente de x(t +1) e\nx\u00bb(t + 1) = sinal\tx?(x?)Tx(t)^ = sinal &lt;x?, x(t) > x?^ ,\t(5.42)\nonde &lt;\u2022, \u2022 > representa o produto interno Euclidiano. O termo &lt;, x(t) > representa a correla\u00e7\u00e3o entre o estado atual da mem\u00f3ria x(t) e o \u00a3-esimo padr\u00e2o armazenado x^. Podemos dizer, de um modo intuitivo, que quanto mais \u201cparecido\u201d o vetor x(t) for de x^, maior sera o valor de &lt;x^, x(t) >. Para enfatizar esta correla\u00e7ao, podemos aplicar uma fun\u00e7ao f no resultado de &lt;x^, x(t) >. A fun\u00e7ao f deve ser mon\u00f3tona nao-decrescente, pois desejamos que vetores pouco correlacionados apresentem um valor menor do que aqueles mais correlacionados.\nAo introduzir a funcao f, encontramos a seguinte expressao para a i-esima componente do vetor x(t + 1):\nx\u00bb(t + 1) = sinal f (&lt;xe, x(t) >)xf^ .\t(5.43)\nNuma notacao matricial temos\nx(t + 1) = sinal (XF (Xtx(t))) ,\n(5.44)\nonde F(\u2022) = [f (\u2022),..., f 0]T. Observando a equacao (5.44), podemos dizer que as RCAM\u2019s sao redes neurais recorrentes de duas camadas totalmente conexa com neur\u00f4nios classicos com funcao de ativacao f na primeira camada e funcao sinal na camada de sa\u00edda. Na figura 5.6 apresentamos a arquitetura das RCAM\u2019s.\nXT\tf (\u2022)\tX\tsinal(-)\nx1(t)\nx2(t)\nxa(t)\nx\u201e(t)\nxi (t + 1)\nx2 (t + 1)\nx3 (t + 1)\nx\u201e(t + 1)\nFig. 5.6: Arquitetura das Memorias Associativas Recorrentes por Correlacao.\nA fun\u00e7\u00e3o f introduzida representa um papel importante na capacidade e din\u00e2mica da RCAM. Existem infinitas fun\u00e7\u00f5es que podem ser usadas, mas precisamos que esta fun\u00e7ao seja facil de ser implementada e forneca uma RCAM assintoticamente estavel com uma capacidade de armazenamento grande. Apresentamos a seguir algumas funcoes usadas na RCAM. Depois apresentaremos condicoes suficientes para obtermos uma mem\u00f3ria assintoticamente estavel.\n1.\tPara f (v) = v, encontramos a mem\u00f3ria associativa de Hopfield discreta com armazenamento por correlacao.\n2.\tPara f (v) = (n+v)q, onde q e um inteiro maior que 1 e n e a dimensao dos vetores , encontramos a memoria associativa de ordem alta por correlacao (High-Order Correlation Associative Memory). A capacidade de armazenamento desta mem\u00f3ria e assintoticamente proporcional a nq [16].\n3.\tPara f (v) = (n \u2014 v)-p, onde p > 1, encontramos a memoria associativa comfuncao potencial por correlacao (Potential-Function Correlation Associative Memory). A capacidade de armazenamento deste modelo aumenta exponencialmente com a dimensao n dos padr\u00f5es armazenados. Este modelo foi proposto independentemente por Dembo e Zeitouni, e por Sayeh e Han [16]. Apresentamos aqui uma versao com tempo discreto e estados bipolares, embora, originalmente tenha sido proposto com tempo cont\u00ednuo e padr\u00f5es com valores reais.\n4.\tQuando f (v) = av, com a > 1, encontramos a memoria associativa com capacidade exponencial. Esta e a unica RCAM discutida neste trabalho.\n5.6.2\tAprendizado\nO aprendizado da RCAM e direto pois a matriz dos pesos sinapticos da camada de entrada e XT e a matriz dos pesos sinapticos da camada de sa\u00edda e X .O armazenamento de um novo padrao xh e feito\nconcatenando as matrizes XT e X com (xh)T e xh, respectivamente.\n5.6.3\tConverg\u00eancia\nAs quatro RCAMs apresentadas anteriormente (itens (1) - (4)) com atualizacao ass\u00edncrona possuem regioes de indecisao vazias. O teorema sobre a convergencia das RCAM\u2019s foi enunciado e demonstrado por Chiueh e Goodman em [15].\nTeorema 5.6.1 (Teorema da Convergencia de Chiueh e Goodman). Seja f (v) uma funcao cont\u00ednua monotona nao-decrescente definida sobre um intervalo fechado. A RCAM descrita pela equacao (5.43) com atualizacao ass\u00edncrona sempre converge para um ponto fixo para qualquer padrao-chave apresentado a RCAM.\nO seguinte teorema, introduzido por Chiueh e Goodman em [15], nos diz que o nilmero de memorias fundamentais que podem ser armazenadas como pontos fixos da ECAM aumenta exponencialmente com n, o numero de componentes dos padr\u00f5es.\nTeorema 5.6.2. Sejam x1,..., xk E {-1,1}\u201d, com n suficientemente grande, padroes nao correlacionados gerados aleatoriamente com distribuicao uniforme. Todos as memorias fundamentais serao pontos fixos com grande probabilidade se k &lt;ac\u201d, onde c E [1, 2] \u00e9 uma constante fixa que depende de a.\nE importante observar que o valor da constante c do teorema acima decresce quando o valor de a diminui [16].\nExemplo 5.6.1. Repetimos o experimento realizado no exemplo 5.3.1. Verificamos que todas as memorias fundamentais sao pontos fixos da ECAM. Depois apresentamos os padr\u00f5es ruidosos da figura 4.5 como entrada e verificamos que os padr\u00f5es recordados sao as respectivas recordacoes fundamentais. Neste exemplo utilizamos a = 1, 007 (ou f (x) = 2(x/100)) para evitar overflow. A ECAM encontrou as recordacoes fundamentais com uma \u00fanica iteracao.\n5.7\tMemoria Associativa Bidirecional com Capacidade Exponencial\nA Memoria Associativa Bidirecional com Capacidade Exponencial (Bidirectional Exponential Capacity Associative Memory, BECAM) e uma generalizacao da mem\u00f3ria ECAM para o caso heteroasso-ciativo. Este modelo de memoria associativa foi introduzido por n\u00f3s nesta dissertacao de mestrado.\n5.7.1\tArquitetura\nA BECAM e descrita por uma rede neural classica recorrente com quatro camadas totalmente conexa com funcao sinal e funcao exponencial como funcoes de ativacao.\nSeja {(x*, y*), \u00a3 = 1, 2,..., k} com x* E {-1,1}\u201d e y* E {-1,1}m, para todo \u00a3 = 1,..., k. Na BAM, a i-esima componente do vetor de estados y (t) e dada por\ny(t)i = sinal (wTx(t)) ,\n(5.45)\n5.7 Memoria Associativa Bidirecional com Capacidade Exponencial\n45\nonde wT e a i-esima linha da matriz W. Quando usamos o armazenamento por correlacao, temos wT = V?' |\t(xf)T. Assim, a i-esima componente de y(t) e\ny(t)i = sinal\tyf(x)Tx(t)^ = sinal\tx, x(t) > yf^ ,\n(5.46)\nonde &lt;\u2022, \u2022 > representa o produto interno Euclidiano. Podemos aplicar uma funcao f no valor &lt;xf, x(t) > a fim de enfatizar esta correlacao. As quatro func\u00f5es apresentadas na secao 5.6 podem ser usadas neste modelo e cada uma produz uma mem\u00f3ria heteroassociativa diferente. Voltamos nossa atencao para a funcao f (v) = av, para a > 1, que chamaremos de BECAM (Bidirectional Exponential Capacity Associativa Memory).\nEncontramos a seguinte expressao para a i-esima componente do vetor y (t) quando introduzimos a funcao f:\nyi(t) = sinal\tf (&lt;x, x(t) >)y^\n(5.47)\nNuma notacao matricial temos\ny(t) = sinal (YF (XTx(t))),\n(5.48)\nonde F(\u2022) = [f (\u2022),..., f (-)]T. Analogamente, podemos encontrar a seguinte expressao para x(t +1):\nx(t + 1) = sinal (XF (Yty(t))) .\n(5.49)\nObservando as equacoes (5.48) e (5.49) percebemos que a BECAM e uma rede recorrente com quatro camadas. Na figura 5.7.1 apresentamos a arquitetura da BECAM.\n\tX T\tf (\u2022)\tY\tsinal(-)\tY T\tf (\u2022)\tX\tsinal(-)\nX1 (t) \t> O \u2014\t\t\t\tO \u2014yi(t)^- O\t\t\t\tH O \u2014> xi(t + 1)\n\t\t71 O =3\t\t\t\t\t\t\nX2(t)  > O\t\t\t\to \u2014y2'\u00ed \u2014 O __\t\t\t\t33 O \u2014> x2(t +1)\n\t\t3b O 33\t\t\t\t3b 1\t\t\nX3(t) \t> O \u25a0\"\t\t\t\tO \tV3(t)^ O \"\t\t\t\tO \u2014> X3(t + 1)\nX\u201e(t)\nO  Vm(t)^~ O\nFig. 5.7: Arquitetura da Memoria Associativa Bidirecional com Capacidade Exponencial.\nXn(t + 1)\n5.7.2\tAprendizado\nAs matrizes dos pesos sinapticos da BECAM sao: XT, Y, YT e X. Note que o armazenamento de um novo par (xh, yh) e feito concatenando as matrizes XT, X, YT e Y com (xh)T, xh, (yh)T e yh, respectivamente.\n5.7.3\tConvergencia\nA convergencia da BECAM pode ser verificada convertendo ela numa ECAM. Tome\nX,\n0,\nx\ny\n0\nY\ne X' =\nx\n/\n(5.50)\ncomo sendo os padroes e a matriz dos pesos sin\u00e1pticos, respectivamente. Pela equacao (5.44), teremos\nx(t + 1)\ny(t +1)\nsinal\nsinal\n0\nY\n0\nY\nX\n0\nX\n0\nYT\n0\n= sinal\nY T y(t)l\\\\ X T x(t)JJJ\nXf (YTy(t)) Yf (XTx(t))\nExemplo 5.7.1. Repetimos o experimento realizado no exemplo 5.5.1. Verificamos que todas as mem\u00f3rias fundamentais sao pontos fixos da BECAM. Depois apresentamos os padr\u00f5es ruidosos x1,..., x5 da figura 4.5 como entrada e verificamos que os padr\u00f5es y1(1),..., y5(1) recordados no final da primeira iteracao sao as respectivas recordac\u00f3es fundamentais. Neste exemplo utilizamos a = 1, 007 (ou f (x) = 2(x/100)) para evitar overflow. A BECAM encontrou as recordac\u00f3es fundamentais com uma ilnica iteracao.\n5.8\tModelo do Estado Cerebral numa Caixa (BSB)\nO Modelo do Estado Cerebral numa Caixa (Brain-State-in-a-Box, BSB) foi proposto por Anderson et al. em 1977 [6] como uma rede recorrente de tempo discreto, nao-linear e totalmente conexa. Como nos modelos anteriores, o modelo do estado cerebral numa caixa pode ser visto como uma mem\u00f3ria auto-associativa que minimiza a energia de um sistema dinamico nao-linear.\n5.8.1\tArquitetura\nO modelo BSB e uma rede neural classica recorrente com tempo discreto de camada unica. Seja x(t +1) o vetor de estado no tempo t + 1. A recorrencia e dada por aWx(t), onde W e a matriz dos pesos sinapticos e a e uma constante positiva que controla o peso deste termo. Na BSB, adicionamos um termo de decaimento ao termo de recorrencia dado pelo vetor x(t), o estado anterior. Assim, o pr\u00f3ximo estado, x(t + 1), sera dado por\nx(t + 1) = L (x(t) + a (Wx(t) + 0)).\n(5.51)\nNeste trabalho consideraremos 0 = 0. A funcao de ativacao usada no modelo BSB e a funcao linear por partes definida como\nL(x) = 1 A [(-1) V x].\t(5.52)\nNote que esta funcao limita os vetores de estados numa regiao restrita do espaco, especificamente, no hipercubo Hn = [-1,1]n. Da\u00ed o nome: Modelo do Estado Cerebral numa Caixa [46].\n5.8.2\tAprendizado\nO modelo original da BSB proposto por Anderson et al. utiliza o aprendizado de Hebb (armazenamento por correlacao) onde tomamos W = XXT [6]. Neste caso, a matriz dos pesos sinapticos e sime\u00edtrica, semi-definida positiva e com diagonal positivia.\nOutras regras de aprendizado podem ser utilizadas na BSB, por exemplo, o armazenamento por projecao ou o armazenamento iterativo proposto em [66]. Neste trabalho discutiremos somente o armazenamento por correlacao.\n5.8.3\tConvergencia\nA BSB sempre converge para um ponto fixo independente da mem\u00f3ria-chave apresentada e minimiza\na funcao energia\nEnergia(x) = \u20141 xT W x.\n(5.53)\nse impormos pequenas condicoes ao modelo [41, 23, 24, 66].\nTeorema 5.8.1 (Teorema de Golden da Minimiza\u00e7\u00e3o da Fun\u00e7\u00e3o Energia do Modelo BSB). Considere o modelo neural descrito pela equa\u00e7\u00e3o (5.51). Se W = WT e semi-definida positiva ou a &lt;(2/| Amin|), onde Xmin \u00e9 o menor autovalor da matriz sim\u00e9trica W, ent\u00e3o:\n1.\tEnergia(x(t + 1)) &lt;Energia(x(t)) se x(t + 1) = x(t),\n2.\tEnergia(x(t + 1)) = Energia(x(t)) se e somente se x(t + 1) = x(t),\nonde Energia(x) e afun\u00e7ao definida na equa\u00e7ao (5.53).\nA demonstracao do teorema 5.8.1 pode ser encontrada em [24].\nTeorema 5.8.2. Seja W E Rnxn com wn > 0 para i = 1,... ,n. Se x E [\u20141,1]n e um ponto fixo da BSB ent\u00e3o x E { \u2014 1,1}n, isto e, x e um v\u00e9rtice do hipercubo [\u20141,1]n.\nA demonstracao do teorema 5.8.2 encontra-se em [66]. Note que o teorema 5.8.2 nao imp\u00f5e nenhuma hip\u00f3tese sobre a simetria da matriz dos pesos sinapticos. O seguinte teorema foi introduzido por n\u00f3s e relaciona o modelo BSB com a mem\u00f3ria associativa de Hopfield.\nTeorem\u00e3 5.8.3. Um padr\u00e3o x G { \u2014 1,1}n sera um ponto fixo da BSB (com 0 = 0) se e somente se x for um ponto fixo da mem\u00f3ria associativa de Hopfield com a mesma matriz de pesos sinapticos.\nDemonstra\u00e7\u00e3o. Seja x G {-1,1}n. Pela definicao da funcao linear por partes, da funcao sinal e lembrando que a > 0, temos:\nx = L(aW x + x) o\t(a(Wx)i + xi) xi > 1 Vi = 1,..., n\t(5.54)\nO\taxi(Wx)i + 1 > 1 Vi = 1,..., n\t(5.55)\no\txi (Wx)i > 0 Vi = 1,..., n\t(5.56)\no\tx = sinal(W x).\t(5.57)\n\u25a1\nNote que o teorema 5.8.3 nao impoe nenhuma hipotese sobre a matriz dos pesos sinapticos W. Portanto, este resultado permanece valido para outras regras de aprendizado, por exemplo, para o armazenamento por projecao. Note tambem que nao impomos nenhuma condicao sobre a constante a, exceto a > 0. Finalmente, observe que este resultado so e valido para padroes nos vertices de [\u20141,1]n. Poderemos ter pontos fixos na BSB que estao no iterior de [-1,1]n se as hipoteses do teorema 5.8.2 nao forem satisfeitas. Estes pontos fixos da BSB obviamente nao ser\u00e2o pontos fixos da memoria associativa de Hopfield e suas variacoes.\nCap\u00edtulo 6\nMemorias Associativas Morfol\u00f3gicas\nNeste cap\u00edtulo discutimos as mem\u00f3rias associativas morfol\u00f3gicas (Morphological Associative Memories, MAM). Discutimos primeiro o caso heteroassociativo em tons de cinza apresentando exemplos e resultados sobre a capacidade de armazenamento e toler\u00e2ncia a ru\u00eddo. Depois voltamos nossa atencao para o caso auto-associativo. Terminamos o cap\u00edtulo discutindo o caso auto-associativo binario e as memorias associativas de duas camadas.\n6.1\tMemorias Associativas Morfologicas Heteroassociativas\nAs memorias associativas morfologicas sao redes neurais descritas pelo modelo neural morfologico e foram introduzidas em [73, 74, 76]. O caso mais simples ocorre quando temos uma rede alimentada adiante, totalmente conexa e de camada l\u00ednica, cuja funcao de ativacao e a funcao identidade. Neste caso, o mapeamento associativo da memoria G : Rn \u25a0 Rm e descrito por uma matriz W G Rmxn ou M G Rmxn e o produto-m\u00ednimo ou o produto-maximo. Dado um padr\u00e2o-chave x G Rn, encontramos o padrao recordado y G Rm atrav\u00e9s da equacao\ny = W 0 x,\t(6.1)\nou\ny = M H x.\t(6.2)\nRepare na semelhanca entre o mapeamento associativo das memorias associativas morfologicas he-teroassociativas e o mapeamento associativo das memorias associativas lineares. A diferenca esta no produto-maximo ou produto-m\u00ednimo usado nos modelos morfologicos. Lembre-se que os modelos morfologicos apresentam um comportamento nao-linear devido a estas duas operacoes.\nPodemos fazer uma interpretacao das equacoes (6.1) e (6.2) usando a morfologia matematica [85, 86, 87]. Podemos verificar que os operadores 5(x) = W0 x e s(x) = MH x sao operacoes de dilatacao e erosao na morfologia matematica, respectivamente. Desta forma, a tarefa na fase de armazenamento das memorias associativas morfologicas seria encontrar uma dilatacao (erosao) tal que 5(x^) = y^ (s(x^) = y^) para todo f = 1,..., k. Nao usaremos esta interpretacao na fase de armazenamento, mas podemos extrair resultados interessantes (e intuitivos) para a fase de recordacao. Por exemplo, a dilatacao (erosao) e um operador extensivo (anti-extensivo), isto e, a dilatacao (erosao) expande (reduz) um objeto preto de uma imagem com fundo branco. A memoria\nassociativa morfol\u00f3gica descrita pela equacao (6.1) (Equacao (6.2)) tambem possui esta caracter\u00edstica. Alem disso, uma dilatacao (erosao) e usada para remover ru\u00eddo erosivo (dilativo) de uma imagem. Dizemos que uma imagem x e uma versao de x corrompida com ru\u00eddo erosivo (dilativo) se x &lt;x (x > x).\n6.1.1\tAprendizado\nSuponha que armazenamos um \u00fanico par (x, y) na mem\u00f3ria. Pelas equa\u00e7\u00f5es (6.1) e (6.2), encontramos, respectivamente,\nn\nyi = (W 0 x)i = \\/ (wij + xj),\t(6.3)\nj=i\ne\nn\nyi = (M 0 x)i = /\\ (Wij + xj).\t(6.4)\nj=i\nPela equa\u00e7ao (6.3) temos que yi &lt;wij + Xj, ou seja, yi \u2014 Xj &lt;wij. Podemos definir W atraves da equa\u00e7ao wij = yi \u2014 Xj. Usando a nota\u00e7ao matricial da algebra de imagens, temos\nyi - xi\ny1 xn\n= y s x*,\n(6.5)\nym x1\t\u2022 \u2022 \u2022\tym\txn\nonde x* = \u2014 xT \u00e9 o conjugado aditivo do vetor x.\nPodemos verificar que WXY recupera o padrao y quando apresentamos x como entrada. De fato,\nVn=i (yi - xi + xi)\nWxy E x =\n= y.\n(6.6)\nVi=1 (ym\txi + xi)\nRepare na semelhanca entre as equacoes (6.5) e (4.3) do armazenamento por correlacao quando armazenamos um unico padrao. Por analogia a (4.3), quando armazenamos o conjunto de mem\u00f3rias fundamentais {(x^, y^), \u00a3 = 1,..., k}, temos\nWxy = Y 0 X *,\n(6.7)\nonde X = [x1, x2,..., xk] e Y = [y1, y2,..., yk]. Note que usamos 0 para gerar a matriz WXY (equacao (6.7)) e usamos 0 na fase de recordacao (equacao (6.6)).\nPartindo de (6.4) e seguindo um racioc\u00ednio anaiogo, encontramos\nMxy = Y I X*,\n(6.8)\ncomo sendo a matriz dos pesos sin\u00e1pticos da mem\u00f3ria associativa morfol\u00f3gica descrita pela equa\u00e7\u00e3o\n(6.2).\nExemplo 6.1.1. Seja\n\t0\t0\t0\t\t0\t-1\t0\nX =\t0\t-2\t-3\te Y =\t1\t-1\t-2\n\t0\t-4\t0\t\t0\t0\t0\nDe acordo com a equacao (6.7) temos\nWxy = Y H X *\n000\t\t-1 1 3\t\t030\n111\tH\t-1 1 3\tH\t-2 1 -2\n000\t\t0\t2 4\t\t030\n-10 0\n\u20142 1 \u20142\n000\n(6.10)\ne pela equacao (6.8) encontramos\ne\nMxy = Y H X *\n000\t\t-1 1 3\t\t030\t\t033\n111\tV\t-1 1 3\tV\t-2 1 -2\t=\t113\n000\t\t0\t2 4\t\t030\t\t034\n(6.11)\nPodemos verificar que WXY V x^ = y^\nMXY H xe, para todo \u00a3 = 1, 2, 3. Por exemplo,\nWxy V x1\n-1\t0\t0\t\t0\n-2\t1\t-2\tV\t0\n0\t0\t0\t\t0\nMxy H x2\n033\t\t0\t\n113\tH\t-2\t=\n034\t\t-4\t\n-1\n-1\n0\n= y1,\n(6.12)\n(6.13)\n0\n1\n0\n= y2.\nChamamos a aten\u00e7\u00e3o do leitor para o fato de que 0 (x^) * = y^ 0 (x^) *. Podemos reduzir a nota\u00e7\u00e3o denotando estes produtos externos morfol\u00f3gicos por y+ (x^) *. Desta forma, as equa\u00e7\u00f5es\n(6.7)\te (6.8) possuem um anaiogo a equa\u00e7ao (4.3), isto e\nk\tk\nWxy = f\\ (y\u20ac + (x\u20ac), e Mxy = \\/ (y\u20ac + (x\u20ac).\t(6.14)\n\u20ac=1\t\u20ac=1\nPodemos adicionar facilmente um novo par de padr\u00f5es nas mem\u00f3rias WXY e MXY usando as equa\u00e7\u00f5es de (6.14). Se armazenamos os pares (x^, y^), para \u00a3 = 1,..., k,e desejamos adi\u00e7ionar um novo par (xk+1, yk+1), entao definimos\nou\nWXy\u201c = WXY\u00b0z h y\nk+1 + (-xk+1)\nyk+1 + (-xk+1) T\n(6.15)\n(6.16)\nk+1 c,\n\n= M'#\"\nV\nEntretanto, nao podemos excluir um padr\u00e2o armazenado.\nA seguinte definicao, introduzida em [76], diz respeito ao armazenamento das mem\u00f3rias fundamentais em uma memoria associativa morfol\u00f3gica.\nDefini\u00e7\u00e3o 6.1.1. Uma matriz A e uma memoria 0 -perfeita para (xf, yf), com \u00a3 = 1,..., k, se e somente se, A 0 xf = yf, para todo \u00a3 = 1,..., k. Analogamente, uma matriz B e uma memoria El -perfeita para (xf, yf), com \u00a3 = 1,..., k, se e somente se, BE xf = yf, para todo \u00a3 = 1,..., k.\nPela definicao, se X = [x1,..., xk], Y = [y1,..., yk] e A e uma memoria 0 -perfeita, entao A 0 X = Y .Se B e El -perfeita, entao B l X = Y. Temos tambem que, se A e 0 -perfeita, entao (A 0 xf). = yf para todo \u00a3 = 1,..., k e todo i = 1,..., m. Equivalentemente,\nn\nV (%\u25a0 + j = yf, V\u00a3 = 1,...,kei = i,...,m.\t(6.17)\nj=i\nDa equacao (6.17) obtemos a seguinte desigualdade para um \u00edndice j G {1,..., n} arbitrario:\n\t&lt;yf,\tV\u00a3 = 1,..\t\t.,k\t(6.18)\nO\tGjj\t&lt;yf - ,\tV\u00a3\t= 1,..\t.,k\t(6.19)\nO X\tk (yi - f=1\txj )\t= \u2022\t\t(6.20)\nTemos com isso que A &lt;WXY e consequentemente\nY = A E X &lt;Wxy E X.\t(6.21)\nPela equacao (6.14), temos WXY &lt;yf x (xf) * &lt;MXY para todo \u00a3 = 1,..., k e usando a equacao (6.6) conclu\u00edmos que WXY E xf &lt;(yf x (xf) *) E xf = yf = (y; x (xf) *) E xf &lt;MXY E xf, para \u00a3 = 1,..., k, ou equivalentemente,\nWxy E X &lt;Y &lt;Mxy E X.\t(6.22)\nFinalmente, pelas equacoes (6.21) e (6.22) conclu\u00edmos que\nY = A E X &lt;Wxy E X &lt;Y,\t(6.23)\ne portanto WXY 0 X = Y. Um argumento similar mostra que se B e E -perfeita para (xf, yf), para todo \u00a3 = 1,..., k, entao MXY &lt;B e MXY E X = Y.\nTeorema 6.1.1. Se A \u00e9 E -perfeita e B \u00e9 E -perfeita para (xf, yf), com \u00a3 = 1,..., k, entao\nA &lt;WXY &lt;Mxy &lt;B e WXY E X = Y = MXY E X.\t(6.24)\nEste teorema mostra que WXY e o maior elemento (maximo) do conjunto das memorias E -perfeitas e MXY e o menor elemento (m\u00ednimo) do conjunto das memorias E -perfeitas. Alem disso, se existe uma memoria E -perfeita, entao WXY e tambem E -perfeita. O mesmo vale para MXY, substituindo E por E .\nO seguinte teorema fornece uma condicao necessarias e suficientes para o perfeito armazenamento das memorias fundamentais em uma memoria associativa morfologica no caso hetero-associativo [73, 76].\nFig. 6.1: Padr\u00f5es recordados pela mem\u00f3ria associativa morfol\u00f3gica MXY ap\u00f3s apresentarmos as chaves fundamentais x1,..., x5 como entrada.\nr\nTeorema 6.1.2. A matriz WXY e 0 -perfeita para (x^, y^), com \u00a3 = 1,..., k, se e somente se, para cada \u00a3 = 1,..., k, cada linha da matriz [y^ x (x^) *] \u2014 WXY cont\u00e9m um elemento nulo. Por dualiade, a matriz MXY e El -perfeita para (x^, y^}, com \u00a3 = 1,..., k, se e somente se, para cada \u00a3 = 1,..., k, cada linha da matriz MXY \u2014 [y^ x (x^) *] contem um elemento nulo.\nNote que o teorema 6.1.2 nao contem nenhuma hipotese sobre o nUmero maximo de memorias fundamentais. Logo, podemos armazenar quantos padroes desejarmos, desde que, cada linha de y^ x (x^ \u2014 WXY e Mxy \u2014 y^ x (x^ contenha um elemento nulo para cada \u00a3 = 1,..., k. Exemplo 6.1.2. No exemplo 6.1.1, verificamos que WXY 0 x^ = y^ = MXY 0 x^, para \u00a3 = 1, 2, 3, ou seja, WXY e MXY sao memorias 0 e E -perfeitas, respectivamente. Logo, elas satisfazem as condicoes do teorema 6.1.2. Por exemplo, para \u00a3 = 2, temos que\nW11\t=\t= vl-\tx2 = 1\ty2\tX\t(x2 )*]\t11 ,\t\t^13\t=\t= V12-\tx2 = x3\t=\ty2\tX\t(x2)*\t13 ,\t\nW22\t=\t= vl-\tx2 = x2 =\ty2\tX\t(x2 )*\t22 ,\te\tm22 =\t=\tx2 = x2 =\ty2\tX\t(x2)*\t22 ,\t(6.25)\nW31 =\t=\tx2 = 1\ty2\tX\t(x2)*\t31 ,\t\tm33\t=\t= V32-\tx2 = x3\t=\ty2\tX\t(x2)*\t33 .\t\nExemplo 6.1.3. Considere os padr\u00f5es bin\u00e1rios x1,..., x5 e y1,..., y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padr\u00f5es nas mem\u00f3rias associativas morfol\u00f3gicas MXY e Wxy. Verificamos que Wxy 0 x- = y-, para todo \u00a3 = 1,..., k. Entretanto somente as equac\u00f3es Wxy 0 x1 = y1 e Wxy 0 x5 = y5 valem para a mem\u00f3ria associativa morfol\u00f3gica MXY. Na figura 6.1 apresentamos os padr\u00f5es recordados pela mem\u00f3ria associativa morfol\u00f3gica MXY ap\u00f3s apresentarmos as chaves fundamentais x1,..., x5 como entrada. O erro quadratico normalizado\nlly- - mxy 0 x- ||\t(626)\n- (6.26)\nlly- h\ncalculado sobre os padr\u00f5es x2, x3 e x4 foi 0, 068, 0, 034 e 0, 016, respectivamente.\nTeorema 6.1.3. Sejam X = [x1,..., xk] G {0,1}nxk e Y = [y1,..., yk] G {0,1}mxk matrizes geradas aleatoriamente com probabilidade 1/2 de um elemento ser 0 ou 1. Se Wxy = Y 0 X* e Mxy = Y 0 X * entao\nPr (Wxy 0 X = Y) = Pr (Mxy E X = Y)\n1 - 1L - 30LG1.1\n4\t4k-1\n(2\nk\u20141\nn\u20141\nk\u20141\n! -i mk k\u20141\\ n\u20141\n1\n4\nD\nDemonstra\u00e7\u00e3o. Vamos demonstrar o teorema 6.1.3 somente para a mem\u00f3ria associativa morfol\u00f3gica WXY. O resultado para MXY pode ser obtido de modo anaiogo. Durante a demonstracao usaremos o fato de xf e serem variaveis aleat\u00f3rias independentes e identicamente distribu\u00eddas para todo z = l,...,rn, j = l,...,n e \u00a3 = l,...,k\nSabemos que WXY 0 X &lt;Y para todo X e Y. Assim, WXY 0 X = Y se e somente se WXY 0 X > Y. A probabilidade de WXY 0 X > Y sera obtida atraves da probabilidade de Vj=i (w j + xj) > y 1- O calculo desta ultima sera dividida em quatro casos distintos, disjuntos e equiprovaveis. No final teremos Pr(V\u201d= 1 (w 1j + xj) = y 1) = (Pr(Caso 1) + Pr(Caso 2) + Pr(Caso 3) + Pr(Caso 4))/4.\nCaso (1). Se x1 = 1 e y1 = 1, teremos\nPr (w 1 1 > 0) = Pr \u00ed\t(yf \u2014 xf) > 0 J = Pr \u00ed 0 A\t(yf \u2014 xf) > 0\nY= 1\t/\t\\\tf=2\nPr (w 1 j + xf &lt;0) = Pr\t(yf - xf + xj )\t&lt;0^ = Pr ^1 A\t(yf - xf\t+\txj ) &lt;0^\n(k\t\\\tk 1\t/1 \\\tk \u2014 1\n/\\(yf - xf + xj)\t>\t0l =1 - [Pr((yf - xf + xj) >\t0)] =1\t,\nPr (wij\t+\txj)\t> 1^ = 1-Pr\t(wij\t+ xj) &lt;1^ =\t1-[Pr (wj + xj &lt;0)]\" 1 = 1-\nAssim,\nf=2\nk-\nk-\n-i n\u2014 1\ni -( 2)\nk-1\\ n-1\nk-1\nPr ( V (wij + xj) > y1\t= Pr ((wii + 1 ) V (y (wb- + xj)^ > 1\n= Pr ((w11 + 1 ) > 1 ) + Pr ( V (w1j + xj) > 1\t\u2014 Pr ((w11 + 1 ) > 1 ) Pr (y (w1j + xj) > 1^\n- (- (\u00ed m-f r)\t\"\nCaso (2). Se x1 = 1 e yj = 0, teremos\nPr (y (w 1 j + xj) > yj^ = Pr ((w 11 + 1 ) V (y (w 1 j + xj)^ > 0^ = Pr ((w11 + 1) > 0) + Pr (y (w1j + xj) > 0^ \u2014 Pr ((w11 + 1) > 0) Pr (y (w1j+ xj) > = 1 + Pr (y (w1j + xj) > 0^ - Pr (y (w1j + xj) > 0^ =1.\nCaso (3). Se x1 = 0 e y1 = 1, teremos\n(k\t\\\t/\tk\t\\\ti f 1 \\ k\u2014 1\nA(yj - xj) > 1| = Pr \u00ed1 A f\\ (yj - xj) > n = [Pr (yj - xj) >\t= (j)\t\u2019\nAssim,\nPr V (w1j + xj) \u2014 y1\t= Pr ^W11 V V (wj + xj)\nPr (w11 \u2014 1 ) + Pr V (w1j + xj) \u2014 1\t\u2014 Pr (w11 \u2014 1 ) Pr V (w1j + xj) \u2014 1\n\u25a0\u2014( \u25a0\u2014(4 O\u00ed \u25a0 - (2 )T\tJ\nCaso (4). Se x} = 0 e yj = 0, teremos\n(k\t\\\t\u00ed k\ti\tk 1 f 3 \\ k\u2014 1\nA (yj - xj) > 0 I = Pr \u00ed 0 A A (yj - xj) > 0 I = [Pr (yj - xj) > 0\t= (j)\t\u2019\nf=2\nk \u2014 1\nPr (wij + xj &lt;-1) = Pr A (yj - xj + xj) &lt;-1^ = Pr ^0 A A (yj - xj + xj) &lt;-1^ (k\t\\\tkl\tf 7 \\ k\u20141\nA (yj - xj + xj) > -1! = 1 - /'A[,p - xj + xj) > oj]\t=1 -\u00c7,\nPr\t(wj + xj) > 0\nAssim,\n= 1-Pr\t(wtj + xj) &lt;0^ = 1-[Pr (w ij + xj &lt;-1)]\" 1 = 1-\n1A T\nq n\u2014 1\nPr V (w 1 j+xj) \u2014 yi1^ = Pr ^w 11 v V (w 1 j+xj)\nPr (w 1 1 \u2014 0) + Pr V (w 1 j + xj) \u2014 0^ \u2014 Pr (w 1 1 \u2014 0) Pr V (w j + xj) \u2014 0^ \u25a0 \u2014 (\u25a0 \u2014 (4)\u201c')(\u25a0 \u2014 (8 )T\nFinalmente teremos\nPr (y (w1j+xJ) = yi\n' \u2019( ' (4)k\u2014')( \u25a0-(8)\n'-8 (> - )('-(8)\nk-1 n-1\n+ 1\nk-1\nk-1\n'-( -I8U)( -\u00ab)-\u25a0) k \"n 1 \u25a0-(\u25a0 - (3 )k 4' -(8)\n4 (' - (4 )k \u25a0)(\u25a0 - (i)\nk\u20141\\n\u20141\nk-1\nk-1\nk-1\nk-1\tn-1\n1\n4\n)\n)\nk -1\n) )\nn\u20141\nPortanto,\nPr (Wxy 0 X = Y) = [Pr (Wxy 0 x* = y*)]k\nPr (y (wj + x*) = y*)\n-i mk\n\u25a0-4(' -\n3k\u20141 + 1\n4k\u20141\nk-1\tn-1\nk-1\nk-1\tn-1\n-i mk\n\u25a1\nExemplo 6.1.4. Considere o conjunto de padr\u00f5es binarios {(x*, y*), x* G {0,1}100, y* G {0,1}80, \u00a3 = 1,..., k} gerado aleatoriamente com distribuyo uniforme. Armazenamos este conjunto de mem\u00f3rias fundamentais nas mem\u00f3rias associativas morfol\u00f3gicas WXY e MXY. Depois verificamos se y* = W0 x* para todo \u00a3 = 1,..., k. Repetimos o experimento 1000 vezes para diferentes valores de k (m\u00edmero de mem\u00f3rias fundamentais armazenados). Na figura 6.2 apresentamos com y a probabilidade emp\u00edrica de y* = W 0 x* e com A a probabilidade emp\u00edrica de y* = M 0 x*, para todo \u00a3 = 1,..., k. A linha tracejada representa a probabilidade te\u00f3rica dada pela equacao (6.27). No eixo horizontal colocamos k, o numero de mem\u00f3rias fundamentais. Note que que as probabilidades emp\u00edricas Pr(W 0 X = Y) e Pr(W 0 X = Y) coincidiram e ambas sao menores que 1 para k > 4. Este resultado mostra que as mem\u00f3rias associativas morfol\u00f3gicas binarias heteroassociativas nao conseguem armazenar muitas mem\u00f3rias fundamentais.\nO seguinte teorema, introduzido em [73, 76], caracteriza a recordacao de um padr\u00e2o y7 ap\u00f3s apresentarmos como entrada uma versao corrompida da chave fundamental xY. Um resultado analogo pode ser obtido para a mem\u00f3ria associativa MXY usando o conceito de dualidade.\nTeorema 6.1.4. Seja xY uma versao ruidosa do padr\u00e3o xY. WXY 0 xY = yY se e somente se\n~Y xJ\nm k\n<xj0 AV(\n-i=1 *=Y\nvi- y* + J , v7 = 1,--\n(6.27)\n\ne para cada \u00edndice de linha i G {1,..., m}, existe um \u00edndice de coluna ji G {1,..., n} tal que\n~Y rp I \u2022J-x A .\nJl\nk\n= x0 V (vY - v*+4)\n-*=7\n(6.28)\nFig. 6.2: A linha com y representa a probabilidade emp\u00edrica = W 0 para todo \u00a3 = 1,..., k. A linha com A representa a probabilidade emp\u00edrica de y^ = M 0 x^ para todo \u00a3 = 1,..., k. A linha tracejada representa a probabilidade dada pela equacao 6.27.\nO seguinte teorema, introduzido por Sussner em [93], determina precisamente a a\u00e7\u00e3o da mem\u00f3ria associativa morfol\u00f3gica Wxy no caso binario. Um resultado anaiogo pode ser obtido para Wxy usando o conceito de dualidade.\nTeorema 6.1.5. Seja X G {0,1}\u201dxk, Y g {0,1}mxk e x G {0,1}\u201d. Se 0 = x&lt;\\j|=1 x^ ent\u00e3o\nq\nWxy 0 x = VV y ,\t(6.29)\nt=i te\u00a9t\nonde {015 02, \u2022 \u2022 \u2022,\t} e o conjunto de todos Qt \u00c7 {1,..., k} tais que\n\\J x^ > x .\t(6.30)\n\u00edeet\nExemplo 6.1.5. Considere os padr\u00f5es binarios x1,..., x5 e y1, \u2022 \u2022 \u2022, y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padr\u00f5es na mem\u00f3ria associativa morfol\u00f3gica Wxy. No exemplo 6.1.3 verificamos que Wxy 0 x^ = y^ para todo \u00a3 = 1,..., k. Considere agora os padr\u00f5es x1,..., x5 apresentados na figura 6.3. Estes padr\u00f5es foram obtidos introduzindo ru\u00eddo salt (ru\u00eddo erosivo) com probabilidade 0, 3. Verificamos que as recordac\u00f5es fundamentais y1,..., y5 foram encontradas como sa\u00edda ap\u00f3s apresentarmos os padr\u00f5es corrompidos x1,..., x5 como entrada. Repetimos o experimento 1000 vezes e verificamos que o erro quadr\u00e1tico medio normalizado\nEQMN = E \u00ed1 V\t~ W*Y B x1\nv\t|y II\nFig. 6.3: Padr\u00f3es corrompidos com ru\u00eddo salt com probabilidade 0, 3 usado no exemplo 6.1.5.\nFig. 6.4: Padr\u00f3es corrompidos com ru\u00eddo pepper com probabilidade 0, 3 usados como entrada, abaixo os respectivos padr\u00f3es recordados pela mem\u00f3ria associativa morfol\u00f3gica MXY.\nTambem armazenamos o conjunto das mem\u00f3rias fundamentais {(xf, yf), \u00a3 = 1,..., 5} na mem\u00f3ria associativa MXY. Utilizamos como entrada os padr\u00f5es corrompidos com ru\u00eddo pepper (ru\u00eddo dila-tivo) com probabilidade 0, 3 apresentados no topo da figura 6.4 e encontramos como resposta da mem\u00f3ria os padr\u00f5es apresentados na segunda linha da figura 6.4. O erro quadr\u00e1tico normalizado ||yf \u2014 MXY H xf ||/Hyf || para \u00a3 = 1,..., 5 foi, respecitvamente, 0, 081, 0, 068, 0, 035, 0, 016 e 0. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 04.\n6.2\tMem\u00f3rias Auto-Associativas Morfol\u00f3gicas\nNesta secao consideramos o caso auto-associativo, isto e, Y = X. Um discucao detalhada das mem\u00f3rias auto-associativas morfol\u00f3gica para padr\u00f5es em tons de cinza encontra-se em [97]. Neste caso, as matrizes WXX e MXX sao dadas por\nwij\nk\n= A (4 - 4)\ne=i\ne\tmij\nk\n=\t4 - 4),\nxi=1\n(6.32)\nFig. 6.5: Padr\u00f5es corrompidos com ruido pepper (ruido erosivo) utilizados no exemplo 6.2.1.\nFig. 6.6: Padr\u00f5es recordados pela mem\u00f3ria auto-associativa morfol\u00f3gica Wxx quando apresentamos os padroes da figura 6.5 como entrada.\ne na fase de recordacao usamos as equacoes (6.1) e (6.2), respectivamente.\nA fase de recordacao das memorias auto-associativas morfol\u00f3gicas sao descritas em termos dos seus pontos fixos e regioes de recordacao [90, 94]. Um resultado analogo pode ser obtido para a memoria associativa morfologica MXX usando o conceito de dualidade.\nTeorema 6.2.1. Para todo X = [x1,..., xk ] G Rnxk, o conjunto dos pontos fixos de WXX inclui os padroes x1,..., xk. Al\u00e9m disso, para todo x G Rn, temos\nWxx El x = x,\t(6.33)\nonde x denota o supremo de x no conjunto dos pontos fixos de WXX.\nO seguinte corolario afirma que a fase de recordacao das memorias auto-associativas morfologicas e efetuada em um unico passo.\nCorolario. Seja X G Rn. O conjunto dos pontos fixos de WXX consiste de todos WXX E x tais que x G Rn. Alem disso, se x, para \u00a3 = 1,... ,k e o padr\u00e3o recordado por WXX ap\u00f3s apresentarmos o padr\u00e3o-chave x, entao x &lt;x.\nLembre-se que uma versao ruidosa x de um padr\u00e2o x e uma versao erodida de x quando x &lt;x e e uma versao dilatada de x quando x > x. Usando esta terminologia temos que se WXX E x7 = x7, entao pelo teorema 6.2.1, x7 deve ser uma versao erodida de x7. Analogamente, se MXX E x7 = x7, entao x7 deve ser uma versao dilatada de x7.\nExemplo 6.2.1. Considere os padroes x1,..., x4 apresentados na figura 1.3. Armazenamos estes padroes na memoria associativa morfologicas WXX usando a equacao (6.32). Depois geramos os\nFig. 6.7: Padr\u00f5es corrompidos com ruido salt (ruido dilativo) utilizados no exemplo 6.2.1.\n\t\t\n\tZr .\t.^1\n\t1 j\t\nFig. 6.8: Padr\u00f5es recordados pela mem\u00f3ria auto-associativa morfol\u00f3gica MXX quando apresentamos os padroes da figura 6.7 como entrada.\npadr\u00f5es x1,..., x4 introduzindo ru\u00eddo pepper (ruido erosivo) com distribui\u00e7\u00e3o uniforme e probabilidade 0, 3. Na figura 6.5 apresentamos os padroes corrompidos x1,..., x4 e na figura 6.6 apresentamos os padroes recordados pela memoria associativa morfol\u00f3gica WXX apos apresentarmos os padroes corrompidos x1,..., x4. Repetimos o experimento acima 1000 vezes e encontramos um erro quadratico medio normalizado (EQMN) de 0, 0028.\nRealizamos um experimento analogo usando a memoria associativa morfologica MXX e os padroes corrompidos com ru\u00eddo salt (ru\u00eddo dilativo) gerados com distribuicao uniforme com probabilidade 0, 3 apresentados na figura 6.7. Na figura 6.8 apresentamos os padroes recordados pela memoria associativa morfologica MXX apos apresentarmos os padroes corrompidos com ru\u00eddo dilativo da figura 6.7. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 0039.\nUm padrao x contendo ru\u00eddo dilativo nao pode ser recordado usando WXX, pois pelo corolario acima, se xY > xj para algum i, entao WXX 0 xY > xY. Analogamente, um padrao xY contendo ru\u00eddo erosivo nao pode ser recordado usando MXX pois se xY &lt;xY para algum i, entao Mxx 0 xY &lt;xY.\nExemplo 6.2.2. Considere os padroes binarios x1,... x10 G {0,1}35 apresentados na figura 6.9. Armazenamos estes padroes na memoria associativa morfologica WXX e verificamos que todas as recordacoes fundamentais sao pontos fixos. Depois, introduzimos ru\u00eddo dilativo nas memorias fundamentais revertendo um ilnico pixel do fundo da imagem obtendo os padroes apresentados na figura 6.10. Na figura 6.11 apresentamos os padroes recordados pela memoria associativa morfologica WXX quando apresentamos os padroes da figura 6.10 como entrada. Note que nenhuma das memorias fundamentais foi recordada. Por outro lado, a memoria associativa morfologica MXX e eficiente para recordar padroes corrompidos com ru\u00eddo dilativo e todas as memorias fundamentais foram recordadas\npor esta memoria apos apresentarmos os padroes da figura 6.10 como entrada.\n123^567890\nFig. 6.9: Padr\u00f5es binarios xi,..., xi0 usados nos exemplos 6.2.2 e 6.3.1.\n123'4567890\nFig. 6.10: Padr\u00f5es x1,..., x10 corrompidos com ru\u00eddo dilativo.\nFig. 6.11: Padroes recordados pela memoria associativa MXX apos apresentar os padroes da figura 6.10 como entrada.\n6.3\tMem\u00f3rias Auto-associativas Morfol\u00f3gicas Bin\u00e1rias\nNesta secao discutimos as memorias associativas morfologicas binarias. Lembramos que um padrao e binarario se xf G {0,1}n para todo \u00a3 = 1,..., k. Sabemos que as memorias auto-associativas morfologicas podem armazenar infinitos padroes. No caso binario poderemos armazenar 2n padroes, onde n e a dimensao do padroes.\nDado um conjunto de padroes binarios {x1,..., xk} e um \u00edndice l, denotaremos por Ll o maior subconjunto de {1,..., k} tal que xf = 1 para todo \u00a3 G Ll. Em outras palavras, o conjunto Ll diz\na)\tb)\tc)\nFig. 6.12: Nos itens a), b) e c) temos e31, WXX E e31 e x5 A x9, respectivamente.\nquais sao as memorias fundamentais xf que possuem valor 1 na l-esima componente. Usando esta notacao podemos enunciar o seguinte teorema [94].\nTeorema 6.3.1. Seja y um ponto fixo de WXX tal que y = 1 e y &lt;f\\feL xf, ent\u00e3o y = AfeL\u00a1 xf. Corolario. Se ei \u00e9 a l-esima coluna da matriz identidade, entao\nWxx E ei =\txf.\t(6.34)\nExemplo 6.3.1. Considere os padroes x1,..., x10 apresentados na figura 6.9. Na figura 6.12 a) apresentamos o padrao e31 G {0,1}35 que foi usado como entrada da memoria WXX. Na figura 6.12\nb)\tapresentamos o padrao recordado pela memoria auto-associativa morfologica, isto e WXX E e31. Note que os padroes x5 e x9 possuem valor 1 na componente 31. Logo, L31 = {5, 9}. Na figura 6.12\nc)\tapresentamos /\\feL xf = x5 A x9. Note que as imagens dos itens b) e c) sao iguais, verificando a validade do corolario 6.3.\nNa demonstracao do corolario, usamos o fato de /\\feLi xf ser um ponto fixo de WXX. Na verdade, podemos mostrar um resultado muito mais forte. Para isso, vamos apresentar a seguinte definicao\n[11]:\nDefinicao 6.3.1. Uma expressao envolvendo x1,..., xk e os s\u00edbolos V e A e chamado polinomio reticulado em x1,..., xk.\nPela definicao acima, temos que \\/f=^\\xf e a forma geral de um polinomio reticulado. Em particular, /\\^eLl e um polinomio reticulado em x1,..., xk e e um ponto fixo de WXX. O seguinte teorema relaciona o conjunto dos pontos fixos das memorias associatias morfologicas com os polinomios reticulados em x1,..., xk [95].\nTeorema 6.3.2 (Teorema dos Pontos Fixos das Memorias Associativas Morfogicas Binarias). Seja X = [x1,..., xk] G {0,1}nxk. Um padrao binario y, diferente dos padroes 0 e 1, e um ponto fixo de WXX se e somente se y e um polinomio reticulado em x1,..., xk. Analogamente, um padrao binario z = 0, 1 e um ponto fixo de MXX se e somente se z e um polinomio reticulado em x1,..., xk.\nO teorema 6.3.2 mostra que uma memoria auto-associativa morfologica binaria tem grande probabilidade de ter um grande numero de estados espurios. Com base nos teoremas apresentados anteriormente podemos dizer:\n1.\tA capacidade absoluta das memorias auto-associativas binarias e 2n se n for a dimensao dos padroes armazenados.\n2.\tTodo padrao recordado e um ponto fixo da memoria (convergencia com uma c\u00ednica iteracao).\n3.\tOs pontos fixos de Wxx e MXX incluem os padroes originais bem como um grande m\u00edmero de estados espurios.\n4.\tA memoria Wxx exibe uma excelente tolerancia com respeito a padroes erodidos e MXX, com respeito a padroes dilatados.\n5.\tWxx e MXX nao sao eficientes na recordacao de padroes corrompidos por ambos ru\u00eddo dilati-vos e erosivos.\nNa proxima secao apresentaremos um modelo com caracter\u00edsticas melhores com respeito aos itens 3, 4 e 5, que mante\u00edm as caracter\u00edsticas dos itens 1 e 2.\n6.4\tMemorias Associativas Morfologicas de Duas Camadas\nAs memorias Wxy e MXY possuem excelente tolerancia com respeito a padroes corrompidos com ru\u00eddo somente erosivo ou somente dilativo, respectivamente, mas nao sao eficientes quando o padrao apresentado possui ambos ru\u00eddos. Para evitar este problema, podemos fazer combinacoes das memorias Wxy e Mxy. Estas combinacoes produzem as mem\u00f3rias associativas morfol\u00f3gicas de duas camadas. Como veremos, as memorias associativas de duas camadas possuem um numero menor de pontos fixos, aumentando assim a tolerancia a ru\u00eddos e diminuindo o numero de memorias espurias.\n6.4.1\tArquitetura\nPrimeiramente vamos lembrar a definicao do niicleo de uma memoria associativa morfologica W.\nDefini\u00e7\u00e3o 6.4.1. Sejam X G {0,1}nxk e Y G {0,1}mxk. Uma matriz Z = [z1, z2,..., zk] de dimens\u00f5es n x k e um n\u00facleo para (X, Y) se e somente se existe uma memoria W tal que\nW 0 (Mzz 0 X) = Y.\t(6.35)\nSe X = Y, dizemos que Z e um niicleo para X.\nSuponha que existe Z (niicleo) tal que WZY 0 (Mzz H X) = Y. Como Mxx H X = X para todo X G {0,1}nxk, entao podemos dizer que:\nWzy 0 (Mzz H (Mxx H X)) = Y.\t(6.36)\nAssim, dado um padrao x, encontramos\ny = Wzy 0 (Mzz El (Mxx 0 x)) = Wzy 0 ((Mzz El Mxx) 0 x) = Wzy 0 (M 0 x),\n(6.37)\nxi (t)\t0\nX2 (t)\t0\nX3 (t)\t0\nMXz\tf (\u2022)\tWzy\n0 \u2014*\u25a0 X1(t + 1)\n0 \u2014* X2(t + 1)\n0\tX3 (t + 1)\n00 \u2014*\u2022 Xn(t + 1)\nFig. 6.13: Arquitetura das Mem\u00f3ria Associativa de Duas Camadas descrita pela equacao (6.40).\nonde M e uma matriz que depende de Z e X. Na deducao acima, M = MZZ 0 MXX, mas faremos uma pequena mudanca na definicao da matriz M.\nNossa dificuldade esta em encontrar um nilcleo para (X, Y). Sabemos que o nilcleo depende\nsomente de X. Precisamente, devemos ter Z &lt;X, com z7 A z^ = 0\npara todo\nEntretanto, vamos supor apenas que zY A = 0 e zY z^ para todo 7, \u00a3, \u00a3 = 7. Note que Z nao depende mais de X .Se X G {0, 1}nxk, Y G {0,1}mxk e Z G {0,1}pxk, entao WZY e MXX sao matrizes m x p e n x n respectivamente. Para manter uma coer\u00eancia nas operacoes, M deve ser uma matriz p x n. Logo, tomaremos\nM = MXz = Mxz A Mxx \u2022\n(6.38)\nXn(t) \u2014> 0\n0\n0\n0\nAssim, dado um padr\u00e2o-chave x, tomamos\ny = Wzy\n(6.39)\ncomo sendo o padrao recordado.\nA operacao M^Z 0 x pode produzir como resposta um vetor que nao pertence ao conjunto {0,1}\u201d e isso pode ser ruim quando trabalhamos com mem\u00f3rias binarias. Introduzimos uma funcao limiar aplicada no resultado de M^_Z 0 x na equacao (6.39) para evitar este problema. Precisamente, definimos a mem\u00f3ria associativa:\ny = Wzy\ny = Wzy E f (MXZ A x), onde MXz = Mxz E Mxx e f (x) = [fi(x), f2(x), ..., fp(x)]' com\n1 se Xj > 0\n0 caso contrario.\n(6.40)\n(6.41)\nNa figura 6.4.1 apresentamos a arquitetura da rede deste modelo neural. O modelo dual pode ser obtido de um modo analogo.\n6.4.2\tAprendizado\nO aprendizado da mem\u00f3ria associativa morfol\u00f3gica de duas camadas consiste em escolher Z = [z1, z2,..., zk] G {0,1}pxk com z^ A zY = 0 e z^ zY, V\u00a3, 7, \u00a3 = 7 e construir as matrizes WZY = Y A Z* e M^Z = MXZ A MXX = (Z H X*) A (X H X*). A escolha de Z e arbitraria desde que satisfaca as condic\u00f5es apresentadas acima. Note que I = [e1, e2,..., ek] G {0,1}pxk, onde ei e a i-esima base do espaco Rp, pode ser usada como a matriz Z neste modelo. Neste trabalho usaremos somente Z = Ipxk.\nTeorema 6.4.1 (Teorema dos Pontos Fixos da Mem\u00f3ria Associativa Morfol\u00f3gica de Duas Camadas). Seja X G {0,1}nxk tal que MXX G {0,1}nxk. Seja Y G {0,1}mxk, Z = [z1, z2,..., zk] G {0,1}pxk e x G {0,1}n arbitr\u00e1rio. Usaremos a nota\u00e7\u00e3o 8 = {\u00a3 | x > x*} e MXjZ = MXZ A MXX. Se 8 = 0, zY > z^ e zY A z^ = 0 para todo \u00a3, 7 com 7 = \u00a3. Entao, para todo x = 1 = [1,1,..., 1]' G {0,1}n, temos\nWzy 0 f (M*Z A x) = \\/ /.\t(6.42)\niee\nAl\u00e9m disso, para todo \u00a3 = 1, 2,..., k e todo x G {0,1}n,\nMxx H x = x^\tWzy H f (MXXZ H x) = y^.\t(6.43)\nA demonstra\u00e7\u00e3o deste teorema encontra-se em [94]. A mem\u00f3ria associativa morfol\u00f3gica de duas camadas apresentada acima possui menos estados espUrios que o modelo WXX e portanto, possui uma melhor toler\u00e2ncia a ru\u00eddo. Um resultado anaiogo pode ser obtido para a versao dual da mem\u00f3ria associativa morfol\u00f3gica de duas camadas usando o conceito de dualidade.\nExemplo 6.4.1. Considere os padr\u00f5es binarios x1,..., x5 e y1,..., y5 apresentados nas figuras 1.1 e 1.2. Armazenamos estes padr\u00f5es na versao dual da mem\u00f3ria associativa morfol\u00f3gica de duas camadas usando Z = I4096x5 e verificamos que y^ = WZY 0 f (MXZ El x^) para todo \u00a3 = 1,..., k. Depois verificamos que as recordac\u00f3es fundamentais y1,..., y5 sao recordadas ap\u00f3s apresentarmos como entrada os padr\u00f5es X1,..., X5 apresentados na figura 6.3. Repetimos o experimento 1000 vezes e verificamos que o erro quadra\u00edtico me\u00eddio normalizado\nEQMN = E\n\u00ed 1A ||yg - Mxy a x- 11 \\ v\tlly II\tJ\ny\u00ab\n= 0.\n(6.44)\nFinalmente, utilizamos com entrada os padr\u00f5es corrompidos com ru\u00eddo pepper (ru\u00eddo dilativo) com probabilidade 0, 3 apresentados no topo da figura 6.4. Encontramos como resposta da mem\u00f3ria mem\u00f3ria associativa de duas camdas os padr\u00f5es apresentados na figura 6.14. O erro quadr\u00e2tico normalizado para \u00a3 = 1,..., 5 foi, respectivamente, 0, 081, 0, 068, 0, 035, 0, 016 e 0. Repetimos o experimento 1000 vezes e encontramos um EQMN = 0, 04.\nFig. 6.14: padroes re\u00e7ordados pela memoria asso\u00e7iativa morfol\u00f3gi\u00e7a de duas \u00e7amadas apos apresentarmos os padroes \u00e7orrompidos apresentados no topo da figura 6.4\nCap\u00edtulo 7\nDesempenho das Memorias Associativas Binarias\nNeste cap\u00edtulo apresentamos compara\u00e7\u00f5es do desempenho das mem\u00f3rias associativas bin\u00e1rias introduzidas nos cap\u00edtulos 5 e 6. Inspirados nas 5 caracter\u00edticas desejaveis em uma mem\u00f3ria associativa apresentadas no cap\u00edtulo 3, formalizamos os conceitos: capacidade de armazenamento, distribui\u00e7\u00e3o da informa\u00e7ao, raio de alra\u00e7ao e probabilidade de memoria esp\u00faria. E importante observar que o conceito de raio de atracao foi utilizado de um modo emp\u00edrico em [44]. Nesta dissertacao apresentamos uma definicao rigorosa deste conceito. O conceito de capacidade de armazenamento apresentado nesta dissertacao de mestrado difere do conceito de capacidade absoluta introduzido em [56]. Discutimos a diferenca entre estes dois conceitos na secao 7.1. Nao encontramos na literatura um trabalho discutindo o esfor\u00e7o computacional das mem\u00f3rias associativas neurais. Tambem nao encontramos um trabalho mencionando os conceitos de distribuicao da informacao e a probabilidade de mem\u00f3ria espuria de uma mem\u00f3ria associativa.\nNao discutimos neste cap\u00edtulo o modelo BSB devido a semelhanca deste com o modelo de Hopfield.\n7.1\tCapacidade de Armazenamento\nA capacidade de armazenamento, denotada por Ca, e uma funcao que diz a probabilidade de armazenarmos um conjunto de mem\u00f3rias fundamentais numa mem\u00f3ria associativa. Esta funcao depende do mapeamento associativo (mem\u00f3ria associativa), da dimensao dos padr\u00f5es de entrada (n), da di-mensao dos padr\u00f5es de sa\u00edda (m) e do numero de mem\u00f3rias fundamentais (k). Usaremos a notacao Ca(Mem\u00f3ria, n, m; k) no caso heteroassociativo e Ca(Mem\u00f3ria, n; k) no caso auto-associativo.\nSeja\nU = {(X, Y) : X = [x1, x2,...] G , Y = [y1, y2,...] G R},\t(7.1)\no conjunto dos pares (X, Y) de matrizes X (gerada aleat\u00f3riamente) com n linhas e infinitas colunas e das matrizes Y (geradas aleat\u00f3riamente) com m linhas e infinitas colunas. Considere a variavel aleat\u00f3ria C : U \u2014> N dada por\nC = max{p G N : Gp(xe) = ye,\t= 1,..., p},\t(7.2)\nonde\te o mapeamento associativo da mem\u00f3ria associativa neural treinada usando os\npares (x^,\t) para \u00a3 = 1,..., p (primeiras p colunas de X e Y).\nDefini\u00e7\u00e3o 7.1.1 (Capacidade de Armazenamento). Considere o espaco amostral Q definido na equacao (7.1) e a variavel aleat\u00f3ria C definida pela equacao (7.2). A capacidade de armazenamento de uma mem\u00f3ria associativa e a funcao dada pela equacao\nCa(Mem\u00f3ria, m, n; k) := 1 \u2014 Fc(k) = Pr (C > k),\t(7.3)\nonde Fc e a funcao de distribui\u00e7\u00e3o da variavel aleat\u00f3ria C.\nNote que Pr (C > k) e a probabilidade de Gk(x^) = y^ para todo \u00a3 &lt;k. Logo,\nCa(Mem\u00f3ria, m, n; k) = Pr (Gfc(x\u20ac) = y\u20ac, \u00a3 = 1,..., k) ,\t(7.4)\nonde e o mapeamento associativo obtido ap\u00f3s armazenar os pares (x^, y^) para \u00a3 = 1,..., k.\nPodemos estimar a capacidade de armazenamento de uma mem\u00f3ria associativa usando o conceito de fun\u00e7ao de distribui\u00e7\u00e3o emp\u00edrica em conjunto com o teorema de Glivenko-Cantelli [10]. A funcao de distribuicao emp\u00edrica para variaveis aleat\u00f3rias C1, C2,..., Cs e a funcao FE (x) dada por\ns\n1s\nFE (x) = - 'S Ind [Ci &lt;x],\ns\n1=1\nonde Ind e a funcao indicadora dada por\nInd[Ci &lt;x] = {\nse Ci &lt;x,\ncaso contrario,\n(7.5)\n(7.6)\nO teorema de Glivenko-Cantelli afirma que se C1, C2,... sao variaveis aleat\u00f3rias independentes \u00e7om uma fun\u00e7ao de distribui\u00e7ao \u00e7omum FC, entao supx |FE (x) \u2014 FC (x)| \u2014> 0 \u00e7om probabilidade 1. O teorema de Glivenko-Cantelli impli\u00e7a que, \u00e7om probabilidade 1, lims FE(x) = FC(x), para \u00e7ada x onde FC e \u00e7ont\u00ednua. Assim, \u00e7om probabilidade 1, teremos\n1s\nCa (Mem\u00f3ria, n; k) = 1 - lim FE(k) = lim (1 - FE(k)) = lim - \\ Ind [Ci > k].\t(7.7)\ns\ts\ts s\ni=1\nO \u00e7on\u00e7eito da \u00e7apa\u00e7idade de armazenamento foi inspirado no \u00e7on\u00e7eito de \u00e7apa\u00e7idade absoluta introduzido por M\u00e7Elie\u00e7e et. al. em [56]. A capadidade absoluta e definida \u00e7omo max{x G N : Ca(Memoria, m, n; x) > 1 \u2014 e}, \u00e7om e > 0 pequeno. Por exemplo, o teorema 5.1.2 apresenta a \u00e7apa\u00e7idade absoluta da rede de Hopfield e a \u00e7onje\u00e7tura 5.2.1 apresenta a \u00e7apa\u00e7idade absoluta da BAM. A\u00e7reditamos que existe uma perda de informa\u00e7ao na medida \u00e7apa\u00e7idade absoluta \u00e7omo apresentado no exemplo abaixo.\nExemplo 7.1.1. Considere duas memorias asso\u00e7iativas A e B. Na figura 7.1 apresentamos \u00e7om linha \u00e7ont\u00ednua a \u00e7apa\u00e7idade de armazenamento da memoria asso\u00e7iativa A e \u00e7om linha tra\u00e7ejada a \u00e7apa\u00e7idade de armazenamento da memoria asso\u00e7iativa B. A linha verti\u00e7al pontilhada representa a \u00e7apa\u00e7idade absoluta da memoria asso\u00e7iativa A e a linha verti\u00e7al \u00e7om ponto-tra\u00e7o representa a \u00e7apa\u00e7idade\nFig. 7.1: Capacidade de armazenamento das mem\u00f3rias associativas A e B do exemplo 7.1.1. A linha cont\u00ednua representa capacidade de armazenamento da mem\u00f3ria associativa A e a linha tracejada representa a capacidade de armazenamento da mem\u00f3ria associativa B. A linha vertical com pontilhada representa a capacidade absoluta da mem\u00f3ria associativa A e a linha vertical com ponto-traco representa a capacidade de armazenamento da mem\u00f3ria associativa B.\nde armazenamento da mem\u00f3ria associativa B. Note que a mem\u00f3ria associativa A tem probabilidade 1 de armazenar um conjunto com menos de 33 mem\u00f3rias fundamentais, mas tem probabilidade pr\u00f3xima de zero para armazenar um conjunto com mais de 70 mem\u00f3rias fundamentais. Por outro lado, a mem\u00f3ria associativa B tem probabilidades 0, 911 e 0, 9 de armazenar um conjunto com 33 e 90 mem\u00f3rias fundamentais, respectivamente. Neste exemplo, a capacidade absoluta da mem\u00f3ria A e maior que a capacidade absoluta da memora B. Entretanto, este resultado nao e suficiente para afirmar que a mem\u00f3ria A pode armazenar mais padr\u00f5es que B. De fato, temos probabilidade 0, 9 de armazenar um conjunto com 90 mem\u00f3rias fundamentais em B. Por outro lado, certamente nao poderemos armazenar o mesmo conjunto em A.\n7.1.1\tMemoria Associativa de Hopfield\nSeja X = [x1,..., xk] G {-1,1}nxk a matriz das mem\u00f3rias fundamentais gerada aleatoriamente com distribuicao uniforme onde Pr(xf = 1) = 1/2 para todo i = 1,..., n e \u00c7 = 1,... ,k. Daniel Amit mostrou em [3] que\nPr(x1 = sinal(W x1)^) = 1\n1 + erf\n5\n(7.8)\nonde erf e a funcao erro definida pela equacao\n2 [x +2 erf(x) =\t1 - dt.\no\nLogo, a capacidade de armazenamento da mem\u00f3ria associativa de Hopfield e\nCa (MA Hofield, n; k)\nPr (sinal(Wx^) = x^, \u00a3 = 1,..., k)\nPr\n(smal(W x* )i = xf)f\n{2\n1 + erf\n(7.9)\n(7.10)\n(7.11)\n(7.12)\nNa figura 5.1 apresentamos o gr\u00e1fico da capacidade de armazenamento pelo n\u00famero de mem\u00f3rias fundamentais. A linha tracejada representa a capacidade de armazenamento obtida pela equacao (7.12) e a linha cont\u00ednua com \u25a1 representa a capacidade de armazenamento estimada usando a equacao (7.7). Neste exemplo usamos n = 100 e s = 1000 para estimar a funcao de distribuicao emp\u00edrica.\n7.1.2\tMemoria Associativa Bidirecional\nSubstituindo o resultado da equacao (5.17) na equacao (5.23) obtemos\nPr (y\u00a3 = sinal (Wx^), V\u00a3 G {1,..., k})\nAnalogamente,\nPr (X = sinal (WT, V\u00a3 G {1,..., k})\n(7.13)\n(7.14)\nLogo,\n1\n1\nCa (BAM, n, m; k) =\n1 + erf\n1 + erf\n(7.15)\nNa figura 5.5 apresentamos com linha tracejada a capacidade de armazenamento da BAM dada pela equacao (7.15) e com linha cont\u00ednua com \u25a1 a capacidade de armazenamento estimada usando a equacao (7.7). Neste exemplo tomamos n = 100, m = 80 e usamos s = 1000 para calcular a funcao de distribuicao emp\u00edrica.\n7.1.3\tMemoria Associativa de Personnaz\nA matriz dos pesos sinapticos da mem\u00f3ria associativa de Personnaz e obtida atraves da equacao (4.8) que sempre tera solucao no caso auto-associativo (W = I e uma solucao ). Logo, a capacidade de armazenamento desta mem\u00f3ria associativa e\nCa (MA Personnaz, n; k) = 1.\n(7.16)\n7.1.4\tMem\u00f3ria Associativa de Kanter-Sompolinsky\nA proposi\u00e7\u00e3o 3 garante que todas as recorda\u00e7\u00f5es fundamentais s\u00e3o pontos fixos da mem\u00f3ria associativa de Kanter-Sompolinsky. Portanto\nCa (MA Kanter, n; k) = 1.\n(7.17)\n7.1.5\tMem\u00f3ria Associativa Bidirecional Assim\u00e9trica\nAs matrizes dos pesos sinapticos da ABAM sao obtidos usando o armazenamento por proje\u00e7ao. Pelo teorema 4.2.1, o erro da dependencia linear sera nulo somente se o vetores x1,..., xk forem linearmente independentes. Entretanto, a probabilidade de um conjunto {x1,..., xk} ser linearmente independente e 1 se k &lt;n e 0 se k > n. Analogamente para os vetores y1,..., yk. Podemos afirmar que\nCa (ABAM, n, m; k) = 1 \u2014 f (k \u2014 (n A m)),\t(7.18)\nonde f e a funcao limiar definida na equacao (6.41).\n7.1.6\tMemoria Associativa com Capacidade Exponencial\nChiueh e Goodman mostraram em [15] que\nxn >] 4^ )\nPr\n&lt;, ,\nV 4nT\n(7.19)\npara n G {1,..., k}, i G {1,..., k} e T grande. Logo,\nCa (ECAM, n, m; k) = Pr ^x^ = sinal exp [&lt;x^, xn >] x^ , G {1,..., k}^(7.20)\nz\tT x nk\n> V =cta\n(7.21)\nonde c e uma constante pr\u00f3xima de 1.\n7.1.7\tMemoria Associativa Bidirecional com Capacidade Exponencial\nPodemos interpretar a BECAM como uma ECAM usando a equacao (5.50). Conclu\u00edmos que a capacidade de armazenamento da BECAM satisfaz\nCa (BECAM, n, m; k) > ck(n+m),\n(7.22)\nonde c e uma constante pr\u00f3xima de 1.\n7.1.8\tMemorias Associativa Morfol\u00f3gicas\nNo teorema 6.2.1 mostramos que WXX 0 X = MXX El X = X. Logo, a capacidade de armazenamento das mem\u00f3rias auto-associativas morfol\u00f3gicas WXX e MXX e\nCa (WXX, n; k) = Ca (MXX, n: k) = 1-\t(7.23)\nPelo teorema 6.1.3 conclu\u00edmos que\nCa (Wxy, n, m; k) = Ca (Mxy , n, m; k)\nV9 _ 3k-1 + 1 4 V\t\u00edk-1 )\n(I\nk-1\nn- 1\n4(' (3)\u201c)\nk-1\n\u00ae\u201c)\nn-n mk\n(7.24)\nNa figura 6.27 apresentamos o gr\u00e1fico da capacidade de armazenamento pelo n\u00famero de mem\u00f3rias fundamentais armazenadas. Este grafico foi constru\u00eddo tomando n = 100, m = 80 e repetindo 1000 vezes cada experimento para obter as probabilidades. As linhas cont\u00ednuas com y e A representam as estimativas da capacidade de armazenamento atraves da equacao (7.7) para as mem\u00f3rias associativas WXY e MXY, respectivamente. A linha tracejada reprensenta a estimativa dada pela equacao (7.24).\n7.1.9\tMem\u00f3ria Associativa Morfol\u00f3gica de Duas Camadas\nO teorema 6.4.1 afirma que WZY 0 f \u00cd-V'2 0 x) = sempre que MXX 0 x =\t. Como\nWXX El xf = xf para todo \u00a3 = 1,..., k, entao a capacidade de armazenamento das mem\u00f3rias associativas morfol\u00f3gicas de duas camadas e\nCa (MAM Duas Camadas, n, m; k) = 1.\n(7.25)\n7.2\tDistribui\u00e7\u00e3o da Informa\u00e7\u00e3o\nA Fun\u00e7\u00e3o Distribui\u00e7\u00e3o da Informa\u00e7\u00e3o (FDI) e uma medida da distribuyo da informacao armazenada numa mem\u00f3ria associatva neural. Esta medida e uma funcao que depende da mem\u00f3ria, da dimensao dos padr\u00f5es de entrada e sa\u00edda, do numero de mem\u00f3rias fundamentais armazenadas e da porcentagem do numero das conex\u00f5es sinapticas exclu\u00eddas da mem\u00f3ria associativa neural.\nConsidere o espaco amostral Q = {(X, Y )|X = [x1,..., xk ] G Rnxk, Y = [y1,..., yk ] g Rmxk} e defina sobre Q a variavel aleat\u00f3ria D : Q \u2014> [0,100] dada por\nD = max{x G [0, 100] : GjX(x^) = y^, V\u00a3 = 1,..., k},\t(7.26)\nonde G\u00a3 :\te o mapeamento associativo da mem\u00f3ria associativa neural treinada com as\nmem\u00f3rias fundamentais (x^, y^), \u00a3 = 1,..., k e com x % do numero total de conex\u00f5es sinapticas exclu\u00eddas (primeiro treinamos a mem\u00f3ria associativa e depois exclu\u00edmos aleatoriamente as conex\u00f5es sinpaticas). Note que Gk = Gk. e o mapeamento associativo com todas as conex\u00f5es sinapticas.\nDefinic\u00e3o 7.2.1 (Func\u00e3o da Distribui\u00e7\u00e3o da Informac\u00e3o). A fun\u00e7\u00e3o da distribui\u00e7\u00e3o da informa\u00e7\u00e3o (FDI) de uma memoria associativa com k padroes armazenados e\nD(Memoria, n, m, k; x) : = 1 \u2014 FD(x) = Pr(D > x)\t(7.27)\n= Pr (GX (x' )= y' ,\u00c7 =1,...,k) ,\t(7.28)\nonde FD e a funcao de distribuyo da variavel aleatoria D definida na equacao (7.26).\nUsamos o conceito de funcao de distribuyo emp\u00edrica e o teorema de Glivenko-Cantelli para estimar a distribuyo da informacao de uma memoria associativa neural. Nas figuras 7.2 e 7.3 apresentamos a FDI emp\u00edrica das memorias associativas binarias apresentadas nesta dissertacao. As memorias associativas binarias foram treinadas com 6 padroes gerados aleatoriamente. Nos experimentos computacionais usamos n = 100, m = 80 e estimamos a FDI realizando 100 simulates.\nNa figura 7.2 apresentamos a FDI emp\u00edrica para o caso auto-associativo. A linha com o representa a ECAM e a linha com x representa a memoria associativa morfologica de duas camadas. Note que estes dois modelos forneceram a mesma FDI emp\u00edrica. A linha com \u25a1 representa a memoria auto-associativa morfologica WXX. A memoria auto-associativa MXX produziu um resultado semelhante e nao foi apresentada no grafico. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a memoria associativa de Personnaz apresentou a maior FDI emp\u00edrica ponto a ponto. A informacao armazenada na ECAM e na memoria associativa morfol\u00f3gica de duas camadas nao e bem distribuida nas conexoes sinapticas pois estas duas memorias apresentaram a menor FDI emp\u00edrica ponto a ponto.\nNa figura 7.3 apresentamos a FDI emp\u00edrica para o caso heteroassociativo bina\u00edrio. A linha com o representa a BECAM e a linha com x representa a memoria associativa morfologica de duas camadas. A FDI emp\u00edrica destes dois modelos coincidem. A linha com \u25a1 representa a memoria associativa morfologica WXY. Note que a memoria associativa morfologica WXY teve uma probabilidade 0, 33 de armazenar todas as memorias fundamentais como pontos fixos. Este resultado confere com o grafico da capacidade de armazenamento discutido na secao anterior. A memoria associativa MXY produziu um resultado semelhante. A linha com * representa a BAM e a linha com A representa a ABAM. Note que a ABAM apresentou a maior FDI emp\u00edrica ponto a ponto.\n7.3\tRaio de Atra\u00e7ao\nO raio da bacia de atracao, ou simplesmente, raio de atra\u00e7\u00e3o e uma medida para a tolerancia a ru\u00eddo de uma memoria associativa. Este conceito foi usado empiricamente por Kanter-Sompolinsky em [44]. Nesta secao fornecemos uma definyo rigorosa deste conceito.\nDefinicao 7.3.1 (Raio de Atracao). Sejam Q = {(X, Y) : X = [x1,..., xk] E Rnxk, Y = [y1,..., yk] E Rmxk} e Rk : Q \u2014> R a variavel aleatoria\nRk = sup{r E R : d(x, x1) &lt;r, Gk(x) = y1},\t(7.29)\nonde Gk : Rn \u2014> Rm e o mapeamento associativo da memoria treinada com as memorias fundamentais (x', y'), para \u00c7 = 1,..., k, e d : Rn x Rn \u2014> [0, +to) e uma metrica. O raio de atracao de\nFig. 7.2: FDI emp\u00edrica das memorias auto-associativa neurais pela porcentagem de conexoes sinapticas exclu\u00eddas. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM (\u25a1), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).\nFig. 7.3: FDI emp\u00edrica das memorias heteroassociativa neurais pela porcentagem de conexoes sinapticas exclu\u00eddas. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM (\u25a1), BAM (*) e ABAM (A).\numa memoria associativa sera\np(Memoria, m, n; k) := E(Rk),\t(7.30)\nonde E(Rk) representa a esperanca da variavel aleatoria Rk.\nNa equacao (7.29), se x e uma versao ruidosa de x1 com d(x, x1) &lt;Rk, entao o padrao recordado pela memoria e y1, que e o mesmo padrao recordado quando apresentamos a memoria fundamental x1. No caso binario, podemos substituir sup por max pois d(x, x1) assume somente valores discretos. Nesta dissertacao usamos como metrica a distancia de Hamming definida como sendo o n\u00famero de componentes distintas de dois padroes binarios ou bipolares.\nNa pratica estimamos Rk da seguinte forma:\n1.\tTome x = x1,\n2.\tEnquanto Gk (x) = y1 faca:\n(a)\tEscolha aleatoriamente um \u00edndice i G {1,..., n} que ainda nao foi escolhido,\n(b)\tDefina x\u00bb = x\u201e- no caso bipolar (ou x\u00bb = 1 \u2014 x\u00bb no caso binario).\n3.\tDefina Rk = dH(x, x1) \u2014 1.\nO procedimento acima fornece um valor Rk maior que o valor teorico definido na equacao (7.29). Entretanto, esta diferenca e irrelevante pois usaremos Rk somente para comparar os modelos de memorias associativas binarias ou bipolares e usaremos sempre o procedimento descrito acima. Tendo Rk, podemos estimar o raio de atracao usando a lei dos grandes numeros.\nLembre-se que o padrao recordado por uma memoria associativa dinamica e obtido somente apos a convergencia para um ponto fixo e a recursividade da fase de recordacao esta implicita no mapeamento associativo da memoria.\nSabemos que a memoria associativa morfologica Mxy e a memoria associativa morfologica de duas camadas apresentam toler\u00e2ncia a ru\u00eddo somente se o padr\u00e2o-chave x > x1. Por esta razao impomos x1 = 0 na hora de gerar as memorias fundamentais (xf, yf), para \u00a3 = 1,..., k. Analogamente, impomos x1 = 1 para a memoria associativa morfologica WXY e a versao dual da memoria associativa de duas camadas. Portanto, o raio de atracao obtido para as memorias associativas morfologicas so fara sentido para padroes corrompidos somente com ru\u00eddo dilativo ou erosivo.\nNa figura 7.4 apresentamos o grafico do raio de atracao pelo ni\u00edmero de memorias fundamentais armazenadas nas memorias auto-associativas bipolares e binarias discutidas nesta dissertacao. Os graficos foram obtidos usando xf G {\u20141,1}100 (ou xf G {0,1}100) e calculando a media apos 100 simulacoes. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a ECAM. Estes dois modelos possuem os maiores raios de atracao. A linha com 0 representa a memoria associativa morfologica WXX. A memoria associativa Mxx produziu um resultado semelhante e nao foi apresentada na figura 7.4. Note que o raio de atracao da memoria associativa morfologica WXX (e Mxx) apresentou um grande deca\u00edmento. Este resultado e uma con-sequencia do teorema 6.3.2. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a memoria associativa de Kanter-Sompolinsky apresentou um raio\nFig. 7.4: Raio de atra\u00e7\u00e3o das memorias auto-associativas bin\u00e1rias pelo n\u00famero de mem\u00f3rias fundamentais. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM Wxx (\u25a1), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V). As linhas tracejadas com x e \u25a1 representam as mem\u00f3rias associativas morfol\u00f3gicas de duas camadas e de camada \u00fanica, respectivamente, sem impor x1 = 0 ou x1 = 1.\nde atra\u00e7\u00e3o maior que o raio de atra\u00e7\u00e3o da mem\u00f3ria associativa de Personnaz. Esta \u00e9 a vantagem de impor wu = 0, para i = 1,..., n [44]. Note que a mem\u00f3ria associativa morfol\u00f3gica de duas camadas apresentou o maior raio de atracao, entretanto, sua correcao de erro e limidada para mem\u00f3rias-chave x &lt;x1.0 segundo maior raio de atracao foi da ECAM que e valido para ambos os tipos de ru\u00eddo, di-lativo e erosivo. As linhas tracejadas com x e \u25a1 representam as mem\u00f3rias associativas morfol\u00f3gicas de duas camadas e de camada ilnica, respectivamente, sem impor x1 = 0 ou x1 = 1. Note que ambas mem\u00f3rias associativas morfol\u00f3gicas tiveram raio de atracao pr\u00f3ximo de zero.\nNa figura 7.5 apresentamos o grafico do raio de atracao pelo m\u00edmero de mem\u00f3rias fundamentais armazenadas. Os gr\u00e1ficos foram obtidos usando x^ G {-1,1}100 (ou x^ G {0,1}100), G {-1,1}80 (ou y^ G {0,1}80) e calculando a media ap\u00f3s 100 simulates. A linha com x representa a mem\u00f3ria associativa morfol\u00f3gica de duas camadas e a linha com o representa a BECAM. A linha com \u25a1 representa a mem\u00f3ria associativa morfol\u00f3gica WXY. A mem\u00f3ria associativa MXY produziu um resultado semelhante. A linha com * representa a BAM e a linha com A representa a ABAM. Em analogia ao caso auto-associativo, a mem\u00f3ria associativa morfol\u00f3gica de duas camadas apresentou o maior raio de atracao, entretanto, sua correcao de erro e limidada para mem\u00f3rias-chave x &lt;x1. O segundo maior raio de atracao foi da BECAM que e a generalizacao da ECAM para o caso heteroassociativo.\nFig. 7.5: Raio de atra\u00e7\u00e3o das memorias heteroassociativa pelo n\u00famero de mem\u00f3rias fundamentais. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM\t(\u25a1), BAM (*) e ABAM\n(A). As linhas tracejadas com x e \u25a1 representam as mem\u00f3rias associativas morfol\u00f3gicas de duas camadas e de camada \u00fanica, respectivamente, sem impor x1 = 0 ou x1 = 1.\nAs linhas tracejadas com x e \u25a1 representam as mem\u00f3rias associativas morfol\u00f3gicas de duas camadas e de camada Unica, respectivamente, sem impor x1 = 0 ou x1 = 1. Note que ambas mem\u00f3rias associativas morfol\u00f3gicas tiveram raio de atracao pr\u00f3ximo de zero.\n7.4\tMem\u00f3rias Esp\u00farias\nNesta secao medimos a probabilidade de um padr\u00e2o-chave convergir para um padrao que nao faz parte do conjunto das mem\u00f3rias fundamentais de uma mem\u00f3ria associativa treinada com k padr\u00f5es.\nDefini\u00e7\u00e3o 7.4.1 (Probabilidade de Mem\u00f3ria Esp\u00faria, PME). Seja Q = {(X, Y, x) |X = [x1,..., xk] G\nRnxk, Y = [y1,..., yk] G Rm*k, x G }. A probabilidade de uma mem\u00f3ria associativa treinada com k mem\u00f3rias fundamentais convergir para uma mem\u00f3ria espi\u00faria e\nE(Mem\u00f3ria, n, m; k)\t:= Pr (Gk(x)\tG\t{y1,..yk})\t= 1 - Pr (Gk(x)\tG {y1,..., yk})\t, (7.31)\nonde Gk :\te o mapeamento associativo da mem\u00f3ria associativa treinada com as mem\u00f3rias\nfundamentais (x^, y^), \u00a3 = 1,..., k.\nUsaremos a lei dos grandes numeros para estimar a probabilidade de memoria espi\u00edria (PME) de uma memoria associativa neural. Devemos ter um cuidado especial com as memorias associativas morfologicas pois sabemos que a MAM e a MAM de duas camadas sempre fornecerao uma memoria espi\u00edria como resposta se o padrao-chave x &lt;/\\|=1 xf, onde xf, \u00a3 = 1, \u2022 \u2022 \u2022 k sao as memorias fundamentais armazenadas. Para evitar este problemas, definimos x1 = 0. Assim, para todo padrao-chave x G {0,1}n, teremos x > /\\|=1 xf e podemos interpretar x como sendo uma versao corrompida com ru\u00eddo dilativo. Analogamente, a MAM e a versao dual da MAM de duas camadas sempre fornecerao uma memoria espi\u00edria se o padrao-chave x > \\/1=1 xf e definiremos x1 = 1 para evitar este problema.\nNa figura 7.6 apresentamos a PME pelo ni\u00edmero de memorias fundamentais armazenadas nas memorias auto-associativas binarias apresentadas nos cap\u00edtulos 5 e 6. Neste experimento usamos n = 100 e realizamos 1000 simulacoes para calcular as probabilidades emp\u00edricas. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a ECAM. Note que a PME destes dois modelos foi sempre nula. A linha com \u25a1 representa a memoria associativa morfol\u00f3gica . A memoria associativa produziu um resultado semelhante e nao foi apresentada na figura 7.6. Note que a PME da memoria associativa morfologica (e ) tende rapidamente para 1. Este resultado e uma consequencia do teorema 6.3.2 sobre os pontos fixos das memorias auto-associativas morfologicas binarias. A linha com * representa a memoria associativa de Hopfield, a linha com V representa a memoria associativa de Personnaz e a linha com A representa a memoria associativa de Kanter-Sompolinsky. Note que a PME e sempre maior que 1/2 pois o mapeamento associativo destes tr\u00eas ultimos modelo e um mapeamento \u00edmpar e portanto, se xf e um ponto fixo, entao \u2014xf tambem e um pontos fixos da memoria associativa. As memorias associativas morfologicas apresentaram ambas um probabilidade emp\u00edrica proxima de 1 se nao impormos a condicao discutida no par\u00e1grafo anterior sobre o padrao x1.\nNa figura 7.7 apresentamos a probabilidade de memoria espi\u00edria pelo ni\u00edmero de memorias fundamentais armazenadas nas memorias heteroassociativas binarias. Neste experimento tomamos n = 100, m = 80 e efetuamos 1000 simulacoes para calcular as probabilidades emp\u00edricas. A linha com x representa a memoria associativa morfologica de duas camadas e a linha com o representa a BECAM. Ambos modelos apresentaram uma probabilidade de memoria espi\u00edria nula. A linha com \u25a1 representa a memoria associativa morfologica . A linha com * representa a BAM e a linha com A representa a ABAM. A probabilidade de memoria espi\u00edria da BAM e da ABAM e sempre maior que 1/2 porque estes modelos possuem um mapeamento associativo \u00edmpar. Novamente, as memorias associativas morfologicas apresentaram ambas um probabilidade emp\u00edrica proxima de 1 se nao impormos a condicao discutida anteriormente sobre o padrao x1.\n7.5\tEsfor\u00e7o Computacional\nO esfoco computacional pode ser medido pelo ni\u00edmero de operacoes e efetuacoes de funcoes nao lineares realizadas pela memoria associativa neural na fase de armazenamento e na fase de recordacao. Nas memorias associativas dinamicas tambem devemos considerar o ni\u00edmero de iteracoes necessarias para encontrar a sa\u00edda da rede na fase de recordacao.\nNesta secao vamos considerar X = [x1, x2,..., xk] G Rnxk e Y = [y1, y2,..., yk] G Rmxk.\nFig. 7.6: Probabilidade de mem\u00f3ria esp\u00faria pelo n\u00famero de mem\u00f3rias fundamentais. As linhas representam: ECAM (o), MAM Duas Camadas (x), MAM WXX (\u25a1), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).\nFig. 7.7: Probabilidade de mem\u00f3ria esp\u00faria pelo n\u00famero de mem\u00f3rias fundamentais armazenadas. As linhas representam: BECAM (o), MAM Duas Camadas (x), MAM WXY (\u25a1), BAM (*) e ABAM (A).\n7.5.1\tNumero de Opera\u00e7\u00f5es na Fase de Armazenamento\nA matriz de pesos sin\u00e1picos da BAM \u00e9 W = YXT. Logo, ser\u00e3o necess\u00e1rias (2k \u2014 1)mn opera\u00e7\u00f5es de soma e de multiplica\u00e7ao para obtermos W. Em particular, a matriz dos pesos sinapticos da memoria associativa de Hopfield requer (2k \u2014 1)n2 operacoes de soma e multiplicacao.\nNa memoria associativa de Personnaz, a matriz dos pesos sinapticos e W = XX1 = UUT, onde\nX\t= L\u00caVT e a decomposicao SVD reduzida de X (Thin SVD Decomposition). O calculo da matriz U usando o metodo R-SVD requer 6nk2 + 11k3 operacoes e o produto UUT requer (2k \u2014 1)n2 operacoes de soma e multiplicacao [25]. O m\u00edmero total de operacoes de soma e multiplicacao necessarias para obter a matriz dos pesos sinapticos da memoria associativa de Personnaz e 6nk2 + 11 k3 + (2k \u2014 1)n2. Na memoria associativa de Kanter-Sompolisky tomamos W = XX1 e impomos wh = 0 para todo i = 1,..., n. Logo, o numero de operacoes necessarias para encontrar a matriz dos pesos sinapticos da memoria associativa de Kanter-Sompolisky e tambem 6nk2 + 11 k3 + (2k \u2014 1)n2 operacoes de soma e multiplicacao.\nNa ABAM definimos W1 = YX1 e W2 = XY\u00ca O numero de operacoes necessaria para calcular\nXI\tusando a decomposicao SVD reduzida e 6nk2 + 20k3 operacoes. Calculado a decomposicao SVD reduzida X = U\u00caVT, computamos Wi atraves do produto Wi = (y(V\u00ca1)) UT que requer (2m + 1)k2 + 2mnk operacoes de soma e multiplicacao. Analogamente, o calculo da matriz W2 requer 6mk2 + 20k3 operacoes para calcular a decomposicao SVD reduzida de Y e (2n + 1)k2 + 2mnk operacoes para calcular o produto matricial em W2. O numero total de operacoes de soma e multiplicacao necessarias para computar as matrizes de pesos sinapticos da ABAM e 40k3 + 2(4n + 4m + 1)k2 + 4mnk.\nNa BECAM usamos as matrizes das mem\u00f3rias fundamentais como matriz dos pesos sinapticos. Portanto, nenhuma operacao e efetuada na fase de armazenamento desta memoria associativa. Em particular, a ECAM tambem nao efetua nenhuma operacao na fase de armazenamento.\nNas memorias associativas morfol\u00f3gicas tomamos WXY = Y El X* e MXY = Y 0 X*. Considerando as operacoes de maximo ou m\u00ednimo como operacoes binarias, serao necessarias kmn operacoes de soma e (k\u20141)mn operacoes de m\u00ednimo para calcular WXY ou kmn operacoes de soma e (k\u20141)mn operacoes de maximo para calcular MXY. O numero total de operacoes necessarias para calcular a matriz dos pesos sinapticos de uma memoria associativa morfol\u00f3gica e (2k \u2014 1)mn operacoes de maximo ou m\u00ednimo e soma.\nNas memorias associativas morfogicas de duas camadas precisamos da matriz WZY que requer (2k \u2014 1)pm operacoes de m\u00ednimo e soma. A matriz Mj^Y = MXZ 0 MXX que requer (2k \u2014 1)pn operacoes para o calculo de MXZ, (2k \u2014 1)n2 operacoes para o calculo de MXX e (2n \u2014 1)pn operacoes para o calculo do produto MXZ 0 MXX. Como as operacoes de maximo e m\u00ednimo requerem o mesmo esfor\u00e7o computacional, o numero total de operacoes necessarias para obter as matrizes dos pesos sinapticos da memoria associativa morfol\u00f3gica de duas camadas sera 2pn(k + n \u2014 1) + (2k \u2014 1)(pm + n2). Nos nossos experimentos computacionas tomamos Z = Ikxk, ou seja, p = k.\nNa tabela 7.1 apresentamos um resumo do que foi dito anteriormente.\nNote que o numero de operacoes necessarias para calcular a matriz dos pesos sinapticos da BAM e de uma MAM sao os mesmos. Entretanto, teremos uma esfor\u00e7o computacional menor na fase de armazenamento das MAM pois as operacoes de maximo ou m\u00ednimo e soma requerem um esfor\u00e7o computacional menor que as operacoes de soma e multiplicacao.\nMem\u00f3ria Associativa\tNumero Aproximado de Operacoes\tTipo das Operacoes\nMA Hopfield\t(2k \u2014 1)n2\tsoma e multiplicacao\nBAM\t(2k \u2014 1)nm\tsoma e multiplicacao\nMA Personnaz\t6nk2 + 11k3 + (2k \u2014 1)n2\tsoma e multiplicacao\nMA Kanter-Sompolinsky\t6nk2 + 11k3 + (2k \u2014 1)n2\tsoma e multiplicacao\nABAM\t40k3 + 2(4n + 4m + 1)k2 + 4mnk\tsoma e multiplicacao\nECAM\t\u2014\t\u2014\nBECAM\t\u2014\t\u2014\nMAM\t(2k \u2014 1)nm\tmaximo ou m\u00ednimo e soma\nMAM Duas Camadas\t2pn(k + n \u2014 1) + (2k \u2014 1)(pm + n2)\tmaximo ou m\u00ednimo e soma\nTab. 7.1: Esfor\u00e7o computacional na fase de armazenamento das memorias associativas neurais.\n7.5.2\tNumero de Opera\u00e7\u00f5es por Itera\u00e7\u00e3o na Fase de Recorda\u00e7\u00e3o\nNa fase de recordacao devemos considerar o esfor\u00e7o computacional realizado pela mem\u00f3ria por iteracao e o numero de iteracoes necessario para recordar um padrao. Discutiremos primeiro o numero de operacoes por iteracao, depois apresentaremos uma estimativa do numero de iteracoes das mem\u00f3rias associativas dinamicas. O esfor\u00e7o computacional sera descrito pelo numero de operacoes efetuadas e o numero de chamadas de func\u00f3es nao-lineares por iteracao na mem\u00f3ria associativa.\nNas mem\u00f3rias associativas de Hopfield, Personnaz e Kanter-Sompolinsky realizamos um produto matriz-vetor e depois aplicamos a funcao sinal em cada componente do resultado do produto matriz-vetor. Estas operacoes requerem (2n \u2014 1)n operacoes de soma e multiplicacao e n efetuac\u00f3es da funcao sinal por iteracao.\nNa ABAM computamos sinal(Wix) e sinal(W2y) por iteracao. Na BAM realizamos os mesmos calculos com W1 = W e W2 = WT na BAM. O total de operacoes efetuadas por iteracao na BAM ou na ABAM e 4mn \u2014 (m + n) operacoes de soma e multiplicacao e m + n efetuac\u00f3es da funcao sinal.\nNa ECAM computamos sinal(X exp(XTx)). Por iteracao realizamos 4kn \u2014 (k + n) operacoes, efetuamos a funcao exponencial k vezes e a efetuamos a funcao sinal n vezes. Na BECAM, computamos sinal(Y exp(XTx)) e sinal(X exp(YTy)) realizando 4k(n + m) \u2014 (2k + m + n) operacoes de soma e multiplicacao, 2k efetuac\u00f3es da funcao exponencial e m + n efetuac\u00f3es da funcao sinal por iteracao.\nNas mem\u00f3rias associativas morfol\u00f3gicas realizamos o produto WXY 0 x ou MXY El y. Ambos requerem (2n \u2014 1)m operacoes de maximo ou m\u00ednimo e soma. O padrao recordado pela mem\u00f3ria associativa morfol\u00f3gica de duas camada e dado por WZY 0 f (M%Z El x), onde Mxz \u00e9 uma matriz p x n e WZY e uma matriz m x p. Logo, na fase de armazenamento das mem\u00f3rias associativas morfol\u00f3gicas de duas camadas efetuamos 2p(m + n) \u2014 (p + m) operacoes de maximo ou m\u00ednimo e soma, e computamos a funcao f definida na equacao (6.41) p vezes. Nos nossos experimentos tomamos Z = Ikxk (p = k).\nNa tabela 7.2 apresentamos o que foi dito anteriormente. A segunda coluna da tabela 7.2 representa o m\u00edmero total de operacoes binarias incluindo soma, produto, maximo e m\u00ednimo. A terceira coluna contem o numero de efetuac\u00f3es da funcao sinal ou da funcao f definida na equacao (6.41). A\nMem\u00f3ria Associativa\tN. Operacoes\tf (x) ou sinal(x)\texp(x)\nMA Hopfield\t2n2 \u2014 n\tn\t\u2014\nBAM\t4nm \u2014 (n + m)\tm + n\t\u2014\nMA Personnaz\t2n2 \u2014 n\tn\t\u2014\nMA Kanter-Sompolinsky\t2n2 \u2014 n\tn\t\u2014\nABAM\t4nm \u2014 (m + n)\tm + n\t\u2014\nECAM\t4kn \u2014 (k + n)\tn\tk\nBECAM\t4k(m + n) \u2014 (2k + m + n)\tm + n\t2k\nMAM\t2mn \u2014 m\t\u2014\t\u2014\nMAM Duas Camadas\t2p(m + n) \u2014 (p + m)\tk\t\u2014\nTab. 7.2: Esfor\u00e7o computacional por iteracao na fase de recordacao das memorias associativas neu-rais.\nquarta coluna representa o numero de eleluacoes da funcao exponencial.\n7.5.3\tN\u00famero de Itera\u00e7\u00f5es na Fase de Recorda\u00e7\u00e3o\nO n\u00famero de itera\u00e7\u00f5es na fase de recorda\u00e7\u00e3o de uma mem\u00f3ria associativa din\u00e2mica treinada com k mem\u00f3rias fundamentais e uma funcao da dimensao dos padr\u00f5es de entrada e sa\u00edda e do n\u00famero de padr\u00f5es armazenados. Precisamente, o n\u00famero de iteracoes na fase de recordacao e definido como sendo a media do n\u00famero de iteracoes necessarias para a convergencia do sistema dinamico para um ponto estacionario quando iniciamos a memoria com um padrao-chave qualquer. As mem\u00f3rias associativas estaticas ser\u00e2o consideraras como sistemas dinamicos que convergem para um ponto estacionario com 1 iteracao.\nNa figura 7.8 apresentamos o grafico do m\u00edmero de iteracoes pelo m\u00edmero de padr\u00f5es armazenados de varias mem\u00f3rias auto-associativas dinamicas. Esta figura foi gerada usando padr\u00f5es com 100 componentes e o grafico foi construido calculando a media ap\u00f3s 1000 simulacoes. A linha marcada com * representa a MA de Hopfield, a linha com A representa a MA de Personnaz, a linha com V representa a MA de Kanter-Sompolinsky e a linha com o representa a ECAM. Note que o m\u00edmero de iteracoes da MA Personnaz e da MA de Kanter-Sompolinsky tendem para 1 quando o numero de mem\u00f3rias fundamentais armazenadas tende para a dimensao do padr\u00f5es armazenados. De fato, W I na MA de Personnaz e W 0 na MA de Kanter-Sompolinky quando k n . Em ambos os casos sinal(Wx) = x para todo padrao-chave x quando k n. Logo, todos os pontos do espaco sao pontos fixos da MA de Personnaz e da MA de Kanter-Sompolinsky quando k n.\nNa figura 7.9 apresentamos o grafico do numero de iteracoes pelo numero de padr\u00f5es armazanados em mem\u00f3rias heteroassociativas dinamicas. Neste exemplo tomamos padr\u00f5es de entrada com 100 componentes, padr\u00f5es de sa\u00edda com 80 componentes e calculamos a media ap\u00f3s 1000 simulacoes. O numero maximo de iteracoes permido foi 1000. A linha com * representa a BAM, a linha com A representa a ABAM e a linha com o representa a BECAM. Note que a ABAM atingiu o numero maximo de iteracoes permitido. Isso mostra que a ABAM pode convergir para um ciclo limite. Lembre-se que nao temos um resultado que garante a convergencia da ABAM. Pelo grafico, a ABAM atingiu o numero maximo de iteracoes para k > 40. Neste caso, podem haver ciclos-limite na ABAM\nFig. 7.8: Numero de iterac\u00f3es na fase de recordacao pelo ni\u00edmero de padr\u00f3es armazenados. As linhas marcadas representam: ECAM (o), MA Hopfield (*), MA Personnaz (A) e MA Kanter (V).\nque impedem a convergencia do padrao-chave para um ponto de equil\u00edbrio.\nIteracoes\nFig. 7.9: Numero de itera\u00e7oes na fase de re\u00e7orda\u00e7ao pelo numero de padroes armazenados. As linhas mar\u00e7adas representam: BECAM (o), BAM (*) e ABAM (A).\nCap\u00edtulo 8\nConclus\u00e3o\nNesta disserta\u00e7\u00e3o apresentamos um estudo comparativo em mem\u00f3rias associativas neurais com \u00eanfase nas mem\u00f3rias associativas morfol\u00f3gicas. Nos concentramos nos modelos de mem\u00f3rias associativas binarias que sao citados frequentemente na literatura de redes neurais.\nNo cap\u00edtulo 1 apresentamos uma revisao hist\u00f3ria e bibliogr\u00e1fica sobre redes neurais, morfologia matematica, algebra de imagens e principalmente sobre mem\u00f3rias associativas neurais. Nos cap\u00edtulos 2 e 3 discutimos conceitos basicos de redes neurais e mem\u00f3rias associativas. Existe uma variedade grande de notac\u00f3es para modelos de mem\u00f3rias associativas neurais e estes cap\u00edtulos basicos esclarecem a notacao usada durante a dissertacao. Adotamos uma notacao matricial comum em ambos livros de algebra linear e redes neurais artificiais, como por exemplo, nas referencias [101] e [33]. As cinco caracter\u00edsticas para um bom desempenho apresentadas por n\u00f3s no cap\u00edtulo 3 foram inspiradas nos trabalhos de Hassoun e Pao [31, 64].\nNo cap\u00edtulo 4 discutimos as mem\u00f3rias associativas lineares. Este cap\u00edtulo tem um objetivo didatico visto que as mem\u00f3rias associativas lineares definem as principais regras de aprendizado utilizadas nas mem\u00f3rias associativas neurais discutidas nesta dissertacao. Verificamos que o armazenamento por correlacao possui limitac\u00f3es devido a interfer\u00eancia cruzada. No armazenamento por projecao temos erro devido a dependencia linear das mem\u00f3rias fundamentais e devido ao ru\u00eddo introduzido no padr\u00e1o-chave. Um m\u00edmero ilimitado de padr\u00f5es podem ser armazenados na OLAM no caso auto-associativo.\nNo cap\u00edtulo 5 discutimos as mem\u00f3rias associativas dinamicas. Este e o maior cap\u00edtulo da dissertacao devido ao grande numero de modelos apresentados na literatura. Para cada modelo apresentamos a arquitetura, regra de aprendizado, exemplos computacionais e uma breve ana\u00edlise sobre a convergencia. A ABAM foi o ilnico modelo que nao possui nenhum resultado garantindo sua convergencia para um ponto fixo. A conjectura 5.2.1 e uma proposta nossa baseada no resultado do artigo de McEliece et. al. [56] e pode ser vista como uma generalizacao do teorema 5.1.2 apresentado para a rede de Hopfield. As mem\u00f3rias associativas de Personnaz e Kanter-Somplinsky sao ambas referidas como \u201crede de Hopfield com armazenamento por projecao\u201d. Verificamos que existem diferencas entre estes dois modelos, principalmente com respeito ao raio de atracao (toler\u00e2ncia a ru\u00eddo). As proposicoes que garantem a convergencia da mem\u00f3ria associativa de Personnaz foram introduzidas por n\u00f3s. A BECAM foi introduzida por n\u00f3s como uma generalizacao da ECAM inspirada na BAM. Todos os resultados relativos a BECAM sao novos. O teorema 5.8.3 e inedito e relaciona o modelo BSB com o modelo de Hopfield.\nNo \u00e7ap\u00edtulo 6 apresentamos um estudo detalhado das memorias asso\u00e7iativas morfologi\u00e7as. Come\u00e7amos dis\u00e7utindo o \u00e7aso hetero-asso\u00e7iativo onde apresentamos a arquitetura, regra de aprendizado, exemplos e teoremas que garantem o armazenamento e a re\u00e7orda\u00e7ao de padroes. O teorema 6.1.3 foi introduzido e demonstrado nesta disserta\u00e7ao. Con\u00e7lu\u00edmos que as memorias asso\u00e7iativas mor-fologi\u00e7as hetero-asso\u00e7iativas de \u00e7amada uni\u00e7a nao sao \u00e7apazes de armazenar muitos padroes binarios e possuem uma toler\u00e2n\u00e7ia a ru\u00eddo restrita a padroes \u00e7orrompidos \u00e7om ru\u00eddo dilatio ou ru\u00eddo erosivo. O \u00e7aso auto-asso\u00e7iativo tambem foi dis\u00e7utido detalhadamente. Mostramos que as memorias auto-asso\u00e7iativas morfologi\u00e7as de \u00e7amada uni\u00e7a podem armazenar um numero ilimitado de padroes, \u00e7onvergem para um ponto fixo \u00e7om uma uni\u00e7a itera\u00e7ao, e tambem apresentam restri\u00e7oes no tipo de ru\u00eddo introduzido nos padroes-\u00e7have. Apresentamos um teorema que \u00e7ara\u00e7teriza todos os pontos fixos das memorias auto-asso\u00e7iativas binarias e verifi\u00e7amos que estas apresentam um grande numero de padroes espilrios. Dis\u00e7utimos brevemente o metodo do nu\u00e7leo e depois introduzimos as memorias asso\u00e7iativas morfologi\u00e7as de duas \u00e7amadas. Terminamos o \u00e7ap\u00edtulo \u00e7om um teorema que \u00e7ara\u00e7teriza os pontos fixos deste ultimo modelo.\nNo \u00e7ap\u00edtulo 7 formalizamos os \u00e7on\u00e7eitos para a medida do desempenho de uma memoria asso-\u00e7iativa e usamos estes \u00e7on\u00e7eitos para \u00e7omparar os varios modelos de memoria asso\u00e7iativa binaria apresentados nos \u00e7ap\u00edtulos 5 e 6. Com base nos resultados obtidos \u00e7on\u00e7lu\u00edmos que:\n\u2022\tAs memorias auto-asso\u00e7iativas morfologi\u00e7as, de Personnaz e Kanter-Sompolinsly apresentaram uma \u00e7apa\u00e7idade de armazenamento \u00e7onstante igual a 1, isto e, podemos armazenar infinitos padroes. A ECAM e a BECAM apresentam uma \u00e7apa\u00e7idade de armazenamento igual a c\ne ck(\u00ab+m) \u00e7om c proximo de 1, respe\u00e7tivamente.\n\u2022\tA fun\u00e7ao de distribui\u00e7ao informa quantos pesos sinapti\u00e7os podemos ex\u00e7luir sem perder a informa\u00e7ao armazenada numa memoria asso\u00e7iativa neural. As memorias asso\u00e7iativas de Kanter-Sompolinsky, Personnaz e Hopfield apresentaram os melhores resultados no \u00e7aso auto-asso\u00e7iativo. A ABAM e a BAM apresentaram os melhores resultados no \u00e7aso heteroasso\u00e7iativo. Os piores resultados para a distribui\u00e7ao da informa\u00e7ao foram obtidas pela ECAM, BECAM e as memorias asso\u00e7iativas de duas \u00e7amadas.\n\u2022\tAs memorias asso\u00e7iativas morfologi\u00e7as de duas \u00e7amadas apresentaram a maior toler\u00e2n\u00e7ia a ru\u00eddo (maior raio de atra\u00e7ao), entretanto, este modelo e restrito a padroes-\u00e7have \u00e7orrompidos \u00e7om ru\u00eddo dilativo ou erosivo. A ECAM e a BECAM sao os modelos mais re\u00e7omendados para padroes-\u00e7have \u00e7orrompidos \u00e7om ambos ru\u00eddo dilativo e erosivo.\n\u2022\tAs memorias asso\u00e7iativas morfologi\u00e7as de duas \u00e7amadas, a ECAM e a BECAM apresentaram as menores probabilidades de \u00e7onvergir para uma memoria esp\u00faria. Lembramos que este resultado e valido para as memorias asso\u00e7iativas morfologi\u00e7as supondo que o padrao-\u00e7have representa uma versao erodida ou dilatada de uma memoria fundamental.\n\u2022\tO esfor\u00e7o \u00e7omputa\u00e7ional depende da dimensao dos padroes de entrada e sa\u00edda (n e m) e do numero de memorias fundamentais armazenadas (k). Se k&lt;&lt;n, m, a ECAM e a BECAM serao os modelos que requerem o menor esfor\u00e7o \u00e7omputa\u00e7ional, supondo que as memorias \u00e7onvergem rapidamente para um ponto-fixo. Se k e proximo de n ou m, entao as memorias asso\u00e7iativas morfologi\u00e7as serao os modelos que efetuam o menor numero de opera\u00e7oes.\nFinalmente, com base nos resultados apresentados no cap\u00edtulo 7, nao podemos afirmar qual e o melhor modelo de memoria associativa neural, pois cada modelo apresenta pontos positivos e negativos. Esta dissertacao de mestrado serve como um guia para a escolha do modelo de memoria associativa que melhor se enquadra a um dado problema. Por exemplo, as memorias associativas morfol\u00f3gicas de duas camadas serao os modelos recomendados para um problema onde queremos armazenar um grande n\u00famero de memorias fundamentais buscando um baixo custo computacional e sabendo-se que os padroes-chave serao versoes corrompidas somente com ru\u00eddo dilativo ou somente com ru\u00eddo erosivo. Lembre-se que no cap\u00edtulo 1 citamos varias referencias contendo aplicacoes de memorias associativas neurais.\nReferencias Bibliogr\u00e1ficas\n[1]\tComputer Vision Group Image Database,\tAvailable at\nhttp://decsai.ugr.es/cvg/index2.php.\n[2]\tAmari, S.-I. Neural theory of association and concept-formulation. Biological Cybernetics 26 (1977), 175-185.\n[3]\tAmit, D. J. Modeling Brain Function-The World of Attractor Neural Network. Cambridge University Press, 1989.\n[4]\tANDERSON, J. A simple neural network generating interactive memory. Mathematical Biosciences 14 (1972), 197-220.\n[5]\tANDERSON, J. An Introduction to Neural Networks. MIT Press, MA, 1995.\n[6]\tAnderson, J., Silverstein, J., Ritz, S., and Jones, R. Distinctive features, categorical perception, and probability learning: Some applications of a neural model. Psychology Review 84 (July 1977), 413-415.\n[7]\tANDERSON, J. A., AND Rosenfeld , E. Neurocomputing: Foundations of Research, vol. 1. MIT Press, Cambridge, MA, 1989.\n[8]\tBeale, R., AND Fiesler , E., Eds. Handbook of Neural Computation. Institue of Physics Publishing and Oxford University Press, 1997.\n[9]\tBhaya, A., Kaszkurewicz, E., and Kozyakin, V. Existence and stability of a unique equilibrium in continuous-valued discrete-time asynchronous hopfield neural networks. IEEE Trans. on Neural Networks 7, 3 (May 1996), 620 - 628.\n[10]\tBillingsley, P. Probability and Measure, 2 ed. John Wiley and Sons, 1986.\n[11]\tBirkhoff, G. Lattice Theory, 3 ed. American Mathematical Society, Providence, 1993.\n[12]\tBrunak, S., AND Lautrup, B. Neural Networks: Computers with Intuition. World Scientific, 1990.\n[13]\tBRYSON, A., AND Ho, Y. Applied Optimal Control, rev ed edition ed. John Wiley and Sons, October 1979.\n[14]\tCasasent, D., and Telfer, B. Associative memory synthesis, performance, storage capacity, and updating: new heteroassociative memory results. SPIE, Int. Robots Comput. Vision 848 (1987), 313-333.\n[15]\tChiueh, T., and Goodman, R. Recurrent correlation associative memories. IEEE Trans. on Neural Networks 2 (Feb. 1991), 275-284.\n[16]\tChiueh, T., AND Goodman, R. Recurrent Correlation Associative Memories and their VLSI Implementation. In Hassoun [31], 1993, ch. 16, pp. 276-287.\n[17]\tCollier, R. J. Some current views on holography. IEEE Spectrum 3 (July 1966), 67-74.\n[18]\tCUNINGHAME-G REEN, R. Minimax Algebra: Lecture Notes in Economics and Mathematical Systems 166. Springer-Verlag, New York, 1979.\n[19]\tCUNINGHAME-GREEN, R. Minimax algebra and applications. In Advances in Imaging and Electron Physics, P. Hawkes, Ed., vol. 90. Academic Press, New York, NY, 1995, pp. 1-121.\n[20]\tDavidson, J., and Ritter, G. A theory of morphological neural networks. In Digital Optical Computing II (July 1990), vol. 1215 of Proceedings of SPIE, pp. 378-388.\n[21]\tFu, L. Neural Networks in Computer Intelligence. McGraw-Hill, New York, NY, 1994.\n[22]\tGabor, D. Associative holographic memories. IBM J. Res. Develop 13 (1969), 156-159.\n[23]\tGolden, R. M. The brain-state-in-a-box neural model is a gradient descent algorithm. Journal of Mathematical Psychology 30 (1986), 73-80.\n[24]\tGolden, R. M. Stability and optimization analysis of the generalized brain-state-in-a-box neural network model. Journal of Mathematical Psychology 30 (1993), 73-80.\n[25]\tGolub, G. H., AND loan, C. F. V. Matrix Computations, 3rd ed. Johns Hopkins University Press, 1996.\n[26]\tGrain a, M., Gallego, J., Torrealdea, F. J., and D\u2019Anjou, A. On the application of associative morphological memories to hyperspectral image analysis. Lecture Notes in Computer Science 2687 (2003), 567-574.\n[27]\tHADWIGER, H. Vorlesungen Uber Inhalt, Oberflache und Isoperimetrie. Springer-Verlag, Berlin, 1957.\n[28]\tHagan, M., Demuth, H., and Beale, M. Neural Network Desing. PWS Publishing Company, Boston, 1996.\n[29]\tHanson, S., and Kegl, J. Parnip: a connectionist network that learns natural language grammar from exposure to natural language sentences. In Proc. 9th Annu. Conf. Cognitive Science (1987), pp. 106-119.\n[30]\tHASSOUN, M. H. Dynamic heteroassociative neural memories. Neural Networks 2, 4 (1989), 275-287.\n[31]\tHASSOUN, M. H. Dynamic associative neural memories. In Associative Neural Memories: Theory and Implementation, M. H. Hassoun, Ed. Oxford University Press, Oxford, U.K., 1993.\n[32]\tHASSOUN, M. H. Fundamentals of Artificial Neural Networks. MIT Press, Cambridge, MA, 1995.\n[33]\tHAYKIN, S. Neural Networks: A Comprehensive Foundation. Prentice Hall, Upper Saddle River, NJ, 1999.\n[34]\tHebb, D. The Organization of Behavior. John Wiley &amp; Sons, New York, 1949.\n[35]\tHEIJMANS, H. Morphological Image Operators. Academic Press, New York, NY, 1994.\n[36]\tHIRSCH, M. W. Convergent activation dynamics in continuous time networks. Neural Networks 2, 5 (1989), 331-349.\n[37]\tHOPFIELD, J. Neurons with graded response have collective computational properties like those of two-state neurons. Proceedings of the National Academy of Sciences 81 (May 1984), 3088-3092.\n[38]\tHopfield, J., and Tank, D. Neural computation of decisions in optimization problems. Biological Cybernetics (1985).\n[39]\tHopfield, J., and Tank, D. Computing with neural circuits: A model. Proceedings of the National Academy of Sciences 233 (August 1986), 625-633.\n[40]\tHopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences 79 (Apr. 1982), 2554-2558.\n[41]\tHUI, S., Lillo, W. E., AND Zak, S. H. Dynamics and Stability Analysis of the Brain-State-in-a-Box (BSB) Neural Models. In Hassoun [31], 1993, ch. 11, pp. 212-224.\n[42]\tHUI, S., AND ZAK, S. H. Dynamical analysis of the brain-state-in-a-box (bsb) neural models. IEEE Transactions on Neural Networks 3, 1 (January 1992), 86-94.\n[43]\tJAMES, B. Probabilidade: Um Curso em Nivel Intermedidrio. Publicacao IMPA, 1996.\n[44]\tKANTER, I., AND Sompolinsky, H. Associative recall of memory without errors. Physical Review 35 (1987), 380-392.\n[45]\tKAPPEN, B., AND Gielen, S. Neural Networks: Best Practice in Europe. Progress in Neural Processing, 8. World Scientific, Amsterdam, 1997.\n[46]\tKawamoto, A. H., and Anderson, J. A. A neural network model of multistable perception. Acta Psychologica 59 (1985), 35-65.\n[47]\tKOHONEN, T. Correlation matrix memory. IEEE Transactions on Computers C-21 (1972), 353-359.\n[48]\tKOHONEN, T. Associative Memory - A System Theoric Approach. Berlin: Springer-Verlag, 1977.\n[49]\tKOHONEN, T. Self-Organization and Associative Memory. Springer Verlag, 1984.\n[50]\tKohonen, T., AND Ruohonen, M. Representation of associated data by computers. IEEE Transactions on Computers C-22 (1973), 701-702.\n[51]\tKosko, B. Adaptive bidirectional associative memories. Applied Optics 26, 23 (Dec. 1987), 4947-4960.\n[52]\tKOSKO, B. Bidirectional associative memories. IEEE Transactions on Systems, Man, and Cybernetics 18 (1988), 49-60.\n[53]\tLima, E. L. Algebra Linear, 3 ed. Instituto de Matematica Pura e Aplicada, Rio de Janeiro, RJ, 1998.\n[54]\tMarcantonio, A., Darken, C., Kuhn, G. M., Santoso, I., Hanson, S. J., and Petsche, T. A neural network autoassociator for induction motor failure prediction. In Adv. Neural Inf. Process. Syst. (1996), vol. 8, pp. 924-930.\n[55]\tMcCulloch, W., and Pitts, W. A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics 5 (1943), 115-133.\n[56]\tMcEliece, R. J., Posner, E. C., Rodemich, E. R., and Venkatesh, S. The capacity of the Hopfield associative memory. IEEE Transactions on Information Theory 1 (1987), 33-45.\n[57]\tMINKOWSKI, H. Gesammelte Abhandlungen. Teubner Verlag, Leipzig-Berlin, 1911.\n[58]\tMinsky, M., and Papert, S. Perceptrons. MIT Press, Cambridge, MA, 1969.\n[59]\tMurakami, K., and Aibara, T. An improvement on the moore-penrose generalized inverse associative memory. IEEE Transactions on Systems, Man, and Cybernetics SMC-17, 4 (July/August 1987), 699-707.\n[60]\tNAKANO, K. Associatron: A model of associative memory. IEEE Trans. on Systems, Man, Cybernetics SMC-2 (1972), 380-388.\n[61]\tNelson, M. M., and Illingworth, W. A Practical Guide to Neural Nets. Addison-Wesley Publishing Company, 1994.\n[62]\tOkajima, K., Tanaka, S., and Fujiwara, S. A heteroassociative memory with feedback connection. In Proceedings of the IEEE First International Conference on Neural Networks (San Diego, 1987), vol. II, pp. 711-718.\n[63]\tOTSU, N. A threshold selection method from gray-level histograms. IEEE Transactions on Systems, Man, and Cybernetics 9, 1 (1979), 62-66.\n[64]\tPao, Y. H. Adaptive Pattern Recognition and Neural Networks. Addison-Wesley, Reading, MA, 1989.\n[65]\tPedrycz, W. Heterogeneous fuzzy logic networks: Fundamentals and development studies. IEEE TRANSACTIONS ON NEURAL NETWORKS 15, 6 (NOV 2004), 1466-1481.\n[66]\tPERFITTI, R. A synthesis procedure for brain-state-in-a-box neural networks. IEEE Transactions on Neural Networks 6, 5 (Sept 1995), 1071-1080.\n[67]\tPersonnaz, L., Guyon, I., and Dreyfus, G. Information storage and retrieval in spin glass like neural networks. Journal of Physics Letter 46 (1985), L359-L365.\n[68]\tRaducanu, B., Grain a, M., and Albizuri, X. F. Morphological scale spaces and associative morphological memories: Results on robustness and practical applications. Journal of Mathematical Imaging and Vision 19, 2 (2003), 113-131.\n[69]\tRitter, G. X. Image algebra with applications. Unpublished manuscript, available via anonymous ftp from ftp://ftp.cis.ufl.edu/pub/src/ia/documents, 1997.\n[70]\tRitter, G. X., de Leon, J. L. D., and Sussner, P. Morphological bidirectional associative memories. Neural Networks 6, 12 (1999), 851-867.\n[71]\tRitter, G. X., Li, D., and Wilson, J. N. Image algebra and its relationship to neural networks. In Technical Symposium Southeast on Optics, Electro-Optics, and Sensors (Orlando, FL, Mar. 1989), Proceedings of SPIE.\n[72]\tRitter, G. X., Shrader-Frechette, M. A., and Wilson, J. N. Image algebra: A rigorous and translucent way of expressing all image processing operations. In Technical Symposium Southeast on Optics, Electro-Optics, and Sensors (Orlando, FL, May 1987), Proceedings of SPIE.\n[73]\tRitter, G. X., and Sussner, P. An introduction to morphological neural networks. In Proceedings of the 13th International Conference on Pattern Recognition (Vienna, Austria, 1996), pp. 709-717.\n[74]\tRitter, G. X., and Sussner, P. Morphological neural networks. In Intelligent Systems: A Semiotic Perspective; Proceedings of the 1996 International Multidisciplinary Conference (Gaithersburg, Maryland, 1996), pp. 221-226.\n[75]\tRitter, G. X., and Sussner, P. Morphological perceptrons. In ISAS'97, Intelligent Systems and Semiotics (Gaithersburg, Maryland, 1997).\n[76]\tRitter, G. X., Sussner, P., and de Leon, J. L. D. Morphological associative memories. IEEE Transactions on Neural Networks 9, 2 (1998), 281-293.\n[77]\tRitter, G. X., and Urcid, G. Lattice algebra approach to single-neuron computation. IEEE Transactions on Neural Networks 14, 2 (March 2003), 282-295.\n[78]\tRitter, G. X., and Wilson, J. N. Image algebra: A unified approach to image processing. In Medical Imaging (Newport Beach, CA, Feb. 1987), vol. 767 of Proceedings ofSPIE.\n[79]\tRitter, G. X., AND WILSON, J. N. Handbook of Computer Vision Algorithms in Image Algebra, 2 ed. CRC Press, Boca Raton, 2001.\n[80]\tRitter, G. X., Wilson, J. N., and Davidson, J. L. AFATL standard image algebra. Tech. Rep. TR 87-04, University of Florida CIS Department, Oct. 1987.\n[81]\tRitter, G. X., Wilson, J. N., and Davidson, J. L. Image algebra: An overview. Computer Vision, Graphics, and Image Processing 49, 3 (Mar. 1990), 297-331.\n[82]\tRosenblatt, F. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review 65 (1958), 386-408.\n[83]\tRUSSELL, S. J., AND Norvig, P. Artificial Intelligence: A Modern Approach, 2nd edition ed. Prentice Hall, December 2002.\n[84]\tSerra, J. Mathematical morphology and cmm : a historical overview. Avaliable at: http://cmm.ensmp.fr/Recherche/pages/nav0b.htm.\n[85]\tSERRA, J. Image Analysis and Mathematical Morphology. Academic Press, London, 1982.\n[86]\tSERRA, J. Image Analysis and Mathematical Morphology, Volume 2: Theoretical Advances. Academic Press, New York, 1988.\n[87]\tSOILLE, P. Morphological Image Analysis. Springer Verlag, Berlin, 1999.\n[88]\tSTEINBRUCH, K. Die lernmatrix. Kybernetick 1 (1961), 36-45.\n[89]\tStiles, G., and Denq, D. On the effect of noise on the moore-penrose generalized inverse associative memory. IEEE Transaction on Pattern Analysis and Machine Intelligence PAMI-7, 3 (May 1985), 358-360.\n[90]\tSUSSNER, P. Fixed points of autoassociative morphological memories. In Proceedings of the International Joint Conference on Neural Networks (Como, Italy, July 2000), pp. 611-616.\n[91]\tSussner, P. Observations on morphological associative memories and the kernel method. Neurocomputing 31 (Mar. 2000), 167-183.\n[92]\tSussner, P. A relationship between binary morphological autoassociative memories and fuzzy set theory. In Proceedings of the IEEE/INNS International Joint Conference on Neural Networks 2001 (Washington, July 2001).\n[93]\tSussner, P. Associative morphological memories based on variations of the kernel and dual kernel methods. Neural Networks 16,5 (July 2003), 625-632.\n[94]\tSUSSNER, P. Generalizing operations of binary morphological autoassociative memories using fuzzy set theory. Journal of Mathematical Imaging and Vision 9, 2 (Sept. 2003), 81-93. Special Issue on Morphological Neural Networks.\n[95]\tSussner, P. New results on binary auto- and heteroassociative morphological memories. In Proceedings of the International Joint Conference on Neural Networks 2005 (Montreal, Canada, 2005), pp. 1199-1204.\n[96]\tSussner, P., and Valle, M. A brief account of the relations between gray-scale mathematical morphologies. In Proceedings of the Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI) (Natal, Brazil, October 2005), pp. 79 - 86.\n[97]\tSussner, P., and Valle, M. Gray-scale morphological associative memories. Accepted for publication in IEEE Transactions on Neural Networks, June 2005.\n[98]\tSussner, P., and Valle, M. Implicative fuzzy associative memories. Accepted for publication in IEEE Transactions on Fuzzy Systems, May 2005.\n[99]\tTank, D., and Hopfield, J. J. Collective computation in neuronlike circuits. Scientific American 257, 6 (December 1987), 104-114.\n[100]\tTAYLOR, W. Eletrical simulation of some nervous system functional activities. Information Theory 3 (1956), 314-328.\n[101]\tTREFETHEN, L. N., AND Bau III, D. Numerical Linear Algebra. SIAM Publications, Philadelphia, PA, 1997.\n[102]\tValle, M., and Sussner, P. IFAMs - memorias associativas baseadas no aprendizado nebuloso implicativo. In Anais do VII Congresso Brasileiro de Redes Neurais (Natal, October 2005).\n[103]\tValle, M., Sussner, P., and Gomide, F. Introduction to implicative fuzzy associative memories. In Proceedings of the IEEE International Joint Conference on Neural Networks (Hungary, July 2004), pp. 925 - 931.\n[104]\tValle, M. E. MATLAB Source Code for Associative Memory Models, Available at http://www.ime.unicamp.br/ ~mevalle/.\n[105]\tVAN Heerden, P. J. A new optical method of storing and retrieving information. Appl. Opt. 2 (1963), 387-392.\n[106]\tVAN Heerden, P. J. Theory of optical information storage in solids. Appl. Opt. 2 (1963), 393-400.\n[107]\tWIDROW, W., AND Hoff, M. Adaptive switching circuits. WESCON Convention Record (1960), 96-104.\n[108]\tWILSON, S. Morphological networks. In Visual Communication and Image Processing IV (Philadelphia, PA, Nov. 1989), Proceedings of SPIE.\n[109]\tXu ZB, Leung Y, H. X. Assymmetric bidirectional associative memories. IEEE Trans. on Systems, Man, and Cybernetics 24, 10 (OCT 1994), 1558-1564.\n[110]\tYEUNG, D., AND Chow, C. Parzen window network intrusion detectors. In Int. Conf. Pattern Recognit. (2002), pp. 385-388.\n[111]\tZhang, B.-L., Zhang, H., and Ge, S. S. Face recognition by applying wavelet subband representation and kernel associative memory. IEEE Transactions on Neural Networks 15,1 (Jan. 2004), 166-177.\n[112]\tZhang, H., Huang, W., Huang, Z., and Zhang, B. A kernel autoassociator approach to pattern classification. IEEE Transactions on Systems, Man and Cybernetics, Part B 35, 3 (June 2005), 593- 606."}]}}}