{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.11296"}, {"@name": "filename", "#text": "16574_Costa_LuisAugustoNagasaki_M.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA MEC\u00c2NICA  \n\nE INSTITUTO DE GEOCI\u00caNCIAS \n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM  \n\nCI\u00caNCIAS E ENGENHARIA DE PETR\u00d3LEO \n\n \n\n \n\nLU\u00cdS AUGUSTO NAGASAKI COSTA \n\n \n\n \n\nAPLICA\u00c7\u00c3O DE REDES NEURAIS \n\nARTIFICIAIS NO PROCESSO DE AJUSTE \n\nDE HIST\u00d3RICO \n \n\n \n\n \n\n \n\n \n\n \n\nCAMPINAS \n\n2012\n\n \n\n\n\ni \n\n \n\nUNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA MEC\u00c2NICA  \n\nE INSTITUTO DE GEOCI\u00caNCIAS \n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM  \n\nCI\u00caNCIAS E ENGENHARIA DE PETR\u00d3LEO \n\n \n\n \n\n \n\n \n\n \n\n \n\nAPLICA\u00c7\u00c3O DE REDES NEURAIS \n\nARTIFICIAIS NO PROCESSO DE AJUSTE \n\nDE HIST\u00d3RICO \n \n\n \n\n \n\n \n\nAutor: Lu\u00eds Augusto Nagasaki Costa \n\nOrientador: Pesq. Dr. C\u00e9lio Maschio \n\nCo-orientador: Prof. Dr. Denis Jos\u00e9 Schiozer \n\n \n\n \n\nCurso: Ci\u00eancias e Engenharia de Petr\u00f3leo \n\n\u00c1rea de Concentra\u00e7\u00e3o: Reservat\u00f3rios e Gest\u00e3o \n\n \n\n \n\nDisserta\u00e7\u00e3o de mestrado acad\u00eamico apresentada \u00e0 Comiss\u00e3o de P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancias e \n\nEngenharia de Petr\u00f3leo da Faculdade de Engenharia Mec\u00e2nica e Instituto de Geoci\u00eancias, como \n\nrequisito para a obten\u00e7\u00e3o do t\u00edtulo de Mestre em Ci\u00eancias e Engenharia de Petr\u00f3leo. \n\n \n\n \n\n \n\n \n\n \n\nCampinas, 2012 \n\nSP \u2013 Brasil. \n\n\n\nii \n\n \n\n \n\n \n\nFICHA CATALOGR\u00c1FICA ELABORADA PELA \n\nBIBLIOTECA DA \u00c1REA DE ENGENHARIA E ARQUITETURA - BAE - UNICAMP \n\n \n\n \n\n \n\n \n\n    C823a \n\n \n\nCosta, Lu\u00eds Augusto Nagasaki \n\n     Aplica\u00e7\u00e3o de redes neurais artificiais no processo de \n\najuste de hist\u00f3rico / Lu\u00eds Augusto Nagasaki Costa. --\n\nCampinas, SP: [s.n.], 2012. \n\n \n\n     Orientador: C\u00e9lio Maschio.  \n\n     Coorientador: Denis Jos\u00e9 Schiozer. \n\n     Disserta\u00e7\u00e3o de Mestrado - Universidade Estadual de \n\nCampinas, Faculdade de Engenharia Mec\u00e2nica. \n\n \n\n     1. Intelig\u00eancia artificial.  2. Redes neurais.  3. \n\nAlgoritmos gen\u00e9ticos.  4. Engenharia de reservat\u00f3rio.  5. \n\nAmostragem.  I. Maschio, C\u00e9lio.  II. Schiozer, Denis \n\nJos\u00e9.  III. Universidade Estadual de Campinas. \n\nFaculdade de Engenharia Mec\u00e2nica.  IV. T\u00edtulo. \n\n \n\n \n\nT\u00edtulo em Ingl\u00eas: Application of artificial neural networks in the history \n\nmatching process \n\nPalavras-chave em Ingl\u00eas: Artificial intelligence, Artificial neural networks, \n\nGenetic algorithms, Oil reservoir engineering, \n\nSampling \n\n\u00c1rea de concentra\u00e7\u00e3o: Reservat\u00f3rios e Gest\u00e3o \n\nTitula\u00e7\u00e3o: Mestre em Ci\u00eancias e Engenharia de Petr\u00f3leo \n\nBanca examinadora: Dirceu Bampi, Alexandre Campane Vidal \n\nData da defesa: 24-05-2012 \n\nPrograma de P\u00f3s Gradua\u00e7\u00e3o: Engenharia Mec\u00e2nica \n\n \n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\niii \n\n \n\n \n\n\n\niv \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\nv \n\n \n\nDEDICAT\u00d3RIA \n\nDedico esse trabalho aos meus pais Gilvan e Eunice e a minha namorada Marcela, que me \n\nincentivaram a realizar o curso de mestrado e com seus exemplos de persist\u00eancia e esp\u00edrito \n\nbatalhadores me serviram de modelo em toda a minha caminhada. \n\n\n\n \n\n \n\nvii \n\n \n\nAGRADECIMENTOS \n\nAo meu orientador, Pesq. Dr. C\u00e9lio Maschio e ao meu co-orientador Prof. Dr. Denis Jos\u00e9 \n\nSchiozer pela compet\u00eancia, paci\u00eancia e generosidade e por acreditarem neste trabalho. \n\nAos meus colegas de trabalho aos quais convivi durante o mestrado e proporcionaram um bom \n\nambiente para estudos e desenvolvimento. \n\nAos pesquisadores, colaboradores e funcion\u00e1rios do UNISIM, pelo suporte e colabora\u00e7\u00e3o. \n\nAos professores e funcion\u00e1rios do DEP pela ajuda direta ou indireta na realiza\u00e7\u00e3o deste \n\ntrabalho. \n\n\u00c0 PETROBRAS pela concess\u00e3o de bolsa de estudo, ao CEPETRO (Centro de estudos de \n\nPetr\u00f3leo) e ao UNISIM pelo suporte financeiro. \n\nAos meus pais Gilvan e Eunice, e minha namorada Marcela, pelo constante incentivo e apoio. \n\nA todos os meus amigos; os de Assis, os de Campinas e aos que conheci em Ilha Solteira, que \n\ndireta ou indiretamente me proporcionaram bons momentos durante o mestrado. \n\n\n\n \n\n \n\nix \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\u201cNingu\u00e9m pode fazer voc\u00ea se sentir inferior sem o seu consentimento.\u201d \n\n\u201cFa\u00e7a o que o seu cora\u00e7\u00e3o dizer que est\u00e1 certo \u2013 voc\u00ea ser\u00e1 criticado pela sua atitude \n\nseja ela qual for.\u201d \n\nEleanor Roosevelt\n\n\n\n \n\n \n\nxi \n\n \n\nRESUMO \n\n \n\nCOSTA, Lu\u00eds Augusto Nagasaki. Aplica\u00e7\u00e3o de Redes Neurais Artificiais no Processo de Ajuste \n\nde Hist\u00f3rico. Campinas: Faculdade de Engenharia Mec\u00e2nica, Universidade Estadual de \n\nCampinas, 2012. 145 p. Disserta\u00e7\u00e3o de Mestrado. \n\n \n\nO processo de ajuste de hist\u00f3rico consiste em uma das etapas mais importantes envolvendo \n\nestudos de reservat\u00f3rios, pois com o modelo de simula\u00e7\u00e3o ajustado pode-se realizar previs\u00f5es de \n\nprodu\u00e7\u00e3o com maior confiabilidade e avaliar diferentes estrat\u00e9gias de produ\u00e7\u00e3o de forma a obter \n\nmaior recupera\u00e7\u00e3o final com menor custo. Por\u00e9m, esse processo traz consigo diversas \n\ndificuldades, sendo uma delas a n\u00e3o unicidade das solu\u00e7\u00f5es, ou seja, v\u00e1rios modelos podem \n\nigualmente proporcionar resultados satisfat\u00f3rios dependendo do objetivo de estudo. Al\u00e9m disso, o \n\nreservat\u00f3rio pode possuir diversas heterogeneidades e n\u00e3o linearidades entre atributos do \n\nreservat\u00f3rio e valores de produ\u00e7\u00e3o e press\u00e3o, o que tamb\u00e9m contribui para aumentar a \n\ncomplexidade do problema. Atrav\u00e9s dos diversos trabalhos j\u00e1 publicados comprovou-se que cada \n\ncaso possui diferentes caracter\u00edsticas, de forma que uma metodologia aplicada com sucesso a um \n\ndeterminado caso pode n\u00e3o ser aplic\u00e1vel a outro e vice e versa. Dessa maneira, estudos nessa \u00e1rea \n\ndevem ser realizados e atualizados constantemente.  \n\nO grande desafio em problemas envolvendo ajuste de hist\u00f3rico est\u00e1 relacionado \u00e0 redu\u00e7\u00e3o \n\ndo n\u00famero de simula\u00e7\u00f5es necess\u00e1rias para alcan\u00e7ar ajustes satisfat\u00f3rios de acordo com o objetivo \n\nproposto. Entre as diversas t\u00e9cnicas que podem ser encontradas na literatura para tal prop\u00f3sito, \n\numa que chama aten\u00e7\u00e3o \u00e9 a aplica\u00e7\u00e3o de metamodelos gerados atrav\u00e9s de Redes Neurais \n\nArtificiais. Os metamodelos, uma vez gerados, s\u00e3o capazes de fornecer os resultados muito mais \n\nr\u00e1pido que o simulador, pois se tratam de modelos simplificados. As RNA, por sua vez, s\u00e3o \n\nestruturas capazes de captar com efici\u00eancia as n\u00e3o linearidades entre entradas e sa\u00eddas de um \n\ndado problema. Assim, os metamodelos gerados por RNA possuem caracter\u00edsticas que os tornam\n\n\n\n \n\n \n\nxiii \n\n \n\npromissores para serem utilizados como substitutos do simulador em etapas do ajuste que \n\ndemandam maior esfor\u00e7o computacional.  \n\nDeste modo, nesse trabalho foi avaliada a aplica\u00e7\u00e3o de metamodelos gerados por RNA no \n\nprocesso de ajuste de hist\u00f3rico, principalmente no que se refere \u00e0 influ\u00eancia que a qualidade do \n\nconjunto de entrada exerce sobre o desempenho do metamodelo gerado e com rela\u00e7\u00e3o \u00e0 \n\nconfiabilidade da utiliza\u00e7\u00e3o do metamodelo como substituto do simulador para casos pr\u00e1ticos, \n\ncom caracter\u00edsticas mais pr\u00f3ximas da realidade. Os resultados mostraram que a ferramenta, \n\napesar dos erros envolvidos, por se tratar de um modelo simplificado, pode ser utilizada como \n\nferramenta auxiliar ao simulador de escoamento no processo de ajuste de hist\u00f3rico. N\u00e3o \u00e9 \n\nrecomendada a sua utiliza\u00e7\u00e3o como substituta do simulador no processo inteiro, por\u00e9m, pode \n\ncontribuir em etapas do processo que n\u00e3o requerem grande precis\u00e3o dos resultados. Para a \n\nconfiabilidade dos resultados, \u00e9 necess\u00e1rio validar a resposta (encontrada por meio do \n\nmetamodelo) usando o simulador de reservat\u00f3rios. \n\n \n\n Palavras-Chave \n\nAjuste de hist\u00f3rico, intelig\u00eancia artificial, redes neurais artificiais, simula\u00e7\u00e3o num\u00e9rica de \n\nreservat\u00f3rios, algoritmo gen\u00e9tico, t\u00e9cnicas de amostragem, otimiza\u00e7\u00e3o.\n\n \n\n \n\n \n\n\n\n \n\n \n\nxv \n\n \n\nABSTRACT \n\nCOSTA, Lu\u00eds Augusto Nagasaki. Application of Artificial Neural Networks in the History \n\nMatching Process. Campinas: Faculdade de Engenharia Mec\u00e2nica, Universidade Estadual de \n\nCampinas, 2012. 145 p. Disserta\u00e7\u00e3o de Mestrado. \n\n \n\nThe history matching process is one of the most important stages involving studies of \n\nreservoirs, because with the adjusted reservoir model, the production forecasts can be done with \n\nhigher reliability and different production strategies can be evaluated to obtain greater final \n\nrecovery associated with less costs. However, this process have several problems associated, one \n\nbeing the multiple solution, meaning that different models provide satisfactory results, depending \n\non the objective of the study. Furthermore, the reservoir in study can have different \n\nheterogeneities and nonlinearities between reservoir attributes and values of production and \n\npressure, which also contributes to increase the complexity. Various published work showed that \n\neach case has different characteristics, so that a methodology that was applied successfully in one \n\ncase, may not be efficient in another and vice-versa. Thus, studies in this area should be \n\ndeveloped and updated constantly. \n\nThe great challenge in problems involving history matching is related to reducing the \n\nnumber of simulations required to achieve satisfactory adjustments in accordance with the \n\nproposed objective. Among several procedures for this purpose, the application of proxy models \n\ngenerated through artificial neural networks (ANN) can be cited. The proxy models, once \n\ngenerated, are able to calculate the results much faster than the simulator due to the fact that they \n\nare simplified models. The ANN are structures capable of efficiently capture nonlinearities \n\nbetween inputs and outputs of a given problem. Thus, these proxy models have characteristics \n\nthat make them promising for use as substitute of simulator in stages that require greater \n\ncomputational effort. \n\nThereby, in this work the application of proxy models generated through ANN in the \n\nhistory matching process was evaluated, primarily regarding to the influence of the input quality \n\nin the proxy performance and the reliability of the use of proxy models as substitutes of the\n\n\n\n \n\n \n\nxvii \n\n \n\nsimulator in a realistic reservoir model. The results showed that the tool, despite the errors \n\ninvolved, because it is simplified model, can be used as auxiliary tool to the flow simulator in the \n\nprocess of history matching. It is not recommended to use as a substitute in the whole process, \n\nhowever, can contribute in the process stages that do not require great precision. For reliable \n\nresults, it is necessary to validate the response (found through the proxy) using the reservoir \n\nsimulator. \n\n \n\n Key Words \n\nHistory matching, artificial intelligence, artificial neural networks, reservoir numeric \n\nsimulation, genetic algorithm, sampling techniques, optimization. \n\n\n\n \n\n \n\nxix \n\n \n\nSUM\u00c1RIO \n\n \n\n1 INTRODU\u00c7\u00c3O ............................................................................................................ 1 \n\n1.1 Motiva\u00e7\u00e3o .............................................................................................................. 5 \n\n1.2 Objetivos ................................................................................................................ 5 \n\n1.3 Organiza\u00e7\u00e3o da Disserta\u00e7\u00e3o ................................................................................... 6 \n\n2 CONCEITOS E FUNDAMENTA\u00c7\u00c3O TE\u00d3RICA ..................................................... 9 \n\n2.1 Ajuste de hist\u00f3rico de produ\u00e7\u00e3o ............................................................................. 9 \n\n2.1.1 Etapas do ajuste ............................................................................................. 11 \n\n2.1.2 Complexidade do ajuste ................................................................................ 12 \n\n2.1.3 Tipos de ajuste: manual, autom\u00e1tico e assistido ............................................ 14 \n\n2.1.4 Qualidade do ajuste ....................................................................................... 15 \n\n2.1.5 Otimiza\u00e7\u00e3o dos valores dos atributos do reservat\u00f3rio ................................... 17 \n\n2.1.6 Utiliza\u00e7\u00e3o de metamodelos gerados por redes neurais artificiais no processo \n\nde ajuste de hist\u00f3rico ............................................................................................................. 18 \n\n2.2 Redes neurais artificiais ....................................................................................... 18 \n\n2.2.1 Redes neurais biol\u00f3gicas ............................................................................... 19 \n\n2.2.2 Neur\u00f4nio artificial.......................................................................................... 20 \n\n2.2.3 Fun\u00e7\u00e3o de transfer\u00eancia (de ativa\u00e7\u00e3o) ........................................................... 21 \n\n2.2.4 Arquitetura ..................................................................................................... 21 \n\n2.2.5 Escolha da arquitetura da rede ....................................................................... 23 \n\n2.2.6 Treinamento de redes neurais artificiais ........................................................ 24 \n\n2.2.6.1 Algoritmo de retro propaga\u00e7\u00e3o (backpropagation) ................................... 25\n\n\n\n \n\n \n\nxxi \n\n \n\n2.2.6.2 Varia\u00e7\u00f5es do algoritmo de Retro propaga\u00e7\u00e3o ........................................... 26 \n\n2.2.6.3 Capacidade de generaliza\u00e7\u00e3o de uma rede neural artificial....................... 26 \n\n2.3 Algoritmo gen\u00e9tico .............................................................................................. 27 \n\n2.4 T\u00e9cnicas de amostragem ...................................................................................... 31 \n\n2.4.1 Box Behnken ................................................................................................. 32 \n\n2.4.2 Hipercubo Latino ........................................................................................... 35 \n\n2.4.3 Sequ\u00eancia de Sobol ........................................................................................ 36 \n\n2.5 Coment\u00e1rio sobre as t\u00e9cnicas de amostragem ..................................................... 37 \n\n3 REVIS\u00c3O BIBLIOGR\u00c1FICA .................................................................................... 39 \n\n3.1 Ajuste de hist\u00f3rico ............................................................................................... 39 \n\n3.2 Aplica\u00e7\u00e3o de redes neurais artificiais e metamodelos ......................................... 41 \n\n3.3 Metamodelos gerados por redes neurais artificiais no processo de ajuste de \n\nhist\u00f3rico  ............................................................................................................................. 42 \n\n4 METODOLOGIA ....................................................................................................... 47 \n\n4.1 Metodologia geral do trabalho ............................................................................. 47 \n\n4.2 Metodologia espec\u00edfica - procedimento de ajuste ................................................ 49 \n\n4.2.1 Treinamento das redes neurais artificiais ...................................................... 50 \n\n4.2.2 Aplica\u00e7\u00e3o de metamodelos no processo de ajuste de hist\u00f3rico ..................... 51 \n\n5 APLICA\u00c7\u00c3O .............................................................................................................. 53 \n\n5.1 Casos anal\u00edticos .................................................................................................... 53 \n\n5.1.1 Premissas e considera\u00e7\u00f5es ............................................................................. 53 \n\n5.1.2 Caso 1A ......................................................................................................... 54 \n\n5.1.3 Caso 1B ......................................................................................................... 55 \n\n5.1.4 Caso 1C ......................................................................................................... 56\n\n\n\n \n\n \n\nxxiii \n\n \n\n5.1.5 Caso 1D ......................................................................................................... 56 \n\n5.2 Casos de reservat\u00f3rio ........................................................................................... 57 \n\n5.2.1 Premissas e considera\u00e7\u00f5es ............................................................................. 57 \n\n5.2.2 Reservat\u00f3rio utilizado .................................................................................... 58 \n\n5.2.3 Caso 2A ......................................................................................................... 59 \n\n5.2.4 Caso 2B ......................................................................................................... 59 \n\n5.3 Gera\u00e7\u00e3o dos conjuntos de entrada para treinamento............................................ 60 \n\n5.4 Treinamento das redes neurais artificias .............................................................. 65 \n\n5.5 Otimiza\u00e7\u00e3o ........................................................................................................... 66 \n\n6 RESULTADOS E DISCUSS\u00c3O ................................................................................ 69 \n\n6.1 Caso 1A ................................................................................................................ 69 \n\n6.1.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento ............................................ 69 \n\n6.1.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados ...................................................................................................... 70 \n\n6.1.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ....................................................................................................................... 72 \n\n6.2 Caso 1B ................................................................................................................ 73 \n\n6.2.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento ............................................ 73 \n\n6.2.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados ...................................................................................................... 75 \n\n6.2.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ....................................................................................................................... 78 \n\n6.2.4 Passo 4: Retreinamento ................................................................................. 78 \n\n6.3 Caso 1C ................................................................................................................ 81\n\n\n\n \n\n \n\nxxv \n\n \n\n6.3.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento ............................................ 81 \n\n6.3.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados ...................................................................................................... 83 \n\n6.3.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ....................................................................................................................... 87 \n\n6.4 Caso 1D ................................................................................................................ 90 \n\n6.4.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento ............................................ 90 \n\n6.4.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados ...................................................................................................... 91 \n\n6.4.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ....................................................................................................................... 94 \n\n6.5 Coment\u00e1rios dos casos anal\u00edticos ........................................................................ 96 \n\n6.6 Caso 2A ................................................................................................................ 99 \n\n6.6.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento ............................................ 99 \n\n6.6.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados .................................................................................................... 102 \n\n6.6.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ..................................................................................................................... 104 \n\n6.6.4 Passo 4: Retreinamento ............................................................................... 106 \n\n6.7 Caso 2B .............................................................................................................. 110 \n\n6.7.1 An\u00e1lise de sensibilidade .............................................................................. 110 \n\n6.7.2 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento .......................................... 112 \n\n6.7.3 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho \n\ndos metamodelos gerados .................................................................................................... 114\n\n\n\n \n\n \n\nxxvii \n\n \n\n6.7.4 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo \n\nencontrado  ..................................................................................................................... 119 \n\n6.7.5 Passo 4: Retreinamento ............................................................................... 123 \n\n6.8 Coment\u00e1rios gerais dos casos de reservat\u00f3rio. .................................................. 127 \n\n7 CONCLUS\u00d5ES E SUGEST\u00d5ES PARA TRABALHOS FUTUROS ...................... 131 \n\nREFER\u00caNCIAS BIBLIOGR\u00c1FICAS ............................................................................... 137 \n\nAP\u00caNDICE ........................................................................................................................ 143 \n\nI. Capacidade de extrapola\u00e7\u00e3o da rede neural artificial \u2013 Caso 2A....................... 143 \n\nII. Melhor configura\u00e7\u00e3o de rede neural artificial obtida \u2013 Caso 2B ....................... 144\n\n\n\n \n\n \n\nxxix \n\n \n\nLISTA DE FIGURAS \n\nFigura 2.1 \u2013 Fluxograma b\u00e1sico a ser seguido para realiza\u00e7\u00e3o do ajuste de hist\u00f3rico. \n\n(ERTEKIN et al., 2001, p.351). .................................................................................................... 11 \n\nFigura 2.2 \u2013 Exemplo da n\u00e3o unicidade (m\u00faltiplas solu\u00e7\u00f5es) no problema de ajuste. ........ 14 \n\nFigura 2.3 \u2013 Compara\u00e7\u00e3o do corte de \u00e1gua simulado com o hist\u00f3rico (Modifica\u00e7\u00e3o de \n\nERTEKIN et al., 2001, p.357). ...................................................................................................... 16 \n\nFigura 2.4 \u2013 Esquema simplificado de um neur\u00f4nio biol\u00f3gico. .......................................... 19 \n\nFigura 2.5 \u2013 Esquema b\u00e1sico de neur\u00f4nio artificial (HAGAN et al., 1996, p.2-3). ............ 20 \n\nFigura 2.6 \u2013 Fun\u00e7\u00e3o de transfer\u00eancia do tipo linear (a) e tangente hiperb\u00f3lica (b). ............ 21 \n\nFigura 2.7 \u2013 Ilustra\u00e7\u00e3o de uma rede neural de 3 camadas (HAGAN et al., 1996, p.2-11). . 22 \n\nFigura 2.8 \u2013 Rede multicamadas na forma abreviada (HAGAN et al., 1996, p.2-12)......... 23 \n\nFigura 2.9 \u2013 Fluxograma b\u00e1sico do algoritmo de RP........................................................... 25 \n\nFigura 2.10 \u2013 Estrutura b\u00e1sica de um AGC. ........................................................................ 28 \n\nFigura 2.11 \u2013 Exemplo de um indiv\u00edduo formado por um vetor de bits (a), ilustra\u00e7\u00e3o dos \n\nprocessos de recombina\u00e7\u00e3o (b) e muta\u00e7\u00e3o (c). .............................................................................. 29 \n\nFigura 2.12 \u2013 Ilustra\u00e7\u00e3o do algoritmo de Roleta Russa. ...................................................... 29 \n\nFigura 2.13 \u2013 M\u00e9todo de BB na forma gr\u00e1fica para 3 fatores, discretizados em 3 n\u00edveis \n\n(FERREIRA et al., 2007, p.183). .................................................................................................. 34 \n\nFigura 2.14 \u2013 Exemplo de discretiza\u00e7\u00e3o de uma distribui\u00e7\u00e3o normal em 7 intervalos. \n\n(MASCHIO et al., 2009, p.3) ........................................................................................................ 35 \n\nFigura 2.15 - Figura ilustrativa da amostragem por HL, para distribui\u00e7\u00e3o normal (a) e \n\nuniforme (b). .................................................................................................................................. 36 \n\nFigura 2.16 \u2013 Exemplo de amostragem pela Sequ\u00eancia de Sobol. ...................................... 37 \n\nFigura 3.1 \u2013 Exemplo do indicador de qualidade de ajuste com rela\u00e7\u00e3o ao Caso Base. ..... 44\n\n\n\n \n\n \n\nxxxi \n\n \n\nFigura 4.1 - Fluxograma descrevendo as etapas utilizadas para realiza\u00e7\u00e3o da metodologia \n\ngeral do trabalho. ........................................................................................................................... 47 \n\nFigura 4.2 \u2013 Fluxograma que descreve os passos seguidos pelo procedimento de ajuste \n\nadotado nesse trabalho. .................................................................................................................. 49 \n\nFigura 5.1 \u2013 Superf\u00edcie de resposta para o Caso 1A. ........................................................... 54 \n\nFigura 5.2 \u2013 Superf\u00edcie de resposta para o Caso 1B. ........................................................... 55 \n\nFigura 5.3 \u2013 Superf\u00edcie de resposta para o Caso 1C. ........................................................... 56 \n\nFigura 5.4 \u2013 Superf\u00edcie de resposta para o Caso 1D. ........................................................... 57 \n\nFigura 5.5 - Modelo de reservat\u00f3rio (permeabilidade horizontal \u2013 md). ............................. 58 \n\nFigura 6.1 \u2013 Amostragem no espa\u00e7o do conjunto de treinamento de 25 pontos; HL (a) e SS \n\n(b) \u2013 Caso 1A. ................................................................................................................................ 70 \n\nFigura 6.2 \u2013 Superf\u00edcies de resposta e conjunto de teste, para os metamodelos gerados com \n\nHL25 (a) e HL50 (b) \u2013 Caso 1A. Pontos em azul: erro m\u00e9dio menor que 1% e pontos em \n\nvermelho: erro m\u00e9dio maior ou igual a 1%. .................................................................................. 71 \n\nFigura 6.3 \u2013 Visualiza\u00e7\u00e3o dos erros ponto a ponto para os metamodelos gerados com HL25 \n\n(a) e HL50 (b) pontos \u2013 Caso 1A. ................................................................................................. 72 \n\nFigura 6.4 \u2013 Amostras de 25 pontos, do HL (a) e da SS (b)  para treinamento \u2013 Caso 1B. 74 \n\nFigura 6.5 - Amostras de 50 pontos, do HL (a) e da SS (b) para treinamento \u2013 Caso 1B. .. 74 \n\nFigura 6.6 \u2013 Superf\u00edcies de resposta dos metamodelos gerados com 25 pontos do HL (a) e \n\nda SS (b) \u2013 Caso 1B. ...................................................................................................................... 75 \n\nFigura 6.7 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), \n\nHL100 (c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1B. ................................................................ 77 \n\nFigura 6.8 \u2013 Conjunto de treinamento e novos limites (a) e superf\u00edcie de resposta do \n\nmetamodelo gerado (b), relativos ao retreinamento \u2013 Caso 1B. ................................................... 79 \n\nFigura 6.9 - Conjunto de treinamento para HL25 (a) e SS25 (b) - Caso 1C. ....................... 81\n\n\n\n \n\n \n\nxxxiii \n\n \n\nFigura 6.10 \u2013 Conjunto de treinamento para HL50 (a) e SS50 (b) - Caso 1C. .................... 82 \n\nFigura 6.11 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a) e SS25 (b) \n\npontos  -  Caso 1C. ........................................................................................................................ 83 \n\nFigura 6.12 - Superf\u00edcies de resposta dos metamodelos gerados com HL50 (a) e SS50 (b) \n\npontos  -  Caso 1C. ........................................................................................................................ 84 \n\nFigura 6.13 - Superf\u00edcies de resposta dos metamodelos gerados com HL100 (a) e SS100 (b) \n\npontos  -  Caso 1C. ........................................................................................................................ 85 \n\nFigura 6.14 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), \n\nHL100 (c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1C. ................................................................ 86 \n\nFigura 6.15 - M\u00ednimos globais, representados pelos pontos em azul (a); e crit\u00e9rio de \n\nvizinhan\u00e7a adotado para identifica\u00e7\u00e3o de m\u00ednimos de interesse (b) \u2013 Caso 1C. ........................... 88 \n\nFigura 6.16 \u2013 Conjunto de treinamento para HL25 (a) e SS25 (b) pontos \u2013 Caso 1D. ....... 91 \n\nFigura 6.17 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a) e SS25 (b) \n\npontos  -  Caso 1D ......................................................................................................................... 92 \n\nFigura 6.18 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), \n\nHL100 (c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1D. ................................................................ 93 \n\nFigura 6.19 \u2013 Conjunto de treinamento de HL100 (a) e SS100 (b) pontos \u2013 Caso 1D........ 95 \n\nFigura 6.20 \u2013 Pontos de treinamento, relativos aos atributos porosidade da f\u00e1cies 2 e \n\ntransmissibilidade da falha 3 para os conjuntos HL25 (a) e HL50 (b) pontos - Caso 2A. .......... 100 \n\nFigura 6.21 \u2013 Histograma dos valores de sa\u00edda para os conjuntos de treinamento HL25 e \n\nHL50 pontos \u2013 Caso 2A. ............................................................................................................. 102 \n\nFigura 6.22 \u2013 Gr\u00e1fico de dispers\u00e3o (crossplot) entre sa\u00edda do simulador e sa\u00edda do \n\nmetamodelo, gerado com HL25 (a), HL50 (b) e HL100 (c) \u2013 Caso 2A; pontos em azul: pontos de \n\ntreinamento e pontos em vermelho: pontos de teste. ................................................................... 103 \n\nFigura 6.23 \u2013 Curva de produ\u00e7\u00e3o de \u00e1gua do campo do Caso base, hist\u00f3rico e metamodelos \n\ngerados com HL25, HL50 e HL100 pontos \u2013 Caso 2A............................................................... 105\n\n\n\n \n\n \n\nxxxv \n\n \n\nFigura 6.24 - Curva de produ\u00e7\u00e3o de \u00e1gua do campo do Caso base, hist\u00f3rico e metamodelos \n\ngerados com HL50, HL100 pontos e HL50 ap\u00f3s retreinamento \u2013 Caso 2A. .............................. 108 \n\nFigura 6.25 \u2013 An\u00e1lise de sensibilidade para o afastamento da produ\u00e7\u00e3o de \u00e1gua do po\u00e7o \n\nPROD4 (a) e m\u00e9dia aritm\u00e9tica (b) \u2013 Caso 2B. ............................................................................ 110 \n\nFigura 6.26 \u2013 Gr\u00e1fico de dispers\u00e3o dos pontos de treinamento entre os atributos Kz2 e Kr2, \n\nrelativo aos conjuntos HL100 (a) e HL250 (b) pontos \u2013 Caso 2B. ............................................. 112 \n\nFigura 6.27 - Gr\u00e1ficos de dispers\u00e3o dos pontos de treinamento entre os atributos Kz2 e Kr2, \n\nreferente aos conjuntos HL396 (a) e BB396 (b) \u2013 Caso 2B. ....................................................... 113 \n\nFigura 6.28 - Gr\u00e1ficos de dispers\u00e3o (crossplot) para o po\u00e7o PROD2 \u2013 PROD_AS \u2013 Caso \n\n2B. ............................................................................................................................................... 116 \n\nFigura 6.29 - Gr\u00e1ficos de dispers\u00e3o (crossplot) para o po\u00e7o PROD3 \u2013 PROD_AS \u2013 Caso \n\n2B. ............................................................................................................................................... 117 \n\nFigura 6.30 \u2013 Coeficiente de correla\u00e7\u00e3o linear em gr\u00e1fico de barras; grupo de barras da \n\nesquerda: resultados do treinamento; grupo de barras da direita: resultados do teste \u2013 Caso 2B.\n\n ..................................................................................................................................................... 118 \n\nFigura 6.31 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os  \n\nmetamodelos para os po\u00e7os PROD1 (a), PROD2 (b), PROD3 (c) e PROD4 (d) \u2013 PROD_AS \u2013 \n\nCaso 2B. ...................................................................................................................................... 121 \n\nFigura 6.32 - Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os \n\nmetamodelos para os po\u00e7os PROD5 (a), PROD6 (b), PROD7 (c) e PROD8 (d) \u2013 PROD_AS \u2013 \n\nCaso 2B. ...................................................................................................................................... 122 \n\nFigura 6.33 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os \n\nmetamodelos para os po\u00e7os PROD1 (a), PROD2 (b), PROD3 (b) e PROD4 (b) \u2013 retreinamento \u2013 \n\nCaso 2B. ...................................................................................................................................... 125 \n\nFigura 6.34 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os \n\nmetamodelos para os po\u00e7os PROD5 (a), PROD6 (b), PROD7 (b) e PROD8 (b) \u2013 retreinamento \u2013 \n\nCaso 2B. ...................................................................................................................................... 126\n\n\n\n \n\n \n\nxxxvii \n\n \n\nLISTA DE TABELAS \n\nTabela 2.1 \u2013 M\u00e9todo de BB na forma matricial para 3 fatores, discretizados em 3 n\u00edveis. . 33 \n\nTabela 5.1 \u2013 Atributos incertos para o caso 2A. .................................................................. 59 \n\nTabela 5.2 \u2013 Atributos incertos para Caso 2B. .................................................................... 60 \n\nTabela 5.3 \u2013 Atributos que mais influenciam os po\u00e7os. ...................................................... 62 \n\nTabela 5.4 \u2013 Principais par\u00e2metros utilizados pelo AG. ...................................................... 67 \n\nTabela 6.1 \u2013 Coeficientes de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimos determinados atrav\u00e9s \n\nda otimiza\u00e7\u00e3o utilizando os metamodelos - Caso 1A. ................................................................... 73 \n\nTabela 6.2 \u2013 Coeficientes de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimos determinados atrav\u00e9s \n\nda otimiza\u00e7\u00e3o utilizando os metamodelos - Caso 1B. ................................................................... 78 \n\nTabela 6.3 \u2013 Defini\u00e7\u00e3o dos novos limites para retreinamento \u2013 Caso 1B. .......................... 79 \n\nTabela 6.4 - Resultados obtidos com o retreinamento \u2013 Caso 1B. ...................................... 80 \n\nTabela 6.5 - Coeficientes de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimos determinados atrav\u00e9s \n\nda otimiza\u00e7\u00e3o utilizando os metamodelos - Caso 1C. ................................................................... 87 \n\nTabela 6.6 - M\u00ednimos de interesse - Caso 1C. ..................................................................... 89 \n\nTabela 6.7 - Coeficientes de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimos determinados atrav\u00e9s \n\nda otimiza\u00e7\u00e3o utilizando os metamodelos - Caso 1D. ................................................................... 94 \n\nTabela 6.8 \u2013 Frequ\u00eancia de ocorr\u00eancia dos valores de sa\u00edda para os conjuntos de \n\ntreinamento, HL25, HL50 e HL100 pontos \u2013 Caso 2A. .............................................................. 101 \n\nTabela 6.9 \u2013 Correla\u00e7\u00e3o linear entre sa\u00edda do simulador e do metamodelo (afastamento da \n\nprodu\u00e7\u00e3o de \u00e1gua, do modelo de simula\u00e7\u00e3o com rela\u00e7\u00e3o ao hist\u00f3rico), relativos aos conjuntos de \n\ntreino e teste \u2013 Caso 2A. .............................................................................................................. 104 \n\nTabela 6.10 \u2013 Afastamento obtido com o m\u00ednimo encontrado na otimiza\u00e7\u00e3o, quando \n\nsimulado com o metamodelo e com o simulador \u2013 Caso 2A. ..................................................... 104\n\n\n\n \n\n \n\nxxxix \n\n \n\nTabela 6.11 \u2013 Defini\u00e7\u00e3o dos atributos para novo treinamento \u2013 Caso 2A. ........................ 107 \n\nTabela 6.12 \u2013 Coeficientes de correla\u00e7\u00e3o linear do retreinamento; m\u00ednimo obtido com a \n\notimiza\u00e7\u00e3o e simula\u00e7\u00e3o do m\u00ednimo com o simulador \u2013 Caso 2A. .............................................. 107 \n\nTabela 6.13 - Rela\u00e7\u00e3o da dist\u00e2ncia relativa dos atributos de treinamento com o hist\u00f3rico \u2013 \n\nCaso 2A ....................................................................................................................................... 109 \n\nTabela 6.14 \u2013 An\u00e1lise de sensibilidade do po\u00e7o PROD4 e da m\u00e9dia dos oito po\u00e7os. ....... 111 \n\nTabela 6.15 \u2013 Coeficientes de correla\u00e7\u00e3o linear dos metamodelos gerados \u2013 PROD_AS \u2013 \n\nCaso 2B. ...................................................................................................................................... 115 \n\nTabela 6.16 \u2013 Valores de afastamento e indicador de qualidade (%), obtidos com a \n\nsimula\u00e7\u00e3o do m\u00ednimo (x10\n6\n) \u2013 PROD_AS \u2013 Caso 2B. ............................................................... 120 \n\nTabela 6.17 - Coeficientes de correla\u00e7\u00e3o linear \u2013 PROD_AS \u2013 Caso 2B \u2013 retreinamento.\n\n ..................................................................................................................................................... 123 \n\nTabela 6.18 \u2013 Valores de afastamento gerados com a simula\u00e7\u00e3o dos m\u00ednimos obtidos com a \n\notimiza\u00e7\u00e3o \u2013 PROD_AS \u2013 Retreinamento \u2013 Caso 2B. ................................................................ 124 \n\n\n\n \n\n \n\nxli \n\n \n\nLISTA DE NOMENCLATURAS \n\nAbrevia\u00e7\u00f5es \n\nAG \u2013 Algoritmo Gen\u00e9tico \n\nAGC \u2013 Algoritmo Gen\u00e9tico Cl\u00e1ssico \n\nBB - Box Behnken \n\nFO \u2013 Fun\u00e7\u00e3o Objetivo \n\nHL \u2013 Hipercubo Latino \n\nLMS - Least Mean Square \n\nLM - Levenberg-Marquardt \n\nRNA \u2013 Redes Neurais Artificiais \n\nRP \u2013 Retro propaga\u00e7\u00e3o \n\nSS \u2013 Sequ\u00eancia de Sobol \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n1 \n\n \n\n1 INTRODU\u00c7\u00c3O \n\nA simula\u00e7\u00e3o num\u00e9rica de reservat\u00f3rios \u00e9 amplamente utilizada por profissionais ligados \u00e0 \n\n\u00e1rea de gerenciamento de reservat\u00f3rios, uma vez que permite realizar estudos sobre reservat\u00f3rios \n\ncomplexos (com falhas, fraturas, geometrias complexas, heterogeneidades etc.) e aplica\u00e7\u00e3o de \n\ndiferentes estrat\u00e9gias de produ\u00e7\u00e3o (inje\u00e7\u00e3o de \u00e1gua, vapor, pol\u00edmeros etc.), sendo fundamental no \n\napoio \u00e0 tomada de decis\u00f5es. O objetivo principal do estudo de reservat\u00f3rios \u00e9 a previs\u00e3o de \n\ncomportamentos futuros, o que possibilita a realiza\u00e7\u00e3o de testes de diferentes estrat\u00e9gias de \n\nprodu\u00e7\u00e3o de modo a se obter o melhor desempenho poss\u00edvel do reservat\u00f3rio considerando \n\naspectos econ\u00f4micos e t\u00e9cnicos. \n\nO modelo de simula\u00e7\u00e3o \u00e9 constru\u00eddo a partir da caracteriza\u00e7\u00e3o geol\u00f3gica e de fluidos, \n\nrealizando simplifica\u00e7\u00f5es at\u00e9 que sua utiliza\u00e7\u00e3o com o simulador seja vi\u00e1vel. Nessa etapa \n\ndiversas informa\u00e7\u00f5es do reservat\u00f3rio s\u00e3o perdidas. Al\u00e9m disso, s\u00e3o muitas as incertezas acerca \n\ndos par\u00e2metros. Os procedimentos de obten\u00e7\u00e3o dos dados do reservat\u00f3rio demandam custos \n\nelevados e s\u00e3o poucas as informa\u00e7\u00f5es diretas obtidas (dados de po\u00e7os), sendo a maioria estimada, \n\ncorrelacionada, ou n\u00e3o obtida.  \n\nPortanto, o modelo de simula\u00e7\u00e3o obtido nessa fase necessita de uma calibra\u00e7\u00e3o para que \n\nrepresente de forma confi\u00e1vel o comportamento observado no reservat\u00f3rio real e possa ser \n\nutilizado para previs\u00e3o de produ\u00e7\u00e3o. Esse processo de calibra\u00e7\u00e3o do modelo \u00e9 chamado de ajuste \n\nde hist\u00f3rico. \n\nO objetivo do ajuste de hist\u00f3rico \u00e9 minimizar uma fun\u00e7\u00e3o objetivo (FO) que representa a \n\nqualidade do ajuste mediante c\u00e1lculo da diferen\u00e7a entre dados observados no campo real e dados \n\nobtidos atrav\u00e9s do modelo de simula\u00e7\u00e3o.  O procedimento consiste basicamente na varia\u00e7\u00e3o dos \n\nvalores dos atributos incertos do reservat\u00f3rio (permeabilidades, porosidade, transmissibilidade \n\netc.) iterativamente, at\u00e9 que um valor de FO aceit\u00e1vel seja alcan\u00e7ado.   \n\nA import\u00e2ncia desse processo reside no fato de que o modelo, uma vez ajustado, tem mais \n\nconfiabilidade para ser utilizado na previs\u00e3o de produ\u00e7\u00e3o do reservat\u00f3rio. Por\u00e9m, a obten\u00e7\u00e3o de \n\n\n\n \n\n \n\n2 \n\n \n\num modelo aceit\u00e1vel n\u00e3o \u00e9 uma tarefa f\u00e1cil, devido a algumas caracter\u00edsticas que o problema de \n\najuste de hist\u00f3rico possui, listadas abaixo: \n\n? \u00c9 do tipo inverso: a resposta (dados de produ\u00e7\u00e3o) \u00e9 conhecida, mas as vari\u00e1veis de \n\nentrada (propriedades do reservat\u00f3rio) s\u00e3o desconhecidas; \n\n? \u00c9, em geral, n\u00e3o linear: a resposta pode variar de forma n\u00e3o linear com as vari\u00e1veis \n\nde entrada; \n\n? Pode ocorrer inconsist\u00eancia nos dados: devido a alguns m\u00e9todos de medi\u00e7\u00e3o serem \n\nrealizados de forma indireta. Por exemplo, esquema de rateio para medi\u00e7\u00e3o da \n\nprodu\u00e7\u00e3o dos po\u00e7os de petr\u00f3leo; \n\n? Pode ocorrer insufici\u00eancia nos dados: principalmente para casos com pouco \n\nhist\u00f3rico de produ\u00e7\u00e3o, em que os dados n\u00e3o s\u00e3o suficientes para calibrar o modelo; \n\n? Pode envolver in\u00fameros par\u00e2metros: quanto maior o n\u00famero de par\u00e2metros \n\nenvolvidos maior \u00e9 a complexidade do problema. Tipicamente estes par\u00e2metros s\u00e3o \n\na porosidade, a permeabilidade (horizontal, vertical ou relativa), a \n\ntransmissibilidade de falhas, satura\u00e7\u00f5es iniciais etc. Se o reservat\u00f3rio for \n\nheterog\u00eaneo pode possuir v\u00e1rias f\u00e1cies\n1\n, o que tamb\u00e9m contribui para o aumento do \n\nn\u00famero de par\u00e2metros. \n\n? Pode assumir m\u00faltiplas solu\u00e7\u00f5es: diferentes combina\u00e7\u00f5es de par\u00e2metros podem \n\nresultar em ajustes aceit\u00e1veis. \n\nDesse modo, o processo torna-se complexo e, dependendo do caso, o tempo demandado \n\npara realizar o ajuste pode ser grande. Na maioria dos casos tal processo \u00e9 realizado de forma \n\nmanual, pelo procedimento de tentativa e erro. Por\u00e9m, com o aumento do n\u00famero de par\u00e2metros \n\nesse tipo de ajuste torna-se ineficaz, uma vez que a probabilidade de ocorr\u00eancia de erros e a \n\nlimita\u00e7\u00e3o da busca no espa\u00e7o de solu\u00e7\u00f5es tamb\u00e9m aumentam. A vantagem \u00e9 que a experi\u00eancia do \n\nprofissional pode levar a solu\u00e7\u00f5es de forma mais eficiente. Nesse contexto, foi desenvolvido o \n\najuste autom\u00e1tico, que consiste na utiliza\u00e7\u00e3o de algoritmos para realizar todo o processo de \n\najuste. Por\u00e9m, esse tipo de ajuste tamb\u00e9m se mostrou ineficiente, uma vez que, diversas \n\n                                                 \n1\n Caracter\u00edstica de um tipo de rocha que reflete sua origem e a diferencia de outros tipos ao seu redor. O termo f\u00e1cies \n\n(em ingl\u00eas, facies) \u00e9 utilizado tanto para o singular quanto para o plural.  \n\nFonte: http://www.glossary.oilfield.slb.com/search.cfm, acesso em 01/06/2012, \u00e0s 22h21min. \n\n \n\nhttp://www.glossary.oilfield.slb.com/search.cfm\n\n\n \n\n \n\n3 \n\n \n\nparticularidades existem para cada tipo de problema e a defini\u00e7\u00e3o de um algoritmo capaz de \n\nrealizar ajustes de forma independente torna-se complicado. Para aproveitar a grande capacidade \n\nde processamento dos algoritmos autom\u00e1ticos e, ao mesmo tempo, utilizar a experi\u00eancia de um \n\nprofissional para guiar o processo, surgiu o ajuste de hist\u00f3rico assistido. Com isso, partes do \n\nprocesso, que exigem grande esfor\u00e7o computacional, passam a ser automatizadas e a presen\u00e7a de \n\num profissional especializado ajuda na escolha de algoritmos e metodologias. \n\nA parte do processo que demanda maior esfor\u00e7o computacional \u00e9 a etapa em que se realiza \n\na varredura do espa\u00e7o de solu\u00e7\u00f5es (etapa de otimiza\u00e7\u00e3o), necessitando normalmente de um \n\ngrande n\u00famero de simula\u00e7\u00f5es. Nessa \u00e1rea, a utiliza\u00e7\u00e3o de metamodelos (proxies) surge como \n\nop\u00e7\u00e3o interessante, uma vez que o metamodelo gerado pode ser utilizado no lugar do simulador \n\nem partes do processo, possibilitando a redu\u00e7\u00e3o do n\u00famero de simula\u00e7\u00f5es necess\u00e1rias para \n\nalcan\u00e7ar ajustes aceit\u00e1veis. O metamodelo consiste, basicamente, em um modelo definido \n\nsegundo fun\u00e7\u00f5es matem\u00e1ticas ou estat\u00edsticas que representam um dado padr\u00e3o de sa\u00edda para um \n\ndado padr\u00e3o de entrada. \n\nExistem diversas t\u00e9cnicas dispon\u00edveis para gera\u00e7\u00e3o de metamodelos. Segundo Zubarev \n\n(2009), o modelo de regress\u00e3o polinomial obtido por planejamento estat\u00edstico \u00e9 bastante utilizado \n\nna ind\u00fastria de petr\u00f3leo devido a sua f\u00e1cil compreens\u00e3o, flexibilidade e efici\u00eancia computacional. \n\nOutra ferramenta que pode ser utilizada para gera\u00e7\u00e3o de metamodelos \u00e9 a Rede Neural Artificial \n\n(RNA), utilizada em diversas \u00e1reas e bastante difundida em an\u00e1lise de risco na \u00e1rea de petr\u00f3leo. \n\nSegundo Maschio et al. (2008), as RNA s\u00e3o estruturas capazes de captar, de forma eficiente, as \n\nn\u00e3o linearidades em problemas tipicamente encontrados nos processos envolvendo simula\u00e7\u00e3o de \n\nreservat\u00f3rios, o que constitui a maior motiva\u00e7\u00e3o para utiliza\u00e7\u00e3o dessa t\u00e9cnica para o problema de \n\najuste de hist\u00f3rico.  \n\nPara gera\u00e7\u00e3o de metamodelos utilizando RNA faz-se necess\u00e1rio treinar a rede para que ela \n\nseja capaz de reproduzir o comportamento do problema desejado. Para tanto, existem fatores \n\nimportantes que devem ser considerados no processo de treinamento. A defini\u00e7\u00e3o do conjunto de \n\ntreinamento em espec\u00edfico \u00e9 fundamental, pois a rede aprende atrav\u00e9s de exemplos e, por conta \n\ndisso, n\u00e3o \u00e9 capaz de representar com confiabilidade regi\u00f5es do espa\u00e7o de solu\u00e7\u00f5es para a qual \n\nn\u00e3o foi treinada. \n\n\n\n \n\n \n\n4 \n\n \n\nUm fator importante na aplica\u00e7\u00e3o de metamodelos gerados por RNA no processo de ajuste \n\nde hist\u00f3rico, \u00e9 que o procedimento de amostragem deve ser realizado utilizando o menor n\u00famero \n\nposs\u00edvel de simula\u00e7\u00f5es. Esse aspecto ganha maior import\u00e2ncia com o aumento da complexidade \n\ndo problema (aumento do n\u00famero de vari\u00e1veis), uma vez que o n\u00famero de simula\u00e7\u00f5es \n\nnecess\u00e1rias para amostragem de todas as combina\u00e7\u00f5es poss\u00edveis dos par\u00e2metros cresce \n\nexponencialmente e se torna invi\u00e1vel. Assim, para defini\u00e7\u00e3o do conjunto de treinamento deve-se \n\natentar primeiramente \u00e0 distribui\u00e7\u00e3o eficaz das amostras de forma a cobrir todo o espa\u00e7o de busca \n\ndos par\u00e2metros e, em segundo lugar, o espa\u00e7o deve ser coberto com efici\u00eancia, ou seja, uma \n\nregi\u00e3o deve conter uma quantidade suficiente de amostras (exemplos), de forma que a rede possa \n\naprender o padr\u00e3o de comportamento da superf\u00edcie de resposta da regi\u00e3o. \n\nUma t\u00e9cnica bastante utilizada para este prop\u00f3sito \u00e9 o planejamento estat\u00edstico, que \n\nsegundo Risso et al. (2006) \u00e9 o m\u00e9todo mais adequado a ser aplicado para reduzir o n\u00famero de \n\nsimula\u00e7\u00f5es de reservat\u00f3rio e vem se mostrando uma boa t\u00e9cnica auxiliar em processos de \n\ndesenvolvimento e gerenciamento de campos de petr\u00f3leo, sendo sua principal aplica\u00e7\u00e3o voltada \u00e0 \n\ngera\u00e7\u00e3o de metamodelos.  \n\nA fim de analisar a influ\u00eancia dos pontos de treinamento no aprendizado da RNA, foram \n\nabordadas tr\u00eas t\u00e9cnicas com diferentes caracter\u00edsticas para gera\u00e7\u00e3o dos conjuntos de treinamento, \n\nsendo uma delas o planejamento Box Behnken (BB). Al\u00e9m dele foi utilizado o Hipercubo Latino \n\n(HL), que \u00e9 um m\u00e9todo estat\u00edstico para gerar distribui\u00e7\u00f5es de par\u00e2metros de forma a honrar as \n\ncaracter\u00edsticas da distribui\u00e7\u00e3o original, e a Sequ\u00eancia de Sobol (SS), que consiste em uma \n\nsequ\u00eancia de baixa discrep\u00e2ncia, constru\u00edda com intuito de obter melhor distribui\u00e7\u00e3o dos \n\npar\u00e2metros no espa\u00e7o.  Uma introdu\u00e7\u00e3o b\u00e1sica a respeito das t\u00e9cnicas ser\u00e1 apresentada no t\u00f3pico \n\nde fundamenta\u00e7\u00e3o te\u00f3rica. \n\n    Uma vez treinada a RNA (chamada ent\u00e3o de metamodelo), ela estar\u00e1 pronta para ser \n\nutilizada no processo de otimiza\u00e7\u00e3o. A ferramenta aplicada para realizar a otimiza\u00e7\u00e3o foi o \n\nAlgoritmo Gen\u00e9tico (AG), amplamente utilizado para determina\u00e7\u00e3o de m\u00ednimos globais, por ser \n\neficiente na realiza\u00e7\u00e3o da varredura do espa\u00e7o de busca dos par\u00e2metros. Uma introdu\u00e7\u00e3o b\u00e1sica \n\nsobre seus conceitos tamb\u00e9m ser\u00e1 abordada no cap\u00edtulo da fundamenta\u00e7\u00e3o te\u00f3rica. \n\n\n\n \n\n \n\n5 \n\n \n\nEstudos j\u00e1 mostraram que metamodelos gerados por RNA podem ser utilizados como \n\nsubstitutos do simulador em casos mais simples de reservat\u00f3rio, por\u00e9m, ainda n\u00e3o existem muitos \n\nestudos para casos mais complexos. Assim, o presente trabalho visa avaliar o desempenho dos \n\nmetamodelos gerados por RNA no processo de ajuste de hist\u00f3rico, buscando analisar a influ\u00eancia \n\ndos dados de entrada na qualidade do modelo final e, desse modo, o potencial de aplica\u00e7\u00e3o da \n\nmetodologia para casos com caracter\u00edsticas reais.  \n\n1.1 Motiva\u00e7\u00e3o \n\nPara casos simples, as RNA s\u00e3o capazes de representar o problema proposto sem maiores \n\ndificuldades. Contudo, \u00e0 medida que a complexidade do problema aumenta os erros envolvidos \n\nno processo de treinamento da rede tamb\u00e9m tendem a aumentar e crit\u00e9rios mais robustos para \n\ndefini\u00e7\u00e3o de confiabilidade para utiliza\u00e7\u00e3o da ferramenta tornam-se necess\u00e1rios. Caso contr\u00e1rio, o \n\nproblema ser\u00e1 mal representado e resultados equivocados podem ser gerados. Assim sendo, a \n\navalia\u00e7\u00e3o da ferramenta faz-se necess\u00e1ria. \n\nH\u00e1 diversas aplica\u00e7\u00f5es de RNA na ind\u00fastria de petr\u00f3leo, em particular na \u00e1rea de an\u00e1lise de \n\nrisco, e poucas aplica\u00e7\u00f5es na \u00e1rea de ajuste de hist\u00f3rico. Por\u00e9m, devido \u00e0 semelhan\u00e7a no modo de \n\naplica\u00e7\u00e3o da ferramenta entre as duas \u00e1reas, acredita-se que esta possui caracter\u00edsticas \n\nimportantes que a tornam promissoras para serem utilizadas na \u00e1rea de ajuste de hist\u00f3rico.  \n\nEm adi\u00e7\u00e3o aos fatores citados acima, a principal motiva\u00e7\u00e3o para realiza\u00e7\u00e3o desse trabalho \n\nsurgiu do trabalho de Maschio et al. (2008), que realizaram estudos para aplica\u00e7\u00e3o da t\u00e9cnica em \n\ncasos de reservat\u00f3rios sint\u00e9ticos e ressaltaram a necessidade de aprimoramento do processo de \n\ntreinamento de redes neurais e gera\u00e7\u00e3o dos metamodelos. \n\n1.2 Objetivos \n\nO objetivo principal desse trabalho consiste em avaliar o potencial de aplica\u00e7\u00e3o de \n\nmetamodelos gerados a partir de RNA no problema de ajuste de hist\u00f3rico em casos pr\u00e1ticos. \n\nAtrav\u00e9s de casos anal\u00edticos de duas vari\u00e1veis aos quais possibilitam uma an\u00e1lise visual, s\u00e3o \n\navaliadas as caracter\u00edsticas e limita\u00e7\u00f5es da ferramenta e, posteriormente, atrav\u00e9s de um caso \n\n\n\n \n\n \n\n6 \n\n \n\nsint\u00e9tico de reservat\u00f3rio com caracter\u00edsticas reais, o real potencial de aplica\u00e7\u00e3o da ferramenta em \n\ncasos complexos ser\u00e1 avaliado.  \n\nOs objetivos espec\u00edficos s\u00e3o: \n\n1) Avaliar a influ\u00eancia do conjunto de treinamento, variando o tipo de amostragem e o \n\nn\u00famero de pontos amostrados, no desempenho dos metamodelos gerados. \n\n2) Avaliar formas de utiliza\u00e7\u00e3o do metamodelo em conjunto com o simulador no processo \n\nde ajuste de hist\u00f3rico para identificar de que maneira pode-se aplicar a ferramenta para \n\nmelhorar a efici\u00eancia do processo. \n\nPara realizar o processo de ajuste de hist\u00f3rico e testar diferentes tipos de dados de entrada, \n\nfoi necess\u00e1rio recorrer a diferentes t\u00e9cnicas de amostragem para gerar os conjuntos de entrada, e \n\nde uma ferramenta de otimiza\u00e7\u00e3o, para realizar a busca no espa\u00e7o de solu\u00e7\u00f5es, sendo essas etapas \n\nnecess\u00e1rias para alcan\u00e7ar o objetivo final (s\u00e3o complementares ao estudo). Portanto, realizar um \n\nestudo aprofundado a respeito dessas t\u00e9cnicas n\u00e3o faz parte dos objetivos. \n\n1.3 Organiza\u00e7\u00e3o da Disserta\u00e7\u00e3o \n\nEsse trabalho foi estruturado em sete cap\u00edtulos. No cap\u00edtulo um fez-se uma breve \n\nintrodu\u00e7\u00e3o ao tema escolhido. \n\nNo cap\u00edtulo dois \u00e9 realizada a fundamenta\u00e7\u00e3o te\u00f3rica, em que s\u00e3o mostrados os conceitos \n\nb\u00e1sicos necess\u00e1rios ao entendimento da aplica\u00e7\u00e3o das ferramentas utilizadas nesse trabalho. S\u00e3o \n\nabordados os temas de ajuste de hist\u00f3rico, redes neurais artificiais, algoritmo gen\u00e9tico e t\u00e9cnicas \n\nde amostragem. \n\nNo cap\u00edtulo tr\u00eas \u00e9 realizada uma breve revis\u00e3o bibliogr\u00e1fica, expondo os principais \n\ntrabalhos que serviram de base para montar a metodologia e aplicar a ferramenta. S\u00e3o abordados \n\nprincipalmente trabalhos relacionados aos temas de ajuste de hist\u00f3rico e redes neurais artificiais. \n\nO cap\u00edtulo quatro consiste na modelagem da metodologia. O presente trabalho foi realizado \n\nem duas etapas, primeiro seguindo uma metodologia geral, modelada com o intuito de deixar \n\nclaro os objetivos do estudo e escolher os casos de acordo com esses objetivos, e posteriormente, \n\nseguindo uma metodologia espec\u00edfica descrevendo o procedimento de ajuste adotado. \n\n\n\n \n\n \n\n7 \n\n \n\nO cap\u00edtulo cinco mostra os casos de estudo escolhidos para buscar alcan\u00e7ar os objetivos \n\npropostos. \n\nOs resultados da aplica\u00e7\u00e3o do procedimento de ajuste adotado aos casos escolhidos s\u00e3o \n\nmostrados no cap\u00edtulo seis. Nesse cap\u00edtulo observa\u00e7\u00f5es e discuss\u00f5es acerca dos resultados s\u00e3o \n\nrealizadas. \n\nPara finalizar, no cap\u00edtulo sete s\u00e3o listadas as conclus\u00f5es que se p\u00f4de chegar com os \n\nexperimentos e sugest\u00f5es futuras para melhorar os resultados e an\u00e1lises. \n\n\n\n\n\n \n\n \n\n9 \n\n \n\n2 CONCEITOS E FUNDAMENTA\u00c7\u00c3O TE\u00d3RICA \n\nO foco principal desse trabalho \u00e9 estudar o desempenho de metamodelos gerados por redes \n\nneurais artificiais (RNA) no processo de ajuste de hist\u00f3rico, sendo que as t\u00e9cnicas de amostragem \n\ne otimiza\u00e7\u00e3o aparecem como ferramentas auxiliares, utilizadas para viabilizar a aplica\u00e7\u00e3o da \n\nmetodologia e, portanto, complementares aos estudos realizados nesse trabalho. Logo, dar-se-\u00e1 \n\nmaior enfoque na fundamenta\u00e7\u00e3o dos temas de ajuste de hist\u00f3rico e RNA e, a respeito das \n\nt\u00e9cnicas de amostragem e otimiza\u00e7\u00e3o, ser\u00e1 feita apenas uma breve contextualiza\u00e7\u00e3o. \n\n2.1 Ajuste de hist\u00f3rico de produ\u00e7\u00e3o \n\nUma das \u00e1reas de maior import\u00e2ncia na ind\u00fastria de petr\u00f3leo \u00e9 a de estudo de reservat\u00f3rios, \n\npois influencia diretamente na defini\u00e7\u00e3o de estrat\u00e9gias de produ\u00e7\u00e3o e no planejamento \n\necon\u00f4mico. Segundo Aziz e Settari (1979), o objetivo principal de um estudo de reservat\u00f3rios \u00e9 a \n\nprevis\u00e3o do comportamento futuro e a determina\u00e7\u00e3o de meios para aumentar a recupera\u00e7\u00e3o final. \n\nGrande parte do planejamento envolvido nessa fase de estudos \u00e9 realizada segundo resultados \n\nobtidos atrav\u00e9s da simula\u00e7\u00e3o num\u00e9rica, procedimento que permite realizar previs\u00f5es e estudar \n\ndiferentes estrat\u00e9gias, possibilitando encontrar o melhor custo benef\u00edcio, que leva \u00e0 maior \n\nrecupera\u00e7\u00e3o final da jazida em estudo. Segundo Consentino (2001), a etapa mais importante, e \n\nque tamb\u00e9m demanda maior tempo dentro da simula\u00e7\u00e3o num\u00e9rica de reservat\u00f3rios, \u00e9 a de ajuste \n\nde hist\u00f3rico. \n\nO modelo de simula\u00e7\u00e3o utilizado para estudos de reservat\u00f3rios \u00e9 constru\u00eddo a partir da \n\ncaracteriza\u00e7\u00e3o geol\u00f3gica e de fluidos e s\u00e3o realizadas simplifica\u00e7\u00f5es para possibilitar sua \n\naplica\u00e7\u00e3o atrav\u00e9s do simulador de escoamento. Nessas etapas de simplifica\u00e7\u00e3o, diversas \n\ninforma\u00e7\u00f5es s\u00e3o perdidas. Al\u00e9m disso, devido ao alto custo para obt\u00ea-las, alguns processos de \n\nmedi\u00e7\u00e3o acabam sendo realizados de forma indireta, gerando incertezas. Dessa maneira, o \n\nmodelo de simula\u00e7\u00e3o obtido na fase de caracteriza\u00e7\u00e3o necessita de ajuste para que represente o \n\n\n\n \n\n \n\n10 \n\n \n\nmais pr\u00f3ximo poss\u00edvel o comportamento observado no reservat\u00f3rio real e possa ser utilizado para \n\nprevis\u00e3o de produ\u00e7\u00e3o. Segundo Aziz e Settari (1979) mesmo com o modelo ajustado, a \n\nconfiabilidade para previs\u00e3o decresce com o tempo, sendo assim, interessante \u2018atualizar\u2019 o estudo \n\nde simula\u00e7\u00e3o realizado ap\u00f3s certo per\u00edodo, efetuando novos ajustes dos par\u00e2metros com dados \n\nadicionais de hist\u00f3rico. A esse processo de ajuste e valida\u00e7\u00e3o do modelo de simula\u00e7\u00e3o aos dados \n\nhist\u00f3ricos d\u00e1-se o nome de ajuste de hist\u00f3rico. \n\nNesse processo, os atributos que descrevem o reservat\u00f3rio (permeabilidade, porosidade, \n\ntransmissibilidade das falhas etc.) s\u00e3o modificados e os dados de produ\u00e7\u00e3o e press\u00e3o s\u00e3o \n\ncomparados com o hist\u00f3rico. Atrav\u00e9s de um processo iterativo, os valores desses atributos s\u00e3o \n\nalterados at\u00e9 que o valor de uma fun\u00e7\u00e3o objetivo (FO) aceit\u00e1vel seja obtido. Essa fun\u00e7\u00e3o, \n\ncaracterizada pela diferen\u00e7a entre dados observados no campo real e dados obtidos atrav\u00e9s da \n\nsimula\u00e7\u00e3o do modelo de simula\u00e7\u00e3o, geralmente possui a forma descrita na Equa\u00e7\u00e3o 2.1 (Ertekin \n\net al., 2001, p.350). \n\n  ?[   (             )\n \n]\n\n \n\n   \n\n Equa\u00e7\u00e3o 2.1 \n\nem que   representa o n\u00famero total de amostras,    representa o peso atribu\u00eddo,        representa \n\no dado observado e        representa o dado simulado. A FO \u00e9 calculada com base nos dados de \n\nprodu\u00e7\u00e3o e press\u00e3o. Por\u00e9m, segundo Maschio (2006), outras informa\u00e7\u00f5es podem ser adicionadas \n\nao processo, como dados de s\u00edsmica (por exemplo, mapas de satura\u00e7\u00e3o e press\u00e3o) e dados \n\nprovenientes de testes e de perfilagem de po\u00e7os. \n\nSegundo Consentino (2001), o ajuste de hist\u00f3rico \u00e9 um procedimento de valida\u00e7\u00e3o de um \n\nmodelo, em que o desempenho passado do reservat\u00f3rio \u00e9 simulado e comparado com os dados \n\nobservados. Quando diferen\u00e7as s\u00e3o encontradas realizam-se modifica\u00e7\u00f5es nos par\u00e2metros de \n\nentrada. O objetivo final, portanto, \u00e9 minimizar as diferen\u00e7as nos dados din\u00e2micos reduzindo as \n\nincertezas dos dados est\u00e1ticos. \n\n \n\n\n\n \n\n \n\n11 \n\n \n\n2.1.1 Etapas do ajuste \n\nEm busca de um ajuste final aceit\u00e1vel, Ertekin et al. (2001) prop\u00f4s um procedimento \n\niterativo b\u00e1sico a ser seguido, ilustrado na Figura 2.1. \n\n \n\nFigura 2.1 \u2013 Fluxograma b\u00e1sico a ser seguido para realiza\u00e7\u00e3o do ajuste de hist\u00f3rico. (ERTEKIN \n\net al., 2001, p.351). \n\nSegundo fluxograma da Figura 2.1, a primeira etapa para se realizar um ajuste \u00e9 definir os \n\nobjetivos do ajuste. Essa etapa \u00e9 importante, pois influencia nas pr\u00f3ximas. Por exemplo, se o \n\nobjetivo for um estudo preliminar sabe-se que n\u00e3o \u00e9 necess\u00e1rio obter grandes precis\u00f5es nos \n\nresultados e \u00e9 poss\u00edvel chegar a respostas mais rapidamente. \n\nA segunda etapa \u00e9 dependente dos objetivos envolvidos no processo, recursos alocados pela \n\ncompanhia, prazos de entrega e disponibilidade dos dados.  \n\n1. Defini\u00e7\u00e3o dos objetivos do ajuste\n\n2. Defini\u00e7\u00e3o do m\u00e9todo a ser utilizado\n\n3. Determina\u00e7\u00e3o do dado de produ\u00e7\u00e3o hist\u00f3rico a ser ajustado e \n\ndefini\u00e7\u00e3o de crit\u00e9rios que descrevam a qualidade do ajuste\n\n6. Compara\u00e7\u00e3o dos resultados com os dados definidos na Etapa 3\n\n7. Altera\u00e7\u00e3o dos dados do reservat\u00f3rio da Etapa 4 dentro dos \n\nlimites\n\n8. Continuar as Etapas 5 a 7 at\u00e9 que o crit\u00e9rio estabelecido na \n\nEtapa 3 seja alcan\u00e7ado\n\n5. Simula\u00e7\u00e3o do modelo com os melhores dados de entrada \n\ndispon\u00edveis\n\n4. Defini\u00e7\u00e3o das vari\u00e1veis a serem ajustadas e seus limites de \n\nvaria\u00e7\u00e3o\n\n\n\n \n\n \n\n12 \n\n \n\nA terceira etapa depende da disponibilidade, da qualidade dos dados de produ\u00e7\u00e3o e do \n\nobjetivo do estudo da simula\u00e7\u00e3o.  \n\nNa quarta etapa, os par\u00e2metros escolhidos devem ser aqueles em que h\u00e1 maiores incertezas, \n\npor\u00e9m, que influenciam mais sobre o desempenho do reservat\u00f3rio. Engenheiros de reservat\u00f3rio, \n\nge\u00f3logos e equipe de operadores, que conhecem bem o campo em estudo, devem trabalhar em \n\nconjunto para o sucesso desta etapa. \n\nNa quinta etapa os dados dispon\u00edveis s\u00e3o simulados para posterior compara\u00e7\u00e3o do modelo \n\ncom o hist\u00f3rico e medi\u00e7\u00e3o de sua qualidade. \n\nNa sexta etapa \u00e9 realizada a compara\u00e7\u00e3o dos resultados obtidos com a simula\u00e7\u00e3o do modelo \n\ne o hist\u00f3rico. Os tipos de resultados a serem comparados dependem dos crit\u00e9rios definidos na \n\nEtapa 3. \n\nNa s\u00e9tima etapa, caso haja necessidade de ajuste, os atributos incertos do reservat\u00f3rio, \n\ndefinidos na Etapa 4, s\u00e3o alterados de forma a minimizar a diferen\u00e7a calculada na Etapa 6. \n\nPor fim, a oitava etapa consiste em repetir as etapas cinco a sete at\u00e9 o modelo de simula\u00e7\u00e3o \n\nrepresentar de forma aceit\u00e1vel o hist\u00f3rico. Uma vez ajustado o modelo com o hist\u00f3rico, ele pode \n\nser utilizado com maior confiabilidade em previs\u00f5es de comportamento.  \n\n2.1.2 Complexidade do ajuste \n\nEsse procedimento de ajuste, no entanto, possui caracter\u00edsticas intr\u00ednsecas a ele que o \n\ntornam complexo. Trata-se, segundo Consentino (2001), de um problema do tipo inverso, em que \n\nse conhecem os valores de sa\u00edda (dados de produ\u00e7\u00e3o), por\u00e9m n\u00e3o se conhecem os par\u00e2metros de \n\nentrada (atributos do reservat\u00f3rio) e depende, basicamente, da qualidade e da quantidade de \n\ndados dispon\u00edveis do reservat\u00f3rio em estudo, dos recursos alocados para o projeto e, \n\neventualmente, da experi\u00eancia e da atitude pessoal do engenheiro que trabalha no modelo. \n\nO problema relacionado \u00e0 qualidade e \u00e0 quantidade dos dados dispon\u00edveis decorre do fato \n\nde que algumas medi\u00e7\u00f5es s\u00e3o realizadas indiretamente (por exemplo, esquema de rateio para \n\nmedi\u00e7\u00e3o da produ\u00e7\u00e3o dos po\u00e7os de petr\u00f3leo), consequ\u00eancia do alto custo para obten\u00e7\u00e3o dessas \n\n\n\n \n\n \n\n13 \n\n \n\ninforma\u00e7\u00f5es. Pode haver, tamb\u00e9m, casos com pouco hist\u00f3rico de produ\u00e7\u00e3o dispon\u00edvel em que os \n\ndados n\u00e3o s\u00e3o suficientes para ajustar o modelo. \n\nDependendo do modelo, o n\u00famero de par\u00e2metros envolvidos pode ser alto (aumentam com \n\na complexidade do problema). Tipicamente, estes par\u00e2metros s\u00e3o a porosidade, permeabilidade \n\n(horizontal, vertical ou relativa), transmissibilidade de falhas, satura\u00e7\u00f5es residuais etc. Se o \n\nreservat\u00f3rio for heterog\u00eaneo pode possuir v\u00e1rias f\u00e1cies, o que tamb\u00e9m contribui para o aumento \n\ndo n\u00famero de par\u00e2metros. \n\nAl\u00e9m dos fatores supracitados, outro aspecto a ser considerado na fase de ajuste de \n\nhist\u00f3rico, segundo Consentino (2001) \u00e9 a n\u00e3o unicidade dos resultados, ou seja, o fato de ajustes \n\nigualmente satisfat\u00f3rios poderem ser obtidos por diferentes descri\u00e7\u00f5es de reservat\u00f3rio. Isso \n\nprov\u00e9m do fato de que a simula\u00e7\u00e3o num\u00e9rica \u00e9 um sistema matem\u00e1tico complexo, tipicamente \n\ncom apenas algumas vari\u00e1veis conhecidas (propriedades dos fluidos, produ\u00e7\u00f5es etc.) e, \n\npossivelmente, milhares de vari\u00e1veis desconhecidas (porosidades, permeabilidade de todos os \n\nblocos do modelo, etc.). Do ponto de vista matem\u00e1tico esse fato gera infinitos n\u00fameros de \n\nsolu\u00e7\u00f5es. \n\nUm exemplo da n\u00e3o unicidade \u00e9 mostrado na Figura 2.2, a qual mostra o gr\u00e1fico da \n\nprodu\u00e7\u00e3o de \u00e1gua versus o tempo de uma jazida de petr\u00f3leo. Os pontos em azul representam o \n\nhist\u00f3rico; a curva s\u00f3lida em vermelho representa o Caso base (modelo desajustado); a curva \n\ntracejada em verde representa o Ajuste 1 e a curva tracejada em azul representa o Ajuste 2. \n\n\n\n \n\n \n\n14 \n\n \n\n \n\nFigura 2.2 \u2013 Exemplo da n\u00e3o unicidade (m\u00faltiplas solu\u00e7\u00f5es) no problema de ajuste. \n\nNo gr\u00e1fico da Figura 2.2 tanto o modelo resultante do Ajuste 1 quanto o modelo resultante \n\ndo Ajuste 2 fornecem respostas com o mesmo grau de afastamento em rela\u00e7\u00e3o ao hist\u00f3rico e, \n\ndependendo da precis\u00e3o exigida no processo, ambos podem ser aceitos como resposta ao \n\nproblema. \n\nSegundo Ertekin et al. (2001) n\u00e3o h\u00e1 como fugir do problema da n\u00e3o unicidade dos \n\nresultados, por\u00e9m, utilizar o m\u00e1ximo de dados de produ\u00e7\u00e3o dispon\u00edvel e ajustar somente os dados \n\nmenos conhecidos do reservat\u00f3rio dentro dos limites aceit\u00e1veis podem resultar em melhores \n\najustes.  \n\n2.1.3 Tipos de ajuste: manual, autom\u00e1tico e assistido \n\nDevido \u00e0s caracter\u00edsticas destacadas no Subitem 2.1.2, o processo pode se tornar complexo \n\ne, dependendo do caso, o tempo demandado para realizar o ajuste pode ser grande. Na maioria \n\ndos casos este processo \u00e9 realizado de forma manual, pelo procedimento de tentativa e erro. \n\nPor\u00e9m, uma grande desvantagem desse tipo de ajuste, segundo Schiozer et al. (2009), \u00e9 o fato de \n\nele exigir grande esfor\u00e7o do profissional envolvido, o que leva \u00e0 limita\u00e7\u00e3o no n\u00famero de \n\nsimula\u00e7\u00f5es e consequente investiga\u00e7\u00e3o insatisfat\u00f3ria do espa\u00e7o de solu\u00e7\u00f5es (possibilidades e \n\n0 1000 2000 3000\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nAjuste 1\n\nAjuste 2\n\n\n\n \n\n \n\n15 \n\n \n\ncombina\u00e7\u00f5es dos atributos incertos dos modelos de simula\u00e7\u00e3o). Em contrapartida, possui a \n\nvantagem de contar com a experi\u00eancia do profissional, podendo levar a uma redu\u00e7\u00e3o do n\u00famero \n\nde simula\u00e7\u00f5es, identificando melhores solu\u00e7\u00f5es e maneiras de se chegar aos resultados \n\n(economizar excluindo an\u00e1lises desnecess\u00e1rias) mais rapidamente. Segundo Aziz e Settari \n\n(1979), devido ao fato das equa\u00e7\u00f5es de fluxo serem resolvidas de forma aproximada e diversas \n\nconsidera\u00e7\u00f5es serem realizadas durante o desenvolvimento do modelo, o julgamento de um \n\nprofissional da \u00e1rea com experi\u00eancia e conhecimento torna-se importante para interpretar os \n\nresultados e auxiliar no processo. \n\nCom o intuito de diminuir as desvantagens do ajuste manual, foi desenvolvido o ajuste \n\nautom\u00e1tico, que segundo Schiozer et al. (2009) consiste na utiliza\u00e7\u00e3o de algoritmos de \n\notimiza\u00e7\u00e3o respons\u00e1veis por minimizar uma Fun\u00e7\u00e3o Objetivo (FO), representativa da qualidade \n\ndo ajuste, sendo respons\u00e1vel por realizar todo o processo de forma independente. Contudo, esse \n\ntipo de ajuste n\u00e3o se mostrou eficiente devido \u00e0 grande variedade de problemas com \n\ncaracter\u00edsticas distintas.  \n\nNesse contexto surge o ajuste assistido, a fim de integrar as vantagens provenientes do \n\najuste manual e autom\u00e1tico. Dessa forma, partes do processo passam a ser automatizadas, \n\naumentando a confiabilidade (melhor avalia\u00e7\u00e3o do espa\u00e7o de solu\u00e7\u00f5es), e partes s\u00e3o realizadas \n\npelo profissional, identificando caracter\u00edsticas importantes do problema (identifica\u00e7\u00e3o dos \n\npar\u00e2metros envolvidos no ajuste, divis\u00e3o de um problema maior em problemas menores para \n\nsolucionar o problema em etapas, identifica\u00e7\u00e3o de melhores metodologias a serem aplicadas a \n\ncada caso etc.) para procurar simplificar e melhorar algumas etapas, tarefa que n\u00e3o \u00e9 poss\u00edvel ser \n\nrealizada em um processo autom\u00e1tico. \n\n2.1.4 Qualidade do ajuste \n\nA quest\u00e3o da defini\u00e7\u00e3o de crit\u00e9rios que indicam quando um modelo pode ser considerado \n\najustado \u00e9 importante, pois influencia na determina\u00e7\u00e3o da metodologia e consequentemente no \n\ntempo dispendido para sua aplica\u00e7\u00e3o. Segundo Ertekin et al. (2001) n\u00e3o h\u00e1 um padr\u00e3o para \n\ndefini\u00e7\u00e3o de quando um modelo est\u00e1 bem ajustado, pois este varia de empresa para empresa, de \n\n\n\n \n\n \n\n16 \n\n \n\npessoa para pessoa dentro da companhia ou de projeto para projeto pelo mesmo indiv\u00edduo. O \n\nimportante \u00e9 que o ajuste seja condizente com o objetivo do estudo.  \n\nUm exemplo desse aspecto \u00e9 mostrado atrav\u00e9s do gr\u00e1fico da Figura 2.3, que mostra a curva \n\ndo corte de \u00e1gua (fw) versus o tempo, em que s\u00e3o mostradas a curva do hist\u00f3rico de corte de \u00e1gua \n\n(curva tracejada) e a simula\u00e7\u00e3o do corte de \u00e1gua (curva s\u00f3lida). \n\n \n\nFigura 2.3 \u2013 Compara\u00e7\u00e3o do corte de \u00e1gua simulado com o hist\u00f3rico (Modifica\u00e7\u00e3o de ERTEKIN \n\net al., 2001, p.357). \n\nNo gr\u00e1fico da Figura 2.3, se o objetivo for realizar previs\u00e3o de produ\u00e7\u00e3o de \u00e1gua a fim de \n\nlidar com o avan\u00e7o de \u00e1gua, ent\u00e3o, o ajuste \u00e9 aceit\u00e1vel, pois a tend\u00eancia no final da curva est\u00e1 \n\nbem ajustada. Por\u00e9m, se o objetivo \u00e9 identificar localiza\u00e7\u00f5es de po\u00e7os ou futuras zonas a perfurar, \n\nent\u00e3o a qualidade n\u00e3o \u00e9 boa, pois o modelo est\u00e1 subestimando o corte de \u00e1gua do po\u00e7o. \n\nEstudos de reservat\u00f3rios t\u00eam mostrado que existem diversos m\u00ednimos locais nos modelos \n\nde simula\u00e7\u00e3o (desde reservat\u00f3rios em fase inicial de produ\u00e7\u00e3o a reservat\u00f3rios maduros). Esse \n\nfator dificulta a defini\u00e7\u00e3o de crit\u00e9rios, pois a presen\u00e7a de diversos m\u00ednimos locais pode fazer com \n\nque se encontrem diversos modelos parecidos. \n\nConsentino (2001) atenta para o fato de que \u201co problema da n\u00e3o unicidade apenas nos \n\npermite concluir que a descri\u00e7\u00e3o do reservat\u00f3rio encontrado \u00e9 apenas uma entre v\u00e1rias outras \n\npossibilidades que n\u00e3o contradizem os dados dispon\u00edveis\u201d (CONSENTINO, 2001, p.273). Dessa \n\nmaneira, o modelo de simula\u00e7\u00e3o ajustado dever\u00e1 ser capaz de capturar os principais mecanismos \n\nque governam a produ\u00e7\u00e3o do campo, por\u00e9m, nunca ser\u00e1 capaz de prever todas as poss\u00edveis \n\nfw\n\nTempo (Anos)\n\nSimula\u00e7\u00e3o do corte\nde \u00e1gua\n\nHis t\u00f3rico de corte de\n\u00e1g ua\n\n\n\n \n\n \n\n17 \n\n \n\nexce\u00e7\u00f5es para as regras gerais de deple\u00e7\u00e3o e deslocamento de fluidos no reservat\u00f3rio, devendo, \n\nportanto, ser considerado como um modelo probabil\u00edstico, o qual fornece uma estimativa \n\nconfi\u00e1vel da produ\u00e7\u00e3o futura do campo. \n\nDevido \u00e0 complexidade do problema, mesmo em posse de grande quantidade de dados de \n\nhist\u00f3rico pode-se fracassar no processo de ajuste e, nesse caso, segundo Aziz e Settari (1979), \n\npode ser um indicativo de que alguma aproxima\u00e7\u00e3o considerada no desenvolvimento do modelo \n\ndeva ser revista (estrutura geol\u00f3gica, comportamento PVT, extens\u00e3o do reservat\u00f3rio, presen\u00e7a de \n\naqu\u00edfero etc.), ou pode significar falta de precis\u00e3o dos resultados (inconsist\u00eancia nos dados).  \n\n2.1.5 Otimiza\u00e7\u00e3o dos valores dos atributos do reservat\u00f3rio  \n\nDentro do processo de ajuste, a etapa que demanda maior esfor\u00e7o computacional e tamb\u00e9m \n\nem pesquisa e desenvolvimento, segundo Schiozer et al. (2009), \u00e9 a de otimiza\u00e7\u00e3o dos valores \n\ndos atributos do reservat\u00f3rio, a fim de ajustar o modelo de simula\u00e7\u00e3o com o hist\u00f3rico. Isso \n\nocorre, pois com o aumento do n\u00famero de atributos incertos, o espa\u00e7o de solu\u00e7\u00f5es torna-se cada \n\nvez maior, o que dificulta a varredura de todo o espa\u00e7o. Al\u00e9m disso, por se tratar de um problema \n\ndo tipo inverso, o n\u00famero de m\u00faltiplas solu\u00e7\u00f5es tamb\u00e9m aumenta com o aumento do n\u00famero de \n\natributos.  \n\nDessa maneira, \u00eanfase maior tem sido dada para realiza\u00e7\u00e3o de estudos comparativos e \n\ndesenvolvimento de novas metodologias para aplica\u00e7\u00e3o nessa \u00e1rea. Outro aspecto que leva \u00e0 \n\nnecessidade de melhorias constantes na \u00e1rea de otimiza\u00e7\u00e3o est\u00e1 relacionado ao fato de que cada \n\ncaso possui particularidades, ou seja, uma ferramenta que serve para um determinado caso pode \n\nn\u00e3o servir para outro. \n\nDentre as alternativas, um procedimento interessante, difundido na \u00e1rea de an\u00e1lise de risco \n\ne que possui aplica\u00e7\u00e3o crescente na \u00e1rea de ajuste de hist\u00f3rico \u00e9 a utiliza\u00e7\u00e3o de metamodelos \n\n(proxies). \n\n \n\n\n\n \n\n \n\n18 \n\n \n\n2.1.6 Utiliza\u00e7\u00e3o de metamodelos gerados por redes neurais artificiais no processo de \n\najuste de hist\u00f3rico \n\nUm metamodelo consiste basicamente em um modelo, definido segundo fun\u00e7\u00f5es \n\nmatem\u00e1ticas ou estat\u00edsticas, o qual apresenta um dado padr\u00e3o de sa\u00edda para um dado padr\u00e3o de \n\nentrada.  \n\nUma caracter\u00edstica importante dos metamodelos \u00e9 que, uma vez gerados, eles simulam os \n\nresultados rapidamente e, por conta disso, s\u00e3o bastante promissores para serem utilizados como \n\nsubstitutos do simulador. Segundo Avansi (2008), os metamodelos, apesar de serem modelos \n\nsimplificados e com menor confiabilidade nos resultados, podem substituir a simula\u00e7\u00e3o num\u00e9rica \n\nem situa\u00e7\u00f5es que demandam muitas simula\u00e7\u00f5es e n\u00e3o \u00e9 necess\u00e1ria grande precis\u00e3o. \n\nExistem diferentes ferramentas dispon\u00edveis para gera\u00e7\u00e3o de metamodelos. Segundo \n\nZubarev (2009) o modelo de regress\u00e3o polinomial \u00e9 bastante utilizado na ind\u00fastria de petr\u00f3leo \n\ndevido a sua f\u00e1cil compreens\u00e3o, flexibilidade e efici\u00eancia computacional. Outra ferramenta \n\nutilizada para gera\u00e7\u00e3o de metamodelos \u00e9 a Rede Neural Artificial (RNA), utilizada em diversas \n\n\u00e1reas, sendo bastante difundida em an\u00e1lise de risco na \u00e1rea de petr\u00f3leo. Segundo Maschio et al. \n\n(2008), as RNA s\u00e3o estruturas capazes de captar de forma eficiente as n\u00e3o linearidades em \n\nproblemas tipicamente encontrados nos processos envolvendo simula\u00e7\u00e3o de reservat\u00f3rios, o que \n\nconstitui a maior motiva\u00e7\u00e3o para utiliza\u00e7\u00e3o dessa t\u00e9cnica para o problema de ajuste de hist\u00f3rico.  \n\nSendo assim, o Subitem 2.2 a seguir ir\u00e1 mostrar os principais conceitos para compreens\u00e3o \n\ndo funcionamento b\u00e1sico de uma RNA. A teoria relacionada \u00e0 RNA \u00e9 ampla e, por conta disso, \n\nser\u00e3o mostrados apenas os aspectos relevantes ao presente trabalho. \n\n2.2 Redes neurais artificiais \n\nAs tarefas realizadas pelo ser humano no dia a dia, como respira\u00e7\u00e3o, pensamento, leitura \n\netc. s\u00e3o comandadas pelo c\u00e9rebro, composto por uma grande quantidade de neur\u00f4nios \n\n(aproximadamente 10\n11\n\n), interconectados com outros milhares, formando, assim, as redes neurais \n\nbiol\u00f3gicas. Parte da estrutura da rede neural do c\u00e9rebro \u00e9 formada antes do nascimento do ser \n\nhumano e parte \u00e9 modificada ao longo da vida.  \n\n\n\n \n\n \n\n19 \n\n \n\nAcredita-se que todas as fun\u00e7\u00f5es neurais, tais como a mem\u00f3ria, s\u00e3o armazenadas nos \n\nneur\u00f4nios e nas conex\u00f5es entre eles. O aprendizado \u00e9 visto como o estabelecimento, perda ou \n\nmodifica\u00e7\u00e3o de conex\u00f5es. A partir desse conceito surge a inspira\u00e7\u00e3o para cria\u00e7\u00e3o das redes \n\nneurais artificias.  \n\nUtilizando neur\u00f4nios artificiais (vers\u00e3o extremamente simplificada de um neur\u00f4nio \n\nbiol\u00f3gico) estrutura-se a rede neural artificial. Apesar de terem capacidade infinitamente inferior \n\ndo que a do c\u00e9rebro humano, as RNA podem ser treinadas para realizar in\u00fameros tipos de tarefas. \n\n2.2.1 Redes neurais biol\u00f3gicas \n\nO neur\u00f4nio pode ser dividido basicamente em tr\u00eas partes principais: os dendritos, o corpo \n\ncelular e o ax\u00f4nio. A Figura 2.4 mostra um esquema simplificado de um neur\u00f4nio biol\u00f3gico. \n\n \n\nFigura 2.4 \u2013 Esquema simplificado de um neur\u00f4nio biol\u00f3gico\n2\n. \n\nOs sinais oriundos de outros neur\u00f4nios s\u00e3o receptados pelos dendritos e enviados ao corpo \n\ncelular, respons\u00e1vel por processar todas as informa\u00e7\u00f5es receptadas e o ax\u00f4nio envia o sinal \n\nprocessado para outros neur\u00f4nios. O contato entre o ax\u00f4nio de um neur\u00f4nio e o dendrito de outro \n\n                                                 \n2\n Retirada do site http://www.din.uem.br/~jmpinhei/IA-CC/08Redes%20Neurais%20Artificiais.pdf. Acesso em 27 \n\nde Abril de 2011 \u00e0s 15h20. \n\nhttp://www.din.uem.br/~jmpinhei/IA-CC/08Redes%20Neurais%20Artificiais.pdf\n\n\n \n\n \n\n20 \n\n \n\n\u00e9 chamado de sinapse. A disposi\u00e7\u00e3o dos neur\u00f4nios e as intensidades das sinapses dos neur\u00f4nios, \n\ndeterminadas por processos qu\u00edmicos complexos, estabelecem as fun\u00e7\u00f5es das redes neurais.  \n\n2.2.2 Neur\u00f4nio artificial \n\nUm exemplo ilustrativo de um neur\u00f4nio artificial \u00e9 mostrado na Figura 2.5.  \n\n \n\nFigura 2.5 \u2013 Esquema b\u00e1sico de neur\u00f4nio artificial (HAGAN et al., 1996, p.2-3). \n\nNo neur\u00f4nio da Figura 2.5, a entrada escalar p \u00e9 multiplicada pelo peso w formando o \n\nproduto w?p, sendo um dos elementos de entrada do neur\u00f4nio. Al\u00e9m dessa entrada existe outra \n\nchamada de auxiliar, de valor \u201c1\u201d, que \u00e9 multiplicada pelo peso auxiliar chamado \u201cbias\u201d (ou \n\noffset) b. Os valores s\u00e3o ent\u00e3o somados, resultando no valor n, geralmente referenciado \n\nefetivamente como entrada do neur\u00f4nio, que ent\u00e3o passa pela fun\u00e7\u00e3o de transfer\u00eancia (ou de \n\nativa\u00e7\u00e3o) f, que, por sua vez, gera a sa\u00edda escalar do neur\u00f4nio a. \n\nFazendo uma analogia com o neur\u00f4nio biol\u00f3gico, o peso w representa a for\u00e7a da sinapse, o \n\ncorpo celular \u00e9 representado pelo somador e pela fun\u00e7\u00e3o de transfer\u00eancia e a sa\u00edda a representa o \n\nsinal mandado ao ax\u00f4nio. A sa\u00edda do neur\u00f4nio \u00e9 calculada pela Equa\u00e7\u00e3o 2.2 (Hagan et al., 1996, \n\np.2-3):  \n\n   (     ) Equa\u00e7\u00e3o 2.2 \n\nem que os escalares w e b s\u00e3o ajust\u00e1veis. \n\n? fp\nan\n\nb\n\n1\n\nw\n\nEntradas Neur\u00f4nio\n\n)( bwpfa ??\n\n\n\n \n\n \n\n21 \n\n \n\n2.2.3 Fun\u00e7\u00e3o de transfer\u00eancia (de ativa\u00e7\u00e3o) \n\nExistem diversas fun\u00e7\u00f5es de transfer\u00eancia que podem ser utilizadas para calcular a sa\u00edda de \n\num neur\u00f4nio. Para problemas de aproxima\u00e7\u00e3o de fun\u00e7\u00e3o, as mais utilizadas s\u00e3o as do tipo \n\nsigmoide e linear. A Figura 2.6 mostra a fun\u00e7\u00e3o do tipo linear (a) e a do tipo tangente hiperb\u00f3lica \n\n(b).  \n\n \n\n(a)                                                                        (b) \n\nFigura 2.6 \u2013 Fun\u00e7\u00e3o de transfer\u00eancia do tipo linear (a) e tangente hiperb\u00f3lica (b). \n\nA Figura 2.6 mostra como a fun\u00e7\u00e3o varia em rela\u00e7\u00e3o ao par\u00e2metro de modelagem x. Os \n\nlimites de varia\u00e7\u00e3o para o par\u00e2metro utilizado foi de -1 a 1 para a fun\u00e7\u00e3o da Figura 2.6 (a) e -3 a 3 \n\npara a fun\u00e7\u00e3o da Figura 2.6 (b), tal que os limites de varia\u00e7\u00e3o da resposta F(x) varie de -1 a 1. \n\n2.2.4 Arquitetura \n\nTipicamente um neur\u00f4nio possui mais de uma entrada. Al\u00e9m disso, geralmente, as redes \n\npossuem diversos neur\u00f4nios e podem ter mais de uma camada. A Figura 2.7 mostra um exemplo \n\nde uma rede neural artificial com tr\u00eas camadas. \n\n \n\n-1 -0.5 0 0.5 1\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\nx\n\nF\n(x\n\n)\n\nLinear\n\n-2 0 2\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\nx\n\nF\n(x\n\n)\n\nTangente hiperb\u00f3lica\n\n\n\n \n\n \n\n22 \n\n \n\n \n\nFigura 2.7 \u2013 Ilustra\u00e7\u00e3o de uma rede neural de 3 camadas (HAGAN et al., 1996, p.2-11). \n\nQuanto \u00e0 arquitetura, a rede da Figura 2.7 \u00e9 do tipo direta ou feedforward (a sa\u00edda de um \n\nneur\u00f4nio da i-\u00e9sima camada n\u00e3o pode ser utilizada como entrada para neur\u00f4nios de camada \n\nmenor ou igual a i), m\u00faltiplas camadas (tr\u00eas) e completamente conectada (cada entrada \u00e9 ligada a \n\ntodos os neur\u00f4nios da camada seguinte).  \n\nQuanto \u00e0 nomenclatura utilizada, o \u00edndice sobrescrito refere-se \u00e0 camada, o \u00edndice subscrito \n\nda esquerda representa o neur\u00f4nio de destino e o da direita representa a entrada de onde o sinal \n\nprov\u00e9m. Assim, a rede em quest\u00e3o possui R entradas, S\n1\n neur\u00f4nios na primeira camada, S\n\n2\n \n\nneur\u00f4nios na segunda camada e S\n3\n neur\u00f4nios na terceira camada. Cada elemento do vetor de \n\nentrada (p) \u00e9 conectado aos neur\u00f4nios da primeira camada atrav\u00e9s da matriz de pesos (W). Cada \n\nneur\u00f4nio possui uma entrada auxiliar, na qual possui um peso auxiliar (bi), chamado de bias, um \n\nsomador, uma fun\u00e7\u00e3o de transfer\u00eancia e uma sa\u00edda (ai). A terceira camada, que gera a sa\u00edda da \n\nrede, \u00e9 chamada de camada de sa\u00edda. As outras camadas s\u00e3o chamadas ocultas ou intermedi\u00e1rias \n\n(hidden layer).  \n\nA rede da Figura 2.7 tamb\u00e9m pode ser apresentada na forma abreviada, conforme mostra a \n\nFigura 2.8 abaixo: \n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n? f\n3\n\n?\n\n?\n\nf\n3\n\nf\n3\n\n.\n\n.\n\n.\n\n1\n\n1\n\n1\n\n1\n\n1 1\n\n1\n\n1\n\n1\n\n1\n\n1\nb\n\n1\n\n2\nb\n\n1\n1\n\nS\nb\n\n1\np\n\n2\np\n\nR\np\n\n1\n\n1\nn\n\n1\n\n2\nn\n\n1\n1\n\nS\nn\n\n2\n1\n\nS\nb\n\n2\n\n2\nb\n\n2\n\n1\nb\n\n2\n\n1\nn\n\n2\n\n2\nn\n\n2\n2\n\nS\nn\n\n3\n\n1\nn\n\n3\n\n2\nn\n\n3\n3\n\nS\nn\n\n3\n\n1\na\n\n3\n\n2\na\n\n3\n3\n\nS\na\n\n3\n3\n\nS\nb\n\n3\n\n2\nb\n\n3\n\n1\nb\n\n1\n\n1\na\n\n1\n\n2\na\n\n1\n1\n\nS\na\n\n2\n\n1\na\n\n2\n\n2\na\n\n2\n2\n\nS\na\n\n1\n\n1,1\nw\n\n1\n\n,\n1\n\nRS\nw\n\n2\n\n1,1\nw\n\n2\n\n,\n12\n\nSS\nw\n\n3\n\n1,1\nw\n\n2\n\n,\n23\n\nSS\nw\n\n.\n\n.\n\n.\n.\n.\n.\n\n.\n\n.\n\n.\n\nEntradas 1\u00aa camada 2\u00aa camada 3\u00aa camada\n\n? ?1111 bpWfa ??? ? ?21222 baWfa ??? ? ?32333 baWfa ???\n? ?? ?? ?3211122333 bbbpWfWfWfa ???????\n\nf \n1\n \n\nf \n1\n \n\nf \n1\n f \n\n2\n \n\nf \n2\n \n\nf \n2\n \n\n\n\n \n\n \n\n23 \n\n \n\n \n\nFigura 2.8 \u2013 Rede multicamadas na forma abreviada (HAGAN et al., 1996, p.2-12).   \n\n2.2.5 Escolha da arquitetura da rede \n\nAlgumas vari\u00e1veis da rede s\u00e3o determinadas pela especifica\u00e7\u00e3o do problema proposto, \n\ncomo por exemplo, o n\u00famero de entradas e sa\u00eddas da rede. Se o problema em estudo possui \n\nquatro par\u00e2metros incertos e um valor de sa\u00edda, ent\u00e3o, o n\u00famero de neur\u00f4nio nas camadas de \n\nentrada e sa\u00edda ser\u00e3o quatro e um, respectivamente. A fun\u00e7\u00e3o de transfer\u00eancia a ser utilizada pode \n\nser escolhida segundo a caracter\u00edstica desejada no sinal de sa\u00edda da rede. Por exemplo, se deseja \n\nter valores -1 ou 1 na sa\u00edda, a fun\u00e7\u00e3o de transfer\u00eancia escolhida deve ser do tipo degrau unit\u00e1rio. \n\nAs entradas auxiliares fornecem vari\u00e1veis extras \u00e0 rede. Segundo Hagan et al. (1996), \n\n\u201cpode-se esperar que redes com entradas auxiliares sejam mais poderosas do que redes sem\u201d \n\n(HAGAN et al., 1996 p.2-12). Por exemplo, se as entradas p forem todas zero, a presen\u00e7a das \n\nentradas auxiliares impede que a entrada n para a fun\u00e7\u00e3o de transfer\u00eancia seja zero.  \n\nQuanto ao n\u00famero de camadas de neur\u00f4nios a ser empregado, a maior parte dos problemas \n\nutilizam de 2 a 3 camadas.  Segundo Hagan et al. (1996), \u201cUma rede com duas camadas e tendo \n\nfun\u00e7\u00e3o de transfer\u00eancia do tipo sigmoide nas camadas intermedi\u00e1rias e do tipo linear na sa\u00edda \n\npode ser treinada para aproximar in\u00fameras fun\u00e7\u00f5es arbitrariamente bem. Redes de camadas \n\n\u00fanicas n\u00e3o\u201d (HAGAN et al., 1996 p.2-12). Silva e Oliveira (2004) sugerem que a utiliza\u00e7\u00e3o de \n\num grande n\u00famero de camadas ocultas n\u00e3o \u00e9 recomend\u00e1vel, uma vez que elas recebem uma \n\nestimativa do erro produzido na camada de sa\u00edda e, quanto mais distante a camada oculta estiver \n\nW\n1\n\nb\n1\n\n f \n1+\n\n1\n\nS\n1\nx R\n\nS\n1\nx 1\n\nS\n1\nx 1\n\nn\n1\n\nR\n1\nx 1\n\np\n\nR\n\nW\n2\n\nb\n2\n\n f \n2+\n\n1\n\nS\n2\nxS\n\n1\n\nS\n2\nx 1\n\nn\n2\n\na\n1\n\nS\n1\nx 1\n\nS\n2\nx 1\n\nS\n1\n\nS\n2\n\nW\n3\n\nb\n3\n\n f \n3+\n\n1\n\nS\n3\nx S\n\n2\n\nS\n3\nx 1\n\nn\n3\n\na\n2\n\nS\n3\nx 1\n\nS\n3\n\nS\n2\nx 1\n\na\n3\n\nS\n3\nx 1\n\nEntrada 1\u00aa camada 2\u00aa camada 3\u00aa camada\n\n? ?1111 bpWfa ??? ? ?21222 baWfa ??? ? ?32333 baWfa ???\n? ?? ?? ?3211122333 bbbpWfWfWfa ???????\n\n\n\n \n\n \n\n24 \n\n \n\nda camada de sa\u00edda, menos precisa \u00e9 a estimativa do erro. Assim, uma camada \u00e9 o suficiente para \n\nproblemas menores e, para problemas maiores, duas camadas devem ser suficientes. \n\nQuanto ao n\u00famero de neur\u00f4nios das camadas ocultas, segundo Silva e Oliveira (2004) o \n\nvalor \u00e9 escolhido de forma emp\u00edrica, por\u00e9m deve-se atentar para n\u00e3o utilizar nem valores de mais \n\n(leva a memoriza\u00e7\u00e3o \u2013 overfitting) nem de menos (a rede n\u00e3o ser\u00e1 capaz de aprender os padr\u00f5es \n\ndesejados). Um procedimento existente para determina\u00e7\u00e3o do n\u00famero de neur\u00f4nios, segundo os \n\nautores, consiste em utilizar um n\u00famero de sinapses dez vezes menor do que o n\u00famero de \n\nexemplos utilizados para treinamento.  \n\nPortanto, infelizmente, n\u00e3o existe nenhuma teoria ou regra a ser seguida para determina\u00e7\u00e3o \n\nda quantidade de camadas e neur\u00f4nios a serem utilizados, sendo na maioria dos casos definidos \n\nde forma emp\u00edrica. \n\n2.2.6 Treinamento de redes neurais artificiais \n\nO processo de treinamento consiste em utilizar uma regra de aprendizado e realizar \n\nmodifica\u00e7\u00f5es (ajustes) nos valores dos pesos, at\u00e9 que os n\u00edveis de discrep\u00e2ncia entre os resultados \n\ngerados na sa\u00edda da rede e os padr\u00f5es apresentados (exemplos de como a rede deve se comportar, \n\nou seja, conjunto de sa\u00eddas desejadas) sejam aceit\u00e1veis. Assim, o objetivo do treinamento \u00e9 fazer \n\ncom que a rede seja capaz de reproduzir um padr\u00e3o de comportamento desejado quando \n\napresentado certo padr\u00e3o de entrada. No processo chamado supervisionado, s\u00e3o apresentados \n\npadr\u00f5es de entradas e sa\u00eddas desejadas (target), e os valores dos pesos da rede s\u00e3o ajustados \n\natrav\u00e9s de um algoritmo de aprendizado. Ao final, se o treinamento for bem sucedido, a rede ser\u00e1 \n\ncapaz de reproduzir o mesmo padr\u00e3o de sa\u00edda todas as vezes que novas entradas forem \n\napresentadas a ela. \n\nExistem diversas regras utilizadas para aprendizado supervisionado, sendo uma delas \n\nbaseada na otimiza\u00e7\u00e3o de uma fun\u00e7\u00e3o de desempenho, que mede a qualidade de generaliza\u00e7\u00e3o da \n\nrede. \u00c9 nessa regra que se baseia o algoritmo de retro propaga\u00e7\u00e3o, bastante utilizado em diversos \n\nproblemas. A t\u00e9cnica de treinamento utilizada nesse trabalho \u00e9 a de Levenberg-Marquardt (LM) \n\ncom regulariza\u00e7\u00e3o Bayesiana, que \u00e9 uma varia\u00e7\u00e3o do m\u00e9todo de Retro propaga\u00e7\u00e3o (RP).  \n\n\n\n \n\n \n\n25 \n\n \n\n2.2.6.1 Algoritmo de retro propaga\u00e7\u00e3o (backpropagation)  \n\nAs etapas b\u00e1sicas do algoritmo de retro propaga\u00e7\u00e3o (RP) s\u00e3o ilustradas no fluxograma da \n\nFigura 2.9. \n\n \n\nFigura 2.9 \u2013 Fluxograma b\u00e1sico do algoritmo de RP. \n\nInicialmente \u00e9 fornecido \u00e0 rede as entradas e sa\u00eddas desejadas (exemplos de como deseja \n\nque a rede se comporte). As entradas s\u00e3o propagadas atrav\u00e9s das camadas da rede e as respectivas \n\nsa\u00eddas s\u00e3o geradas. Essas sa\u00eddas s\u00e3o comparadas com as sa\u00eddas desejadas e os erros entre elas s\u00e3o \n\ncalculados. Caso o valor do erro obtido n\u00e3o esteja de acordo com o objetivo, os valores dos pesos \n\nda rede s\u00e3o alterados de forma a reduzir esse valor. Para tanto, utiliza-se a regra da cadeia para \n\nretro propagar o valor do erro, partindo da camada de sa\u00edda at\u00e9 chegar \u00e0 camada de entrada, \n\npossiblitanto a atualiza\u00e7\u00e3o dos pesos de acordo com o erro obtido. Ap\u00f3s atualiza\u00e7\u00e3o dos pesos \n\ninicia-se nova itera\u00e7\u00e3o, propagando as entradas pela rede e calculando novamente o erro.  \n\nUma observa\u00e7\u00e3o importante sobre o  algoritmo, j\u00e1 mencionada anteriormente, \u00e9 que, ao \n\nusar a regra da cadeia, cada camada oculta recebe uma estimativa do erro da camada subsequente, \n\nde forma que, quanto maior for o n\u00famero de camadas, maior ser\u00e1 a incerteza do valor do erro que \n\na camada de entrada receber\u00e1. Uma abordagem mais completa do algoritmo pode ser encontrada \n\nem Hagan et al. (1996). \n\nApresenta\u00e7\u00e3o \n\ndos padr\u00f5es:\n\n{p1,t1},...,{pn,tn}\n\nRede Neural Artificial\nSa\u00eddas:\n\n{a1,...,an}\n\nSa\u00eddas \n\ndesejadas:\n\n{t1,...,tn}\n\nComputa\u00e7\u00e3o do \n\nerro:\n\nE=[(t-a)\nT\n\u00b7(t-a)]\n\nAtualiza\u00e7\u00e3o dos pesos\n\nCrit\u00e9rios de \n\nconverg\u00eancia \n\nalcan\u00e7ados?\n\nN\u00e3o\n\nSim\n\nIn\u00edcio\n\nPropaga\u00e7\u00e3o direta\n\n(Feedforward)\n\nRetro propaga\u00e7\u00e3o do erro\n\n(Backpropagation) Fim\n\nvia algoritmo de treinamento\n\n  Propaga\u00e7\u00e3o das entradas \n\n atrav\u00e9s dos pesos da rede\n\n\n\n \n\n \n\n26 \n\n \n\n2.2.6.2 Varia\u00e7\u00f5es do algoritmo de Retro propaga\u00e7\u00e3o  \n\nO algoritmo de RP, apesar de ter revolucionado as pesquisas de RNA, segundo Hagan et al. \n\n(1996) \u201c\u00e9 muito lento e ineficaz para muitas aplica\u00e7\u00f5es pr\u00e1ticas\u201d (HAGAN et al., 1996, p.12-1). \n\nSegundo Hagan e Menhaj (1994), desde que o algoritmo se popularizou, surgiram diversas \n\nmetodologias para acelerar sua converg\u00eancia, sendo a incorpora\u00e7\u00e3o de m\u00e9todos num\u00e9ricos ao \n\nalgoritmo padr\u00e3o uma dessas vertentes. O algoritmo de Levenberg-Marquardt (LM), uma \n\nvaria\u00e7\u00e3o do m\u00e9todo de Newton\n3\n, \u00e9 utilizado para minimiza\u00e7\u00e3o da soma quadrada de fun\u00e7\u00f5es n\u00e3o \n\nlineares. Trata-se de uma caracter\u00edstica importante que se enquadra muito bem no contexto de \n\nRNA, em que o desempenho \u00e9 medido pela soma ou m\u00e9dia do quadrado do erro. Por esse motivo, \n\nesse algoritmo foi escolhido para ser aplicado ao presente trabalho. Hagan e Menhaj (1994) \n\npublicaram um artigo no qual o algoritmo \u00e9 aplicado a alguns problemas de aproxima\u00e7\u00e3o de \n\nfun\u00e7\u00f5es e, segundo os autores, \u00e9 muito eficiente para aplica\u00e7\u00e3o em redes que tenham at\u00e9 algumas \n\ncentenas de pesos. O desenvolvimento desse algoritmo pode ser encontrado em Hagan et al. \n\n(1996).  \n\n2.2.6.3 Capacidade de generaliza\u00e7\u00e3o de uma rede neural artificial \n\nUm dos maiores problemas que ocorrem durante o processo de treinamento de uma RNA \u00e9 \n\na memoriza\u00e7\u00e3o (overfitting) dos dados. Em tais situa\u00e7\u00f5es, a rede acaba memorizando as sa\u00eddas ao \n\ninv\u00e9s de aprender o padr\u00e3o de gera\u00e7\u00e3o para poder generalizar para outras entradas. Como \n\nresultado, a rede produz sa\u00eddas diferentes do padr\u00e3o desejado quando s\u00e3o apresentadas entradas \n\nque n\u00e3o foram utilizadas para treinamento.   \n\nDois m\u00e9todos utilizados para melhorar a capacidade de generaliza\u00e7\u00e3o de uma RNA s\u00e3o o \n\nm\u00e9todo de regulariza\u00e7\u00e3o e de parada prematura (early stopping), descritos suscintamente a \n\nseguir. \n\nO m\u00e9todo de parada prematura consiste em dividir o conjunto de treinamento em tr\u00eas \n\nsubconjuntos: treinamento, valida\u00e7\u00e3o e teste. O primeiro conjunto \u00e9 utilizado para atualiza\u00e7\u00e3o dos \n\npesos, ou seja, aplica\u00e7\u00e3o do algoritmo de aprendizagem. O segundo conjunto \u00e9 utilizado durante \n\n                                                 \n3\n A teoria referida pode ser encontrada no livro de Hagan et al., 1996. \n\n\n\n \n\n \n\n27 \n\n \n\no treinamento para monitorar o desempenho da rede. Se o erro de valida\u00e7\u00e3o come\u00e7ar a subir \n\nenquanto o erro de treinamento continua diminuindo \u00e9 sinal de que est\u00e1 ocorrendo memoriza\u00e7\u00e3o \n\ndos resultados. Nessa hora o treinamento \u00e9 interrompido. O terceiro conjunto \u00e9 utilizado ap\u00f3s o \n\ntreinamento para testar a capacidade de generaliza\u00e7\u00e3o da rede.  \n\nJ\u00e1 o m\u00e9todo de regulariza\u00e7\u00e3o consiste em modificar a fun\u00e7\u00e3o de desempenho. O m\u00e9todo \n\nbayesiano, em particular, utiliza informa\u00e7\u00f5es adicionais para c\u00e1lculo dessa fun\u00e7\u00e3o, que assume a \n\nforma mostrada na Equa\u00e7\u00e3o 2.3 (Foresse e Hagan, 1997, p.1). \n\n            Equa\u00e7\u00e3o 2.3 \n\nem que    representa a soma do quadrado do erro entre sa\u00edda desejada e gerada pela rede,    \n\nrepresenta a soma do quadrado dos pesos da rede e   e   s\u00e3o par\u00e2metros da fun\u00e7\u00e3o objetivo. \n\nBasicamente considera-se que os pesos da rede s\u00e3o vari\u00e1veis aleat\u00f3rias com uma determinada \n\ndistribui\u00e7\u00e3o de probabilidades, de forma que os par\u00e2metros   e   estejam atrelados a eles, e, seus \n\nvalores s\u00e3o, ent\u00e3o, estimados utilizando t\u00e9cnicas estat\u00edsticas. A descri\u00e7\u00e3o do m\u00e9todo bayesiano de \n\nregulariza\u00e7\u00e3o pode ser encontrada no trabalho de Foresse e Hagan (1997) e no livro de Bishop \n\n(1995). \n\n2.3 Algoritmo gen\u00e9tico \n\nAo gerar o metamodelo, que no caso de RNA nada mais \u00e9 do que uma rede treinada, ele \n\nest\u00e1 pronto para ser utilizado no lugar do simulador no processo de otimiza\u00e7\u00e3o. Nesse trabalho \n\nfoi utilizado o algoritmo gen\u00e9tico (AG) para esse processo. \n\nO AG consiste em uma metodologia de otimiza\u00e7\u00e3o baseada no mecanismo de sele\u00e7\u00e3o \n\nnatural proposto pela teoria de Darwin, na qual diz que indiv\u00edduos mais adaptados ao meio \n\nsobrevivem e contribuem para propaga\u00e7\u00e3o das gera\u00e7\u00f5es. Assim, as caracter\u00edsticas dos indiv\u00edduos \n\nmais fortes v\u00e3o se propagando para as futuras gera\u00e7\u00f5es, enquanto que, caracter\u00edsticas \n\ndesfavor\u00e1veis v\u00e3o se tornando menos frequentes. Proposto por Holland\n4\n em 1975, inicialmente \n\nfoi desenvolvido e aplicado em m\u00e1quinas com intuito de encontrar explica\u00e7\u00f5es para os processos \n\n                                                 \n4\n John Henry Holland \u00e9 conhecido por ter criado os algoritmos gen\u00e9ticos. \n\n\n\n \n\n \n\n28 \n\n \n\nadaptativos em sistemas naturais. Por\u00e9m, devido ao seu grande potencial em varrer espa\u00e7os \n\nmultidimensionais, passou a ser amplamente empregado em problemas de otimiza\u00e7\u00e3o.  \n\nNo AG um indiv\u00edduo \u00e9 constitu\u00eddo pelo seu cromossomo, o qual representa uma poss\u00edvel \n\nresposta ao problema (conjunto de vari\u00e1veis). O processo de evolu\u00e7\u00e3o ocorre alterando-se os \n\nvalores de cada gene (vari\u00e1vel) do cromossomo.  \n\nO funcionamento b\u00e1sico do algoritmo consiste em, a partir de uma popula\u00e7\u00e3o inicial de \n\nindiv\u00edduos (cromossomos), realizar opera\u00e7\u00f5es de recombina\u00e7\u00e3o (crossover), muta\u00e7\u00e3o e sele\u00e7\u00e3o \n\nem um processo iterativo de modo que, a cada gera\u00e7\u00e3o (itera\u00e7\u00e3o), a popula\u00e7\u00e3o gere indiv\u00edduos \n\nmais aptos (melhores solu\u00e7\u00f5es) ao ambiente (problema). \n\nNo in\u00edcio o AG era padronizado e seguia, em geral, o mesmo procedimento, sendo \n\ndenominado de Algoritmo Gen\u00e9tico Cl\u00e1ssico (AGC). A estrutura apresentada na Figura 2.10 \n\nrepresenta as principais etapas a serem realizadas em um AGC. \n\n \n\nFigura 2.10 \u2013 Estrutura b\u00e1sica de um AGC. \n\nNesse algoritmo (AGC), cada indiv\u00edduo da popula\u00e7\u00e3o \u00e9 composto por um vetor de bits, \n\nconforme mostra a Figura 2.11 (a), e representam poss\u00edveis respostas ao dado problema. \n\nIn\u00edcio\n\nFim\n\n1. Cria Popula\u00e7\u00e3o inicial\n\n2. Avalia\u00e7\u00e3o da popula\u00e7\u00e3o \n\n(c\u00e1lculo do fitness)\n\n3. Sele\u00e7\u00e3o\n\n5. Muta\u00e7\u00e3o\n\n4. Recombina\u00e7\u00e3o \n\n(Crossover)\n\n  6. Condi\u00e7\u00f5es de \n\n   parada atingidas?\n\nSim \n\nN\u00e3o\n\n\n\n \n\n \n\n29 \n\n \n\n \n\nFigura 2.11 \u2013 Exemplo de um indiv\u00edduo formado por um vetor de bits (a), ilustra\u00e7\u00e3o dos \n\nprocessos de recombina\u00e7\u00e3o (b) e muta\u00e7\u00e3o (c). \n\nPara iniciar o algoritmo, \u00e9 gerado um conjunto de indiv\u00edduos para compor a popula\u00e7\u00e3o \n\ninicial. Em seguida, a popula\u00e7\u00e3o \u00e9 avaliada a fim de encontrar os indiv\u00edduos mais fortes. A cada \n\nindiv\u00edduo \u00e9 atribu\u00eddo um valor, determinado atrav\u00e9s da fun\u00e7\u00e3o de fitness, na qual representa o \n\nqu\u00e3o distante a solu\u00e7\u00e3o gerada pelo indiv\u00edduo est\u00e1 do valor desejado. Ap\u00f3s a avalia\u00e7\u00e3o ocorre a \n\nsele\u00e7\u00e3o dos indiv\u00edduos para serem submetidos aos operadores gen\u00e9ticos (recombina\u00e7\u00e3o e \n\nmuta\u00e7\u00e3o). No AGC \u00e9 utilizado o algoritmo da Roleta Russa (Roulette Wheel) para o processo de \n\nsele\u00e7\u00e3o, ilustrado na Figura 2.12. \n\n \n\n \n\nFigura 2.12 \u2013 Ilustra\u00e7\u00e3o do algoritmo de Roleta Russa\n5\n. \n\n                                                 \n5\n Retirada do site ftp://ftp.dca.fee.unicamp.br/pub/docs/vonzuben/ia707_01/topico6_01.pdf Acesso em 28 de Maio \n\nde 2011, \u00e0s 13h44. \n\n0 1 1 0 1 1 1 1 1 0\n\n(a)\n\n0 1 1 0 1 1 1 1 1 0\n\n1 1 1 0 1 0 0 0 1 1\n\n0 1 1 0 1 0 0 0 1 1\n\n1 1 1 0 1 1 1 1 1 0\n\n(b)\n\n0 1 1 0 1 1 1 1 1 0\n\n0 1 1 1 1 1 1 1 0 0\n\n(c)\n\nnovo indiv\u00edduo 2\n\nindiv\u00edduo\n\nnovo indiv\u00edduo\n\nindiv\u00edduo 1\n\nindiv\u00edduo\n\nindiv\u00edduo 2\n\nnovo indiv\u00edduo 1\n\nn\u00ba Fitness Graus\n1 0 0 0 1 1 0 0 1 0 1 0 1 0 6.0 180\n2 0 1 0 1 0 0 1 0 1 0 1 0 1 3.0 90\n3 1 0 1 1 1 1 0 1 0 0 1 0 1 1.5 45\n4 1 0 1 0 0 1 0 1 0 1 0 0 1 1.5 45\n\nCromossomo\n\n \n\n1\n\n2\n3\n\n4\n\n0.25\n\n0\n\n1\n0.5\n\n0.75\n\nftp://ftp.dca.fee.unicamp.br/pub/docs/vonzuben/ia707_01/topico6_01.pdf\n\n\n \n\n \n\n30 \n\n \n\nO algoritmo da Roleta Russa, mostrada na Figura 2.12, consiste em atribuir um valor de \n\nprobabilidade de sele\u00e7\u00e3o a cada indiv\u00edduo, proporcional ao seu valor de fitness e, a partir da\u00ed, os \n\nindiv\u00edduos s\u00e3o selecionados de forma aleat\u00f3ria de acordo com a probabilidade.  \n\nAos indiv\u00edduos selecionados s\u00e3o aplicados os operadores gen\u00e9ticos. O operador de \n\nrecombina\u00e7\u00e3o \u00e9 realizado pelo m\u00e9todo de recombina\u00e7\u00e3o de um ponto, em que os cromossomos \n\nde dois indiv\u00edduos s\u00e3o cortados em um ponto espec\u00edfico e, ent\u00e3o, os dois peda\u00e7os s\u00e3o trocados, \n\nconforme mostra a Figura 2.11 (b). No AGC ainda pode ocorrer muta\u00e7\u00e3o em que, com uma \n\nprobabilidade baixa, um indiv\u00edduo pode sofrer altera\u00e7\u00e3o em seu gene, conforme mostra a Figura \n\n2.11 (c).  \n\nOs indiv\u00edduos, ap\u00f3s a realiza\u00e7\u00e3o dos mecanismos de reprodu\u00e7\u00e3o (sele\u00e7\u00e3o, recombina\u00e7\u00e3o e \n\nmuta\u00e7\u00e3o), comp\u00f5em a nova popula\u00e7\u00e3o para a pr\u00f3xima gera\u00e7\u00e3o. Caso as informa\u00e7\u00f5es desejadas \n\nn\u00e3o estejam contidas dentro dos indiv\u00edduos dessa nova popula\u00e7\u00e3o, ou seja, se as condi\u00e7\u00f5es de \n\nparada n\u00e3o forem satisfeitas, inicia-se novamente o processo de reprodu\u00e7\u00e3o. Um ciclo completo \n\ndo processo de reprodu\u00e7\u00e3o (etapas de 2 a 6 do fluxograma da Figura 2.10) consiste em uma \n\ngera\u00e7\u00e3o. \n\nA desvantagem do AGC \u00e9 que o procedimento de reprodu\u00e7\u00e3o acaba possibilitando a perda \n\ndo melhor indiv\u00edduo da popula\u00e7\u00e3o. Al\u00e9m disso, a codifica\u00e7\u00e3o de valores em bin\u00e1rio torna-se \n\ncomplicada com aumento da precis\u00e3o dos dados. Para tentar superar essas desvantagens, \n\nalgoritmos modificados t\u00eam sido utilizados, com mecanismos alternativos de sele\u00e7\u00e3o/reprodu\u00e7\u00e3o \n\ne codifica\u00e7\u00e3o dos indiv\u00edduos da popula\u00e7\u00e3o em n\u00famero reais.  \n\nUm exemplo de modifica\u00e7\u00e3o no processo de reprodu\u00e7\u00e3o e sele\u00e7\u00e3o consiste em gerar uma \n\nsubpopula\u00e7\u00e3o a partir da popula\u00e7\u00e3o atual, utilizando operadores gen\u00e9ticos (recombina\u00e7\u00e3o e \n\nmuta\u00e7\u00e3o), avaliar e organizar essa subpopula\u00e7\u00e3o e realizar nova sele\u00e7\u00e3o dentro dela para compor \n\na pr\u00f3xima gera\u00e7\u00e3o.  \n\nO AG \u00e9 uma ferramenta bastante utilizada para problemas de otimiza\u00e7\u00e3o devido a suas \n\ncaracter\u00edsticas particulares, que o diferenciam dos algoritmos de busca e otimiza\u00e7\u00e3o tradicionais. \n\nConforme p\u00f4de ser observado na introdu\u00e7\u00e3o, o AG realiza busca sobre uma popula\u00e7\u00e3o de pontos, \n\nao inv\u00e9s de um ponto, de forma paralela e independente. Outra caracter\u00edstica importante da \n\n\n\n \n\n \n\n31 \n\n \n\nferramenta \u00e9 que ela n\u00e3o requer informa\u00e7\u00f5es de derivadas, sendo necess\u00e1rio fornecer apenas o \n\nvalor da fun\u00e7\u00e3o objetivo (fitness).  \n\n2.4 T\u00e9cnicas de amostragem \n\nUm ponto cr\u00edtico que merece destaque no processo de treinamento de RNA \u00e9 a quest\u00e3o dos \n\ndados de entrada (dados de treinamento), pois a qualidade do metamodelo est\u00e1 diretamente ligada \n\na eles (Zubarev, 2009). Como as RNA aprendem atrav\u00e9s de exemplos, se n\u00e3o forem expostos os \n\npontos relevantes e necess\u00e1rios que descrevem a superf\u00edcie de resposta a qual se deseja modelar, \n\na estrutura gerada muito provavelmente n\u00e3o ir\u00e1 realizar representa\u00e7\u00f5es satisfat\u00f3rias nessas \n\nregi\u00f5es onde ocorre falta de informa\u00e7\u00e3o.  \n\nEssa observa\u00e7\u00e3o traz a tona uma importante caracter\u00edstica das RNA para representa\u00e7\u00e3o de \n\num simulador de escoamento. A RNA tende a suavizar (atenuar) o formato da superf\u00edcie de \n\nresposta quando pouca informa\u00e7\u00e3o lhe \u00e9 fornecida. Sendo assim, em regi\u00f5es onde a superf\u00edcie \n\npossui muitas irregularidades e pouca informa\u00e7\u00e3o a respeito, essa caracter\u00edstica se torna mais \n\nimportante. \n\nAdicionalmente, em problemas de ajuste de hist\u00f3rico, na pr\u00e1tica, o tempo demandado para \n\nrealizar uma simula\u00e7\u00e3o pode ser muito grande e, portanto, a utiliza\u00e7\u00e3o do menor n\u00famero de \n\npontos poss\u00edvel para realizar o processo se torna importante. Nesse contexto, pode-se dizer que \n\npara a metodologia de ajuste a ser adotada nesse trabalho, que \u00e9 a aplica\u00e7\u00e3o de metamodelos \n\ngerados a partir de RNA no problema de ajuste de hist\u00f3rico, a t\u00e9cnica utilizada para amostrar os \n\npontos de treinamento das RNA \u00e9 de suma import\u00e2ncia. \n\nSegundo Mckay et al. (1979) quando se pretende modelar algum fen\u00f4meno do mundo real \n\natrav\u00e9s de simula\u00e7\u00e3o computacional, um problema que surge \u00e9 saber quais valores devem ser \n\nutilizados como entrada. Essa incerteza acerca dos par\u00e2metros \u00e9 modelada tratando-os como \n\nvari\u00e1veis aleat\u00f3rias. O comportamento da sa\u00edda \u00e9 obtido realizando-se experimentos num\u00e9ricos a \n\npartir da amostragem das vari\u00e1veis de entrada, com distribui\u00e7\u00e3o de probabilidades conhecida. \n\nExistem diversas t\u00e9cnicas de amostragem na literatura, com o prop\u00f3sito de amostrar o \n\nespa\u00e7o de busca dos par\u00e2metros utilizando quantidades reduzidas de amostras. \n\n\n\n \n\n \n\n32 \n\n \n\nA ideia original do trabalho consistiu em comparar as t\u00e9cnicas do Hipercubo Latino e Box \n\nBehnken para gerar os dados de entrada e os metamodelos a serem utilizados em casos pr\u00e1ticos \n\nde reservat\u00f3rio. A escolha dessas duas t\u00e9cnicas se deu pelo fato de existirem trabalhos publicados \n\nda \u00e1rea de an\u00e1lise de risco em que elas foram aplicadas com sucesso.  \n\nA t\u00e9cnica do HL possibilita gerar qualquer quantidade de amostras que se desejar. No \n\nentanto, no planejamento BB um conjunto pr\u00e9-determinado de pontos \u00e9 selecionado (segundo \n\numa metodologia particular) dentre todas as poss\u00edveis combina\u00e7\u00f5es das vari\u00e1veis de modelagem \n\ndo problema, ou seja, a t\u00e9cnica n\u00e3o tem a flexibilidade que o HL fornece para escolha da \n\nquantidade de amostras. Por conta disso, para poucas vari\u00e1veis, a quantidade de pontos \n\namostrados \u00e9 pequena, n\u00e3o sendo interessante utilizar a t\u00e9cnica nesses casos.  \n\nPara aplica\u00e7\u00e3o da metodologia foram escolhidos, inicialmente, casos anal\u00edticos de duas \n\nvari\u00e1veis a serem estudados antes de um caso pr\u00e1tico de reservat\u00f3rio. Com isso, fez-se necess\u00e1ria \n\na escolha de outra t\u00e9cnica de amostragem para aplicar a metodologia nesses casos preliminares. \n\nAtrav\u00e9s de uma r\u00e1pida pesquisa bibliogr\u00e1fica decidiu-se utilizar a Sequ\u00eancia de Sobol por se \n\ntratar de uma sequ\u00eancia que gera um bom espa\u00e7amento entre as vari\u00e1veis no espa\u00e7o, sendo uma \n\nalternativa interessante em rela\u00e7\u00e3o ao m\u00e9todo de Monte Carlo. Portanto, foram comparadas em \n\num primeiro momento as t\u00e9cnicas do HL e SS, e em um segundo momento as t\u00e9cnicas do HL e \n\nBB. Uma breve introdu\u00e7\u00e3o dessas tr\u00eas t\u00e9cnicas \u00e9 realizada nos itens subsequentes. \n\n2.4.1 Box Behnken \n\nAliar um bom espa\u00e7amento entre pontos no espa\u00e7o com menor n\u00famero de simula\u00e7\u00f5es \n\nposs\u00edvel \u00e9 uma tarefa complicada quando envolve grande quantidade de par\u00e2metros. Nesse \n\ncontexto, o planejamento estat\u00edstico tem sido empregado a fim de gerar dados de entrada com \n\nqualidade. Segundo Risso (2007) apud Montgomery (1996) a t\u00e9cnica consiste basicamente na \n\nutiliza\u00e7\u00e3o de m\u00e9todos estat\u00edsticos e matem\u00e1ticos para modelagem de problemas nos quais as \n\nfun\u00e7\u00f5es-objetivo s\u00e3o influenciadas por v\u00e1rios atributos com a finalidade de otimizar a resposta.  \n\nDessa maneira, a etapa de amostragem dos dados de entrada \u00e9 realizada de forma planejada, \n\nutilizando t\u00e9cnicas estat\u00edsticas. Segundo Mason et al. (2003), a utiliza\u00e7\u00e3o do planejamento \n\nestat\u00edstico permite que informa\u00e7\u00f5es sejam obtidas de forma eficiente nas regi\u00f5es de interesse. \n\n\n\n \n\n \n\n33 \n\n \n\nA t\u00e9cnica possui diversas aplica\u00e7\u00f5es na ind\u00fastria de petr\u00f3leo. Como exemplo pode-se citar \n\no planejamento do tipo Box Behnken (BB), utilizado por Risso (2006) em conjunto com o \n\nm\u00e9todo de superf\u00edcie de resposta para estudar o tratamento de atributos na an\u00e1lise de risco. No \n\nprocesso de ajuste de hist\u00f3rico Lima (2009) utilizou metamodelos gerados por planejamento \n\nestat\u00edstico para realizar o ajuste. Esse m\u00e9todo possibilita realizar uma boa amostragem com \n\nn\u00famero bastante reduzido de amostras em situa\u00e7\u00f5es em que o n\u00famero de fatores (par\u00e2metros) \u00e9 \n\nelevado. \n\nSupondo que se tenha   fatores (ou vari\u00e1veis), discretizados em tr\u00eas n\u00edveis. O modelo \n\nfatorial    \u00e9 um modelo contendo todas as combina\u00e7\u00f5es poss\u00edveis de   fatores (vari\u00e1veis) nesses \n\ntr\u00eas n\u00edveis. O m\u00e9todo de amostragem BB \u00e9 formado selecionando valores do modelo    fatorial \n\ncompleto segundo uma metodologia particular, detalhada em Box e Behnken (1960). Os tr\u00eas \n\nn\u00edveis s\u00e3o normalizados de tal forma que se tenha apenas valores 1, -1 e 0.  \n\nOs pontos resultantes formam uma matriz na qual as linhas representam o n\u00famero de \n\nsimula\u00e7\u00f5es e as colunas representam o n\u00famero de fatores. A Tabela 2.1 mostra um modelo de \n\ntr\u00eas fatores.  \n\nTabela 2.1 \u2013 M\u00e9todo de BB na forma matricial para 3 fatores, discretizados em 3 n\u00edveis. \n\nExperimento X1 X2 X3 Experimento X1 X2 X3 \n\n1 -1 -1 0 9 0 -1 -1 \n\n2 1 -1 0 10 0 1 -1 \n\n3 -1 1 0 11 0 -1 1 \n\n4 1 1 0 12 0 1 1 \n\n5 -1 0 -1 C 0 0 0 \n\n6 1 0 -1 C 0 0 0 \n\n7 -1 0 1 C 0 0 0 \n\n8 1 0 1     \n\n \n\nO m\u00e9todo de planejamento estat\u00edstico foi proposto originalmente para ser utilizado na \n\nmodelagem de experimentos de laborat\u00f3rio, em que erros de experimento e medi\u00e7\u00e3o est\u00e3o \n\npresentes. Assim, os experimentos s\u00e3o repetidos para que se possam estimar esses erros. Os \n\nexperimentos identificados pela letra \u201cC\u201d na Tabela 2.1 representam, ent\u00e3o, a repeti\u00e7\u00e3o dos \n\nexperimentos.  \n\n\n\n \n\n \n\n34 \n\n \n\nOs pontos da Tabela 2.1 formam um cubo em que eles se situam no meio das arestas e no \n\ncentro, conforme mostra a Figura 2.13 \n\n \n\nFigura 2.13 \u2013 M\u00e9todo de BB na forma gr\u00e1fica para 3 fatores, discretizados em 3 n\u00edveis \n\n(FERREIRA et al., 2007, p.183). \n\nO n\u00famero total de simula\u00e7\u00f5es realizadas \u00e9       (   )     (Ferreira et al., 2007, \n\np.182), em que   \u00e9 o n\u00famero de fatores e    \u00e9 o n\u00famero de pontos centrais. Assim, o m\u00e9todo de \n\nBB utiliza menos simula\u00e7\u00f5es do que o modelo fatorial completo. Outra caracter\u00edstica do modelo \u00e9 \n\nque ele n\u00e3o cont\u00e9m pontos em que todos os fatores s\u00e3o simultaneamente m\u00e1ximos ou m\u00ednimos, \n\nou seja, ele n\u00e3o gera condi\u00e7\u00f5es extremas (Ferreira et al., 2007, p.182). Dessa maneira, o modelo \n\nn\u00e3o \u00e9 indicado para experimentos nos quais se deseja determinar condi\u00e7\u00f5es extremas. \n\nSegundo Mason et al. (2003) a utiliza\u00e7\u00e3o de planejamento estat\u00edstico nas etapas de coleta \n\nde dados permite realizar conclus\u00f5es diretas e precisas, o que geralmente n\u00e3o \u00e9 poss\u00edvel quando a \n\ncoleta \u00e9 realizada de forma n\u00e3o planejada. \n\nUma explica\u00e7\u00e3o completa sobre a metodologia pode ser encontrada em Box e Behnken \n\n(1960). \n\n \n\n \n\n \n\n\n\n \n\n \n\n35 \n\n \n\n2.4.2 Hipercubo Latino \n\nO m\u00e9todo do HL foi proposto por Mckay et al. (1979) como alternativa atrativa ao m\u00e9todo \n\naleat\u00f3rio simples (Monte Carlo) em experimentos computacionais. Segundo o autor, o m\u00e9todo \u00e9 \n\nutilizado quando se deseja que cada vari\u00e1vel de entrada tenha todas as regi\u00f5es de sua distribui\u00e7\u00e3o \n\nrepresentadas. \n\nNa amostragem por Hipercubo Latino (HL), o dom\u00ednio de cada vari\u00e1vel \u00e9 dividido em   \n\nintervalos e valores s\u00e3o sorteados dentro de cada intervalo. A quantidade de valores sorteados \n\ndentro de cada faixa pode variar de acordo com sua probabilidade, conforme mostra a Figura \n\n2.14. \n\n \n\nFigura 2.14 \u2013 Exemplo de discretiza\u00e7\u00e3o de uma distribui\u00e7\u00e3o normal em 7 intervalos. (MASCHIO \n\net al., 2009, p.3) \n\nObserva-se que a quantidade de valores sorteados na faixa central (em azul) \u00e9 maior e vai \n\ndiminuindo conforme a curva de probabilidade da faixa. Para uma distribui\u00e7\u00e3o uniforme, as \n\nfaixas possuem probabilidades iguais (  ? ). Os componentes sorteados de cada vari\u00e1vel s\u00e3o \n\npermutados de forma aleat\u00f3ria. A Figura 2.15 mostra um exemplo da amostragem para \n\ndistribui\u00e7\u00e3o normal e uniforme. \n\n\n\n \n\n \n\n36 \n\n \n\n \n\n(a) (b) \n\nFigura 2.15 - Figura ilustrativa da amostragem por HL, para distribui\u00e7\u00e3o normal (a) e uniforme \n\n(b).  \n\nSegundo Maschio et al. (2009), \u201cuma caracter\u00edstica importante dessa t\u00e9cnica \u00e9 que, \n\nindependentemente do n\u00famero de sorteios, o n\u00famero de amostras representa de forma adequada a \n\ndistribui\u00e7\u00e3o de probabilidades\u201d (MASCHIO et al., 2009, p.3).  \n\nMaiores detalhes podem ser encontrados no trabalho de Mckay et al. (1979), o qual realiza \n\ncompara\u00e7\u00f5es da t\u00e9cnica do HL com o m\u00e9todo aleat\u00f3rio simples e estratificado e apresenta as \n\nvantagens em estimar empiricamente distribui\u00e7\u00f5es. \n\n2.4.3 Sequ\u00eancia de Sobol  \n\nA Sequ\u00eancia de Sobol (SS) faz parte do tipo de sequ\u00eancia de baixa discrep\u00e2ncia ou quase-\n\naleat\u00f3ria (quasirandom) em que, segundo Frota (2003), as amostras s\u00e3o selecionadas de modo a \n\npreencherem igualmente todo o dom\u00ednio do espa\u00e7o de busca. O autor estudou a aplica\u00e7\u00e3o de \n\nsequ\u00eancias desse tipo em avalia\u00e7\u00e3o de op\u00e7\u00f5es americanas tradicionais e complexas (modelos de \n\nprecifica\u00e7\u00e3o), mostrando que a aplica\u00e7\u00e3o traz vantagens em rela\u00e7\u00e3o \u00e0 simula\u00e7\u00e3o de Monte Carlo. \n\n-4 -2 0 2 4\n\n-4\n\n-2\n\n0\n\n2\n\n4\n\nVari\u00e1vel 1\n\nV\na\nri\n\n\u00e1\nv\ne\nl \n\n2\n\n-4\n-2\n\n0\n2\n\n4\n05\n\n1\n0\n\n1\n5\n\n2\n0\n\n2\n5\n\n-4 -2 0 2 4\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n0 0.5 1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nVari\u00e1vel 1\n\nV\na\nri\n\n\u00e1\nv\n\ne\nl \n\n2\n\n0 0.5 1\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n0\n0\n\n.5\n1\n\n02468\n\n1\n0\n\n1\n2\n\n\n\n \n\n \n\n37 \n\n \n\nO procedimento seguido para construir a SS envolve conceitos de n\u00fameros direcionais e \n\npolin\u00f4mios primitivos e n\u00e3o ser\u00e1 demonstrado nesse trabalho. Uma descri\u00e7\u00e3o sucinta pode ser \n\nencontrada no trabalho de Frota (2003) e uma descri\u00e7\u00e3o completa desse procedimento foi \n\nrealizada por Bratley e Fox (1988). Um exemplo de amostragem pela t\u00e9cnica para 100 pontos \u00e9 \n\nmostrado na Figura 2.16. \n\n \n\nFigura 2.16 \u2013 Exemplo de amostragem pela Sequ\u00eancia de Sobol. \n\nUma observa\u00e7\u00e3o importante dada por Bratley e Fox (1988) \u00e9 que a SS deve utilizada para \n\nproblemas de 2 a 6 dimens\u00f5es. \n\n2.5 Coment\u00e1rio sobre as t\u00e9cnicas de amostragem \n\nPara os casos anal\u00edticos, a amostragem pelo HL, devido a suas caracter\u00edsticas, deve \n\nproporcionar uma cobertura bem espa\u00e7ada das amostras no espa\u00e7o. O mesmo se pode dizer da \n\nSS, pois com at\u00e9 seis vari\u00e1veis ela proporciona bom espa\u00e7amento. Assim, no caso de duas \n\nvari\u00e1veis, para avalia\u00e7\u00e3o de uma regi\u00e3o maior (toda a superf\u00edcie de busca dos par\u00e2metros), \n\npodem-se esperar resultados semelhantes para as duas t\u00e9cnicas em quest\u00e3o de qualidade da \n\nmodelagem. Por\u00e9m, se for realizada avalia\u00e7\u00e3o em regi\u00f5es espec\u00edficas (menores) deve-se destacar \n\na diferen\u00e7a entre elas, pois cada uma tem um padr\u00e3o de gera\u00e7\u00e3o das amostras. \n\n-1 -0.5 0 0.5 1\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\nVari\u00e1vel 1\n\nV\na\nri\n\n\u00e1\nv\ne\nl \n\n2\n\n\n\n \n\n \n\n38 \n\n \n\nPara o caso com mais vari\u00e1veis, quando comparada \u00e0s t\u00e9cnicas do HL e BB, a diferen\u00e7a \n\nentre elas deve aumentar conforme se aumenta a quantidade de amostras e o tamanho do espa\u00e7o \n\nde busca dos par\u00e2metros, ou seja, os limites de varia\u00e7\u00e3o de cada vari\u00e1vel de modelagem. O BB \n\namostra valores normalizados (m\u00ednimo, m\u00e9dio e m\u00e1ximo) enquanto que o HL, apesar de dividir a \n\nvari\u00e1vel em intervalos, realiza o sorteio em uma faixa cont\u00ednua de valores. Assim, para espa\u00e7os \n\nmaiores, o HL deve fornecer melhor cobertura de todo o espa\u00e7o de busca. \n\nDeste modo, pode-se esperar que, para o caso pr\u00e1tico de reservat\u00f3rio em que se tem maior \n\nquantidade de atributos incertos, melhores resultados sejam obtidos atrav\u00e9s da utiliza\u00e7\u00e3o da \n\nt\u00e9cnica do HL. \n\n\n\n \n\n \n\n39 \n\n \n\n3 REVIS\u00c3O BIBLIOGR\u00c1FICA \n\n3.1 Ajuste de hist\u00f3rico \n\nPara compreender melhor o processo de ajuste de hist\u00f3rico, bem como sua import\u00e2ncia \n\npara estudos de reservat\u00f3rio, uma pesquisa bibliogr\u00e1fica foi realizada para conhecimento e \n\nan\u00e1lise de diferentes metodologias que v\u00eam sendo aplicadas nessa \u00e1rea.  \n\nConforme visto no item de fundamenta\u00e7\u00e3o te\u00f3rica, o problema de ajuste de hist\u00f3rico \u00e9 \n\ncomplexo e mal condicionado. Segundo Schiozer et al. (2009) dentre as etapas que envolvem \n\nesse processo, a que demanda maior esfor\u00e7o computacional e tamb\u00e9m em pesquisa e \n\ndesenvolvimento, \u00e9 a de otimiza\u00e7\u00e3o dos valores dos par\u00e2metros do reservat\u00f3rio, a fim de ajustar o \n\nmodelo de simula\u00e7\u00e3o com o hist\u00f3rico. Nesse contexto, muitos trabalhos t\u00eam sido propostos e o \n\nsurgimento de novos estudos a fim de melhorar essa etapa \u00e9 frequente. \n\nA seguir s\u00e3o analisados alguns trabalhos nos quais se realizam testes e compara\u00e7\u00f5es de \n\ndiferentes t\u00e9cnicas nessa \u00e1rea.  \n\nLeit\u00e3o e Schiozer (1998) realizaram uma compara\u00e7\u00e3o entre algoritmos de primeira ordem \n\n(que utilizam informa\u00e7\u00f5es de gradiente) e de busca direta (que n\u00e3o necessitam do c\u00e1lculo da \n\nderivada). Os autores ainda utilizaram o pacote PVM (Parallel Virtual Machine), que paraleliza \n\nas simula\u00e7\u00f5es de modo a reduzir o tempo necess\u00e1rio para alcan\u00e7ar os resultados.  \n\nSegundo os autores, nos casos em que a fun\u00e7\u00e3o objetivo \u00e9 complexa, existe alta n\u00e3o \n\nlinearidade e a superf\u00edcie de resposta \u00e9 bastante irregular, com diversos m\u00ednimos, sendo que \n\nnessas situa\u00e7\u00f5es, os m\u00e9todos de busca direta s\u00e3o mais eficientes do que os de primeira ordem, \n\npois o segundo acaba convergindo para m\u00ednimos locais. Foi testado tamb\u00e9m um algoritmo \n\nh\u00edbrido, no qual se inicializa a otimiza\u00e7\u00e3o com o algoritmo de primeira ordem utilizando o pacote \n\nPVM e, posteriormente, utiliza-se o algoritmo de busca direta para refinar a solu\u00e7\u00e3o. \n\n\n\n \n\n \n\n40 \n\n \n\nSantos e Schiozer (2000) realizaram uma breve revis\u00e3o bibliogr\u00e1fica sobre o assunto de \n\najuste de hist\u00f3rico e apresentaram uma metodologia utilizando os m\u00f3dulos de paraleliza\u00e7\u00e3o de \n\nsimula\u00e7\u00f5es, an\u00e1lise de sensibilidades e otimiza\u00e7\u00e3o mostrando que a automatiza\u00e7\u00e3o de partes do \n\nprocesso possibilita a redu\u00e7\u00e3o de tempo. \n\nMaschio e Schiozer (2004) aplicaram o m\u00e9todo de busca linear diagonal, baseado no \n\nm\u00e9todo de busca direta em um espa\u00e7o discreto, para aumentar a efici\u00eancia em rela\u00e7\u00e3o ao \n\nalgoritmo de busca linear em conjunto com uma metodologia de ajuste de hist\u00f3rico assistido. A \n\nt\u00e9cnica resultou na redu\u00e7\u00e3o do n\u00famero de simula\u00e7\u00f5es para os casos estudados.  \n\nMaschio e Schiozer (2005) realizaram ainda um estudo comparativo da aplica\u00e7\u00e3o de uma \n\nt\u00e9cnica de busca direta e um m\u00e9todo baseado em c\u00e1lculo de gradientes. O m\u00e9todo dos gradientes \n\nreduz o esfor\u00e7o computacional, por\u00e9m, o m\u00e9todo baseado em busca global se mostrou mais \n\neficiente para casos que possuem mais de um m\u00ednimo local. \n\nMaschio et al. (2006) testaram a aplica\u00e7\u00e3o do m\u00e9todo Simplex ao problema de ajuste de \n\nhist\u00f3rico, avaliando a vers\u00e3o cont\u00ednua e discreta do algoritmo e comparando com o algoritmo de \n\nHooke &amp; Jeeves, baseado na busca direta, mostrando que a t\u00e9cnica possui um bom potencial para \n\nser aplicada em problemas de ajuste de hist\u00f3rico.  \n\nSchulze-Riegert e Ghedan (2007) realizaram revis\u00e3o de t\u00e9cnicas e metodologias para ajuste \n\nde hist\u00f3rico e an\u00e1lise de incertezas, ressaltando suas import\u00e2ncias e aplicabilidades. Para ajuste \n\nde hist\u00f3rico foi realizada uma introdu\u00e7\u00e3o e discuss\u00e3o sobre algoritmos evolucion\u00e1rios, otimiza\u00e7\u00e3o \n\nmultiobjectivo e m\u00e9todos de superf\u00edcie de resposta (metamodelos). Segundo os autores, a \n\naplicabilidade de metamodelos deve ser avaliada caso a caso, analisando os objetivos do estudo.  \n\nSchiozer et al. (2009) realizaram uma breve introdu\u00e7\u00e3o ao assunto de ajuste de hist\u00f3rico e \n\nutilizaram tr\u00eas exemplos de aplica\u00e7\u00e3o para mostrar os problemas inerentes ao processo com \n\naumento da complexidade. Um enfoque maior no processo de ajuste assistido foi dado, \n\nressaltando suas caracter\u00edsticas e vantagens em rela\u00e7\u00e3o ao m\u00e9todo tradicional e autom\u00e1tico de \n\najuste. \n\nEsses s\u00e3o apenas alguns dos exemplos encontrados na literatura. Uma boa \u00eanfase tem sido \n\ndada \u00e0 realiza\u00e7\u00e3o de estudos comparativos e ao desenvolvimento de novas metodologias para \n\naplica\u00e7\u00e3o nessa \u00e1rea.  \n\n\n\n \n\n \n\n41 \n\n \n\n3.2 Aplica\u00e7\u00e3o de redes neurais artificiais e metamodelos  \n\nA utiliza\u00e7\u00e3o de RNA para gerar metamodelos tem ganhado destaque, pois se trata de uma \n\nexcelente ferramenta para ser utilizada em casos com alta n\u00e3o linearidade entre entrada e sa\u00edda. \n\nCom rela\u00e7\u00e3o \u00e0 aplica\u00e7\u00e3o de RNA aos problemas da \u00e1rea de petr\u00f3leo os seguintes trabalhos foram \n\nconsultados: \n\nDoraisamy et al. (2000) aplicaram RNA para determinar a localiza\u00e7\u00e3o para aloca\u00e7\u00e3o de \n\nnovos po\u00e7os em problemas de desenvolvimento de campo. Para tal prop\u00f3sito a RNA foi treinada \n\nutilizando as sa\u00eddas geradas pelo simulador de escoamento. Seus resultados mostraram que a \n\nferramenta pode trazer vantagens quando aplicada em conjunto com o simulador.  \n\nMohaghegh (2000) realizou uma breve introdu\u00e7\u00e3o a respeito de RNA e citou algumas de \n\nsuas aplicabilidades na ind\u00fastria de petr\u00f3leo, ressaltando que \u00e9 recomend\u00e1vel a utiliza\u00e7\u00e3o da \n\nferramenta em casos em que a modelagem matem\u00e1tica do problema se torna muito complexa. Em \n\ntais casos, as RNA podem ser constru\u00eddas para observar o comportamento do sistema (que tipo de \n\nsa\u00edda \u00e9 produzido como resposta a certos valores de entrada) de modo a buscar \u201cimitar\u201d esse \n\ncomportamento. \n\nHirschen e Schafer (2006) aplicaram o m\u00e9todo de regulariza\u00e7\u00e3o bayesiana para treinamento \n\nde RNA para determina\u00e7\u00e3o de par\u00e2metros que definam a geometria de uma jun\u00e7\u00e3o de canais, nos \n\nquais resultem na menor queda de press\u00e3o. O metamodelo gerado foi utilizado no lugar do \n\nsimulador num\u00e9rico no processo de otimiza\u00e7\u00e3o dos par\u00e2metros. Seus estudos mostraram que \n\nredes treinadas com regulariza\u00e7\u00e3o bayesiana s\u00e3o melhores que redes convencionais (sem \n\naplica\u00e7\u00e3o da regulariza\u00e7\u00e3o) para o caso estudado, por\u00e9m, mais estudos na modelagem da rede e \n\nconsequente melhoria na capacidade de aproxima\u00e7\u00e3o se mostraram necess\u00e1rios. \n\nLima et al. (2009) estudaram algumas formas de aplica\u00e7\u00e3o de metamodelos gerados por \n\nt\u00e9cnicas de planejamento estat\u00edstico em um reservat\u00f3rio sint\u00e9tico para o problema de ajuste de \n\nhist\u00f3rico. Seus estudos mostraram que utilizar o metamodelo para determinar a regi\u00e3o de m\u00ednimo \n\ne depois realizar uma otimiza\u00e7\u00e3o local \u00e9 mais eficiente do que realizar todo o processo de ajuste \n\napenas com o metamodelo ou apenas com um algoritmo de otimiza\u00e7\u00e3o (usando somente o \n\nsimulador). Os autores ainda observaram que o metamodelo n\u00e3o foi capaz de representar com \n\n\n\n \n\n \n\n42 \n\n \n\nprecis\u00e3o a regi\u00e3o de m\u00ednimo quando treinada para representar o reservat\u00f3rio inteiro para o caso \n\nestudado.  \n\n3.3 Metamodelos gerados por redes neurais artificiais no processo de ajuste de hist\u00f3rico \n\nPara aplica\u00e7\u00e3o de RNA no processo de ajuste de hist\u00f3rico, encontram-se diversos estudos \n\nna literatura, sendo destacados alguns deles. \n\nAl-Thuwaini et al. (2006) estudaram a aplica\u00e7\u00e3o de SOM (Self Organizing Maps) para \n\nagrupar blocos do reservat\u00f3rio por regi\u00f5es com propriedades semelhantes, possibilitando realizar \n\najustes de par\u00e2metros de forma regional. \n\nCullick et al. (2006) utilizaram RNA em conjunto com planejamento estat\u00edstico para gerar \n\nmetamodelos representativos do simulador. O metamodelo gerado foi utilizado no processo de \n\notimiza\u00e7\u00e3o para gerar valores iniciais para otimiza\u00e7\u00e3o direta com o simulador. Seus resultados \n\nmostraram que a utiliza\u00e7\u00e3o do metamodelo proporciona bons resultados com menos simula\u00e7\u00f5es. \n\nSilva et al. (2006) testaram a utiliza\u00e7\u00e3o de algumas RNA como RBN (Radial Basis \n\nNetwork) e GRNN (Generalized Regression Neural Network) para serem utilizadas para \n\nrepresentar o simulador, ressaltando a import\u00e2ncia de escolher uma arquitetura adequada ao caso \n\na ser estudado a fim de obter boa generaliza\u00e7\u00e3o e n\u00famero reduzido de simula\u00e7\u00f5es. Segundo o \n\nautor, um metamodelo pode ser considerado \u00f3timo se o coeficiente de correla\u00e7\u00e3o linear entre \n\nresultados do simulador e do metamodelo estiverem entre 0.7 e 0.8.  \n\nA ferramenta MATLAB\n\u00ae\n da empresa The Mathworks Inc., utilizada nesse trabalho, calcula \n\no coeficiente de correla\u00e7\u00e3o linear de acordo com a Equa\u00e7\u00e3o 3.1. \n\n   \n? (    ?)  (      ?)\n \n   \n\n? (    ?)\n  \n\n   \n\n Equa\u00e7\u00e3o 3.1 \n\nem que    representa o dado no passo  ,   representa o atraso e a m\u00e9dia \u00e9 dada pela Equa\u00e7\u00e3o 3.2. \n\n\n\n \n\n \n\n43 \n\n \n\n ?  ?\n  \n \n\n \n\n   \n\n Equa\u00e7\u00e3o 3.2 \n\nA fun\u00e7\u00e3o corrcoef utilizada pelo MATLAB\n\u00ae\n produz uma matriz de coeficientes de \n\ncorrela\u00e7\u00e3o linear, na qual cada coluna representa um dado diferente. Os valores podem variar de -\n\n1 a 1, onde valores pr\u00f3ximos de 1 indicam que h\u00e1 uma correla\u00e7\u00e3o linear positiva entre os dados; \n\nvalores perto de -1 indicam que h\u00e1 uma correla\u00e7\u00e3o linear negativa entre os dados e valores \n\npr\u00f3ximos de zero indicam que n\u00e3o existe nenhuma correla\u00e7\u00e3o linear entre os dados. \n\nZangl et al. (2006) estudaram a aplica\u00e7\u00e3o de metamodelos gerados por RNA em processos \n\nde otimiza\u00e7\u00e3o de produ\u00e7\u00e3o e atentaram para alguns fatores.  \n\n? O resultado gerado pela rede n\u00e3o ser\u00e1 confi\u00e1vel caso seja utilizado como entrada \n\nvalores que excedam os limites de varia\u00e7\u00e3o para a qual a rede foi treinada. \n\n?  As configura\u00e7\u00f5es da rede s\u00e3o baseadas unicamente em observa\u00e7\u00f5es num\u00e9ricas, sem \n\nlevar em considera\u00e7\u00e3o qualquer natureza ou conhecimento do problema, n\u00e3o \n\nexistindo uma equa\u00e7\u00e3o ou fun\u00e7\u00e3o que explique o valor gerado na sa\u00edda. Assim, com \n\nos atuais recursos tecnol\u00f3gicos n\u00e3o \u00e9 poss\u00edvel representar de forma anal\u00edtica a \n\nresposta gerada pela rede, em rela\u00e7\u00e3o \u00e0 entrada. \n\n? Se o problema possui grau de liberdade elevado, muitas restri\u00e7\u00f5es e depend\u00eancias \n\npresentes ou pouca correla\u00e7\u00e3o entre espa\u00e7o de entrada e sa\u00edda, a rede n\u00e3o ser\u00e1 capaz \n\nde realizar uma representa\u00e7\u00e3o satisfat\u00f3ria do comportamento desejado. \n\nAssim a aplica\u00e7\u00e3o da ferramenta, para cada caso, deve ser cuidadosamente analisada. \n\nRamgulam et al. (2007) estudaram a aplica\u00e7\u00e3o de RNA para estimativa de atributos \n\nincertos do reservat\u00f3rio, realizando diversos testes para defini\u00e7\u00e3o de arquiteturas de redes e \n\nadi\u00e7\u00e3o de propriedades nos dados de treinamento da rede. Um exemplo de adi\u00e7\u00e3o de propriedades \n\nutilizada pelo autor \u00e9 o quociente entre a dist\u00e2ncia do po\u00e7o produtor at\u00e9 o limite do reservat\u00f3rio e \n\na permeabilidade em cada regi\u00e3o (D/k) ou o quociente entre a \u00e1rea de cada regi\u00e3o e a \n\npermeabilidade de cada regi\u00e3o (A/k). A metodologia foi capaz de reduzir o erro entre os dados \n\nhist\u00f3rico e simulado e o n\u00famero de simula\u00e7\u00f5es necess\u00e1rias para atingir um bom ajuste.  \n\n\n\n \n\n \n\n44 \n\n \n\nMaschio et al. (2008) utilizaram metamodelos gerados a partir de RNA para utiliza\u00e7\u00e3o no \n\nprocesso de ajuste de hist\u00f3rico, mostrando que a ferramenta \u00e9 capaz de reduzir bem o erro entre \n\nresultados do simulador e do hist\u00f3rico com um n\u00famero menor de simula\u00e7\u00f5es. A qualidade do \n\najuste foi medida utilizando a express\u00e3o mostrada na Equa\u00e7\u00e3o 3.3: \n\n  [  (\n        \n\n    \n)]      Equa\u00e7\u00e3o 3.3 \n\nem que \u201cAjustado\u201d representa o afastamento (com rela\u00e7\u00e3o ao hist\u00f3rico) do modelo ajustado e \n\n\u201cBase\u201d representa o afastamento (com rela\u00e7\u00e3o ao hist\u00f3rico) do modelo base (Caso Base). \n\nOs autores atentaram para o fato de que, para uma aplica\u00e7\u00e3o correta da t\u00e9cnica, a escolha \n\ndos pontos e m\u00e9todo de treinamento \u00e9 fundamental, sendo necess\u00e1rio aprimorar o processo de \n\ntreinamento e gera\u00e7\u00e3o dos metamodelos. Esse constituiu o ponto de partida e motiva\u00e7\u00e3o para o \n\npresente trabalho.  \n\nDe acordo com a Equa\u00e7\u00e3o 3.3, a qualidade do ajuste \u00e9 medida em uma escala que vai de \n\nalgum valor negativo at\u00e9 100%. Valores entre 0% e 100% representam uma melhora do ajuste em \n\nrela\u00e7\u00e3o ao Caso Base. O valor 100% representa um ajuste perfeito. Em contrapartida, valores \n\nnegativos significam que houve uma piora em rela\u00e7\u00e3o ao Caso Base. Vale lembrar que o \n\nafastamento \u00e9 calculado utilizando a f\u00f3rmula descrita pela Equa\u00e7\u00e3o 2.1. Assim, por se tratar de \n\numa soma quadr\u00e1tica o valor do coeficiente de ajuste (A) ter\u00e1 o mesmo valor, caso a curva esteja \n\nespelhada do lado oposto em rela\u00e7\u00e3o ao hist\u00f3rico. \n\nA  Figura 3.1 mostra o comportamento do indicador (Q) da curva de produ\u00e7\u00e3o de um \n\ndeterminado modelo com rela\u00e7\u00e3o \u00e0 curva de produ\u00e7\u00e3o do Caso Base. \n\n \n\nFigura 3.1 \u2013 Exemplo do indicador de qualidade de ajuste com rela\u00e7\u00e3o ao Caso Base. \n\nQ = 0% (base)\n\nQ = 100% (ajuste perfeito)\n\n0 &lt;Q &lt;100% (melhor que o base)\n\nBase\n\nQ &lt;0% (pior que o base)\n\nQw\n\nTempo\n\n\n\n \n\n \n\n45 \n\n \n\nSampaio et al. (2009) estudaram a aplica\u00e7\u00e3o de RNA do tipo direta (feedforward) para \n\naplica\u00e7\u00e3o em um caso de ajuste de hist\u00f3rico, relatando as dificuldades existentes em definir e \n\nconfigurar uma rede adequadamente. \n\nZubarev (2009) estudou a utiliza\u00e7\u00e3o de metamodelos como substitutos do simulador em \n\nalgumas \u00e1reas, entre elas o ajuste de hist\u00f3rico. Em seu estudo o autor utilizou diferentes t\u00e9cnicas \n\npara cria\u00e7\u00e3o dos metamodelos. Seus resultados mostraram que a qualidade do metamodelo \u00e9 \n\naltamente dependente da qualidade dos dados de treinamento e de teste, sendo este seu ponto \n\nfraco. Se o conjunto de entrada possui informa\u00e7\u00f5es a respeito do \u00f3timo global, ent\u00e3o este poder\u00e1 \n\nser encontrado, caso contr\u00e1rio, h\u00e1 uma grande chance de parar em \u00f3timos locais. Tratando \n\nespecificamente de RNA, diferentes topologias geram diferentes resultados. Al\u00e9m disso, elas \n\n\u201csuavizam\u201d o formato da superf\u00edcie de resposta (fato que se torna mais evidente em casos em que \n\na superf\u00edcie possui muitas irregularidades), o que gera um erro inerente \u00e0 aplica\u00e7\u00e3o da t\u00e9cnica. \n\nAssim, conhecer suas limita\u00e7\u00f5es e definir um crit\u00e9rio de confiabilidade \u00e9 importante. \n\nComo p\u00f4de ser observado existem in\u00fameras metodologias para ajuste de hist\u00f3rico, como \n\nm\u00e9todos de gradiente, de busca global, algoritmos h\u00edbridos, paraleliza\u00e7\u00e3o de simula\u00e7\u00f5es, \n\nutiliza\u00e7\u00e3o de metamodelos e tantos outros n\u00e3o mencionados aqui. Isso refor\u00e7a o fundamento de \n\nque o processo de ajuste pode ser extremamente complexo, possuindo particularidades a cada \n\ncaso, de modo que uma ferramenta que serve para um determinado caso pode n\u00e3o servir para \n\noutro, e vice-e-versa. Assim, estudos e atualiza\u00e7\u00f5es nessa \u00e1rea devem ser constantemente \n\nrealizados. \n\nV\u00e1rios trabalhos mostram que a aplica\u00e7\u00e3o de RNA no problema de ajuste de hist\u00f3rico pode \n\ncontribuir para a redu\u00e7\u00e3o do n\u00famero de simula\u00e7\u00f5es. Por\u00e9m, conforme ressaltado por Zubarev \n\n(2009) a qualidade das RNA depende das vari\u00e1veis de entrada, utilizadas no processo de \n\ntreinamento. Em problemas de ajuste de hist\u00f3rico a escolha das vari\u00e1veis de entrada influencia \n\ndiretamente nos resultados, em que a quantidade e qualidade dos dados s\u00e3o cruciais para obter \n\nbons ajustes. Como cada caso possui particularidades, \u00e9 dif\u00edcil definir a quantidade m\u00ednima \n\nnecess\u00e1ria para proporcionar resultados satisfat\u00f3rios. A melhor maneira de se determinar a \n\nquantidade e qualidade desejada dos dados \u00e9 atrav\u00e9s da experi\u00eancia dos profissionais da \u00e1rea e da \n\navalia\u00e7\u00e3o dos objetivos propostos. Dessa maneira, a compreens\u00e3o da influ\u00eancia que as vari\u00e1veis \n\n\n\n \n\n \n\n46 \n\n \n\nde entrada exercem sobre o desempenho do metamodelo gerado por RNA contribui para \n\nmelhorar a qualidade dos resultados proporcionados pela ferramenta. \n\n\n\n \n\n \n\n47 \n\n \n\n4 METODOLOGIA \n\nCom o intuito de deixar claros os objetivos do estudo e escolher os casos de aplica\u00e7\u00e3o mais \n\nadequados \u00e0 proposta, a metodologia desse trabalho foi dividida em duas partes. A primeira parte \n\nconsistiu na metodologia geral do trabalho, na qual foram definidos basicamente os objetivos de \n\nestudo, os casos de aplica\u00e7\u00e3o e o procedimento de ajuste a ser adotado. A segunda parte consistiu \n\nna metodologia espec\u00edfica, em que o procedimento de ajuste foi definido. \n\n4.1 Metodologia geral do trabalho \n\nAs etapas seguidas pela metodologia geral do trabalho est\u00e3o descritas no fluxograma da  \n\nFigura 4.1.  \n\n  \n\nFigura 4.1 - Fluxograma descrevendo as etapas utilizadas para realiza\u00e7\u00e3o da metodologia geral do \n\ntrabalho. \n\nA Etapa1 da metodologia geral consistiu em definir os objetivos a serem alcan\u00e7ados atrav\u00e9s \n\ndo processo de ajuste. Conforme especificado no Subitem 1.2 (Objetivos), o objetivo foi avaliar a \n\naplica\u00e7\u00e3o de metamodelos gerados atrav\u00e9s de redes neurais artificiais no processo de ajuste de \n\nhist\u00f3rico, e, posteriormente, avaliar a forma de utiliza\u00e7\u00e3o da ferramenta em conjunto com o \n\nsimulador de escoamento. Dessa maneira, o foco n\u00e3o foi conseguir ao final um ajuste perfeito das \n\n1. Defini\u00e7\u00e3o dos objetivos do trabalho.\n\n2. Defini\u00e7\u00e3o dos casos de estudo a serem utilizados no trabalho.\n\n2A. Defini\u00e7\u00e3o de casos te\u00f3ricos (valida\u00e7\u00e3o).\n\n2B. Defini\u00e7\u00e3o do caso pr\u00e1tico (aplica\u00e7\u00e3o).\n\nMetodologia geral do trabalho\n\n3. Defini\u00e7\u00e3o do procedimento de ajuste a ser utilizado, ferramenta \n\nde otimiza\u00e7\u00e3o e indicadores de qualidade do ajuste.\n\n\n\n \n\n \n\n48 \n\n \n\ncurvas, mas avaliar a aplica\u00e7\u00e3o da ferramenta como substituta do simulador de escoamento ou \n\ncomplementar. \n\nA Etapa2 consistiu em definir os casos de estudo para aplica\u00e7\u00e3o da metodologia, sendo \n\ndividida em duas subetapas.  \n\nNa Etapa 2A foram definidos casos anal\u00edticos, de apenas duas vari\u00e1veis, para avaliar as \n\ncaracter\u00edsticas e limita\u00e7\u00f5es com rela\u00e7\u00e3o \u00e0 aplica\u00e7\u00e3o das RNA, e um caso simples de reservat\u00f3rio \n\npara validar os resultados obtidos. Como s\u00e3o casos de duas vari\u00e1veis, foi poss\u00edvel realizar uma \n\navalia\u00e7\u00e3o direta do comportamento da superf\u00edcie de resposta gerada pela RNA. \n\nNa Etapa 2B, foi definido um caso complexo de reservat\u00f3rio com caracter\u00edsticas reais, para \n\nsimular uma aplica\u00e7\u00e3o pr\u00e1tica. \n\n A Etapa3 consistiu em definir o procedimento de ajuste, a ferramenta utilizada para \n\notimiza\u00e7\u00e3o e os indicadores de qualidade a serem utilizados nos casos de estudo. \n\nPara esse trabalho um procedimento semelhante ao de Ertekin et al. (2001) e aos utilizados \n\nnos trabalhos de Cullick et al. (2006), Maschio et al. (2008) e Sampaio et al. (2009) foi elaborado \n\ne ser\u00e1 descrito no Subitem 4.2 a seguir.  \n\nPara realizar o processo de otimiza\u00e7\u00e3o dos par\u00e2metros foi escolhido o Algoritmo Gen\u00e9tico. \n\nComo indicador da qualidade dos metamodelos gerados, nos casos anal\u00edticos, utilizou-se \n\nprincipalmente a superf\u00edcie de resposta para avaliar os resultados. Como indicadores secund\u00e1rios \n\nforam utilizados o coeficiente de correla\u00e7\u00e3o (Equa\u00e7\u00e3o 3.1) e o erro m\u00e9dio. Para os casos de \n\nreservat\u00f3rio, foi empregado o coeficiente de correla\u00e7\u00e3o como indicador principal para avalia\u00e7\u00e3o e \n\ncompara\u00e7\u00e3o do desempenho de cada metamodelo gerado, uma vez que n\u00e3o \u00e9 poss\u00edvel realizar a \n\nan\u00e1lise visual da superf\u00edcie de resposta. Os valores do coeficiente de correla\u00e7\u00e3o e erro m\u00e9dio s\u00e3o \n\ncalculados entre a sa\u00edda desejada e a gerada pela RNA. Para o caso pr\u00e1tico ainda foi utilizado um \n\nindicador que mede quanto o modelo ajustado melhorou o afastamento em rela\u00e7\u00e3o ao Caso Base, \n\ndescrito pela Equa\u00e7\u00e3o 3.3 e ilustrado na Figura 3.1, al\u00e9m da an\u00e1lise das curvas de produ\u00e7\u00e3o, \n\ncomparando os resultados do modelo de simula\u00e7\u00e3o ajustado com o hist\u00f3rico. \n\n \n\n\n\n \n\n \n\n49 \n\n \n\n4.2 Metodologia espec\u00edfica - procedimento de ajuste \n\nO procedimento de ajuste adotado consistiu em utilizar metamodelos gerados atrav\u00e9s de \n\nRNA para realizar a parte de altera\u00e7\u00e3o dos atributos do reservat\u00f3rio (explora\u00e7\u00e3o do espa\u00e7o de \n\nsolu\u00e7\u00f5es). Os passos seguidos para realizar o procedimento de ajuste est\u00e3o descritos no \n\nfluxograma mostrado na Figura 4.2. \n\n \n\nFigura 4.2 \u2013 Fluxograma que descreve os passos seguidos pelo procedimento de ajuste adotado \n\nnesse trabalho. \n\nO procedimento mostrado na Figura 4.2 pode ser dividido em duas partes principais; \n\ntreinamento das RNA (gera\u00e7\u00e3o dos conjuntos de treinamento; treinamento e avalia\u00e7\u00e3o dos \n\nmetamodelos gerados \u2013 Passos 1 e 2) e a aplica\u00e7\u00e3o do metamodelo gerado no processo de ajuste \n\nde hist\u00f3rico (Passos 3). E adicionalmente, se necess\u00e1rio, Passo 4.  \n\n \n\n \n\nProcedimento de ajuste\n\nIn\u00edcio\n\n1. Defini\u00e7\u00e3o do conjunto de treinamento\n\n2. Treinamento das RNA e an\u00e1lise de \n\ndesempenho dos metamodelos gerados.\n\n3. Otimiza\u00e7\u00e3o utilizando o metamodelo e \n\nvalida\u00e7\u00e3o do m\u00ednimo encontrado.\n\nFim\n\n4. Retreinamento?Sim\n\nN\u00e3o\n\n\n\n \n\n \n\n50 \n\n \n\n4.2.1 Treinamento das redes neurais artificiais \n\nO Passo 1 consistiu na defini\u00e7\u00e3o do conjunto de treinamento, formado pelas entradas e \n\nsa\u00eddas desejadas da rede, que s\u00e3o as vari\u00e1veis de modelagem e as respostas do dado problema, \n\nrespectivamente. Para gerar as entradas foram utilizadas t\u00e9cnicas de amostragem distintas. Dessa \n\nmaneira, conjuntos com diferentes caracter\u00edsticas de espa\u00e7amento e quantidade de amostras \n\nforam obtidos. Ao serem geradas, as entradas foram utilizadas para determinar as sa\u00eddas \n\ndesejadas, que representam as respostas do problema.  \n\nOs casos anal\u00edticos s\u00e3o fun\u00e7\u00f5es em que a superf\u00edcie de resposta \u00e9 descrita por uma equa\u00e7\u00e3o. \n\nAssim, as sa\u00eddas desejadas para esses casos foram determinadas calculando o valor da fun\u00e7\u00e3o, \n\ndado pela resposta da equa\u00e7\u00e3o para as duas vari\u00e1veis de entrada. \n\nNos casos de reservat\u00f3rio, as sa\u00eddas desejadas foram determinadas com o aux\u00edlio do \n\nsimulador de escoamento. A entrada gerada (atributos que modelam o reservat\u00f3rio, ou seja, um \n\nmodelo de simula\u00e7\u00e3o) \u00e9 simulada com o simulador de escoamento e as curvas de produ\u00e7\u00e3o s\u00e3o \n\ncomparadas com o hist\u00f3rico para calcular o afastamento do modelo com rela\u00e7\u00e3o ao hist\u00f3rico, \n\npar\u00e2metro que consistiu na sa\u00edda desejada para esses casos. \n\nO Passo 2 consistiu no treinamento das RNA e an\u00e1lise de desempenho dos metamodelos \n\ngerados. Nesse trabalho, a RNA treinada \u00e9 chamada de metamodelo. Para avaliar o desempenho \n\ndos metamodelos gerados foram utilizados os indicadores definidos na Etapa 4 da metodologia \n\ngeral (Subitem 4.1).  \n\nA respeito do treinamento das RNA, alguns algoritmos empregados no processo de \n\ntreinamento, que n\u00e3o ser\u00e3o abordados nesse trabalho, possuem certo grau de aleatoriedade, de \n\nmodo que, a cada treinamento, um metamodelo diferente \u00e9 gerado. Dessa maneira, o processo de \n\ntreinamento foi realizado diversas vezes com a finalidade de encontrar um metamodelo capaz de \n\natender aos requisitos com a confiabilidade desejada.  \n\n \n\n \n\n \n\n\n\n \n\n \n\n51 \n\n \n\n4.2.2 Aplica\u00e7\u00e3o de metamodelos no processo de ajuste de hist\u00f3rico \n\nCullick et al. (2006), Maschio et al. (2008) e Sampaio et al. (2009) utilizaram metamodelos \n\ngerados a partir de RNA para realizar a otimiza\u00e7\u00e3o dos atributos no lugar do simulador de \n\nescoamento. Apesar de utilizarem diferentes ferramentas para otimiza\u00e7\u00e3o e diferentes FO, a \n\nmetodologia utilizada pelos autores foi capaz de encontrar resultados satisfat\u00f3rios, e dessa \n\nmaneira, serviu como base para a aplica\u00e7\u00e3o do algoritmo gen\u00e9tico. \n\nO Passo 3 do procedimento de ajuste consistiu na otimiza\u00e7\u00e3o utilizando o metamodelo e \n\nvalida\u00e7\u00e3o do m\u00ednimo encontrado. Na otimiza\u00e7\u00e3o utilizou-se o metamodelo gerado para calcular o \n\nvalor da Fun\u00e7\u00e3o Objetivo (FO) a ser minimizada pelo AG. Para valida\u00e7\u00e3o, nos casos anal\u00edticos o \n\nm\u00ednimo encontrado foi comparado com o m\u00ednimo global do problema, determinado atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando a equa\u00e7\u00e3o que define cada caso. J\u00e1 para os casos de reservat\u00f3rio, como na \n\npr\u00e1tica a resposta n\u00e3o \u00e9 conhecida, a valida\u00e7\u00e3o \u00e9 realizada atrav\u00e9s da compara\u00e7\u00e3o das curvas de \n\nprodu\u00e7\u00e3o do modelo simulado com o hist\u00f3rico de produ\u00e7\u00e3o e press\u00e3o medido nos po\u00e7os. O \n\nmodelo simulado consiste no m\u00ednimo encontrado atrav\u00e9s da otimiza\u00e7\u00e3o, em que, conforme \n\nmencionado, o metamodelo gerado \u00e9 utilizado para calcular a FO no processo de minimiza\u00e7\u00e3o. \n\nUma maneira de melhorar a capacidade de representa\u00e7\u00e3o e generaliza\u00e7\u00e3o da rede \u00e9 realizar \n\num novo treinamento em uma regi\u00e3o espec\u00edfica do espa\u00e7o de solu\u00e7\u00f5es, pois com uma regi\u00e3o \n\nreduzida torna-se mais f\u00e1cil a representa\u00e7\u00e3o pelo metamodelo. Com o Passo 3, acredita-se que o \n\nm\u00ednimo encontrado esteja nas proximidades do m\u00ednimo global do problema. \n\nDessa maneira, para casos em que o valor encontrado com a otimiza\u00e7\u00e3o n\u00e3o atenda aos \n\ncrit\u00e9rios de precis\u00e3o desejados, mas que estejam relativamente pr\u00f3ximos (o qu\u00e3o pr\u00f3ximo varia \n\nde caso a caso) define-se uma regi\u00e3o de interesse ao redor do valor encontrado (m\u00ednimo) e \n\nrealiza-se novo procedimento de treinamento (repeti\u00e7\u00e3o dos Passos 1, 2 e 3). Assim, o Passo 4 \n\nconsistiu em avaliar e, caso constatada a necessidade, realizar novo treinamento em um \n\nsubdom\u00ednio do espa\u00e7o de busca. \n\nPara a defini\u00e7\u00e3o de uma nova faixa de varia\u00e7\u00e3o dos atributos adotou-se o seguinte \n\nprocedimento:  \n\n\n\n \n\n \n\n52 \n\n \n\n? A partir dos resultados da otimiza\u00e7\u00e3o com o metamodelo tomou-se como base os \n\nindiv\u00edduos que geraram valores de FO inferiores a um determinado valor de corte \n\n(particular a cada caso);  \n\n? Dentre os indiv\u00edduos que atendem \u00e0 condi\u00e7\u00e3o anterior consideraram-se os limites \n\nm\u00e1ximo e m\u00ednimo de varia\u00e7\u00e3o dos atributos para nova amostragem, os quais \n\ndefiniram uma nova regi\u00e3o de interesse.  \n\nPara os casos anal\u00edticos, em caso de novo retreinamento, a defini\u00e7\u00e3o da nova regi\u00e3o para \n\namostragem foi feita a partir da avalia\u00e7\u00e3o visual da superf\u00edcie de resposta, por ser mais direta e \n\nprecisa. \n\n\n\n \n\n \n\n53 \n\n \n\n5 APLICA\u00c7\u00c3O \n\nEsse cap\u00edtulo \u00e9 dedicado \u00e0 apresenta\u00e7\u00e3o dos casos de estudo aos quais a metodologia \n\nproposta foi aplicada. Foram escolhidos quatro casos anal\u00edticos, um caso de reservat\u00f3rio simples \n\n(valida\u00e7\u00e3o) e um caso de reservat\u00f3rio complexo com caracter\u00edsticas reais.  \n\n5.1 Casos anal\u00edticos \n\nPara os casos anal\u00edticos, primeiramente foi escolhido um caso simples para iniciar os \n\nestudos; posteriormente, um caso em que a superf\u00edcie de resposta a ser modelada possui alguma \n\nirregularidade, com um m\u00ednimo global e um local; um caso com diversas irregularidades, com \n\nmais de um m\u00ednimo global e diversos m\u00ednimos locais (diversas regi\u00f5es de interesse) e, \n\nfinalmente, um caso em que a superf\u00edcie de resposta \u00e9 modelada por mais de uma fun\u00e7\u00e3o. Com \n\nisso buscou-se avaliar as caracter\u00edsticas e limita\u00e7\u00f5es da ferramenta para casos com diferentes \n\ngraus de dificuldade. \n\n5.1.1 Premissas e considera\u00e7\u00f5es \n\nOs casos anal\u00edticos consistiram em casos de duas vari\u00e1veis, em que a superf\u00edcie de reposta \u00e9 \n\ndescrita por uma equa\u00e7\u00e3o. \n\nO m\u00ednimo global de cada problema foi determinado atrav\u00e9s do processo de otimiza\u00e7\u00e3o, \n\nutilizando a equa\u00e7\u00e3o que descreve a fun\u00e7\u00e3o para calcular o valor da Fun\u00e7\u00e3o Objetivo. O m\u00ednimo \n\nencontrado nesse processo foi considerado como sendo o m\u00ednimo global da fun\u00e7\u00e3o.  \n\nPara esses casos, a superf\u00edcie de resposta foi utilizada como par\u00e2metro principal na \n\navalia\u00e7\u00e3o do desempenho do metamodelo gerado. \n\n\n\n \n\n \n\n54 \n\n \n\nComo complementos foram utilizados o coeficiente de correla\u00e7\u00e3o, cuja express\u00e3o foi \n\nmostrada na Equa\u00e7\u00e3o 3.1 (Cap\u00edtulo 3), e o erro entre a sa\u00edda desejada e gerada pelo metamodelo, \n\nmostrado abaixo na Equa\u00e7\u00e3o 5.1. \n\n   \n?[(\n\n         \n    \n\n)     ]\n\n \n \n\nEqua\u00e7\u00e3o 5.1 \n\nem que      representa os dados simulados com o metamodelo,      representa os dados \n\ncalculados diretamente com a fun\u00e7\u00e3o anal\u00edtica e   representa o n\u00famero total de amostras \n\n5.1.2 Caso 1A \n\nO Caso 1A \u00e9 o mais simples, representado por uma superf\u00edcie com concavidade voltada \n\npara baixo e com um m\u00ednimo global, conforme mostra a Figura 5.1. Esse caso foi utilizado \n\napenas para iniciar os estudos, com o intuito de mostrar que para casos extremamente simples a \n\nRNA modela a resposta desejada sem dificuldades. \n\n \n\nFigura 5.1 \u2013 Superf\u00edcie de resposta para o Caso 1A. \n\n \n\n\n\n \n\n \n\n55 \n\n \n\nA sa\u00edda desejada para treinar a RNA consistiu no valor da fun\u00e7\u00e3o que modela a superf\u00edcie, \n\nmostrada pela Equa\u00e7\u00e3o 5.2: \n\n        Equa\u00e7\u00e3o 5.2 \n\n5.1.3 Caso 1B \n\nA superf\u00edcie de resposta do Caso 1B \u00e9 mostrada na Figura 5.2, a seguir: \n\n \n\nFigura 5.2 \u2013 Superf\u00edcie de resposta para o Caso 1B. \n\nA sa\u00edda desejada para treinar a RNA consistiu no valor da fun\u00e7\u00e3o que modela a superf\u00edcie, \n\nmostrada pela Equa\u00e7\u00e3o 5.3: \n\n    (    )    ( \n  (   ) )     (  ?       )   (  \n\n    )   ... \n\n  ?   ( (   )\n    ) \n\nEqua\u00e7\u00e3o 5.3 \n\nComo pode ser observado na Figura 5.2, a superf\u00edcie de resposta para esse caso possui um \n\nm\u00ednimo global e um m\u00ednimo local. \n\n \n\n\n\n \n\n \n\n56 \n\n \n\n5.1.4 Caso 1C \n\nEsse caso possui uma superf\u00edcie de resposta irregular, com mais de um m\u00ednimo global \n\n(quatro no total) e diversos m\u00ednimos locais, conforme pode ser observado na Figura 5.3: \n\n \n\nFigura 5.3 \u2013 Superf\u00edcie de resposta para o Caso 1C. \n\nA sa\u00edda desejada para treinar a RNA consistiu no valor da fun\u00e7\u00e3o que modela a superf\u00edcie, \n\nmostrada pela Equa\u00e7\u00e3o 5.4: \n\n       (   )       (     )    Equa\u00e7\u00e3o 5.4 \n\n5.1.5 Caso 1D \n\nO Caso 1D \u00e9 modelado por uma fun\u00e7\u00e3o pr\u00e9-definida pelo Matlab chamada NonSmothFcn, \n\nformada por tr\u00eas diferentes superf\u00edcies e possui apenas um m\u00ednimo global, conforme pode ser \n\nvisualizado na Figura 5.4. \n\n\n\n \n\n \n\n57 \n\n \n\n \n\nFigura 5.4 \u2013 Superf\u00edcie de resposta para o Caso 1D. \n\nA sa\u00edda desejada para treinar a RNA consistiu no valor da fun\u00e7\u00e3o no espa\u00e7o que \u00e9 retornada \n\nautomaticamente atrav\u00e9s da fun\u00e7\u00e3o NonSmothFcn do Matlab. \n\n5.2 Casos de reservat\u00f3rio \n\nPara os dois casos de reservat\u00f3rio (2A e 2B) utilizou-se o mesmo modelo, por\u00e9m para o \n\nCaso 2A foram realizadas algumas modifica\u00e7\u00f5es para simular um caso simples de valida\u00e7\u00e3o dos \n\nresultados obtidos com o estudo dos casos anal\u00edticos.  \n\n5.2.1 Premissas e considera\u00e7\u00f5es \n\nTrata-se de um reservat\u00f3rio sint\u00e9tico no qual o hist\u00f3rico \u00e9 conhecido. Os atributos incertos \n\ndo reservat\u00f3rio s\u00e3o fornecidos, de forma que se considera que sejam os que mais influenciam na \n\nprodu\u00e7\u00e3o de \u00e1gua do reservat\u00f3rio.  \n\nPara os casos de reservat\u00f3rio foi utilizado como indicador principal para avalia\u00e7\u00e3o de \n\ndesempenho o coeficiente de correla\u00e7\u00e3o linear entre sa\u00edda do metamodelo e do simulador. Como \n\nn\u00e3o \u00e9 poss\u00edvel realizar a visualiza\u00e7\u00e3o da superf\u00edcie de resposta, graficamente foi utilizada a \n\n\n\n \n\n \n\n58 \n\n \n\nrela\u00e7\u00e3o entre os afastamentos gerados atrav\u00e9s da simula\u00e7\u00e3o com o simulador de escoamento \n\n(hist\u00f3rico) e aqueles gerados atrav\u00e9s da simula\u00e7\u00e3o com os metamodelos. \n\n5.2.2 Reservat\u00f3rio utilizado \n\nO modelo, constru\u00eddo sinteticamente atrav\u00e9s de t\u00e9cnicas geoestat\u00edsticas, \u00e9 mostrado na \n\nFigura 5.5. \n\n \n\nFigura 5.5 - Modelo de reservat\u00f3rio (permeabilidade horizontal \u2013 md). \n\nO modelo mostrado na Figura 5.5 representa um reservat\u00f3rio de \u00f3leo leve, cujo mecanismo \n\nde produ\u00e7\u00e3o natural \u00e9 a expans\u00e3o de l\u00edquido e g\u00e1s em solu\u00e7\u00e3o e o mecanismo de recupera\u00e7\u00e3o \u00e9 a \n\ninje\u00e7\u00e3o de \u00e1gua. Ele foi discretizado em uma malha corner point com dimens\u00e3o 90x110x5 \n\n(49500 blocos), \u00e9 composto por tr\u00eas f\u00e1cies caracterizadas de acordo com tr\u00eas faixas de \n\npermeabilidades (baixa, intermedi\u00e1ria e alta) e possui quatro falhas, representadas pelas linhas em \n\npreto na Figura 5.5. Um modelo de refer\u00eancia (escolhido dentre as poss\u00edveis combina\u00e7\u00f5es dos \n\ndezesseis atributos incertos do problema) foi simulado para gerar o hist\u00f3rico de dez anos. O \n\nreservat\u00f3rio \u00e9 drenado por quinze po\u00e7os verticais (oito produtores e sete injetores). Os dezesseis \n\natributos, considerados de maior impacto sobre o comportamento do reservat\u00f3rio s\u00e3o mostrados \n\nno Subitem 5.2.4. \n\n\n\n \n\n \n\n59 \n\n \n\n5.2.3 Caso 2A \n\nOs atributos incertos do reservat\u00f3rio para o Caso 2A foram definidos como sendo o \n\nmultiplicador da porosidade da f\u00e1cies 2, o multiplicador do logaritmo da permeabilidade \n\nhorizontal da f\u00e1cies 2, a transmissibilidade da falha 3 e o expoente do modelo de Corey para a \n\npermeabilidade relativa da \u00e1gua da f\u00e1cies 2, totalizando 4 atributos incertos, que est\u00e3o elencados \n\nna Tabela 5.1.  \n\nTabela 5.1 \u2013 Atributos incertos para o caso 2A. \n\nAtributos Descri\u00e7\u00e3o Tipo Min. M\u00e1x. \n\n1 Porosidade (Por2) \u2013 f\u00e1cies 2 Multiplicador 0.85 1.15 \n\n2 Permeabilidade horizontal (kx2) - f\u00e1cies 2 Multiplicador do log. (kx) 0.75 1.1 \n\n3 Transmissibilidade da falha (T3) Multiplicador 0 1 \n\n4 Permeabilidade relativa (kr2) - f\u00e1cies 2 \nExpoente da fase \u00e1gua \n\n(modelo de Corey) \n1 5 \n\n \n\nPara esse caso o problema consistiu em realizar o ajuste da vaz\u00e3o de \u00e1gua do campo \n\n(apenas uma sa\u00edda). Assim, para gerar os metamodelos, os valores de sa\u00edda desejada utilizados \n\nforam o afastamento da vaz\u00e3o de \u00e1gua do campo (entre modelo simulado e o hist\u00f3rico). \n\n5.2.4 Caso 2B \n\nPara o Caso 2B os atributos incertos foram definidos como sendo o multiplicador do \n\nlogaritmo da permeabilidade horizontal, multiplicador da porosidade, raz\u00e3o entre \n\npermeabilidades vertical e horizontal e o expoente do modelo de Corey para permeabilidade \n\nrelativa da \u00e1gua. Como o modelo \u00e9 caracterizado por tr\u00eas f\u00e1cies, conforme citado anteriormente \n\nno Subitem 5.2.2, os atributos incertos s\u00e3o considerados distintos para cada f\u00e1cies. O modelo \n\nainda possui quatro falhas, conforme pode ser observado na Figura 5.5. A transmissibilidade de \n\ncada falha tamb\u00e9m foi inclu\u00edda na lista de atributos incertos, totalizando, assim, dezesseis \n\nvari\u00e1veis incertas, mostradas na Tabela 5.2, que ser\u00e3o modificadas no processo. \n\n \n\n \n\n\n\n \n\n \n\n60 \n\n \n\nTabela 5.2 \u2013 Atributos incertos para Caso 2B. \n\nAtributos Descri\u00e7\u00e3o Tipo Min. M\u00e1x. \n\n1 ao 3 Porosidade (Por1-3) Multiplicador 0.85 1.15 \n\n4 ao 6 Permeabilidade horizontal (kx1-3) Multiplicador do log. (kx) 0.75 1.1 \n\n7 ao 9 Permeabilidade vertical (kz1-3) Porcentagem de kx 4 25 \n\n10 ao 13 Transmissibilidade da falha (T1-4) Multiplicador 0 1 \n\n14 ao 16 Permeabilidade relativa (kr1-3) \nExpoente da fase \u00e1gua \n\n(modelo de Corey) \n1 5 \n\n \n\nO objetivo desse caso foi realizar o ajuste de vaz\u00e3o de \u00e1gua de cada po\u00e7o (oito po\u00e7os \n\nprodutores), sendo assim, o problema composto por uma Fun\u00e7\u00e3o Objetivo calculada pela m\u00e9dia \n\nde oito componentes a ser minimizada. Deste modo, cada valor de sa\u00edda desejada, utilizada para \n\ngerar os metamodelos, corresponde ao afastamento de um po\u00e7o (diferen\u00e7a entre vaz\u00e3o de \u00e1gua do \n\nmodelo simulado e o hist\u00f3rico). \n\n5.3 Gera\u00e7\u00e3o dos conjuntos de entrada para treinamento  \n\nPara a amostragem dos conjuntos de entrada foram utilizadas as t\u00e9cnicas de Hipercubo \n\nLatino (HL), Sequencia de Sobol (SS) e Box Behnken (BB). Com isso obtiveram-se amostras \n\ndispostas de forma diferente no espa\u00e7o dos par\u00e2metros. Adicionalmente a esse fator foram \n\ngerados amostras de 25, 50 e 100 pontos para os Casos 1A a 1D. Para o Caso 2A as mesmas \n\nquantidades de pontos foram amostradas, por\u00e9m utilizando apenas a t\u00e9cnica do HL. Para o Caso \n\n2B, 100, 250 e 396 pontos foram amostrados utilizando as t\u00e9cnicas do HL e BB (apenas um \n\nconjunto com 396 pontos, que \u00e9 o n\u00famero de combina\u00e7\u00f5es pr\u00e9-fixado pela t\u00e9cnica de Box \n\nBehnken para 16 vari\u00e1veis). Nos casos de reservat\u00f3rio, cada conjunto de dados de entrada \n\namostrado representa um modelo de simula\u00e7\u00e3o. \n\nUma observa\u00e7\u00e3o a ser feita \u00e9 que a t\u00e9cnica do BB, conforme mencionado no Subitem 2.5, \n\npara poucas vari\u00e1veis a quantidade de pontos amostrados \u00e9 muito baixa. Por isso ela foi aplicada \n\napenas para o Caso 2B, que cont\u00e9m 16 vari\u00e1veis de entrada (resultando em 396 amostras: 16 \n\natributos, 3 n\u00edveis = 16\n3\n pontos). Dessa maneira, para os Casos 1A e 1D foi utilizada a SS, a fim \n\nde obter uma compara\u00e7\u00e3o entre dados com diferentes caracter\u00edsticas de espa\u00e7amento. No Caso \n\n2A, como foi utilizado apenas para valida\u00e7\u00e3o, aplicou-se somente o HL para gerar os dados. \n\n\n\n \n\n \n\n61 \n\n \n\nCom rela\u00e7\u00e3o aos dados de teste, nos Casos 1A a 1D, os pontos de treinamento gerados por \n\numa t\u00e9cnica foram utilizados como pontos de teste para a outra e vice-e-versa, ou seja, foram \n\nutilizados os 100 pontos amostrados da SS para testar os metamodelos gerados com dados do HL \n\ne os 100 pontos amostrados pelo HL para testar os metamodelos gerados com dados da SS. J\u00e1 \n\npara o Caso 2A foram gerados 25 pontos pelo HL e para o Caso 2B foram gerados 100 pontos \n\npelo HL para realizarem as simula\u00e7\u00f5es de teste. \n\nAs sa\u00eddas desejadas, para os Casos 1A a 1D foram calculadas utilizando a equa\u00e7\u00e3o que \n\ndefine cada caso. J\u00e1 para os casos de reservat\u00f3rio, foram geradas atrav\u00e9s do simulador de \n\nescoamento utilizando o programa IMEX da CMG. Os dados de produ\u00e7\u00e3o de \u00e1gua, resultantes da \n\nsimula\u00e7\u00e3o, foram comparados com o hist\u00f3rico de produ\u00e7\u00e3o dispon\u00edvel e o afastamento foi \n\ncalculado, compondo dessa maneira as sa\u00eddas desejadas para esses casos.  \n\nConforme citado anteriormente, para o Caso 2A o afastamento do campo foi utilizado. J\u00e1 \n\npara o Caso 2B, foi usado o afastamento po\u00e7o a po\u00e7o. A f\u00f3rmula utilizada para calcular o \n\nafastamento \u00e9 a mesma mostrada na Equa\u00e7\u00e3o 2.1, considerando o valor do peso igual a \u201c1\u201d para \n\ntodos os dados, resultando na express\u00e3o mostrada na Equa\u00e7\u00e3o 5.5. \n\n  ?(    \n      \n\n )\n \n\n \n\n   \n\n Equa\u00e7\u00e3o 5.5 \n\nem que   representa o n\u00famero de dados observados de cada s\u00e9rie (produ\u00e7\u00e3o de \u00e1gua, por \n\nexemplo),     \n  e     \n\n \n s\u00e3o os dados observados e simulados, respectivamente. \n\nPara o Caso 2B, foi realizada a an\u00e1lise de sensibilidade para gerar conjuntos adicionais de \n\nentrada a fim de melhorar a qualidade dos resultados e realizar compara\u00e7\u00f5es entre os diversos \n\nmetamodelos gerados. Assim, para entrada foram utilizadas duas op\u00e7\u00f5es de configura\u00e7\u00e3o:  \n\n? Op\u00e7\u00e3o 1: utilizar todos os atributos incertos; \n\n? Op\u00e7\u00e3o 2: utilizar os atributos determinados atrav\u00e9s da AS.  \n\nNesse procedimento foram realizadas simula\u00e7\u00f5es (com o simulador de escoamento) com \n\nvalores extremos de cada atributo (m\u00e1ximo e m\u00ednimo) enquanto se mant\u00e9m as outras no valor do \n\nCaso Base (valor m\u00e9dio). Assim, avaliou-se quanto o afastamento dos po\u00e7os variou com um dado \n\n\n\n \n\n \n\n62 \n\n \n\natributo em rela\u00e7\u00e3o ao afastamento do Caso Base, possibilitando, dessa maneira, identificar quais \n\natributos influenciam mais a FO para o caso a ser estudado. Com isso, determinaram-se os \n\nseguintes atributos para treinar a RNA, mostrados na Tabela 5.3. \n\nTabela 5.3 \u2013 Atributos que mais influenciam os po\u00e7os. \n\nAtributo Atributo \nPo\u00e7o \n\nM\u00e9dia \n1 2 3 4 5 6 7 8 \n\nPorosidade (Por) \n\n1 x         \n\n2 x x x  x x x x x \n\n3  x x   x   x \n\nPermeabilidade horizontal (kx) \n\n4  x   x x  x x \n\n5 x  x  x x  x x \n\n6 x x    x  x x \n\nPermeabilidade vertical (kz) \n\n7          \n\n8  x   x   x x \n\n9   x   x    \n\nTransmissibilidade da falha (T) \n\n10      x    \n\n11 x x        \n\n12      x    \n\n13   x x x  x  x \n\nPermeabilidade relativa (kr) \n\n14 x       x  \n\n15 x x x x x x x x x \n\n16  x x x x x   x \n\n \n\nNa Tabela 5.3, os atributos marcados com \u201cx\u201d representam aqueles considerados de maior \n\nimpacto sobre cada po\u00e7o e sobre a m\u00e9dia dos oito po\u00e7os, em rela\u00e7\u00e3o ao comportamento da curva \n\nde produ\u00e7\u00e3o de \u00e1gua. Para tanto foram considerados aqueles atributos nos quais a soma das \n\nvaria\u00e7\u00f5es causadas pela altera\u00e7\u00e3o de seus valores para os limites m\u00e1ximo e m\u00ednimo resultaram \n\nem valor maior ou igual a 20%. \n\nPara sa\u00edda da RNA tamb\u00e9m foram definidas duas configura\u00e7\u00f5es diferentes:  \n\n? Op\u00e7\u00e3o 1: treinar uma RNA para cada po\u00e7o, ou seja, treinar oito RNA \n\nindependentes, de uma sa\u00edda;  \n\n? Op\u00e7\u00e3o 2: treinar uma RNA para os oito po\u00e7os, ou seja, ou RNA com oito sa\u00eddas.  \n\nAssim, os atributos marcados nas colunas \u201cPo\u00e7o 1\u201d a \u201cPo\u00e7o 8\u201d foram escolhidos para \n\ncompor os dados de treinamento para treinar uma RNA para representar cada po\u00e7o, e os atributos \n\n\n\n \n\n \n\n63 \n\n \n\nselecionados na coluna \u201cM\u00e9dia\u201d foram utilizados na op\u00e7\u00e3o em que foi treinada uma RNA para \n\nrepresentar os oito po\u00e7os. \n\nJuntando as duas op\u00e7\u00f5es de configura\u00e7\u00e3o para entrada e para a sa\u00edda, puderam-se definir \n\nquatro op\u00e7\u00f5es de configura\u00e7\u00e3o de RNA a serem estudadas nesse caso: \n\n? Op\u00e7\u00e3o 1 (PP_SAS \u2013 por po\u00e7o, sem an\u00e1lise de sensibilidade):  \n\no Entrada: todos os dezesseis atributos incertos do reservat\u00f3rio; \n\no Sa\u00edda: treinamento de oito RNA de uma sa\u00edda, cada uma representando um \n\npo\u00e7o produtor. \n\n? Op\u00e7\u00e3o 2 (PROD_SAS \u2013 produtores, sem an\u00e1lise de sensibilidade): \n\no Entrada: todos os dezesseis atributos incertos do reservat\u00f3rio; \n\no Sa\u00edda: treinamento de uma RNA com oito sa\u00eddas, representando os oito \n\npo\u00e7os produtores. \n\n? Op\u00e7\u00e3o 3 (PP_AS \u2013 por po\u00e7o, com an\u00e1lise de sensibilidade):  \n\no Entrada: apenas os atributos que mais influenciam no comportamento de \n\ncada po\u00e7o. Nesse caso quais atributos s\u00e3o utilizados como entrada para cada \n\nRNA varia. Por exemplo, para a RNA treinada para representar o po\u00e7o \n\nPROD4, de acordo com o resultado da AS mostrada na Tabela 5.3, foram \n\nutilizados como entrada os atributos 13, 15 e 16.  \n\no Sa\u00edda: treinamento de oito RNA de uma sa\u00edda, cada uma representando um \n\npo\u00e7o produtor. \n\n? Op\u00e7\u00e3o 4 (PROD_AS \u2013 produtores, com an\u00e1lise sensibilidade): \n\no Entrada: apenas os atributos que mais influenciam no comportamento dos \n\noito po\u00e7os produtores, ou seja, mais influenciam na m\u00e9dia do afastamento da \n\nvaz\u00e3o de \u00e1gua dos oito po\u00e7os produtores. \n\no Sa\u00edda: treinamento de uma RNA com oito sa\u00eddas, representando os oito \n\npo\u00e7os produtores. \n\nResumindo, as configura\u00e7\u00f5es de RNA aplicadas a cada caso foram: \n\n \n\n \n\n\n\n \n\n \n\n64 \n\n \n\nCasos 1A a 1D: \n\no Dados de treinamento: \n\no 25, 50 e 100 pontos gerados pela t\u00e9cnica do HL \n\no 25, 50 e 100 pontos gerados pela t\u00e9cnica da SS \n\no Dados de teste: \n\no RNA treinadas com pontos do HL: 100 pontos gerados pela t\u00e9cnica da SS  \n\no RNA treinadas com pontos da SS: 100 pontos gerados pela t\u00e9cnica do HL  \n\no Configura\u00e7\u00f5es de RNA:  \n\no Entrada: as duas vari\u00e1veis de modelagem da fun\u00e7\u00e3o \n\no Sa\u00edda: uma rede para representar o valor da fun\u00e7\u00e3o no ponto \n\nCaso 2A: \n\no Dados de treinamento: \n\no 25, 50 e 100 pontos gerados pela t\u00e9cnica do HL \n\no Dados de teste:  \n\no 25 pontos gerados pela t\u00e9cnica do HL \n\no Configura\u00e7\u00f5es de RNA:  \n\no Entrada: quatro atributos incertos do reservat\u00f3rio (Tabela 5.2) \n\no Sa\u00edda: uma rede para representar o afastamento da produ\u00e7\u00e3o de \u00e1gua do modelo \n\nsimulado com rela\u00e7\u00e3o ao hist\u00f3rico \n\nCaso 2B: \n\no Dados de treinamento: \n\no 100, 250 e 396 pontos gerados pela t\u00e9cnica do HL \n\no 396 pontos gerados pela t\u00e9cnica do BB \n\no Dados de teste:  \n\no 100 pontos gerados pela t\u00e9cnica do HL \n\no Configura\u00e7\u00f5es de RNA:  \n\no As quatro op\u00e7\u00f5es descritas anteriormente, \n\n\n\n \n\n \n\n65 \n\n \n\n5.4 Treinamento das redes neurais artificias \n\nPara treinar uma RNA \u00e9 necess\u00e1rio inicialmente cri\u00e1-la e configur\u00e1-la. Para isso foi \n\nutilizado o software MATLAB\n\u00ae\n\n da Mathworks Inc., pois ele possui um Toolbox de RNA (Neural \n\nNetwork Toolbox) o qual disponibiliza diversas estruturas, algoritmos e fun\u00e7\u00f5es pr\u00e9-definidas, \n\ncabendo ao usu\u00e1rio configurar a rede da maneira que desejar. Os principais par\u00e2metros \n\nconfigurados s\u00e3o mostrados a seguir. \n\n? Tipo de rede Feed-Forward completamente ligada. \n\n? Fun\u00e7\u00f5es de transfer\u00eancia do tipo tangente hiperb\u00f3lica na camada oculta e linear \n\n(Casos 1A a 1D e 2A) ou tangente hiperb\u00f3lica (Casos 2A e 2B) na camada de sa\u00edda. \n\n? M\u00e9todos de pr\u00e9 e p\u00f3s-processamento dos dados: fixunknnows, removeconstantrows \n\ne mapminmax para os dados de entrada e removeconstantrows e mapminmax para os \n\ndados de sa\u00edda. A fun\u00e7\u00e3o fixunknnows transforma linhas contendo valores definidos \n\ncomo NAN (desconhecidos) em linhas que processam a mesma informa\u00e7\u00e3o \n\nnumericamente; a fun\u00e7\u00e3o removeconstantrows remove linhas com valores \n\nconstantes e a fun\u00e7\u00e3o mapminmax transforma valores para intervalos entre -1 e 1 \n\n(normaliza\u00e7\u00e3o). \n\n? Algoritmo de treinamento Levenberg-Marquardt com Regulariza\u00e7\u00e3o Bayesiana -\n\ntrainbr- (maiores informa\u00e7\u00f5es no Cap\u00edtulo 2, de fundamenta\u00e7\u00e3o te\u00f3rica). \n\n? M\u00e9todo para prevenir contra memoriza\u00e7\u00e3o Early Stopping: separa\u00e7\u00e3o do conjunto \n\nde treinamento em tr\u00eas partes: treinamento, valida\u00e7\u00e3o e teste. O conjunto de \n\ntreinamento \u00e9 utilizado para atualizar os valores dos pesos; o conjunto de valida\u00e7\u00e3o \n\nserve para que, de tempo em tempo, a rede seja testada para verificar se est\u00e1 \n\nhavendo memoriza\u00e7\u00e3o dos dados e, caso positivo, parar o treinamento no momento \n\nem que a rede come\u00e7ar a perder a capacidade de generaliza\u00e7\u00e3o (erro de treinamento \n\nbaixo, por\u00e9m erro de valida\u00e7\u00e3o alto); e o conjunto de teste serve parar testar a rede \n\nap\u00f3s o treinamento. Os dados s\u00e3o separados de forma aleat\u00f3ria, de maneira que o \n\ntreinamento deve ser realizado diversas vezes para obter resultados satisfat\u00f3rios. O \n\nvalor default \u00e9 60%, 20% e 20% (treino, valida\u00e7\u00e3o e teste) do total de dados de \n\ntreinamento e foi o valor adotado nesse trabalho. \n\n\n\n \n\n \n\n66 \n\n \n\nAinda existe o algoritmo de inicializa\u00e7\u00e3o dos pesos da rede. Esse algoritmo utiliza um \n\npadr\u00e3o tal que os valores dos pesos s\u00e3o inicializados em uma faixa determinada com certo grau \n\nde aleatoriedade. Assim, a cada inicializa\u00e7\u00e3o t\u00eam-se diferentes valores dos pesos, o que pode \n\nlevar a RNA diferentes no final do treinamento. Portanto, \u00e9 altamente recomend\u00e1vel realizar \n\ndiversos treinamentos com a mesma configura\u00e7\u00e3o para determinar o melhor modelo para o \n\nproblema.  \n\nCom base nas sugest\u00f5es de Silva e Oliveira (2004), para os Casos 1A a 1D e Caso 2A foi \n\nutilizada uma camada oculta e para o Caso 2B,  de maior complexidade, foram testadas uma, \n\nduas e tr\u00eas camadas para treinar as redes. Para o n\u00famero de neur\u00f4nios em cada camada utilizou-\n\nse um valor tal que a quantidade de sinapses fosse em torno de dez vezes menor do que o n\u00famero \n\nde amostras utilizadas para treinamento. Para o Caso 2B foram testados mais valores situados ao \n\nredor desse patamar para expandir o horizonte de resultados.  \n\nAinda em rela\u00e7\u00e3o ao Caso 2B, devido \u00e0 complexidade do problema, foi testada a gera\u00e7\u00e3o \n\nde um metamodelo por po\u00e7o e um metamodelo para os oito po\u00e7os produtores, ou seja, foram \n\ntreinadas oito redes de uma sa\u00edda e uma \u00fanica rede com oito sa\u00eddas. Dentre essas duas \n\npossibilidades, foram utilizadas como entrada um conjunto de dados contendo as dezesseis \n\nvari\u00e1veis incertas e um conjunto de dados provenientes da an\u00e1lise de sensibilidade, conforme \n\ndescrito no Item 5.3. \n\n5.5 Otimiza\u00e7\u00e3o  \n\nO processo de otimiza\u00e7\u00e3o foi realizado utilizando o algoritmo gen\u00e9tico. O MATLAB\n\u00ae\n\n \n\ntamb\u00e9m disponibiliza um Toolbox (Global Optimization Toolbox) para essa t\u00e9cnica j\u00e1 com as \n\nconfigura\u00e7\u00f5es padr\u00e3o. Foram utilizadas para todos os casos essas configura\u00e7\u00f5es com algumas \n\nmodifica\u00e7\u00f5es, julgadas necess\u00e1rias a cada caso. A Tabela 5.4 mostra os valores dos par\u00e2metros \n\nprincipais que foram utilizados pelo AG nos processos de otimiza\u00e7\u00e3o. O tamanho da popula\u00e7\u00e3o \n\nrefere-se \u00e0 quantidade de modelos que ser\u00e3o utilizados em cada itera\u00e7\u00e3o; o n\u00famero m\u00e1ximo de \n\ngera\u00e7\u00f5es representa o m\u00e1ximo de itera\u00e7\u00f5es que o algoritmo ir\u00e1 executar; a fra\u00e7\u00e3o de crossover \n\nindica quantos indiv\u00edduos, do total, sofrer\u00e3o recombina\u00e7\u00e3o (crossover); e a taxa de muta\u00e7\u00e3o \n\nrepresenta a probabilidade de o indiv\u00edduo sofrer muta\u00e7\u00e3o. \n\n\n\n \n\n \n\n67 \n\n \n\nTabela 5.4 \u2013 Principais par\u00e2metros utilizados pelo AG. \n\nPar\u00e2metro \nValor \n\nCasos 1A a 1D Caso 2A Caso 2B \n\nTamanho da popula\u00e7\u00e3o 50 50 50 \n\nM\u00e1ximo de gera\u00e7\u00f5es 100 500 500 \n\nFra\u00e7\u00e3o de crossover 0.6 0.8 0.8 \n\nTaxa de muta\u00e7\u00e3o 50% 50% 50% \n\n\n\n\n\n \n\n \n\n69 \n\n \n\n6 RESULTADOS E DISCUSS\u00c3O \n\nOs resultados mostrados nesse cap\u00edtulo referem-se \u00e0 aplica\u00e7\u00e3o do procedimento de ajuste \n\ndefinido no Subitem 4.2 de Metodologia. \n\nConforme citado no Subitem 4.1, primeiramente foram estudados os casos anal\u00edticos \n\n(Casos 1A a 1D) e, posteriormente, o caso intermedi\u00e1rio (Caso 2A) e complexo (Caso 2B). \n\nAssim, os itens subsequentes foram ordenados dessa maneira: casos anal\u00edticos, caso intermedi\u00e1rio \n\ne caso complexo, seguindo os passos do procedimento de ajuste adotado. \n\nConforme detalhado no cap\u00edtulo de aplica\u00e7\u00e3o, foram utilizados diferentes conjuntos de \n\npontos para treinamento, gerados atrav\u00e9s das t\u00e9cnicas do HL e SS. Nos itens subsequentes, no \n\nentanto, n\u00e3o ser\u00e3o mostrados todos os gr\u00e1ficos e resultados devido ao grande volume de dados \n\nobtidos. Ser\u00e3o mostrados apenas alguns resultados, considerados de maior relev\u00e2ncia para as \n\nconclus\u00f5es. \n\n6.1 Caso 1A \n\n6.1.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.1 mostra os conjuntos de treinamento de 25 pontos amostrados utilizando as \n\nt\u00e9cnicas do HL (a) e SS (b), respectivamente, representados pelos pontos em preto, sobrepostos \n\nna superf\u00edcie de resposta, gerada pela fun\u00e7\u00e3o anal\u00edtica. \n\n\n\n \n\n \n\n70 \n\n \n\n \n\n(a) (b) \n\nFigura 6.1 \u2013 Amostragem no espa\u00e7o do conjunto de treinamento de 25 pontos; HL (a) e SS (b) \u2013 \n\nCaso 1A. \n\nPela Figura 6.1 observa-se que o HL proporcionou espa\u00e7amento melhor dos pontos do que \n\na SS, que possui algumas regi\u00f5es concentradas de pontos. Pode-se observar ainda que 25 pontos \n\ns\u00e3o capazes de proporcionar boa cobertura de todo o espa\u00e7o. Para os demais conjuntos de pontos \n\np\u00f4de-se observar melhor cobertura do espa\u00e7o de solu\u00e7\u00f5es e a diminui\u00e7\u00e3o dos espa\u00e7os vazios \n\nentre pontos, constantes na  Figura 6.1 (b). \n\n6.1.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nPor se tratar de um caso muito simples, pouca diferen\u00e7a com rela\u00e7\u00e3o \u00e0 qualidade do \n\nmetamodelo p\u00f4de ser observada entre os dados gerados com HL25 e SS25.  \n\nPara obter um comparativo entre quantidade de amostras, a Figura 6.2 mostra as sa\u00eddas dos \n\nmetamodelos, gerados atrav\u00e9s do treinamento das RNA com HL25 (a) e HL50 (b) pontos. \n\n\n\n \n\n \n\n71 \n\n \n\n \n\n(a) (b) \n\nFigura 6.2 \u2013 Superf\u00edcies de resposta e conjunto de teste, para os metamodelos gerados com HL25 \n\n(a) e HL50 (b) \u2013 Caso 1A. Pontos em azul: erro m\u00e9dio menor que 1% e pontos em vermelho: erro \n\nm\u00e9dio maior ou igual a 1%. \n\nPode-se observar que o erro para o metamodelo gerado com HL50 pontos foi menor, \n\nmostrando que o aumento da quantidade de amostras para treinamento melhorou a capacidade de \n\nrepresenta\u00e7\u00e3o. \n\nO erro pode ser visualizado de forma diferente na Figura 6.3, que mostra os gr\u00e1ficos do erro \n\nem fun\u00e7\u00e3o da coordenada do eixo horizontal, para os metamodelos gerados com HL25 (a) e \n\nHL50 (b) pontos, em que s\u00e3o mostrados apenas os pontos nos quais o erro foi menor que 4% e \n\n0.012% para os metamodelos gerados com HL25 e HL50 pontos, respectivamente.  \n\n \n\n\n\n \n\n \n\n72 \n\n \n\n \n\n(a) (b) \n\nFigura 6.3 \u2013 Visualiza\u00e7\u00e3o dos erros ponto a ponto para os metamodelos gerados com HL25 (a) e \n\nHL50 (b) pontos \u2013 Caso 1A. \n\nOs valores dos coeficientes de correla\u00e7\u00e3o linear e erro m\u00e9dio entre sa\u00eddas do metamodelo e \n\nsa\u00eddas calculadas pela equa\u00e7\u00e3o que modela a fun\u00e7\u00e3o, referentes ao conjunto de teste est\u00e3o \n\nmostrados na Tabela 6.1. \n\n6.1.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nOs valores dos m\u00ednimos encontrados, assim como suas respectivas localiza\u00e7\u00f5es no espa\u00e7o \n\nest\u00e3o mostrados na Tabela 6.1. Observa-se que a utiliza\u00e7\u00e3o de 25 amostras para treinamento foi \n\nsuficiente para alcan\u00e7ar resultados satisfat\u00f3rios, pois o metamodelo Caso1A_RNA_HL25 \n\nencontrou uma coordenada para o m\u00ednimo muito pr\u00f3ximo da localiza\u00e7\u00e3o real do m\u00ednimo global \n\n(0,0). Observa-se ainda que a adi\u00e7\u00e3o de mais amostras para treinamento ajudou a melhorar a \n\nprecis\u00e3o.  \n\n\u00c9 importante salientar que, mesmo com muitos pontos, o erro ainda existe, pois o m\u00ednimo \n\nglobal n\u00e3o est\u00e1 inclu\u00eddo no conjunto de treinamento, o que induz a RNA a \u201csuavizar\u201d a superf\u00edcie \n\nnessa \u00e1rea. \n\n-10 -5 0 5 10\n0\n\n1\n\n2\n\n3\n\n4\n\nx\n\nE\nrr\n\no\n (\n\n%\n)\n\n-10 -5 0 5 10\n0\n\n0.002\n\n0.004\n\n0.006\n\n0.008\n\n0.01\n\n0.012\n\nx\n\nE\nrr\n\no\n (\n\n%\n)\n\n\n\n \n\n \n\n73 \n\n \n\nComparando-se os resultados obtidos entre os metamodelos gerados com HL e SS, \n\nobserva-se que os resultados foram bem pr\u00f3ximos, n\u00e3o sendo poss\u00edvel concluir qual tipo de \n\nferramenta de amostragem foi a melhor. \n\nNesse caso n\u00e3o foi necess\u00e1rio realizar o processo de retreinamento, pois conforme \n\nmostrado atrav\u00e9s dos gr\u00e1ficos e tabela apresentados, a regi\u00e3o de m\u00ednimo foi identificada com \n\nprecis\u00e3o com apenas 25 pontos. \n\nTabela 6.1 \u2013 Coeficiente de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimo determinados atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando o metamodelo - Caso 1A. \n\nTipo de \n\ndado \n\nNome \n\nMetamodelo \n\nErro \n\n(%) \n\nCoeficiente \n\ncorrela\u00e7\u00e3o \n\nM\u00ednimo \n\n(FO) \nVari\u00e1vel x Vari\u00e1vel y \n\nHL25 Caso1A_RNA_HL25 0.4195 0.9999 0.16320 0.012000 0.001400 \n\nHL50 Caso1A_RNA _ HL50 0.0017\n\n1 \n1.0000 0.01700 -0.0000398 0.000034 \n\nHL100 Caso1A_RNA _ HL100 0.0011 1.0000 0.000192 0.0000235 0.000257 \n\nSS25 Caso1A_RNA _ SS25 0.6919 0.9999 0.07630 0.022700 0.014100 \n\nSS50 Caso1A_RNA _ SS50 0.0065 1.0000 0.00100 -0.000227 -0.0000357 \n\nSS100 Caso1A_RNA _ SS100 0.0021 1.0000 -0.00034 0.000095 0.0000518 \n\n6.2 Caso 1B \n\n6.2.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.4 mostra os pontos amostrados para treinamento (em preto) no espa\u00e7o de busca \n\ndos par\u00e2metros para os conjuntos de HL25 (a) e SS25 (b) pontos.  \n\n\n\n \n\n \n\n74 \n\n \n\n \n\n(a) (b) \n\nFigura 6.4 \u2013 Amostras de 25 pontos, do HL (a) e da SS (b) para treinamento \u2013 Caso 1B. \n\nObserva-se que com 25 pontos poucas informa\u00e7\u00f5es do comportamento da superf\u00edcie foram \n\nobtidas. Por outro lado, dentre os 25 pontos do HL, um deles se situa na regi\u00e3o de interesse, \n\npr\u00f3xima do m\u00ednimo global, o que pode fazer com que essa regi\u00e3o seja identificada pelo \n\nmetamodelo gerado.  \n\nA Figura 6.5 mostra os conjuntos de treinamento para HL50 e SS50 pontos. \n\n \n\n(a) (b) \n\nFigura 6.5 - Amostras de 50 pontos, do HL (a) e da SS (b) para treinamento \u2013 Caso 1B. \n\nObserva-se que com 50 pontos, informa\u00e7\u00f5es mais detalhadas sobre a superf\u00edcie de resposta \n\nforam amostradas. Por\u00e9m, ainda existem espa\u00e7os vazios, o que pode fazer com que nessas regi\u00f5es \n\n\n\n \n\n \n\n75 \n\n \n\na precis\u00e3o seja inferior \u00e0s demais. No entanto, a presen\u00e7a de um n\u00famero maior de amostras nas \n\nvizinhan\u00e7as pode fazer com que uma representa\u00e7\u00e3o satisfat\u00f3ria seja obtida (padr\u00e3o de \n\ncomportamento dessas regi\u00f5es seja representado). \n\nApesar de n\u00e3o apresentada, a amostragem com 100 pontos, para as duas t\u00e9cnicas, foi capaz \n\nde proporcionar uma boa cobertura do espa\u00e7o de solu\u00e7\u00f5es. \n\n6.2.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nAs superf\u00edcies de resposta dos metamodelos gerados com HL25 e SS25 pontos s\u00e3o \n\nmostradas na Figura 6.6 (a) e (b), respectivamente. Para mostrar os pontos sobre o espa\u00e7o foi \n\nutilizado um \u00e2ngulo de vis\u00e3o diferente para proporcionar uma avalia\u00e7\u00e3o mais eficiente da \n\ndistribui\u00e7\u00e3o dos mesmos. Por outro lado, para mostrar a superf\u00edcie de reposta do metamodelo foi \n\nutilizado outro \u00e2ngulo de vis\u00e3o, com o intuito de proporcionar uma avalia\u00e7\u00e3o mais eficiente de \n\ntodas as irregularidades presentes. Esse padr\u00e3o foi aplicado para todos os casos anal\u00edticos \n\napresentados.  \n\n \n\n(a) (b) \n\nFigura 6.6 \u2013 Superf\u00edcies de resposta dos metamodelos gerados com 25 pontos do HL (a) e da SS \n\n(b) \u2013 Caso 1B. \n\n\n\n \n\n \n\n76 \n\n \n\nPela Figura 6.6, observa-se que apesar de apresentar maior dificuldade em rela\u00e7\u00e3o ao Caso \n\n1A, o metamodelo gerado com HL25 pontos foi capaz de identificar a regi\u00e3o de m\u00ednimo. \u00c9 de se \n\nnotar que nas regi\u00f5es \u00e0s quais pouca ou nenhuma informa\u00e7\u00e3o (amostragem) foi obtida, o \n\nmetamodelo n\u00e3o proporcionou boa representa\u00e7\u00e3o. Esse aspecto pode ser analisado comparando-\n\nse as imagens da Figura 6.6 com as da Figura 6.4, que mostram as regi\u00f5es com e sem informa\u00e7\u00e3o \n\nna amostragem. \n\nObserva-se ainda que, conforme previsto no Subitem 6.2.1, a presen\u00e7a de uma amostra \n\ndentro da regi\u00e3o de interesse fez com que o metamodelo fosse capaz de fornecer uma boa \n\nmodelagem dessa regi\u00e3o. Da mesma forma a aus\u00eancia de amostras dentro dessa \u00e1rea para o \n\nconjunto SS25 fez com que o metamodelo gerado com esse conjunto n\u00e3o a representasse bem. A \n\ncompara\u00e7\u00e3o entre as duas superf\u00edcies da Figura 6.6 deixa claro que a representa\u00e7\u00e3o \n\nproporcionada pelo metamodelo gerado com 25 pontos do HL foi superior a SS, conforme \n\nprevisto no Subitem 6.2.1. \n\nA Tabela 6.2 mostra os valores dos coeficientes de correla\u00e7\u00e3o e do erro m\u00e9dio entre sa\u00edda \n\ndo metamodelo e da equa\u00e7\u00e3o que modela a superf\u00edcie, relativos ao conjunto de teste para todos os \n\nmetamodelos.  \n\nObserva-se que foram obtidos valores elevados para o erro m\u00e9dio. O motivo se deve ao fato \n\nde a parte plana da superf\u00edcie de resposta desse caso situar-se muito pr\u00f3xima do zero \n\n(z=4.1030x10\n-5\n\n), de modo que, assim como no Caso 1A, qualquer valor um pouco diferente desse \n\nponto acabou gerando erro elevado. Uma poss\u00edvel sa\u00edda seria utilizar a diferen\u00e7a simples para \n\nmedi\u00e7\u00e3o do erro ao inv\u00e9s da Equa\u00e7\u00e3o 5.1, ou adicionar uma constante \u00e0 fun\u00e7\u00e3o para \u201celevar\u201d a \n\nsuperf\u00edcie de resposta de modo a evitar a realiza\u00e7\u00e3o de c\u00e1lculos de valores pr\u00f3ximos do zero.  \n\nAo se compararem os valores dos coeficientes de correla\u00e7\u00e3o dos metamodelos gerados com \n\nHL25 e SS25, os valores foram semelhantes, por\u00e9m, como mostra a Figura 6.6, a superf\u00edcie \n\nmodelada com cada um foi diferente. Assim, atrav\u00e9s da an\u00e1lise dos resultados fica clara a \n\nimport\u00e2ncia de realizar a avalia\u00e7\u00e3o conjunta entre resultados num\u00e9ricos e gr\u00e1ficos, pois a \n\navalia\u00e7\u00e3o de apenas um deles isoladamente pode levar a conclus\u00f5es equivocadas. \n\nCom o aumento do n\u00famero de amostras para 50 e 100 pontos, para ambas as t\u00e9cnicas, a \n\nrepresenta\u00e7\u00e3o da superf\u00edcie de resposta proporcionada pelos metamodelos melhorou \n\n\n\n \n\n \n\n77 \n\n \n\nsignificativamente, conforme pode ser observado pelos coeficientes de correla\u00e7\u00e3o mostrados na \n\nTabela 6.2, que superaram 0.97, e pelos resultados gr\u00e1ficos apresentados na Figura 6.7, que \n\nmostram as superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), HL100 \n\n(c) pontos e a superf\u00edcie da fun\u00e7\u00e3o anal\u00edtica (d). \n\n \n\n(a) (b) \n\n \n\n(c) (d) \n\nFigura 6.7 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), HL100 \n\n(c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1B. \n\nA partir da visualiza\u00e7\u00e3o das superf\u00edcies da Figura 6.7, fica clara a melhora na qualidade da \n\nsuperf\u00edcie gerada pelos metamodelos com o aumento do n\u00famero de amostras, podendo observar \n\ncomo a superf\u00edcie vai se assemelhando \u00e0 da fun\u00e7\u00e3o anal\u00edtica com a adi\u00e7\u00e3o de mais amostras para \n\ntreinamento. \n\n\n\n \n\n \n\n78 \n\n \n\n6.2.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nOs valores dos m\u00ednimos encontrados, assim como suas coordenadas no espa\u00e7o para a \n\notimiza\u00e7\u00e3o utilizando os metamodelos gerados est\u00e3o mostradas na Tabela 6.2, que tamb\u00e9m \n\nmostra o m\u00ednimo e suas coordenadas para a fun\u00e7\u00e3o anal\u00edtica (\u201cFun\u00e7\u00e3o peaks\u201d). \n\nApesar da semelhan\u00e7a num\u00e9rica entre os coeficientes de correla\u00e7\u00e3o linear obtidos com os \n\nmetamodelos gerados com HL25 e SS25 (0.6911 e 0.7110, respectivamente), o valor final da \n\notimiza\u00e7\u00e3o para o metamodelo Caso1B_RNA_HL25 foi melhor (valor do m\u00ednimo), conforme \n\ncomprovado visualmente pela Figura 6.6.  \n\nTabela 6.2 \u2013 Coeficiente de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimo determinados atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando o metamodelo - Caso 1B. \n\nTipo de \n\ndado \n\nNome \n\nRede \n\nErro \n\nm\u00e9dio (%) \n\nCoeficiente \n\ncorrela\u00e7\u00e3o \nM\u00ednimo Vari\u00e1vel x Vari\u00e1vel y \n\n- Fun\u00e7\u00e3o peaks - - -6.5511 0.2258 -1.6258 \n\nHL25 Caso1B_RNA_HL25 22590 0.6911 -5.6154 0.2258 -1.7159 \n\nHL50 Caso1B_RNA_HL50 12849 0.9755 -6.4047 0.2878 -1.5755 \n\nHL100 Caso1B_RNA_HL100 3032.2 0.9993 -6.5550 0.2106 -1.6257 \n\nSS25 Caso1B_RNA_SS25 8200.8 0.7110 -1.8480 0.4061 -1.9087 \n\nSS50 Caso1B_RNA_SS50 6563.5 0.9786 -5.3705 0.2572 -1.5941 \n\nSS100 Caso1B_RNA_SS100 4346.3 0.9996 -6.5573 0.2325 -1.6353 \n\n6.2.4 Passo 4: Retreinamento \n\nA partir dos resultados da otimiza\u00e7\u00e3o utilizando o metamodelo Caso1B_RNA_HL25 \n\ndefiniu-se uma nova regi\u00e3o de busca, de acordo com o m\u00ednimo encontrado na otimiza\u00e7\u00e3o. O \n\ncrit\u00e9rio adotado para definir a nova regi\u00e3o de amostragem foi baseado em uma inspe\u00e7\u00e3o visual. \n\nUma vez que os casos te\u00f3ricos possibilitarem avaliar diretamente a superf\u00edcie de resposta, a \n\ndefini\u00e7\u00e3o da nova regi\u00e3o tamb\u00e9m foi realizada de forma direta, sem utilizar o crit\u00e9rio de valor de \n\ncorte, conforme consta na metodologia. Assim, a partir das coordenadas do m\u00ednimo encontrado \n\npela otimiza\u00e7\u00e3o utilizando o metamodelo, foi considerada uma regi\u00e3o ao redor com dist\u00e2ncia \n\nunit\u00e1ria, respeitando os limites m\u00e1ximos dos par\u00e2metros. \n\n\n\n \n\n \n\n79 \n\n \n\nA Tabela 6.3 mostra as coordenadas do m\u00ednimo encontrado com a otimiza\u00e7\u00e3o utilizando o \n\nmetamodelo e o novo limite de varia\u00e7\u00e3o dos par\u00e2metros para amostragem. \n\nTabela 6.3 \u2013 Defini\u00e7\u00e3o dos novos limites para retreinamento \u2013 Caso 1B. \n\nMetamodelo \nCoordenada do \n\nm\u00ednimo x \n\nCoordenada do \n\nm\u00ednimo y \n\nNovos limites- \n\nvari\u00e1vel x \n\nNovos limites-\n\nvari\u00e1vel y \n\nRNA_caso2_HL25 0.2258 -1.7159 [-0.7, 1.3] [-2.7, -0.7] \n\n  \n\nNa Figura 6.8 (a) \u00e9 mostrado o novo limite de varia\u00e7\u00e3o dos par\u00e2metros (\u00e1rea em negrito) e \n\nos respectivos pontos amostrados (em vermelho, sendo amostrados 25 pontos pelo HL), e a \n\nFigura 6.8 (b) mostra a superf\u00edcie de resposta do metamodelo gerado com o novo treinamento \n\n(regi\u00e3o em negrito da Figura 6.8 (a)), em que os pontos em vermelho representam regi\u00f5es onde o \n\nerro em rela\u00e7\u00e3o \u00e0 superf\u00edcie da fun\u00e7\u00e3o anal\u00edtica ultrapassa 10% e os pontos em azul representam \n\nerros at\u00e9 10%.  \n\n \n\n(a) (b) \n\nFigura 6.8 \u2013 Conjunto de treinamento e novos limites (a) e superf\u00edcie de resposta do metamodelo \n\ngerado (b), relativos ao retreinamento \u2013 Caso 1B. \n\nOs valores encontrados com a otimiza\u00e7\u00e3o utilizando o novo metamodelo s\u00e3o mostrados na  \n\nTabela 6.4, juntamente com os resultados do treinamento com 25 e 100 pontos e da fun\u00e7\u00e3o \n\nanal\u00edtica (Peaks). O erro m\u00e9dio apresentado corresponde ao valor calculado na \u00e1rea retreinada. \n\n \n\n\n\n \n\n \n\n80 \n\n \n\nTabela 6.4 - Resultados obtidos com o retreinamento \u2013 Caso 1B. \n\nMetamodelo Erro M\u00e9dio (%) M\u00ednimo Vari\u00e1vel x Vari\u00e1vel y \n\nPeaks - -6.5511 0.2258 -1.6258 \n\nHL25 149.6033 -5.6154 0.2258 -1.7159 \n\nHL100 10.2412 -6.5550 0.2106 -1.6257 \n\nHL25_RTR 10.7055 -6.5517 0.2265 -1.6265 \n\n \n\nConforme pode ser observado, o retreinamento melhorou o resultado, obtendo erro m\u00e9dio \n\nbaixo para a regi\u00e3o retreinada e foi capaz de encontrar um valor muito pr\u00f3ximo do m\u00ednimo global \n\nda fun\u00e7\u00e3o.  \n\nCom o processo de retreinamento foram utilizadas, no total, 50 amostras (25 pontos para o \n\nprimeiro treinamento e mais 25 pontos para o retreinamento) e os resultados obtidos foram \n\nsemelhantes aos obtidos com 100 pontos. Para esse caso, realizar um treinamento geral \n\n(representa\u00e7\u00e3o de todo o espa\u00e7o de solu\u00e7\u00f5es) para encontrar a regi\u00e3o de m\u00ednimo e, \n\nposteriormente, realizar uma busca mais refinada (amostragem em uma regi\u00e3o menor, espec\u00edfica) \n\nse mostrou eficiente. Por\u00e9m, vale ressaltar que se fosse utilizado o metamodelo treinado com \n\nSS25, talvez o objetivo n\u00e3o fosse alcan\u00e7ado, uma vez que ele n\u00e3o foi capaz de representar bem a \n\nsuperf\u00edcie de resposta na regi\u00e3o de m\u00ednimo, conforme comentado no Subitem 6.2.3. Isso mostra a \n\ngrande import\u00e2ncia da qualidade dos dados de treinamento, os quais influenciam fortemente no \n\nsucesso do processo de retreinamento e representa\u00e7\u00e3o da superf\u00edcie de resposta. \n\nMesmo para o caso do metamodelo gerado com SS25 pontos poderia ser adotado um \n\nprocedimento de retreinamento diferente daquele proposto na metodologia (que consiste em \n\nrealizar busca em um regi\u00e3o espec\u00edfica). Um novo treinamento, ap\u00f3s uma nova amostragem de \n\ntodo o espa\u00e7o de busca dos par\u00e2metros poderia ser realizada. Assim, podem-se adicionar pontos \n\nde treinamento em etapas, de forma iterativa, possibilitando encontrar uma quantidade reduzida \n\nde amostras que s\u00e3o capazes de proporcionar boa representa\u00e7\u00e3o do espa\u00e7o de solu\u00e7\u00f5es.  \n\nEsse procedimento, no entanto, considerando as t\u00e9cnicas de amostragem utilizadas nesse \n\ntrabalho se restringe a ser aplicada somente com a t\u00e9cnica do HL, uma vez que a SS possui \n\nlimita\u00e7\u00f5es quanto a gerar conjuntos com caracter\u00edsticas diferentes e o BB gera sempre o mesmo \n\nconjunto de pontos.  \n\n\n\n \n\n \n\n81 \n\n \n\nApesar de visualmente estar claro quando um m\u00ednimo est\u00e1 na regi\u00e3o de interesse, para \n\ncasos com mais de duas vari\u00e1veis essa conclus\u00e3o deve ser tomada realizando-se a valida\u00e7\u00e3o do \n\nm\u00ednimo obtido atrav\u00e9s da otimiza\u00e7\u00e3o com o m\u00ednimo real. \n\nO mesmo princ\u00edpio aplica-se \u00e0 defini\u00e7\u00e3o da nova regi\u00e3o de retreinamento. Nesse caso foi \n\nposs\u00edvel obter maior confian\u00e7a atrav\u00e9s da inspe\u00e7\u00e3o visual, por\u00e9m, em casos com mais de duas \n\nvari\u00e1veis deve-se utilizar a metodologia do valor de corte, descrita no Cap\u00edtulo 4. \n\n6.3 Caso 1C \n\n6.3.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.9 mostra os conjuntos de treinamento para 25 pontos gerados atrav\u00e9s das \n\nt\u00e9cnicas do HL (a) e da SS (b), distribu\u00eddos na superf\u00edcie de busca dos par\u00e2metros. \n\n \n\n(a) (b) \n\nFigura 6.9 - Conjunto de treinamento para HL25 (a) e SS25 (b) - Caso 1C. \n\nObserva-se que, tanto para o HL quanto para a SS, existe uma amostra em cada uma das \n\nquatro regi\u00f5es de m\u00ednimo global da fun\u00e7\u00e3o. Por\u00e9m, quase nenhum ponto foi amostrado nas \n\nregi\u00f5es de m\u00ednimos locais, situados pr\u00f3ximos aos m\u00ednimos globais. Isso pode fazer com que o \n\nmetamodelo n\u00e3o seja capaz de determinar as regi\u00f5es de m\u00ednimos globais precisamente, pois n\u00e3o \n\n\n\n \n\n \n\n82 \n\n \n\nse sabe que padr\u00e3o de comportamento a superf\u00edcie ter\u00e1 ao redor dessas regi\u00f5es que n\u00e3o tem \n\ninforma\u00e7\u00e3o. \n\nA Figura 6.10 mostra os conjuntos de treinamento para 50 pontos amostrados utilizando as \n\nt\u00e9cnicas do HL (a) e da SS (b). \n\n \n\n(a) (b) \n\nFigura 6.10 \u2013 Conjunto de treinamento para HL50 (a) e SS50 (b) - Caso 1C. \n\nA Figura 6.10 mostra que com 50 pontos foi poss\u00edvel melhorar a amostragem em algumas \n\nregi\u00f5es, por\u00e9m ainda ficaram algumas \u201clacunas\u201d entre os pontos amostrados, de forma que \n\npossivelmente uma representa\u00e7\u00e3o precisa da superf\u00edcie de resposta em todo o espa\u00e7o tamb\u00e9m n\u00e3o \n\nseja poss\u00edvel para essa quantidade de amostras. J\u00e1 com a amostragem realizada com 100 pontos, \n\nambas as t\u00e9cnicas proporcionaram boa amostragem do espa\u00e7o. \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n83 \n\n \n\n6.3.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nA Figura 6.11 mostra as superf\u00edcies de resposta dos metamodelos gerados com HL25 (a) e \n\nSS25 (b) pontos.  \n\n \n\n(a) (b) \n\nFigura 6.11 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a) e SS25 (b) pontos - \n\nCaso 1C. \n\nObserva-se que 25 pontos foram incapazes de extrair todas as informa\u00e7\u00f5es necess\u00e1rias para \n\numa representa\u00e7\u00e3o confi\u00e1vel da fun\u00e7\u00e3o. A partir da Figura 6.11 (a) pode-se visualizar que os \n\nquatro pontos amostrados na regi\u00e3o de m\u00ednimo global contribu\u00edram para que o metamodelo \n\nrepresentasse bem essas regi\u00f5es (regi\u00f5es em cor azul acentuada). No entanto, o padr\u00e3o de \n\ncomportamento nas outras regi\u00f5es ficou mal representado devido \u00e0 falta de informa\u00e7\u00f5es.  \n\nA Tabela 6.5 mostra os valores dos coeficientes de correla\u00e7\u00e3o e do erro m\u00e9dio entre a sa\u00edda \n\ngerada pelo metamodelo e a sa\u00edda calculada pela equa\u00e7\u00e3o que descreve a fun\u00e7\u00e3o, com rela\u00e7\u00e3o ao \n\nconjunto de teste para todos os metamodelos gerados. Os resultados quantitativos apresentados na \n\nTabela 6.5 mostram que os metamodelos gerados com 50 pontos tamb\u00e9m foram incapazes de \n\nproporcionar uma boa representa\u00e7\u00e3o, pois os coeficientes de correla\u00e7\u00e3o foram baixos e os erros \n\nm\u00e9dios foram altos, apesar de melhorarem bastante os resultados em rela\u00e7\u00e3o aos metamodelos \n\ngerados com 25 pontos. \n\n\n\n \n\n \n\n84 \n\n \n\nA Figura 6.12 mostra as superf\u00edcies de resposta dos metamodelos gerados com HL50 (a) e \n\nSS50 (b) pontos. \n\n \n\n(a) (b) \n\nFigura 6.12 - Superf\u00edcies de resposta dos metamodelos gerados com HL50 (a) e SS50 (b) pontos - \n\nCaso 1C. \n\nAo se comparar as superf\u00edcies geradas pode-se observar que, apesar de resultarem em \n\ncoeficientes de correla\u00e7\u00e3o parecidos, o formato das superf\u00edcies resultantes das duas t\u00e9cnicas foi \n\ndiferente, refletindo os valores de erro m\u00e9dio. Como o caso possui muitas irregularidades, a \n\ndiferen\u00e7a da caracter\u00edstica de espa\u00e7amento dos pontos ficou evidente (quando analisada \n\nvisualmente). \n\nOs valores encontrados para os coeficientes de correla\u00e7\u00e3o e erros m\u00e9dios dos metamodelos \n\ngerados com 100 pontos mostram que uma representa\u00e7\u00e3o confi\u00e1vel foi obtida, conforme pode ser \n\nvisualizado pela Figura 6.13, que mostra as superf\u00edcies de resposta dos metamodelos gerados com \n\nHL100 (a) e SS100 (b) pontos. \n\n\n\n \n\n \n\n85 \n\n \n\n \n\n(a) (b) \n\nFigura 6.13 - Superf\u00edcies de resposta dos metamodelos gerados com HL100 (a) e SS100 (b) \n\npontos - Caso 1C. \n\nPela Figura 6.13 fica claro que com 100 pontos a diferen\u00e7a entre a caracter\u00edstica de \n\nespa\u00e7amento entre as duas t\u00e9cnicas desapareceu, e ambas proporcionaram resultados \n\nsemelhantes. \n\nPara mostrar como a qualidade de representa\u00e7\u00e3o melhorou com o aumento da quantidade \n\nde amostras a Figura 6.14 mostra as superf\u00edcies dos metamodelos gerados com HL 25 (a), 50 (b) \n\ne 100 (c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d). \n\n \n\n \n\n \n\n\n\n \n\n \n\n86 \n\n \n\n \n\n(a) (b) \n\n \n\n(c) (d) \n\nFigura 6.14 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), HL100 \n\n(c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1C. \n\nAssim, pode-se concluir que, para esse caso, foram necess\u00e1rios 100 pontos para uma boa \n\nrepresenta\u00e7\u00e3o do problema. O erro m\u00e9dio mostrado na Tabela 6.5 indica que os 25 e 50 pontos \n\nn\u00e3o forneceram toda a informa\u00e7\u00e3o necess\u00e1ria para reproduzir as irregularidades presentes. O \n\nresultado da escassez de informa\u00e7\u00e3o foi uma superf\u00edcie \u201cmais suave\u201d, com menos irregularidades \n\ne, portanto, com menos precis\u00e3o. \n\nNesses casos de duas vari\u00e1veis ficou expl\u00edcito, atrav\u00e9s da inspe\u00e7\u00e3o visual do espa\u00e7o de \n\nsolu\u00e7\u00f5es, quando um conjunto de treinamento n\u00e3o \u00e9 capaz de representar o comportamento \n\ndesejado. Para casos com mais de duas vari\u00e1veis, por\u00e9m, devem-se utilizar indicadores de \n\n\n\n \n\n \n\n87 \n\n \n\nqualidade para chegar a essas conclus\u00f5es. O coeficiente de correla\u00e7\u00e3o linear pode mostrar se o \n\nmetamodelo gerado foi capaz de representar a regi\u00e3o descrita pelos dados de teste. Por\u00e9m, uma \n\nconclus\u00e3o precisa \u00e9 extra\u00edda atrav\u00e9s da valida\u00e7\u00e3o do m\u00ednimo encontrado com a resposta \n\nconhecida do problema. \n\n6.3.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nOs m\u00ednimos encontrados e suas respectivas coordenadas no espa\u00e7o para o processo de \n\notimiza\u00e7\u00e3o utilizando os metamodelos gerados s\u00e3o mostrados na Tabela 6.5.  \n\nTabela 6.5 - Coeficiente de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimo determinados atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando os metamodelos - Caso 1C. \n\nTipo de \n\ndado \nMetamodelo \n\nErro \n\nm\u00e9dio (%) \n\nCoeficiente \n\ncorrela\u00e7\u00e3o \nM\u00ednimo Vari\u00e1vel x Vari\u00e1vel y \n\nHL25 Caso1C_RNA_HL25 177.1364 0.3235 -0.8669 -0.7414 -0.9896 \n\nHL50 Caso1C_RNA_HL50 253.3417 0.4132 -1.9427 -0.9959 0.9973 \n\nHL100 Caso1C_RNA_HL100 1.5516 0.9999 -0.7433 -0.8816 0.8863 \n\nSS25 Caso1C_RNA_SS25 137.5484 0.2155 -1.4290 -0.9696 0.4463 \n\nSS50 Caso1C_RNA_SS50 87.9112 0.4872 -2.7439 0.9844 0.9934 \n\nSS100 Caso1C_RNA_SS100 0.9445 0.9999 -0.7504 0.8833 -0.8902 \n\n \n\nCom 25 e 50 pontos, como a superf\u00edcie \u00e9 bastante irregular, a diferen\u00e7a quanto ao tipo de \n\ndado p\u00f4de ser observado. J\u00e1 com o aumento do n\u00famero de pontos a diferen\u00e7a entre as t\u00e9cnicas \n\ndesapareceu, pois todas as irregularidades presentes no problema foram devidamente \n\nrepresentadas. Em diversas situa\u00e7\u00f5es pr\u00e1ticas, no entanto, a utiliza\u00e7\u00e3o de uma quantidade \n\nsuficientemente grande para alcan\u00e7ar tal situa\u00e7\u00e3o \u00e9 muitas vezes invi\u00e1vel.  \n\nQuando n\u00e3o se conhece o comportamento da superf\u00edcie de resposta e n\u00e3o \u00e9 vi\u00e1vel realizar \n\namostragem grande, o melhor procedimento a ser adotado \u00e9 ir adicionando amostras aos poucos \n\nat\u00e9 se atingir uma precis\u00e3o desejada atrav\u00e9s da valida\u00e7\u00e3o. Nesse sentido, o HL \u00e9 a melhor \n\nescolha, pois a t\u00e9cnica da SS, al\u00e9m de amostrar sempre o mesmo padr\u00e3o de espa\u00e7amento \n\n(limitado quanto \u00e0 diversifica\u00e7\u00e3o dos pontos quando se realizada diversas amostragens), passa a \n\nn\u00e3o ser vi\u00e1vel para problemas de dimens\u00f5es maiores. \n\n\n\n \n\n \n\n88 \n\n \n\nQuanto ao valor do m\u00ednimo apresentado, nota-se que apenas um valor foi encontrado em \n\ncada otimiza\u00e7\u00e3o, enquanto que, conforme pode ser observado na Figura 6.15 (a), a fun\u00e7\u00e3o possui \n\n4 m\u00ednimos. O motivo \u00e9 que o AG, apesar de varrer com efici\u00eancia o espa\u00e7o de busca dos \n\npar\u00e2metros, tende a convergir para apenas um valor final de m\u00ednimo. Para contornar esse \n\nproblema, os resultados de todas as gera\u00e7\u00f5es foram armazenados em um arquivo para posterior \n\nan\u00e1lise e poss\u00edvel identifica\u00e7\u00e3o de outros m\u00ednimos. Para essa an\u00e1lise foi adotado o seguinte \n\nprocedimento: \n\n1. Estabeleceu-se um valor de corte de FO e consideraram-se apenas valores menores \n\nou iguais a esse valor de corte; \n\n2. Estabeleceu-se um crit\u00e9rio de vizinhan\u00e7a baseado na discrep\u00e2ncia das vari\u00e1veis. Se \n\num m\u00ednimo que atende a condi\u00e7\u00e3o anterior for vizinho a outro m\u00ednimo, apenas um \n\ndeles \u00e9 considerado m\u00ednimo de interesse. \n\nA Figura 6.15 mostra a localiza\u00e7\u00e3o dos m\u00ednimos globais da fun\u00e7\u00e3o sobre a superf\u00edcie de \n\nresposta (a), representados por pontos azuis, e uma ilustra\u00e7\u00e3o do crit\u00e9rio de vizinhan\u00e7a adotado \n\npara identifica\u00e7\u00e3o de m\u00ednimos de interesse (b). A Figura 6.15 (b) mostra tr\u00eas pontos (preto, \n\nvermelho e azul) sendo que, atrav\u00e9s do crit\u00e9rio de vizinhan\u00e7a adotado, os pontos em preto e \n\nvermelho s\u00e3o considerados vizinhos (regi\u00e3o em vermelho) e, portanto apenas um deles (o menor) \n\n\u00e9 tomado como m\u00ednimo de interesse. \n\n \n\n(a) (b) \n\nFigura 6.15 - M\u00ednimos globais, representados pelos pontos em azul (a); e crit\u00e9rio de vizinhan\u00e7a \n\nadotado para identifica\u00e7\u00e3o de m\u00ednimos de interesse (b) \u2013 Caso 1C. \n\n\n\n \n\n \n\n89 \n\n \n\nA Tabela 6.6 mostra os resultados da avalia\u00e7\u00e3o dos m\u00ednimos de interesse para HL25, \n\nHL100 pontos e F(x), que representa os quatro m\u00ednimos globais da fun\u00e7\u00e3o.  \n\nTabela 6.6 - M\u00ednimos de interesse - Caso 1C. \n\nMetamodelo Conjunto M\u00ednimo Vari\u00e1vel x Vari\u00e1vel y \n\nF(x) \n\n1 -0.7558 0.8865  0.8813 \n\n2 -0.7554 -0.8871 -0.8830 \n\n3 -0.7513  0.8798 -0.8733 \n\n4 -0.7515  0.8798  0.8735 \n\nCaso1C_RNA_HL25 \n\n1 -0.8669 -0.7414 -0.9896 \n\n2 0.5555  0.8629 -0.4172 \n\n3 -0.5361 -0.8713 -0.6043 \n\n4 -0.9675 -0.9115 -0.6270 \n\n5  0.7603  0.7757 -0.4175 \n\n6 -0.9820 -0.5928 -0.4164 \n\nCaso1C_RNA_HL100 \n\n1 -0.7433 -0.8816  0.8863 \n\n2 -0.7236  0.8822 -0.8709 \n\n3 -0.7360  0.8822  0.8662 \n\n4 -0.7428 -0.8816 -0.8709 \n\n \n\nPelos resultados da Tabela 6.6, a otimiza\u00e7\u00e3o utilizando o metamodelo Caso1C_RNA_HL25 \n\nidentificou, pelo crit\u00e9rio adotado anteriormente, seis regi\u00f5es de m\u00ednimo, sendo que apenas uma \n\ndelas \u00e9 realmente uma regi\u00e3o de m\u00ednimo global (resultado em negrito). Em contrapartida, com o \n\nmetamodelo Caso1C_RNA_HL100 foi poss\u00edvel identificar as quatro regi\u00f5es de m\u00ednimo global da \n\nfun\u00e7\u00e3o. \n\nFoi comentado no Subitem 6.3.1 que, apesar do conjunto de HL25 pontos englobar \n\namostras nas quatro regi\u00f5es de m\u00ednimos globais, a falta de informa\u00e7\u00e3o nas regi\u00f5es vizinhas, com \n\nirregularidades, poderia gerar um padr\u00e3o de comportamento que poderia vir a prejudicar a \n\nrepresenta\u00e7\u00e3o pelo metamodelo. Analisando novamente a Figura 6.9 (a) observa-se que perto do \n\nm\u00ednimo que se situa pr\u00f3ximo \u00e0s coordenadas (-1,-1) existem ainda duas amostras vizinhas nas \n\nregi\u00f5es de m\u00ednimos locais. Isso fez com que fosse poss\u00edvel identificar esse m\u00ednimo, enquanto que \n\nos outros m\u00ednimos globais n\u00e3o foram identificados devido ao padr\u00e3o de comportamento menos \n\npreciso da regi\u00e3o pr\u00f3xima. \n\n\n\n \n\n \n\n90 \n\n \n\nO procedimento adotado de analisar posteriormente os valores obtidos com a otimiza\u00e7\u00e3o, \n\nfoi aplicado para verificar se o metamodelo foi capaz de modelar as outras regi\u00f5es de m\u00ednimo. \n\nNesse caso tamb\u00e9m foi utilizada a inspe\u00e7\u00e3o visual, por\u00e9m em casos com mais de duas vari\u00e1veis, a \n\ndecis\u00e3o de utilizar tal procedimento deve surgir a partir da valida\u00e7\u00e3o dos m\u00ednimos encontrados na \n\notimiza\u00e7\u00e3o com a resposta conhecida do problema.  \n\nAo inv\u00e9s de realizar a an\u00e1lise posterior dos resultados, uma op\u00e7\u00e3o seria estudar a utiliza\u00e7\u00e3o \n\nde um algoritmo de otimiza\u00e7\u00e3o que determine mais de uma regi\u00e3o de m\u00ednimo. Isso n\u00e3o foi \n\nrealizado nesse trabalho, pois o foco n\u00e3o foi a etapa de otimiza\u00e7\u00e3o dos par\u00e2metros (utilizada \n\napenas como parte complementar aos estudos). \n\nA partir da an\u00e1lise do resultado da otimiza\u00e7\u00e3o com o metamodelo HL25 pontos, foi \n\nconstatado que uma regi\u00e3o de m\u00ednimo foi identificada. Essa informa\u00e7\u00e3o poderia ser inclu\u00edda no \n\nprocedimento de novo treinamento proposto no Subitem 6.3.2, de forma que na nova amostragem \n\nmenos pontos fossem amostrados nessa regi\u00e3o e mais pontos seriam amostrados nas outras \n\nregi\u00f5es, de menor precis\u00e3o. \n\n6.4 Caso 1D \n\n6.4.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.16 mostra os conjuntos de treinamento (pontos em preto), amostrados sobre o \n\nespa\u00e7o de busca dos par\u00e2metros, de HL25 (a) e SS25 (b) pontos. \n\n \n\n\n\n \n\n \n\n91 \n\n \n\n  \n\n(a) (b) \n\nFigura 6.16 \u2013 Conjunto de treinamento para HL25 (a) e SS25 (b) pontos \u2013 Caso 1D. \n\nPode-se observar que, na regi\u00e3o de m\u00ednimo, enquanto que para o conjunto HL25 existem \n\napenas duas amostras na regi\u00e3o, para o conjunto SS25 existem tr\u00eas amostras mais concentradas \n\nperto do m\u00ednimo global. Assim, pode-se esperar que na regi\u00e3o de m\u00ednimo, o desempenho do \n\nmetamodelo gerado com SS25 pontos seja melhor do que o do metamodelo gerado com HL25 \n\npontos. \n\n6.4.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nAs superf\u00edcies de resposta dos metamodelos gerados com HL25 e SS25 pontos s\u00e3o \n\nmostradas na Figura 6.17. \n\n\n\n \n\n \n\n92 \n\n \n\n \n\n(a) (b) \n\nFigura 6.17 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a) e SS25 (b) pontos - \n\nCaso 1D \n\nPode-se observar que a superf\u00edcie da Figura 6.17 (b) est\u00e1 melhor do que a da Figura 6.17 \n\n(a), por\u00e9m ambas foram capazes de identificar a regi\u00e3o de interesse, representada pela cor azul \n\nacentuada.  \n\nPara mostrar como a qualidade de representa\u00e7\u00e3o melhorou com o aumento da quantidade \n\nde amostras a Figura 6.18 mostra as superf\u00edcies de resposta dos metamodelos gerados com 25 (a), \n\n50 (b) e 100 (c) pontos do HL e da fun\u00e7\u00e3o anal\u00edtica (d).   \n\n \n\n \n\n \n\n\n\n \n\n \n\n93 \n\n \n\n \n\n(a) (b) \n\n \n\n(c) (d) \n\nFigura 6.18 - Superf\u00edcies de resposta dos metamodelos gerados com HL25 (a), HL50 (b), HL100 \n\n(c) pontos e da fun\u00e7\u00e3o anal\u00edtica (d) \u2013 Caso 1D. \n\nA Figura 6.18 mostra que o aumento do n\u00famero de pontos proporciona melhor \n\nrepresenta\u00e7\u00e3o da superf\u00edcie de resposta. Uma observa\u00e7\u00e3o interessante \u00e9 que perto de regi\u00f5es de \n\nmudan\u00e7a brusca no formato da superf\u00edcie de resposta, os erros foram maiores. Isso comprova \n\nque, para situa\u00e7\u00f5es de alta irregularidade da fun\u00e7\u00e3o, a rede tende a gerar erros maiores. \n\nOs valores dos coeficientes de correla\u00e7\u00e3o linear e erro m\u00e9dio entre sa\u00edda gerada pelo \n\nmetamodelo e pela equa\u00e7\u00e3o que descreve a fun\u00e7\u00e3o para todos os metamodelos gerados s\u00e3o \n\nmostrados na Tabela 6.7. Os resultados mostram que desempenhos semelhantes foram obtidos \n\n\n\n \n\n \n\n94 \n\n \n\nentre os metamodelos gerados com HL e SS, em quest\u00e3o de representa\u00e7\u00e3o de todo o espa\u00e7o de \n\nsolu\u00e7\u00f5es. \n\n6.4.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nOs valores dos m\u00ednimos encontrados, assim como suas coordenadas no espa\u00e7o para a \n\notimiza\u00e7\u00e3o utilizando os metamodelos gerados, s\u00e3o mostrados na Tabela 6.7. \u00c9 mostrado tamb\u00e9m \n\no valor do m\u00ednimo e sua coordenada para a fun\u00e7\u00e3o anal\u00edtica F(x).  \n\nTabela 6.7 - Coeficiente de correla\u00e7\u00e3o linear, erro m\u00e9dio e m\u00ednimo determinados atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando o metamodelo - Caso 1D. \n\nTipo de \n\ndado \nNome da rede \n\nErro m\u00e9dio \n\n(%) \n\nCoeficiente \n\ncorrela\u00e7\u00e3o \nM\u00ednimo Vari\u00e1vel x Vari\u00e1vel y \n\n- F(x) - - 13.000 -4.7124 8.1258e-06 \n\nHL25 Caso1D_RNA_HL25 7.8769 0.8875 5.6007 -6.0000 0.4753 \n\nHL50 Caso1D_RNA_HL50 5.6256 0.9031 13.1296 -3.7178 -2,3725 \n\nHL100 Caso1D_RNA_HL100 2.2230 0.9808 12.5672 -3.6153 -0.2673 \n\nSS25 Caso1D_RNA_SS25 6.2709 0.8781 12.5224 -4.4154 0.1136 \n\nSS50 Caso1D_RNA_SS50 5.2949 0.9256 11.9259 -3.9644 0.1949 \n\nSS100 Caso1D_RNA_SS100 2.8880 0.9633 12.7835 -3.5991 -0.3232 \n\n \n\nObserva-se que, apesar de todos os metamodelos encontrarem a regi\u00e3o de m\u00ednimo, a \n\ndetermina\u00e7\u00e3o mais precisa da localiza\u00e7\u00e3o n\u00e3o foi poss\u00edvel. O fato da superf\u00edcie de resposta \n\npossuir mudan\u00e7as repentinas em seu formato acabou criando dificuldades para a rede definir \n\npadr\u00f5es que representem a fun\u00e7\u00e3o nessa regi\u00e3o com precis\u00e3o.  \n\nApesar de n\u00e3o ter encontrado a localiza\u00e7\u00e3o precisa da coordenada do m\u00ednimo, a regi\u00e3o foi \n\nencontrada facilmente com poucos pontos, de maneira que um retreinamento posterior seria \n\ncapaz de encontrar a localiza\u00e7\u00e3o do m\u00ednimo com precis\u00e3o.  \n\nA Tabela 6.7 mostra que o m\u00ednimo encontrado pela otimiza\u00e7\u00e3o utilizando o metamodelo \n\nCaso1D_RNA_SS25, assim como suas coordenadas, foi mais preciso comparativamente \u00e0 \n\notimiza\u00e7\u00e3o utilizando o metamodelo Caso1D_RNA_HL25, complementando a observa\u00e7\u00e3o \n\nrealizada no Subitem 6.4.1. Apesar dessa diferen\u00e7a na precis\u00e3o com rela\u00e7\u00e3o ao m\u00ednimo global, no \n\naspecto geral, a qualidade da representa\u00e7\u00e3o da superf\u00edcie de resposta proporcionada pelos dois \n\n\n\n \n\n \n\n95 \n\n \n\nmetamodelos foi semelhante, uma vez que os valores dos coeficientes de correla\u00e7\u00e3o linear e do \n\nerro m\u00e9dio foram parecidos. \n\nEsse resultado mostra que, apesar de globalmente as duas t\u00e9cnicas de amostragem \n\nproporcionarem desempenhos semelhantes aos metamodelos, quando analisadas em regi\u00f5es \n\nespec\u00edficas, elas possuem suas particularidades, podendo uma destacar-se em rela\u00e7\u00e3o \u00e0 outra. \n\nA Tabela 6.7 mostra que, apesar de ter resultado em menor correla\u00e7\u00e3o linear, o metamodelo \n\ngerado com SS25 pontos foi melhor que todos os demais para determinar o m\u00ednimo. Para avaliar \n\nmelhor esse resultado, na Figura 6.19 s\u00e3o expostos os conjuntos de treinamento de HL100 (a) e \n\nSS100 (b) pontos. \n\n \n\n(a) (b) \n\nFigura 6.19 \u2013 Conjunto de treinamento de HL100 (a) e SS100 (b) pontos \u2013 Caso 1D. \n\nComparando os conjuntos SS25 (Figura 6.16 (b)) e HL100 observa-se que em rela\u00e7\u00e3o ao \n\nm\u00ednimo, a amostragem proporcionada pelo conjunto SS25 foi melhor, por\u00e9m para a modelagem \n\nde toda a superf\u00edcie o conjunto HL100 foi melhor, refletindo os valores de correla\u00e7\u00e3o e do \n\nm\u00ednimo encontrado. Por\u00e9m, com rela\u00e7\u00e3o \u00e0 amostragem do conjunto SS100, a Figura 6.19 (b) \n\nmostra que mesmo na regi\u00e3o de m\u00ednimo sua amostragem foi melhor do que a de SS25 pontos. O \n\nmotivo do resultado inferior com rela\u00e7\u00e3o ao m\u00ednimo \u00e9 que, provavelmente, o treinamento n\u00e3o \n\ntenha ocorrido bem para o metamodelo SS100 escolhido. Um novo treinamento deve resultar em \n\num metamodelo com melhor qualidade.  \n\n\n\n \n\n \n\n96 \n\n \n\nEsse resultado mostra que o treinamento da RNA deve ser realizado diversas vezes para \n\ngarantir a gera\u00e7\u00e3o de um metamodelo de qualidade. Adicionalmente, um crit\u00e9rio de escolha bem \n\ndefinido deve ser utilizado para obter um metamodelo que proporcione o melhor desempenho \n\nposs\u00edvel para o problema. \n\n6.5 Coment\u00e1rios dos casos anal\u00edticos  \n\nOs casos anal\u00edticos estudados possibilitaram a identifica\u00e7\u00e3o de particularidades e, \n\nprincipalmente, limita\u00e7\u00f5es das RNA que devem ser consideradas em sua aplica\u00e7\u00e3o. \n\nCaso 1A \n\nA primeira caracter\u00edstica importante das RNA observada foi que, por mais simples que seja \n\na superf\u00edcie de resposta a qual se deseja modelar, existe um erro envolvido na representa\u00e7\u00e3o \n\nproporcionada pelo metamodelo gerado (comentado no Subitem 6.1.3), a n\u00e3o ser que se tenha um \n\nn\u00famero suficiente de amostras convenientemente espa\u00e7adas (tal que cubram de forma eficiente \n\ntodo o espa\u00e7o de busca dos par\u00e2metros), o que dificilmente ser\u00e1 poss\u00edvel. Portanto, pode-se \n\nesperar que em regi\u00f5es que n\u00e3o possuem informa\u00e7\u00f5es sobre o comportamento da superf\u00edcie de \n\nresposta, a RNA provavelmente proporcione uma modelagem menos confi\u00e1vel. Isso faz com que \n\na otimiza\u00e7\u00e3o utilizando o metamodelo n\u00e3o seja capaz de determinar a coordenada precisa do \n\nm\u00ednimo de interesse, por\u00e9m, n\u00e3o significa que a regi\u00e3o de m\u00ednimo n\u00e3o seja identificada. Para \n\nsaber se o metamodelo foi capaz de modelar a superf\u00edcie de resposta tal que a regi\u00e3o de interesse \n\nseja identificada, deve-se realizar a valida\u00e7\u00e3o do resultado da otimiza\u00e7\u00e3o com a resposta \n\nconhecida do problema.  \n\nPor outro lado, caso a regi\u00e3o a ser modelada seja comportada, a RNA \u00e9 capaz de modelar o \n\nseu padr\u00e3o de comportamento com poucas amostras sem maiores dificuldades. \n\nCaso 1B \n\nPelo Caso 1B p\u00f4de-se comprovar que a presen\u00e7a de amostras na regi\u00e3o de interesse \n\ncontribuiu para que o metamodelo fosse capaz de modelar com maior precis\u00e3o essa regi\u00e3o, \n\nconforme comentado no Subitem 6.2.2. \n\n\n\n \n\n \n\n97 \n\n \n\nA falta de informa\u00e7\u00e3o em determinadas regi\u00f5es faz com que a RNA atenue ou acentue a \n\ncurvatura da superf\u00edcie, dependendo do padr\u00e3o de curva que a superf\u00edcie possuir ao seu redor, \n\npodendo levar a solu\u00e7\u00e3o a regi\u00f5es de m\u00ednimo equivocadas (Subitem 6.2.2). \n\nPara esse caso a determina\u00e7\u00e3o de uma nova regi\u00e3o de treinamento foi realizada atrav\u00e9s da \n\ninspe\u00e7\u00e3o visual (Subitem 6.2.4), por\u00e9m conforme j\u00e1 comentado nos resultados, para casos de \n\nmais de duas vari\u00e1veis o procedimento a ser adotado \u00e9 o proposto na metodologia.  \n\nCaso 1C \n\nPara casos com mais de um m\u00ednimo global \u00e9 necess\u00e1rio utilizar uma ferramenta de \n\notimiza\u00e7\u00e3o que seja capaz de fornecer m\u00ednimos situados em regi\u00f5es diferentes como resposta. \n\nOutra op\u00e7\u00e3o \u00e9 realizar o procedimento proposto no Caso 1C, que consiste em analisar os \n\nresultados da otimiza\u00e7\u00e3o posteriormente, para verificar se o metamodelo encontrou regi\u00f5es \n\npotencialmente de m\u00ednimos (Subitem 6.3.3). \n\nNo caso de duas vari\u00e1veis foram utilizados mais pontos para treinar as RNA (utiliza\u00e7\u00e3o de \n\n100 pontos). Por\u00e9m, em casos complexos, e, portanto, com mais vari\u00e1veis, um procedimento \n\nadequado consistiria em iniciar o treinamento com poucos pontos, em seguida validar os \n\nresultados com a resposta conhecida do problema e, caso houvesse necessidade, mais pontos \n\nseriam adicionados (novamente para todo o espa\u00e7o de busca dos par\u00e2metros) e as RNA seriam \n\ntreinadas novamente. Esse processo iterativo de treinamento e valida\u00e7\u00e3o de RNA prosseguiria at\u00e9 \n\nque uma qualidade satisfat\u00f3ria de representa\u00e7\u00e3o dos metamodelos fosse alcan\u00e7ada. \n\nCom isso possivelmente pode-se encontrar uma quantidade reduzida de pontos capaz de \n\nmodelar o padr\u00e3o desejado do problema. No Caso 1C, por exemplo, poderia se chegar a um \n\nmetamodelo bom com 75 pontos ou at\u00e9 com 50 pontos atrav\u00e9s desse procedimento. \n\nCaso 1D \n\nCom o Caso 1D, que mostra uma superf\u00edcie modelada por mais de uma fun\u00e7\u00e3o, p\u00f4de-se \n\nobservar que nas regi\u00f5es de mudan\u00e7a brusca de comportamento a RNA gera maior erro. \n\nConforme comentado no Subitem 6.4.3, ao se analisarem os resultados em algumas regi\u00f5es \n\nespec\u00edficas, a diferen\u00e7a em rela\u00e7\u00e3o \u00e0 t\u00e9cnica utilizada para amostrar os dados p\u00f4de ser \n\n\n\n \n\n \n\n98 \n\n \n\nevidenciada, ao realizar a compara\u00e7\u00e3o entre os metamodelos Caso1D_RNA_HL25 e \n\nCaso1D_RNA_SS25. \n\nDessa maneira, mesmo um metamodelo com menos pontos pode proporcionar melhores \n\nresultados, quando regi\u00f5es espec\u00edficas s\u00e3o analisadas.  \n\nOs resultados do Caso 1D ainda mostraram que a realiza\u00e7\u00e3o de v\u00e1rios treinamentos e a \n\nutiliza\u00e7\u00e3o de um crit\u00e9rio bem definido para escolher o metamodelo treinado \u00e9 importante para \n\nobter aquele que proporcione melhor modelagem para o problema. Pelos resultados apresentados \n\nno Subitem 6.4.3, p\u00f4de-se observar que apesar de o metamodelo SS100 resultar em melhor \n\ncoeficiente de correla\u00e7\u00e3o linear comparativamente ao metamodelo SS25, as coordenadas do \n\nm\u00ednimo encontradas pelo metamodelo SS25 foram melhores. Isso mostra que a defini\u00e7\u00e3o de um \n\ncrit\u00e9rio espec\u00edfico, que realize a compara\u00e7\u00e3o na regi\u00e3o de interesse \u00e9 importante. \n\nComent\u00e1rios gerais \n\nUma maneira de melhorar a qualidade da modelagem \u00e9 atrav\u00e9s de um novo treinamento, \n\nem uma regi\u00e3o menor. Por\u00e9m, a realiza\u00e7\u00e3o desse tipo de procedimento nem sempre \u00e9 vantajosa, \n\nde forma que s\u00f3 agregar\u00e1 valor caso a regi\u00e3o identificada para realizar o novo treinamento seja \n\nrealmente uma regi\u00e3o de interesse (que cont\u00e9m o m\u00ednimo). No entanto, identificar se o m\u00ednimo \n\nencontrado \u00e9 um m\u00ednimo local ou global n\u00e3o \u00e9 f\u00e1cil em situa\u00e7\u00f5es em que exista muita \n\nirregularidade na superf\u00edcie de resposta e pouca informa\u00e7\u00e3o do problema. \n\nAtrav\u00e9s dos resultados dos casos anal\u00edticos, p\u00f4de-se identificar e propor mais um tipo de \n\nretreinamento. Esse procedimento deve proporcionar bons resultados para casos complexos.  \n\nAs t\u00e9cnicas de amostragem, em geral, proporcionaram metamodelos de qualidade \n\nsemelhante quando analisadas globalmente, ou seja, o resultado para toda a superf\u00edcie de \n\nresposta, conforme p\u00f4de ser observado pelos coeficientes de correla\u00e7\u00e3o linear e pelos erros para \n\ntodos os casos. Por\u00e9m, quando analisada em regi\u00f5es espec\u00edficas, a diferen\u00e7a entre o tipo de \n\namostragem apareceu. Dessa maneira, um estudo para defini\u00e7\u00e3o de crit\u00e9rios de compara\u00e7\u00e3o em \n\nregi\u00f5es espec\u00edficas deve agregar valor aos resultados. \n\nNos casos anal\u00edticos, v\u00e1rias conclus\u00f5es puderam ser tiradas a partir da an\u00e1lise visual da \n\nsuperf\u00edcie de resposta (espa\u00e7o de solu\u00e7\u00f5es). Por\u00e9m, o procedimento de valida\u00e7\u00e3o com a resposta \n\n\n\n \n\n \n\n99 \n\n \n\nconhecida do problema permite realizar avalia\u00e7\u00f5es importantes, como determinar se o m\u00ednimo \n\nencontrado est\u00e1 na regi\u00e3o de interesse, se o caso possui diversas regi\u00f5es de interesse, se \u00e9 \n\nnecess\u00e1rio realizar novo treinamento, se esse retreinamento deve ser refinado ou global etc. de \n\nforma que qualquer decis\u00e3o ou avalia\u00e7\u00e3o mais precisa dos resultados seja realizada atrav\u00e9s da \n\nvalida\u00e7\u00e3o. \n\nAssim, pode-se concluir que atrav\u00e9s da an\u00e1lise visual do espa\u00e7o de solu\u00e7\u00f5es foram \n\nidentificadas caracter\u00edsticas e particularidades das RNA, de forma que novos procedimentos e \n\nnecessidades de estudo puderam ser identificados e propostos. O conhecimento proporcionado \n\npelos estudos dos casos anal\u00edticos contribuiu para formar uma base te\u00f3rica para aplica\u00e7\u00e3o da \n\nferramenta nos casos de maior complexidade. \n\n6.6 Caso 2A \n\n6.6.1 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.20 mostra a rela\u00e7\u00e3o existente (amostragem no espa\u00e7o) entre dois dos quatro \n\natributos incertos (porosidade da f\u00e1cies 2 e transmissibilidade da falha 3), do conjunto de \n\ntreinamento HL25 (a) e HL50 (b) pontos. \n\n \n\n \n\n\n\n \n\n \n\n100 \n\n \n\n \n\n      (a)   (b) \n\nFigura 6.20 \u2013 Pontos de treinamento, relativos aos atributos porosidade da f\u00e1cies 2 e \n\ntransmissibilidade da falha 3 para os conjuntos HL25 (a) e HL50 (b) pontos - Caso 2A. \n\nObserva-se que para o conjunto de HL50 existem mais pontos concentrados bem pr\u00f3ximos. \n\nEsse fator pode resultar em melhor representa\u00e7\u00e3o do espa\u00e7o de solu\u00e7\u00f5es nessas \u00e1reas, quando \n\ncomparado com o metamodelo gerado com o conjunto HL25, que possui apenas pontos \n\nindividualmente dispersos no espa\u00e7o e algumas regi\u00f5es sem amostragem. Em contrapartida \n\nexistem tamb\u00e9m espa\u00e7os vazios para o conjunto de HL50, onde h\u00e1 falta de informa\u00e7\u00f5es. O \n\nmotivo disso vem do fato de que a t\u00e9cnica do hipercubo latino, apesar de amostrar valores bem \n\ndistribu\u00eddos para cada atributo, realiza a combina\u00e7\u00e3o entre eles de forma aleat\u00f3ria, o que acaba \n\ngerando regi\u00f5es com mais e com menos informa\u00e7\u00f5es. \n\nPara avaliar os valores de afastamento com rela\u00e7\u00e3o ao hist\u00f3rico gerado, a Tabela 6.8 mostra \n\na frequ\u00eancia de ocorr\u00eancia dos valores de sa\u00eddas desejadas, geradas pela simula\u00e7\u00e3o dos conjuntos \n\nde treinamento amostrados (afastamento da produ\u00e7\u00e3o de \u00e1gua do campo, do modelo simulado \n\ncom rela\u00e7\u00e3o ao hist\u00f3rico). \n\n \n\n \n\n \n\n \n\n0.85 0.9 0.95 1 1.05 1.1\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nPor2\n\nT\n3\n\n0.85 0.9 0.95 1 1.05 1.1\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nPor2\n\nT\n3\n\n\n\n \n\n \n\n101 \n\n \n\nTabela 6.8 \u2013 Frequ\u00eancia de ocorr\u00eancia dos valores de sa\u00edda para os conjuntos de \n\ntreinamento, HL25, HL50 e HL100 pontos \u2013 Caso 2A. \n\nHL25 HL50 HL100 \n\nFrequ\u00eancia Faixa (x10\n8\n) Frequ\u00eancia Faixa (x10\n\n8\n) Frequ\u00eancia Faixa (x10\n\n8\n) \n\n16 0.0125 - 1.35 33 0.00585 - 1.25 59 0.00224 - 1.19 \n\n4 1.35 - 2.69 4 1.25 - 2.50 17 1.19 - 2.37 \n\n3 2.69 - 4.03 3 2.50 - 3.75 3 2.37 - 3.55 \n\n0 4.03 - 5.37 5 3.75 - 5.00 6 2.55 - 4.74 \n\n1 5.37 - 6.71 1 5.99 - 6.25 4 4.74 - 5.92 \n\n1 6.71 - 8.05 3 6.25 - 7.50 3 5.92 - 7.10 \n\n    0 7.50 - 8.75 5 7.10 - 8.29 \n\n    1 8.75 \u2013 10 2 8.29 - 9.47 \n\n        0 9.47 \u2013 10.7 \n\n  \n\n \n\n    1 10.7 \u2013 11.8 \n\nM\u00ednimo 0.0125   0.00585   0.00224 \n\nM\u00e1ximo 8.05   10   11.8 \n\n \n\nAs faixas mostradas na Tabela 6.8 indicam que uma determinada quantidade de amostras \n\n(coluna \u201cFrequ\u00eancia\u201d), possui valor de afastamento situado entre um limite m\u00ednimo e m\u00e1ximo, \n\nespecificados (coluna \u201cFaixa\u201d). Por exemplo, para o conjunto HL25 pontos, dezesseis amostras \n\ngeraram afastamentos que se situam em uma faixa entre 1.25x10\n6\n e 1.35x10\n\n8\n. Assim, quanto \n\nmenor for o valor de afastamento gerado pela amostra, significa que mais perto do m\u00ednimo de \n\ninteresse ela se situa e, quanto mais amostras de valores menores existirem dentro do conjunto de \n\ntreinamento, melhor a qualidade do conjunto de treinamento.  \n\nA Figura 6.21 mostra os histogramas com a frequ\u00eancia de ocorr\u00eancia dos valores de sa\u00edda \n\nda Tabela 6.8, referente aos conjuntos de treinamento HL25 e HL50 pontos. \n\n\n\n \n\n \n\n102 \n\n \n\n   \n\n(a) (b) \n\nFigura 6.21 \u2013 Histograma dos valores de sa\u00edda para os conjuntos de treinamento HL25 e HL50 \n\npontos \u2013 Caso 2A. \n\nA avalia\u00e7\u00e3o dos histogramas da Figura 6.21 mostra a superioridade do conjunto de HL50 \n\npontos comparativamente ao conjunto de HL25 pontos, com rela\u00e7\u00e3o a fornecer informa\u00e7\u00f5es mais \n\npr\u00f3ximas \u00e0 regi\u00e3o de interesse (de m\u00ednimo). Al\u00e9m de amostrar um valor m\u00ednimo mais pr\u00f3ximo \n\ndo m\u00ednimo de interesse, o conjunto HL50 pontos tamb\u00e9m concentra maior quantidade de pontos \n\nem regi\u00f5es mais pr\u00f3ximas do m\u00ednimo. \n\n6.6.2 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nA Figura 6.22 mostra os gr\u00e1ficos de dispers\u00e3o (crossplot) entre os afastamentos da \n\nprodu\u00e7\u00e3o de \u00e1gua obtidos com o simulador e com o metamodelo, com rela\u00e7\u00e3o \u00e0s amostragens \n\nrealizadas com HL25 (a), HL50 (b) e HL100 (c) pontos. \n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n0.0125 1.62 3.23 4.83 6.44 Mais\n\nF\nr\ne\nq\n\n\u00fc\n\u00ea\nn\n\nc\nia\n\nBloco\n\nHL25 (x108)\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0.00585 1.43 2.86 4.29 5.71 7.14 8.57 Mais\n\nF\nr\ne\nq\n\n\u00fc\n\u00ea\nn\n\nc\nia\n\nBloco\n\nHL50 (x108)\n\n\n\n \n\n \n\n103 \n\n \n\n \n\n(a) (b) \n\n \n\n(c) \n\nFigura 6.22 \u2013 Gr\u00e1fico de dispers\u00e3o (crossplot) entre sa\u00edda do simulador e sa\u00edda do metamodelo, \n\ngerado com HL25 (a), HL50 (b) e HL100 (c) \u2013 Caso 2A; pontos em azul: pontos de treinamento e \n\npontos em vermelho: pontos de teste. \n\nA partir dos gr\u00e1ficos da Figura 6.22 fica claro que com 100 pontos a rede obteve melhor \n\ncorrela\u00e7\u00e3o dos dados. Em contrapartida, a diferen\u00e7a entre os gr\u00e1ficos obtidos com os \n\nmetamodelos gerados com 25 e 50 pontos n\u00e3o ficou evidente. \n\nA Tabela 6.9 mostra os valores obtidos para os coeficientes de correla\u00e7\u00e3o linear entre sa\u00edda \n\ndo simulador e do metamodelo, relativos aos conjuntos de treino e teste. \n\n2 4 6 8\n\nx 10\n8\n\n2\n\n4\n\n6\n\n8\n\nx 10\n8\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\nd\ne\nlo\n\n (\nY\n\n)\nCorrela\u00e7\u00e3o linear - HL25\n\n \n\n \n\nY=T\n\nDados de treino - HL 25\n\nDados de teste - HL 25\n\n2 4 6 8\n\nx 10\n8\n\n2\n\n4\n\n6\n\n8\n\nx 10\n8\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\nd\ne\nlo\n\n (\nY\n\n)\n\nCorrela\u00e7\u00e3o linear - HL50\n\n \n\n \n\nY=T\n\nDados de treino - HL 50\n\nDados de teste - HL 25\n\n2 4 6 8 10\n\nx 10\n8\n\n2\n\n4\n\n6\n\n8\n\n10\n\nx 10\n8\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\nd\ne\nlo\n\n (\nY\n\n)\n\nCorrela\u00e7\u00e3o linear - HL100\n\n \n\n \n\nY=T\n\nDados de treino - HL 100\n\nDados de teste - HL 25\n\n\n\n \n\n \n\n104 \n\n \n\nTabela 6.9 \u2013 Correla\u00e7\u00e3o linear entre sa\u00edda do simulador e do metamodelo (afastamento da \n\nprodu\u00e7\u00e3o de \u00e1gua, do modelo de simula\u00e7\u00e3o com rela\u00e7\u00e3o ao hist\u00f3rico), relativos aos conjuntos de \n\ntreino e teste \u2013 Caso 2A. \n\nMetamodelo Tipo Correla\u00e7\u00e3o linear \n\nCaso2A_HL25 \nTreino 0.9880 \n\nTeste 0.9743 \n\nCaso2A_HL50 \nTreino 0.9794 \n\nTeste 0.9825 \n\nCaso2A_HL100 \nTreino 0.9948 \n\nTeste 0.9972 \n\n \n\nA avalia\u00e7\u00e3o apenas dos valores de coeficiente de correla\u00e7\u00e3o linear, mostrados na Tabela \n\n6.9, n\u00e3o possibilita definir qual metamodelo foi melhor, pois os valores s\u00e3o bem pr\u00f3ximos. \n\nApesar disso, os valores para teste sugerem que o aumento da quantidade de amostras contribuiu \n\npara melhorar a qualidade do metamodelo.  \n\n6.6.3 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nOs resultados da otimiza\u00e7\u00e3o utilizando os metamodelos gerados s\u00e3o apresentados na Tabela \n\n6.10, que mostra os afastamentos do m\u00ednimo determinado pela otimiza\u00e7\u00e3o, com rela\u00e7\u00e3o ao \n\nhist\u00f3rico, quando simulado com o metamodelo (coluna \u201cMetamodelo\u201d) e com o simulador \n\n(coluna \u201cSimulador\u201d).  \n\nTabela 6.10 \u2013 Afastamento obtido com o m\u00ednimo encontrado na otimiza\u00e7\u00e3o, quando \n\nsimulado com o metamodelo e com o simulador \u2013 Caso 2A. \n\nMetamodelo Metamodelo (x10\n6\n) Simulador (x10\n\n6\n) \n\nCaso2A_HL25 7.3398 209.6290 \n\nCaso2A_HL50 24.5380 4.8652 \n\nCaso2A_HL100 11.5150 0.2701 \n\n \n\nObserva-se que para o metamodelo Caso2A_HL25, a simula\u00e7\u00e3o do m\u00ednimo com o \n\nmetamodelo gerou um valor baixo de afastamento, enquanto que a simula\u00e7\u00e3o com o simulador \n\nresultou em valor de afastamento elevado. Isso indica que a otimiza\u00e7\u00e3o utilizando esse \n\nmetamodelo n\u00e3o identificou de fato a regi\u00e3o de m\u00ednimo, ou seja, levou a solu\u00e7\u00e3o para uma regi\u00e3o \n\n\n\n \n\n \n\n105 \n\n \n\nde afastamento elevado. Portanto, pode-se concluir que esse metamodelo n\u00e3o foi capaz de \n\nrepresentar o simulador com precis\u00e3o.  \n\nJ\u00e1 em rela\u00e7\u00e3o aos metamodelos Caso2A_HL50 e Caso2A_HL100, observa-se que os \n\nvalores obtidos com o metamodelo e o simulador n\u00e3o foram t\u00e3o discrepantes, provando que \n\nproporcionaram bons resultados. Vale ressaltar que essa diferen\u00e7a existente dificilmente ser\u00e1 \n\neliminada, pois o metamodelo fornece apenas uma estimativa da resposta, na qual deve ser \n\nconfirmada posteriormente atrav\u00e9s da simula\u00e7\u00e3o. \n\nA Figura 6.23 mostra as curvas de produ\u00e7\u00e3o de \u00e1gua do campo dos modelos encontrados \n\ncom a otimiza\u00e7\u00e3o utilizando os metamodelos.  \n\n \n\nFigura 6.23 \u2013 Curva de produ\u00e7\u00e3o de \u00e1gua do campo do Caso base, hist\u00f3rico e metamodelos \n\ngerados com HL25, HL50 e HL100 pontos \u2013 Caso 2A. \n\nCom rela\u00e7\u00e3o \u00e0s curvas apresentadas na Figura 6.23, a do metamodelo Caso2A_HL25 ficou \n\ndistante do hist\u00f3rico, chegando at\u00e9 a piorar o ajuste em compara\u00e7\u00e3o com o Caso base, enquanto \n\nque as curvas dos metamodelos Caso2A_HL50 e Caso2A_HL100 melhoraram o Caso base e \n\najustaram bem a curva de produ\u00e7\u00e3o de \u00e1gua do campo em rela\u00e7\u00e3o ao hist\u00f3rico. Foi tra\u00e7ada a \n\ncurva e produ\u00e7\u00e3o de \u00e1gua obtida atrav\u00e9s do metamodelo Caso2A_HL25, por\u00e9m, na pr\u00e1tica essa \n\nsolu\u00e7\u00e3o seria descartada, pois piorou o Caso base. Nesse trabalho o resultado foi considerado \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\n\u00e7\n\u00e3\no\n\n d\ne\n \u00e1\n\ng\nu\n\na\n d\n\no\n c\n\na\nm\n\np\no\n\n (\nm\n\n3\n/d\n\nia\n)\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nCaso2A-HL25\n\nCaso2A-HL50\n\nCaso2A-HL100\n\n\n\n \n\n \n\n106 \n\n \n\npara mostrar que, com menos pontos, a probabilidade de obter resultados equivocados \u00e9 maior. \n\nEm outras palavras, a probabilidade de amostrar a regi\u00e3o de m\u00ednimo \u00e9 menor. \n\nO fato de a correla\u00e7\u00e3o linear para o metamodelo Caso2A_HL25 ter resultado em valores \n\nelevados indica que o espa\u00e7o de solu\u00e7\u00f5es do problema possui irregularidades e que os 25 pontos \n\nn\u00e3o foram capazes de fornecer essa informa\u00e7\u00e3o, de forma que, mesmo que nos pontos amostrados \n\na correla\u00e7\u00e3o esteja boa, n\u00e3o significa que nas demais regi\u00f5es a correla\u00e7\u00e3o tamb\u00e9m esteja. O \n\nmesmo se pode dizer a respeito dos dados de teste que indicaram boa correla\u00e7\u00e3o. Se os dados de \n\nteste englobassem alguma regi\u00e3o com irregularidade ou a regi\u00e3o de interesse, o resultado da \n\ncorrela\u00e7\u00e3o para o metamodelo HL25 provavelmente teria sido ruim. Assim, o ideal para os dados \n\nde teste \u00e9 que eles indiquem a qualidade do metamodelo na regi\u00e3o de interesse. A melhor maneira \n\nde obter uma avalia\u00e7\u00e3o precisa dos resultados, no entanto, \u00e9 atrav\u00e9s da valida\u00e7\u00e3o da curva de \n\nprodu\u00e7\u00e3o do modelo com o hist\u00f3rico. \n\nUm procedimento que poderia melhorar essa etapa de an\u00e1lise seria calcular o coeficiente de \n\ncorrela\u00e7\u00e3o linear em uma regi\u00e3o mais refinada, em que os valores de afastamento fossem abaixo \n\nde um determinado valor de corte. Assim, ter-se-ia uma avalia\u00e7\u00e3o para uma regi\u00e3o maior e uma \n\navalia\u00e7\u00e3o em uma regi\u00e3o menor, possivelmente pr\u00f3xima do m\u00ednimo de interesse.  \n\n6.6.4 Passo 4: Retreinamento \n\nO processo de retreinamento realizado nesse trabalho, conforme consta no Cap\u00edtulo 4 de \n\nmetodologia, consistiu em realizar uma nova amostragem e treinamento das RNA em uma regi\u00e3o \n\nmenor, ao redor do m\u00ednimo encontrado atrav\u00e9s da otimiza\u00e7\u00e3o utilizando o metamodelo. O ideal \n\nseria realizar o retreinamento com o metamodelo que utilizou menos pontos, no caso o \n\nCaso2A_HL25. Por\u00e9m, conforme p\u00f4de ser observado pelo gr\u00e1fico da Figura 6.23, o m\u00ednimo \n\nencontrado com esse metamodelo gerou uma curva de produ\u00e7\u00e3o distante do m\u00ednimo desejado \n\npara esse caso. Por esse motivo, a realiza\u00e7\u00e3o de um novo treinamento, utilizando como base o \n\nm\u00ednimo encontrado com esse metamodelo, n\u00e3o iria melhorar os resultados, pois o retreinamento \n\nocorreria em uma regi\u00e3o de m\u00ednimo errada.  \n\n\n\n \n\n \n\n107 \n\n \n\nApesar disso, para comprovar essa hip\u00f3tese foi realizado o retreinamento utilizando os \n\nresultados desse metamodelo. O gr\u00e1fico de produ\u00e7\u00e3o gerado, conforme se esperava, continuou \n\ndistante do hist\u00f3rico, apesar de melhorar um pouco com rela\u00e7\u00e3o ao treinamento. O retreinamento \n\nfoi, ent\u00e3o, realizado a partir do m\u00ednimo encontrado com a otimiza\u00e7\u00e3o utilizando o metamodelo \n\ngerado com HL50 pontos. \n\nPara a defini\u00e7\u00e3o dos novos limites de treinamento (conforme Subitem 4.2.2 do cap\u00edtulo de \n\nmetodologia) foi adotado um valor de corte de 40% de FO (modelos que resultam em FO menor \n\nque 40% do valor m\u00e1ximo alcan\u00e7ado na otimiza\u00e7\u00e3o, ou seja, que proporcionaram mais de 60% de \n\nredu\u00e7\u00e3o em rela\u00e7\u00e3o ao valor m\u00e1ximo atingido com a otimiza\u00e7\u00e3o), e chegou-se aos limites \n\nmostrados na Tabela 6.11. \n\nTabela 6.11 \u2013 Defini\u00e7\u00e3o dos atributos para novo treinamento \u2013 Caso 2A. \n\nLimite Por2 Kx2 Falha3 Kr2 \n\nInferior 0.9727 0.7500 0.4512 2.6180 \n\nSuperior 1.1492 0.9402 1.0000 3.6223 \n\n \n\nPara dados de teste foram utilizados 5 dos 25 pontos amostrados, sendo os outros 20 pontos \n\nutilizados para valida\u00e7\u00e3o e treino (5 pontos para valida\u00e7\u00e3o e 15 pontos para treino).  \n\nA Tabela 6.12 a seguir mostra o coeficiente de correla\u00e7\u00e3o linear (treino e teste) entre sa\u00edda \n\ndo simulador e do metamodelo e o valor do m\u00ednimo encontrado com a otimiza\u00e7\u00e3o, referente ao \n\nretreinamento. \n\nTabela 6.12 \u2013 Coeficientes de correla\u00e7\u00e3o linear do retreinamento; m\u00ednimo obtido com a \n\notimiza\u00e7\u00e3o e simula\u00e7\u00e3o do m\u00ednimo com o simulador \u2013 Caso 2A. \n\nMetamodelo \n\nTreinamento Otimiza\u00e7\u00e3o (x10\n6\n) \n\nCorrela\u00e7\u00e3o linear \nMetamodelo Simula\u00e7\u00e3o \n\nTreino Teste \n\nCaso2A_HL50_novo 0.9906 0.9536 -0.6984 0.2644 \n\n \n\nObserva-se que o valor de afastamento obtido com a simula\u00e7\u00e3o do modelo resultante da \n\notimiza\u00e7\u00e3o foi pr\u00f3ximo ao adquirido com o metamodelo gerado com 100 pontos \n\n(Caso2A_HL100), mostrado na Tabela 6.10 (0.2701). \n\n\n\n \n\n \n\n108 \n\n \n\nO valor negativo que aparece para o m\u00ednimo encontrado com a otimiza\u00e7\u00e3o foi devido \u00e0 \n\nutiliza\u00e7\u00e3o da fun\u00e7\u00e3o linear como fun\u00e7\u00e3o de transfer\u00eancia para treinamento da RNA. Apesar disso, \n\ncomo os valores dos atributos do reservat\u00f3rio respeitam os limites impostos, a simula\u00e7\u00e3o do \n\nmodelo resultante da otimiza\u00e7\u00e3o com o simulador gerou valores plaus\u00edveis, conforme mostra o \n\nresultado da simula\u00e7\u00e3o da Tabela 6.12. \n\nA Figura 6.24 mostra as curvas de produ\u00e7\u00e3o de \u00e1gua do campo para o retreinamento. \n\n \n\nFigura 6.24 - Curva de produ\u00e7\u00e3o de \u00e1gua do campo do Caso base, hist\u00f3rico e metamodelos \n\ngerados com HL50, HL100 pontos e HL50 ap\u00f3s retreinamento \u2013 Caso 2A.  \n\nObserva-se que a curva de produ\u00e7\u00e3o de \u00e1gua obtida com o retreinamento assemelhou-se \u00e0 \n\ncurva resultante da otimiza\u00e7\u00e3o utilizando o metamodelo gerado com HL100 pontos. \n\nNesse caso, a realiza\u00e7\u00e3o de um novo treinamento agregou valor aos resultados uma vez que \n\ncom um total de 75 simula\u00e7\u00f5es (50 para primeiro treinamento e mais 25 para retreinamento) foi \n\nposs\u00edvel obter uma curva de produ\u00e7\u00e3o igual \u00e0 obtida com 100 simula\u00e7\u00f5es. Isso foi poss\u00edvel \n\nporque o conjunto de treinamento inclu\u00eda informa\u00e7\u00f5es suficientes a respeito da regi\u00e3o de interesse \n\n(de m\u00ednimo).  \n\nCom o intuito de entender melhor a influ\u00eancia dos dados de entrada na qualidade do \n\nmetamodelo gerado, foi realizada uma an\u00e1lise comparativa entre os dados HL50, HL100 e HL25 \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n d\n\no\n c\n\na\nm\n\np\no\n (\n\nm\n3\n/d\n\nia\n)\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nCaso2A-HL50\n\nCaso2A-HL100\n\nCaso2A-HL50ret\n\n\n\n \n\n \n\n109 \n\n \n\n(este \u00faltimo utilizado no retreinamento). Para cada atributo, foi calculada a diferen\u00e7a absoluta e \n\nnormalizada entre o valor do atributo do modelo amostrado e o valor real conhecido pelo modelo \n\nde refer\u00eancia. A soma dessas diferen\u00e7as para os quatro atributos que comp\u00f5em o modelo de \n\nsimula\u00e7\u00e3o foi considerada como sendo a dist\u00e2ncia m\u00e9dia da amostra em rela\u00e7\u00e3o ao valor \n\nconhecido. A f\u00f3rmula utilizada para calcular a diferen\u00e7a \u00e9 mostrada pela Equa\u00e7\u00e3o 6.1. \n\n     [\n           \n\n    \n] \n\nEqua\u00e7\u00e3o 6.1 \n\nem que      representa o dado observado e        representa o dado amostrado. \n\nCom isso foi poss\u00edvel obter uma ideia de qu\u00e3o distante do modelo de refer\u00eancia o modelo \n\namostrado estava, conforme consta na Tabela 6.13, que mostra para cada conjunto de entrada a \n\nquantidade de amostras que se situam nas diferentes faixas de dist\u00e2ncias m\u00e9dias (0% - 100% de \n\ndist\u00e2ncia) em rela\u00e7\u00e3o ao modelo de refer\u00eancia. \n\nTabela 6.13 - Rela\u00e7\u00e3o da dist\u00e2ncia relativa dos atributos de treinamento com o hist\u00f3rico \u2013 \n\nCaso 2A \n\nFaixa de varia\u00e7\u00e3o HL50 HL100 HL25 (retreino) \n\n0% - 10% 1 0 0 \n\n11% - 50% 5 6 17 \n\n51% - 100% 17 37 8 \n\n>101% 27 57 0 \n\n \n\nObserva-se pela Tabela 6.13 que s\u00e3o poucos os dados amostrados perto da regi\u00e3o de \n\nm\u00ednimo, ou seja, amostragem de modelos pr\u00f3ximos ao hist\u00f3rico (faixa de 0% - 10%). Por\u00e9m, a \n\namostragem de 50 pontos conseguiu reunir a mesma quantidade de dados (informa\u00e7\u00f5es), situados \n\nna faixa entre 0% a 50%, que os 100 pontos, justificando o bom desempenho do metamodelo \n\ngerado com esses dados e, consequentemente, o retreinamento.  \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n110 \n\n \n\n6.7 Caso 2B \n\n6.7.1 An\u00e1lise de sensibilidade \n\nA Figura 6.25 mostra os resultados da an\u00e1lise de sensibilidade dos atributos utilizados no \n\nmodelo de reservat\u00f3rio do Caso 2B, com rela\u00e7\u00e3o a um dos po\u00e7os (PROD4) e a m\u00e9dia aritm\u00e9tica \n\ndos oito po\u00e7os. O eixo horizontal apresenta a varia\u00e7\u00e3o percentual no valor do afastamento, em \n\nrela\u00e7\u00e3o ao Caso Base (modelo de refer\u00eancia), e o eixo vertical apresenta os atributos, sendo que a \n\ncor em bege indica que o atributo foi alterado para o seu limite inferior e a cor em azul indica que \n\nele foi alterado para o seu limite superior. Valores negativos indicam que o afastamento do \n\nmodelo piorou em rela\u00e7\u00e3o ao afastamento do Caso Base e valores positivos indicam o contr\u00e1rio \n\n(exemplo na Figura 3.1). \n\n    \n\n                                       (a)                                      (b) \n\nFigura 6.25 \u2013 An\u00e1lise de sensibilidade para o afastamento da produ\u00e7\u00e3o de \u00e1gua do po\u00e7o PROD4 \n\n(a) e m\u00e9dia aritm\u00e9tica (b) \u2013 Caso 2B. \n\nA Figura 6.25 (a) mostra que existem atributos que n\u00e3o influenciam na produ\u00e7\u00e3o de \u00e1gua \n\ndo po\u00e7o PROD4, sendo que, nesse caso em particular, s\u00e3o poucos os atributos que influenciam.  \n\nO po\u00e7o PROD7 tamb\u00e9m teve poucos atributos que o influenciam, por\u00e9m, para os demais po\u00e7os a \n\nquantidade de atributos que influenciam \u00e9 maior (ver Tabela 5.3).  \n\nObserva-se que enquanto a varia\u00e7\u00e3o de um atributo acarreta uma redu\u00e7\u00e3o do afastamento \n\n(em rela\u00e7\u00e3o ao Caso Base) para o po\u00e7o PROD4, para a m\u00e9dia ela acarreta em aumento. Como os \n\n0% 10% 20% 30% 40% 50%\n\npor1\npor2\npor3\nkx1\nkx2\nkx3\nkz1\nkz2\nkz3\nF1\nF2\nF3\nF4\n\nkr1\nkr2\nkr3\n\nSensibilidade\n\nA\ntr\n\nib\nu\n\nto\ns\n\nLimite inferior\n\nLimite superior\n\n-200% -160% -120% -80% -40% 0% 40%\n\npor1\npor2\npor3\nkx1\nkx2\nkx3\nkz1\nkz2\nkz3\nF1\nF2\nF3\nF4\n\nkr1\nkr2\nkr3\n\nSensibilidade\n\nA\ntr\n\nib\nu\n\nto\ns\n\nLimite inferior\n\nLimite superior\n\n\n\n \n\n \n\n111 \n\n \n\natributos s\u00e3o regionalizados, o impacto que cada um deles causa varia de po\u00e7o a po\u00e7o, podendo \n\nser at\u00e9 desprez\u00edvel, como pode ser observado pelos resultados para o PROD4.  \n\nA Tabela 6.14 mostra os valores num\u00e9ricos do gr\u00e1fico apresentado na Figura 6.25. As \n\ncolunas do limite inferior e limite superior mostram a varia\u00e7\u00e3o porcentual do afastamento quando \n\nos valores dos atributos s\u00e3o alterados para os seus limites inferior e superior, respectivamente. \n\nTabela 6.14 \u2013 An\u00e1lise de sensibilidade do po\u00e7o PROD4 e da m\u00e9dia dos oito po\u00e7os. \n\n Varia\u00e7\u00e3o no afastamento - PROD4 (%) Varia\u00e7\u00e3o no afastamento - m\u00e9dia (%) \n\nAtributo Limite inferior Limite superior Limite inferior Limite superior \n\nPor1 0 0 -2 1 \n\nPor2 0 0 -45 10 \n\nPor3 0 0 -13 -6 \n\nKx1 0 0 -23 16 \n\nKx2 4 0 33 -10 \n\nKx3 0 8 11 -19 \n\nKz1 0 0 0 0 \n\nKz2 0 0 -19 10 \n\nKz3 0 0 7 -7 \n\nF1 0 0 1 0 \n\nF2 0 0 -9 -1 \n\nF3 0 0 6 -3 \n\nF4 19 0 22 0 \n\nKr1 0 0 4 -5 \n\nKr2 42 0 -176 7 \n\nKr3 41 0 -23 -11 \n\n \n\nConforme mostra a Tabela 6.14, para o po\u00e7o PROD4, ao modificar o valor da \n\npermeabilidade horizontal da f\u00e1cies 3 para o limite superior, reduziu-se em 8% o afastamento em \n\nrela\u00e7\u00e3o ao Caso Base; ao modificar os atributos permeabilidade horizontal da f\u00e1cies 2, \n\ntransmissibilidade da falha 4 e os expoentes do modelo de Corey para a permeabilidade relativa \n\nda \u00e1gua das f\u00e1cies 2 e 3 para o limite inferior reduziu-se o afastamento em 4%, 19%, 42% e 41%, \n\nrespectivamente. Os demais atributos n\u00e3o influenciaram no afastamento para esse po\u00e7o, nem ao \n\naumentar seus valores para o limite superior, nem ao reduzir seus valores para o limite inferior. \n\nBaseado nesses valores obtidos definiu-se que os atributos F4, kr2 e kr3 devem ser aqueles a \n\n\n\n \n\n \n\n112 \n\n \n\nserem utilizados como entrada para treinar a RNA na representa\u00e7\u00e3o do comportamento do po\u00e7o \n\nPROD4, pois apresentaram maior influ\u00eancia nos resultados para esse po\u00e7o.  \n\nConsiderando a m\u00e9dia dos afastamentos dos po\u00e7os, foram utilizados nove atributos: por2, \n\npor3, kx1, kx2, kx3, kz2, F4, kr2 e kr3. Foram escolhidos os atributos nos quais a soma da \n\nvaria\u00e7\u00e3o porcentual quando alterados os limites inferior e superior fosse maior que 20%. O \n\nmesmo procedimento foi adotado para defini\u00e7\u00e3o dos atributos que mais influenciam os demais \n\npo\u00e7os. \n\nOs atributos encontrados com a an\u00e1lise de sensibilidade po\u00e7o a po\u00e7o foram utilizados para \n\ntreinar as RNA no caso de treinar um RNA para cada po\u00e7o. No caso de treinar uma \u00fanica RNA \n\npara representar todos os po\u00e7os, os atributos determinados segundo a an\u00e1lise de sensibilidade \n\npara a m\u00e9dia dos afastamentos dos po\u00e7os foram utilizados. \n\n6.7.2 Passo 1: Defini\u00e7\u00e3o do conjunto de treinamento \n\nA Figura 6.26 mostra os gr\u00e1ficos de dispers\u00e3o dos pontos de treinamento entre os atributos \n\nKz2 e Kr2, relativos aos conjuntos HL100 (a) e HL250 (b) pontos. \n\n  \n\n                                     (a)                                     (b) \n\nFigura 6.26 \u2013 Gr\u00e1fico de dispers\u00e3o dos pontos de treinamento entre os atributos Kz2 e Kr2, \n\nrelativo aos conjuntos HL100 (a) e HL250 (b) pontos \u2013 Caso 2B. \n\nOs gr\u00e1ficos da Figura 6.26 mostram que os pontos seguem uma distribui\u00e7\u00e3o \n\naproximadamente uniforme no espa\u00e7o, sendo que os HL250 pontos apresentam mais pontos \n\npr\u00f3ximos uns dos outros. Em contrapartida os HL100 pontos apresentam algumas \u201clacunas\u201d, que \n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\n4 9 14 19 24\n\nK\nr\n2\n\nKz2\n\nHL100\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\n4 9 14 19 24\n\nK\nr\n2\n\nKz2\n\nHL250\n\n\n\n \n\n \n\n113 \n\n \n\ns\u00e3o as \u00e1reas sem informa\u00e7\u00e3o do comportamento a respeito do espa\u00e7o de solu\u00e7\u00f5es. Essa \n\ncaracter\u00edstica de espa\u00e7amento, conforme j\u00e1 mencionado (Subitem 6.1.1), se deve ao fato de que o \n\nHL realiza a combina\u00e7\u00e3o das diversas vari\u00e1veis de forma aleat\u00f3ria, de modo que, mesmo que os \n\npontos estejam bem espa\u00e7ados entre uma vari\u00e1vel e outra, eles podem n\u00e3o estar com rela\u00e7\u00e3o a \n\numa terceira. Observa-se ainda que os pontos extremos de cada vari\u00e1vel foram amostrados, \n\npor\u00e9m, as condi\u00e7\u00f5es extremas n\u00e3o (amostras situadas em um dos quatro cantos dos gr\u00e1ficos da \n\nFigura 6.26). O mesmo padr\u00e3o foi observado para os demais atributos. \n\nA Figura 6.27 mostra os gr\u00e1ficos de dispers\u00e3o dos pontos de treinamento entre os atributos \n\nKz2 e Kr2, referente aos conjuntos HL396 (a) e BB396 (b). \n\n  \n\n                                    (a)                                     (b) \n\nFigura 6.27 - Gr\u00e1ficos de dispers\u00e3o dos pontos de treinamento entre os atributos Kz2 e Kr2, \n\nreferente aos conjuntos HL396 (a) e BB396 (b) \u2013 Caso 2B. \n\nA diferen\u00e7a entre as duas t\u00e9cnicas pode ser observada comparando-se os gr\u00e1ficos da Figura \n\n6.27. Apesar da Figura 6.27 (b) mostrar que as condi\u00e7\u00f5es extremas entre essas duas vari\u00e1veis \n\nforam amostradas, o mesmo n\u00e3o ocorre quando comparadas todas as dezesseis vari\u00e1veis, ou seja, \n\num modelo composto pelos valores extremos de todas as vari\u00e1veis. Conforme mencionado no \n\nSubitem 2.4.1 de fundamenta\u00e7\u00e3o te\u00f3rica, o BB utiliza uma matriz pr\u00e9-definida, composta por \n\nvalores extremos e m\u00e9dios, em que as combina\u00e7\u00f5es entre as vari\u00e1veis tamb\u00e9m s\u00e3o pr\u00e9-definidas. \n\n \n\n \n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\n4 9 14 19 24\n\nK\nr\n2\n\nKz2\n\nHL396\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\n5\n\n4 9 14 19 24\n\nK\nr\n2\n\nKz2\n\nBB396\n\n\n\n \n\n \n\n114 \n\n \n\n6.7.3 Passo 2: Treinamento das redes neurais artificiais e an\u00e1lise de desempenho dos \n\nmetamodelos gerados \n\nDas op\u00e7\u00f5es de configura\u00e7\u00e3o de RNA definidas no Subitem 5.3 de aplica\u00e7\u00e3o, nesse cap\u00edtulo \n\nser\u00e3o mostrados apenas os resultados relativos aos metamodelos gerados com a op\u00e7\u00e3o 4 de \n\nconfigura\u00e7\u00e3o (PROD_AS \u2013 produtores, com an\u00e1lise de sensibilidade), que consistiu em treinar \n\numa RNA com oito sa\u00eddas (representando os oito po\u00e7os produtores), em que, como entrada, foi \n\nutilizado o conjunto com os nove atributos que mais influenciam a m\u00e9dia dos afastamentos dos \n\noito po\u00e7os produtores.  \n\nDe acordo com os resultados, atrav\u00e9s dessa op\u00e7\u00e3o de configura\u00e7\u00e3o, obteve-se o modelo de \n\nRNA que gerou o menor afastamento final, ao simular o modelo de simula\u00e7\u00e3o encontrado com a \n\notimiza\u00e7\u00e3o utilizando o metamodelo. Esses resultados s\u00e3o mostrados resumidamente no \n\nAp\u00eandice II.  \n\nCom a utiliza\u00e7\u00e3o de oito RNA independentes, eventuais inter-rela\u00e7\u00f5es existentes entre os \n\npo\u00e7os n\u00e3o puderam ser captadas. Por outro lado, a utiliza\u00e7\u00e3o de uma \u00fanica rede para representar \n\nos oito po\u00e7os pode captar essas inter-rela\u00e7\u00f5es, o que constitui uma poss\u00edvel causa para seus \n\nmelhores resultados.  \n\nQuanto aos atributos, utilizar apenas aqueles sugeridos pela an\u00e1lise de sensibilidade \n\ntamb\u00e9m pode contribuir para melhorar os resultados, pois s\u00e3o selecionados apenas os mais \n\nimportantes para a Fun\u00e7\u00e3o Objetivo (maior influ\u00eancia). \n\nA Tabela 6.15 mostra os valores dos coeficientes de correla\u00e7\u00e3o linear entre sa\u00eddas geradas \n\natrav\u00e9s do simulador de escoamento (afastamento da produ\u00e7\u00e3o de \u00e1gua do modelo com rela\u00e7\u00e3o \n\nao hist\u00f3rico) e do metamodelo (coeficientes po\u00e7o a po\u00e7o, geral e m\u00e9dia, para as oito sa\u00eddas), com \n\nrela\u00e7\u00e3o aos metamodelos gerados com BB396, HL396, HL250 e HL100 pontos com a \n\nconfigura\u00e7\u00e3o de rede do tipo PROD_AS. As linhas \u201cTr\u201d s\u00e3o relativas ao conjunto de treinamento \n\ne as linhas \u201cTt\u201d s\u00e3o relativas ao conjunto de teste.  \n\n \n\n \n\n\n\n \n\n \n\n115 \n\n \n\nTabela 6.15 \u2013 Coeficientes de correla\u00e7\u00e3o linear dos metamodelos gerados \u2013 PROD_AS \u2013 \n\nCaso 2B. \n\nMeta \n\nmodelo \n\nTipo \n\ndado \n\nPROD \nGeral M\u00e9dia \n\n1 2 3 4 5 6 7 8 \n\nBB396 \nTr 0.80 0.95 0.89 0.75 0.88 0.87 0.98 0.72 0.93 0.86 \n\nTt 0.63 0.95 0.86 0.66 0.91 0.86 0.78 0.60 0.89 0.78 \n\nHL396 \nTr 0.74 0.98 0.96 0.83 0.95 0.91 0.95 0.70 0.96 0.88 \n\nTt 0.62 0.96 0.95 0.78 0.92 0.92 0.86 0.70 0.94 0.84 \n\nHL250 \nTr 0.83 0.97 0.95 0.82 0.93 0.90 0.95 0.83 0.95 0.90 \n\nTt 0.56 0.97 0.96 0.80 0.90 0.83 0.90 0.85 0.93 0.85 \n\nHL100 \nTr 0.87 0.98 0.96 0.78 0.94 0.89 0.91 0.82 0.96 0.89 \n\nTt 0.67 0.95 0.94 0.73 0.91 0.84 0.83 0.75 0.92 0.83 \n\n \n\nConforme mostra a Tabela 6.15, alguns po\u00e7os como o PROD1, PROD4 e PROD8 \n\nresultaram em baixos valores de correla\u00e7\u00e3o linear comparativamente aos demais po\u00e7os, \n\nindependentemente da quantidade e tipo de dado amostrado. Isso pode ser devido a n\u00e3o \n\nlinearidade existente entre entrada e sa\u00edda, diferente para cada po\u00e7o.  \n\nApesar das diferen\u00e7as nos coeficientes po\u00e7o a po\u00e7o, os coeficientes de correla\u00e7\u00e3o linear \n\ngeral (coeficiente analisando as oito sa\u00eddas juntas) atingiram valores relativamente altos.  \n\nAo comparar a m\u00e9dia dos valores de correla\u00e7\u00e3o linear pode-se observar que os \n\nmetamodelos gerados com HL tiveram valores semelhantes (HL396-0.84, HL250-0.85 e HL100-\n\n0.83) e foram superiores ao metamodelo gerado com BB (BB396-0.78). \n\nAs Figura 6.28 e Figura 6.29 mostram os gr\u00e1ficos de dispers\u00e3o (crossplot) entre sa\u00edda do \n\nsimulador de escoamento e do metamodelo, para os po\u00e7os PROD2 (Figura 6.28) e PROD3 \n\n(Figura 6.29). \n\n \n\n\n\n \n\n \n\n116 \n\n \n\n \n\n                                   (a)                                  (b) \n\n \n\n(c) (d) \n\nFigura 6.28 - Gr\u00e1ficos de dispers\u00e3o (crossplot) para o po\u00e7o PROD2 \u2013 PROD_AS \u2013 Caso 2B.  \n\n2 4 6 8 10\n\nx 10\n7\n\n2\n\n4\n\n6\n\n8\n\n10\nx 10\n\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\nCaso2B - PROD2 - Metamodelo BB396\n\n \n\n \n\nY=T\n\nDados de treino - BB 396\n\nDados de teste - HL 100\n\n2 4 6 8\n\nx 10\n7\n\n2\n\n4\n\n6\n\n8\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD2 - Metamodelo HL396\n\n \n\n \n\nY=T\n\nDados de treino - HL 396\n\nDados de teste - HL 100\n\n2 4 6 8\n\nx 10\n7\n\n2\n\n4\n\n6\n\n8\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD2 - Metamodelo HL250\n\n \n\n \n\nY=T\n\nDados de treino - HL 250\n\nDados de teste - HL 100\n\n2 4 6 8\n\nx 10\n7\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD2 - Metamodelo HL100\n\n \n\n \n\nY=T\n\nDados de treino - HL 100\n\nDados de teste - HL 100\n\n\n\n \n\n \n\n117 \n\n \n\n \n\n                                    (a)                                 (b) \n\n \n\n(c)                                 (d) \n\nFigura 6.29 - Gr\u00e1ficos de dispers\u00e3o (crossplot) para o po\u00e7o PROD3 \u2013 PROD_AS \u2013 Caso 2B.  \n\nComparando os gr\u00e1ficos de HL396 e BB396 observa-se que o gr\u00e1fico do metamodelo \n\nHL396 est\u00e1 um pouco melhor, pois os pontos se concentram mais ao redor da linha \u201cy=x\u201d em \n\npreto, o que demostra melhor correla\u00e7\u00e3o linear. Analisando apenas os gr\u00e1ficos do HL, pode-se \n\nobservar uma melhoria do metamodelo gerado com 250 pontos em rela\u00e7\u00e3o ao gerado com 100 \n\npontos, relativo \u00e0 concentra\u00e7\u00e3o dos pontos ao redor da linha \u201cy=x\u201d, por\u00e9m entre os metamodelos \n\ngerados com 250 e 396 pontos n\u00e3o \u00e9 poss\u00edvel observar grandes diferen\u00e7as. \n\nRealizando a compara\u00e7\u00e3o entre os po\u00e7os, a correla\u00e7\u00e3o linear para o po\u00e7o PROD2 foi \n\nmelhor do que para o po\u00e7o PROD3, concentrando mais valores com afastamento menor (mais \n\n1 2 3 4\n\nx 10\n7\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\nCaso2B - PROD3 - Metamodelo BB396\n\n \n\n \n\nY=T\n\nDados de treino - BB 396\n\nDados de teste - HL 100\n\n1 2 3 4\n\nx 10\n7\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD3 - Metamodelo HL396\n\n \n\n \n\nY=T\n\nDados de treino - HL 396\n\nDados de teste - HL 100\n\n1 2 3 4\n\nx 10\n7\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD3 - Metamodelo HL250\n\n \n\n \n\nY=T\n\nDados de treino - HL 250\n\nDados de teste - HL 100\n\n1 2 3 4\n\nx 10\n7\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso2B - PROD3 - Metamodelo HL100\n\n \n\n \n\nY=T\n\nDados de treino - HL 100\n\nDados de teste - HL 100\n\n\n\n \n\n \n\n118 \n\n \n\nperto da origem) enquanto que para o po\u00e7o PROD3, al\u00e9m de poucos valores perto do m\u00ednimo, \n\neles ficaram dispersos. Isso demonstra que o po\u00e7o PROD3 oferece mais dificuldade em rela\u00e7\u00e3o \n\nao po\u00e7o PROD2 para modelar, de modo que se pode inferir que existe maior n\u00e3o linearidade \n\nentre entrada e sa\u00edda para o po\u00e7o PROD3, comparativamente ao po\u00e7o PROD2. Para os demais \n\npo\u00e7os, tamb\u00e9m existem aqueles com dados mais concentrados perto da origem, quanto dados \n\nmais afastados ou mais dispersos. \n\nA Figura 6.30 mostra os coeficientes de correla\u00e7\u00e3o linear da Tabela 6.15 na forma de barras \n\npara visualizar melhor a diferen\u00e7a que existe entre os po\u00e7os e os tipos de dados. O grupo de \n\nbarras que aparecem na esquerda representam os resultados do treinamento e os que aparecem na \n\ndireita representam os resultados do teste. \n\n0.5\n\n0.55\n\n0.6\n\n0.65\n\n0.7\n\n0.75\n\n0.8\n\n0.85\n\n0.9\n\n0.95\n\n1\nBB396 HL396 HL250 HL100\n\n \n\nFigura 6.30 \u2013 Coeficiente de correla\u00e7\u00e3o linear em gr\u00e1fico de barras; grupo de barras da esquerda: \n\nresultados do treinamento; grupo de barras da direita: resultados do teste \u2013 Caso 2B. \n\nA diferen\u00e7a entre a correla\u00e7\u00e3o dos dados de treinamento e dos dados de teste que pode ser \n\nvisualizada na Figura 6.30 \u00e9 natural, uma vez que os dados de teste s\u00e3o dados para os quais o \n\n\n\n \n\n \n\n119 \n\n \n\nmetamodelo nunca foi utilizado. Pela Figura 6.30 fica claro que os valores de correla\u00e7\u00e3o linear \n\npara os po\u00e7os PROD1, PROD4 e PROD8 foram piores, comparativamente aos demais po\u00e7os.  \n\nOs valores de correla\u00e7\u00e3o linear mostrados na coluna \u201cGeral\u201d (relativo aos oito po\u00e7os) e na \n\ncoluna \u201cM\u00e9dia\u201d mostram que o desempenho dos metamodelos gerados pelo HL foi melhor que o \n\ndo metamodelo gerado pelo BB. Em rela\u00e7\u00e3o aos metamodelos gerados pelo HL pode-se observar \n\nque o aumento do n\u00famero de amostras contribuiu para melhorar o valor da correla\u00e7\u00e3o linear \n\nquando comparados os resultados para os conjuntos de 100 e 250 pontos. Por\u00e9m, n\u00e3o houve \n\nmelhora na qualidade dos resultados ao aumentar de 250 para 396 pontos.  \n\nUma possibilidade para a qualidade inferior do metamodelo HL396 com rela\u00e7\u00e3o ao Hl250 \n\nseria a memoriza\u00e7\u00e3o, ou seja, o metamodelo, apesar de boa correla\u00e7\u00e3o, estaria come\u00e7ando a \n\nmemorizar os resultados e isso pode ter contribu\u00eddo para que n\u00e3o houvesse um aumento \n\nexpressivo na sua capacidade de generaliza\u00e7\u00e3o. \n\n6.7.4 Passo 3: Otimiza\u00e7\u00e3o utilizando o metamodelo e valida\u00e7\u00e3o do m\u00ednimo encontrado \n\nA Tabela 6.16 mostra os afastamentos dos oito po\u00e7os produtores e a sua m\u00e9dia aritm\u00e9tica \n\n(coluna \u201cM\u00e9dia\u201d) em valores num\u00e9ricos e porcentuais, que indicam a redu\u00e7\u00e3o do afastamento em \n\nrela\u00e7\u00e3o ao Caso Base (qualidade do metamodelo \u2013 Equa\u00e7\u00e3o 3.3 e Figura 3.1). Para obter os \n\nvalores dos afastamentos, os valores dos atributos (ponto de m\u00ednimo) encontrados com a \n\notimiza\u00e7\u00e3o utilizando o metamodelo foram substitu\u00eddos no simulador de escoamento.  O objetivo \n\ndessa tabela \u00e9 apresentar um resumo dos resultados de forma quantitativa. Por\u00e9m, a sua an\u00e1lise \n\ndever ser realizada em conjunto com os gr\u00e1ficos de produ\u00e7\u00e3o apresentados mais adiante. \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n120 \n\n \n\nTabela 6.16 \u2013 Valores de afastamento e indicador de qualidade (%), obtidos com a \n\nsimula\u00e7\u00e3o do m\u00ednimo (x10\n6\n) \u2013 PROD_AS \u2013 Caso 2B. \n\nModelo \nPROD (x10\n\n6\n) \u2013 Valores absolutos \n\nM\u00e9dia \n1 2 3 4 5 6 7 8 \n\nBase 0.973 4.32 31.9 5.76 11.5 0.738 1.10 0.439 7.09 \n\nBB396 1.44 1.34 0.493 0.0176 7.28 0.273 0.204 0.108 1.39 \n\nHL396 3.49 0.179 2.16 9.17 1.44 0.0153 0.919 0.0083 2.17 \n\nHL250 2.68 0.0413 1.28 0.0791 1.92 0.716 0.0738 0.0052 0.851 \n\nHL100 0.924 0.525 2.40 7.75 0.278 0.217 0.257 0.708 1.63 \n\nValores em % \n\nBB396 -48 69 98 100 37 63 81 75 80 \n\nHL396 -259 96 93 -59 87 98 16 98 69 \n\nHL250 -175 99 96 99 83 3 93 99 88 \n\nHL100 5 88 92 -35 98 71 77 -61 77 \n\n \n\nAnalisando a coluna \u201cM\u00e9dia\u201d para porcentagem da Tabela 6.16 pode-se observar que em \n\ngeral a metodologia proposta foi capaz de melhorar o Caso Base, variando de uma redu\u00e7\u00e3o de \n\n69% no valor do afastamento para o metamodelo gerado com HL396 pontos at\u00e9 uma redu\u00e7\u00e3o de \n\n88% no valor do afastamento para o metamodelo gerado com HL250 pontos.  \n\nA otimiza\u00e7\u00e3o utilizando o metamodelo gerado com HL396 acabou piorando o afastamento \n\ndo po\u00e7o PROD1 em rela\u00e7\u00e3o ao Caso Base muito mais que em rela\u00e7\u00e3o aos demais metamodelos (-\n\n259%), o que explica a m\u00e9dia menor em rela\u00e7\u00e3o aos demais. Por\u00e9m, o po\u00e7o PROD1, conforme \n\nmostrado na Figura 6.31 (a) adiante, j\u00e1 estava razoavelmente ajustado e possui vaz\u00e3o de produ\u00e7\u00e3o \n\nde \u00e1gua elevada, de modo que uma pequena varia\u00e7\u00e3o da curva acaba gerando valor de erro muito \n\nalto. \n\nOs resultados para o metamodelo HL250 mostrados na Tabela 6.16 indicam que os ajustes \n\npara os po\u00e7os PROD1 e PROD6 n\u00e3o foram bons. Por\u00e9m, conforme citado no par\u00e1grafo anterior o \n\npo\u00e7o PROD1 j\u00e1 se encontrava relativamente bem ajustado e a Figura 6.32 (b), apresentada \n\nadiante, mostra que o po\u00e7o PROD6 tamb\u00e9m se encontrava bem ajustado. Para os demais po\u00e7os a \n\notimiza\u00e7\u00e3o utilizando o metamodelo HL250 melhorou o Caso Base consideravelmente (todos \n\nacima de 80%), indicando que a sua utiliza\u00e7\u00e3o proporcionou bons ajustes das curvas de produ\u00e7\u00e3o \n\ndos po\u00e7os. \n\n\n\n \n\n \n\n121 \n\n \n\nA Figura 6.31 e a Figura 6.32 mostram as curvas de produ\u00e7\u00e3o de \u00e1gua geradas pela \n\nsimula\u00e7\u00e3o dos modelos encontrados atrav\u00e9s da otimiza\u00e7\u00e3o utilizando os metamodelos, assim \n\ncomo a curva do Caso Base e do hist\u00f3rico, para todos os po\u00e7os.   \n\n \n\n                                     (a)                                    (b) \n\n \n\n    (c) (d) \n\nFigura 6.31 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os metamodelos \n\npara os po\u00e7os PROD1 (a), PROD2 (b), PROD3 (c) e PROD4 (d) \u2013 PROD_AS \u2013 Caso 2B.  \n\nA Figura 6.32 (a) do po\u00e7o PROD1 mostra que a curva do Caso Base (vermelho) j\u00e1 estava \n\nbem ajustada, justificando o coment\u00e1rio realizado anteriormente sobre pequenos desvios das \n\ncurvas gerarem grandes erros, conforme observado na Tabela 6.16. Para os po\u00e7os PROD2, \n\nPROD3 e PROD4 \u00e9 poss\u00edvel notar que houve melhora na curva de produ\u00e7\u00e3o de \u00e1gua com rela\u00e7\u00e3o \n\nao Caso Base, sendo que para o PROD4 a curva do metamodelo HL250 se aproximou bastante do \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n500\n\n1000\n\n1500\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD1\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\nTempo (dias)\nP\n\nro\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD2\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n500\n\n1000\n\n1500\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD3\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD4\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n\n\n \n\n \n\n122 \n\n \n\nhist\u00f3rico e do metamodelo BB396 ajustou o hist\u00f3rico perfeitamente. Em contrapartida, os \n\nmodelos otimizados atrav\u00e9s dos metamodelos HL100 e HL396 apresentaram uma vaz\u00e3o de \u00e1gua \n\nmaior que o hist\u00f3rico.  \n\n \n\n                                    (a)                                    (b) \n\n \n\n  (c) (d) \n\nFigura 6.32 - Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os metamodelos \n\npara os po\u00e7os PROD5 (a), PROD6 (b), PROD7 (c) e PROD8 (d) \u2013 PROD_AS \u2013 Caso 2B. \n\nPara os po\u00e7os PROD5, PROD6, PROD7 e PROD8, os modelos encontrados melhoraram o \n\nCaso Base.  \n\nOs resultados mostram que a qualidade dos resultados para os metamodelos gerados com \n\nHL foi superior em compara\u00e7\u00e3o a do BB. Em rela\u00e7\u00e3o \u00e0 quantidade de amostras, comparando os \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\n1400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD5\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nTempo (dias)\nP\n\nro\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD6\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n100\n\n200\n\n300\n\n400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD7\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nCaso2B - Simula\u00e7\u00e3o - PROD8\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nBB396\n\nHL396\n\nHL250\n\nHL100\n\n\n\n \n\n \n\n123 \n\n \n\nresultados do HL, houve uma melhora significativa dos resultados para os casos com 250 e 396 \n\npontos em rela\u00e7\u00e3o ao caso com 100 pontos. No entanto, o resultado do metamodelo gerado com \n\nHL250 pontos foi melhor do que o HL396 pontos. Pode ser que o metamodelo HL396 escolhido \n\nestivesse come\u00e7ando a memorizar os resultados, o que constitui uma poss\u00edvel explica\u00e7\u00e3o para sua \n\nqualidade inferior. Uma maneira de contornar esse problema seria utilizar diversos metamodelos \n\ngerados para a fase de otimiza\u00e7\u00e3o e realizar a compara\u00e7\u00e3o entre eles atrav\u00e9s da valida\u00e7\u00e3o com o \n\nsimulador de escoamento. Os resultados tamb\u00e9m sugerem que a melhora na qualidade do \n\nmetamodelo n\u00e3o \u00e9 linear com o aumento da quantidade de amostras, tendendo, possivelmente, a \n\num ponto de satura\u00e7\u00e3o, em que a adi\u00e7\u00e3o de mais pontos n\u00e3o aumenta de forma expressiva a \n\nqualidade dos resultados.  \n\n6.7.5 Passo 4: Retreinamento \n\nPara avaliar o efeito do retreinamento foi utilizado o metamodelo HL100_PROD_AS. \n\nConforme crit\u00e9rio de corte definido na se\u00e7\u00e3o 4.2.2, o valor de corte considerado para esse caso \n\nfoi 70% da Fun\u00e7\u00e3o Objetivo do Caso Base, ou seja, o valor m\u00e1ximo e m\u00ednimo de varia\u00e7\u00e3o dos \n\natributos dos modelos que conseguiram reduzir mais que 30% a m\u00e9dia dos afastamentos (oito \n\npo\u00e7os) com rela\u00e7\u00e3o \u00e0 m\u00e9dia do Caso Base. \n\nA Tabela 6.17 mostra os coeficientes de correla\u00e7\u00e3o linear entre sa\u00edda do simulador e do \n\nmetamodelo para o conjunto de treinamento, relativo ao novo processo de treinamento. \n\nTabela 6.17 - Coeficientes de correla\u00e7\u00e3o linear \u2013 PROD_AS \u2013 Caso 2B \u2013 retreinamento. \n\nNome \nPROD \n\nGeral M\u00e9dia \n1 2 3 4 5 6 7 8 \n\nHL100 \n\nRTR \nTr 0.83 0.98 0.97 0.90 0.94 0.98 0.75 0.82 0.97 0.90 \n\n \n\nA Tabela 6.17 mostra que o retreinamento resultou em boa correla\u00e7\u00e3o linear, atingindo uma \n\nm\u00e9dia de 0.90. \n\n\n\n \n\n \n\n124 \n\n \n\nA Tabela 6.18 mostra os valores num\u00e9ricos e porcentuais (que indicam a redu\u00e7\u00e3o com \n\nrela\u00e7\u00e3o ao Caso Base, ou seja, qualidade do ajuste \u2013 Equa\u00e7\u00e3o 3.3) dos afastamentos po\u00e7o a po\u00e7o \n\ne a m\u00e9dia aritm\u00e9tica dos oito po\u00e7os, calculados a partir da simula\u00e7\u00e3o do modelo obtido com a \n\notimiza\u00e7\u00e3o utilizando o metamodelo HL100_PROD_AS e HL100_PROD_AS_RTR \n\n(retreinamento). Foram inseridos tamb\u00e9m os valores de afastamento do Caso Base para \n\ncompara\u00e7\u00e3o. \n\nTabela 6.18 \u2013 Valores de afastamento gerados com a simula\u00e7\u00e3o dos m\u00ednimos obtidos com a \n\notimiza\u00e7\u00e3o \u2013 PROD_AS \u2013 Retreinamento \u2013 Caso 2B. \n\nNome \nPROD (x10\n\n6\n) Total  \n\n1 2  3  4  5  6  7  8  M\u00e9dia \n\nPROD (x10\n6\n) \u2013 Valores absolutos \n\nBase 0.973 4.32 31.9 5.76 11.5 0.738 1.10 0.439 7.09 \n\nHL100  0.924 0.525 2.40 7.75 0.278 0.217 0.257  0.708 1.63 \n\nHL100 RTR  0.761 0.111 0.849 1.20 1.87 1.62 0.00708 0.0575 0.809 \n\nValores em % \n\nHL100  5 88 92 -35 98 71 77 -61 77 \n\nHL100 RTR  22 97 97 79 84 -120 99 87 89 \n\n \n\nA Tabela 6.18 mostra que o retreinamento proporcionou melhor ajuste em compara\u00e7\u00e3o ao \n\ntreinamento anterior (HL100), conseguindo um valor m\u00e9dio de 89% do indicador de ajuste, \n\nchegando a valores similares aos alcan\u00e7ados no treinamento com HL250 pontos (88%). Apesar \n\ndisso, os ajustes para os po\u00e7os PROD5 e PROD6 pioraram em rela\u00e7\u00e3o ao treinamento. O po\u00e7o \n\nPROD6, por\u00e9m, estava bem ajustado, e qualquer pequena altera\u00e7\u00e3o resultaria em grandes \n\nmudan\u00e7as em seu valor porcentual.  \n\nOs gr\u00e1ficos da Figura 6.33 mostram as curvas de produ\u00e7\u00e3o de \u00e1gua dos po\u00e7os PROD1 (a), \n\nPROD2 (b), PROD3 (b) e PROD4 (b), referente ao retreinamento, comparadas com o hist\u00f3rico. \n\n\n\n \n\n \n\n125 \n\n \n\n  \n\n                                      (a)                                    (b) \n\n \n\n                                     (c)                                     (d) \n\nFigura 6.33 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os metamodelos \n\npara os po\u00e7os PROD1 (a), PROD2 (b), PROD3 (b) e PROD4 (b) \u2013 retreinamento \u2013 Caso 2B. \n\nOs gr\u00e1ficos para esses quatro po\u00e7os mostram que houve melhora no ajuste dos po\u00e7os. \n\nOs gr\u00e1ficos da Figura 6.34 mostram as curvas de produ\u00e7\u00e3o de \u00e1gua dos po\u00e7os PROD5 (a), \n\nPROD6 (b), PROD7 (b) e PROD8 (b), referente ao retreinamento. \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n500\n\n1000\n\n1500\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD1\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\n\u00e7\n\u00e3\no\n\n d\ne\n \u00e1\n\ng\nu\n\na\n (\n\nm\n3\n\n/d\nia\n\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD2\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n500\n\n1000\n\n1500\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\n\u00e7\n\u00e3\no\n\n d\ne\n \u00e1\n\ng\nu\n\na\n (\n\nm\n3\n\n/d\nia\n\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD3\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\n\u00e7\n\u00e3\no\n\n d\ne\n \u00e1\n\ng\nu\n\na\n (\n\nm\n3\n\n/d\nia\n\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD4\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n\n\n \n\n \n\n126 \n\n \n\n \n\n                                      (a)                                     (b) \n\n \n\n                                     (c)                                    (d) \n\nFigura 6.34 \u2013 Simula\u00e7\u00e3o dos m\u00ednimos encontrados com a otimiza\u00e7\u00e3o utilizando os metamodelos \n\npara os po\u00e7os PROD5 (a), PROD6 (b), PROD7 (b) e PROD8 (b) \u2013 retreinamento \u2013 Caso 2B. \n\nPela Figura 6.34 (b) pode-se observar que a curva para o po\u00e7o PROD6 n\u00e3o ficou t\u00e3o \n\ndesajustada quanto mostra o resultado da Tabela 6.18 (-120%). J\u00e1 as curvas dos po\u00e7os PROD7 e \n\nPROD8 melhoraram significativamente. \n\nEm rela\u00e7\u00e3o aos po\u00e7os PROD7 e PROD8 pode-se observar que, apesar de seus coeficientes \n\nde correla\u00e7\u00e3o linear resultarem em valores menores, as curvas de produ\u00e7\u00e3o ficaram bem \n\najustadas. Uma explica\u00e7\u00e3o para esse fato \u00e9 que, ao utilizar uma rede para representar os oitos \n\npo\u00e7os produtores, \u00e9 poss\u00edvel que o metamodelo tenha conseguido captar eventuais \n\ninterdepend\u00eancias que podem existir entre os po\u00e7os. Al\u00e9m disso, o fato de o processo de \n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\n1400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD5\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD6\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD7\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n500 1000 1500 2000 2500 3000 3500\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\n400\n\nTempo (dias)\n\nP\nro\n\nd\nu\n\u00e7\n\u00e3\no\n d\n\ne\n \u00e1\n\ng\nu\na\n (\n\nm\n3\n/d\n\nia\n)\n\nProdu\u00e7\u00e3o de \u00e1gua - PROD8\n\n \n\n \n\nHist\u00f3rico\n\nCaso base\n\nHL100-prod-AS\n\nHL100-prod-AS-retreino\n\n\n\n \n\n \n\n127 \n\n \n\notimiza\u00e7\u00e3o consistir em otimizar a m\u00e9dia de produ\u00e7\u00e3o dos oito po\u00e7os, os po\u00e7os com valores \n\nelevados de afastamento acabaram influenciando mais nos resultados, comparativamente aos \n\npo\u00e7os com valores de afastamento menores, como nos po\u00e7os PROD7 e PROD8. Dessa maneira, \n\numa avalia\u00e7\u00e3o do processo de otimiza\u00e7\u00e3o tamb\u00e9m pode contribuir para melhorar os resultados, o \n\nque pode ser feito em trabalhos futuros, pois n\u00e3o era o foco desse trabalho. \n\n6.8 Coment\u00e1rios gerais dos casos de reservat\u00f3rio. \n\nO estudo dos casos de reservat\u00f3rio (Caso 2A e Caso 2B) possibilitou validar os resultados \n\nobtidos com os casos te\u00f3ricos (Casos 1A a 1D) e estend\u00ea-los a casos complexos.  \n\nCom o aumento da dificuldade de modelagem do problema, ou seja, \u00e0 medida que o espa\u00e7o \n\nde solu\u00e7\u00f5es a qual se deseja modelar se torna mais irregular e o n\u00famero de atributos aumenta, a \n\nobten\u00e7\u00e3o de um metamodelo capaz de modelar esse espa\u00e7o se torna mais dif\u00edcil. \n\nOs resultados obtidos indicam que, para o caso estudado nesse trabalho (Caso 2B), a \n\nutiliza\u00e7\u00e3o do metamodelo para substituir o simulador de escoamento em todo o processo de \n\najuste n\u00e3o \u00e9 recomend\u00e1vel. Para casos com muitos atributos incertos, a melhor utiliza\u00e7\u00e3o \u00e9 como \n\nferramenta auxiliar, para ajudar em etapas que demandam maior esfor\u00e7o computacional, sendo a \n\netapa de varredura do espa\u00e7o de solu\u00e7\u00f5es uma das que deve agregar mais valor aos resultados. \n\nDessa maneira, em situa\u00e7\u00f5es em que o n\u00famero de simula\u00e7\u00f5es de escoamento \u00e9 um fator limitante, \n\ncomo no caso de ajuste de hist\u00f3rico, a utiliza\u00e7\u00e3o do metamodelo \u00e9 indicada em etapas que n\u00e3o \n\nexigem muita precis\u00e3o nos resultados. \n\nQuando a quantidade de amostras se torna um fator limitante para o problema, o espa\u00e7o a \n\nser modelado tamb\u00e9m se torna um fator limitante. Com o aumento do n\u00famero de vari\u00e1veis, se o \n\nespa\u00e7o a ser modelado for muito grande, a amostragem eficiente do espa\u00e7o de busca dos \n\npar\u00e2metros se torna dif\u00edcil, e \u00e0s vezes at\u00e9 invi\u00e1vel, e, consequentemente, a gera\u00e7\u00e3o de um \n\nmetamodelo capaz de modelar o espa\u00e7o de solu\u00e7\u00f5es precisamente tamb\u00e9m se torna dif\u00edcil.  Uma \n\nsugest\u00e3o para essas situa\u00e7\u00f5es \u00e9 realizar a adi\u00e7\u00e3o gradativa de amostras \u00e0 medida que se gera e \n\nvalida o metamodelo com o simulador. Outra sugest\u00e3o seria realizar um estudo preliminar para \n\n\n\n \n\n \n\n128 \n\n \n\nsimplificar o problema, como por exemplo, realizar uma otimiza\u00e7\u00e3o r\u00e1pida, a fim de determinar \n\numa regi\u00e3o menor do espa\u00e7o de solu\u00e7\u00f5es para treinar as redes. \n\nConforme mostrado nos resultados, a avalia\u00e7\u00e3o do coeficiente de correla\u00e7\u00e3o linear e erro \n\nm\u00e9dio serviram apenas para uma an\u00e1lise preliminar, n\u00e3o sendo poss\u00edvel chegar a conclus\u00f5es \n\nprecisas se os metamodelos gerados s\u00e3o adequados para representar o problema em quest\u00e3o. \n\nDessa maneira, um metamodelo que possivelmente tenha sido capaz de modelar com mais \n\nprecis\u00e3o a regi\u00e3o de m\u00ednimo de interesse pode n\u00e3o ter sido escolhido. Assim, recomenda-se \n\nutilizar diversos metamodelos com coeficiente de correla\u00e7\u00e3o bom e posteriormente realizar a \n\nvalida\u00e7\u00e3o deles com a resposta do simulador para verificar qual metamodelo melhor representou \n\no problema. Pelo fato do problema de ajuste de hist\u00f3rico possuir m\u00faltiplas solu\u00e7\u00f5es n\u00e3o se deve \n\nrealizar a escolha dos metamodelos apenas atrav\u00e9s do coeficiente de correla\u00e7\u00e3o linear. \n\nMesmo amostrando valores extremos dos atributos, as t\u00e9cnicas de amostragem dificilmente \n\namostram condi\u00e7\u00f5es extremas, de modo que se o m\u00ednimo se encontrar na regi\u00e3o pr\u00f3xima de uma \n\ncondi\u00e7\u00e3o extrema o metamodelo n\u00e3o ser\u00e1 capaz de detect\u00e1-lo. Para esses casos, uma solu\u00e7\u00e3o seria \n\nrealizar novo treinamento, por\u00e9m amostrando novamente todo o espa\u00e7o de busca dos par\u00e2metros \n\nfor\u00e7ando a inclus\u00e3o das condi\u00e7\u00f5es extremas, para garantir a varredura, mesmo que superficial de \n\ntodo o espa\u00e7o de solu\u00e7\u00f5es. \n\nA respeito do Hipercubo Latino, foi adotada uma distribui\u00e7\u00e3o uniforme para todos os \n\natributos. Por\u00e9m, caso se tenha conhecimento da fun\u00e7\u00e3o de distribui\u00e7\u00e3o de probabilidade dos \n\natributos, a t\u00e9cnica permite que essa informa\u00e7\u00e3o seja inclu\u00edda no processo de amostragem, o que \n\npode contribuir para melhorar a qualidade dos dados de treinamento, pois o conjunto de \n\ntreinamento estaria englobando atributos com valores mais prov\u00e1veis. \n\nA escolha de uma regi\u00e3o adequada para retreinamento (que contenha o m\u00ednimo de \n\ninteresse) est\u00e1 diretamente relacionada com a qualidade do metamodelo gerado, de modo que, se \n\no metamodelo n\u00e3o for capaz de identificar a regi\u00e3o do m\u00ednimo de interesse, h\u00e1 grandes chances \n\nde que esse m\u00ednimo n\u00e3o seja encontrado com o retreinamento (refinamento). Nesse aspecto a \n\ncaracter\u00edstica de suaviza\u00e7\u00e3o da superf\u00edcie de resposta se torna fator limitante para utiliza\u00e7\u00e3o de \n\nmetamodelos gerados por RNA, pois, se informa\u00e7\u00f5es suficientes n\u00e3o forem amostradas na regi\u00e3o \n\ndo m\u00ednimo de interesse, a RNA ir\u00e1 suavizar essa regi\u00e3o e ela dificilmente ser\u00e1 encontrada.  \n\n\n\n \n\n \n\n129 \n\n \n\nIsso mostra a grande import\u00e2ncia que a t\u00e9cnica de amostragem tem sobre o desempenho do \n\nmetamodelo gerado. Se a t\u00e9cnica de amostragem for capaz de coletar informa\u00e7\u00f5es suficientes da \n\nregi\u00e3o do m\u00ednimo de interesse, essa regi\u00e3o ser\u00e1 identificada e um refinamento posterior \n\nidentificar\u00e1 o m\u00ednimo, por\u00e9m, se na amostragem inicial essas informa\u00e7\u00f5es n\u00e3o forem coletadas o \n\nmetamodelo dificilmente ir\u00e1 identificar essa regi\u00e3o. Um procedimento que poderia ser adotado, \n\ncaso n\u00e3o se tenha informa\u00e7\u00f5es da regi\u00e3o de interesse, consiste em utilizar a rede ap\u00f3s uma pr\u00e9-\n\notimiza\u00e7\u00e3o com alguma t\u00e9cnica que demande menos simula\u00e7\u00f5es. \n\nLima et al.(2009) realizaram um procedimento diferente, avaliando os resultados em duas \n\nregi\u00f5es distintas: uma regi\u00e3o macro, englobando todo o espa\u00e7o de solu\u00e7\u00f5es, e uma regi\u00e3o micro, \n\nmais espec\u00edfica, possivelmente englobando o m\u00ednimo. A realiza\u00e7\u00e3o de uma avalia\u00e7\u00e3o em uma \n\nregi\u00e3o espec\u00edfica possibilita identificar um metamodelo que, mesmo que n\u00e3o proporcione uma \n\nrepresenta\u00e7\u00e3o satisfat\u00f3ria quando avaliado globalmente, possa ter sido capaz de modelar melhor a \n\nregi\u00e3o de m\u00ednimo. Assim, a utiliza\u00e7\u00e3o desse tipo de procedimento pode ajudar na etapa de an\u00e1lise \n\nde desempenho das RNA treinadas e na escolha dos melhores metamodelos gerados.  \n\n   \n\n\n\n\n\n \n\n \n\n131 \n\n \n\n7 CONCLUS\u00d5ES E SUGEST\u00d5ES PARA TRABALHOS \n\nFUTUROS \n\nNesse trabalho foi avaliada a aplica\u00e7\u00e3o de metamodelos gerados por redes neurais artificiais \n\n(RNA) para serem utilizados no lugar do simulador de escoamento no processo de ajuste de \n\nhist\u00f3rico. Por ser tratar de uma estrutura simplificada, j\u00e1 era esperado que o metamodelo n\u00e3o \n\nfosse capaz de reproduzir o comportamento do simulador com 100% de precis\u00e3o. Apesar disso, \n\natrav\u00e9s do estudo do Caso 2B foi poss\u00edvel concluir que a ferramenta pode ser uma op\u00e7\u00e3o vi\u00e1vel \n\npara ser aplicada ao processo de ajuste de hist\u00f3rico para casos pr\u00e1ticos.  \n\nCasos te\u00f3ricos \n\nO objetivo dos casos te\u00f3ricos foi avaliar as caracter\u00edsticas do metamodelo atrav\u00e9s de uma \n\nan\u00e1lise visual do espa\u00e7o de solu\u00e7\u00f5es (superf\u00edcie de resposta), o que \u00e9 imposs\u00edvel de se obter para \n\ncasos pr\u00e1ticos de reservat\u00f3rio, que tipicamente possuem bem mais que duas vari\u00e1veis incertas. \n\nEssa an\u00e1lise permitiu identificar e avaliar as qualidades e limita\u00e7\u00f5es que o modelo de RNA \n\nescolhido possui. As principais caracter\u00edsticas identificadas foram: \n\n? Amostragem: para casos simples a diferen\u00e7a na caracter\u00edstica de espa\u00e7amento entre as \n\namostras n\u00e3o exerceu grande influ\u00eancia. Para casos com irregularidades, por\u00e9m, as \n\ndiferen\u00e7as apareceram. No entanto, quando se aumentaram o n\u00famero de amostras, a \n\ndiferen\u00e7a entre as t\u00e9cnicas se tornou irrelevante, al\u00e9m de melhorar a qualidade da \n\nresposta. \n\n? Retreinamento: mesmo se no conjunto de treinamento n\u00e3o houver informa\u00e7\u00f5es \n\nsuficientes para detectar o m\u00ednimo, mas apresentar boas informa\u00e7\u00f5es sobre a regi\u00e3o na \n\nqual ele se situa a RNA ser\u00e1 capaz de identificar essa regi\u00e3o e um refinamento \n\nposterior, poder\u00e1 ter grandes possibilidades de identificar com boa precis\u00e3o o m\u00ednimo \n\nde interesse. Esse aspecto foi comprovado atrav\u00e9s do estudo do Caso 1B. \n\n\n\n \n\n \n\n132 \n\n \n\n? Suaviza\u00e7\u00e3o do espa\u00e7o de solu\u00e7\u00f5es: a caracter\u00edstica em que nas \u00e1reas do espa\u00e7o de \n\nsolu\u00e7\u00f5es para a qual n\u00e3o h\u00e1 dados de treinamento (n\u00e3o h\u00e1 informa\u00e7\u00f5es) a RNA tende a \n\nsuavizar a curvatura da superf\u00edcie p\u00f4de ser observada em todos os casos, na ocasi\u00e3o de \n\ncomparar as superf\u00edcies de resposta geradas.  \n\n? Superf\u00edcie irregular: quando o espa\u00e7o de busca dos par\u00e2metros \u00e9 muito grande e \n\nirregular, o metamodelo necessita de uma quantidade maior de amostras que descrevam \n\no padr\u00e3o de comportamento da superf\u00edcie nessas regi\u00f5es para obter resultados \n\nconfi\u00e1veis, conforme constatado pelo estudo do Caso 1C. \n\n? Indicadores: o indicador de erro utilizado nos casos te\u00f3ricos n\u00e3o foi eficaz em situa\u00e7\u00f5es \n\nem que o m\u00ednimo se localizava perto da origem, pois o erro tendia a ficar muito grande. \n\nComo esse indicador era apenas um complemento para os resultados n\u00e3o houve \n\nproblemas, por\u00e9m isso mostra que a escolha de um indicador adequado ao problema \u00e9 \n\nimportante para avaliar os resultados corretamente.  \n\nCasos de reservat\u00f3rio \n\nAtrav\u00e9s do Caso 2B foi poss\u00edvel avaliar a aplicabilidade do metamodelo gerado atrav\u00e9s de \n\nRNA em situa\u00e7\u00f5es pr\u00e1ticas. O Caso 2A mostrou que em casos mais simples a RNA encontra \n\nbons resultados facilmente. Por\u00e9m com o aumento da complexidade do problema (Caso 2B), fica \n\nmais dif\u00edcil encontrar uma configura\u00e7\u00e3o de rede capaz de representar o problema. \n\nAlgumas conclus\u00f5es que foram realizadas atrav\u00e9s dos casos de reservat\u00f3rio foram: \n\n? T\u00e9cnica de amostragem: a t\u00e9cnica do Hipercubo Latino (HL) proporcionou melhores \n\nresultados comparativamente ao Box Behnken (BB). Uma poss\u00edvel explica\u00e7\u00e3o vem do \n\nfato do HL realizar amostragem em um intervalo cont\u00ednuo de valores, ao passo que o \n\nBB amostra apenas valores normalizados (m\u00ednimo, m\u00e9dio e m\u00e1ximo).  \n\n? Quantidade de amostras: no Caso 2B, houve melhora nos resultados com o aumento \n\nde 100 para 250 pontos, por\u00e9m, o mesmo n\u00e3o foi observado ao se aumentar de 250 \n\npara 396 pontos. Duas poss\u00edveis explica\u00e7\u00f5es s\u00e3o: ou o metamodelo gerado com 396 \n\npontos memorizou os resultados ou a qualidade dos resultados n\u00e3o segue uma rela\u00e7\u00e3o \n\nlinear com a quantidade de amostras, ou seja, seria necess\u00e1rio aumentar ainda mais a \n\nquantidade de amostras para observar melhora nos resultados. \n\n\n\n \n\n \n\n133 \n\n \n\n? Indicador de qualidade do metamodelo: foi utilizado o valor do coeficiente de \n\ncorrela\u00e7\u00e3o linear para escolher um dado metamodelo ap\u00f3s o treinamento para ser \n\nsubmetido ao processo de otimiza\u00e7\u00e3o. A avalia\u00e7\u00e3o da sua capacidade de representar o \n\nsimulador de escoamento, no entanto, deve ser realizada atrav\u00e9s da valida\u00e7\u00e3o com o \n\nsimulador, executando a simula\u00e7\u00e3o da resposta encontrada pelo metamodelo e \n\ncomparando as curvas de produ\u00e7\u00e3o e press\u00e3o simuladas com o hist\u00f3rico, pois, \n\ndependendo do conjunto de teste, um metamodelo com coeficiente de correla\u00e7\u00e3o \n\nlinear bom pode n\u00e3o proporcionar bons resultados.  \n\n? Configura\u00e7\u00e3o da RNA: nesse trabalho, mais especificamente no Caso 2B, foi testada a \n\nutiliza\u00e7\u00e3o de um metamodelo para representar cada po\u00e7o produtor e um metamodelo \n\npara representar todos os po\u00e7os produtores. Os resultados mostraram que a utiliza\u00e7\u00e3o \n\nde um metamodelo para representar todos os po\u00e7os produtores \u00e9 a melhor op\u00e7\u00e3o. O \n\nmotivo, conforme explicado no Subitem 6.7, reside principalmente no fato de que a \n\nutiliza\u00e7\u00e3o de um metamodelo possibilita que poss\u00edveis intera\u00e7\u00f5es que possam existir \n\nentre os po\u00e7os sejam captadas. \n\n? Atributos de entrada: a utiliza\u00e7\u00e3o dos atributos escolhidos segundo a an\u00e1lise de \n\nsensibilidade proporcionou resultados mais precisos, comparativamente \u00e0 utiliza\u00e7\u00e3o de \n\ntodos os dezesseis atributos do reservat\u00f3rio previamente escolhidos. Conforme \n\nmostrado no Subitem 6.7.1, a an\u00e1lise de sensibilidade possibilitou a elimina\u00e7\u00e3o de \n\natributos que n\u00e3o estavam exercendo grandes influ\u00eancias na produ\u00e7\u00e3o de \u00e1gua. \n\n? Metamodelo como substituto do simulador: foi mostrado que \u00e9 poss\u00edvel utilizar o \n\nmetamodelo como substituto do simulador em partes do processo em que n\u00e3o se \n\nrequer grande precis\u00e3o dos resultados, sendo que o controle de qualidade da resposta \n\n(valida\u00e7\u00e3o) pode dar mais confiabilidade aos resultados.  \n\n? Retreinamento: os resultados do retreinamento com HL100 pontos mostraram que a \n\nutiliza\u00e7\u00e3o de menos pontos no treinamento e um refinamento posterior pode ser uma \n\nboa op\u00e7\u00e3o. Com um total de 200 pontos foi poss\u00edvel obter \u00edndice de qualidade de \n\najuste semelhante ao obtido com o treinamento utilizando 250 pontos. \n\nA partir dos resultados obtidos conclui-se que a ferramenta pode ser utilizada em conjunto \n\ncom o simulador de escoamento no processo de ajuste de hist\u00f3rico, por\u00e9m, n\u00e3o \u00e9 recomend\u00e1vel a \n\n\n\n \n\n \n\n134 \n\n \n\nsua utiliza\u00e7\u00e3o como substituta do simulador no processo inteiro, pois por se tratar de um modelo \n\nsimplificado sempre h\u00e1 uma margem de erro envolvida. Assim, a ferramenta pode contribuir em \n\netapas do processo que n\u00e3o demandem grande precis\u00e3o dos resultados. Para avaliar a \n\nconfiabilidade dos resultados gerados pelo metamodelo deve-se realizar a valida\u00e7\u00e3o da resposta \n\nencontrada com o mesmo utilizando o simulador de escoamento. \n\nSugest\u00f5es para trabalhos futuros \n\nSegue abaixo algumas sugest\u00f5es que surgiram durante o desenvolvimento do trabalho, que \n\npodem contribuir para o prosseguimento da pesquisa: \n\n? Ferramenta de otimiza\u00e7\u00e3o: nesse trabalho foi utilizado o Algoritmo Gen\u00e9tico (AG) \n\napenas como ferramenta complementar ao estudo, sem se preocupar em obter a \n\nmelhor configura\u00e7\u00e3o ou a melhor forma de aplica\u00e7\u00e3o da ferramenta. Assim, a etapa de \n\notimiza\u00e7\u00e3o dos atributos utilizando o AG pode ser melhorada atrav\u00e9s de um estudo \n\nmais detalhado da ferramenta. Existem ainda diversas ferramentas que podem ser \n\nutilizadas no processo de otimiza\u00e7\u00e3o, que podem ser testadas e, possivelmente, ajudar \n\na melhorar os resultados. \n\n? Utiliza\u00e7\u00e3o de pesos: a utiliza\u00e7\u00e3o de uma Fun\u00e7\u00e3o Objetivo (FO) com peso para cada \n\nsa\u00edda ou a utiliza\u00e7\u00e3o de algum crit\u00e9rio que torne a influ\u00eancia maior dos po\u00e7os com \n\nmelhor desempenho pode contribuir para obten\u00e7\u00e3o de melhores ajustes. \n\n? Pr\u00e9-otimiza\u00e7\u00e3o: uma op\u00e7\u00e3o interessante seria a realiza\u00e7\u00e3o de uma pr\u00e9-otimiza\u00e7\u00e3o \n\nantes de amostrar os dados. O procedimento consistiria em realizar uma otimiza\u00e7\u00e3o \n\nr\u00e1pida, com poucas simula\u00e7\u00f5es, de forma que se reduza o espa\u00e7o de busca dos \n\natributos. Esse resultado poderia ser utilizado para forma\u00e7\u00e3o do conjunto de entrada \n\npara treinamento das RNA. \n\n? Coeficiente de correla\u00e7\u00e3o: conforme discutido no Cap\u00edtulo 6 de resultados, s\u00e3o \n\nnecess\u00e1rios crit\u00e9rios mais consistentes para escolha dos melhores metamodelos \n\ngerados para serem utilizados na otimiza\u00e7\u00e3o. A utiliza\u00e7\u00e3o apenas do coeficiente de \n\ncorrela\u00e7\u00e3o linear entre sa\u00eddas do simulador e do metamodelo n\u00e3o foi capaz de refletir \n\ncom precis\u00e3o os resultados obtidos a partir da valida\u00e7\u00e3o com o simulador, ou seja, \n\nmesmo um metamodelo com coeficiente de correla\u00e7\u00e3o bom pode n\u00e3o proporcionar \n\n\n\n \n\n \n\n135 \n\n \n\nbons resultados na regi\u00e3o do m\u00ednimo. Uma op\u00e7\u00e3o seria utilizar o procedimento \n\ndescrito por Lima et al. (2009) de avaliar os resultados em regi\u00f5es espec\u00edficas. \n\n? Outros tipos de RNA: existem ainda outros tipos de RNA que podem ser utilizadas \n\npara gerar os metamodelos, sendo uma delas as redes radiais. Mesmo sobre as redes \n\ndo tipo direta (FeedForward) existem ainda diversas configura\u00e7\u00f5es, como por \n\nexemplo, ao inv\u00e9s de representar o afastamento do modelo com rela\u00e7\u00e3o ao hist\u00f3rico, \n\npoderia gerar na sa\u00edda, os valores de produ\u00e7\u00e3o do po\u00e7o (curva).  \n\n? Retreinamento: outra metodologia para retreinamento poderia ser utilizada, que \n\nconsistiria em gerar o metamodelo com poucos pontos, realizar a otimiza\u00e7\u00e3o \n\nutilizando o metamodelo, validar o resultado, e ir adicionando mais pontos aos poucos \n\niterativamente, repetindo o processo (gera\u00e7\u00e3o do metamodelo, otimiza\u00e7\u00e3o e \n\nvalida\u00e7\u00e3o). A escolha por adicionar mais pontos pode ser, ou em todo o espa\u00e7o de \n\nbusca dos par\u00e2metros ou em regi\u00f5es espec\u00edficas, a variar de acordo com o resultado \n\nobtido na valida\u00e7\u00e3o. Atrav\u00e9s dessa metodologia pode-se chegar a uma quantidade \n\nreduzida de amostras que proporcionem o desempenho desejado do metamodelo sobre \n\no problema em estudo. \n\n? Escolha dos melhores metamodelos ap\u00f3s o treinamento: nesse trabalho foi escolhido \n\num entre diversos metamodelos que resultaram em coeficiente de correla\u00e7\u00e3o linear \n\nbom. Outra possibilidade, contudo, seria utilizar diversos metamodelos com \n\ncoeficientes de correla\u00e7\u00e3o linear bom e valid\u00e1-los com o simulador, ap\u00f3s a otimiza\u00e7\u00e3o.\n\n\n\n\n\n \n\n \n\n137 \n\n \n\nREFER\u00caNCIAS BIBLIOGR\u00c1FICAS \n\nAl-THUWAINI, J. S., SAUDI ARAMCO, ZANGL, G. e PHELPS, R., Innovative Approach to \n\nAssist History Matching Using Artificial Intelligence. SPE Intelligent Energy Conference and \n\nExhibition, paper number SPE99882, Amsterdam, Netherlands, 11 \u2013 13 April, 2006. \n\nAVANSI, G. D., Uso de Metamodelos na Sele\u00e7\u00e3o de Estrat\u00e9gias de Produ\u00e7\u00e3o e Avalia\u00e7\u00e3o \n\nEcon\u00f4mica de Campos de Petr\u00f3leo. Campinas, 2008. 156pp. Disserta\u00e7\u00e3o (Mestrado em \n\nCi\u00eancias e Engenharia de Petr\u00f3leo) \u2013 Faculdade de Engenharia Mec\u00e2nica e Instituto de \n\nGeoci\u00eancias, Universidade Estadual de Campinas \u2013 UNICAMP. \n\nAZIZ, K. e SETTARI, A., Petroleum Reservoir Simulation. Applied Science Publishers LTD, \n\nLondon, 1979. \n\nBISHOP, C. M., Neural Networks for Pattern Recognition. Oxford University Press Inc., New \n\nYork, 1995. \n\nBOX, G. E. P. e BEHNKEN, D. W., Some New Three Level Design for the Study of \n\nQuantitative Variables. Technometrics, Vol. 2, No. 4, p.455-475, 1960. \n\nBRATLEY, P. e FOX, B. L., Algorithm 659 Implementing Sobol\u2019s Quasirandom Sequence \n\nGenerator. ACM Transactions on Mathematical Software, Vol. 14, No. 1, p. 88-100, 1988. \n\nCARVALHO, C. P. V., MASCHIO, C. E SCHIOZER, D., Aplica\u00e7\u00e3o da T\u00e9cnica de Hipercubo \n\nLatino na Integra\u00e7\u00e3o do Ajuste de Hist\u00f3rico com a An\u00e1lise de Incertezas. 5\u00ba Congresso \n\nBrasileiro de Pesquisa e Desenvolvimento em Petr\u00f3leo e G\u00e1s, Fortaleza, Cear\u00e1, Brasil, 15 a 22 de \n\nOutubro, 2009. \n\nCONSENTINO, L., Integrated Reservoir Studies. Editions Technip, Paris, 2001. \n\n\n\n \n\n \n\n138 \n\n \n\nCULLICK, A. S., JOHNSON, D. e SHI, G., Improved and More-Rapid History Matching \n\nwith a Nonlinear Proxy and Global Optimization. SPE Annual Technical Conference and \n\nExhibition, paper number SPE101933, San Antonio, Texas, U. S. A., 24-27 September, 2006. \n\nDEMUTH, F., BEALE, M. e HAGAN, M., Neural Network Toolbox\nTM\n\n 6 \u2013 User\u2019s Guide. The \n\nMathWorks Inc., 2010. \n\nDORAISAMY, H., ERTEKIN, T. e GRADER, A. S., Field Development Studies by Neuro-\n\nSimulation: an Effective Coupling of Soft and Hard Computing Protocols. Science Direct, \n\nComputer &amp; Geosciences 26, p.963-973, 2000. \n\nELPHICK, R. Y., Facies. Reservoir Characterization Module, Schlumberger Oilfield Glossary, \n\nhttp://www.glossary.oilfield.slb.com/search.cfm, acesso em 01/06/2012, \u00e0s 22h21min. \n\nERTEKIN, T., ABOU-KASSEM, J. H. e KING, G. R., Basic Applied Reservoir Simulation. \n\nSociety of Petroleum Engineering Inc., Texas, USA, 2001. \n\nFERREIRA, S. L. C., BRUNS, R. E., FERREIRA, H. S., MATOS, G. D., DAVID, J. M., \n\nBRAND\u00c3O, G. C., da SILVA, E. G. D., PORTUGAL, L. A., dos REIS, P. S., SOUZA, A. S. e \n\ndos SANTOS W. N. L., Box-Behnken: An Alternative for the Optimization of Analytical \n\nMethods. Science Direct, Analytica Chimica Acta 597, p179-186, 2007. \n\nFORESEE, F. D. e HAGAN, M. T., Gauss-Newton Approximation to Bayesian Learning. \n\nProceendings of the International Joint Conference on Neural Networks, 1997. \n\nFROTA, A. E. F., Aplica\u00e7\u00e3o de Op\u00e7\u00f5es Americanas Tradicionais e Complexas. Rio de \n\nJaneiro, 2003. Disserta\u00e7\u00e3o (mestrado em Engenharia de Produ\u00e7\u00e3o: Finan\u00e7as e An\u00e1lise de \n\nInvestimentos). Departamento de Engenharia Industrial, Pontif\u00edcia Universidade Cat\u00f3lica.   \n\nHAGAN, M. T., DEMUTH, H. B. e BEALE M., Neural Network Design.  PWS Publishing \n\nCompany, a division of Thompson Learning, United States of America, 1996. \n\n\n\n \n\n \n\n139 \n\n \n\nHAGAN, M. T. e MENHAJ, M. B., Training Feedforward Networks with the Marquardt \n\nAlgorithm. IEEE Transactions on Neural Networks, Vol.5, No. 6, p.989-993, November 1994. \n\nHIRSCHEN, K. e SCH\u00c4FER, M., Bayesian Regularization Neural Networks for Optimizing \n\nFluid Flow Processes. Science Direct , Comput. Methods Appl. Mech. Engrg. 195, p.481-500, \n\n2006. \n\nLEIT\u00c3O, H. C. e SCHIOZER, D. J., Ajuste de Hist\u00f3rico Automatizado Atrav\u00e9s de \n\nOtimiza\u00e7\u00e3o Multivariada e Paraleliza\u00e7\u00e3o Direta.  Rio Oil &amp; Gas Conference, paper n\u00famero \n\nIBP25498, Rio de Janeiro, Rio de Janeiro, Brasil, 05 a 08 de Outubro, 1998. \n\nLIMA, A., RISSO, F. V. A. e SCHIOZER, D. J., Uso de Meta-Modelos Gerados por \n\nPlanejamento Estat\u00edstico no Ajuste de Hist\u00f3rico de Produ\u00e7\u00e3o de Campos de Petr\u00f3leo. 5\u00ba \n\nCongresso Brasileiro de P&amp;D em Petr\u00f3leo e G\u00e1s, Fortaleza, Cear\u00e1, Brasil, 15 a 22 de Outubro, \n\n2009.  \n\nMASCHIO, C., dos SANTOS, A. A. e SCHIOZER, D. J., Aplica\u00e7\u00e3o do M\u00e9todo Simplex no \n\nProcesso de Ajuste de Hist\u00f3rico Assistido. Rio Oil &amp; Gas Expo and Conference, paper n\u00famero \n\nIBP1343_06, Rio de Janeiro, Rio de Janeiro, Brasil, 11 a 14 de Setembro, 2006. \n\nMASCHIO, C., NAKAJIMA, L. e SCHIOZER, D. J., Uso de Redes Neurais Artificiais no \n\nProcesso de Ajuste de Hist\u00f3rico de Produ\u00e7\u00e3o.  Rio Oil &amp; Gas 2008 Expo and Conference, \n\npaper n\u00famero IBP2444_08, Rio de Janeiro, Rio de Janeiro, Brasil, 15 a 18 de Setembro, 2008. \n\nMASCHIO, C. e SCHIOZER, D. J., Ajuste de Hist\u00f3rico Assistido Usando M\u00e9todos de \n\nOtimiza\u00e7\u00e3o de Busca Direta. Rio Oil &amp; Gas Expo and Conference, paper n\u00famero IBP06204, \n\nRio de Janeiro, Rio de Janeiro, Brasil, 04 a 07 de Outubro, 2004. \n\nMASCHIO, C. e SCHIOZER, D. J., Compara\u00e7\u00e3o entre Metodologia de Otimiza\u00e7\u00e3o Global e \n\nM\u00e9todo de Gradientes para Ajuste de Hist\u00f3rico Assistido. 3\u00ba Congresso Brasileiro de P&amp;D \n\nem Petr\u00f3leo e G\u00e1s, Salvador, Bahia, Brasil, 02 a 05 de Outubro, 2005.  \n\n\n\n \n\n \n\n140 \n\n \n\nMASON, R. L., GUNST, R. F. e HESS, J. L., Statistical Design and Analysis of Experiments \n\nWith Application to Engineering and Science. Second edition, John Wiley &amp; Sons Inc., \n\nHoboken, New Jersey, 2003. \n\nMCKAY, M. D., BECKMAN, R. J. e CONOVER, W. J., A Comparison of Three Methods for \n\nSelecting Values of Input Variables in the Analysis of Output from a Computer Code. \n\nTechnometrics, Vol. 21, No. 2, 1979. \n\nMOHAGHEGH, S., Virtual-Intelligence Applications in Petroleum Engineering: Part I \u2013 \n\nArtificial Neural Networks.  Society of Petroleum Engineers, SPE58046, September, 2000.  \n\nMONTGOMERY, D. C., Design and Analysis of Experiments. 4\nth\n\n Ed. John Wiley &amp; Sons Inc., \n\nU.S.A., 1996.   \n\nNGUYEN, D. e WIDROW, B., Improving the Learning Speed of 2-Layer Neural Networks \n\nby Choosing Initial Values of the Adaptive Weights. Proceedings of the IJCNN, Vol. 3, p. 21-\n\n26, Julho, 1990. \n\nPINHEIRO, J. M., Redes Neurais Artificiais. http://www.din.uem.br/~jmpinhei/IA-\n\nCC/08Redes%20Neurais%20Artificiais.pdf, acesso em 27 de Abril de 2011 \u00e0s 15h20.  \n\nRAMGULAM, A., ERTEKIN, T. e FLEMINGS, P., Utilization of Artificial Neural Networks \n\nin the Optimization of History Matching. SPE Latin American and Caribbean Petroleum \n\nEngineering Conference, paper number SPE107568, Buenos Aires, Argentina, 15-18 April, 2007. \n\nRISSO, F. V. A., RISSO, V. F. e SCHIOZER, D. J., Estudo de Influ\u00eancia de Tratamento de \n\nAtributos em An\u00e1lise de risco usando Planejamento Estat\u00edstico e Superf\u00edcie de Resposta. \n\nXXVII CILAMCE, Bel\u00e9m, Par\u00e1, Brasil, 03-06 Setembro, 2006. \n\nhttp://www.din.uem.br/~jmpinhei/IA-CC/08Redes%20Neurais%20Artificiais.pdf\nhttp://www.din.uem.br/~jmpinhei/IA-CC/08Redes%20Neurais%20Artificiais.pdf\n\n\n \n\n \n\n141 \n\n \n\nRISSO, F. V. A., RISSO, V. F. e SCHIOZER, D. J., A Influ\u00eancia do Tipo de Distribui\u00e7\u00e3o dos \n\nAtributos Cr\u00edticos na Obten\u00e7\u00e3o da Curva de Risco Utilizando Planejamento Estat\u00edstico . \n\nXXVII CILAMCE, Porto, Portugal, 13-15 junho, 2007. \n\nSAMPAIO, T. P., FERREIRA FILHO, V. J. M. e de SA NETO, A., An Application of Feed \n\nForward Neural Network as Nonlinear Proxies for the Use During the History Matching \n\nPhase. SPE Latin American and Caribbean Petroleum Engineering Conference, paper number \n\nSPE122148, Cartagena, Colombia, 31 May-3 June April, 2009. \n\nSANTOS, J. P. M. e SCHIOZER, D. J., Determina\u00e7\u00e3o de Metodologia de Ajuste \n\nAutomatizado de Hist\u00f3rico. Rio Oil &amp; Gas Conference, paper n\u00famero IBP19300, Rio de \n\nJaneiro, Rio de Janeiro, Brasil, 16 a 19 de Outubro, 2000. \n\nSCHIOZER, D. J., SOUSA, S. H. G. e MASCHIO, C., Ajuste de Hist\u00f3rico de Produ\u00e7\u00e3o \n\nAssistido. Boletim T\u00e9cnico da Produ\u00e7\u00e3o de Petr\u00f3leo, volume 3, n\u00ba 1, p. 63-82, Rio de Janeiro, \n\nRio de Janeiro, Brasil, 2009. \n\nSCHULZE-RIEGERT, R. E GHEDAN, S., Modern Techniques for History Matching. 9\nth\n\n \n\nInternational Forum on Reservoir Simulation, Abud Dhabi, United Arab Emirates, 9-13 \n\nDecember, 2007. \n\nSILVA, P. C., MASCHIO, C. e SCHIOZER, D. J., Use of Neuro-Simulation Techniques as \n\nProxies to Reservoir Simulation: Application in Production History Matching. Science \n\nDirect, Journal of Petroleum Science and Engineering 57, p.273-280, 2006. \n\nSILVA, E. e OLIVEIRA, A. C., Dicas para a Configura\u00e7\u00e3o de Redes Neurais. Universidade \n\nFederal do Rio de Janeiro - NCE, 2004. \n\nTHE MATHWORKS, MATLAB7 - Data Analysis. The MathWorks Inc., 2007. \n\n\n\n \n\n \n\n142 \n\n \n\nVON ZUBEN, F. J., Algoritmos Gen\u00e9ticos (AG\u2019s). \n\nftp://ftp.dca.fee.unicamp.br/pub/docs/vonzuben/ia707_01/topico6_01.pdf, \n\nDCA/FFEC/UNICAMP, acesso em 28 de Maio de 2011, \u00e0s 13h44. \n\nZANGL, G., GRAF, T. e Al-Kinani, A., Proxy Modeling in Production Optimization. SPE \n\nEuropec/EAGE Annual Conference and Exhibition, paper number SPE100131, Vienna, Austria, \n\n12-15 June, 2006. \n\nZUBAREV, D. I., Pros and Cons of Applying Proxy-Models as a Substitute for Full \n\nReservoir Simulations. SPE Annual Technical Conference and Exhibition, paper number \n\nSPE124815, New Orleans, Louisiana, USA, 4-7 October, 2009. \n\n\n\n \n\n \n\n143 \n\n \n\nAP\u00caNDICE \n\nI. Capacidade de extrapola\u00e7\u00e3o da rede neural artificial \u2013 Caso 2A \n\nPara avaliar a capacidade de extrapola\u00e7\u00e3o foi utilizado o metamodelo Caso2A_HL50_novo \n\n(resultante do retreinamento do metamodelo gerado com HL50 pontos) e, como entrada, o \n\nconjunto de 25 pontos utilizados no novo treinamento e o conjunto de teste utilizado para \n\ntreinamento das RNA para todo o espa\u00e7o de busca dos atributos. O resultado \u00e9 indicado na Figura \n\nI.1, que mostra o gr\u00e1fico de dispers\u00e3o entre valores de afastamentos obtidos com a sa\u00edda do \n\nsimulador e valores de afastamentos gerados pelo metamodelo, sendo os pontos em azul relativos \n\naos dados que respeitam a faixa de varia\u00e7\u00e3o dos atributos (conjunto utilizado no retreinamento) e \n\nos pontos em vermelho relativos aos dados que ultrapassam os limites de varia\u00e7\u00e3o dos atributos \n\n(utilizados para treinar as RNA em todo o espa\u00e7o de busca dos atributos). Na Figura I.1 (a) \u00e9 \n\nmostrado o gr\u00e1fico para todos os pontos e na Figura I.1 (b) \u00e9 mostrado o gr\u00e1fico numa \u00e1rea mais \n\npr\u00f3xima do m\u00ednimo. \n\n \n\n(a) (b) \n\nFigura I.1\u2013 Exemplo sobre capacidade de extrapola\u00e7\u00e3o da RNA. Em (a) s\u00e3o mostrados todos os \n\npontos; em (b) \u00e9 mostrada uma \u00e1rea menor, mais pr\u00f3xima do m\u00ednimo. \n\nComo pode ser observado, quando inseridos atributos de entrada fora dos limites de \n\nvaria\u00e7\u00e3o do treinamento o metamodelo n\u00e3o extrapola os resultados, saturando em um valor \n\n2 4 6 8\n\nx 10\n8\n\n2\n\n4\n\n6\n\n8\n\nx 10\n8\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\nd\ne\nlo\n\n (\nY\n\n)\n\nCaso 2A - metamdodelo HL50 - novo\n\n \n\n \n\nY=T\n\nDados de treino - HL 25\n\nDados de teste - HL 25\n\n1 2 3 4 5\n\nx 10\n7\n\n1\n\n2\n\n3\n\n4\n\n5\n\nx 10\n7\n\nSa\u00edda do simulador (T)\n\nS\na\n\u00edd\n\na\n d\n\no\n m\n\ne\nta\n\nm\no\n\nd\ne\nlo\n\n (\nY\n\n)\n\n \n\n \nCaso 2A - metamodelo HL50 - novo\n\nY=T\n\nDados de treino - HL 25\n\nDados de teste - HL 25\n\n\n\n \n\n \n\n144 \n\n \n\npr\u00f3ximo ao m\u00e1ximo e m\u00ednimo contidos no conjunto de treinamento. Dessa maneira, pode-se \n\ndeduzir que esse tipo de RNA n\u00e3o possui a capacidade de extrapolar os resultados em regi\u00f5es \n\npara as quais ela n\u00e3o foi treinada.  \n\nAo definir um novo conjunto de treinamento, reduziu-se a regi\u00e3o de busca dos atributos e, \n\nconsequentemente, o intervalo de varia\u00e7\u00e3o da sa\u00edda do metamodelo. Com isso, o metamodelo \n\ngerado com o novo treinamento se limita a representar apenas a nova regi\u00e3o espec\u00edfica para o \n\nqual foi gerado, n\u00e3o sendo capaz de extrapolar os resultados para vari\u00e1veis de entrada e sa\u00edda fora \n\ndos limites de varia\u00e7\u00e3o.  \n\nDessa maneira caso se deseje utilizar o metamodelo para realizar uma tarefa diferente da \n\nqual ele foi gerado \u00e9 necess\u00e1rio realizar novamente o treinamento para o prop\u00f3sito espec\u00edfico. \n\nII. Melhor configura\u00e7\u00e3o de rede neural artificial obtida \u2013 Caso 2B \n\nAp\u00f3s gerar os metamodelos e realizar a otimiza\u00e7\u00e3o com cada um deles, o modelo de \n\nsimula\u00e7\u00e3o resultante de cada otimiza\u00e7\u00e3o foi simulado utilizando o simulador de escoamento, de \n\nforma a obter uma valida\u00e7\u00e3o final da qualidade do metamodelo gerado. Atrav\u00e9s dessa valida\u00e7\u00e3o \n\np\u00f4de-se identificar qual foi o melhor metamodelo obtido atrav\u00e9s dos diversos treinamentos de \n\nRNA realizados. \n\nA Tabela II.1 mostra a m\u00e9dia dos afastamentos dos oito po\u00e7os produtores, obtidos atrav\u00e9s \n\nda simula\u00e7\u00e3o do modelo resultante da otimiza\u00e7\u00e3o utilizando os metamodelos com BB396, \n\nHL396, HL250 e HL100 pontos com as configura\u00e7\u00f5es PROD (um \u00fanico metamodelo \n\nrepresentando todos os po\u00e7os, ou seja, com oito sa\u00eddas), PP (um metamodelo para representar um \n\npo\u00e7o, ou seja, oito metamodelos), SAS (utiliza\u00e7\u00e3o de todos os dezesseis atributos incertos como \n\nentrada) e AS (utiliza\u00e7\u00e3o apenas dos atributos que impactam mais na produ\u00e7\u00e3o de \u00e1gua, definidos \n\nde acordo com a an\u00e1lise de sensibilidade). A linha \u201cMetamodelo\u201d mostra o afastamento obtido \n\ncom a simula\u00e7\u00e3o utilizando o metamodelo, e a linha \u201cSimulador\u201d mostra o afastamento obtido \n\ncom a simula\u00e7\u00e3o utilizando o simulador de escoamento. \n\n \n\n \n\n\n\n \n\n \n\n145 \n\n \n\nTabela II.1\u2013M\u00e9dia dos afastamentos para os modelos de simula\u00e7\u00e3o encontrados atrav\u00e9s da \n\notimiza\u00e7\u00e3o utilizando o metamodelo \u2013 Caso 2B. \n\nM\u00e9dia dos afastamentos dos oito po\u00e7os (x10\n6\n) \n\nNome  \nTipo de metamodelo \n\nPP_AS PP_SAS PROD_AS PROD_SAS \n\nBB396 \nMetamodelo 1.140 0.735 2.094 1.744 \n\nSimulador 3.306 2.480 1.395 1.345 \n\nHL396 \nMetamodelo 1.006 1.097 2.059 1.811 \n\nSimulador 2.580 1.991 2.172 3.248 \n\nHL250 \nMetamodelo 1.406 1.112 2.448 2.092 \n\nSimulador 1.314 3.063 0.851 2.742 \n\nHL100 \nMetamodelo 1.865 0.966 2.397 1.888 \n\nSimulador 1.647 2.616 1.632 1.278 \n\n \n\nDe acordo com a Tabela II.1, o metamodelo que proporcionou a menor m\u00e9dia dos \n\nafastamentos ap\u00f3s simular o modelo no simulador de escoamento foi o metamodelo HL250 com \n\nconfigura\u00e7\u00e3o PROD_AS (destacada em negrito). Isso significa que ele foi o que conseguiu \n\nconduzir a otimiza\u00e7\u00e3o para mais perto do m\u00ednimo. \n\nO procedimento de an\u00e1lise de sensibilidade possibilitou eliminar atributos com pouca \n\ninflu\u00eancia sobre a Fun\u00e7\u00e3o Objetivo (FO), de forma a reduzir o n\u00famero de atributos de entrada que \n\ncontribuem apenas para aumentar o n\u00edvel de dificuldade do processo de treinamento. Al\u00e9m disso, \n\na utiliza\u00e7\u00e3o de um metamodelo para representar as oito sa\u00eddas fez com que poss\u00edveis inter-\n\nrela\u00e7\u00f5es existentes entre os po\u00e7os fossem captadas, pois com essa configura\u00e7\u00e3o qualquer \n\naltera\u00e7\u00e3o realizada nos par\u00e2metros da rede interfere em todas as sa\u00eddas. Em contrapartida, com a \n\nutiliza\u00e7\u00e3o de um metamodelo para representar cada po\u00e7o n\u00e3o \u00e9 poss\u00edvel captar eventuais inter-\n\nrela\u00e7\u00f5es."}]}}}