{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.14275"}, {"@name": "filename", "#text": "20569_DM_FernandoMeireles_2014_MEEC.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "Instituto Superior de Engenharia do Porto\nDepartamento de Engenharia Eletrot\u00e9cnica\n\nRua Dr. Ant\u00f3nio Bernardino de Almeida, 431, 4200-072 Porto\n\nIntegrated Management of\nCloud Computing Resources\n\nDissertation submitted to the Instituto Superior de Engenharia do Porto in partial\nfulfilment of the requirements for the degree of Master of Science in Electrical and\n\nComputers Engineering - Major in Telecommunications\n\nFernando Miguel Dias Meireles\n\nSupervisor: Prof. Maria Benedita Campos Neves Malheiro\nCo-supervisor: Prof. Pedro Jo\u00e3o De-Francesco Assis\n\nAcademic Year: 2013-2014\n\n\n\n\n\nTo my family.\n\n\n\n\n\nAbstract\n\nLunacloud is a cloud service provider with offices in Portugal, Spain, France and\nUK that focus on delivering reliable, elastic and low cost cloud Infrastructure as\na Service (IaaS) solutions. The company currently relies on a proprietary IaaS\nplatform - the Parallels Automation for Cloud Infrastructure (PACI) - and wishes\nto expand and integrate other IaaS solutions seamlessly, namely open source\nsolutions. This is the challenge addressed in this thesis. This proposal, which\nwas fostered by Eurocloud Portugal Association, contributes to the promotion of\ninteroperability and standardisation in Cloud Computing.\n\nThe goal is to investigate, propose and develop an interoperable open source\nsolution with standard interfaces for the integrated management of IaaS Cloud\nComputing resources based on new as well as existing abstraction libraries or\nframeworks. The solution should provide both Web and application programming\ninterfaces.\n\nThe research conducted consisted of two surveys covering existing open source\nIaaS platforms and PACI (features and API) and open source IaaS abstraction\nsolutions. The first study was focussed on the characteristics of most popular\nopen source IaaS platforms, namely OpenNebula, OpenStack, CloudStack and\nEucalyptus, as well as PACI and included a thorough inventory of the provided\nApplication Programming Interfaces (API), i.e., offered operations, followed by\na comparison of these platforms in order to establish their similarities and dis-\nsimilarities. The second study on existing open source interoperability solutions\nincluded the analysis of existing abstraction libraries and frameworks and their\ncomparison.\n\nThe approach proposed and adopted, which was supported on the conclusions\nof the carried surveys, reuses an existing open source abstraction solution - the\nApache Deltacloud framework. Deltacloud relies on the development of software\ndriver modules to interface with different IaaS platforms, officially provides and\nsupports drivers to sixteen IaaS platform, including OpenNebula and OpenStack,\nand allows the development of new provider drivers. The latter functionality\n\nv\n\n\n\nwas used to develop a new Deltacloud driver for PACI. Furthermore, Deltacloud\nprovides a Web dashboard and REpresentational State Transfer (REST) API\ninterfaces.\n\nTo evaluate the adopted solution, a test bed integrating OpenNebula, Open-\nStack and PACI nodes was assembled and deployed. The tests conducted involved\ntime elapsed and data payload measurements via the Deltacloud framework as\nwell as via the pre-existing IaaS platform API. The Deltacloud framework be-\nhaved as expected, i.e., introduced additional delays, but no substantial over-\nheads. Both the Web and the REST interfaces were tested and showed identical\nmeasurements.\n\nThe developed interoperable solution for the seamless integration and provi-\nsion of IaaS resources from PACI, OpenNebula and OpenStack IaaS platforms\nfulfils the specified requirements, i.e., provides Lunacloud with the ability to ex-\npand the range of adopted IaaS platforms and offers a Web dashboard and REST\nAPI for the integrated management. The contributions of this work include the\nsurveys and comparisons made, the selection of the abstraction framework and,\nlast, but not the least, the PACI driver developed.\n\n\n\nResumo\n\nA Lunacloud \u00e9 uma empresa provedora de servi\u00e7os de computa\u00e7\u00e3o em nuvem com\nescrit\u00f3rios em Portugal, Espanha, Fran\u00e7a e Reino Unido e focada no provisiona-\nmento de servi\u00e7os ao n\u00edvel da infraestrutura (IaaS). Atualmente, esta empresa de-\npende de uma plataforma de IaaS propriet\u00e1ria \u2013 Parallels Automation for Cloud\nInfrastructure (PACI) \u2013 e pretende expandir a oferta de servi\u00e7os atrav\u00e9s da in-\ntegra\u00e7\u00e3o de novas plataformas de IaaS, nomeadamente do tipo open-source. Este\n\u00e9 o desafio proposto para a presente tese. Esta proposta encontra-se enquadrada\nnos esfor\u00e7os de promo\u00e7\u00e3o da normaliza\u00e7\u00e3o e interoperabilidade da Associa\u00e7\u00e3o\nEuroCloud Portugal.\n\nO objetivo do trabalho consiste em investigar, propor e desenvolver uma\nsolu\u00e7\u00e3o interoper\u00e1vel, baseada na utiliza\u00e7\u00e3o de ferramentas ou bibliotecas de\nabstrac\u00e7\u00e3o open-source existentes ou a desenvolver, que apresente interfaces nor-\nmalizadas para a gest\u00e3o integrada de recursos de computa\u00e7\u00e3o em nuvem ao n\u00edvel\nda infraestrutura. A solu\u00e7\u00e3o desenvolvida dever\u00e1 incluir interfaces Web e de\nprograma\u00e7\u00e3o de aplica\u00e7\u00f5es.\n\nO trabalho de investiga\u00e7\u00e3o realizado foi composto por dois estudos referentes\n\u00e0 caracteriza\u00e7\u00e3o e an\u00e1lise das interfaces das plataformas de IaaS (plataforma pro-\npriet\u00e1ria PACI e plataformas open-source) selecionadas e solu\u00e7\u00f5es open-source de\nabstrac\u00e7\u00e3o de plataformas de IaaS existentes. O primeiro estudo caracterizou as\nplataformas de IaaS open-source mais conhecidas, nomeadamente, OpenNebula,\nOpenStack, CloudStack e Eucalyptus, assim como a plataforma de IaaS propri-\net\u00e1ria (usada pela Lunacloud) PACI. Foi tamb\u00e9m inclu\u00eddo neste estudo um in-\nvent\u00e1rio extensivo e compara\u00e7\u00e3o das respetivas bibliotecas de interface de forma a\nidentificar similaridades e diferen\u00e7as. O segundo estudo contemplou as solu\u00e7\u00f5es\nde interoperabilidade open-source existentes, incluindo a an\u00e1lise e compara\u00e7\u00e3o\ndas ferramentas e bibliotecas de abstrac\u00e7\u00e3o.\n\nA abordagem escolhida e adoptada, que se suportou nas conclus\u00f5es dos estudos\nrealizados, reutiliza uma solu\u00e7\u00e3o de abstrac\u00e7\u00e3o j\u00e1 existente \u2013 o framework de ab-\nstrac\u00e7\u00e3o Deltacloud. Esta ferramenta recorre a m\u00f3dulos de software \u2013 drivers \u2013\n\nvii\n\n\n\npara interagir com as diferentes plataformas de IaaS, fornece oficialmente drivers\npara dezasseis plataformas de IaaS, incluindo as plataformas OpenNebula e Open-\nStack, e permite o desenvolvimento de novos drivers para a integra\u00e7\u00e3o de novos\nfornecedores/plataformas de IaaS. Esta \u00faltima funcionalidade foi utilizada para\ncriar um novo driver para a plataforma de IaaS propriet\u00e1ria PACI. Para al\u00e9m\ndas funcionalidades referidas, a ferramenta Deltacloud, tamb\u00e9m fornece um Web\ndashboard e bibliotecas de interface REpresentational State Transfer (REST).\n\nDe forma a avaliar a solu\u00e7\u00e3o desenvolvida, foi montada uma plataforma de\ntestes integrando as plataformas OpenNebula, OpenStack, CloudStack e PACI.\nOs testes realizados consistiram na medi\u00e7\u00e3o dos tempos de resposta e da quan-\ntidade de informa\u00e7\u00e3o trocada durante a invoca\u00e7\u00e3o das opera\u00e7\u00f5es atrav\u00e9s da ferra-\nmenta Deltacloud e diretamente \u00e0s bibliotecas de interface das respectivas plata-\nformas de IaaS. A ferramenta Deltacloud comportou-se como era esperado, i.e.,\no tempo de execu\u00e7\u00e3o das opera\u00e7\u00f5es sofreu atrasos adicionais e a quantidade de\ninforma\u00e7\u00e3o trocada manteve-se em geral. Ambas as interfaces Web e REST foram\ntestadas, apresentando medidas similares.\n\nA solu\u00e7\u00e3o de interoperabilidade desenvolvida para a integra\u00e7\u00e3o e disponibil-\niza\u00e7\u00e3o de recursos de IaaS, atrav\u00e9s das plataformas PACI, OpenNebula e Open-\nStack, cumpre os requisitos especificados, i.e., permite que a empresa Lunacloud\nexpanda a gama de plataformas de IaaS adoptadas, oferecendo um Web dash-\nboard e uma biblioteca de interface para a gest\u00e3o integrada dos recursos de IaaS.\nAs contribui\u00e7\u00f5es deste trabalho incluem os estudos e compara\u00e7\u00f5es realizados, a\nselec\u00e7\u00e3o da ferramenta de abstrac\u00e7\u00e3o e, por \u00faltimo, mas n\u00e3o menos importante,\no driver desenvolvido para a plataforma PACI.\n\n\n\nContents\n\nContents i\n\nList of Figures xiii\n\nList of Tables xv\n\nList of Code Snippets xvii\n\nAcronyms xix\n\n1 Introduction 1\n1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.3 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.4 Functional Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.5 Expected Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.6 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.7 Work Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.8 Structure of the Dissertation . . . . . . . . . . . . . . . . . . . . . 5\n\n2 Cloud Computing 7\n2.1 Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 Deployment Models . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n2.2.1 Private Clouds . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2.2 Community Clouds . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2.3 Public Clouds . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.2.4 Hybrid Clouds . . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n2.3 Service Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.3.1 Infrastructure as a Service . . . . . . . . . . . . . . . . . . . 16\n2.3.2 Platform as a Service . . . . . . . . . . . . . . . . . . . . . 16\n2.3.3 Software as a Service . . . . . . . . . . . . . . . . . . . . . . 17\n\ni\n\n\n\nii CONTENTS\n\n2.4 Characteristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.4.1 On-demand Self-service . . . . . . . . . . . . . . . . . . . . 18\n2.4.2 Broad Network Access . . . . . . . . . . . . . . . . . . . . . 18\n2.4.3 Resource Pooling . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4.4 Rapid Elasticity . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.4.5 Measured Service . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n2.5 Concerns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.5.1 Unpredictable Performance . . . . . . . . . . . . . . . . . . 19\n2.5.2 Security Risks . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.5.3 Data Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n2.6 Virtualization Technologies . . . . . . . . . . . . . . . . . . . . . . 22\n2.6.1 Full Virtualization . . . . . . . . . . . . . . . . . . . . . . . 23\n2.6.2 Para-virtualization . . . . . . . . . . . . . . . . . . . . . . . 24\n2.6.3 OS-level Virtualization . . . . . . . . . . . . . . . . . . . . . 25\n\n2.7 Business Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.7.1 IaaS Business Model . . . . . . . . . . . . . . . . . . . . . . 27\n2.7.2 PaaS Business Model . . . . . . . . . . . . . . . . . . . . . . 27\n2.7.3 SaaS Business Model . . . . . . . . . . . . . . . . . . . . . . 28\n2.7.4 Stakeholders . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.7.5 Service Life-Cycle . . . . . . . . . . . . . . . . . . . . . . . 29\n2.7.6 Service Level Agreements . . . . . . . . . . . . . . . . . . . 30\n\n2.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n3 Cloud Infrastructure Platforms 35\n3.1 OpenNebula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n3.1.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.1.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.1.3 Platform Deployment . . . . . . . . . . . . . . . . . . . . . 40\n3.1.4 Authentication and Authorization . . . . . . . . . . . . . . 42\n\n3.2 OpenStack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.2.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.2.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.2.3 Platform Deployment . . . . . . . . . . . . . . . . . . . . . 47\n3.2.4 Authentication and Authorization . . . . . . . . . . . . . . 49\n\n3.3 CloudStack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.3.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.3.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.3.3 Platform Deployment . . . . . . . . . . . . . . . . . . . . . 52\n3.3.4 Authentication and Authorization . . . . . . . . . . . . . . 54\n\n3.4 Eucalyptus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n3.4.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n3.4.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n\n\nCONTENTS iii\n\n3.4.3 Platform Deployment . . . . . . . . . . . . . . . . . . . . . 58\n3.4.4 Authentication and Authorization . . . . . . . . . . . . . . 59\n\n3.5 Parallels Automation for Cloud Infrastructure . . . . . . . . . . . . 60\n3.5.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.5.2 Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n3.5.3 Platform Deployment . . . . . . . . . . . . . . . . . . . . . 63\n3.5.4 Authentication and Authorization . . . . . . . . . . . . . . 64\n\n3.6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n4 Interface Libraries Comparison 67\n4.1 Interface Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n4.1.1 RESTful API . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.1.2 XML-RPC API . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.1.3 Query API . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n4.2 OpenNebula Interface . . . . . . . . . . . . . . . . . . . . . . . . . 70\n4.2.1 Server Management . . . . . . . . . . . . . . . . . . . . . . 70\n\n4.2.1.1 Allocate VM . . . . . . . . . . . . . . . . . . . . . 71\n4.2.1.2 VM Actions . . . . . . . . . . . . . . . . . . . . . 71\n4.2.1.3 Save Disk . . . . . . . . . . . . . . . . . . . . . . . 71\n4.2.1.4 Attach Disk . . . . . . . . . . . . . . . . . . . . . 72\n4.2.1.5 Detach Disk . . . . . . . . . . . . . . . . . . . . . 72\n4.2.1.6 Attach NIC . . . . . . . . . . . . . . . . . . . . . . 72\n4.2.1.7 Detach NIC . . . . . . . . . . . . . . . . . . . . . 72\n4.2.1.8 Change VM Ownership . . . . . . . . . . . . . . . 72\n4.2.1.9 Rename VM . . . . . . . . . . . . . . . . . . . . . 73\n4.2.1.10 Create Snapshot . . . . . . . . . . . . . . . . . . . 73\n4.2.1.11 Revert Snapshot . . . . . . . . . . . . . . . . . . . 73\n4.2.1.12 Delete Snapshot . . . . . . . . . . . . . . . . . . . 73\n4.2.1.13 Resize VM . . . . . . . . . . . . . . . . . . . . . . 73\n4.2.1.14 Update VM . . . . . . . . . . . . . . . . . . . . . . 73\n4.2.1.15 VM Information . . . . . . . . . . . . . . . . . . . 74\n4.2.1.16 VM Pool Information . . . . . . . . . . . . . . . . 74\n4.2.1.17 VM Monitoring . . . . . . . . . . . . . . . . . . . 74\n4.2.1.18 VM Pool Monitoring . . . . . . . . . . . . . . . . 75\n\n4.2.2 Template Management . . . . . . . . . . . . . . . . . . . . . 75\n4.2.2.1 Allocate Template . . . . . . . . . . . . . . . . . . 76\n4.2.2.2 Clone Template . . . . . . . . . . . . . . . . . . . 76\n4.2.2.3 Delete Template . . . . . . . . . . . . . . . . . . . 76\n4.2.2.4 Instantiate Template . . . . . . . . . . . . . . . . 76\n4.2.2.5 Update Template . . . . . . . . . . . . . . . . . . 76\n4.2.2.6 Change Template Ownership . . . . . . . . . . . . 77\n4.2.2.7 Rename Template . . . . . . . . . . . . . . . . . . 77\n\n\n\niv CONTENTS\n\n4.2.2.8 Template Information . . . . . . . . . . . . . . . . 77\n4.2.2.9 Template Pool Information . . . . . . . . . . . . . 77\n\n4.2.3 Image Management . . . . . . . . . . . . . . . . . . . . . . 78\n4.2.3.1 Allocate Image . . . . . . . . . . . . . . . . . . . . 78\n4.2.3.2 Clone Image . . . . . . . . . . . . . . . . . . . . . 79\n4.2.3.3 Delete Image . . . . . . . . . . . . . . . . . . . . . 79\n4.2.3.4 Enable Image . . . . . . . . . . . . . . . . . . . . . 79\n4.2.3.5 Persistent . . . . . . . . . . . . . . . . . . . . . . . 79\n4.2.3.6 Change Image Type . . . . . . . . . . . . . . . . . 79\n4.2.3.7 Update Image . . . . . . . . . . . . . . . . . . . . 79\n4.2.3.8 Change Image Ownership . . . . . . . . . . . . . . 80\n4.2.3.9 Rename Image . . . . . . . . . . . . . . . . . . . . 80\n4.2.3.10 Image Information . . . . . . . . . . . . . . . . . . 80\n4.2.3.11 Image Pool Information . . . . . . . . . . . . . . . 80\n\n4.2.4 Network Management . . . . . . . . . . . . . . . . . . . . . 81\n4.2.4.1 Allocate VN . . . . . . . . . . . . . . . . . . . . . 81\n4.2.4.2 Delete VN . . . . . . . . . . . . . . . . . . . . . . 82\n4.2.4.3 Add VN Leases . . . . . . . . . . . . . . . . . . . 82\n4.2.4.4 Remove VN Leases . . . . . . . . . . . . . . . . . 82\n4.2.4.5 Hold VN Lease . . . . . . . . . . . . . . . . . . . . 82\n4.2.4.6 Release VN Lease . . . . . . . . . . . . . . . . . . 82\n4.2.4.7 Update VN . . . . . . . . . . . . . . . . . . . . . . 83\n4.2.4.8 Change VN Ownership . . . . . . . . . . . . . . . 83\n4.2.4.9 Rename VN . . . . . . . . . . . . . . . . . . . . . 83\n4.2.4.10 VN Information . . . . . . . . . . . . . . . . . . . 83\n4.2.4.11 VN Pool Information . . . . . . . . . . . . . . . . 83\n\n4.2.5 Data-Store Management . . . . . . . . . . . . . . . . . . . . 84\n4.2.5.1 Allocate Data-store . . . . . . . . . . . . . . . . . 84\n4.2.5.2 Delete Data-store . . . . . . . . . . . . . . . . . . 85\n4.2.5.3 Update Data-store . . . . . . . . . . . . . . . . . . 85\n4.2.5.4 Change Data-store Ownership . . . . . . . . . . . 85\n4.2.5.5 Data-store Information . . . . . . . . . . . . . . . 85\n4.2.5.6 Data-store Pool Information . . . . . . . . . . . . 85\n\n4.3 OpenStack Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n4.3.1 Server Management . . . . . . . . . . . . . . . . . . . . . . 87\n\n4.3.1.1 List Servers . . . . . . . . . . . . . . . . . . . . . . 87\n4.3.1.2 Create Server . . . . . . . . . . . . . . . . . . . . . 88\n4.3.1.3 Get Server Details . . . . . . . . . . . . . . . . . . 88\n4.3.1.4 Update Server . . . . . . . . . . . . . . . . . . . . 88\n4.3.1.5 Delete Server . . . . . . . . . . . . . . . . . . . . . 88\n4.3.1.6 List Addresses . . . . . . . . . . . . . . . . . . . . 89\n\n\n\nCONTENTS v\n\n4.3.1.7 List Addresses by Network . . . . . . . . . . . . . 89\n4.3.1.8 Change Administrator Password . . . . . . . . . . 89\n4.3.1.9 Reboot Server . . . . . . . . . . . . . . . . . . . . 89\n4.3.1.10 Rebuild Server . . . . . . . . . . . . . . . . . . . . 90\n4.3.1.11 Resize Server . . . . . . . . . . . . . . . . . . . . . 90\n4.3.1.12 Confirm Resized Server . . . . . . . . . . . . . . . 90\n4.3.1.13 Revert Resized Server . . . . . . . . . . . . . . . . 90\n4.3.1.14 Create Image . . . . . . . . . . . . . . . . . . . . . 90\n\n4.3.2 Flavour Management . . . . . . . . . . . . . . . . . . . . . . 92\n4.3.2.1 List Flavours . . . . . . . . . . . . . . . . . . . . . 92\n4.3.2.2 Get Flavour Details . . . . . . . . . . . . . . . . . 92\n\n4.3.3 Image Management . . . . . . . . . . . . . . . . . . . . . . 93\n4.3.3.1 Create Image . . . . . . . . . . . . . . . . . . . . . 93\n4.3.3.2 List Images . . . . . . . . . . . . . . . . . . . . . . 93\n4.3.3.3 Get Image Details . . . . . . . . . . . . . . . . . . 94\n4.3.3.4 Update Image . . . . . . . . . . . . . . . . . . . . 94\n4.3.3.5 Delete Image . . . . . . . . . . . . . . . . . . . . . 94\n4.3.3.6 Upload Image . . . . . . . . . . . . . . . . . . . . 94\n4.3.3.7 Download Image . . . . . . . . . . . . . . . . . . . 94\n4.3.3.8 Add Image Tag . . . . . . . . . . . . . . . . . . . . 95\n4.3.3.9 Delete Image Tag . . . . . . . . . . . . . . . . . . 95\n\n4.3.4 Network Management . . . . . . . . . . . . . . . . . . . . . 96\n4.3.4.1 List Networks . . . . . . . . . . . . . . . . . . . . 96\n4.3.4.2 Show Network . . . . . . . . . . . . . . . . . . . . 96\n4.3.4.3 Create Network . . . . . . . . . . . . . . . . . . . 96\n4.3.4.4 Update Network . . . . . . . . . . . . . . . . . . . 97\n4.3.4.5 Delete Network . . . . . . . . . . . . . . . . . . . . 97\n4.3.4.6 List Subnets . . . . . . . . . . . . . . . . . . . . . 97\n4.3.4.7 Show Subnet . . . . . . . . . . . . . . . . . . . . . 97\n4.3.4.8 Create Subnet . . . . . . . . . . . . . . . . . . . . 97\n4.3.4.9 Update Subnet . . . . . . . . . . . . . . . . . . . . 98\n4.3.4.10 Delete Subnet . . . . . . . . . . . . . . . . . . . . 98\n4.3.4.11 List Ports . . . . . . . . . . . . . . . . . . . . . . . 98\n4.3.4.12 Show Port . . . . . . . . . . . . . . . . . . . . . . 98\n4.3.4.13 Create Port . . . . . . . . . . . . . . . . . . . . . . 99\n4.3.4.14 Update Port . . . . . . . . . . . . . . . . . . . . . 99\n4.3.4.15 Delete Port . . . . . . . . . . . . . . . . . . . . . . 99\n\n4.3.5 Volume Management . . . . . . . . . . . . . . . . . . . . . . 100\n4.3.5.1 Create Volume . . . . . . . . . . . . . . . . . . . . 100\n4.3.5.2 List Volumes . . . . . . . . . . . . . . . . . . . . . 100\n4.3.5.3 List Volume Details . . . . . . . . . . . . . . . . . 101\n\n\n\nvi CONTENTS\n\n4.3.5.4 Show Volume . . . . . . . . . . . . . . . . . . . . . 101\n4.3.5.5 Update Volume . . . . . . . . . . . . . . . . . . . 101\n4.3.5.6 Delete Volume . . . . . . . . . . . . . . . . . . . . 101\n\n4.4 CloudStack Interface . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n4.4.1 Server Management . . . . . . . . . . . . . . . . . . . . . . 103\n\n4.4.1.1 Deploy Virtual Machine . . . . . . . . . . . . . . . 103\n4.4.1.2 Destroy Virtual Machine . . . . . . . . . . . . . . 105\n4.4.1.3 Reboot Virtual Machine . . . . . . . . . . . . . . . 105\n4.4.1.4 Start Virtual Machine . . . . . . . . . . . . . . . . 105\n4.4.1.5 Stop Virtual Machine . . . . . . . . . . . . . . . . 105\n4.4.1.6 Reset Password For Virtual Machine . . . . . . . . 105\n4.4.1.7 Reset SSH Key For Virtual Machine . . . . . . . . 105\n4.4.1.8 Update Virtual Machine . . . . . . . . . . . . . . 106\n4.4.1.9 List Virtual Machines . . . . . . . . . . . . . . . . 106\n4.4.1.10 Get VM Password . . . . . . . . . . . . . . . . . . 107\n4.4.1.11 Restore Virtual Machine . . . . . . . . . . . . . . 107\n4.4.1.12 Change Service For Virtual Machine . . . . . . . . 107\n4.4.1.13 Add NIC To Virtual Machine . . . . . . . . . . . . 107\n4.4.1.14 Remove NIC From Virtual Machine . . . . . . . . 108\n4.4.1.15 Update Default NIC For Virtual Machine . . . . . 108\n\n4.4.2 Template Management . . . . . . . . . . . . . . . . . . . . . 108\n4.4.2.1 Create Template . . . . . . . . . . . . . . . . . . . 108\n4.4.2.2 Update Template . . . . . . . . . . . . . . . . . . 110\n4.4.2.3 Copy Template . . . . . . . . . . . . . . . . . . . . 111\n4.4.2.4 Delete Template . . . . . . . . . . . . . . . . . . . 111\n4.4.2.5 List Templates . . . . . . . . . . . . . . . . . . . . 111\n4.4.2.6 Update Template Permissions . . . . . . . . . . . 112\n4.4.2.7 List Template Permissions . . . . . . . . . . . . . 112\n4.4.2.8 Extract Template . . . . . . . . . . . . . . . . . . 113\n\n4.4.3 Image Management . . . . . . . . . . . . . . . . . . . . . . 113\n4.4.3.1 Attach ISO . . . . . . . . . . . . . . . . . . . . . . 113\n4.4.3.2 Detach ISO . . . . . . . . . . . . . . . . . . . . . . 115\n4.4.3.3 List ISO . . . . . . . . . . . . . . . . . . . . . . . 115\n4.4.3.4 Update ISO . . . . . . . . . . . . . . . . . . . . . 115\n4.4.3.5 Delete ISO . . . . . . . . . . . . . . . . . . . . . . 115\n4.4.3.6 Copy ISO . . . . . . . . . . . . . . . . . . . . . . . 116\n4.4.3.7 Update ISO Permissions . . . . . . . . . . . . . . 116\n4.4.3.8 List ISO Permissions . . . . . . . . . . . . . . . . 116\n4.4.3.9 Extract ISO . . . . . . . . . . . . . . . . . . . . . 116\n\n4.4.4 Network Management . . . . . . . . . . . . . . . . . . . . . 117\n4.4.4.1 List Port Forwarding Rules . . . . . . . . . . . . . 118\n\n\n\nCONTENTS vii\n\n4.4.4.2 Create Port Forwarding Rule . . . . . . . . . . . . 118\n4.4.4.3 Delete Port Forwarding Rule . . . . . . . . . . . . 118\n4.4.4.4 Update Port Forwarding Rule . . . . . . . . . . . 119\n4.4.4.5 Create Firewall Rule . . . . . . . . . . . . . . . . . 119\n4.4.4.6 Delete Firewall Rule . . . . . . . . . . . . . . . . . 119\n4.4.4.7 List Firewall Rules . . . . . . . . . . . . . . . . . . 119\n4.4.4.8 Create Egress Firewall Rule . . . . . . . . . . . . . 119\n4.4.4.9 Delete Egress Firewall Rule . . . . . . . . . . . . . 119\n4.4.4.10 List Egress Firewall Rules . . . . . . . . . . . . . . 120\n4.4.4.11 Create Network . . . . . . . . . . . . . . . . . . . 120\n4.4.4.12 Delete Network . . . . . . . . . . . . . . . . . . . . 121\n4.4.4.13 List Networks . . . . . . . . . . . . . . . . . . . . 121\n4.4.4.14 Restart Network . . . . . . . . . . . . . . . . . . . 122\n4.4.4.15 Update Network . . . . . . . . . . . . . . . . . . . 122\n\n4.4.5 Volume Management . . . . . . . . . . . . . . . . . . . . . . 124\n4.4.5.1 Attach Volume . . . . . . . . . . . . . . . . . . . . 124\n4.4.5.2 Upload Volume . . . . . . . . . . . . . . . . . . . . 124\n4.4.5.3 Detach Volume . . . . . . . . . . . . . . . . . . . . 124\n4.4.5.4 Create Volume . . . . . . . . . . . . . . . . . . . . 124\n4.4.5.5 Delete Volume . . . . . . . . . . . . . . . . . . . . 125\n4.4.5.6 List Volumes . . . . . . . . . . . . . . . . . . . . . 125\n4.4.5.7 Extract Volume . . . . . . . . . . . . . . . . . . . 125\n4.4.5.8 Migrate Volume . . . . . . . . . . . . . . . . . . . 125\n4.4.5.9 Resize Volume . . . . . . . . . . . . . . . . . . . . 125\n\n4.5 PACI Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n4.5.1 Server Management . . . . . . . . . . . . . . . . . . . . . . 127\n\n4.5.1.1 List Servers . . . . . . . . . . . . . . . . . . . . . . 127\n4.5.1.2 Start/Stop a Server . . . . . . . . . . . . . . . . . 127\n4.5.1.3 Create Server . . . . . . . . . . . . . . . . . . . . . 127\n4.5.1.4 Create Server From Image . . . . . . . . . . . . . 128\n4.5.1.5 Clone Server . . . . . . . . . . . . . . . . . . . . . 129\n4.5.1.6 Modify Server Configuration . . . . . . . . . . . . 129\n4.5.1.7 Reset Server Administrator Password . . . . . . . 129\n4.5.1.8 Obtain Server Information . . . . . . . . . . . . . 129\n4.5.1.9 Obtain Server History . . . . . . . . . . . . . . . . 130\n4.5.1.10 Delete Server . . . . . . . . . . . . . . . . . . . . . 130\n4.5.1.11 Set Backup Schedule . . . . . . . . . . . . . . . . 130\n4.5.1.12 Cancel Backup Schedule . . . . . . . . . . . . . . 130\n4.5.1.13 List Backup Schedule . . . . . . . . . . . . . . . . 131\n4.5.1.14 Restore a Server . . . . . . . . . . . . . . . . . . . 131\n\n4.5.2 Firewall Management . . . . . . . . . . . . . . . . . . . . . 132\n\n\n\nviii CONTENTS\n\n4.5.2.1 List Firewall Rules . . . . . . . . . . . . . . . . . . 133\n4.5.2.2 Create Firewall Rule . . . . . . . . . . . . . . . . . 133\n4.5.2.3 Modify Firewall Rule . . . . . . . . . . . . . . . . 133\n4.5.2.4 Delete Firewall Rule . . . . . . . . . . . . . . . . . 133\n\n4.5.3 Application Template Management . . . . . . . . . . . . . . 134\n4.5.3.1 List Application Templates . . . . . . . . . . . . . 134\n4.5.3.2 Get Application Templates Information . . . . . . 134\n4.5.3.3 Install Application Templates . . . . . . . . . . . . 134\n\n4.5.4 Image Management . . . . . . . . . . . . . . . . . . . . . . 135\n4.5.4.1 List Images . . . . . . . . . . . . . . . . . . . . . . 135\n4.5.4.2 Get Image Information . . . . . . . . . . . . . . . 135\n4.5.4.3 Create Image . . . . . . . . . . . . . . . . . . . . . 136\n4.5.4.4 Delete Image . . . . . . . . . . . . . . . . . . . . . 136\n\n4.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n\n5 Interoperable Interface Proposal 139\n5.1 Dedicated Interoperable Service . . . . . . . . . . . . . . . . . . . . 139\n\n5.1.1 Interaction Layer . . . . . . . . . . . . . . . . . . . . . . . . 140\n5.1.2 Abstraction Layer . . . . . . . . . . . . . . . . . . . . . . . 140\n5.1.3 Interface Layer . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n5.1.3.1 Virtual Machine Management . . . . . . . . . . . 141\n5.1.3.2 Image Management . . . . . . . . . . . . . . . . . 142\n\n5.2 Cloud Abstraction Interface Solutions . . . . . . . . . . . . . . . . 143\n5.2.1 Deltacloud . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n5.2.2 jClouds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n5.2.3 Libcloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n\n5.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n\n6 Project Development 151\n6.1 Development Environment . . . . . . . . . . . . . . . . . . . . . . . 151\n\n6.1.1 Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n6.1.1.1 General Purpose Language . . . . . . . . . . . . . 151\n6.1.1.2 Domain Specific Languages . . . . . . . . . . . . . 152\n\n6.1.2 Back-end Technology . . . . . . . . . . . . . . . . . . . . . . 152\n6.1.2.1 Web Application Library . . . . . . . . . . . . . . 153\n6.1.2.2 Web Application Server . . . . . . . . . . . . . . . 153\n\n6.1.3 Front-end Technologies . . . . . . . . . . . . . . . . . . . . . 153\n6.1.4 Development Tools . . . . . . . . . . . . . . . . . . . . . . . 154\n\n6.2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n6.3 Interoperable Service API . . . . . . . . . . . . . . . . . . . . . . . 155\n\n6.3.1 Realms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n6.3.1.1 List Realms . . . . . . . . . . . . . . . . . . . . . . 155\n\n\n\nCONTENTS ix\n\n6.3.1.2 Show Realm Information . . . . . . . . . . . . . . 155\n6.3.2 Hardware Profiles . . . . . . . . . . . . . . . . . . . . . . . 156\n\n6.3.2.1 List Hardware Profiles . . . . . . . . . . . . . . . . 156\n6.3.2.2 Show Hardware Profile Information . . . . . . . . 156\n\n6.3.3 Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n6.3.3.1 List Images . . . . . . . . . . . . . . . . . . . . . . 156\n6.3.3.2 Show Image Information . . . . . . . . . . . . . . 156\n6.3.3.3 Create Image from Instance . . . . . . . . . . . . . 157\n6.3.3.4 Delete Image . . . . . . . . . . . . . . . . . . . . . 157\n\n6.3.4 Instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n6.3.4.1 List Instances . . . . . . . . . . . . . . . . . . . . 157\n6.3.4.2 Show Instance Information . . . . . . . . . . . . . 157\n6.3.4.3 Instance Action . . . . . . . . . . . . . . . . . . . 157\n6.3.4.4 Create Instance . . . . . . . . . . . . . . . . . . . 158\n6.3.4.5 Delete Instance . . . . . . . . . . . . . . . . . . . . 158\n\n6.3.5 Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n6.3.5.1 List Keys . . . . . . . . . . . . . . . . . . . . . . . 158\n6.3.5.2 Show Key Information . . . . . . . . . . . . . . . . 158\n6.3.5.3 Create Key . . . . . . . . . . . . . . . . . . . . . . 158\n6.3.5.4 Delete Key . . . . . . . . . . . . . . . . . . . . . . 158\n\n6.3.6 Firewalls . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n6.3.6.1 List Firewalls . . . . . . . . . . . . . . . . . . . . . 159\n6.3.6.2 Show Firewall Information . . . . . . . . . . . . . 159\n6.3.6.3 Create Firewall . . . . . . . . . . . . . . . . . . . . 159\n6.3.6.4 Delete Firewall . . . . . . . . . . . . . . . . . . . . 159\n6.3.6.5 Create Firewall Rule . . . . . . . . . . . . . . . . . 159\n6.3.6.6 Delete Firewall Rule . . . . . . . . . . . . . . . . . 160\n\n6.3.7 Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n6.3.7.1 List Addresses . . . . . . . . . . . . . . . . . . . . 160\n6.3.7.2 Shows Address Information . . . . . . . . . . . . . 160\n6.3.7.3 Create Address . . . . . . . . . . . . . . . . . . . . 160\n6.3.7.4 Delete Address . . . . . . . . . . . . . . . . . . . . 160\n6.3.7.5 Associate Address . . . . . . . . . . . . . . . . . . 160\n6.3.7.6 Dissociate Address . . . . . . . . . . . . . . . . . . 161\n\n6.3.8 Load Balancers . . . . . . . . . . . . . . . . . . . . . . . . . 161\n6.3.8.1 List Load Balancers . . . . . . . . . . . . . . . . . 161\n6.3.8.2 Show Load Balancer Information . . . . . . . . . . 161\n6.3.8.3 Create Load Balancer . . . . . . . . . . . . . . . . 161\n6.3.8.4 Delete Load Balancer . . . . . . . . . . . . . . . . 161\n6.3.8.5 Register Instance to Load Balancer . . . . . . . . 162\n6.3.8.6 Unregister Instance from Load Balancer . . . . . . 162\n\n\n\nx CONTENTS\n\n6.3.9 Volumes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n6.3.9.1 List Volumes . . . . . . . . . . . . . . . . . . . . . 162\n6.3.9.2 Show Volume Information . . . . . . . . . . . . . . 162\n6.3.9.3 Create Volume . . . . . . . . . . . . . . . . . . . . 162\n6.3.9.4 Delete Volume . . . . . . . . . . . . . . . . . . . . 162\n6.3.9.5 Attach Volume . . . . . . . . . . . . . . . . . . . . 163\n6.3.9.6 Detach Volume . . . . . . . . . . . . . . . . . . . . 163\n\n6.3.10 Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n6.3.10.1 List Snapshots . . . . . . . . . . . . . . . . . . . . 163\n6.3.10.2 Show Snapshot Information . . . . . . . . . . . . . 163\n6.3.10.3 Create Snapshot . . . . . . . . . . . . . . . . . . . 163\n6.3.10.4 Delete Snapshot . . . . . . . . . . . . . . . . . . . 163\n\n6.3.11 Blobs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164\n6.3.11.1 List Buckets . . . . . . . . . . . . . . . . . . . . . 164\n6.3.11.2 Show Bucket Information . . . . . . . . . . . . . . 164\n6.3.11.3 Create Bucket . . . . . . . . . . . . . . . . . . . . 164\n6.3.11.4 Delete Bucket . . . . . . . . . . . . . . . . . . . . 164\n6.3.11.5 Show Blob Information . . . . . . . . . . . . . . . 164\n6.3.11.6 Create Blob . . . . . . . . . . . . . . . . . . . . . 165\n6.3.11.7 Delete Blob . . . . . . . . . . . . . . . . . . . . . . 165\n6.3.11.8 Show Blob Metadata . . . . . . . . . . . . . . . . 165\n6.3.11.9 Update Blob Metadata . . . . . . . . . . . . . . . 165\n\n6.4 Interoperable Service GUI . . . . . . . . . . . . . . . . . . . . . . . 165\n6.5 Deployment Configurations . . . . . . . . . . . . . . . . . . . . . . 166\n\n6.5.1 Single Tenant Configuration . . . . . . . . . . . . . . . . . . 166\n6.5.2 Multiple Tenant Configuration . . . . . . . . . . . . . . . . 167\n\n6.6 Reused Driver Modules . . . . . . . . . . . . . . . . . . . . . . . . 167\n6.6.1 OpenNebula Driver . . . . . . . . . . . . . . . . . . . . . . . 168\n6.6.2 OpenStack Driver . . . . . . . . . . . . . . . . . . . . . . . 168\n6.6.3 CloudStack Driver . . . . . . . . . . . . . . . . . . . . . . . 169\n\n6.7 Developed PACI Driver . . . . . . . . . . . . . . . . . . . . . . . . 169\n6.8 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n\n7 Test, Debugging and Validation 179\n7.1 Test Bed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179\n\n7.1.1 Interoperable Service Installation . . . . . . . . . . . . . . . 180\n7.1.1.1 Deltacloud Installation . . . . . . . . . . . . . . . 181\n7.1.1.2 CloudStack Driver Module Inclusion . . . . . . . . 181\n7.1.1.3 PACI Driver Module Inclusion . . . . . . . . . . . 182\n\n7.1.2 OpenNebula Installation . . . . . . . . . . . . . . . . . . . . 182\n7.1.2.1 OS Configuration . . . . . . . . . . . . . . . . . . 182\n7.1.2.2 OpenNebula Services Installation . . . . . . . . . 183\n\n\n\nCONTENTS xi\n\n7.1.2.3 KVM Hypervisor Installation . . . . . . . . . . . . 184\n7.1.2.4 Virtual Resources Allocation . . . . . . . . . . . . 185\n\n7.1.3 OpenStack Installation . . . . . . . . . . . . . . . . . . . . . 186\n7.1.3.1 OS Configuration . . . . . . . . . . . . . . . . . . 186\n7.1.3.2 Keystone Installation . . . . . . . . . . . . . . . . 188\n7.1.3.3 Glance Installation . . . . . . . . . . . . . . . . . . 191\n7.1.3.4 Nova Installation . . . . . . . . . . . . . . . . . . . 192\n7.1.3.5 Virtual Resources Allocation . . . . . . . . . . . . 195\n\n7.1.4 CloudStack Installation . . . . . . . . . . . . . . . . . . . . 197\n7.1.4.1 OS Configuration . . . . . . . . . . . . . . . . . . 197\n7.1.4.2 Management Server Installation . . . . . . . . . . 200\n7.1.4.3 KVM Hypervisor Installation . . . . . . . . . . . . 200\n7.1.4.4 Cloud Infrastructure Configuration . . . . . . . . 201\n7.1.4.5 Virtual Resources Allocation . . . . . . . . . . . . 203\n\n7.2 Testing and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 204\n7.2.1 OpenNebula Interaction Tests and Results . . . . . . . . . . 206\n7.2.2 OpenStack Interaction Tests and Results . . . . . . . . . . 209\n7.2.3 CloudStack Interaction Tests and Results . . . . . . . . . . 212\n7.2.4 PACI Interaction Tests and Results . . . . . . . . . . . . . 213\n\n7.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n\n8 Conclusions 219\n8.1 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n8.2 Envisaged Use Cases . . . . . . . . . . . . . . . . . . . . . . . . . . 221\n8.3 Future Developments . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n\nAppendices 223\n\nA Cloud Client Library 225\n\nB PACI Client 227\n\nC PACI Driver 233\n\nBibliography 241\n\n\n\n\n\nList of Figures\n\n1.1 Project schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n2.1 On-site (on premises) private cloud representation . . . . . . . . . . . 10\n2.2 Outsourced (off premises) private cloud representation . . . . . . . . . 11\n2.3 On-site (on premises)community cloud representation . . . . . . . . . 12\n2.4 Outsourced (off premises)community cloud representation . . . . . . . 13\n2.5 Public cloud representation . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.6 Hybrid cloud representation . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.7 Service orchestration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.8 Type 1 and Type 2 hypervisors. . . . . . . . . . . . . . . . . . . . . . . 23\n2.9 Full virtualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n2.10 Para-virtualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.11 OS level virtualization. . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.12 Service life-cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n\n3.1 VM management layers of a cloud infrastructure platform . . . . . . . 35\n3.2 OpenNebula architecture . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.3 OpenNebula interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4 OpenNebula system . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n3.5 OpenStack conceptual architecture . . . . . . . . . . . . . . . . . . . . 43\n3.6 OpenStack Grizzly global architecture . . . . . . . . . . . . . . . . . . 46\n3.7 OpenStack nodes, networks and components . . . . . . . . . . . . . . 48\n3.8 CloudStack architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.9 CloudStack interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n3.10 CloudStack deployment architecture . . . . . . . . . . . . . . . . . . . 53\n3.11 Eucalyptus software modules organization . . . . . . . . . . . . . . . . 55\n3.12 Eucalyptus software component interfaces . . . . . . . . . . . . . . . . 57\n3.13 Eucalyptus generalized deployment structure . . . . . . . . . . . . . . 59\n3.14 PACI architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.15 PACI RESTful interface . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n\nxiii\n\n\n\nxiv LIST OF FIGURES\n\n3.16 PACI deployment structure . . . . . . . . . . . . . . . . . . . . . . . . 63\n\n5.1 Dedicated Interoperable Service architecture . . . . . . . . . . . . . . . 140\n5.2 Deltacloud architecture . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n\n6.1 Deltacloud framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n6.2 Interoperable Service architecture . . . . . . . . . . . . . . . . . . . . . 154\n6.3 Deltacloud GUI service . . . . . . . . . . . . . . . . . . . . . . . . . . 166\n6.4 UML classes diagram of the PACI driver . . . . . . . . . . . . . . . . . 170\n\n7.1 Test bed platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n7.2 OpenNebula time response comparison . . . . . . . . . . . . . . . . . . 207\n7.3 OpenNebula HTTP request/response length comparison . . . . . . . . 208\n7.4 OpenStack time response comparison . . . . . . . . . . . . . . . . . . . 211\n7.5 OpenStack HTTP request/response length comparison . . . . . . . . . 212\n7.6 PACI IaaS platform API connection latency values . . . . . . . . . . . 214\n7.7 PACI time response comparison . . . . . . . . . . . . . . . . . . . . . . 215\n7.8 PACI HTTP request/response length comparison . . . . . . . . . . . . 216\n\n\n\nList of Tables\n\n3.1 IaaS platforms comparison. . . . . . . . . . . . . . . . . . . . . . . . . 64\n\n4.1 Server Management Table (operations vs parameters). . . . . . . . . . 75\n4.2 Template Management Table (Parameters vs Operations). . . . . . . . 78\n4.3 Image Management Table (Parameters vs Operations). . . . . . . . . . 81\n4.4 Virtual Network Management Table (Parameters vs Operations). . . . 84\n4.5 Data-store Management Table (Parameters vs Operations). . . . . . . 86\n4.6 Server Management Table (Parameters vs Operations). . . . . . . . . . 91\n4.7 Flavour Management Table (Parameters vs Operations). . . . . . . . . 93\n4.8 Image Management Table (Parameters vs Operations). . . . . . . . . . 95\n4.9 Network Management Table (Parameters vs Operations). . . . . . . . 100\n4.10 Volume Management Table (Parameters vs Operations). . . . . . . . . 102\n4.11 Server Management Table (Parameters vs Operations). . . . . . . . . . 109\n4.12 Template Management Table (Parameters vs Operations). . . . . . . . 114\n4.13 Image Management Table (Parameters vs Operations). . . . . . . . . . 117\n4.14 Network Management Table (Parameters vs Operations). . . . . . . . 123\n4.15 Volume Management Table (Parameters vs Operations). . . . . . . . . 126\n4.16 Server Management Table (Parameters vs Operations). . . . . . . . . . 132\n4.17 Firewall Management Table (Parameters vs Operations). . . . . . . . 133\n4.18 Application Template Management Table (Parameters vs Operations). 135\n4.19 Image Management Table (Parameters vs Operations). . . . . . . . . . 136\n4.20 Management Components Comparison. . . . . . . . . . . . . . . . . . 137\n\n5.1 Abstraction solutions comparison. . . . . . . . . . . . . . . . . . . . . 150\n\n6.1 Deltacloud daemon Operations. . . . . . . . . . . . . . . . . . . . . . . 167\n\nxv\n\n\n\n\n\nList of Code Snippets\n\n6.1 PACI client create_instance method . . . . . . . . . . . . . . . . . . . 171\n6.2 PACI client post method . . . . . . . . . . . . . . . . . . . . . . . . . 171\n6.3 PACI client do_request method . . . . . . . . . . . . . . . . . . . . . . 172\n6.4 PACI driver define_hardware_profile method . . . . . . . . . . . . . . 172\n6.5 PACI driver XML server template . . . . . . . . . . . . . . . . . . . . 173\n6.6 PACI driver create_instance method . . . . . . . . . . . . . . . . . . . 174\n6.7 PACI driver convert_image method . . . . . . . . . . . . . . . . . . . 175\n6.8 PACI driver define_instance_states method . . . . . . . . . . . . . . . 176\n6.9 PACI driver VE_STATES hash . . . . . . . . . . . . . . . . . . . . . . 176\n\nxvii\n\n\n\n\n\nAcronyms\n\nNotation Description Page\nACL Access Control List 42\nAD Active Directory 42\nAMD Advanced Micro Devices 24\nAMD-V AMD Virtualization 24\nAMQP Advanced Message Queue Protocol 44\nAPI Application Programming Interface 3\nASF Apache Software Foundation 49\nASLv2 Apache Software License version 2.0 49\nAWS Amazon Web Services 1\nB2B Business to Business 31\nB2C Business to Consumer 31\nBLOB Binary Large Object 145\nBSS Business Support System 62\nC12G Cloud Computing 37\nCIDR Classless Inter-Domain Routing 98\nCIFS Common Internet File System 110\nCIM Cloud Infrastructure Manager 35\nCIMI Cloud Infrastructure Management Interface 144\nCLI Command Line Interface 18\nCPU Central Unit Processor 16\nDAS Direct Attached Storage 41\nDB DataBase 40\nDHCP Dynamic Host Configuration Protocol 45\nDIS Dedicated Interoperable Service 139\nDMTF Distributed Management Task Force 144\nDNS Domain Name System 51\nDNSaaS DNS as a Service 147\nDS Data-Store 84\n\nxix\n\n\n\nxx ACRONYMS\n\nNotation Description Page\nDSA Distributed Systems Architecture 36\nDSL Domain Specific Language 152\nEBS Elastic Block Store 45\nEC2 Elastic Compute Cloud 1\nERB Embedded Ruby 175\nEXT linux EXTended file system 51\nFP6 6th Framework Programme 149\nFP7 7th Framework Programme 149\nFS File System 41\nGPL Generic Purpose Language 151\nGPLv3 GNU General Public License version 3 49\nGUI Graphical User Interface 3\nHA High Availability 50\nHAML HTML Abstraction Markup Language 152\nHTML HyperText Markup Language 145\nHTTP HyperText Transfer Protocol 37\nHTTPS HyperText Transfer Protocol Secure 62\nI/O Input/Output 22\nIaaS Infrastructure as a Service 1\nIAM Identity and Access Management 55\nIBM International Business Machines 22\nID IDentification 32\nIDE Integrated Development Environments 16\nIM Instance Manager 63\nIP Internet Protocol 44\nIRB Interactive Ruby Shell 154\niSCSI internet Small Computer System Interface 38\nISV Independent Software Vendors 27\nIT Information Technology 8\nJSON JavaScript Object Notation 51\nKVM Kernel-based Virtual Machine 24\nL2 Layer 2 44\nL3 Layer 3 44\nLBaaS Load Balancer as a Service 147\nLDAP Lightweight Directory Access Protocol 42\nLEAD Linked Environment for Atmosphere Discovery 54\nLVM Logical Volume Manage 47\nMIME Multipurpose Internet Mail Extensions 86\nMPS Microsoft Provisioning System 63\nMVC Model View Controller 153\n\n\n\nACRONYMS xxi\n\nNotation Description Page\nNAS Network-Attached Storage 41\nNASA National Aeronautics and Space Administra-\n\ntion\n42\n\nNAT Network Address Translation 45\nNFS Network File System 38\nNIC Network Interface Controller 54\nNIST National Institute of Standards and Techno-\n\nlogy\n9\n\nNSF National Science Foundation 54\nNTP Network Time Protocol 180\nOAM&amp;P Operation, Administration, Maintenance &amp;\n\nProvision\n51\n\nOCA OpenNebula Cloud API 40\nOCCI Open Cloud Computing Interface 39\nOGF Open Grid Forum 31\nONF Open Networking Foundation 44\nORM Object-Relational Mapper 47\nOS Operating System 16\nOSS Operations Support System 62\nPaaS Platform as a Service 15\nPACI Parallels Automation for Cloud Infrastructure 4\nPAM Pluggable Authentication Modules 49\nPBA Parallels Business Automation 62\nPC Personal Computer 1\nPET Privacy Enhancing Technologies 22\nPID Process ID 167\nPKI Public Key Infrastructure 189\nPOA Parallels Operations Automation 62\nPSBM Parallels Server Bare Metal 62\nQCOW2 QEMU Copy on Write Version 2 185\nQEMU Quick Emulator 185\nQoS Quality of Service 7\nR&amp;D Research &amp; Development 1\nRBD Rados Block Device 44\nRDBMS Relational Database Management System 187\nREST REpresentational State Transfer 39\nREXML Ruby Electric XML 175\nRFC Request For Comments 68\nRHEL Red Hat Enterprise Linux 188\nRPC Remote Procedure Call 37\n\n\n\nxxii ACRONYMS\n\nNotation Description Page\nRSA Rivest r., Shamir a. and Adleman l. 42\nS3 Simple Storage Service 44\nSaaS Software as a Service 9\nSAGA Simple API for Grid Applications 149\nSAN Storage Area Network 41\nSDK Software Development Kits 17\nSELinux Security-Enhanced Linux 188\nSHA-1 Secure Hash Algorithm 1 206\nSME Small and Medium sized Enterprises 19\nSOA Service Oriented Architecture 31\nSPI Software, Platform, and Infrastructure 15\nSQL Structured Query Language 37\nSSH Secure Shell 38\nSSL Secure Sockets Layer 205\nTCP Transmission Control Protocol 59\nUI User Interface 50\nUID Unique IDentifier 74\nUML Unified Model Language 170\nURI Uniform Resource Identifier 67\nURL Uniform Resource Locator 69\nUUID Universally Unique IDentifier 92\nVE Virtual Environment 25\nVGrDS Virtual Grid application Development Soft-\n\nware\n54\n\nVIM Virtual Infrastructure Manager 35\nVLAN Virtual Local Area Network 11\nVM Virtual Machine 3\nVM/370 Virtual Machine Facilityl370 23\nVMM Virtual Machine Monitor 16\nVN Virtual Network 16\nVNC Virtual Network Computing 46\nVPC Virtual Private Cloud 121\nVPN Virtual Private Network 10\nVPS Virtual Private Server 25\nvSwitch virtual Switch 41\nWS-Agreement Web Service Agreement 31\nWSDL Web Services Description Language 31\nWSGI Web Server Gateway Interface 46\nWSLA Web Service Level Agreement 31\nWWW World Wide Web 21\n\n\n\nACRONYMS xxiii\n\nNotation Description Page\nXaaS Something as a Service 15\nXML eXtensible Mark-up Language 32\nYAML YAML Ain\u2019t Markup Language 165\n\n\n\n\n\nAcknowledgements\n\nFirst and foremost, I have to sincerely thank my supervisor, Professor Maria\nBenedita Campos Neves Malheiro, for the essential guidance, availability, and\ndedicated involvement in every step throughout the process.\n\nI am also eternally grateful to Professor Pedro Jo\u00e3o De-Francesco Assis. He\nintroduced me to an interesting topic on which I have enjoyed so much to work.\n\nI would like to show gratitude to Eng.o Paulo Cal\u00e7ada for the opportunity\ngiven to me to work and learn from a state-of-the-art project.\n\nLast, but not least, To my family and friends for the continuous encourage-\nment and support during all this time.\n\nThank you all.\n\nxxv\n\n\n\n\n\nChapter 1\n\nIntroduction\n\nCloud Computing has become a mainstream topic. This technology trend integ-\nrates the underlying structure, from the infrastructural to the applicational level,\nof well-known public Web services, e.g., Amazon Web Services (AWS), Google\nApp Engine and Spotify [1][2][3]. While the emergence of this technology on the\nmarket is related to the creation of Salesforce Web applications for enterprises or\nthe launch of AWS, the concept of Cloud Computing dates back to 1984 when\nSun Microsystems adopted John Gage\u2019s famous expression \u201cThe Network Is The\nComputer\u201d [4].\n\nThe term \u201cCloud Computing\u201d idealizes the provision of computing resources,\nabstracted by the cloud as Web services, that are available everywhere through\na different set of interface devices connected to the Internet. This technology\ncontrasts with the individual and local computation resources, e.g., Personal\nComputer (PC), that provide confined software programs with a limited hardware\nconfiguration.\n\nThe Elastic Compute Cloud (EC2) created by Amazon back in 2006 initiated\nthe provision of the Infrastructure as a Service (IaaS) concept. This type of ser-\nvice was rapidly adopted by other well-known technology enterprises with large\ncomputing resources, that launched their own IaaS platforms. As a result, the\nResearch &amp; Development (R&amp;D) community as well as the involved enterprises\nconcentrated efforts on the development of open source and proprietary software\nsolutions for enabling the deployment of IaaS platforms. Since Cloud Comput-\ning was a recent concept, lacking pre-defined standards and even a consensual\ndefinition, the resulting platforms were highly heterogeneous in terms of function-\nalities, architecture and interface libraries. This diversity hinders the selection of\nan IaaS platform and, above all, constitutes an obstacle to the interoperability\namong service providers.\n\n1\n\n\n\n2 CHAPTER 1. INTRODUCTION\n\nThe European cloud services provider LunaCloud intends to face the presen-\nted problems with the implementation of multiple IaaS platforms for the high\nlevel management and monitoring of its IaaS business segment. Currently, the\nservices offered by LunaCloud have limitations in terms of automation and or-\nchestration of actions and the advanced proprietary platform management library\nhas no compatibility and interoperability support with other IaaS platforms. To\novercome this problem, LunaCloud considers the development of an open source\nsolution for the integrated management of cloud computing infrastructure re-\nsources, promoting interoperability and standardization between heterogeneous\nIaaS platforms. This thesis addresses this challenge and proposes a solution.\n\n1.1 Motivation\n\nThe choice for the current thesis theme relates to the personal interest to deepen\nthe knowledge associated with the Cloud Computing paradigm, more specifically,\nwith the IaaS provisioning level. To work with technologies related with cloud\ninfrastructure resources supply - virtualization, management, monitoring and\ninterface systems - as well as to get to know the existing diversity of IaaS platform\napproaches, mainly the open-source solutions.\n\nFrom a professional perspective, this work provided the opportunity to parti-\ncipate and learn from a state-of-the-art project - the study, selection and deploy-\nment of a multi-platform IaaS abstraction solution.\n\nFrom a social perspective, this project provided the possibility to contribute\nto existing open-source communities.\n\n1.2 Thesis\n\nThis research work intends to answer the following questions:\n\n1. How much do current IaaS platforms differ?\n\n2. Can heterogeneous IaaS platforms be managed in an integrated manner\nfrom the client perspective?\n\n3. Is it possible to develop an interoperable service for the high level manage-\nment of the IaaS business segment of Lunacloud?\n\n1.3 Objectives\n\nThe main goal of this project is to propose and develop an interoperable service for\nthe integrated management of cloud resources provisioned by the IaaS platforms\n\n\n\n1.4. FUNCTIONAL TESTS 3\n\nused by Lunacloud. Inherent to the accomplishment of the presented objective,\nthe following tasks were defined:\n\n\u2022 Survey of open-source IaaS platforms as well as of Lunacloud proprietary\nsolution;\n\n\u2022 Study of different IaaS platform Application Programming Interface (API);\n\n\u2022 Survey on existing IaaS interoperable open-source solutions;\n\n\u2022 Proposal, specification and development of a meta-library/meta-service to\ninterface with the IaaS platforms used by Lunacloud;\n\n\u2022 Development of a front-end application to configure and manage the infra-\nstructure resources provided by the Lunacloud proprietary infrastructure\nplatform as well as other open source IaaS platforms.\n\n1.4 Functional Tests\n\nThe implemented solution should allow the interaction with the included IaaS\nplatforms via a common interface. In particular, the solution adopted for the\nintegration of the different IaaS platforms should allow the user to:\n\n\u2022 Create and delete a Virtual Machine (VM);\n\n\u2022 Perform a VM action (e.g., start and stop a VM);\n\n\u2022 List his/her resources (e.g., images and VM);\n\n\u2022 Obtain the characteristics of a specified resource.\n\n1.5 Expected Results\n\nThe proposed solution should:\n\n\u2022 Provide a standard programming interface for the integrated management\nof the Lunacloud proprietary interface platform as well as other open-source\nIaaS platforms;\n\n\u2022 Perform all adaptation and data transformation operations;\n\n\u2022 Allow the management of virtual resources from the different integrated\nIaaS platforms;\n\n\u2022 Expose a Graphical User Interface (GUI).\n\n\n\n4 CHAPTER 1. INTRODUCTION\n\n1.6 Contributions\n\nThe provision of an interoperable service for the integrated management of IaaS\nresources was supported by the following contributions: (i) a thorough survey and\ncomparison of the IaaS platforms to be integrated (open source platforms and the\nPACI proprietary platform); (ii) a detailed survey, comparison and selection of a\ncandidate abstraction solution; (iii) the set up of a test bed platform to assess the\ninteroperable service proposed; and (iv) the implementation of the PACI driver.\nThe PACI driver was shared with the Deltacloud community and the detected\nOpenNebula and OpenStack driver malfunctions were also reported.\n\n1.7 Work Plan\n\nThe project Gantt chart presented in Figure 1.1 includes the project milestones\nand the tasks performed: the state of the art on Cloud Computing, a survey on\nthe most popular open-source IaaS platforms, the study of OpenNebula, Open-\nStack, CloudStack and Parallels Automation for Cloud Infrastructure (PACI)\nuser interfaces, a survey on cloud interface abstraction solutions, the familiar-\nization with the Ruby programming language and the Deltacloud abstraction\nframework, the development of a PACI driver for Deltacloud, the test bed set-\nup, the tests conducted and corrections performed and the dissertation writing.\n\nFigure 1.1: Project schedule\n\n\n\n1.8. STRUCTURE OF THE DISSERTATION 5\n\n1.8 Structure of the Dissertation\n\nThis document is structured in nine chapters:\n\n\u2022 The first chapter, \u201cIntroduction\u201d, presents and contextualizes the developed\nproject, emphasizing the motivation, main objectives and followed method-\nology, the work plan and the structure of the dissertation;\n\n\u2022 The second chapter, \u201cCloud Computing\u201d, addresses the fundamental con-\ncepts of the Cloud Computing technology, including its definition, the de-\nployment and service models, the main features and concerns, virtualization\ntechnologies and the business perspective;\n\n\u2022 The third chapter, \u201cCloud Infrastructure Platforms\u201d, describes and com-\npares four different open-source cloud infrastructure platform solutions -\nOpenNebula, OpenStack, CloudStack and Eucalyptus - and a proprietary\ncloud infrastructure platform - PACI - for the identification of common\nfeatures that will enable the development of a management solution that\nmeets the scope of this thesis;\n\n\u2022 The fourth chapter, \u201cInterface Libraries Comparison\u201d, presents a study\nand comparison of the client interface libraries provided by OpenNebula,\nOpenStack, CloudStack and PACI IaaS platforms in terms of program-\nmable components and available operations, including the parameters and\nrespective attributes;\n\n\u2022 The fifth chapter, \u201cInteroperable Interface Proposal\u201d, considers two possible\napproaches for the creation of an interoperable service for the IaaS platforms\nstudied in the previous chapter;\n\n\u2022 The sixth chapter, \u201cProject Development\u201d, describes the development en-\nvironment, including the programming languages, front-end and back-end\ntechnologies and the development tools adopted in the development of the\nproject, and details the project development process, including the con-\nceptual architecture, the interoperable front-end services, the deployment\nconfiguration of the interoperable service and the included driver modules\n(reused and developed);\n\n\u2022 The seventh chapter, \u201cTest, Debugging and Validation\u201d, describes the in-\nstallation and configuration of the Interoperable Service test bed and presents\nthe tests performed and results obtained;\n\n\u2022 The eighth chapter, \u201cConclusions\u201d, presents the discussion of the main dis-\nsertation conclusions, the envisaged usage cases and future developments.\n\n\n\n\n\nChapter 2\n\nCloud Computing\n\nThis chapter presents the definition, deployment and service models, character-\nistics, concerns, virtualization technologies and business perspective of the Cloud\nComputing paradigm.\n\n2.1 Definition\n\nCloud computing is a relatively recent computation paradigm in which virtual-\nized and dynamically scalable resources are provided as services (i.e. programs,\nstorage, application-development platforms) over a network, usually the Internet,\nto end users/clients through a variety of devices (i.e. laptops, smartphones, PC)\n[5].\n\nThis new computation model has evolved from itspredecessor computation\nparadigm, Grid Computing, introduced by Foster and Kesselman in 1999 [6],\ncombining the Grid (as its backbone and infrastructure support) with Virtual-\nization and Utility Computing. Grid Computing can be defined as a type of\nparallel and distributed system that enables the sharing, selection and aggrega-\ntion of geographically distributed autonomous computer resources dynamically\nat run-time depending on their availability, capability, performance, cost, and\nusers Quality of Service (QoS) requirements [7]. In other words, this computa-\ntion model provides a combination of computer resources linked by a network\nor by the Internet (forming a grid) to provide the essential requirements (per-\nformance, cost, QoS) to deal with scientific or technical problems that require\na great number of computer processing cycles or large amounts of data. Thus,\na cloud computing technology takes advantage of the Grid to provide his com-\nputation resources but a Grid is not necessarily a Cloud or part of a Cloud.\nIn fact, the evolution from Grid Computing to Cloud Computing was a result\n\n7\n\n\n\n8 CHAPTER 2. CLOUD COMPUTING\n\nof a shift in focus from the infrastructure concept of the Grid Computing (an\ninfrastructure that delivers storage and compute resources and that focus on a\nsingle specific utilization) to one that is economy based aiming to deliver more\nabstract resources and services [8]. Virtualization and Utility Computing have\nan important role in this \u201ceconomy concept shift\u201d. The first, virtualization, is\nessential for the provision of an encapsulation and abstraction layer from the un-\nderlying hardware and system software running in the data centre servers. This\nway, at the raw hardware level, resources can be added or withdraw according\nwith the demand, while the interface to the user is not changing. The application\nof virtualization as part of the technologies used by Cloud Computing enables\nscalability, making computation resources more profitable, and flexibility on its\nphysical layer [9]. The virtualization techniques and their architecture will be\ndiscussed, at greater detail, further, in the Virtualization Technologies section.\nThe last, Utility Computing, is a technology that can be defined as \u201ca business\nmodel in which computing resources, such as computation and storage, are pack-\naged as metered services similar to a physical public utility, such as electricity\nand public switched telephone network.\u201d [8]. The concept is similar to the one\nfound in the electric grid or water supply payment models, as the user/client\npays for the amount of resources consumed. In a similar way, Cloud Computer\nservices are charged based on the processing power or amount of data utilized,\nin a \u201cpay as you go\u201d philosophy [5]. This approach provides advantages to the\nend users as the need for up-front infrastructure investment is minimal, either\nwith software licences (and unused but paid software licenses) or with hardware\ninfrastructure and related maintenance and staff [10].\n\nThe definition of Cloud Computing has been a controversial topic. Computer\nresearchers, analyst firms and Information Technology (IT) institutions define\nCloud Computing in different manners [9][11][12]. Gartner and Merrill Lynch\n[13][14], two analyst firms, define Cloud Computing as follows:\n\n\u2022 \u201cA style of computing in which massively scalable IT related capabilities\nare provided \u201cas a service\u201d using Internet technologies to multiple external\ncustomers.\u201d - Gartner 2008 [15];\n\n\u2022 \u201cThe idea of delivering personal (e.g., email, word processing, presenta-\ntions.) and business productivity applications (e.g., sales force automation,\ncustomer service, accounting) from centralized servers\u201d - Merrill Lynch 2008\n[16].\n\nAs from the scientific community the opinion on the mater is not unanimous\nbut still adds, to the end user perspective of the earlier definitions, the archi-\ntectural aspects of the infrastructure (data centre hardware, virtualization and\nscalability), as can be read in the following definitions:\n\n\n\n2.2. DEPLOYMENT MODELS 9\n\n\u2022 \u201cCloud Computing refers to both the applications delivered as services over\nthe Internet and the hardware and systems software in the datacenters that\nprovide those services. The services themselves have long been referred to as\nSoftware as a Service (SaaS), so we use that term. The datacenter hardware\nand software is what we will call a Cloud.\u201d - Berkeley RAD [17];\n\n\u2022 \u201cA Cloud is a type of parallel and distributed system consisting of a collec-\ntion of interconnected and virtualised computers that are dynamically pro-\nvisioned and presented as one or more unified computing resources based\non service-level agreements established through negotiation between the\nservice provider and consumers.\u201d - R. Buyya, C.S Yeo, and S.Venugopal\n[18];\n\n\u2022 Cloud Computing is \u201cA large-scale distributed computing paradigm that\nis driven by economies of scale, in which a pool of abstracted, virtualized,\ndynamically-scalable, managed computing power, storage, platforms, and\nservices are delivered on demand to external customers over the Internet.\u201d\n- Foster, Zhao and Raicu, Lu, 2008 [8].\n\nMore recently, the U.S. Government\u2019s National Institute of Standards and\nTechnology (NIST) published on September 2011 his 16th and final version Cloud\nComputing definition [19] as:\n\n\u2022 \u201cCloud computing is a model for enabling convenient, on-demand network\naccess to a shared pool of configurable computing resources (e.g., networks,\nservers, storage, applications, and services) that can be rapidly provisioned\nand released with minimal management effort or service provider interac-\ntion.\u201d.\n\nThe NIST definition lists four deployment models (private, community, public\nand hybrid), three service models (software, platform and infrastructure) and\nfive essential characteristics of cloud computing (on-demand self-service, broad\nnetwork access, resource pooling, rapid elasticity or expansion and measured\nservice). Although this definition is not consensual, e.g. [20][21][22], it is a\nreference for cloud services and deployment strategies broad comparisons and to\nprovide a baseline for cloud computing discussions [23]. Thus, the NIST cloud\ncomputing definition will be the reference used in the development of this project.\n\n2.2 Deployment Models\n\nClouds can be classified based on the underlying infrastructure deployment model.\nA deployment model defines the architecture and the purpose of the cloud and\n\n\n\n10 CHAPTER 2. CLOUD COMPUTING\n\nrefers to the location and management of its infrastructure. As previously men-\ntioned, the NIST [19] identifies four different deployment models for Cloud Com-\nputing: private clouds, community clouds, public clouds and hybrid clouds.\n\n2.2.1 Private Clouds\n\nThe private cloud infrastructures are intended for the exclusive use of a single\norganization with multiple consumers and emulate cloud computing on private\nnetworks. In this operating model the infrastructure may be owned, managed\nand operated by the organization (on premises), a cloud provider (off premises)\nwith some contractual SLA or a combination of them. The on premises and\noff premises variants can be separated in: (i) on-site private clouds and (ii)\noutsourced private clouds. This two models, regarding its physical location, affect\ndifferently the security perimeter of the computation resources. On the on-site\nprivate cloud scenario presented in Figure 2.1 the private cloud may be secured\nand controlled inside the organization by its IT personal. Clients can access the\ncloud from within the security perimeter or outside the security perimeter trough\na boundary controller composed by firewalls and Virtual Private Network (VPN)\nconnections.\n\nFigure 2.1: On-site (on premises) private cloud representation [24].\n\nThe outsourced private cloud scenario represents an off premises type of\nprivate cloud. The cloud provider, sells to the cloud consumer (e.g., an organiza-\ntion) its infrastructure services. In this case, the physical infrastructure belongs\nto the cloud provider, which is responsible for its management, but the provided\nservice must follow the parameters celebrated by the contractual SLA between\nthe two parties regarding security, management, privacy and other policies that\nconcern the cloud consumer. Thus, as presented in Figure 2.2, the outsourced\n\n\n\n2.2. DEPLOYMENT MODELS 11\n\nprivate cloud has two security perimeters that are connected by a protected com-\nmunication link: one implemented by a cloud consumer (on the right) and one\nimplemented by the cloud provider (on the left).\n\nFigure 2.2: Outsourced (off premises) private cloud representation [24].\n\nThe cloud provider is responsible to enforce his security perimeter and to sep-\narate the private cloud resources from the other cloud resources that are outside\nthe security perimeter. Depending on the consumer\u2019s security requirements, dif-\nferent mechanisms such as Virtual Local Area Network (VLAN), VPN, separate\nnetwork segments or clusters can be applied. The cloud consumer implements and\ncontrols within his security perimeter the access to the private cloud resources.\n\n2.2.2 Community Clouds\n\nThe community cloud deployment model is provisioned for exclusive use by a\nspecific community of consumers from organizations that have shared concerns\n(e.g., mission, security requirements, policy, and compliance considerations) [19].\nThe use of a community cloud can help to reduce the costs of organizations as\ncompared to the implementation of individual private clouds as the costs are\nsupported and shared by a larger group. Like in the private model of the cloud,\nthe infrastructure can be owned, managed and operated within the community\nby one or more of its organizations, in an on premises on-site scenario, or outside\nthe community by a third party (cloud provider), in an off premises outsourced\nscenario.\n\nFigure 2.3 presents an example of an on-site community cloud. The repres-\n\n\n\n12 CHAPTER 2. CLOUD COMPUTING\n\nented community is composed by a set of participant organizations. Each one of\nthem may provide cloud infrastructure services, consume cloud services or both.\n\nFigure 2.3: On-site (on premises)community cloud representation [24].\n\nIn Figure 2.3 the organizations providing services are on the left and the ones\nthat consume cloud services are on the right. The inner organizations networks\nare separated from the outside networks by a security perimeter like in the on-site\nprivate cloud scenario. The communication inside the community, i.e., between\nthe organizations that provide services and the ones that consume them, are\nmade through the boundary controller of each organization. Optionally, organ-\nizations can protect the cloud infrastructure from other computation resources\nwith another layer of a security perimeter.\n\nOn the outsourced community cloud, the organizations of a community only\nconsume cloud resources. As Figure 2.4 illustrates, the infrastructure of the cloud\nis provided by a third party with conditions identical to the ones described in the\noutsourced private cloud scenario, i.e., the provided service is specified trough\nSLA agreements. The community organizations access the cloud resources inside\nthe security perimeter trough the boundary controller and the communication\nbetween the provider and consumers is done through a security link between the\ncloud provider and the community\u2019s organizations.\n\n\n\n2.2. DEPLOYMENT MODELS 13\n\nFigure 2.4: Outsourced (off premises)community cloud representation [24].\n\n2.2.3 Public Clouds\n\nThe public cloud model describe cloud computing in the traditional mainstream\nsense whereby resources are dynamically provisioned on a self-service basis over\nthe Internet. The cloud infrastructure that may be owned, managed and operated\nby a business, academic or governmental organization is intended for open use\nby the general public.\n\nFigure 2.5: Public cloud representation [24].\n\n\n\n14 CHAPTER 2. CLOUD COMPUTING\n\nIn a public cloud, security management, infrastructure maintenance and other\noperations are relegated to the cloud provider as illustrated in Figure 2.5. Com-\npared to the private cloud model, the public cloud service offering has a low\ndegree of control and oversight of the physical and logical security aspects such\nas security perimeter to separate computational resources (usually present in the\noutsourced type of the private cloud model).\n\n2.2.4 Hybrid Clouds\n\nAny composition of private, community or public clouds form a hybrid cloud as\nrepresented in Figure 2.6. The individual clouds remain unique entities and, so,\na hybrid cloud can change over time with constituent clouds joining or leaving.\n\nFigure 2.6: Hybrid cloud representation [24].\n\nThe aggregation of clouds are bound together by standardised or proprietary\ntechnologies that enable data and application portability. Some of the benefits\nof this complex model are the access to external clouds during periods of high\ndemand, to perform or to provide backup resources or to run non-core applications\nin a public cloud while maintaining core applications and sensitive data in an on\npremises private cloud.\n\n\n\n2.3. SERVICE MODELS 15\n\n2.3 Service Models\n\nCloud Computing enables hardware and software to be delivered as services. In\nthis context, the term service is used to reflect the fact that they are provided\non demand and paid on a utility computing usage basis. As cloud computing\nhas developed, different vendors offer clouds that have different services associ-\nated. These services are usually described on a Something as a Service (XaaS)\ntaxonomy. According to NIST, these services can be classified into one of three\ndelivery models: Software as a Service (SaaS), Platform as a Service (PaaS), and\nInfrastructure as a Service (IaaS). These three service classes are also known as\nthe Software, Platform, and Infrastructure (SPI) service model.\n\nAs shown in Figure 2.7, the SaaS layer is built on top of the PaaS and this\non the top of the Iaas service layer and all of these services are abstracted from\nthe physical resource layer through a resource abstraction and control layer.\n\nFigure 2.7: Service orchestration (modified from [24]).\n\nThis service orchestration indicates that the provisioning of a top service, like\nSaaS, implies the deployment of the underlying services by the cloud provider.\nThis means that when the cloud consumer requests a SaaS the cloud provider\nassumes most of the responsibilities in managing and controlling the underlying\napplications and infrastructure. In the same way, when a IaaS is required by the\ncloud consumer the cloud provider has control over the physical hardware and\n\n\n\n16 CHAPTER 2. CLOUD COMPUTING\n\ncloud software that makes the provisioning of infrastructure services possible,\ni.e., the physical servers, network equipments, storage devices, host Operating\nSystem (OS) and hypervisors for virtualization, while the cloud consumer uses the\navailable computing resources (e.g., a virtual computer), for their fundamental\ncomputing needs. Compared to SaaS and PaaS cloud consumers, an IaaS cloud\nconsumer has access to more fundamental forms of computing resources and thus\nhas more control over the more software components in an application stack,\nincluding the OS and network [24]. Depending on the contracted service, the\ncloud consumer can choose from a variety of solutions, i.e., from top level software\napplications to lower level computation. The SPI service model is described in\nthe following Infrastructure as a Service, Platform as a Service and Software as\na Service subsections.\n\n2.3.1 Infrastructure as a Service\n\nThis type of service provides on-demand virtualized resources (e.g., virtual com-\nputers, virtual storage, Virtual Network (VN) and other hardware assets as re-\nsources to the cloud consumer (system administrators). The service consumer\ngets access to the contracted virtual machines, network-accessible storage and\nnetwork infrastructure components (e.g., firewalls and configuration services).\nThe cloud infrastructure orchestration will be discussed in detail in the Cloud\nInfrastructure Platforms chapter since it is the main focus of this work.\n\nThe IaaS cloud provider manages all the physical infrastructure hardware and\nhas administrative control over the resource abstraction and control layer that\nwraps the Virtual Machine Monitor (VMM), also known as Hypervisor, billing\nsoftware solutions, servers host OS and other necessary tools to implement the\ndesired service. The consumer, on the other hand, can make requests to the\nhypervisor through the interface provided by the service vendor to manage and\ncreate new virtual machines, chose the guest OS to run on the VM and control\nall the middle-ware and applications that run on top of the guest OS.\n\nThe service usage fees for a IaaS are calculated typically per Central Unit\nProcessor (CPU) hour, GB stored per hour, network bandwidth consumed and\nvalue-added services used (e.g., monitoring and automatic scaling) [25]. The\nAmazon Web Services [1] and LunaCloud Cloud Server [26] are two examples of\nIaaS providers.\n\n2.3.2 Platform as a Service\n\nIn a platform as a service model, the PaaS provider supports the development,\ndeployment and management process of the PaaS consumers (developers) by\nproviding tools such as application programming interfaces, Integrated Devel-\n\n\n\n2.4. CHARACTERISTICS 17\n\nopment Environments (IDE), development version of cloud software, Software\nDevelopment Kits (SDK) as well as deployment and management tools.\n\nIn comparison to the IaaS model this service offers a higher level of abstraction\nto the cloud consumer. The PaaS provider manages the cloud infrastructure, the\noperating system and the enabling software whereas the PaaS consumers are\nresponsible for installing and managing the applications that are deploying, to\ncomply with the programming language requirements and distribution as well as\nwith the payment terms.\n\nIn this service model cloud consumers have limited or no access to the underly-\ning infrastructure platform, i.e., network, servers, operating systems, or storage.\nThe billing procedures are implemented according to, processing, database stor-\nage and network resources consumed by the developed application as well as the\nduration of the platform usage. The Google AppEngine [2] is an example of an\nPaaS provider.\n\n2.3.3 Software as a Service\n\nThe software as a service model resides on the top of the SPI stack. This service\nprovides applications accessible trough a Web browser to the cloud consumers.\nIn comparison with the other two underlying models (PaaS and IaaS), the SaaS\nis the most abstract service solution from a consumer\u2019s perspective.\n\nThe SaaS provider is responsible for the underlying infrastructure, the plat-\nform where the applications are located and for the installation and maintenance\nof the provided applications. On the other hand, the SaaS consumer is only\nresponsible for the accession, uploading, maintaining and interaction with his\ndata.\n\nThe SaaS can ease the burden with software licensing and maintenance and\nthe billing system applied is based on the number of end users, the time of use,\nthe network bandwidth consumed, the amount of data stored or duration of\nstored data. Examples of SaaS providers are Google Apps for Business [27] and\nSalesforce productivity applications [28].\n\n2.4 Characteristics\n\nDue to the lack of standardization and consensus regarding cloud computing, the\ncharacteristics that define the cloud vary between all interested parties [29][30][31].\n\nIn order to promote simplification and standardization, the NIST identifies\nfive essential and generally accepted main characteristics to describe the beha-\nviour of cloud computing [19]: (i) on-demand self-service; (ii) broad network\naccess; (iii) resource pooling; (iv) scalability; and (v) measured service. These\ncloud characteristics will be described in the following subsections.\n\n\n\n18 CHAPTER 2. CLOUD COMPUTING\n\n2.4.1 On-demand Self-service\n\nThe cloud consumer can use resources of the contracted cloud computing ser-\nvice (e.g., system and network resources) as needed without human interaction\nbetween consumer and cloud provider in an on-demand form. The resources\ncan be controlled trough a user-friendly self-service interface, i.e., a Web page\ndashboard or Command Line Interface (CLI) console, providing effective means\nto manage the service offerings and promoting cost savings to both consumer\nand cloud provider. This cloud feature is important since it solves the resource\ndemand fluctuation problem and reduces the costs by avoiding planning ahead\nthe installation of additional resources to meet peak demands and underusing\ninstalled resources.\n\n2.4.2 Broad Network Access\n\nThe access to resources hosted in the cloud is available over private networks, e.g.,\nprivate cloud and community cloud implementations, or over the Internet, e.g.,\npublic clouds, trough standard mechanisms. Therefore, the broad network access\nprovides platform independence as the services can be accessed from all types of\noperating systems and different types of computating platforms, e.g., desktops,\ntablets or smart phones. This feature provides access to resources (mainly in\nthe public cloud scenario) from a wide range of locations as long as they have\nInternet access.\n\n2.4.3 Resource Pooling\n\nThe resources pooled together by the cloud provider are assigned and reassigned\nto serve multiple consumers in a multi-tenant model. These physical and virtual\nsystems are dynamically allocated or reallocated as needed. The resources can\nbe physically located at many geographic locations and assigned as virtual com-\nponents of the service as they are requested, creating, as defined by NIST [19],\n\u201ca sense of location independence in that the customer generally has no control\nor knowledge over the exact location of the provided resources\u201d.\n\n2.4.4 Rapid Elasticity\n\nRapid elasticity refers to the ability of the cloud to expand or reduce provisioned\nresources quickly and efficiently, fulfilling the requirements of the on-demand,\nself-service, characteristic of cloud computing. Through the broad network access\ndiverse computing infrastructures can cooperate to allocate these resources (e.g.,\nfederation of clouds in a hybrid cloud or a community cloud scenario), manually\nor automatically, appearing to the consumer as a large, limitless pool of dynamic\nresources that can be purchased at any time and in any quantity. This feature\n\n\n\n2.5. CONCERNS 19\n\nis normally negotiated between cloud providers and cloud consumers through a\nSLA. In this type of agreement, the provider negotiates with the client the time\nresponse for the provision of the resources (as they are in fact finite) in order\nto respond to the customer\u2019s instant demands as well as the penalties for not\nmeeting the negotiated contract terms (in this case the accorded time response).\n\n2.4.5 Measured Service\n\nDue to the service oriented characteristics of cloud computing, the amount of\ncloud resources used by the cloud consumer are monitored, measured and repor-\nted as they are allocated or consumed. This procedure is important for quality\nof service requirements, billing procedures and to provide transparency for both\nthe cloud provider and the cloud consumer. As previously mentioned, cloud com-\nputing uses an utility computing model in which the consumer is charged based\non the level of service provided. For that purpose known indicators (metrics)\nsuch as amount of storage, network resources or level of processing power are\nused to measure the amount of consumed resources. Normally, this information\nis provided automatically via the consumer self-service interface.\n\n2.5 Concerns\n\nAlthough cloud computing has several benefits, there are also significant con-\ncerns that challenge this new computing model. Generally, the advantages of\ncloud computing are more compelling to Small and Medium sized Enterprises\n(SME) than to large organizations. Larger organizations can easily create and\nmaintain IT teams and infrastructures as well as develop custom software solu-\ntions customised to their own needs. On the other hand, the adoption of cloud\ncomputing implies application customization, control delegation and, most im-\nportantly, cloud provider dependency. Outsourced services evade the physical,\nlogical and personal control of an organization IT staff and data outside a com-\npany\u2019s control security perimeter pose an intrinsic risk threat. These concerns\ncan make corporate executives hesitate to migrate to cloud computing.\n\nThree of the most significant barriers to the adoption of cloud computing are:\n(i) unpredictable performance; (ii) security risks and (iii) data privacy.\n\n2.5.1 Unpredictable Performance\n\nCloud consumers have certain expectations regarding the service level provided\nby the cloud. Some of these expectations include availability of service and over-\nall performance. Due to the abstracting nature of the cloud provided services,\nthe cloud consumer ignores how or where the contracted service runs on top of\nthe physical infrastructure, the number of physical machines used or their loca-\n\n\n\n20 CHAPTER 2. CLOUD COMPUTING\n\ntion. On the other hand, the economic benefits of cloud computing are based on\nthe ability to increase the usage level of the infrastructure trough multi-tenancy.\nMulti-tenancy introduces service under-performances, since it cannot ensure that\nthe activities of different consumers are totally independent and disjoint. The\ncloud provider\u2019s challenge is, thus, to efficiently manage all the virtualized re-\nsources so that service performance remains as unaffected as possible by the\nnumber of consumer cloud resource demands.\n\nPhysical resources such as CPU cores, disk space and network bandwidth\nmust be divided and shared among virtual machines, running potentially hetero-\ngeneous workloads. The mapping between resources and corresponding virtual\nmachines becomes increasingly complex to ensure the profitability of the physical\ninterface. The VM mapping is dynamic and provides mechanisms to suspend,\nmigrate and resume virtual machines by pre-empting low-priority allocations in\nfavour of high-priority ones. These operations combined with load balancing,\nbackup and recovery mechanisms cause performance variation.\n\nOther important factors are external, non controllable network related para-\nmeters such as latency, introduced by the communication link to the data centre\nwhere the servers are hosted, as well as competing traffic in the communications\nlink.\n\nCloud consumers expect a fixed performance/fee ratio, but this cannot be\nguaranteed as the performance may fluctuate. Furthermore, the cloud end user\nis unable to predict these variations, their magnitude and duration because the\nservice measurements are made by the cloud provider services and not at the\nconsumer premises. In these cases, the SLA is used as a warranty to compensate\nthe cloud under-performance.\n\n2.5.2 Security Risks\n\nSecurity is one of the main problems for enterprises to move their \u201cin-house\u201d\ndata to public clouds or outsource private clouds. Most of the cloud providers do\nnot guarantee the security of data while being transferred to the cloud [32]. On\nthe other hand, because cloud computing represents a relatively new computing\nmodel, there is a great deal of uncertainty regarding how to provide security at\nthe network, host, application and data levels.\n\nCompared to traditional technologies, the cloud has many specific features\nthat make the traditional security mechanisms (e.g., identity, authentication and\nauthorization) insufficient [33]. Additionally, the cloud service models are inter-\ndependent since, on one hand, PaaS and SaaS are hosted on top of IaaS and, on\nthe other hand, PaaS provides the platform to SaaS. Due to these dependencies,\nany attack to a cloud service layer can compromise the upper layers, e.g., a breach\nin the infrastructure service will impact the security of both PaaS and SaaS.\n\n\n\n2.5. CONCERNS 21\n\nRelationships and dependencies between cloud models can also be a source of\nsecurity risks. One SaaS provider may rent a development environment from a\nPaaS provider, which might also rent an infrastructure from other IaaS provider.\nThis may result in an inconsistent combination of security models because each\nprovider implements its own security model.\n\nThere are security issues in all three SPI model layers: (i) SaaS applications\nsuffer from vulnerabilities and accessibility issues; (ii) PaaS, which depend on\nthird-party relationships and development life-cycle, inherits security issues; and\n(iii) IaaS experiences virtualization vulnerabilities and malicious VM images.\nSome of the countermeasures used to minimize security threats are the imple-\nmentation of dynamic credentials, digital signatures, data encryption, frameworks\nto secure virtual networks and hypervisor data flow inspection [32].\n\n2.5.3 Data Privacy\n\nPrivacy issues are increasingly important in the World Wide Web (WWW). The\ndelegation of data control to the cloud provider when utilizing a cloud service\nmakes cloud consumers uncomfortable because of the risks to data integrity, con-\nfidentiality and privacy principles as data becomes accessible to an augmented\nnumber of parties.\n\nSecurity breaches like the ones described in the security section can lead to\nprivate data access and exposure, but data confidentiality can also be breached\nunintentionally through, e.g., data remanence. Data remanence is the residual\nrepresentation of data that have been erased or removed. Due to the virtual\nseparation of logical drivers and lack of hardware separation between multiple\nusers on a single infrastructure, the remaining residual information stored in the\nphysical storage units may lead to the unwilling disclosure of private data or to\nthe intentional exploitation of that vulnerability.\n\nAnother fact that concerns enterprises regarding data privacy protection is\nthe geographical distribution of the information inside the clouds. The fault\ntolerance and backup mechanisms that cloud providers utilize to prevent system\nfailures as well as to provide data storage resources to fulfil cloud consumers\nspace requirements lead to data fragmentation across geographic locations. The\nchallenge with this distribution is that information can be stored in different\ncountries with diverse data protection laws and, thus, cloud providers cannot\nensure personal data requirements against the countries legal framework.\n\nThe solution to this problem involves the creation of initiatives like the Mad-\nrid International Conference [34] with the aim to create international agreements\nwhere a set of principles and rights guaranteeing the effective and internation-\nally uniform protection of privacy with regard to the processing of personal data\n\n\n\n22 CHAPTER 2. CLOUD COMPUTING\n\nbetween different countries to facilitate the international flow of personal inform-\nation and ease data privacy concerns.\n\nOn the other hand, the adoption of proactive-measures requirements can be\nmet trough the implementation of Privacy Enhancing Technologies (PET)1. PET\nsafeguards the individual data privacy and rights by protecting personal data and\npreventing its unnecessary or undesired processing.\n\n2.6 Virtualization Technologies\n\nVirtualization is a key technology that enables part of the most important char-\nacteristics of cloud computing (e.g., resource pooling and rapid elasticity). This\ntechnology has been proposed and developed over a relatively long period by\nInternational Business Machines (IBM) in 1960-1970 [35][36][37]. Its purpose in\ncloud computing is to divide and abstract the resources of the physical infra-\nstructure into multiple segregated virtual systems, virtual machines with virtual\nnetworks and virtual storage, through which the cloud services are provided.\n\n\u201cA virtual machine is taken to be an efficient, isolated duplicate of the real\nmachine.\u201d [38]. It has its own address space memory, processor resource alloca-\ntion and device Input/Output (I/O) trough virtual device drivers. These virtual\nsystems are independent from each other and provide a complete platform for\nsupport and execution of an operating system. Normally, an OS running on a\nvirtual machine is called a Guest OS and the OS that runs on top of the physical\nhardware is called Host OS. The VM lifetime has six phases: create, suspend,\nresume, save, migrate and destroy. Multiple virtual machines can run simultan-\neously on the same physical infrastructure node (i.e., physical server) and each\nVM, residing in the same computing node, can execute different Guest OS.\n\nTo enable virtualization a low-level program called Virtual Machine Monitor\nor Hypervisor is required to provide an environment for programs identical with\nthe one of an physical machine. The hypervisor is in control of the system re-\nsources and provides them access to virtual machines and management functions\nregarding the existing VM on the computing node. The access to resources relies\non load balancing techniques that are responsible for mapping logic addresses\nto physical addresses and for the management of workloads. Depending on the\ninstallation of the virtualization layer, two types of hypervisors can be identified:\n\n\u2022 Type 1 hypervisors: The virtualization layer containing the hypervisor is\ndirectly installed on top of the physical infrastructure node without the\nneed for a host operating system. This architecture, also known as bare\nmetal virtualization, enables direct access to the hardware resources;\n\n1Set of computer technologies that allow online users to protect the privacy of their person-\nally identifiable information provided to and handled by Web services or applications.\n\n\n\n2.6. VIRTUALIZATION TECHNOLOGIES 23\n\n\u2022 Type 2 hypervisor: The virtualization layer is installed on top of the host\noperating system rather than on top of the infrastructure hardware of the\nnode. This architecture has no direct access to the hardware resources.\n\nFigure 2.8 shows a diagram of Type 1 and Type hypervisors.\n\nFigure 2.8: Type 1 and Type 2 hypervisors.\n\nType 1 hypervisor architectures are more robust and, due to the close proxim-\nity to hardware resources, they can achieve higher virtualization efficiency than\nType 2 hypervisors. Type 2 hypervisors are used mainly on client systems where\nefficiency is less critical as well as on systems where support for a broad range of\nI/O devices is important and can be provided by the host operating system [39].\n\nThere are also virtualization methods that are classified according to whether\nor not the Guest OS kernel needs to be modified, e.g., Full virtualization and\nPara-virtualization, or whether the kernel of an operating system allows for mul-\ntiple isolated user-space instances, instead of just one, e.g., OS-level virtualiza-\ntion. These methods will be presented in the following subchapters.\n\n2.6.1 Full Virtualization\n\nFull virtualization was fist introduced by IBM with the Virtual Machine Facil-\nityl370 (VM/370) operating system [40]. This virtualization method, illustrated\non Figure 2.9, emulates a full hardware environment of a computing node, through\nbinary code translation/rewriting, into independent virtual machines created on\ntop of the hypervisor layer. The binary translations are used to adapt the non-\nvirtualizable instructions into virtualized ones. More recently, the full virtualiz-\nation method was enhanced with the appearance of the hardware virtualization.\n\n\n\n24 CHAPTER 2. CLOUD COMPUTING\n\nHardware virtualization provide mechanisms to run unmodified guest virtual ma-\nchines without the overheads inherent in the standard full virtualization method.\nIt started to be supported by Intel and Advanced Micro Devices (AMD) x86\narchitecture processor families through the Intel VT-X and AMD Virtualization\n(AMD-V) hardware assisting virtualization technologies [41][42].\n\nFigure 2.9: Full virtualization (modified from [37]).\n\nSince the VM emulates the full system, the guest operating system does not\nneed to be modified, meaning that both open-source and proprietary OS work\nwith this type of virtualization methodology. The guest OS kernel is not aware\nof the virtualization and communicates directly to the hypervisor layer as if it is\nthe hardware base physical infrastructure. In this approach, the hypervisor has\nto manage, control and map the requests from all guest OS to the limited amount\nof physical resources. As a consequence, the performance experienced is lower\nwhen compared to native OS. On the other hand, the actual quantity of hardware\nresources, provided by the computing node, define the limits to the creation of\nsimultaneous VM. Two examples of known commercial and open-source software\ntools for server virtualization that implement full virtualization are: VMware and\nKernel-based Virtual Machine (KVM) [43][44].\n\n2.6.2 Para-virtualization\n\nIn para-virtualization the guest OS kernel is modified to become aware of the\nhypervisor as illustrated on Figure 2.10. In this virtualization methodology, OS-\n\n\n\n2.6. VIRTUALIZATION TECHNOLOGIES 25\n\nlevel information about the virtual machines can be passed explicitly from the\nguest OS to the hypervisor without the necessity to trap or translate every OS\ncall.\n\nFigure 2.10: Para-virtualization (modified from [37]).\n\nDue to the described behaviour, the para-virtualization approach results in\nlower virtualization overhead than full virtualization , providing close to native\nguest OS performance. However, this methodology is less flexible than full virtu-\nalization because it requires OS-level modifications, which is a characteristic non\nimplementable with legacy and close-sourced commercial OS. An open-source ex-\nample of a software tool for server virtualization capable of para-virtualization\nimplementation is Xen [45].\n\n2.6.3 OS-level Virtualization\n\nThe operating system level virtualization differs from the other two virtualization\nmethodologies by utilizing the kernel of a host OS embedded with a virtualiz-\nation layer. This approach allows the creation of multiple user-space instances\nknown as containers, Virtual Private Server (VPS) or Virtual Environment (VE)\nas illustrated on Figure 2.11. Since normal OS calls are used and no emulation\nis performed, the container performance is practically native with little imposed\noverhead. However, this methodology is not as flexible as the other two virtualiz-\nation approaches since it cannot use guest OS that are different from the host OS,\nas the OS kernel is shared by the host OS and all guest OS containers. Examples\n\n\n\n26 CHAPTER 2. CLOUD COMPUTING\n\nof software tools capable of OS level virtualization implementation are OpenVZ\nand Parallels Virtuozzo Containers [46][47].\n\nFigure 2.11: OS level virtualization.\n\n2.7 Business Perspective\n\nAn important factor that differentiates cloud computing from the other com-\nputing technologies is its ability to converge existing technologies with an utility\nmodel making the cloud flexible to the adoption of different business perspectives.\nThis approach enables IT efficiency, whereby the power of modern computers is\nutilized more efficiently trough highly scalable hardware and software resources,\nas well as business agility, whereby IT can be used as a competitive tool trough\nrapid deployment, parallel batch processing, use of computing-intense business\nanalysis and mobile interactive applications that respond in real time to user\nrequirements. Thus, cloud attractiveness increases as organizations discover that\ntheir substantial capital investments in information technology are often grossly\nunderutilized and that the maintenance and service costs do not pay [48]. On the\nother hand, the adoption of an utility model enables cloud computing services to\nreuse the traditional electricity, water, telecommunications price models, includ-\ning the pay as you go model or metering model, the subscription model and the\ncombination of both.\n\nThis chapter will describe the business models used for the three service layers\n\n\n\n2.7. BUSINESS PERSPECTIVE 27\n\n(IaaS, PaaS and SaaS), the existing stakeholders and the contracted SLA between\nservice vendors and service consumers in the cloud.\n\n2.7.1 IaaS Business Model\n\nThe infrastructure layer focuses on enabling technologies with the provision of\nstorage capabilities and computer power through an outsourcing business model.\nThe processing power, use of storage devices, servers and other hardware that\ncomposes the IaaS layer are standardized, regulated products allowing the adop-\ntion of a metered usage model, i.e., following the utility model.\n\nWhen cloud providers define the IaaS layer pricing models, they need to\nconsider: (i) the availability of the infrastructure and the financial and legal\nconsequences of any disruptions; (ii) the readiness of the service regarding both\nplace and time; (iii) the friendliness of the consumer interface; (iv) the capabil-\nity to respond to user demand fluctuations; (v) the determination of a realistic\nthreshold in order to maintain QoS; and (vi) the legal issues (e.g., physical infra-\nstructures that are outside the cloud consumer\u2019s national boundaries, involving\nthe provided infrastructure).\n\nAll the factors presented need to be considered and their costs included in the\nprice models. This way, the pay as you go model or metering model combined\nwith the subscription model, to allow for more flexibility, are the most frequent\nmodels used in the IaaS.\n\n2.7.2 PaaS Business Model\n\nThe platform layer offers solutions on top of a cloud infrastructure that provide\nvalue-added services from both a technical and a business perspective.\n\nAll PaaS solutions are based on integration of SaaS applications in the un-\nderlying infrastructure via a cloud-capable PaaS development environment. The\nlargest variant involves extensive middle-ware components. A further aspect of\nPaaS will give rise to further business model components: billing, metering and\nmonitoring. These components provide the pay as you go price model with the\nnecessary inputs. Creation of full-service platform solutions means that independ-\nent software vendors (ISV) and system integrators IT departments can develop\nand deliver on line applications using third-party infrastructure services [49]. So,\nthe PaaS cloud provider may allow Independent Software Vendors (ISV) to de-\nvelop and sell their software for a percentage of the sales or for an additional fee\nfor the utilization of the environment platform.\n\n\n\n28 CHAPTER 2. CLOUD COMPUTING\n\n2.7.3 SaaS Business Model\n\nThe SaaS layer delivers software services on demand on top of the platform and\ninfrastructure layers (which are transparent to the end user). SaaS can drastically\nreduce software expenses (e.g., software purchase price or licensing) that have\nbeen a major part of IT expenses of companies. It can also offer ISV opportunities\nto lower their application development costs, reduce their time to market (e.g.,\nutilizing Web stores), extend the service offerings of other vendors (as is the\ncase of SME ISV) and create new software solutions. These opportunities are\ndivided between the larger ISV like Microsoft [50], Google [51] and Facebook\n[52], which take advantage of their cloud infrastructure to focus on distribution\nand marketing, and the SME ISV, which build solutions around the offerings of\nlarger ISV to provide specialized services, following the example of the mobile\napplication model.\n\nThe SaaS layer provides a diverse number of applications and, so, the business\napproach has to be more flexible. In terms of payment model, the pay as you go,\nthe subscription model, the \u201cfreemium\u201d and free models are utilized to promote\nnew products and services or to explore new ways to attract customers. The\nsubscription model is the most frequently adopted model.\n\nThe \u201cfreemium\u201d model is a combination of the free model and the subscription\nmodel. It offers a limited free subscription, temporal or with service limitations,\nand an upgrade to a premium subscription version where all the program\u2019s fea-\ntures are available. The free version helps to demonstrate the product while\nit intends to encourage users to upgrade to the premium version. On the other\nhand, the free model is supported most exclusively by advertising where the profit\nis obtained trough the number of clicks or views per minute.\n\n2.7.4 Stakeholders\n\nIn the previous computing paradigms the main stakeholders were the system pro-\nviders and their consumers. Whereas the providers were responsible for the sales,\ninstallation, licensing, consulting and maintenance of the involved technologies,\nthe consumers used, maintained and owned the provided systems.\n\nWith the technological shift to the cloud, some of the previously described\nfunctions of these stakeholders changed and due to the outsource characteristics\nof the cloud deployment services two new important parties must be considered\nin addition to the traditional stakeholders: the cloud enablers and the cloud\nregulators [48].\n\nIn cloud computing the consumers are effectively subscribers that, in com-\nparison with the previous computing technologies, only purchase from the cloud\nproviders the use of the system on an operational expense basis. The cloud pro-\n\n\n\n2.7. BUSINESS PERSPECTIVE 29\n\nviders own and operate the cloud computing systems in order to deliver third\nparty services. They provide the infrastructure, systems and software, perform\nthe corresponding maintenance and implement the pricing of the cloud services.\n\nCloud enablers are organizations that sell products or services that facilitate\nthe delivery, adoption, use and management of the cloud infrastructures. This\ntype of stakeholder is an intermediary class that stands between the cloud pro-\nvider and the service consumer, taking advantage of the currently lake of core\ncompetencies from the infrastructure providers in the interaction with the cus-\ntomers. Some cloud enablers examples are RightScale and Vordel [53][54].\n\nThe cloud regulators, on the other hand, regulate all aspects of the cloud\ncomputing value-chain and are, typically, sovereign government bodies and inter-\nnational entities. Their main functions are the provision of regulations regarding\ndata, residency, privacy and other related cloud computing features, harmonisa-\ntion of a cross border regulations and promotion of inter-governance cooperation\nin the adoption and formulation of new cloud computing regulations.\n\n2.7.5 Service Life-Cycle\n\nA service life-cycle describes the different phases of a particular service provided\nby a cloud vendor and includes the service delivery systems necessary to meet\nthe QoS objectives specified in SLA, i.e., all stages of the SLA life-cycle. The\nSLA life-cycle is composed of four stages:\n\n\u2022 SLA template design - The service provider defines the types of SLA he is\nwilling to propose in order to ensure that the agreed QoS guarantees are\nrealistic;\n\n\u2022 SLA negotiation - The service provider and customer attempt to define the\nterms of the SLA that will bind their business relationship, i.e., that the\nagreed QoS guarantees are realizable and that the end-to-end QoS require-\nments are satisfied;\n\n\u2022 SLA runtime - The service provider and customer verify that the agreed\nQoS guarantees are satisfied;\n\n\u2022 SLA (template) archiving - The established SLA are stored for future reuse\nor learning.\n\nOn the other hand, the overall life-cycle, represented in Figure 2.12, consists\nof the following stages:\n\n\u2022 Design and development - consists on the development of artifacts needed\nfor the service implementation;\n\n\n\n30 CHAPTER 2. CLOUD COMPUTING\n\n\u2022 Service offering - includes the SLA template design, where to offer the\nintended service and results in the specification of SLA templates;\n\n\u2022 Service negotiation - includes parts of the SLA negotiation, represents the\nactual negotiation between customers and provider and results in an agreed\nSLA;\n\n\u2022 Service provisioning - covers parts of the SLA negotiation, represents all\nactivities required in system preparation and set-up for service operation,\nincluding booking, deployment (if needed) and configuration;\n\n\u2022 Service operations - includes SLA runtime, i.e., it is a stage where the actual\nservice instance is up and running and where adjustments can be made to\nenforce a SLA;\n\n\u2022 Service decommissioning - represents the end of a service instance (the\ncustomer can no longer access it) and corresponds to the stage where the\nSLA (template) archiving is done.\n\nFigure 2.12: Service life-cycle [55].\n\n2.7.6 Service Level Agreements\n\nWhen consumers adopt cloud computing solutions, the quality and reliability\nof the provided services are essential features. As previously described in the\nConcerns section, it is impossible to fulfil all consumer expectations from the\n\n\n\n2.7. BUSINESS PERSPECTIVE 31\n\nservice provider perspective and, hence, a balance needs to be achieved via a\nnegotiation process. At the end of the negotiation, provider and consumer commit\nto an agreement. In a Service Oriented Architecture (SOA) this agreement is\nreferred to as a Service Level Agreement [55].\n\nThe establishment of SLA is frequently adopted in the software and telecom-\nmunications domain to specify a mutual understanding regarding the transactions\nbetween providers and consumers. Typically, a SLA is a bilateral binding state-\nment signed between a service provider and a service consumer stating the agreed\nterms and conditions of the given service [9]. Additionally, it also defines the re-\nmedial actions and the penalties if performance falls bellow the agreed standard.\n\nIdeally, SLA can be negotiated between any type of service providers and\nconsumers but, in reality, in cloud-based business scenarios, SLA are usually not\nnegotiated in Business to Consumer (B2C) and in Business to Business (B2B)\ntransactions involving small clients (e.g., SME) [56].\n\nA SLA acts as a legally enforceable document traditionally composed of:(i)\nsubject terms; (ii) scope of rights; (iii) financial terms; (iv) representation; (v)\nservice credits, credit requests and compensation procedures; (vi) evolution and\nsupport terms; (vii) warranty; (viii) indemnification; and (ix) limitation of liab-\nility. Although a standard cloud SLA model does not exist, the subject has been\nstudied by the European Telecommunications Standards Institute (ETSI) [57].\n\nIn the Web service domain there are two main specifications for describing a\nSLA: (i) Web Service Agreement (WS-Agreement) from the Open Grid Forum\n(OGF) [58]; and (ii) Web Service Level Agreement (WSLA) language and frame-\nwork from IBM [59].\n\nThe WSLA was proposed in 2001 and it allows the creation of machine-\nreadable SLA for services implemented using Web services technology that define\nservice interfaces in the Web Services Description Language (WSDL). This frame-\nwork comprises several monitoring services that can be replaced with services de-\nveloped by third parties. An SLA created using the WSLA language is normally\nformed the following sections:\n\n\u2022 Definition of the parties - contains the description of the service provider,\nthe service consumer and, eventually, the third parties involved (when com-\nmission part of the SLA management responsibility to additional supporting\nparties). The information provided about each party should include contact\ninformation and a description of the organization as well as the name of a\nWSDL file that contains the interface description of the Web services the\nparty implements;\n\n\u2022 Definition of the services and their operations - contains the description of\nthe service provider interfaces. The services are represented by service ob-\n\n\n\n32 CHAPTER 2. CLOUD COMPUTING\n\njects and each service object is associated with one or more SLA parameters.\nThis section specifies the relevant service level parameters and respective\nmetrics as well as indicates which party is responsible for measuring each\nmetric and how the measuring should be done;\n\n\u2022 Obligations of the agreement - specifies the conditions (e.g., the average\noperation response time shall not exceed a predefined value) and the ac-\ntion guarantees, i.e., the actions the parties commit to perform in a given\nsituation.\n\nOn the other hand, the WS-Agreement specification appeared in 2007 and\ndefines an XML-based language for agreements as well as a protocol for ad-\nvertising the capabilities of service providers, creating agreements between ser-\nvice consumers and providers and monitoring agreement compliance [58]. WS-\nAgreement also provides an eXtensible Mark-up Language (XML) schema that\ndefines the overall structure of an agreement document and, in addiction to the\nWSLA, defines a protocol for negotiating and establishing agreements dynamic-\nally based on Web services. The main differences to the WSLA are the use of:\n(i) SLA templates that embody all the customization aspects of an agreement;\n(ii) an extensible XML structure containing several sections where the intended\nusers are expected to define domain-specific elements and properties; and (iii)\nmetrics defined by a domain-specific extension rather than associated with the\nparameters used in the agreement. The XML document of the WS-Agreement\nconsists of the following parts:\n\n\u2022 IDentification (ID) - holds an unique mandatory agreement ID followed by\nan optional name;\n\n\u2022 Parties - contains the identification of the different parties, various metadata\nabout the agreement (such as an expiration time and template ID in case the\nagreement was created following a template) and user-defined attributes;\n\n\u2022 Interface declarations - defines all the information about the functional\ncapabilities of the service. It encapsulates the information contained in\ntraditional service description (e.g., WSDL documents);\n\n\u2022 Agreement terms - specifies the service terms and guarantee terms. Ser-\nvice terms identify and describe the services that are the subject of the\nagreement and also comprise the measurable service properties and exposed\nproperties that can be used to express service level objectives. Guarantee\nterms specify the QoS that the parties are agreeing to, e.g., minimum avail-\nability of a service, the number and type of CPU that the service provider\nshould make available or the average response time for requests.\n\n\n\n2.8. CONCLUSIONS 33\n\nAn SLA template document has the same structure as an SLA, but is not\ninstantiated.\n\n2.8 Conclusions\n\nCloud computing represents a switch from the in-home generated computing ap-\nproach to the utility-supplied computing resources delivered over the Internet as\nWeb services. This new IT paradigm is essentially a business model supported\nby the aggregation of several well developed technologies, i.e., resource virtualiz-\nation, Internet technologies (e.g., Web services, SOA and Web 2.0), distributed\ncomputing (e.g., cluster and grid computing) and automated management sys-\ntems (e.g., data centre automation).\n\nThe exact definition of Cloud Computing is not consensual among the IT\ninstitutions and scientific community. However, the NIST definition provides a\ncommon framework for the discussion of this subject. The NIST Cloud Com-\nputing features enable the adoption of different business perspectives, according\nto the utility, service and deployment models used, introducing advantages to\nthe cloud consumers, mainly Small and Medium Enterprises. The reliance on\nthird-party service providers is a major issue, i.e., organizations and enterprises\ncontemplating the migration to the cloud are concerned with data privacy, se-\ncurity and unpredictable performance. In terms of service features, the Service\nLevel Agreements are essential to define the duties and obligations, i.e., the ser-\nvice contract, binding cloud consumers and service providers. However, standard\ncloud consumers are offered pre-defined non-negotiable SLA, favouring the service\nproviders.\n\nNormally, large enterprises prefer to build and manage the cloud IT infra-\nstructure themselves. They tackle the identified concerns by deploying private\nclouds that eventually evolve into hybrid ecosystems, using the public cloud to\nexpose service offerings that amortize the investment on the IT infrastructure.\n\nThe following chapter analyses and compares different software solutions that\nuse different cloud IT infrastructures for the provision of IaaS.\n\n\n\n\n\nChapter 3\n\nCloud Infrastructure Platforms\n\nPhysical and virtual resources (e.g., servers, storage and networks) present a chal-\nlenge for IaaS providers when building a cloud infrastructure as their organization\nand availability must be ensured to allow a rapid and dynamical provision of re-\nsources to the end application. This resource orchestration is done by software\ntool-kits composed of a Cloud Infrastructure Manager (CIM) with remote and\nsecure interfaces for creating, controlling and monitoring virtualized resources on\na IaaS cloud, and a Virtual Infrastructure Manager (VIM) that provides prim-\nitives to schedule and manage virtual machines across multiple physical hosts.\nBelow the VIM, there are the hypervisors that provide simple primitives (e.g.,\nstart, stop, suspend) to manage the virtual machines life-cycle of one physical\nhost (node).\n\nFigure 3.1: VM management layers of a cloud infrastructure platform [37].\n\n35\n\n\n\n36 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nThrough these management stacks, illustrated on Figure 3.1, physical re-\nsources are abstracted into a virtualized pool of resources accessible by the cloud\nconsumer through virtual machines. Thus, CIM and VIM as well as the hyper-\nvisors (virtual machine managers) are the essential set of tools to deploy cloud\nIaaS platforms from the organizations IT physical infrastructures.\n\nThe software tool-kit that implements CIM and VIM aggregates resources\nfrom multiple nodes, presenting a uniform view to the user and applications.\nThe features available to manage VM can either be basic or advanced depending\non the type of the software, i.e., if it is focussed on the cloud infrastructure man-\nagement or on the virtual infrastructure management (presenting more advanced\nfeatures to manage virtual machines). From a general point of view, software\ntool-kits can offer the following main features [25]: (i) Virtualization Support;\n(ii) Self-Service, On-Demand Resource Provisioning; (iii) Multiple back-end Hy-\npervisors; (iv) Storage Virtualization; (v) Interface to Public Clouds; (vi) Virtual\nNetworking; (vii) Dynamic Resource Allocation; (viii) Reservation and Negoti-\nation Mechanism; and (ix) High Availability and Data Recovery.\n\nThe first commercial cloud infrastructure that contributed to the popular-\nization of the IaaS paradigm was arguably initiated by Amazon\u2019s public Elastic\nCompute Cloud (EC2) [60] back in 2006, offering virtual machines for US $0.10/h,\nusing both a simple Web interface and a programmer-friendly API. Although the\ninitial ecosystem was focussed around the utilization of public clouds, the appear-\nance of open source tools enabled organizations to build their own IaaS clouds\nutilizing their internal infrastructure, increasing the appearance and the deploy-\nment of private clouds [61].\n\nDue to the lack of standardization, the architecture, features and interface\ncompatibility (either with the underlying hypervisors or the manager interface\nfrom other clouds) varies. This way in the following sub-chapters are presented,\ncharacterized and compared four different open source and one proprietary (used\nby LunaCloud) solutions for the deployment of cloud infrastructure platforms: (i)\nOpenNebula [62]; (ii) OpenStack [63]; (iii) CloudStack [64]; (iv) Eucalyptus [65];\nand (v) PACI [66]. The comparison of the referred tools will also by important\nfor the identification of common features that will enable the development of a\nmanagement solution that meets the scope of this thesis.\n\n3.1 OpenNebula\n\nOpenNebula started as a research project by Distributed Systems Architecture\n(DSA) research group [67] and it was released in 2008 as a completely open\nsource software solution. It was originally conceived to manage local virtual\ninfrastructures, but the inclusion of remote interfaces made it also available to\n\n\n\n3.1. OPENNEBULA 37\n\nbuild public and hybrid clouds, turning OpenNebula into one of the most feature-\nrich open source solutions for the deployment of cloud computing IaaS platforms\n[68][25]. Recently, it is developed, maintained and distributed by the Cloud\nComputing (C12G) Labs organization [69] that follows a benevolent dictator\ngovernance model1.\n\n3.1.1 Architecture\n\nOpenNebula is at the moment on the 4.0 release. Its architecture is modular\nallowing integration with different storage, network infrastructure configurations\nand hypervisor technologies. Figure 3.2 presents a diagram with the the com-\nponents organized in three layers: (i) Drivers layer; (ii) Core layer; and (iii)\nTools layer. These components communicate via a set of API that support sys-\ntem and cloud end user interfaces. The API are not presented in Figure 3.2 for\nsimplification purposes (they are illustrated in Figure 3.3).\n\nFigure 3.2: OpenNebula architecture [68].\n\nThe core layer is composed by modules that manage the virtual machines, the\nhosts and the virtual networks as well as a request manager to interact between\nthe user requests and the resource managers via Remote Procedure Call (RPC).\nThe RPC calls are transported over the HyperText Transfer Protocol (HTTP)\nand encoded in XML. Structured Query Language (SQL) databases, e.g., SQLite\n[70] and MySQL [71], are also part of the core components and they are respons-\nible for the storage of all the monitoring information collected from physical\nhosts, VM instances, VM configurations, available disk images and virtual net-\nworks. The OpenNebula core is written in a highly optimized C++ programming\nlanguage to improve the system performance and scalability.\n\nThe lowest layer provides modules containing drivers to communicate with the\nunderlying host OS components (the hypervisors, virtual networks, file systems,\n\n1Focuses on the interest of the project and strategically leads it to ensure that it meets the\nneeds of the users and the community\n\n\n\n38 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nhost information). These drivers, which are used to connect OpenNebula\u2019s core\nto the hosts physical system, are divided in the following tree categories:\n\n\u2022 Transfer drivers - used to manage shared disk images, e.g., Network File\nSystem (NFS) or internet Small Computer System Interface (iSCSI), and\nnon-shared disk images, e.g., simple copies over Secure Shell (SSH), on the\ncurrent storage system;\n\n\u2022 VM drivers - used to interact with the specific supported hypervisors to\nmanage the virtual machine instances on the current hosts;\n\n\u2022 Information drivers - used to retrieve the current status of virtual machine\ninstances and hosts. They are copied and remotely executed via SSH in\nevery physical host, according to the specifications of the host hypervisor.\n\nOn the other hand, the top layer provides tools to remotely interact with the\nend users, channelling the requests to the core management systems. The tools\nthat can be attached to this layer are end-user interface-related. They can be Web\ninterfaces, CLI interfaces or other standard types of interfaces to enable hybrid\nand/or public cloud interactions (e.g., EC2 tools compatibility). OpenNebula\nalso comes with a scheduler (separated from the core management function) to\ndeploy VM on host nodes following specific user requirements and resource-aware\npolicies. An ecosystem catalogue is also provided in the community wiki Web\npage [72], containing tools, extensions and plug-ins that enhance the functionality\nprovided by the standard tool-kit or enable its integration with existing products.\n\n3.1.2 Interfaces\n\nOpenNebula provides a set of interfaces to interact with multiple end-user tools\nand provider infrastructure configurations. Through them, it is possible to man-\nage different node hypervisor technologies at the same time as well as ensure the\ncompatibility and interaction between tools required for the provision of public\nclouds or drivers to interact with other cloud infrastructures. OpenNebula\u2019s in-\nterfaces are divided into two categories: (i) End-user cloud interfaces; and (ii)\nSystem interfaces. Figure 3.3 exposes the provided interfaces and API in Open-\nNebula\u2019s architecture as well as some of the supported tools.\n\n\n\n3.1. OPENNEBULA 39\n\nFigure 3.3: OpenNebula interfaces [73].\n\nCloud interfaces are useful as they provide partners or external users with\naccess to the infrastructure. Providers convert private or hybrid clouds into\npublic clouds through the exposure of REpresentational State Transfer (REST)\nWeb API. OpenNebula provides support for the integration and development\nof RESTful cloud interfaces and includes software modules that implement two\nstandard cloud interfaces:\n\n\u2022 The EC2 Query API - implements the functionalities offered by the Amazon\u2019s\nAWS EC2 API [74] to interact with EC2 Query tools [75] and, thus, perform\nVM management tasks;\n\n\u2022 The OGF OCCI API - is an OGF [76] Open Cloud Computing Interface\n(OCCI) API [77] that addresses the interoperability and extensibility. It\nenables the use of a OCCI server as a Web service to create, control and\nmonitor cloud resources on an OpenNebula platform [78].\n\nSystem interfaces, on the other hand, are utilized to enable the interconnection\nbetween different modules of the OpenNebula\u2019s architecture as well as to provide\ncompatibility capabilities with third party technologies. OpenNebula offers the\nfollowing system interfaces:\n\n\u2022 XML-RPC interface - exposes all the functionalities to interface with the\nOpenNebula deamon. Using the XML-RPC interface it is possible to control\n\n\n\n40 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nand manage any resource (e.g., VM, networks, images, users, hosts and\nclusters). The XML-RPC interface is the primary interface for OpenNebula;\n\n\u2022 OpenNebula Cloud API (OCA) - provides a simplified way to interface\nthe OpenNebula core, exposing the same functionalities as the XML-RPC\ninterface via Java and Ruby programming language bindings. This interface\nallows the integration of developed advanced IaaS tools that need full access\nto the software main functionalities;\n\n\u2022 Drivers interfaces - allow the integration between OpenNebula and the cloud\ninfrastructures. Drivers interfaces are utilized for storage (permitting other\nprograms implementations to interface special storage back end and file sys-\ntems), virtualization (allowing the integration with different hypervisors),\nmonitoring (enabling the inclusion of additional external probes to monitor\ninformation) and authorization (use of external programs to authorize and\nauthenticate user requests). OpenNebula provides drivers for VMware [43],\nKVM [44] and XEN [45] hypervisors;\n\n\u2022 DataBase (DB) interfaces - enables the use of MySQL [71] or SQLite [70]\nas DB tools.\n\n3.1.3 Platform Deployment\n\nOpenNebula\u2019s installation follows a cluster-like architecture with a front-end, a\nset of hosts where the virtual machines will be executed (one host can provide\na cluster of virtual machines), storage (data-stores) and physical networks that\nconnect all components represented in Figure 3.4.\n\nFigure 3.4: OpenNebula system (modified from [68]).\n\nThe front-end server, which can also be utilized to execute virtual machines in\nsmaller configurations, is where the software is installed. It contains the manage-\n\n\n\n3.1. OPENNEBULA 41\n\nment daemon (called ONED), the scheduler, the monitoring and account daemon,\na Web interface server (called Sunstone) and the cloud API servers (EC2-query\nand/or OCCI). These components communicate together through an XML-RPC\ninterface and can also be installed in different machines for security or perform-\nance reasons. The front-end and the host machines also need to have the Ruby\n1.8.7 software installed (or a more recent version). The front-end machine must\nbe able to communicate with all the other hosts and has access to the network\nstorage mounts.\n\nThe hosts are managed directly by the OpenNebula daemons running on the\nfront-end platform, using SSH for communication. There is no need to install any\nparticular OpenNebula package on the hosts. The only software requirements\nare the SSH server, a compatible hypervisor and the Ruby 1.8.7 software. The\nimage repository and storage is used to manage the virtual machine image files\nand has to be accessible through the front end using any suitable technology\nsuch as: (i) Network-Attached Storage (NAS); (ii) Storage Area Network (SAN);\n(iii) Direct Attached Storage (DAS); or (iv) any GNU/Linux distributed-network\nFile System (FS). The Image Repository has to be large enough to store all the\nVM images of the provider infrastructures. When a VM is initiated, it can be\nconfigured to work with cloned copies of master images or directly use an image\navailable in the repository. Shared storage between hosts is optional.\n\nThe OpenNebula front-end daemons require network connection with the\nhosts to manage and monitor the hypervisors and move image files. To offer\nnetwork connectivity to the VM across the different hosts, the default configur-\nation connects the virtual machine network interface to a bridge in the physical\nhost. When a new virtual machine is launched, OpenNebula will connect its net-\nwork interfaces to the bridge or physical device specified in the virtual network\ndefinition, allowing the VM to have access to different networks, public or private.\nTo ensure that an user can only interact with his VM and, thus, restrict the net-\nwork access to the owned Virtual Machines, the following secure mechanisms are\nimplemented:\n\n\u2022 Firewall - Firewall rules are applied, but networking isolation is ignored;\n\n\u2022 802.1Q [79] - restrict network access through VLAN tagging, which also\nrequires support from the hardware switches;\n\n\u2022 Ebtables - restrict network access through Ebtables rules. No special hard-\nware configuration required;\n\n\u2022 Open vSwitch - restrict network access with Open virtual Switch (vSwitch)\n[80];\n\n\n\n42 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\n\u2022 VMware - uses the VMware networking infrastructure to provide an isolated\nand 802.1Q compatible network for virtual machines launched with the\nVMware hypervisor [43].\n\n3.1.4 Authentication and Authorization\n\nOpenNebula comes by default with an internal user/password authentication\nand an Access Control List (ACL) authorization system. The authentication\nand authorization is handled by the OpenNebula Core and can also be delegated\nto an external module. Any interface to OpenNebula (CLI, Sunstone, Ruby or\nJava OCA) communicates with the core using XML-RPC containing the user\u2019s\nsession string (which is authenticated comparing the username and password\nwith the registered users). Each operation generates an authorization request\nthat is checked against the registered ACL rules granting permission or rejecting\nthe request. OpenNebula also includes a user and group management system\ninstallation that can be classified in four types:\n\n\u2022 Administrators - an admin user belongs to the oneadmin group and can\nperform any operation;\n\n\u2022 Regular users - a regular user accesses most OpenNebula functionalities;\n\n\u2022 Public users - a public user accesses only the basic functionalities (and\npublic interfaces) that are open to public users;\n\n\u2022 Service users - a service user account is used by the OpenNebula services\n(e.g., EC2 and OCCI API, Sunstone) to proxy authorization and authen-\ntication requests.\n\nIt is possible to customize other authentication and authorization mechanisms\nsuch as the Rivest r., Shamir a. and Adleman l. (RSA) keypairs over SSH,\nX509 certificates [81], Lightweight Directory Access Protocol (LDAP) or Active\nDirectory (AD).\n\n3.2 OpenStack\n\nOpenStack is a project managed since 2012 by the OpenStack foundation [63].\nIt was initiated in 2010 through the collaboration and the code contribution of\nRackspace cloud files platform [82] and the National Aeronautics and Space Ad-\nministration (NASA) open-source cloud computing project named Nebula [83].\nThe OpenStack project aims \u201cto produce the ubiquitous Open Source Cloud Com-\nputing platform that will meet the needs of public and private clouds regardless\nof size, by being simple to implement and massively scalable.\u201d [84].\n\n\n\n3.2. OPENSTACK 43\n\n3.2.1 Architecture\n\nThe OpenStack architecture is continuously changing between releases. The first\nrelease named Austin combined only an object store and a compute module (with\nbasic management of VM, networks and authentication/authorization proced-\nures) and a simple support for a virtual machine image registry service as well as\na preview Web control panel. Over the following years some functions provided by\nthe legacy components become deprecated and migrated to new project compon-\nents (with new and enhanced features) providing a more modular and balanced\narchitecture as well as a more complete IaaS platform implementation. The cur-\nrent software release of OpenStack is named Grizzly and its architecture provides\nthe following seven interrelated and independently developed components (di-\nvided into computing, networking and storage functions): (i) Compute module\nnamed Nova; (ii) Object storage named Swift; (iii) Block storage named Cinder\n(iv) Image service module named Glance; (v) Network service module named\nNeutron (with the previous name of Quantum); (vi) Identity module named\nKeystone; and (vii) Dashboard named Horizon.\n\nAll of the constituent modules are written in Python programming language\nand a simplified architecture with basic interactions of the referred components\nis presented in Figure 3.5.\n\nFigure 3.5: OpenStack conceptual architecture [85].\n\nThe compute functionalities are implemented through the combination of\nNova and Glance. Nova implements both cloud management and virtual resource\nmanagement functions, being the core of the OpenStack cloud. It manages the\ncommunication with the hypervisor and the VM, the authentication and author-\n\n\n\n44 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nization as well as standard network functionalities for the VM (IP forwarding,\nbridges and VLAN). Nova also provides interaction with external cloud adminis-\ntration tools (e.g., Web Dashboard, CLI) and contains a scheduler which distrib-\nutes the VM through the hosts (defines where a new instance must be created).\nThe communication between Nova components is made via RPC (both request\nand response) over an Advanced Message Queue Protocol (AMQP). The Open-\nStack image service, Glance, provides functionalities for discovering, registering\nand retrieving virtual machine images. Furthermore, it allows users to query the\nVM image metadata stored in a SQL DB (either MySQL [71] or SQLite [70] can\nbe used) and, then, to retrieve the actual image using HTTP requests. The VM\nimages made available through Glance can be stored in a variety of locations.\nThe image service supports the following back end stores:\n\n\u2022 OpenStack Object Storage - OpenStack Object Storage (code-named Swift)\nis the highly-available object storage project within OpenStack;\n\n\u2022 File System - The default back end that Glance uses to store virtual machine\nimages is the FS back end. It writes image files to the local file system;\n\n\u2022 Simple Storage Service (S3) - This back end allows Glance to store virtual\nmachine images in Amazon\u2019s S3 service [86];\n\n\u2022 HTTP - Glance can read virtual machine images that are available via\nHTTP somewhere on the Internet (this store is read only);\n\n\u2022 Rados Block Device (RBD) - This back end stores images inside of a Ceph\nstorage cluster using Ceph\u2019s RBD interface [87];\n\n\u2022 GridFS [88] - This back-end stores images inside of MongoDB [89]\n\nThe network functionalities of OpenStack can be managed with the Neutron\ncomponent instead of the Nova network manager. This module provides flexible\nnetworking models to suit the needs of different applications or user groups. The\noriginal OpenStack Nova network implementation assumed a very basic isolation\nmodel through Linux VLAN and Internet Protocol (IP) tables. OpenStack Net-\nworking introduces the concept of a plugin, which enables the utilization of basic\nLinux VLAN and IP tables, as well as more advanced technologies, e.g., Layer\n2 (L2) in Layer 3 (L3) tunneling or OpenFlow [90]. The current set of avail-\nable plugins include: (i) Open vSwitch [80]; (ii) Cisco products [91] ; (iii) Linux\nBridge; and (iv) Open Networking Foundation (ONF) OpenFlow [90]. Typically,\nthe plugins require access to a database for persistent storage, similar to other\nOpenStack services. OpenStack Networking also includes additional agents that\nmight be required depending on the intended deployment:\n\n\n\n3.2. OPENSTACK 45\n\n\u2022 Plugin agent - runs on each hypervisor to perform the local vSwitch con-\nfiguration. Depending on the plugin used, a different type of agent or no\nagent at all, as some plugins do not require an agent, is launched;\n\n\u2022 Dynamic Host Configuration Protocol (DHCP) agent - Provides DHCP\nservices to tenant networks (is the same across all plugins);\n\n\u2022 L3 agent - Provides L3/ Network Address Translation (NAT) forwarding\nfor external network access to VM on tenant networks (is the same across\nall plugins).\n\nThe storage functions in OpenStack are performed by the object storage com-\nponent Swift and the block storage component Cinter. Swift offers cloud storage\nsoftware based on the Cloud Files of Rackspace. The Swift architecture is highly\ndistributed to prevent any single point of failure as well as to scale horizontally.\nIt includes:\n\n\u2022 A proxy server to accept incoming requests (via the OpenStack Object\nAPI or just raw HTTP), i.e., data upload, metadata update or container\ncreation as well as data download or container listing to Web browsers;\n\n\u2022 Account servers to manage accounts defined with the object storage service;\n\n\u2022 Container servers to manage a mapping of containers (i.e., folders) within\nthe object store service;\n\n\u2022 Object servers to manage actual objects (i.e., files) on the storage nodes.\n\nSwift also ensures consistency and availability through the implementation of\nperiodic processes which run to perform housekeeping tasks on the large data-\nstores like replication services, auditors, updaters and reapers. On the other hand,\nCinter is similar to the Amazon\u2019s Elastic Block Store (EBS) [92]. It provides\npersistent block storage to guest VM and uses a SQL central database that is\nshared by all Cinder services. The block storage is appropriate for performance\nsensitive scenarios such as database storage, expandable file systems or to provide\nservers with access to raw block level storage. It enables snapshot management\nfor backing up data stored on block storage volumes. Snapshots2 can be restored\nor used to create a new block storage volume. In addition to local Linux server\nstorage, it can use other storage platforms.\n\n2Backups of the state of a system at a particular point in time.\n\n\n\n46 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\n3.2.2 Interfaces\n\nOpenStack is, opposed to OpenNebula, an aggregation of different projects rather\nthan a single project and, as a result, the platform interfaces have a more complex\nrepresentation. Figure 3.6 illustrates the complexity of the global architecture\nof OpenStack as well as the interactions between different project components\nthrough their underlying modules. Every component has its own internal ar-\nchitecture that exposes interfaces as well as specific modules to (i) interact with\nother project component interfaces and cloud control tools; (ii) manage messaging\ndistribution; (iii) communicate with the hypervisor nodes; and (iv) enable ag-\ngregation of network plugins. As in OpenNebula, the provided interfaces can be\ngrouped into two categories: end-user cloud interfaces and system interfaces.\n\nFigure 3.6: OpenStack Grizzly global architecture (modified from [93]).\n\nOpenStack provides command line, Web based and API based instance man-\nagement through the Nova component. It has a native RESTful API, a console\ninterface for issuing commands through a Virtual Network Computing (VNC)\nconnection to run instances and a compatible EC2 API. The native API in-\nteracts with the provided dashboard Horizon and using a Web Server Gateway\nInterface (WSGI) framework is also possible to provide an OCCI interface to sup-\nport the OGF standard [94]. On the other hand, the EC2 API provides support\nfor Amazon\u2019s EC2 tools. It is also possible to emulate the Amazon\u2019s S3 API\n\n\n\n3.2. OPENSTACK 47\n\nthrough the installation of a middleware on top of the object storage block Swift\n[95].\n\nThe offered system interfaces implemented in OpenStack enable the intercon-\nnection between the project components, the communication between internal\ncomponent modules and third party technologies. The following list of interfaces\nare supported:\n\n\u2022 OpenStack native API - includes the identity API (Keystone), compute API\n(Nova), image service API (Glance), block storage service API (Cinder) and\nobject storage API (Swift). They interact through REST calls to, respect-\nively, provide authorization tokens to access the compute API, VM launch\nbased on the images uploaded as well as establishment of secure connec-\ntions to server instances through SSH, management (i.e., create, update\nand delete) of virtual image metadata records and the upload/download\nof raw image data as well as the image loading to the compute API, the\nmanagement of volumes and snapshots for the block storage service and the\nmanagement of accounts, containers and objects in the object store system;\n\n\u2022 AMQP Broker - handles the interaction between services. It can be im-\nplemented using RabbitMQ [96] or Qpid [97]. The AMQP broker stands\nbetween any two Nova, Cinder and Neutron internal components and allow\nthem to communicate via RPC over AMQP;\n\n\u2022 Driver Interfaces - allow OpenStack to be compatible with third-party tech-\nnology solutions and different infrastructure setups. Cinder uses by default\na Logical Volume Manage (LVM) on a local volume group, but also provides\ndrivers to add support for other storage devices. A list of all supported\nvolume drivers can be obtained from the OpenStack Block Storage Service\nAdministration Guide [98]. On the other hand, Nova interacts with the\nunderlying hypervisors through drivers using the internal Nova-compute\nmodule. OpenStack is compatible with KVM [44], XEN [45], VMware [43],\nHyper-V [99] and other hypervisors. A complete list can be obtained from\nthe OpenStack Compute Administration Guide [100];\n\n\u2022 Database interfaces - enables the use of MySQL [71] or SQLite [70] as DB\ntools through HTTP with the open source SQL toolkit and the Object-\nRelational Mapper (ORM) SQLAlchemy [101].\n\n3.2.3 Platform Deployment\n\nOpenStack installation depends on the purpose of the intended IaaS platform.\nThe installation of all the referred project components is not required for the\nimplementation of a standard platform. Unlike the OpenNebula deployment, the\n\n\n\n48 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nOpenStack suite installation is divided between the chosen OpenStack project\ncomponents. A straight-forward installation involves a three node set-up (con-\ntroller, network and compute nodes) - see Figure 3.7.\n\nFigure 3.7: OpenStack nodes, networks and components (modified from [102]).\n\nThe cloud controller provides the central management system for multi-node\ndeployment. Typically, the cloud controller manages authentication and sends\nmessages to all systems through a message queue. It includes the image service\nGlance, the block storage service Cinder, the identity service Keystone, the dash-\nboard Horizon, portions of the compute service Nova (API servers, scheduler,\nconductor, console authentication and VNC service) and the API endpoint for\nthe OpenStack network service Neutron. The compute node (or compute nodes\nas there can be more than one server node) has the resources to provide virtual\nmachines. It includes the Nova internal module compute for interaction with\nthe hypervisor, the network service agent (e.g., the Open vSwitch plug-in agent)\nand a compatible hypervisor. The network node provides the bulk of the Open-\nStack services (e.g., DHCP, L2 switching, L3 routing, floating IP and metadata\nconnectivity) as well as the Neutron component project functionalities.\n\nA standard network set-up can have up to four distinct physical data centre\nnetworks, which can also be combined and re-used, e.g., the Management, Data,\nand API networks are commonly in the same network. The management network\nis used for internal communication between OpenStack components. The IP\n\n\n\n3.3. CLOUDSTACK 49\n\naddresses should be reachable only within the platform infrastructure. The data\nnetwork is used for VM data communication within the cloud deployment and\nthe IP addressing requirements depend on the OpenStack networking plug-in in\nuse. In some deployment scenarios, the external network provides VM Internet\naccess and the IP addresses are accessible through the Internet. The API network\nexposes all OpenStack API, including the Networking API to tenants (running\nvirtual machines). The IP addresses on this network are reachable through the\nInternet.\n\n3.2.4 Authentication and Authorization\n\nThe authentication and authorization process is performed by the identity service\nKeystone. Keystone provides the authentication services and maintains user\nattribute information, which is then used by the other OpenStack services to grant\nauthorization. Users have credentials for authentication and can be a member of\none or more groups, e.g., a cloud administrator might be able to list all instances\nin the cloud, whereas a user can only see those in their current group. The\npolicy applied by the service can also be modified by the configuration of a\npolicy.json file. Keystone supports different plug-ins for back-end authentication\ndecisions and storing information. These range from internal storage choices\nto external systems and currently include: (i) In-memory Key-Value Store; (ii)\nSQL database; (iii) Pluggable Authentication Modules (PAM); (iv) LDAP and\n(v) X509 credentials.\n\n3.3 CloudStack\n\nCloudStack is a top level project of Apache Software Foundation (ASF) [103]. The\nproject was initiated by a company named Cloud.com [104] that released the first\nversion of CloudStack in May 2010. Most of the software released was under the\nGNU General Public License version 3 (GPLv3), but a small portion was kept\nproprietary. Meanwhile, Cloud.com was purchased by Citrix [105] that released\nthe remaining CloudStack proprietary software under the GPLv3 and managed\nthe project for almost one year. In April 2012 Citrix re-licensed CloudStack under\nthe Apache Software License version 2.0 (ASLv2), submitted it to the Apache\nIncubator [106] and ceased its involvement in the project.\n\nCloudStack is an open source software platform that pools computing re-\nsources to build public, private and hybrid IaaS clouds and can be used to deploy,\nmanage and configure cloud computing environments.\n\n\n\n50 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\n3.3.1 Architecture\n\nCloudStack is focussed on the automation and management of various data\ncentres. As a result, the available documentation gives more relevance to the\noverall cloud platform architecture and features (e.g., data centre network organ-\nization of pods and clusters) than to a detailed description of individual software\nfunctions (layers). The current Apache CloudStack release is 4.1 (although release\n4.2 in imminent). The software is entirely written in Java and it is organized in\nthe following layers: (i) Interface; (ii) Business logic; (iii) Orchestration engine;\nand (iv) Controllers - see Figure 3.8.\n\nFigure 3.8: CloudStack layers [107].\n\nThe interface layer is composed by the User Interface (UI) application and\nexternal API, allowing to configure, request, provision and enable programmatic\naccess to the IT infrastructure.\n\nThe business layer contains management functions that provide access control\nand authentication, process workflow to enhance the High Availability (HA) of\nthe service and availability and/or applicability of the required resources such as\nnetworks and storage.\n\nThe orchestration engine layer is responsible for the management of the virtual\ninfrastructure. It manages the virtual machines, storage, network and it interacts\nwith the data centre Cloud DB (which stores the configuration information of a\ncluster) to manage and deploy the templates, images and snapshots. It automates\nthe distribution of compute, network and storage resources for the controllers and\nsupplies the business layer with the necessary information for the definition of\n\n\n\n3.3. CLOUDSTACK 51\n\npolicies on load balancing, data security and compliance. This layer also has an\nasynchronous job manager to manage the scheduling and prioritising of requested\ntasks.\n\nThe controllers enable the communication with the underlying hypervisors\nas well as network and storage resources. CloudStack uses by default a Virtual\nRouter as network service provider. This Virtual Router implements the following\nfeatures: (i) Remote access through VPN; (ii) Firewall protection; (iii) Source\nand static NAT; (iv) Load balancing; (v) Port forwarding; and (vi) DHCP and\nDomain Name System (DNS). The storage resources are divided into two types:\n\n\u2022 Primary storage - is shared among all the hosts of a cluster and is used to\nhost the guest VM instances. It supports SAN (e.g., iSCSI), NAS (e.g.,\nNFS) or DAS, e.g., the linux EXTended file system (EXT);\n\n\u2022 Secondary storage - is used to store templates, ISO images and snapshots\nto be deployed in the IT infrastructure. Every data centre (zone) has at\nleast one secondary storage server that is shared by all the pods (containing\nthe clusters). Unlike primary storage, it only uses NFS like the OpenStack\nObject storage Swift, NFS storage or Linux NFS servers.\n\n3.3.2 Interfaces\n\nCloudStack provides three different API interfaces: (i) Platform API; (ii) Agent\nAPI; and (iii) Plug-in API. The platform API is a native REST end-user cloud\ninterface through which administrators and users control the infrastructure plat-\nform. This API is divided into an end-user API, an Operation, Administration,\nMaintenance &amp; Provision (OAM&amp;P) API, an AWS API, compatible with EC2,\nand a pluggable service API engine to enable direct management access to third\nparty plug-ins.\n\nThe remaining Agent and Plug-in API are system interfaces. The Agent API\nis JavaScript Object Notation (JSON) based and is used to provide interaction\nbetween the CloudStack components and the resource server, where the hyper-\nvisor and the hardware resources are located. In the current CloudStack version,\nthe supported hypervisors are VMware [43], KVM [44], Citrix XenServer [108]\nand Citrix Cloud Platform [109]. The plug-in API allows the insertion of code\ndirectly into CloudStack deployments to add or modify the default behaviour of\nCloudStack. This Java-implemented API defines adapters which expose the func-\ntionalities required by CloudStack to implement cloud operations [110]. Through\nthis API it is possible to aggregate third party technologies, e.g., Citrix NetScaler\n[111]. Figure 3.9 illustrates the integration of the software components with the\navailable plug-ins as well as the platform API.\n\n\n\n52 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nFigure 3.9: CloudStack interfaces (modified from [107]).\n\n3.3.3 Platform Deployment\n\nA CloudStack installation consists of two parts: (i) the Management Server;\nand (ii) the managed cloud infrastructure. This infrastructure is hierarchically\norganized. At the top there is the zone, which is equivalent to a single data\ncentre. It consists of one or more pods isolated by a L3 router and secondary\nstorage. A pod is usually one rack of hardware that includes a L2 switch and\none or more hardware servers that provide the VM clusters. The clusters consist\nof one or more hosts with the same hypervisor and the primary storage. On the\nother hand, the host is a single compute node within a cluster. The hosts are\nwhere the actual cloud services run in the form of guest virtual machines.\n\nThe Management Server is where the CloudStack software is installed and\nit runs on a dedicated server or VM. It controls allocation of virtual machines\nto hosts and assigns storage and IP addresses to the virtual machine instances.\nThe Management Server runs in a Tomcat [112] container and requires a MySQL\ndatabase [71] for persistence.\n\nThe minimum production installation consists of one machine running the\nCloudStack Management Server and another machine to act as the cloud infra-\nstructure (in this case, a very simple infrastructure consisting of one host running\nhypervisor software). A more complex and extended installation consists of a\n\n\n\n3.3. CLOUDSTACK 53\n\nhighly-available multi-node Management Server installation and multiple hosts\nusing any of several advanced networking set-ups. Figure 3.10 illustrates a basic\nCloudStack deployment architecture.\n\nFigure 3.10: CloudStack deployment architecture [113].\n\nCloudStack offers two types of networking scenarios:\n\n\u2022 Basic network for AWS type networking - Provides a single network where\nguest isolation can be achieved through L3 means such as security groups\n(IP address source filtering);\n\n\u2022 Advanced network for more sophisticated network topologies - Provides the\nmost flexibility in defining guest networks.\n\nEach physical network can carry one or more types of network traffic. The\nbasic zone traffic types are:\n\n\u2022 Guest - generated when end users run a VM. The guest VM instances\ncommunicate with each other over a network that can be referred to as\nthe guest network. Each pod in a basic zone is a broadcast domain and,\ntherefore, each pod has a different IP range for the guest network;\n\n\n\n54 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\n\u2022 Management - generated when internal resources communicate with each\nother. This includes communication between hosts, system VM (VM used\nby CloudStack to perform various tasks in the cloud) and any other compon-\nent that communicates directly with the CloudStack Management Server;\n\n\u2022 Public - generated when a cloud VM accesses the Internet. Public IP must\nbe allocated for this purpose;\n\n\u2022 Storage - generated when data is replicated in the secondary storage. It\nincludes traffic such as VM templates and snapshots, which is sent between\nthe secondary storage VM and secondary storage servers. CloudStack uses a\nseparate Network Interface Controller (NIC) named storage NIC for storage\nnetwork traffic. The use of this NIC allows fast template and snapshot\ncopying.\n\n3.3.4 Authentication and Authorization\n\nAuthentication and authorization is performed by the access control component of\nthe management server. It cross-checks the authorization of the users requesting\nthe action. The user must be authenticated and then mapped to the domain,\nproject or other groups to which he/she belongs. The requested action has an\nauthentication token that authorizes the user to perform the action and specifies\nthe permissions. The actions performed are recorded into logs. In addition to\nthe credentials authentication, CloudStack supports the usage of SSH keys to log\nin to the cloud infrastructure for additional security as well as of LDAP servers.\n\n3.4 Eucalyptus\n\nEucalyptus is an open-source software managed by Eucalyptus Systems [65] (fol-\nlowing the same governance model approach as OpenNebula) to build private\nand hybrid IaaS cloud computing platforms compatible with the Amazon Web\nServices cloud solutions. Eucalyptus started as a research project in the Com-\nputer Science Department at the University of California, Santa Barbara, in the\nfall of 2007. It was initially conceived as a local infrastructure solution to test\nthe integration of the Virtual Grid application Development Software (VGrDS)\nproject, from the National Science Foundation (NSF), with the AWS, which was\nthe most appealing public cloud choice to execute large-scale scientific workflows\ngenerated by the NFS\u2019s Linked Environment for Atmosphere Discovery (LEAD)\nweather forecasting system. The first public release of Eucalyptus occurred in\nMay 2008 [114].\n\n\n\n3.4. EUCALYPTUS 55\n\n3.4.1 Architecture\n\nEucalyptus possesses a modular architecture composed by software components\norganized in three layers: (i) Cloud layer; (ii) Cluster layer; and (iii) Node layer.\nThis disposition enables the IT infrastructure fast expansion and consequentially\nthe high availability of the platform resources. Eucalyptus current release 3.3 is\ncomprised of five software components : (i) Cloud controller; (ii) Walrus; (iii)\nCluster controller; (iv) Storage controller; and (v) Node controller. Additionally,\na sixth optional component, VMware broker, is also available but only through\na subscription service. Figure 3.11 illustrates the software organization of Euca-\nlyptus.\n\nFigure 3.11: Eucalyptus software modules organization [115].\n\nThe cloud layer is the platform front end and is formed by the Cloud Control-\nler and the Walrus components. Additionally, software modules associated with\nadministration services can also be included in this layer, e.g., the AWS compat-\nible Identity and Access Management (IAM) API for the enterprise cloud. The\nCloud Controller is the entry point into the cloud for administrators, developers,\nproject managers and end users. It is a Java program that queries other software\ncomponents for information about resources and makes high-level scheduling de-\ncisions and requests to the Cluster Controllers. The Cloud Controller is also\nresponsible for exposing and managing the underlying virtualized resources and\nto handle authentication, accounting and quota management. Walrus, also writ-\nten in Java, is the Eucalyptus equivalent of AWS S3 [86] and OpenStack Swift\nobject storage services. It allows users to store persistent data, organized as\nbuckets, and objects and it is used to create, delete, and list buckets, put/get\nand delete objects or to set access control policies in the cloud. There are no\ndata type restrictions for Walrus and it can contain images, volume snapshots\n\n\n\n56 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nand application data.\n\nThe cluster layer is the Eucalyptus middle end and is composed by the Cluster\nController, Storage Controller and, optionally, the VMware Broker [116]. The\nCluster Controller is written in C and acts as the front end for a cluster within a\nEucalyptus cloud. It gathers information about Node Controllers and schedules\nvirtual machine executions to them. The Cluster Controller also interacts with\nthe Linux host OS and the virtual machine networks, managing the iptables\nrules (to implement firewalls and address translations), the Linux routing table\nentries, the DHCP server and the Linux network interface state. The Node\ncontrollers associated with a Cluster Controller are in the same sub-network.\nThe Storage Controller provides functionalities similar to the Amazon EBS [92]\nand OpenStack Cinder and is able to interface with various storage systems,\ne.g., NFS, iSCSI or SAN devices. It is written in Java and communicates with\nthe Cluster Controller, Walrus and Node Controller to manage the block volumes\nand to request snapshots of the instances within its specific cluster. The VMware\nBroker enables Eucalyptus to deploy virtual machines on VMware infrastructure\nelements, mediating all the interactions between Cluster Controller and VMware\nhypervisors (ESX/ESXi) [116].\n\nThe node layer is formed by the Node Controllers and is the back end of a\nEucalyptus platform. The Node Controllers are written in C. They control virtual\nmachine activities (e.g., execution, inspection and termination of VM instances),\nthe host OS, the hypervisors and are also responsible for the management of the\nvirtual network endpoints.\n\n3.4.2 Interfaces\n\nEucalyptus is an IaaS platform created to test software integration with the\nAmazon public cloud. Thus, as an AWS compatible platform, Eucalyptus offers\nboth a variety of user interface tools as well as the option to use third party\nAWS compatible interfaces (e.g., EC2, S3 and EBS API) that interoperate with\nAWS and Eucalyptus. As Figure 3.12 illustrates, the Eucalyptus cloud interfaces\nare implemented at the cloud level by the Cloud Controller and Walrus software\ncomponents. The first (Cloud Controller) exposes a Web interface and a HTTP\nSimple Object Access Protocol (SOAP) and Query API on top of the Amazon\nEC2 compatible layer. The Web interface enables account management (e.g.,\nuser sign-up and account configuration) and supports basic queries (e.g., listing\nof images and instances), whereas the SOAP and Query API allow full control\nof executions, network and storage through command line tools (e.g., Euca2ools\n[117]) or programming libraries compatible with the AWS EC2 API. The second\n(Walrus) exposes a SOAP and a REST API on top of the Amazon S3 compatible\nlayer. Using this API, Walrus can store data from all the VM running in the\n\n\n\n3.4. EUCALYPTUS 57\n\nEucalyptus cloud and also provide a simple HTTP put/get Storage as a Service\nsolution for users outside the Eucalyptus cloud.\n\nFigure 3.12: Eucalyptus software component interfaces (modified from [118]).\n\nInternally, Eucalyptus uses an EBS that provides a logical communication\nchannel between Web service containers that run in the Web services layer of\nevery software component. These containers take care of messaging integrity,\nsecurity and dispatch while the Web service layer is responsible for the container\nlife-cycle. This system abstracts communication and location functions, enabling\nall services to use a common framework that handles secure message routing\nand delivery regardless of physical location. The Eucalyptus software compon-\nents uses SOAP and REST API as system interfaces to communicate with other\nsoftware components running in different physical machines implementing the\nfollowing steps:\n\n\n\n58 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\n1. The sending service component puts the message on the ESB;\n\n2. The ESB calls the Web service stack to implement the communication;\n\n3. The Web service stack connects to the network and sends the message to\nthe Web server of the desired Eucalyptus service component;\n\n4. The receiving Web server hands the message to the receiving Web service\nstack to perform the message validation;\n\n5. The receiving Web service stack puts the message on the ESB;\n\n6. The receiving service component gets the message from the ESB.\n\nWhen the services are located on the same machine they communicate ex-\nactly in the same way (they do not need to be specialized for local and remote\ncommunication) as the implementation of the ESB short-cuts the message path\nto bypass the Web service stack and to deliver the message directly when two\nservices are co-located and they are both written in Java (as they run in the same\nJava virtual machine). If the services are written in different languages, e.g., C\nand Java, then they must use the full procedure and communicate through local\nLinux networking via sockets. On the other hand, the communication with the\nhypervisors is done using drivers that provide support to XEN [45] and KVM [44]\n(support to VMware hypervisors [43] is also available through the implementation\nof the optional software component VMware Broker [116]).\n\n3.4.3 Platform Deployment\n\nLike the previous software solutions, Eucalyptus installation varies with the pur-\npose and complexity of the desired IaaS platform (deployment simplicity, per-\nformance or high availability). All Eucalyptus components need to be installed\non 64-bit architecture physical machines (not virtual machines) with synchronized\nclocks. It is possible to install the Cloud Controller, Walrus, Cluster Controller\nand Storage Controller on one machine and a Node Controller on one or more\nmachines. Alternatively, one can install each component on an independent phys-\nical server which gives each component maximal local resource usage. Placing all\ncloud and cluster components on a single machine simplifies the administration\nas there is only one machine to monitor and control. However, each component\ndeploys as an independent Web service. If these components share a single phys-\nical server, the physical resources that can be given to each service may become\na performance bottleneck. On the other hand, separating service components in\ndifferent physical machines that share the same internal network can decrease\nmessaging efficiency (e.g., software services written in Java that share the same\n\n\n\n3.4. EUCALYPTUS 59\n\nJava environment) but increase the responsiveness of the overall Eucalyptus sys-\ntem due to the availability of physical resources (e.g., large memory footprint).\nFigure 3.13 represents an Eucalyptus deployment structure where all standard\nsoftware components run in different physical machines.\n\nFigure 3.13: Eucalyptus generalized deployment structure [118].\n\nThe Cloud Controller must have Transmission Control Protocol (TCP)/IP\nconnectivity to all other Eucalyptus components except for Node Controllers,\nwhich may reside on their own private networks. In addition, Walrus needs to be\nable to route network traffic directly to the Node Controllers for image delivery.\nThe Cluster Controller physical machine can act as a software IP gateway between\nVM instances and the public network when elastic IP and security groups are im-\nplemented. This means that the physical server on which the Cluster Controller\nis deployed should have fast, dedicated network access to both the Node Con-\ntroller network and the public network (where the Cloud Controller, Walrus and\nStorage Controller are located). The machine on which the Storage Controller\nis deployed must always have TCP/IP connectivity to the Cloud Controller and\nto the chosen SAN device (SAN integration drivers are only provided through\nsubscription service). When SAN is not implemented the Storage Controller re-\nquires only TCP/IP connectivity to the Node Controllers in the cluster to provide\nnetwork access to the dynamic block volumes residing on the Storage Controller\u2019s\nstorage. The optional VMware Broker runs in the same machine as the Cluster\nController (when the infrastructure platform has more than one cluster it runs on\nthe Cluster controller of the cluster that will use the VMware components (vCen-\nter Server or ESX/ESXi). All Node Controllers must have network connectivity\nto either a SAN or the Storage Controller and Walrus.\n\n3.4.4 Authentication and Authorization\n\nEucalyptus manages access control through an authentication, authorization, and\naccounting system. This system manages user identities, enforces access controls\nover resources and provides reporting on resource usage as a basis for auditing\n\n\n\n60 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nand managing cloud activities. The user identity organizational model and the\nscheme of authorizations used to access resources are based on and compatible\nwith the AWS IAM system, with some Eucalyptus extensions provided for a\nprivate cloud environment. Authentication can also be performed by integrat-\ning Eucalyptus with an existing LDAP or Active Directory. In this case, the\nuser, group and account information and the Eucalyptus administrator console\nlogin authenticate using the LDAP/AD service. Eucalyptus-specific information\nabout user, group and account is stored within the local database of Eucalyptus,\nincluding certificates, secret keys and attached policies. Each user has a unique\nset of credentials. These credentials are used to authenticate access to resources.\nThere are three types of credentials:\n\n\u2022 An X.509 certificate, used to authenticate requests to the SOAP API ser-\nvice;\n\n\u2022 A secret access key, used to authenticate requests to the REST API service;\n\n\u2022 A login password, used to authenticate the Eucalyptus administrator con-\nsole access.\n\nEucalyptus also has two special identities for the convenience of administra-\ntion and use of the system:\n\n\u2022 The Eucalyptus account - each user has unrestricted access to all of the\ncloud\u2019s resources, similar to the super user on a typical Linux system. This\naccount is automatically created when the system starts for the first time\nand cannot be removed from the system;\n\n\u2022 The admin user of an account - each account, including the Eucalyptus\naccount, has a user named admin. This user is created automatically by\nthe system when an account is created. The admin of an account has full\naccess to the resources owned by the account. It can not be removed from\nan account and it can delegate resource access to other users in the account\nby using policies.\n\n3.5 Parallels Automation for Cloud Infrastructure\n\nParallels is a hosting and cloud services company that released in November 2011\nthe Parallels Automation for Cloud Infrastructure (PACI) [119]. PACI allows\nthe deployment of cloud services on top of the Parallels Automation 5.3 software.\nWhile the Parallels Automation is a global software solution to implement cloud\nservices, PACI is only a service module of Parallels Automation that is focused\n\n\n\n3.5. PARALLELS AUTOMATION FOR CLOUD INFRASTRUCTURE 61\n\non the provision of IaaS to SME. As opposed to the previous open source IaaS\nplatform solutions, PACI (as well as all Parallels software services) is propri-\netary. PACI, is currently on its third release (as part of Parallels Automation\n5.5), offers virtualization, automated operations and billing, customer self-service\ndashboards and an on line store to service providers.\n\n3.5.1 Architecture\n\nPACI has a distributed and modular architecture that is composed by four core\nservices (Customer Self-service, Provision and Orchestration, Billing and Infra-\nstructure Management layers) and two interfaces that enable integration with\nother systems and services as represented on Figure 3.14 in red and in orange\nrespectively. Through the services interface are attached the business Storefront\nand Marketplace where a Web store or a provided business infrastructure exposes\nto customers the service provider offerings.\n\nFigure 3.14: PACI architecture [120].\n\nThe Customer Self-Service layer implements the usual IaaS dashboards or\ncontrol panels for the administrators, re-sellers and end-users. Through them it\nis possible to execute tasks over the provided services (add, modify or remove\nservices), manage accounts and users, run reports and obtain support. Pro-\nvisioning and Orchestration as well as Billing Automation stack on top of the\nInfrastructure Management layer and inter-operate in order to enable providers\nto manage and commercialize raw infrastructure resources. Provisioning and Or-\nchestration services are responsible for the resources provision and management,\nservice monitoring and management and to report to the Billing Automation\nBlock. The Billing Automation service processes the payment and taxation, rat-\ning, billing and interact with the Storefront and Marketplace services managing\nthe product catalogue and to deal with new orders, marketing and promotions.\n\n\n\n62 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nFinally, the Infrastructure Management layer is responsible for the coordination\nof tasks related to the management of the physical and virtual infrastructure re-\nsources. It manages the networks, virtual environment, the storage and servers\nas well as implement security procedures, motorisation and the provision of the\nservers.\n\nThe presented services that are illustrated in Figure 3.14 are implemented\nthrough a combination of four main technologies: (i) PACI modules, e.g., in-\nstances management and instances management DB; (ii) Parallels Operations\nAutomation (POA), which is the Operations Support System (OSS) responsible\nfor the centralized service management, service provisioning and the global in-\nfrastructure management; (iii) Parallels Business Automation (PBA), which is\nthe Business Support System (BSS) that performs the resource usage accounting\nand billing; and (iv) Parallels Server Bare Metal (PSBM), which provides hybrid\nserver virtualization, allowing hardware virtualization and OS virtualization to\nrun side-by-side on the same physical server.\n\n3.5.2 Interfaces\n\nAs a cloud interface PACI provides a RESTful API that enables the programmatic\naccess to the PACI server through HyperText Transfer Protocol Secure (HTTPS)\n, as illustrated on Figure 3.15. This API allows getting Information about and\nperforming actions on the resources through external cloud management pro-\ngrams. On the other hand, the OSS POA located in the Parallels Automation\nlayer provides compatibility with an extensive list of third-party products [121]\nand external billing integration systems through a public API.\n\nFigure 3.15: PACI RESTful interface [119].\n\n\n\n3.5. PARALLELS AUTOMATION FOR CLOUD INFRASTRUCTURE 63\n\n3.5.3 Platform Deployment\n\nPACI is deployed by combining the POA and PBA services with a cloud in-\nfrastructure module. POA services include a management node which runs the\nPOA management software and system DB, a cluster of physical or virtual UI and\nbranding servers that provide both high availability and load balancing functions\nregarding the access and interaction with the system\u2019s control panel, a private\nproxy responsible for transferring XML-RPC or HTTPS requests from the pro-\ntected internal network to the front network and, finally, DNS servers which can\nalso be implemented externally but without direct management from the POA\nmanagement node. The PBA services are composed by an application server con-\ntaining the PBA core software and UI, a DB server and a on line store server. On\nthe other hand, the cloud infrastructure model consists of an Instance Manager\n(IM) node which acts as the control centre for all operations related to the Cloud\nInfrastructure module, an IM database node to store all information related to\nPACI, at least two PSBM nodes to deploy virtual machines or virtual containers,\ntwo storage nodes for each PSBM node to store backups and images, a Microsoft\nProvisioning System (MPS) node and an Active Directory domains controller\nto run the AD service. Figure 3.16 illustrates the deployment of a PACI IaaS\nplatform.\n\nFigure 3.16: PACI deployment structure [122].\n\nThe deployed services nodes are organized as front end and back end services.\n\n\n\n64 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nThe front end nodes are exposed to the front and back networks in order to expose\nthe infrastructure services to the Internet while being managed by the respective\ncore service nodes from a secure network. These nodes are typically the ones\nthat interact with the end users, e.g., nodes containing the virtual machines or\nvirtual containers, servers containing the control panels from POA and PBA,\nDNS servers and third-party services like Watchdog. The infrastructure can also\nimplement system backups through a third network (Backup net), contributing\nfor the high availability of the system and the provided services.\n\n3.5.4 Authentication and Authorization\n\nPACI utilizes an user authentication and authorization strategy based on groups,\npermissions, roles and authentication databases. Users are objects characterized\nby the roles delegated to them in a certain scope. Users can be members of groups.\nUsers and groups can be retrieved either from local databases or from databases\non external computers in the network. The information on those databases is\nstored on the physical server in the form of authentication databases. Roles are\nsets of abstract privileges that can be assigned to a user or a group to form a\npermission. Permissions enable users or groups to perform certain operations in\ndifferent scopes, e.g., virtual environments, physical servers, logical units.\n\n3.6 Conclusion\n\nThis chapter summarises the main features of the five IaaS platforms studied.\nTable 3.1 presents a global comparison regarding authentication, hypervisors,\nmanagement, interfaces, network, storage and governance.\n\nTable 3.1: IaaS platforms comparison.\n\nIaaS Frameworks\nFeatures OpenNebula OpenStack CloudStack Eucalyptus PACI\n\nAuthorization/\nAuthentication\n\nPassword,\nSSH RSA\nkeypair,\nX509, LDAP\n\nIn-memory\nKey-Value Store,\nPAM, LDAP,\nX509\n\nPassword, LDAP,\nSSH RSA keypair\n\nPassword,\naccess Key,\nX509, LDAP\n\nPassword,\nLDAP\n\nHypervisors XEN, KVM,VMware vSphere\n\nKVM, LXC, UML,\nVMWare vSphere,\nXen, PowerVM,\nHyper-V\n\nVMware vSphere,\nKVM, Citrix Xen\n\nXEN, KVM,\nVMware vSphere\n\nParallels\nhypervisor,\nKVM\n\nManagement Centralized Scattered Centralized Centralized Centralized\n\nInterfaces\n\nNative:\nXML-RPC API;\nAdditional:\nAWS EC2,\nOCCI, OCA\n\nNative:\nRESTful API;\nAdditional:\nAWS EC2, S3,\nEBS and OCCI\n\nNative:\nQuery API;\nAdditional:\nAWS EC2,\nPlug-in API\n\nAWS API RESTfulAPI\n\nNetwork Virtual router,Contextualization\nNova-network,\nNewtron Virtual router\n\nCluster\nController POA\n\nStorage Volume Storage\nVolume and\nObject storage\n(Glance, Swift,\nCinder)\n\nVolume Storage\nVolume and\nObject storage\n(Walrus, Storage\nController)\n\nSystem DB\n\nGovernance\nModel\n\nBenevolent\nDictator Foundation\n\nTechnical\nMeritocracy\n\nBenevolent\nDictator Proprietary\n\n\n\n3.6. CONCLUSION 65\n\nThe IaaS platforms studied contemplate both open source and proprietary\nsoftware frameworks with different architectures and cloud deployment focuses.\nThis diversity is caused by the absence of well defined architectural standards for\nthe commoditization of IaaS systems (converting functionalities with economic\nvalue, distinguishable in terms of attributes,uniqueness or brand, to simple com-\nmodities in the eyes of the market or consumers). Every IaaS platform tends\nto provide distinct functionalities and be compatible with specific third-party\nservices in order to monopolize the market and impose the its technologies as\nstandards.\n\nOpenNebula is the only non-US IaaS platform studied. This platform is\nready to manage virtual resources from public and hybrid clouds. It presents a\nlayered architecture, that enables a centralized management of the data-centre,\nand provides a well structured documentation. At the top of the stack, it ex-\nposes multiple API that enable the communication with AWS and OCCI-based\nsolutions.\n\nOpenStack is a perfect example of an IaaS platform that tries to monopol-\nize the market. This platform is highly dynamic, presenting several new func-\ntionalities with each software release. However, it is fragmented into multiple\nsoftware modules (OpenStack projects) with dedicated interface libraries. This\nis the consequence for being a foundation that serves essentially the needs of\nthe vendors (the enterprises that form the OpenStack Foundation). This frag-\nmentation hardens the installation process, the management of the platform and\nincreases the complexity of the system. On the other hand, it interacts with sev-\neral third-party applications, uses RESTful interfaces and offers OCCI and AWS\ninterface libraries.\n\nApache CloudStack uses a modular architecture. This platform is focussed on\nthe automation and management of various data centres in a centralized manner,\nbeing organized by zones, pods and clusters. It uses a Query API as well as an\nAPI translator so that applications written for CloudStack can also run in AWS.\nHowever, CloudStack does not provide OCCI support.\n\nEucalyptus is the only open source IaaS platform that is not licensed under\nApache 2.0 (GPL v3.0). Its main purpose is to enable the deployment of private\nand hybrid open-source clouds that behave and are compatible with the Amazon\nWeb Services. It presents a modular architecture exposing interfaces exclusively\nfor AWS, which excludes the communication with other IaaS platforms and Open\nCloud Computing Interfaces like OCCI. Due to the nature of this IaaS platform\n(deployment of private clouds compatible with AWS only) and the absence of ded-\nicated user application programming interfaces, it will be discarded from future\nstudies.\n\n\n\n66 CHAPTER 3. CLOUD INFRASTRUCTURE PLATFORMS\n\nThe Parallels Automation for Cloud Infrastructure (PACI) includes various\nproprietary (Parallels) products (e.g., POA, PBA, PSBM) that are combined to\nenable the creation, management, monitoring and billing of a public or hybrid\n(if the same PACI platform is used) cloud IaaS platform. This platform exposes\nan open interface (RESTful API) to enable the development of third-party ap-\nplications for the interaction with the system. However, PACI is essentially a\n\u201cclosed\u201d platform without software modules that support directly the interaction\nwith other IaaS platforms. This approach is common among proprietary solu-\ntions where the user lock-in phenomenon is purposely originated to monetize new\nproducts and paid support services.\n\nThe following chapter analyses in detail the corresponding client interface\nAPI of the OpenNebula, OpenStack, CloudStack and PACI, in order to identify\nboth their common and distinctive interface features.\n\n\n\nChapter 4\n\nInterface Libraries Comparison\n\nThis chapter compares the client interface libraries provided by OpenNebula,\nOpenStack, CloudStack and PACI IaaS platforms in terms of programmable com-\nponents, available operations, including the parameters and respective attributes.\n\n4.1 Interface Types\n\nThis section describes the RESTful, REST-like (Query) and RPC interface lib-\nraries used natively by the OpenStack, PACI, CloudStack and OpenNebula IaaS\nplatforms respectively.\n\n4.1.1 RESTful API\n\nA RESTful API is a Web API conforming to the REST architectural style intro-\nduced in 2000 by Roy Fielding in his academic dissertation entitled \u201cArchitectural\nStyles and the Design of Network-based Software Architectures\u201d [123]. These\ninterface libraries expose a set of data and functions to facilitate interactions\nbetween computer programs and allow them to exchange information following\nfour basic design principles:\n\n\u2022 Expose a directory structure-like Uniform Resource Identifier (URI);\n\n\u2022 Are stateless;\n\n\u2022 Use HTTP access methods explicitly;\n\n\u2022 Transfer resources through different media-types representations (usually\nJSON or XML).\n\n67\n\n\n\n68 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nRESTful API use URI to address resources (sources of specific information).\nThe generic URI syntax is defined by the Request For Comments (RFC) 3986\n[124] and it consists of a hierarchical sequence of components referred to as the\nscheme, authority, path, query and fragment:\nURI = scheme \" :// \" authority \" / \" path [ \" ? \" query ] [ \"#\" fragment ]\n\nThe most common schema are HTTP and HTTPS. The authority corresponds\nto the IP address or host name and port of the server containing the Web API,\nthe path represents the path to the RESTful API resource within the server.\nThe query component starts after the question mark (\u201c?\u201d) character and is\nterminated by a number sign (\u201c#\u201d) character or by the end of the URI. It can\nbe used to send additional (optional) parameters, e.g., add a filter to retrieve\na particular information. The fragment identifier component of a URI allows\nindirect identification of a secondary resource by reference to a primary resource\nand additional identifying information.\n\nThe stateless constraint dictates that a Web server is not required to memorize\nthe state of the client application. As a result, each client must include the\ncontextual information that it considers relevant in each interaction with the Web\nserver. Web servers expect clients to manage the complexity of communicating\ntheir application state so that the Web server can service a larger number of\nclients [125].\n\nREST API libraries embrace all aspects of the HyperText Transfer Protocol,\nversion 1.1 (HTTP/1.1) [126], including request methods, response codes and\nmessage headers. Each HTTP method has specific, well-defined semantics within\nthe context of a REST API library resource model:\n\n\u2022 GET is used to retrieve data from a resource;\n\n\u2022 HEAD is used to retrieve the metadata associated with a resource;\n\n\u2022 PUT is used to add or update a resource;\n\n\u2022 DELETE removes a resource from its parent;\n\n\u2022 POST is used to submit data to a resource.\n\nThe result of a request is obtained by the Status-Line part of an HTTP\nresponse message. HTTP defines forty standard status codes divided into the\nfollowing five categories:\n\n\u2022 1xx - Communicates transfer protocol-level information;\n\n\u2022 2xx - Indicates that the client\u2019s request was accepted successfully;\n\n\n\n4.1. INTERFACE TYPES 69\n\n\u2022 3xx - Indicates that the client must take some additional action in order to\ncomplete their request;\n\n\u2022 4xx - Client Error;\n\n\u2022 5xx - Server Error.\n\nREST API libraries often employ a text-based format to represent a resource\nstate as a set of meaningful fields. This resource representation typically reflects\nthe current state of the resource and its attributes at the time of the client\napplication request. The last set of constraints of a RESTful Web service design\nhas to do with the exchanged data format (both in the request/response payload\nor in the HTTP body). The most commonly data exchange formats are XML\nand JSON.\n\n4.1.2 XML-RPC API\n\nThe XML-RPC library uses the HTTP as a transport protocol to execute remote\nprocedure calls encoded in XML in a remote machine [127]. An RPC is initiated\nby the client, which sends a request message (the calling process) to a known\nremote server (the called process) to execute a specified procedure with supplied\nparameters. A server API is made available at a particular Uniform Resource\nLocator (URL), e.g., http://server.org:8080/rpcapi/.\n\nTo consume the server-side available procedures, the following steps are ne-\ncessary:\n\n\u2022 The client program makes a procedure call using the XML-RPC client,\nspecifying a method name, parameters and a target server;\n\n\u2022 The XML-RPC client takes the method name and parameters and then\npackages them as XML (this procedure is known by marshalling). Then the\nclient issues an HTTP POST request, containing the request information\nto the target server;\n\n\u2022 The target HTTP server receives the POST request and passes the XML\ncontent to an XML-RPC listener;\n\n\u2022 The XML-RPC listener parses the XML (also known as un-marshalling) to\nget the method name and parameters and, then, invokes the appropriate\nmethod, passing the received parameters;\n\n\u2022 The method returns a response to the XML-RPC process, which, in turn,\npackages the response as XML;\n\n\u2022 The Web server returns this XML response to the client;\n\nhttp://server.org:8080/rpcapi/\n\n\n70 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The XML-RPC client parses the XML response, extracts and sends the\nreturn value to the client program.\n\n4.1.3 Query API\n\nQuery interfaces use standard components of the HTTP protocol to invoke the\nlibrary actions. However, unlike the RESTful interfaces, they do not use the\nHTTP message components to fully describe API operations. The HTTP en-\nvelope serves merely as a way of delivering parameters (simple name and value\npairs), specifying both the operation and the data. With Query interfaces the\noperations are performed by rewriting the parameters in a GET request URI or\nin the body of a POST request.\n\nAlthough Query interfaces differ from RESTful interfaces, they only use\nstandard HTTP message components to perform operations. This type of in-\nterfaces are also known as REST-like interfaces.\n\n4.2 OpenNebula Interface\n\nOpenNebula uses a XML-RPC API to expose a set of operations, with a name\nand a set of input values, for end-users to interact with the OpenNebula system.\nThe returned response uses always three pre-defined output values. The first\nand the third output values are fixed and correspond, respectively, to the status\nof the invoked operation and to an error code. The second value is used to\nindicate failure or to return information about the performed operation. The\ninformation returned by one..info operations is XML-formatted. The version 4.2\nof OpenNebula interface libraries provides a large list of components that can be\nconsulted at [128].\n\nThe access to the available operation provided by each component uses a\nsession string generated by the user authentication. The study of the OpenNeb-\nula API will be focussed on the components that are common with the other\nIaaS platforms under analysis and covers: (i) Virtual Machine Management; (ii)\nTemplate Management; (iii) Image Management; (iv) Virtual Network Manage-\nment; and (v) Data-store Management. These components and their respective\noperations will be described in the next sections.\n\n4.2.1 Server Management\n\nThe Server (virtual machine) Management component is used to perform man-\nagement operation to specific virtual machines in OpenNebula. The provided\noperations are: (i) Allocate VM; (ii) VM Actions; (iii) Save Disk; (iv) Attach\nDisk; (v) Detach Disk; (vi) Attach NIC; (vii) Detach NIC; (viii) Change VM\nOwnership; (ix) Rename VM; (x) Create Snapshot; (xi) Revert Snapshot; (xii)\n\n\n\n4.2. OPENNEBULA INTERFACE 71\n\nDelete Snapshot; (xiii) Resize VM; (xiv) Update VM; (xv) VM Information; (xvi)\nVM Pool Information, (xvii) VM Monitoring; and (xviii) VM Pool Monitoring.\nThere are other operations that are specific to cloud administrators that will not\nbe considered in this description like: (i) VM Deploy; (ii) VM Reschedule and\nUnreschedule actions; (iii) VM Permissions Mode (chmod); (iv) VM Migrate; (v)\nVM Recover and (vi) VM Pool Accounting.\n\n4.2.1.1 Allocate VM\n\nThe Allocate VM operation allocates a new virtual machine in OpenNebula. This\noperation invokes the one.vm.allocate method with the TEMPLATE and Boolean\ndata type parameters. If the operation is successful the allocated resource ID\nparameter is returned.\n\n\u2022 The TEMPLATE parameter is a string containing the template attributes\nto be applied to the allocated VM. These attributes are organized by Ca-\npacity, OS and Boot, Disk, Network, I/O Devices, Context, Placement and\nRaw sections. The structure and parameters of each section are described\nin the Virtual Machine Definition File 4.2 [129];\n\n\u2022 The Boolean data type parameter sets the VM creation on pending (if false)\nor on hold (if true).\n\n4.2.1.2 VM Actions\n\nThe VM Actions operation submits an action to be performed on a virtual ma-\nchine. This operation invokes the one.vm.action method with the action and ID\nparameters. If the operation is successful, it returns the VM ID parameter.\n\n\u2022 The action parameter is a string that indicates the name of the action to be\nperformed on the VM. The available action names are: (i) SHUTDOWN;\n(ii) SHUTDOWN-HARD; (iii) HOLD; (iv) RELEASE; (v) STOP; (vi)\nSUSPEND; (vii) RESUME; (viii) BOOT; (ix) DELETE; (x) DELETE-\nRECREATE; (xi) REBOOT; (xii) REBOOT-HARD; (xiii) POWEROFF;\n(xiv) POWEROFF-HARD; (xv) UNDEPLOY; and (xvi)\nUNDEPLOY-HARD.\n\n4.2.1.3 Save Disk\n\nThe Save Disk operation sets the disk to be saved in the given image. This\noperation invokes the one.vm.savedisk method with the ID, DISK ID, NAME,\nTYPE and Boolean data type parameters. If the operation is successful, it returns\nthe new Image ID parameter.\n\n\n\n72 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The Name parameter indicates the name for the new image where the disk\nwill be saved;\n\n\u2022 The Type parameter indicates the image type;\n\n\u2022 The Boolean data type parameter sets if the disk is saved immediately (true)\nor if the operation will be performed when the VM shuts down (false).\n\n4.2.1.4 Attach Disk\n\nThe Attach Disk operation attaches a new disk to a specific virtual machine. This\noperation invokes the one.vm.attach method with the ID and DISK parameters.\nIf the operation is successful, it returns the VM ID parameter.\n\n\u2022 The DISK parameter is a string containing pre-defined attributes from the\nDisk section template [129].\n\n4.2.1.5 Detach Disk\n\nThe Detach Disk operation detaches a disk from a specific virtual machine. This\noperation invokes the one.vm.detach method with the ID and DISK ID. If the\noperation is successful, it returns the VM ID parameter.\n\n4.2.1.6 Attach NIC\n\nThe Attach NIC operation attaches a new network interface to the virtual ma-\nchine. This operation invokes the one.vm.attachnic method with the ID and NIC\nparameters. If the operation is successful, it returns the VM ID parameter\n\n\u2022 The NIC parameter is a string containing pre-defined attributes from the\nNIC section template [129].\n\n4.2.1.7 Detach NIC\n\nThe Detach NIC operation detaches a network interface from a virtual machine.\nThis operation invokes the one.vm.detachnic method with the ID and NIC ID\nparameters. If the operation is successful, it returns the VM ID parameter.\n\n4.2.1.8 Change VM Ownership\n\nThe Change VM Ownership operation changes the ownership of a virtual ma-\nchine. This operation invokes the one.vm.chown method with the ID, USER ID\nand GROUP ID parameters. If the operation is successful, it returns the resource\nID parameter.\n\n\n\n4.2. OPENNEBULA INTERFACE 73\n\n4.2.1.9 Rename VM\n\nThe Rename VM operation renames a virtual machine. This operation invokes\nthe one.vm.rename method with the ID and NAME parameters. If the operation\nis successful, it returns the VM ID parameter.\n\n\u2022 The NAME parameter is a string with the new name for the VM.\n\n4.2.1.10 Create Snapshot\n\nThe Create Snapshot operation creates a new virtual machine snapshot. This\noperation invokes the one.vm.snapshotcreate method with the ID and NAME\nparameters. If the operation is successful, it returns the new SNAPSHOT ID\nparameter.\n\n\u2022 The NAME parameter is a string containing the new snapshot name (it\nmay be empty).\n\n4.2.1.11 Revert Snapshot\n\nThe Revert Snapshot operation reverts a virtual machine to a specified snap-\nshot. This operation invokes the one.vm.snapshotrevert method with the ID and\nSNAPSHOT ID parameters. If the operation is successful, it returns the VM ID\nparameter.\n\n4.2.1.12 Delete Snapshot\n\nThe Delete Snapshot operation deletes a virtual machine snapshot. This opera-\ntion invokes the one.vm.snapshotdelete method with the ID and SNAPSHOT ID\nparameters. If the operation is successful, it returns the VM ID parameter.\n\n4.2.1.13 Resize VM\n\nThe Resize VM operation changes the capacity of the virtual machine. This\noperation invokes the one.vm.resize method with the ID and TEMPLATE para-\nmeters. If the operation is successful, it returns the VM ID parameter.\n\n\u2022 The TEMPLATE parameter with the new capacity elements (CPU, VCPU\nand MEMORY).\n\n4.2.1.14 Update VM\n\nThe Update VM operation replaces the user template contents. This operation\ninvokes the one.vm.update method with the ID and TEMPLATE and parameters.\nIf the operation is successful, it returns the resource ID parameter.\n\n\n\n74 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The TEMPLATE parameter contains the new user template contents for\nthe VM.\n\n4.2.1.15 VM Information\n\nThe VM Information operation retrieves information about the virtual machine.\nThis operation invokes the one.vm.info method with the ID parameter. If the\noperation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a specific VM.\n\n4.2.1.16 VM Pool Information\n\nThe VM Pool Information operation retrieves information for all or part of the vir-\ntual machines associated with an user. This operation invokes the one.vmpool.info\nmethod with the filter, range start ID, range end ID and state parameters. If the\noperation is successful, it returns the information parameter.\n\n\u2022 The filter parameter is a filter flag that can be set to: -3 (or lower values)\nto show connected user\u2019s resources, -2 to show all resources, -1 to show\nconnected user\u2019s and his group\u2019s resources and 0 (or higher values) to show\nUnique IDentifier (UID) User\u2019s Resources;\n\n\u2022 The range start ID and range end ID parameters filter the result by a VM\nID range;\n\n\u2022 The state parameter filters the result by the VM state;\n\n\u2022 The information parameter is a string XML-formatted containing informa-\ntion about all VM.\n\n4.2.1.17 VM Monitoring\n\nThe VM Monitoring operation returns the virtual machine monitoring records.\nThis operation invokes the one.vm.monitoring with the ID parameters. If the\noperation is successful, it returns the monitoring parameter.\n\n\u2022 The monitoring parameter is a string with a list of XML element records\nof a specific VM.\n\n\n\n4.2. OPENNEBULA INTERFACE 75\n\n4.2.1.18 VM Pool Monitoring\n\nThe VM Pool Monitoring operation returns all the virtual machine monitoring\nrecords. This operation invokes the one.vmpool.monitoring with the filter para-\nmeter. If the operation is successful, it returns the monitoring parameter.\n\n\u2022 The monitoring parameter returns a list of VM elements. Each VM element\ncontains the complete xml of the VM with the updated information returned\nby the poll action.\n\nThe operations and parameters used by the Server Management component\nfrom OpenNebula interface are resumed in Table 4.1.\n\nTable 4.1: Server Management Table (operations vs parameters).\n\nOperations\nParameters A\n\nllo\nca\nte\n\nV\nM\n\nA\nctio\n\nn\ns\n\nS\nav\ne\nD\nisk\n\nA\ntta\n\nch\nD\nisk\n\nD\neta\n\nch\nD\nisk\n\nA\ntta\n\nch\nN\nIC\n\nD\neta\n\nch\nN\nIC\n\nC\nh\na\nn\ng\ne\nV\nM\n\nO\nw\nn\nersh\n\nip\n\nR\nen\n\na\nm\ne\nV\nM\n\nC\nrea\n\nte\nS\nn\na\np\nsh\no\nt\n\nR\nev\nert\n\nS\nn\na\np\nsh\no\nt\n\nD\nelete\n\nS\nn\na\np\nsh\no\nt\n\nR\nesize\n\nV\nM\n\nU\np\nd\na\nte\n\nV\nM\n\nV\nM\n\nIn\nfo\nrm\n\na\ntio\n\nn\n\nV\nM\n\nP\no\no\nl\nIn\nfo\nrm\n\na\ntio\n\nn\n\nV\nM\n\nM\no\nn\nito\n\nrin\ng\n\nV\nM\n\nP\no\no\nl\nM\no\nn\nito\n\nrin\ng\n\nSent Parameters\nTEMPLATE x x x x\nNAME x x\nBoolean\ndata type x x\n\nID x x x x x x x x x x x x x x x\naction x\nDISK ID x x\nTYPE x\nDISK x\nNIC x\nNIC ID x\nUSER ID x\nGROUP ID x\nSNAPSHOT\nID x x\n\nReceived Parameters\nSNAPSHOT\nID x\n\nID x x x x x x x x x x x x\ninformation x x\nmonitoring x x\nIMAGE ID x\n\n4.2.2 Template Management\n\nThe Template Management component provides methods to manage the template\nrepository in OpenNebula using the following list of operations: (i) Allocate\nTemplate; (ii) Clone Template; (iii) Delete Template; (iv) Instantiate Template;\n(v) Update Template; (vi) Change Template Ownership; (vii) Rename Template;\n(viii) Template Information; and (ix) Template Pool Information.\n\n\n\n76 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.2.2.1 Allocate Template\n\nThe Allocate Template operation allocates a new template in OpenNebula. This\noperation invokes the one.template.allocate method with the TEMPLATE para-\nmeters. If the operation is successful, it returns the allocated resource ID para-\nmeter.\n\n4.2.2.2 Clone Template\n\nThe Clone Template operation clones an existing virtual machine template. This\noperation invokes the one.template.clone method with the ID and NAME para-\nmeters. If the operation is successful, it returns the new template ID parameter.\n\n\u2022 The ID parameter indicates the identifier of the template;\n\n\u2022 The NAME parameter defines the new cloned template name.\n\n4.2.2.3 Delete Template\n\nThe Delete Template operation deletes the given template from the repository.\nThis operation invokes the one.template.delete method with the ID parameter.\nIf the operation is successful, it returns the resource ID parameter.\n\n4.2.2.4 Instantiate Template\n\nThe Instantiate Template operation instantiates a new virtual machine from a\ntemplate. This operation invokes the one.template.instantiate method with the\nID, VM NAME, EXTRA TEMPLATE and Boolean data type parameters. If\nthe operation is successful, it returns the new VM ID parameter.\n\n\u2022 The VM NAME parameter indicates the name for the new VM instance;\n\n\u2022 The EXTRA TEMPLATE parameter contains an extra template to be\nmerged with the one being instantiated;\n\n\u2022 The Boolean data type parameter sets the VM creation on pending (if false)\nor on hold (if true).\n\n4.2.2.5 Update Template\n\nThe Update Template operation replaces the template contents. This operation\ninvokes the one.template.update method with the ID, TEMPLATE and TYPE\nparameters. If the operation is successful, it returns the resource ID parameter.\n\n\u2022 The TYPE parameter specifies the update type, indicating if the template\nis going to be partial or completely modified.\n\n\n\n4.2. OPENNEBULA INTERFACE 77\n\n4.2.2.6 Change Template Ownership\n\nThe Change Template Ownership operation changes the ownership of a template.\nThis operation invokes the one.template.chown method with the ID, USER ID\nand GROUP ID parameters. If the operation is successful, it returns the resource\nID parameter.\n\n\u2022 The USER ID parameter identifies the new template owner;\n\n\u2022 The GROUP ID parameter identifies the new group for the template.\n\n4.2.2.7 Rename Template\n\nThe Rename Template operation renames a template. This operation invokes\nthe one.template.rename method with the ID and NAME parameters. If the\noperation is successful, it returns the VM ID parameter.\n\n4.2.2.8 Template Information\n\nThe Template Information operation retrieves information from the template.\nThis operation invokes the one.template.info method with the ID parameter. If\nthe operation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a specific VM template.\n\n4.2.2.9 Template Pool Information\n\nThe Template Pool Information operation retrieves information about all or part\nof the public and owned templates in the repository. This operation invokes the\none.templatepool.info method with the filter, range-start-ID and range-end-ID\nparameters. If the operation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a list of VM templates.\n\nThe operations and parameters used by the Template Management compon-\nent from OpenNebula interface are resumed in Table 4.2.\n\n\n\n78 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.2: Template Management Table (Parameters vs Operations).\n\nOperations\nParameters A\n\nllo\ncate\n\nT\nem\n\np\nlate\n\nC\nlon\n\ne\nT\nem\n\np\nlate\n\nD\nelete\n\nT\nem\n\np\nlate\n\nIn\nstan\n\ntiate\nT\nem\n\np\nlate\n\nU\np\nd\nate\n\nT\nem\n\np\nlate\n\nC\nh\nan\n\nge\nT\nem\n\np\nlate\n\nO\nw\nn\nersh\n\nip\n\nR\nen\n\nam\ne\nT\nem\n\np\nlate\n\nT\nem\n\np\nlate\n\nIn\nform\n\nation\n\nT\nem\n\np\nlate\n\nP\no\nol\n\nIn\nform\n\nation\n\nSent Parameters\nTEMPLATE x x\nID x\nEXTRA\nTEMPLATE\n\nx x\n\nNAME x\nVM NAME x\nBoolean\ndata type\n\nx\n\nUSER ID x\nGROUP ID x\nfilter x\nrange start\nID\n\nx\n\nrange end ID x\nTYPE x\n\nReceived Parameters\nID x x x x x x x\ninformation x x\nVM ID x x\n\n4.2.3 Image Management\n\nThe Image Management component provides methods to manage the image re-\npository in OpenNebula using the following list of operations: (i) Allocate Im-\nage; (ii) Clone Image; (iii) Delete Image; (iv) Enable Image; (v) Persistent; (vi)\nChange Image Type; (vii) Update Image; (viii) Change Image Ownership; (ix)\nRename Image; (x) Image Information; and (xi) Image Pool Information.\n\n4.2.3.1 Allocate Image\n\nThe Allocate Image operation allocates a new image in OpenNebula. This oper-\nation invokes the one.image.allocate method with the TEMPLATE and DATA-\nSTORE ID parameters. If the operation is successful, it returns the allocated\nresource ID parameter.\n\n\u2022 The template parameter is a string XML-formated containing the template\nattributes of the image [130].\n\n\n\n4.2. OPENNEBULA INTERFACE 79\n\n4.2.3.2 Clone Image\n\nThe Clone Image operation clones an existing image. This operation invokes the\none.image.clone method with the ID and NAME parameters. If the operation is\nsuccessful, it returns the new image ID parameter.\n\n4.2.3.3 Delete Image\n\nThe Delete Image operation deletes the given image from the repository. This\noperation invokes the one.image.delete method with the ID parameter. If the\noperation is successful, it returns the resource ID parameter.\n\n4.2.3.4 Enable Image\n\nThe Enable/Disable Image operation enables or disables an image. This oper-\nation invokes the one.image.enable method with the ID and Boolean data type\nparameters. If the operation is successful, it returns the image ID parameter.\n\n\u2022 The Boolean data type parameter enables (if true) or disables (if false) the\nimage.\n\n4.2.3.5 Persistent\n\nThe Persistent operation sets the image as persistent or non-persistent. This\noperation invokes the one.image.persistent method with the ID and Boolean data\ntype parameters. If the operation is successful, it returns the image ID parameter.\n\n\u2022 The Boolean data type parameter defines the image as persistent (if true)\nor as non-persistent (if false).\n\n4.2.3.6 Change Image Type\n\nThe Change Image Type operation changes the type of an image. This operation\ninvokes the one.image.chtype method with the ID and image-type parameters. If\nthe operation is successful, it returns the image ID parameter.\n\n\u2022 The image-type parameter identifies the image type. The available types\nare: OS, CDROM, DATABLOCK, KERNEL, RAMDISK and CONTEXT.\nThe default type OS is attributed in case this parameter is omitted.\n\n4.2.3.7 Update Image\n\nThe Update Image operation replaces the image template contents. This opera-\ntion invokes the one.image.update method with the ID, TEMPLATE and TYPE\nparameters. If the operation is successful, it returns the resource ID parameter.\n\n\n\n80 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.2.3.8 Change Image Ownership\n\nThe Change Image Ownership operation changes the ownership of an image.\nThis operation invokes the one.image.chown method with the ID, USER ID and\nGROUP ID parameters. If the operation is successful, it returns the resource ID\nparameter.\n\n4.2.3.9 Rename Image\n\nThe Rename Image operation renames an image. This operation invokes the\noe.image.rename method with the ID and NAME parameters. If the operation\nis successful, it returns the VM ID parameter.\n\n4.2.3.10 Image Information\n\nThe Image Information operation retrieves information about a specific image.\nThis operation invokes the one.image.info method with the ID parameter. If the\noperation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a specific image.\n\n4.2.3.11 Image Pool Information\n\nThe Image Pool Information retrieves information for all or part of the images\nin the repository. This operation invokes the one.imagepool.info with the filter,\nrange start ID and range end ID parameters. If the operation is successful, it\nreturns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a list of images.\n\nThe operations and parameters used by the Image Management component\nfrom OpenNebula interface are resumed in Table 4.3.\n\n\n\n4.2. OPENNEBULA INTERFACE 81\n\nTable 4.3: Image Management Table (Parameters vs Operations).\n\nOperations\nParameters A\n\nllo\nca\nte\n\nIm\na\ng\ne\n\nC\nlo\nn\ne\nIm\n\na\ng\ne\n\nD\nelete\n\nIm\na\ng\ne\n\nE\nn\na\nb\nle/\n\nD\nisa\n\nb\nle\n\nIm\na\ng\ne\n\nP\nersisten\n\nt\n\nC\nh\na\nn\ng\ne\nIm\n\na\ng\ne\nT\ny\np\ne\n\nU\np\nd\na\nte\n\nIm\na\ng\ne\n\nC\nh\na\nn\ng\ne\nIm\n\na\ng\ne\nO\nw\nn\nersh\n\nip\n\nR\nen\n\na\nm\ne\nIm\n\na\ng\ne\n\nIm\na\ng\ne\nIn\nfo\nrm\n\na\ntio\n\nn\n\nIm\na\ng\ne\nP\no\no\nl\nIn\nfo\nrm\n\na\ntio\n\nn\n\nSent Parameters\nTEMPLATE x x\nDATASTORE\nID\n\nx\n\nID x x x x x x x x x\nNAME x x\nimage-type x\nBoolean data\ntype\n\nx x\n\nUSER ID x\nGROUP ID x\nfilter x\nrange start ID x\nrange end ID x\nTYPE x\n\nReceived Parameters\nID x x x x x x x x\nVM ID x\ninformation x x\n\n4.2.4 Network Management\n\nThe VN Management component provides methods to manage virtual networks\nin OpenNebula using the following list of operations: (i) Allocate VN; (ii) Delete\nVN; (iii) Add VN Leases; (iv) Remove VN Leases; (v) Hold VN Lease; (vi)\nRelease VN Lease; (vii) Update VN; (viii) Change VN Ownership; (ix) Rename\nVN; (x) VN Information; and (xi) VN Pool Information.\n\n4.2.4.1 Allocate VN\n\nThe Allocate VN operation allocates a new virtual network in OpenNebula.\nThis operation invokes the one.vn.allocate method with the TEMPLATE and\nCLUSTER ID parameters. If the operation is successful, it returns the allocated\nresource ID parameter.\n\n\u2022 The TEMPLATE parameter is a string that specifies the template attrib-\nutes of the virtual network [131];\n\n\u2022 The CLUSTER ID parameter indicates the ID of the cluster. If the value\nis defined as -1 the virtual network will not be added to any cluster.\n\n\n\n82 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.2.4.2 Delete VN\n\nThe Delete VN operation deletes the given virtual network from the pool. This\noperation invokes the one.vn.delete method with the ID parameter. If the oper-\nation is successful, it returns the resource ID parameter.\n\n4.2.4.3 Add VN Leases\n\nThe Add VN Leases operation adds a new lease to the virtual network (only avail-\nable for FIXED networks). This operation invokes the one.vn.addleases method\nwith the ID and LEASE parameters. If the operation is successful, it returns the\nresource ID parameter.\n\n\u2022 The LEASE parameter specifies the template of the lease to add.\n\n4.2.4.4 Remove VN Leases\n\nThe Remove VN Leases operation removes a lease from the virtual network (also\nis only available for FIXED networks). This operation invokes the one.vn.rmleases\nmethod with the ID and LEASE parameters. If the operation is successful, it\nreturns the resource ID parameter.\n\n\u2022 The LEASE parameter specifies the template of the lease to removed.\n\n4.2.4.5 Hold VN Lease\n\nThe Hold VN Lease operation holds a virtual network lease on use. This operation\ninvokes the one.vn.hold method with the ID and LEASE parameters. If the\noperation is successful, it returns the resource ID parameter.\n\n\u2022 The LEASE parameter specifies the template of the lease to hold.\n\n4.2.4.6 Release VN Lease\n\nThe Release VN Lease operation releases a virtual network lease on hold. This\noperation invokes the one.vn.release method with the ID and LEASE parameters.\nIf the operation is successful, it returns the resource ID parameter.\n\n\u2022 The lease_template parameter specifies the template of the lease to release.\n\n\n\n4.2. OPENNEBULA INTERFACE 83\n\n4.2.4.7 Update VN\n\nThe Update VN operation replaces the virtual network template contents. This\noperation invokes the one.vn.update method with the ID, TEMPLATE and\nTYPE parameters. If the operation is successful, it returns the resource ID\nparameter.\n\n\u2022 The TEMPLATE parameter specifies the contents of the new template;\n\n\u2022 The TYPE parameter specifies if the update operation is to replace the\nwhole template (value=\u20190\u2019) or to merge the new template with the existing\none (value=\u20191\u2019).\n\n4.2.4.8 Change VN Ownership\n\nThe Change VN Ownership operation changes the ownership of a virtual network.\nThis operation invokes the one.vn.chown method with the ID, USER ID and\nGROUP ID parameters. If the operation is successful, it returns the resource ID\nparameter.\n\n4.2.4.9 Rename VN\n\nThe Rename VN operation renames a virtual network. This operation invokes\nthe one.vn.rename method with the ID and NAME parameters. If the operation\nis successful, it returns the VM ID parameter.\n\n4.2.4.10 VN Information\n\nThe VN Information operation retrieves information about the virtual network.\nThis operation invokes the one.vn.info method with the ID parameter. If the\noperation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a specific virtual network.\n\n4.2.4.11 VN Pool Information\n\nThe VN Pool Information operation retrieves information about all or part of the\nvirtual networks in the pool. This operation invokes the one.vnpool.info method\nwith the filter, range start ID and range end ID parameters. If the operation is\nsuccessful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a list of virtual networks.\n\n\n\n84 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nThe operations and parameters used by the Virtual Network Management\ncomponent from OpenNebula interface are resumed in Table 4.4.\n\nTable 4.4: Virtual Network Management Table (Parameters vs Operations).\n\nOperations\nParameters A\n\nllo\ncate\n\nD\nelete\n\nA\nd\nd\nL\nease\n\nR\nem\n\nove\nL\nease\n\nH\nold\n\nR\nelease\n\nU\np\nd\nate\n\nC\nh\nan\n\nge\nO\nw\nn\nersh\n\nip\n\nR\nen\n\nam\ne\n\nIn\nform\n\nation\n\nV\nN\n\nP\no\nol\n\nIn\nform\n\nation\n\nSent Parameters\nTEMPLATE x x\nLEASE x x x x\nNAME x\nTYPE x\nUSER ID x\nID x x x x x x x x x\nGROUP ID x\nCLUSTER\nID\n\nx\n\nfilter x\nrange start\nID\n\nx\n\nrange end ID x\nReceived Parameters\n\nID x x x x x x x x\ninformation x x\nVM ID x\n\n4.2.5 Data-Store Management\n\nThe Data-Store (DS) Management Component provides the following list of oper-\nations to manage the data-store types used with OpenNebula: (i) Allocate Data-\nstore; (ii) Delete Data-store; (iii) Update Data-store; (iv) Change Data-store\nOwnership; (v) Data-store Information; and (vi) Data-store Pool Information.\n\n4.2.5.1 Allocate Data-store\n\nThe Allocate Data-store operation allocates a new data-store in OpenNebula.\nThis operation invokes the one.datastore.allocate method with the TEMPLATE\nparameter. If the operation is successful, it returns the resource ID parameter.\n\n\u2022 The TEMPLATE parameter is a string that specifies the template of the\ndata-store. The template attributes vary with the type of the desired data-\nstore: LVM [132], vmware [133], ceph [134], system [135], file system [136],\niSCSI [137] and kernel&amp;file [138].\n\n\n\n4.2. OPENNEBULA INTERFACE 85\n\n4.2.5.2 Delete Data-store\n\nThe Delete Data-store operation deletes the given data-store from the pool. This\noperation invokes the one.datastore.delete parameter with the ID parameter. If\nthe operation is successful, it returns the resource ID parameter.\n\n4.2.5.3 Update Data-store\n\nThe Update Data-store operation replaces the data-store template contents. This\noperation invokes the one.datastore.update method with the ID, TEMPLATE\nand TYPE parameters. If the operation is successful, it returns the resource ID\nparameter.\n\n\u2022 The TEMPLATE parameter specifies a new data-store template;\n\n\u2022 The TYPE parameter specifies if the update operation is to replace the\nwhole template (value=\u20190\u2019) or to merge the new template with the existing\none (value=\u20191\u2019).\n\n4.2.5.4 Change Data-store Ownership\n\nThe Change Data-store Ownership operation changes the ownership of a data-\nstore. This operation invokes the one.datastore.chown method with the ID, USER\nID and GROUP ID parameters. If the operation is successful, it returns the\nresource ID parameter.\n\n4.2.5.5 Data-store Information\n\nThe Data-store Information operation retrieves information from an OpenNebula\ndata-store. This operation invokes the one.datastore.info method with the ID\nparameter. If the operation is successful, it returns the information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a specific data-store.\n\n4.2.5.6 Data-store Pool Information\n\nThe Data-store Pool Information operation retrieves information from all or part\nof the data-stores in the pool. This operation invokes the one.datastorepool.info\nmethod without additional parameters. If the operation is successful, it returns\nthe information parameter.\n\n\u2022 The information parameter is a string XML-formatted containing the in-\nformation of a list of data-stores.\n\n\n\n86 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nThe operations and parameters used by the Data-store Management compon-\nent from OpenNebula interface are resumed in Table 4.5.\n\nTable 4.5: Data-store Management Table (Parameters vs Operations).\n\nOperations\nParameters Allocate\n\nDS\nDelete\nDS\n\nUpdate\nDS\n\nChange DS\nOwnership\n\nDS\nInformation\n\nDS Pool\nInformation\n\nSent Parameters\nTEMPLATE x x\nTYPE x\nUSER ID x\nID x x x x\nGROUP ID x\n\nReceived Parameters\nID x x x x\ninformation x x\n\n4.3 OpenStack Interface\n\nThe OpenStack API is a RESTful HTTP service that supports both JSON and\nXML data serialization request and response formats. The request format is\nspecified in the Content-Type header of the request. The response format is\nspecified either through the Accept header or by adding an .xml or .json extension\nto the request URI. JSON will be used by default. The API uses both URI and\nMultipurpose Internet Mail Extensions (MIME) type versioning schemas. In the\nURI schema, the first element of the path contains the target version identifier,\ne.g.:\nhttps :// s e r v e r s . api . openstack . org /v1 .0/224532/ s e r v e r s /123.\n\nThe MIME type versioning schema uses HTTP content negotiation where the\nAccept or Content-Type headers contains a MIME type that identifies the version\n(application/vnd.openstack.compute.v2+xml). A version MIME type is always\nlinked to a base MIME type (application/xml or application/json) and allows\nthe creation of permanent links because the version schema is not specified in the\nURI path, e.g.:\nhttps :// api . s e r v e r s . openstack . org /224532/ s e r v e r s /123.\n\nIf conflicting versions are specified using both an HTTP header and a URI,\nthe URI takes precedence.\n\nThe OpenStack interface libraries of the current release - Grizzly - are provided\nby the Nova, Cinder, Glance, Neutron, Keystone and Horizon modules. The\ncomplete list of components provided by these modules is available at [139]. This\nanalysis will be focussed on the common functionalities between the OpenStack\nNova, Glance, Neutron and Cinder interface libraries and the API of the other\n\n\n\n4.3. OPENSTACK INTERFACE 87\n\nIaaS platforms under comparison. The OpenStack Nova API manages the fol-\nlowing components: (i) Servers; (ii) Server Addresses; (iii) Server Actions; and\n(iv) Flavours. The OpenStack Glance, Neutron and Cinder interface libraries,\non the other hand, are responsible for the provision of Images, Networking and\nVolume management components respectively. To simplify the comparison with\nother interface libraries, the referred components will be listed and describe in\nthe following sections as: (i) Server Management (aggregating the Servers, Server\nAddresses and Server Actions); (ii) Flavour Management; (iii) Image Manage-\nment; (iv) Network Management and (v) Volume Management.\n\n4.3.1 Server Management\n\nThe Servers Management component is used to perform actions, obtain informa-\ntion and list addresses for a specific server through the following list of operations:\n(i) List Servers; (ii) Create Server; (iii) Get Server Details; (iv) Update Server;\n(v) Delete Server; (vi) List Addresses; (vii) List Addresses by Network; (viii)\nChange Administrator Password; (ix) Reboot Server; (x) Rebuild Server; (xi)\nResize Server; (xii) Confirm Resized Server; (xiii) Revert Resized Server; and\n(xiv) Create Image.\n\n4.3.1.1 List Servers\n\nThe List Servers operation lists the servers associated with an account. This oper-\nation does not require a request body and the list can be filtered by the imageRef,\nflavorRef, name, status, marker, limit and change-Since query parameters. The\nresponse returns a servers parameter.\n\n\u2022 The imageRef parameter specifies the image reference as an ID or full URL;\n\n\u2022 The flavorRef parameter specifies the flavour reference as an ID or full URL;\n\n\u2022 The name parameter specifies the name of the server;\n\n\u2022 The status parameter specifies the server status values. These values can\nbe: ACTIVE, BUILD, DELETED, ERROR, HARD_REBOOT, PASS-\nWORD, REBOOT, REBUILD, RESCUE, RESIZE, REVERT_RESIZE,\nSHUTOFF, SUSPENDED, UNKNOWN and VERIFY_RESIZE;\n\n\u2022 The marker parameter specifies the ID of the last item in the previous list;\n\n\u2022 The limit parameter specifies the page size limit;\n\n\u2022 The change-Since parameter specifies the time since the last change oc-\ncurred;\n\n\n\n88 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The servers parameter contains the id, tenant-id, user-id, name, status,\nprogress, hostId, updated, created, accessIPv4, accessIPv6, imageRef, fla-\nvorRef, metadata, addresses and link attributes for each listed server.\n\n4.3.1.2 Create Server\n\nThe Create Server operation asynchronously provisions a new server. This oper-\nation sends the required imageRef, flavorRef and name parameters as well as the\noptional metadata, personality and networks parameters. The response message\nreturns the Location, id, link and adminPass parameters.\n\n\u2022 The metadata parameter specifies the Metadata key and value pairs;\n\n\u2022 The personality parameter specifies the file path and contents (text only)\nattributes to inject into the server at launch;\n\n\u2022 The networks parameter specifies the list of networks and their attributes;\n\n\u2022 The Location parameter contains a full URL to the created server.\n\n4.3.1.3 Get Server Details\n\nThe Get Server Details operation returns the details of a specific server by its ID.\nThis operation does not require a request body as the id parameter is included\nin the URI path. The response returns a server parameter.\n\n\u2022 The server parameter contains the id, tenant-id, user-id, name, status, pro-\ngress, hostId, updated, created, accessIPv4, accessIPv6, imageRef, flavor-\nRef, metadata, addresses and link attributes for the specified server.\n\n4.3.1.4 Update Server\n\nThe Update Server operation is used to update the editable attributes of a spe-\ncified server. This operation sends the name, accessIPv4 and accessIPv6 para-\nmeters as well as the id parameter in the URI path. The response returns the\nserver parameter.\n\n4.3.1.5 Delete Server\n\nThe Delete Server operation deletes a specified cloud server instance from the\nsystem. This operation does not require a request body as the id parameter is\nincluded in the URI path. It also does not returns a response body. The success\nor failure of this operation is returned in the response header in the Status-Line.\n\n\n\n4.3. OPENSTACK INTERFACE 89\n\n4.3.1.6 List Addresses\n\nThe List Addresses operation lists networks and addresses for a specified tenant\nand server. This operation does not require a request body as the id parameter\nis included in the URI path. The response returns the addresses parameter.\n\n\u2022 The addresses parameter include the list of public and private elements that\nprovide information about the IP addresses through the version and addr\nattributes.\n\n4.3.1.7 List Addresses by Network\n\nThe List Addresses by Network operation lists addresses for a specified tenant,\nserver and network. This operation does not require a request body sending the\nid and network_label parameters in the URI path. The response returns the\nnetwork parameter.\n\n\u2022 The network_label parameter specifies the network label, such as public or\nprivate;\n\n\u2022 The network parameter contains a list of IP elements with the version and\naddr attributes for the IP addresses defined in the network and the ID\nelement that identifies the network type (public or private).\n\n4.3.1.8 Change Administrator Password\n\nThe Change Administrator Password operation changes the administrator pass-\nword for a specified server. This operation sends a adminPass parameter in the\nrequest body and the id parameter in the URI path. It does not return a response\nbody. The success or failure of this operation is returned in the response header\nin the Status-Line.\n\n4.3.1.9 Reboot Server\n\nThe Reboot Server operation enables a soft or hard reboot of the specified server.\nWith a soft reboot, the operating system is signalled to restart. A hard reboot\nis the equivalent of power cycling the server. This operation sends the type\nparameter in the request body and the id parameter in the URI path. It does\nnot return a response body. The success or failure of this operation is returned\nin the response header in the Status-Line.\n\n\u2022 The type parameter specifies the type of reboot. The available values are\nSOFT and HARD.\n\n\n\n90 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.3.1.10 Rebuild Server\n\nThe Rebuild Server operation removes all data on the server and replaces it\nwith the specified image. This operation sends the name, imageRef, accessIPv4,\naccessIPv6, adminPass, metadata and personality parameters in the request body\nand the id parameter in the URI path. The Response returns the server container\nwith the parameters described in the List Servers operation.\n\n4.3.1.11 Resize Server\n\nThe Resize Server operation converts an existing server to a different flavour,\nscaling the server up or down. This operation sends the flavorRef parameter in\nthe request body and the id parameter in the URI path. It does not return a\nresponse body. The success or failure of this operation is returned in Status-Line\nresponse header.\n\n4.3.1.12 Confirm Resized Server\n\nThe Confirm Resized Server operation confirms the success of the operation, i.e.,\nafter it has been verified that the newly resized server is functioning properly.\nThis operation send the confirmResize parameter in the request body and the id\nparameter in the URI path. It does not return a response body. The success or\nfailure of this operation is returned in the Status-Line response header.\n\n4.3.1.13 Revert Resized Server\n\nThe Revert Resized Server operation is used to revert the resize and roll back\nto the original server. This operation sends the revertResize parameter in the\nrequest body and the id parameter in the URI path. It does not return a response\nbody. The success or failure of this operation is found in the Status-Line response\nheader.\n\n4.3.1.14 Create Image\n\nThe Create Image operation creates a new image for the given server that can\nlater be used to rebuild or create servers. This operation sends the name and\nmetadata parameters in the request body and the id parameter in the URI path.\nIt does not return a response body. The success or failure of this operation is\nreturned in the Status-Line response header.\n\nThe operations and parameters used by the Server Management component\nfrom OpenStack interface are resumed in Table 4.6.\n\n\n\n4.3. OPENSTACK INTERFACE 91\n\nTable 4.6: Server Management Table (Parameters vs Operations).\n\nOperations\nParameters L\n\nist\nS\nervers\n\nC\nreate\n\nS\nerver\n\nG\net\n\nS\nerver\n\nD\netails\n\nU\np\nd\nate\n\nS\nerver\n\nD\nelete\n\nS\nerver\n\nL\nist\n\nA\nd\nd\nresses\n\nL\nist\n\nad\nd\nresses\n\nb\ny\nN\netw\n\nork\n\nC\nh\nan\n\nge\nA\nd\nm\nin\nistrator\n\nP\nassw\n\nord\n\nR\neb\n\no\not\n\nS\nerver\n\nR\neb\n\nu\nild\n\nS\nerver\n\nR\nesize\n\nS\nerver\n\nC\non\n\nfi\nrm\n\nR\nesize\n\nS\nerver\n\nR\nevert\n\nR\nesized\n\nS\nerver\n\nC\nreate\n\nIm\nage\n\nSent Parameters\nimageRef x x x\nflavorRef x x x\nname x x x x x\nstatus x\nmarker x\nlimit x\nchange-since x\nid x x x x x x x x x x x x\naccessIPv4 x x\naccessIPv6 x x\nmetadata x x x\npersonality x x\nnetworks x\nadminPass x x\nnetwork_label x\nconfirmResize x\nrevertResize x\ntype x\n\nReceived Parameters\nlocation x\nadminPass x\nnetwork x\nlink x\naddresses x\nservers x\nserver x x x\nid x\n\n\n\n92 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.3.2 Flavour Management\n\nIn OpenStack the virtual hardware templates are called \u201cflavors\u201d which are avail-\nable hardware configurations for the servers. Each flavour has a unique combin-\nation of virtual CPU cores, disk space and memory capacity. The Flavour Man-\nagement component is used to obtain the list of available flavours and to provide\ninformation about a specified flavour through the following operations: (i) List\nFlavours and (ii) Get Flavour Details.\n\n4.3.2.1 List Flavours\n\nThe List Flavours operation lists information for all available flavours in the\nresponse body. This operation does not require a request body and the list can\nbe filtered by the minDisk, minRam, marker and limit query parameters. The\nresponse returns the flavors parameter.\n\n\u2022 The minDisk parameter specifies the minimum number of gigabytes of disk\nstorage;\n\n\u2022 The minRam parameter specifies the minimum amount of RAM in mega-\nbytes;\n\n\u2022 The marker parameter identifies the Universally Unique IDentifier (UUID)\nof the flavor to set a marker;\n\n\u2022 The limit parameter specifies the integer limit value of flavours returned;\n\n\u2022 The flavors parameter contains the id, links and name attributes of each\nflavour.\n\n4.3.2.2 Get Flavour Details\n\nThe Get Flavour Details operation returns details of the specified flavour in the\nresponse body. This operation does not require a request body as the id para-\nmeter is included in the URI path. The response returns the flavor parameter.\n\n\u2022 The flavor parameter contains the the disk, id, links, name, ram and vcpus\nattributes of a specified flavour.\n\nThe operations and parameters used by the Flavour Management component\nfrom OpenStack interface are resumed in Table 8.\n\n\n\n4.3. OPENSTACK INTERFACE 93\n\nTable 4.7: Flavour Management Table (Parameters vs Operations).\n\nOperations\nParameters List Flavours Get Flavour Details\n\nSent Parameters\nminDisk x\nminRam x\nmarker x\nlimit x\nid x\n\nReceived Parameters\nflavors x\nflavor x\n\n4.3.3 Image Management\n\nThe Image Management component is used to manage OpenStack images used\nto create or rebuild a server. This component allows the following operations: (i)\nCreate Image; (ii) List Images; (iii) Get Image Details; (iv) Update Image; (v)\nDelete Image; (vi) Upload Image; (vii) Download Image; (viii) Add Image Tag;\nand (ix) Delete Image Tag.\n\n4.3.3.1 Create Image\n\nThe Create Image operation creates a virtual machine image. This operation\nsends the required name parameter as well as the optional id, visibility and tags\nparameters. The response returns the Location parameter.\n\n\u2022 The id parameter specifies the image ID;\n\n\u2022 The name parameter specifies the image name file;\n\n\u2022 The visibility parameter indicates if the image is public or private; by de-\nfault it is public;\n\n\u2022 The tags parameter attributes a tag to the image;\n\n\u2022 The Location parameter contains a full URL to the created image.\n\n4.3.3.2 List Images\n\nThe List Images operation lists a subset of a larger collection of images. This\noperation does not require a request body and the list can be filtered by the\nvisibility, member_status, owner, size_min, size_max, sort_key, sort_dir, name,\nstatus, marker, and limit query parameters. The response returns the images\nparameter.\n\n\n\n94 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The images parameter is composed by the id, name, deleted, container_format,\ncreated_at disk_format, updated_at, min_disk, protected, min_ram, check-\nsum, owner, is_public, deleted_at and size attributes as well as the proper-\nties element with the kernel_id and ramdisk_id attributes for each image.\n\n4.3.3.3 Get Image Details\n\nThe Get Image Details operation returns information about a specified image.\nThis operation does not require a request body as the id parameter is rewritten\nin the URI. The response returns the corresponding image parameter.\n\n\u2022 The image parameter is composed by the id, name, deleted, container_format,\ncreated_at disk_format, updated_at, min_disk, protected, min_ram, check-\nsum, owner, is_public, deleted_at and size attributes as well as the prop-\nerties element with the kernel_id and ramdisk_id attributes of a specific\nimage.\n\n4.3.3.4 Update Image\n\nThe Update Image operation updates a specified image. This operation sends\nthe new id, name, visibility and tags parameters in the request body and the old\nid parameter in the URI. The response returns the updated image parameter.\n\n4.3.3.5 Delete Image\n\nThe Delete Image deletes a specified image. This operation does not require a\nrequest body and sends the id parameter in the URI. It does not return a response\nbody. The success or failure of this operation is returned in the response header\nin the Status-Line.\n\n4.3.3.6 Upload Image\n\nThe Upload Image operation is used to upload binary image data. This operation\ndoes not require a request body and sends the id parameter and file location in the\nURI. It does not return a response body. The success or failure of this operation\nis returned in the response header in the Status-Line.\n\n4.3.3.7 Download Image\n\nThe Download Image operation is used to download binary image data. This\noperation does not require a request body and sends the id parameter and file\nlocation in the URI. It does not return a response body. The success or failure\nof this operation is returned in the response header in the Status-Line.\n\n\n\n4.3. OPENSTACK INTERFACE 95\n\n4.3.3.8 Add Image Tag\n\nThe Add Image Tag operation adds a specified tag to a specified image. This\noperation does not require a request body and sends the id and tag parameters\nin the URI. It does not return a response body. The success or failure of this\noperation is returned in the response header in the Status-Line.\n\n4.3.3.9 Delete Image Tag\n\nThe Delete Image Tag operation deletes a specified tag from a specified image.\nThis operation does not require a request body and sends the id and tag para-\nmeters in the URI. It does not return a response body. The success or failure of\nthis operation is returned in the response header Status-Line parameter.\n\nThe operations and parameters used by the Image Management component\nfrom OpenStack interface are resumed in Table 4.8.\n\nTable 4.8: Image Management Table (Parameters vs Operations).\n\nOperations\nParameters C\n\nreate\nIm\n\nage\n\nL\nist\n\nIm\nages\n\nG\net\n\nIm\nage\n\nD\netails\n\nU\np\nd\nate\n\nIm\nage\n\nD\nelete\n\nIm\nage\n\nU\np\nload\n\nIm\nage\n\nD\now\n\nn\nload\n\nIm\nage\n\nA\nd\nd\nIm\n\nage\nT\nag\n\nD\nelete\n\nIm\nage\n\nT\nag\n\nSent Parameters\nname x x x\nid x x x x x x x x x\nnew id x\nvisibility x x\ntags x x x x\nmarker x\nlimit x\nstatus x\nmember_status x\nowner x\nsize_min x\nsize_max x\nsort_key x\nsort_dir x\nfile x x\n\nReceived Parameters\nimages x\nimage x x\nlocation x\n\n\n\n96 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.3.4 Network Management\n\nThe Network Management component provides virtual networking services among\ndevices that are managed by the OpenStack Compute service using the following\nlist of operations: (i) List Networks; (ii) Show Network; (iii) Create Network;\n(iv) Update Network; (v) Delete Network; (vi) List Subnets; (vii) Show Subnet;\n(viii) Create Subnet; (ix) Update Subnet; (x) Delete Subnet; (xi) List Ports;\n(xii) Show Port; (xiii) Create Port (xiv) Update Port; and (xv) Delete Port;\n\n4.3.4.1 List Networks\n\nThe List Networks operation lists the available networks to which the specified\ntenant has access. This operation does not require a request body and rewrites the\ntenant_id parameter in the URI. The response returns the networks parameter.\n\n\u2022 The tenant_id parameter specifies the ID of the VM;\n\n\u2022 The networks parameter contains the status, subnets, name,\nadmin_state_up, tenant_id, id and shared list of attributes for every listed\nnetwork.\n\n4.3.4.2 Show Network\n\nThe Show Network operation shows information about the specified network.\nThis operation does not require a request body and sends the tenant_id and\nnetwork_id parameters in the URI. The response returns the network parameter.\n\n\u2022 The network_id parameter contains the ID of a specific network;\n\n\u2022 The network parameter contains the status, subnets, name,\nadmin_state_up, tenant_id, id and shared list of attributes for the specific\nnetwork.\n\n4.3.4.3 Create Network\n\nThe Create Network operation creates a network. This operation sends the name,\nadmin_state_up and shared parameters in the request body and the tenant_id\nparameter in the URI. The response returns the network parameter.\n\n\u2022 The name parameter contains the network name;\n\n\u2022 The admin_state_up parameter is a Boolean value that indicates the ad-\nministrative status of the network;\n\n\u2022 The shared parameter indicates whether the network is shared across all\ntenants. Only administrative users can change this value.\n\n\n\n4.3. OPENSTACK INTERFACE 97\n\n4.3.4.4 Update Network\n\nThe Update Network operation updates the specified network. This operation\nsends the name, admin_state_up and shared parameters in the request body\nas well as the tenant_id and network_id parameters in the URI. The response\nreturns the network parameter.\n\n4.3.4.5 Delete Network\n\nThe Delete Network operation deletes the specified network and its associated re-\nsources. This operation does not require a request body and sends the tenant_id\nand network_id parameters in the URI. The success or failure of this operation\nis returned in the response header Status-Line parameter.\n\n4.3.4.6 List Subnets\n\nThe List Subnets operation lists the subnets to which the specified tenant has\naccess. This operation does not require a request body and sends the tenant_id\nparameter in the URI. The response returns the subnets parameter.\n\n\u2022 The subnets parameter contains , for each subnet, the allocation_pools,\ncidr, dns_nameservers, enable_dhcp, gateway_ip, host_routes, id,\nip_version, name, network_id and tenant_id attributes.\n\n4.3.4.7 Show Subnet\n\nThe Show Subnet operation returns data about a specific subnet. This opera-\ntion does not require a request body and sends the tenant_id and subnet_id\nparameters in the URI. The response returns the subnet parameter.\n\n\u2022 The subnet_id parameter contains the ID of the desired subnet. The value\nis obtained from the id attribute from the subnet or subnets parameters;\n\n\u2022 The subnet parameter contains the allocation_pools, cidr, enable_dhcp,\ngateway_ip, id, ip_version, name, network_id and tenant_id attributes of\nthe requested subnet.\n\n4.3.4.8 Create Subnet\n\nThe Create Subnet operation creates a subnet on a specified network. This op-\neration sends the allocation_pools, cidr, enable_dhcp, gateway_ip, ip_version,\nname and network_id parameters in the request body as well as the tenant_id\nparameter in the URI. The response returns the subnet parameter.\n\n\n\n98 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The allocation_pools parameter specifies the start and end addresses for\nthe allocation pools;\n\n\u2022 The cidr parameter containing the Classless Inter-Domain Routing (CIDR)\nnotation;\n\n\u2022 The enable_dhcp parameter is a Boolean that indicates if DHCP is enabled\n(when set to True), disabled (when set to False);\n\n\u2022 The gateway_ip parameter specifies the gateway IP address;\n\n\u2022 The ip_version parameter contains the IP version (IPv4 or IPv6);\n\n\u2022 The name parameter contains the subnet name.\n\n4.3.4.9 Update Subnet\n\nThe Update Subnet operation updates a specified subnet. This operation sends\nthe gateway_ip and name parameters in the request body as well as the ten-\nant_id and subnet_id parameters in the URI. The response returns the subnet\nparameter.\n\n4.3.4.10 Delete Subnet\n\nThe Delete Subnet operation removes a subnet from a OpenStack network. This\noperation does not require a request body and sends the tenant_id and subnet_id\nparameters in the URI. The success or failure of this operation is returned in the\nresponse header Status-Line parameter.\n\n4.3.4.11 List Ports\n\nThe List Ports operation lists the ports to which the tenant has access. This\noperation does not require a request body and sends the tenant_id parameter in\nthe URI. The response returns the ports parameter.\n\n\u2022 The ports parameter contains the status, name, admin_state_up, net-\nwork_id, tenant_id, binding:vif_type, device_owner, binding:capabilities,\nport_filter, mac_address, fixed_ips, id, device_id and security_groups at-\ntributes for each available port.\n\n4.3.4.12 Show Port\n\nThe Show Port operation Shows information on a specified port. This operation\ndoes not require a request body and sends the tenant_id and port_id parameters\nin the URI. The response returns the port parameter.\n\n\n\n4.3. OPENSTACK INTERFACE 99\n\n\u2022 The port parameter contains the status, name, admin_state_up, network_id,\ntenant_id, binding:vif_type, device_owner, binding:capabilities, port_filter,\nmac_address, fixed_ips, id, device_id and security_groups attributes for\nthe specified port.\n\n4.3.4.13 Create Port\n\nThe Create Port operation creates a port on a specified network. This opera-\ntion sends the status, name, admin_state_up, mac_address, fixed_ips, secur-\nity_groups and network_id parameters in the request body as well as the ten-\nant_id parameter in the URI. The response returns the port parameter.\n\n\u2022 The status parameter specifies the status of the port (UP or DOWN);\n\n\u2022 The admin_state_up parameter specifies the administrative state of the\nrouter;\n\n\u2022 The mac_address parameter indicates the MAC address of the port;\n\n\u2022 The fixed_ips parameter indicates the IP address and subnet ID of the\nfixed ports;\n\n\u2022 The security_groups parameter indicates the ID of any attached security\ngroup.\n\n4.3.4.14 Update Port\n\nThe Update Port operation updates a specified port. This operation sends the\nstatus, name, admin_state_up, mac_address, fixed_ips, security_groups and\nnetwork_id parameters in the request body as well as the tenant_id and port_id\nparameters in the URI. The response returns the port parameter.\n\n4.3.4.15 Delete Port\n\nThe Delete Port operation deletes a specified port. This operation does not\nrequire a request body and sends the tenant_id and port_id parameters in the\nURI. The success or failure of this operation is returned in the response header\nStatus-Line parameter.\n\nThe operations and parameters used by the Network Management component\nfrom OpenStack interface are resumed in Table 4.9.\n\n\n\n100 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.9: Network Management Table (Parameters vs Operations).\n\nOperations\nParameters L\n\nist\nN\netw\n\nork\ns\n\nS\nh\now\n\nN\netw\n\nork\n\nC\nreate\n\nN\netw\n\nork\n\nU\np\nd\nate\n\nN\netw\n\nork\n\nD\nelete\n\nN\netw\n\nork\n\nL\nist\n\nS\nu\nb\nn\nets\n\nS\nh\now\n\nS\nu\nb\nn\net\n\nC\nreate\n\nS\nu\nb\nn\net\n\nU\np\nd\nate\n\nS\nu\nb\nn\net\n\nD\nelete\n\nS\nu\nb\nn\net\n\nL\nist\n\nP\norts\n\nS\nh\now\n\nP\nort\n\nC\nreate\n\nP\nort\n\nU\np\nd\nate\n\nP\nort\n\nD\nelete\n\nP\nort\n\nSent Parameters\nsubnet_id x x x\ntenant-id x x x x x x x x x x x x x x x\nport_id x x x\nnetwork_id x x x x x x\nAdmin_state_up x x x x\nname x x x x x x\ncidr x\nip_version x\nmac_address x x\nfixed_ips x x\nallocation_pools x\nenable_dhcp x\nsecurity_groups x x\nstatus x x\nshared x x\ngateway_ip x x\n\nReceived Parameters\nnetworks x\nnetwork x x x\nsubnets x\nsubnet x x x\nports x\nport x x x\n\n4.3.5 Volume Management\n\nThe Volume Management component is used to provide the following operations\nfor managing volumes in OpenStack: (i) Create Volume; (ii) List Volumes; (iii)\nList Volume Details; (iv) Show Volume; (v) Update Volume; and (vi) Delete\nVolume.\n\n4.3.5.1 Create Volume\n\nThe Create Volume operation creates an OpenStack volume. This operation\nsends the name, description, size, volume_type, availability_zone and metadata\nparameters in the request body as well as the tenant_id parameter in the URI.\nThe response returns the id, links and name parameters.\n\n4.3.5.2 List Volumes\n\nThe List Volumes operation lists summary information for all Cinder volumes\naccessible to the tenant who submits the request. This operation sends the ten-\n\n\n\n4.3. OPENSTACK INTERFACE 101\n\nant_id parameter in the URI. The response returns the id, links and name para-\nmeters for each available volume.\n\n4.3.5.3 List Volume Details\n\nThe List Volume Details operation lists detailed information for all Cinder volumes\naccessible to the tenant who submits the request. This operation sends the ten-\nant_id parameter in the URI. The response returns the volumes parameter.\n\n\u2022 The volumes parameter contains the status, attachment, links, availab-\nility_zone, os-vol-host-attr:host, source_volid, snapshot_id, id, descrip-\ntion, name, created_at, volume_type, os-vol-tenant-attr:tenant_id, size\nand metadata parameters for each available volume.\n\n4.3.5.4 Show Volume\n\nThe Show Volume operation shows information about a specified volume. This\noperation sends the tenant_id and volume_id parameters in the URI. The re-\nsponse returns the volume parameter.\n\n\u2022 The volume parameter contains the status, attachment, links, availab-\nility_zone, os-vol-host-attr:host, source_volid, snapshot_id, id, descrip-\ntion, name, created_at, volume_type, os-vol-tenant-attr:tenant_id, size\nand metadata parameters for the specified volume.\n\n4.3.5.5 Update Volume\n\nThe Update Volume operation updates a specific volume. This operation sends\nthe name and description parameters in the request body as well as the ten-\nant_id and volume_id parameters in the URI. The response returns the volume\nparameter.\n\n4.3.5.6 Delete Volume\n\nThe Delete Volume operation deletes a specified volume from the pool. This\noperation sends the tenant_id and volume_id parameters in the URI. The suc-\ncess or failure of this operation is returned in the response header Status-Line\nparameter.\n\nThe operations and parameters used by the Volume Management component\nfrom OpenStack interface are resumed in Table 4.10.\n\n\n\n102 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.10: Volume Management Table (Parameters vs Operations).\n\nOperations\nParameters Create\n\nVolume\nList\n\nVolumes\nList\n\nVolume\nDetails\n\nShow\nVolume\n\nUpdate\nVolume\n\nDelete\nVolume\n\nSent Parameters\navailability_zone x\ndescription x x\ntenant_id x x x x x x\nvolume_id x x x\nvolume_type x\nmetadata x\nname x x\nsize x\n\nReceived Parameters\nname x x\nlinks x x\nid x x\nvolumes x\nvolume x x\n\n4.4 CloudStack Interface\n\nCloudStack uses a Query API to interface with the CloudStack system. This way,\nall CloudStack API requests are submitted by rewriting the URL of the HTTP\nGET/POST request messages with the required operation and parameters. A\nCloudStack API request presents the following syntax (HTTP/HTTPS):\nBase URL + API Path + Operation + Signature\n\n\u2022 Base URL + API Path - the CloudStack API URL,\ne.g., http://www.cloud.com:8080/client/api;\n\n\u2022 Operation - the Web services operation to be executed, e.g., start a virtual\nmachine or create a disk volume which is invoked after the question mark\ncharacter;\n\n\u2022 Signature - the additional required or optional operation parameters, e.g.,\nthe signature so that CloudStack can verify the caller has been authentic-\nated and authorized to invoke the operation.\n\nCloudStack supports both JSON and XML response formats. The default re-\nsponse is XML-formatted. To change the response format to JSON it is necessary\nto add &amp;response=json to the URL query string [140].\n\nThe current CloudStack interface libraries version 4.1.0 provide a variety of\noperation groups that are listed at [141]. The Virtual Machine, ISO, Template,\nVolume, Network and Firewall operation groups provide the user with function-\nalities common to the ones found in the other IaaS platform API under analysis.\n\nhttp://www.cloud.com:8080/client/api\n\n\n4.4. CLOUDSTACK INTERFACE 103\n\nIn this section, these groups of operation are aggregated as following: (i) Server\nManagement (containing the operations from the Virtual Machine group); (ii)\nTemplate Management; (iii) Image Management (containing the operations from\nthe ISO group); (iv) Network Management (containing the operations from Net-\nwork and Firewall groups); and (v) Volume Management.\n\n4.4.1 Server Management\n\nThe Server Management component is used to manage a specific virtual machine\nthrough the following list of operations: (i) Deploy Virtual Machine; (ii) Destroy\nVirtual Machine; (iii) Reboot Virtual Machine; (iv) Start Virtual Machine; (v)\nStop Virtual Machine; (vi) Reset Password For Virtual Machine; (vii) Reset\nSSH Key For Virtual Machine; (viii) Update Virtual Machine; (ix) List Virtual\nMachines; (x) Get VM Password; (xi) Restore Virtual Machine; (xii) Change\nService For Virtual Machine; (xiii) Add NIC To Virtual Machine; (xiv) Remove\nNIC From Virtual Machine; (xv) Update Default NIC For Virtual Machine.\n\nWith the exception of the Get VM Password operation, the other Server\nManagement operations return the same response that is composed by the id, ac-\ncount, cpunumber, cpuspeed, cpuused, created, displayname, domain, domainid,\nforvirtualnetwork, group, groupid, guestosid, haenable, hostid, hostname, hyper-\nvisor, instancename, isodisplaytext, isoid, isoname, keypair, memory, name, net-\nworkkbsread, networkkbswrite, password, passwordenabled, project, projectid,\npublicip, publicipid, rootdeviceid, rootdevicetype, serviceofferingid, serviceoffer-\ningname, state, templatedisplaytext, templateid, templatename, zoneid, zone-\nname, jobid, jobstatus, nic, securitygroup and tag parameters. This common\nresponse will be referred as standard response parameter to simplify the exten-\nded list of parameters.\n\n4.4.1.1 Deploy Virtual Machine\n\nThe Deploy Virtual Machine operation creates and automatically starts a virtual\nmachine based on a service offering, disk offering, and template. This operation\nsends the serviceofferingid, templateid and zoneid required parameters and op-\ntionally the account, diskofferingid, displayname, domainid, group, hypervisor,\nip6address, ipaddress, iptonetworklist, keyboard, keypair, name, networkids, pro-\njectid, securitygroupids, securitygroupnames, size, startvm and userdata para-\nmeters. The response returns the standard response parameter.\n\n\u2022 The serviceofferingid parameter specifies the ID of the service offering for\nthe virtual machine;\n\n\u2022 The templateid parameter specifies the ID of the template for the virtual\nmachine;\n\n\n\n104 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The zoneid parameter specifies the zone ID for the virtual machine;\n\n\u2022 The account parameter specifies an optional account for the virtual ma-\nchine;\n\n\u2022 The diskofferingid parameter specifies the ID of the disk offering for the\nvirtual machine;\n\n\u2022 The displayname parameter specifies an optional user generated name for\nthe virtual machine;\n\n\u2022 The domainid parameter specifies an optional domain ID for the virtual\nmachine;\n\n\u2022 The group parameter specifies an optional group for the virtual machine;\n\n\u2022 The hypervisor parameter specifies the hypervisor on which to deploy the\nvirtual machine;\n\n\u2022 The ip6address parameter specifies the IPv6 address for a default VM\u2019s\nnetwork;\n\n\u2022 The ipaddress parameter specifies the IPv4 address for a default VM\u2019s net-\nwork;\n\n\u2022 The iptonetworklist parameter specifies the IP to network mapping;\n\n\u2022 The keyboard parameter specifies an optional keyboard device type for\nthe virtual machine. Valid values can be one of: de,de-ch,es,fi,fr,fr-be,fr-\nch,is,it,jp,nl-be,no,pt,uk and us;\n\n\u2022 The keypair parameter specifies the name of the SSH key pair used to login\nto the virtual machine;\n\n\u2022 The name parameter specifies the host name for the virtual machine;\n\n\u2022 The networkids parameter specifies the list of network ID used by virtual\nmachine;\n\n\u2022 The projectid parameter specifies the project ID;\n\n\u2022 The securitygroupids parameter specifies a comma separated list of security\ngroups ID that are going to be applied to the virtual machine;\n\n\u2022 The securitygroupnames parameter specifies a comma separated list of se-\ncurity groups names that are going to be applied to the virtual machine;\n\n\u2022 The size parameter specifies the arbitrary size for the DATADISK volume;\n\n\n\n4.4. CLOUDSTACK INTERFACE 105\n\n\u2022 The startvm parameter specifies if thenetwork offering supports specifying\nIP ranges (if true, default value) or not (if false);\n\n\u2022 The userdata parameter specifies an optional binary data that can be sent\nto the virtual machine upon a successful deployment;\n\n4.4.1.2 Destroy Virtual Machine\n\nThe Destroy Virtual Machine operation destroys a virtual machine (once des-\ntroyed, only the administrator can recover it). This operation sends the id\u2019 para-\nmeter with the virtual machine identifier. The response returns the standard\nresponse parameter.\n\n4.4.1.3 Reboot Virtual Machine\n\nThe Reboot Virtual Machine operation reboots a virtual machine. This operation\nsends the id parameter with the virtual machine identifier. The response returns\nthe standard response parameter.\n\n4.4.1.4 Start Virtual Machine\n\nThe Start Virtual Machine operation starts a virtual machine. This operation\nsend the id parameter with the virtual machine identifier. The response returns\nthe standard response parameter.\n\n4.4.1.5 Stop Virtual Machine\n\nThe Stop Virtual Machine operation stops a virtual machine. This operation\nsends the id parameter with the virtual machine identifier as well as the optional\nforced parameter to force stop the virtual machine. The response returns the\nstandard response parameter.\n\n4.4.1.6 Reset Password For Virtual Machine\n\nThe Reset Password For Virtual Machine operation resets the password for a\nvirtual machine when the virtual machine is in a stopped state. This operation\nsends the id parameter with the virtual machine identifier. The response returns\nthe standard response parameter.\n\n4.4.1.7 Reset SSH Key For Virtual Machine\n\nThe Reset SSH Key For Virtual Machine operation resets the virtual machine\nSSH Key when the virtual machine is in a stopped state. This operation sends the\nid parameter with the virtual machine identifier, the keypar parameter with the\nname of the SSH key pair used to login into the virtual machine and, optionally,\n\n\n\n106 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nthe account, domainid and projectid parameters to associate an optional account,\ndomain or project to the SSH key. The response returns the standard response\nparameter.\n\n4.4.1.8 Update Virtual Machine\n\nThe Update Virtual Machine operation updates the properties of a virtual ma-\nchine. The virtual machine has to be stopped and restarted for the new properties\nto take effect. This operation sends the id parameter with the virtual machine\nidentifier and, optionally, the displayname with the user generated name, the\ngroup parameter to define the virtual machine group, the haenable parameter\nto enable or disable the high availability for the virtual machine, the ostypeid\nparameter that identifies the ID of the OS type that best represents the virtual\nmachine and the userdata parameter containing an optional binary data to be\nsent to the virtual machine upon a successful deployment. The response returns\nthe standard response parameter.\n\n4.4.1.9 List Virtual Machines\n\nThe List Virtual Machines operation enumerates the virtual machines associated\nwith an account. This operation sends the account, details, domainid, forvirtual-\nnetwork, groupid, hostid, hypervisor, id, isoid, isrecursive, keyword, listall, name,\nnetworkid, page, pagesize, podid, projectid, state, storageid, tags, templateid,\nvpcid and zoneid parameters. The response returns the standard response para-\nmeter.\n\n\u2022 The details parameter provides a comma separated list of host details re-\nquested. If no parameter is passed, the details will be defaulted to all;\n\n\u2022 The forvirtualnetwork parameter lists the resources by network type;\n\n\u2022 The hosted parameter lists the host identifier;\n\n\u2022 The id parameter lists the identifier of the virtual machine;\n\n\u2022 The isoid parameter lists the virtual machines by ISO;\n\n\u2022 The isrecursive parameter lists all resources from the parent specified by\nthe domainId parameter;\n\n\u2022 The keyword parameter lists the resources by keyword;\n\n\u2022 The listall parameter lists only the resources belonging to the caller (when\nset to false) or lists the resources that the caller is authorized to see (when\nthe parameter\u2019s value is true). The default value is false;\n\n\n\n4.4. CLOUDSTACK INTERFACE 107\n\n\u2022 The networkid parameter lists by network ID;\n\n\u2022 The page parameter sets the number of pages;\n\n\u2022 The pagesize parameter sets the size of the page;\n\n\u2022 The podid parameter lists the pod ID;\n\n\u2022 The state parameter lists the state of the virtual machine;\n\n\u2022 The storageid parameter lists the storage identifier holding the virtual ma-\nchine\u2019s volumes;\n\n\u2022 The tags parameter lists resources by tags (key/value pairs);\n\n\u2022 The templateid parameter lists virtual machines by template;\n\n\u2022 Tee vpcid parameter lists virtual machines by virtual private cloud;\n\n4.4.1.10 Get VM Password\n\nThe Get VM Password operation returns an encrypted password for the virtual\nmachine. This operation sends the id parameter with the virtual machine identi-\nfier and returns the encryptedpassword parameter with the encrypted password\nof the virtual machine.\n\n4.4.1.11 Restore Virtual Machine\n\nThe Restore Virtual Machine operation restores a virtual machine to the ori-\nginal template or specific snapshot. This operation sends the virtualmachineid\nparameter. The response returns the standard response parameter.\n\n4.4.1.12 Change Service For Virtual Machine\n\nThe Change Service For Virtual Machine operation changes the service offering\nfor a virtual machine when the virtual machine is in a stopped state. This\noperation sends the id and the serviceofferingid parameters. The response returns\nthe standard response parameter.\n\n4.4.1.13 Add NIC To Virtual Machine\n\nThe Add NIC To Virtual Machine operation adds a virtual machine to the spe-\ncified network by creating a NIC. This operation sends the networkid, virtualma-\nchineid and, optionally, ipaddress parameters. The response returns the standard\nresponse parameter.\n\n\n\n108 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.4.1.14 Remove NIC From Virtual Machine\n\nThe Remove NIC From Virtual Machine operation removes the virtual machine\nfrom a specified network by deleting a NIC. This operation sends the nicid and\nthe virtualmachineid parameters. The response returns the standard response\nparameter.\n\n4.4.1.15 Update Default NIC For Virtual Machine\n\nThe Update Default NIC For Virtual Machine operation changes the default NIC\non a virtual machine. This operation sends the nicid and the virtualmachineid\nparameters. The response returns the standard response parameter.\n\nThe operations and parameters used by the Server Management component\nfrom CloudStack interface are resumed in Table 4.11.\n\n4.4.2 Template Management\n\nThe Template Management component of CloudStack enables the management of\ntemplates using the following list of operations: (i) Create Template; (ii) Update\nTemplate; (iii) Copy Template; (iv) Delete Template; (v) List Templates; (vi)\nUpdate Template Permissions; (vii) List Template Permissions; and (viii) Extract\nTemplate.\n\nThe Update Template, Copy Template and List Templates return the same id,\naccount, accountid, bootable, checksum, created, crossZones, details, displaytext,\ndomain, domainid, format, hostid, hostname, hypervisor, isextractable, isfea-\ntured, ispublic, isready, name, ostypeid, ostypename, passwordenabled, project,\nprojectid, removed, size, sourcetemplateid, sshkeyenabled, status, templatetag,\ntemplatetype, zoneid, zonename, tags, jobid and jobstatus response parameters.\nThis common response will be referred as standard response parameter to simplify\nthe extended list of parameters.\n\n4.4.2.1 Create Template\n\nThe Create Template operation creates a template of a virtual machine when\nthe VM is in a stopped state. This operation sends the required displaytext,\nname and ostypeid parameters as well as the optional bits, details, isfeatured,\nispublic, passwordenabled, requireshvm, snapshotid, templatetag, url, virtual-\nmachineid and volumeid parameters. The response to this operation returns the\ncreatetemplaterespose parameter.\n\n\u2022 The displaytext parameter displays the text of the template;\n\n\u2022 The name parameter indicates the name of the template;\n\n\n\n4.4. CLOUDSTACK INTERFACE 109\n\nTable 4.11: Server Management Table (Parameters vs Operations).\n\nOperations\nParameters D\n\nep\nloy\n\nV\nM\n\nD\nestroy\n\nV\nM\n\nR\neb\n\no\no\nt\nV\nM\n\nS\nta\nrt\n\nV\nM\n\nS\nto\np\nV\nM\n\nR\neset\n\nV\nM\n\np\na\nssw\n\no\nrd\n\nR\neset\n\nV\nM\n\nS\nS\nH\n\nK\ney\n\nU\np\nd\na\nte\n\nV\nM\n\nL\nist\n\nV\nM\n\nG\net\n\nV\nM\n\nP\na\nssw\n\no\nrd\n\nR\nesto\n\nre\nV\nM\n\nC\nh\na\nn\ng\ne\nV\nM\n\nS\nerv\n\nice\n\nA\nd\nd\nV\nM\n\nN\nIC\n\nR\nem\n\nov\ne\nV\nM\n\nN\nIC\n\nU\np\nd\na\nte\n\nV\nM\n\nN\nIC\n\nSent Parameters\naccount x x x\ndetails x\ndiskoffering x\ndisplayname x x\ndomainid x x x\nforvirtualnetwork x\nforced x\ngroup x x\ngroupid x\nhaenable x\nhostid x x\nhypervisor x x\nid x x x x x x x x x x\nipaddress x x\nip6address x\niptonetworklist x\nisoid x\nisrecursive x\nkeyboard x\nkeyword x\nkeypair x x\nlistall x\nname x x\nnetworkids x x x\nnicid x x\nostypeid x\npage x\npagesize x\npodid x\nprojectid x x x\nsecuritygroupids x\nsecuritygroupnames x\nstate x\nsize x\nstartvm x\nstorageid x\nserviceofferingid x x\ntags x\ntempalteid x x\nuserdata x x\nvpcid x\nvirtualmachineid x x x x\nzoneid x x\n\nReceived Parameters\nencriptedpassword x\nstandard response x x x x x x x x x x x x x x\n\n\u2022 The ostypeid parameter indicates the ID of the OS type that best represents\nthe OS of the template;\n\n\u2022 The bits parameter indicates if the OS architecture is 32 bit or 64 bit;\n\n\u2022 The details parameter indicates the template details in key/value pairs;\n\n\n\n110 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The isfeatured parameter indicates if the template is a featured template\n(true) or not (false);\n\n\u2022 The ispublic parameter indicates if the template is a public (true) or not\n(false);\n\n\u2022 The passwordenabled parameter indicates if the template supports the pass-\nword reset feature (true) or not (false). By default the value is false;\n\n\u2022 The requireshvm parameter indicates if the template requires a Hardware\nVM (true) or not (false);\n\n\u2022 The snapshotid parameter identifies the ID of the snapshot the template is\nbeing created from;\n\n\u2022 The templatetag parameter indicates the tag for the template;\n\n\u2022 The url parameter is only used for baremetal hypervisors and indicates the\nCommon Internet File System (CIFS) server directory where the template\nis stored;\n\n\u2022 The virtualmachineid parameter indicates the VM ID for a baremetal VM;\n\n\u2022 The volumeid parameter indicates the ID of the disk volume the template\nis being created from;\n\n\u2022 The createtemplaterespose parameter is composed by the id, clusterid,\nclustername, created, disksizeallocated, disksizetotal, disksizeused, ipad-\ndress, name, path, podid, podname, state, tags, type, zoneid, zonename,\njobid and jobstatus attrbutes.\n\n4.4.2.2 Update Template\n\nThe Update Template operation updates the attributes of a template. This opera-\ntion sends the required id parameter as well as the optional bootable, displaytext,\nformat, name, ostype, passwordenabled and sortkey parameters. The response\nto this operation returns the standard response parameter.\n\n\u2022 The id parameter indicates the ID of the image file;\n\n\u2022 The bootable parameter indicates if the image is bootable (true) or not\n(false);\n\n\u2022 The format parameter indicates the image format;\n\n\u2022 The name parameter indicates the image file name;\n\n\u2022 The sortkey parameter indicates the sort key of the template.\n\n\n\n4.4. CLOUDSTACK INTERFACE 111\n\n4.4.2.3 Copy Template\n\nThe Copy Template operation copies a template from one zone to another. This\noperation sends the id, destzoneid and sourcezoneid parameters. The response\nto this operation returns the standard response parameter.\n\n\u2022 The destzoneid parameter indicates the ID of the zone the template is being\ncopied to;\n\n\u2022 The sourcezoneid parameter indicates the ID of the zone the template is\ncurrently hosted on.\n\n4.4.2.4 Delete Template\n\nThe Delete Template operation deletes a template from the system. All virtual\nmachines using the deleted template will not be affected. This operation sends\nthe required id parameter as well as the optional zoneid parameter. The response\nto this operation returns the deletetemplaterespose parameter.\n\n\u2022 The zoneid parameter indicates the template zone ID;\n\n\u2022 The deletetemplateresponse parameter contaisn the displaytext and success\nattributes;\n\n4.4.2.5 List Templates\n\nThe List Templates operation list all public, private and privileged templates.\nThis operation sends the required templatefilter parameter as well as the op-\ntional account, domainid, hypervisor, id, isrecursive, keyword, llistall, name,\npage, pagesize, projectid, tags and zoneid. The response to this operation re-\nturns the standard response parameter.\n\n\u2022 The templatefilter parameter indicates the filter values;\n\n\u2022 The account parameter lists resources by account;\n\n\u2022 The domainid parameter lists only the resources belonging to the specified\ndomain;\n\n\u2022 The hypervisor parameter identifies the hypervisor under search;\n\n\u2022 The keyword parameter lists the resources by keyword;\n\n\u2022 The listall parameter lists only resources belonging to the caller (if set to\nfalse) or list resources that the caller is authorized to see (if set to true).\nDefault value is false;\n\n\n\n112 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The projectid parameter lists objects by project;\n\n\u2022 The tags parameter lists resources by tags (key/value pairs).\n\n4.4.2.6 Update Template Permissions\n\nThe Update Template Permissions updates a template visibility permissions. A\npublic template is visible to all accounts within the same domain. A private\ntemplate is visible only to the owner of the template. A privileged template\nis a private template with account permissions added. This operation sends\nthe required id parameter and the optional acounts, isextractable, isfeatured,\nispublic, op and projectids parameters. The response to this operation returns\nthe updatetemplatepermresponse parameter.\n\n\u2022 The accounts parameter holds a comma delimited list of accounts;\n\n\u2022 The isextractable parameter indicates if the template/ISO is extractable (if\ntrue) or not (if false);\n\n\u2022 The isfeatured parameter indicates if is a featured template/ISO (if true)\nor not (if false);\n\n\u2022 The ispublic parameter indicates if the template/ISO is public (if true) or\nprivate (if false);\n\n\u2022 The op parameter represents the permission operator (add, remove, reset);\n\n\u2022 The projectids parameter holds a comma delimited list of projects;\n\n\u2022 The updatetemplatepermresponse parameter contains the displaytext and\nsuccess attributes.\n\n4.4.2.7 List Template Permissions\n\nThe List Template Permissions operation lists the template visibility and all\naccounts that have permissions to view the template. This operation sends the\nid parameter and returns the listtemplatepermresponse parameter.\n\n\u2022 The listtemplatepermresponse parameter contains the id, account, domainid,\nispublic and projectids attributes.\n\n\n\n4.4. CLOUDSTACK INTERFACE 113\n\n4.4.2.8 Extract Template\n\nThe Extract Template operation extracts a template. This operation sends the re-\nquired id and mode parameters as well as the optional url and zoneid parameters.\nThe response to this operation returns the extracttemplateresponse parameter.\n\n\u2022 The extracttemplateresponse parameter contains the id, accountid, created,\nextractId, extractMode, name, state, status, storagetype, uploadpercent-\nage, url, zoneid and zonename attributes.\n\nThe operations and parameters used by the Template Management compon-\nent from CloudStack interface are resumed in Table 4.12.\n\n4.4.3 Image Management\n\nCloudStack supports the Image image format for defining guest virtual machines.\nThe Image Management component manages ISO images through the following\noperations: (i) Attach ISO; (ii) Detach ISO; (iii) List ISO; (iv) Update ISO;\n(v) Delete ISO; (vi) Copy ISO; (vii) Update ISO Permissions; (viii) List ISO\nPermissions; and (ix) Extract ISO.\n\nThe List ISO, Update ISO and Copy ISO operations return the same id, ac-\ncount, accountid, bootable, checksum, created, crossZones, details, displaytext,\ndomain, domainid, format, hostid, hostname, hypervisor, isdynamicallyscalable,\nisextractable, isfeatured, ispublic, isready, name, ostypeid, ostypename, pass-\nwordenabled, project, projectid, removed, size, sourcetemplateid, sshkeyenabled,\nstatus, templatetag, templatetype, zoneid, zonename, tags, jobid and jobstatus\nresponse parameters. This common response will be referred as standard response\nparameter to simplify the extended list of parameters.\n\n4.4.3.1 Attach ISO\n\nThe Attach ISO operation attaches an ISO image to a virtual machine. This op-\neration sends the id and virtualmachineid parameters and returns the attachisore-\nsponse parameter.\n\n\u2022 The attachissoresponse parameter contains the id, account, cpunumber,\ncpuspeed, cpuused, created, diskioread, diskiowrite, diskkbsread,\ndiskkbswrite, displayname, displayvm, domain, domainid, forvirtualnet-\nwork, group, groupid, guestosid, haenable, hostid, hostname, hypervisor, in-\nstancename, isdynamicallyscalable, isodisplaytext, isoid, isoname, keypair,\nmemory, name, networkkbsread, networkkbswrite, password, passworde-\nnabled, project, projectid, publicip, publicipid, rootdeviceid, rootdevice-\ntype, serviceofferingid, serviceofferingname, servicestate, state, templatedis-\n\n\n\n114 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.12: Template Management Table (Parameters vs Operations).\n\nOperations\nParameters C\nreate\n\nT\nem\n\np\nlate\n\nU\np\nd\nate\n\nT\nem\n\np\nlate\n\nC\nop\n\ny\nT\nem\n\np\nlate\n\nD\nelete\n\nT\nem\n\np\nlate\n\nL\nist\n\nT\nem\n\np\nlates\n\nU\np\nd\nate\n\nT\nem\n\np\nlate\n\nP\nerm\n\nission\ns\n\nL\nist\n\nT\nem\n\np\nlate\n\nP\nerm\n\nission\ns\n\nE\nx\ntract\n\nT\nem\n\np\nlate\n\nSent Parameters\naccount x x\nbits x\nbootable x\ndetails x\ndestzoneid x\ndisplaytext x x\ndomainid x\nformat x\nhypervisor x\nid x x x x x x x\nisexecutable x\nisfeatured x x\nispublic x x\nisrecursive x\nkeyword x\nkeypair\nlistall x\nmode x\nname x x x\nop x\nostypeid x x\npage x\npagesize x\npasswordenable x x\nprojectid x x\nrequireshvm x\nsortkey x\nsourcezoneid x\nsnapshotid x\ntags x\ntempaltetag x\ntemplatefilter x\nurl x x\nvirtualmachineid x\nvolumeid x\nzoneid x x x\n\nReceived Parameters\ncreatetemplateresponse x\ndeletetemplateresponse x\nupdatetemplatepermresponse x\nlisttemplatepermresponse x\nextracttemplateresponse x\nstandard response x x x\n\n\n\n4.4. CLOUDSTACK INTERFACE 115\n\nplaytext, templateid, templatename, zoneid, zonename, affinitygroup, nic,\nsecuritygroup, tags, jobid and jobstatus attributes.\n\n4.4.3.2 Detach ISO\n\nThe Detach ISO operation detaches any ISO file currently attached to a virtual\nmachine. This operation sends the virtualmachineid parameter and returns the\ndetachisoresponse parameter.\n\n\u2022 The detachisorespose parameter contains the same list of attributes as the\nattachisoresponse parameter.\n\n4.4.3.3 List ISO\n\nThe List ISO operation lists all available ISO files. This operation sends the\noptional account, bootable, domainid, hypervisor, id, isofilter, ispublic, isready,\nisrecursive, keyword, listall, name, page, pagesize, projectid, tags and zoneid\nparameters and returns the standard response parameter.\n\n4.4.3.4 Update ISO\n\nThe Update ISO operation updates an ISO file. This operation sends the required\nid parameter and the optional bootable, displaytext, format, isdynamicallyscal-\nable, isrouting, name, ostype, passwordenabled and sortkey parameters. The\nresponse returns the standard response parameter.\n\n\u2022 The bootable parameter indicates if the image is bootable (if true) or not\n(if false);\n\n\u2022 The isdynamicallyscalable is a boolean parameter that indicates if the ISO\nimage contains Xen Server or VMWare tools in order to support dynamic\nscaling of VM CPU or memory (true) or not (false);\n\n\u2022 The isrouting parameter indicates if the template type is routing i.e., if the\ntemplate is used to deploy a router.\n\n4.4.3.5 Delete ISO\n\nThe Delete ISO operation deletes an ISO file. This operation sends the required\nid parameter and the optional zoneid parameter and returns the deleteisoresponse\nparameter.\n\n\u2022 The deleteisoresponse parameter contains the displaytext and success at-\ntributes.\n\n\n\n116 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.4.3.6 Copy ISO\n\nThe Copy ISO operation copies an ISO from one zone to another. This operation\nsends the id, destzoneid and sourcezoneid parameters and returns the standard\nresponse parameter.\n\n4.4.3.7 Update ISO Permissions\n\nThe Update ISO Permissions updates the ISO file permissions. This operation\nsends the required id parameter and the optional accounts, isextractable, isfea-\ntured, ispublic, op and projectids parameters. The response message return the\nupdateissopermresponse parameter.\n\n\u2022 The updateisopermresponse parameter contains the displaytext and success\nattributes.\n\n4.4.3.8 List ISO Permissions\n\nThe List ISO Permissions lists the visibility and all accounts that have permissions\nto view the referred ISO image. This operation sends the id parameter and returns\nthe listisopermresponse parameter.\n\n\u2022 The listisopermresponse parameter contains the id, account, domainid, is-\npublic and projectids attributes.\n\n4.4.3.9 Extract ISO\n\nThe Extract ISO operation extracts an ISO image. This operation sends the\nrequired id and mode parameters and the optional url and zoneid parameters.\nThe response message returns the extractisoresponse parameter.\n\n\u2022 The extractisoresponse parameter ontains the id, accountid, created, ex-\ntractId, extractMode, name, state, status, storagetype, uploadpercentage,\nurl, zoneid and zonename attributes.\n\nThe operations and parameters used by the Image Management component\nfrom CloudStack interface are resumed in Table 4.13.\n\n\n\n4.4. CLOUDSTACK INTERFACE 117\n\nTable 4.13: Image Management Table (Parameters vs Operations).\n\nOperations\nParameters A\n\ntta\nch\n\nIS\nO\n\nD\neta\n\nch\nIS\nO\n\nL\nist\n\nIS\nO\n\nU\np\nd\na\nte\n\nIS\nO\n\nD\nelete\n\nIS\nO\n\nC\no\np\ny\nIS\nO\n\nU\np\nd\na\nte\n\nIS\nO\n\nP\nerm\n\nissio\nn\ns\n\nL\nist\n\nIS\nO\n\nP\nerm\n\nissio\nn\ns\n\nE\nx\ntra\n\nct\nIS\nO\n\nSent Parameters\naccount x x\nbootable x x\ndestzoneid x\ndisplaytext x\ndomainid x\nformat x\nhypervisor x\nid x x x x x x x x\nisextractable x\nisfeatured x\nispublic x x\nisready x\nisrecursive x\nisdinamicallyscalable x\nisofilter x\nisrouting x\nkeyword x\nlistall x\nmode x\nname x x\nop x\nostypeid x\npage x\npagesize x\npasswordenable x\nprojectid x x\nsortkey x\nsourcezoneid x\ntags x\nurl x\nvirtualmachineid x x\nzoneid x x x\n\nReceived Parameters\nattachisoresponse x\ndetachisoresponse x\nstandard response x x x\ndeleteisoresponse x\nupdateisopermresponse x\nlistisopermresponse x\nextractisoresponse x\n\n4.4.4 Network Management\n\nThe Network Management component is used to manage port forwarding and\nfirewall rules through the following list of operations: (i) List Port Forwarding\nRules; (ii) Create Port Forwarding Rule; (iii) Delete Port Forwarding Rule; (iv)\nUpdate Port Forwarding Rule; (v) Create Firewall Rule; (vi) Delete Firewall\nRule; (vii) List Firewall Rules; (viii) Create Egress Firewall Rule; (ix) Delete\nEgress Firewall Rule; (x) List Egress Firewall Rules; (xi) Create Network; (xii)\n\n\n\n118 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nDelete Network; (xiii) List Networks; (xiv) Restart Network; and (xv) Update\nNetwork;\n\nThe List Port Forwarding Rules, Create Port Forwarding Rule, Update Port\nForwarding Rule, Create Firewall Rule, List Firewall Rules, Create Egress Fire-\nwall Rule and List Egress Firewall Rules return the same id, cidrlist, ipad-\ndress, ipaddressid, privateendport, privateport, protocol, publicendport, public-\nport, state, virtualmachinedisplayname, virtualmachineid, virtualmachinename\nand tags response parameters. This common response will be referred as firewall\nresponse parameter to simplify the extended list of parameters.\n\nThe Create Network, List Networks and Update Network return the same id,\naccount, acltype, broadcastdomaintype, broadcasturi, canusefordeploy, cidr, dis-\nplaytext, dns1, dns2, domain, domainid, gateway, ip6cidr, ip6gateway, isdefault,\nispersistent, issystem, name, netmask, networkdomain, networkofferingavailabil-\nity, networkofferingdisplaytext, networkofferingid, networkofferingname, physic-\nalnetworkid, project, projectid, related, restartrequired, specifyipranges, state,\nsubdomainaccess, traffictype, type, vlan, vpcid, zoneid, zonename, service and\ntags response parameters. This common response will be referred as network\nresponse parameter to simplify the extended list of parameters.\n\n4.4.4.1 List Port Forwarding Rules\n\nThe List Port Forwarding Rules operation lists all port forwarding rules for an IP\naddress. This operation sends the account, domainid, id, ipaddressid, isrecursive,\nkeyword, listall, page, pagesize, projectid and tags parameters and returns the\nfirewall response parameter.\n\n4.4.4.2 Create Port Forwarding Rule\n\nThe Create Port Forwarding Rule operation creates a port forwarding rule. This\noperation sends the required ipaddressid, privateport, protocol, publicport and\nvirtualmachineidparameters as well as the optional cidrlist, networkid, openfire-\nwall, privateendport and publicendport parameters. The response to this opera-\ntion returns the firewall response parameter.\n\n4.4.4.3 Delete Port Forwarding Rule\n\nThe Delete Port Forwarding Rule operation deletes a port forwarding rule. This\noperation sends the id parameter with the identifier of the port forwarding rule\nand returns the displaytext and success parameters.\n\n\n\n4.4. CLOUDSTACK INTERFACE 119\n\n4.4.4.4 Update Port Forwarding Rule\n\nThe Update Port Forwarding Rule operation updates the private port and the\nvirtual machine information of a port forwarding rule. This operation sends the\nrequired ipaddressid, privateport, protocol and publicport parameters as well as\nthe optional privateip and virtualmachineid parameters. The response to this\noperation returns the the firewall response parameter.\n\n4.4.4.5 Create Firewall Rule\n\nThe Create Firewall Rule operation creates a firewall rule for a given IP address.\nThis operation sends the required ipaddressid and protocol parameters as well as\nthe optional cidrlist, endport, icmpcode, icmptype, startport and type paramet-\ners. The response to this operation returns the firewall response parameter.\n\n4.4.4.6 Delete Firewall Rule\n\nThe Delete Firewall Rule operation deletes a firewall rule. This operation sends\nthe id parameter with the identifier of the firewall rule and returns the displaytext\nand success parameters.\n\n4.4.4.7 List Firewall Rules\n\nThe List Firewall Rules operation lists all firewall rules for an IP address. This op-\neration sends the optional account, domainid, id, ipaddressid, isrecursive, keyword,\nlistall, page, pagesize, projectid and tags parameters and returns the firewall re-\nsponse parameter.\n\n4.4.4.8 Create Egress Firewall Rule\n\nThe Create Egress Firewall Rule operation creates a egress firewall rule for a\ngiven network. This operation sends the required networkid and protocol para-\nmeters as well as the optional cidrlist, endport, icmpcode, icmptype, startport\nand type parameters. The response to this operation returns the firewall response\nparameter.\n\n4.4.4.9 Delete Egress Firewall Rule\n\nThe Delete Egress Firewall Rule operation deletes an egress firewall rule. This\noperation sends the id parameter with the identifier of the firewall rule and returns\nthe displaytext and success parameters.\n\n\n\n120 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.4.4.10 List Egress Firewall Rules\n\nThe List Egress Firewall Rules operation lists all egress firewall rules for a network\nidentifier. This operation sends the optional account, domainid, id, ipaddressid,\nisrecursive, keyword, listall, networkid, page, pagesize, projectid and tags para-\nmeters and returns the firewall response parameter.\n\n4.4.4.11 Create Network\n\nThe Create Network operation creates a CloudStack network. This operation\nsends the required displaytext, name, networkofferingid, zoneid parameters and\nthe optional account, acltype, domainid, endip, endipv6, gateway, ip6cidr,\nip6gateway, netmask, networkdomain, physicalnetworkid, projectid, startip, star-\ntipv6, subdomainaccess, vlan and vpcid parameters. The response returns the\nnetwork response parameter.\n\n\u2022 The displaytext parameter specifies the display text of the network;\n\n\u2022 The networkofferingid parameter specifies the network offering ID;\n\n\u2022 The account parameter specifies the account who will own the network;\n\n\u2022 The acltype parameter specifies the access control type;\n\n\u2022 The endip parameter specifies the ending IP address in the network IP\nrange. If not specified, will be defaulted to startIP;\n\n\u2022 The endipv6 parameter specifies the ending IPv6 address in the IPv6 net-\nwork range;\n\n\u2022 The gateway parameter specifies the gateway of the network;\n\n\u2022 The ip6cidr parameter specifies the CIDR of IPv6 network (must be at least\n/64);\n\n\u2022 The ip6gateway parameter specifies the gateway of the IPv6 network;\n\n\u2022 The netmask parameter specifies the netmask of the network;\n\n\u2022 The physicalnetworkid parameter specifies the network Physical Network\nID;\n\n\u2022 The startip parameter specifies the initial IP address in the network IP\nrange;\n\n\u2022 The startipv6 parameter specifies the initial IPv6 address in the IPv6 net-\nwork range;\n\n\n\n4.4. CLOUDSTACK INTERFACE 121\n\n\u2022 The subdomainaccess parameter defines if subdomains can use their parent\ndedicated networks domain(s);\n\n\u2022 The vlan parameter specifies the ID or VID of the network;\n\n\u2022 The vpcid parameter specifies the ID of the Virtual Private Cloud (VPC))\nthe network belongs to.\n\n4.4.4.12 Delete Network\n\nThe Delete Network operation deletes a CloudStack network. This operation\nsends the id parameter of the network and returns the displaytext and success\nparameters.\n\n4.4.4.13 List Networks\n\nThe List Networks operation lists all available networks. This operation sends\nthe account, acltype, canusefordeploy, domainid, forvpc, id, isrecursive, issystem,\nkeyword, listall, page, pagesize, physicalnetworkid, projectid, restartrequired,\nspecifyipranges, supportedservices, tags, traffictype, type, vpcid and zoneid para-\nmeters. The response returns the network response parameter.\n\n\u2022 The canusefordeploy parameter lists the networks available for VM deploy-\nment;\n\n\u2022 The domainid parameter lists the resources belonging to the specified do-\nmain;\n\n\u2022 The forvpc parameter specifies the network as belonging to a VPC;\n\n\u2022 The physicalnetworkid parameter lists networks by physical network ID;\n\n\u2022 The restartrequired parameter lists networks by restartRequired;\n\n\u2022 The specifyipranges parameter indicates whether to show (true) or not\n(false) the IP ranges of the networks;\n\n\u2022 The supportedservices parameter lists the networks supporting certain ser-\nvices;\n\n\u2022 The traffictype parameter indicates the type of traffic;\n\n\u2022 The type parameter indicates the type of the network (Isolated or Shared);\n\n\u2022 The vpcid parameter lists networks by VPC.\n\n\n\n122 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.4.4.14 Restart Network\n\nThe Restart Network operation restarts a CloudStack network. This operation\nsends the id and cleanup parameters and returns the restartnetworkresponse para-\nmeters.\n\n\u2022 The restartnetworkparameter contains the id, account, allocated, associ-\natednetworkid, associatednetworkname, domain, domainid, forvirtualnet-\nwork, ipaddress, issourcenat, isstaticnat, issystem, networkid, physicalnet-\nworkid, project, projectid, purpose, state, virtualmachinedisplayname, vir-\ntualmachineid, virtualmachinename, vlanid, vlanname, vpcid, zoneid, zon-\nename, tags, jobid and jobstatus attributes.\n\n4.4.4.15 Update Network\n\nThe Update Network operation updates a CloudStack network. This operation\nsends the network id parameter and the optional changecidr, displaytext, name,\nnetworkdomain and networkofferingid parameters. The response returns the net-\nwork response parameter.\n\n\u2022 The changecidr parameter forces the update even if the CIDR type is dif-\nferent;\n\n\u2022 The displaytext parameter specifies the new display text for the network;\n\n\u2022 The name parameter specifies the new name for the network;\n\n\u2022 The networkdomain parameter identifies the network domain;\n\n\u2022 The networkofferingid identifies the network offering ID.\n\nThe operations and parameters used by the Network Management component\nfrom CloudStack interface are resumed in Table 4.14.\n\n\n\n4.4. CLOUDSTACK INTERFACE 123\n\nTable 4.14: Network Management Table (Parameters vs Operations).\n\nOperations\nParameters L\n\nist\nP\no\nrt\n\nF\no\nrw\n\na\nrd\nin\ng\nR\nu\nles\n\nC\nrea\n\nte\nP\no\nrt\n\nF\no\nrw\n\na\nrd\nin\ng\nR\nu\nle\n\nD\nelete\n\nP\no\nrt\n\nF\no\nrw\n\na\nrd\nin\ng\nR\nu\nle\n\nU\np\nd\na\nte\n\nP\no\nrt\n\nF\no\nrw\n\na\nrd\nin\ng\nR\nu\nle\n\nC\nrea\n\nte\nF\nirew\n\na\nll\nR\nu\nle\n\nD\nelete\n\nF\nirew\n\na\nll\nR\nu\nle\n\nL\nist\n\nF\nirew\n\na\nll\nR\nu\nles\n\nC\nrea\n\nte\nF\nirew\n\na\nll\nR\nu\nle\n\nD\nelete\n\nE\ng\nress\n\nF\nirew\n\na\nll\nR\nu\nle\n\nL\nist\n\nE\ng\nress\n\nF\nirew\n\na\nll\nR\nu\nles\n\nC\nrea\n\nte\nN\netw\n\no\nrk\n\nD\nelete\n\nN\netw\n\no\nrk\n\nL\nist\n\nN\netw\n\no\nrk\ns\n\nR\nesta\n\nrt\nN\netw\n\no\nrk\n\nU\np\nd\na\nte\n\nN\netw\n\no\nrk\n\nSent Parameters\naccount x x x x x\nacltype x x\ncanusefordeploy x\nchangecidr x\ncidrlist x x x\ncleanup x\ndisplaytext x x\ndomainid x x x x x\nendip x\nendipv6 x\nendport x x\nforvpc x\ngateway x\nicmpcode x x\nicmptype x x\nid x x x x x x x x x x x\nipaddressid x x x x x x\nipv6cidr x\nipv6gateway x\nisrecursive x x x x\nissystem x\nkeyword x x x x\nlistall x x x x\nname x x\nnetmask x\nnetworkid x x x\nnetworkdomain x x\nnetworkofferingid x x\nopenfirewall x\npage x x x x\npagesize x x x x\nphysicalnetworkid x x\nprivateendport x x\nprivateip x\nprivateport x x x\nprojectid x x x x x\nprotocol x x x x\npublicendport x x\npublicport x x x\nrestartrequired x\nspecifyipranges x\nstartip x\nstartipv6 x\nstartport x x\nstate x\nsubdomainaccess x\nsupportedservices x\ntags x x x x x\ntraffictype x\ntype x x x\nvirtualmachinedisplayname x\nvirtualmachineid x x\nvirtualmachinename x\nvlan x\nvpcid x x\nzoneid x\n\nReceived Parameters\nfirewall response x x x x x x x\nnetwork response x x x\nrestartnetworkresponse x\ndisplaytext x x x x\nsuccess x x x x\n\n\n\n124 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.4.5 Volume Management\n\nThe Volume Management component is used to manage CloudStack volumes\nthrough the following list of operations: (i) Attach Volume; (ii) Upload Volume;\n(iii) Detach Volume; (iv) Create Volume; (v) Delete Volume; (vi) List Volumes;\n(vii) Extract Volume; (viii) Migrate Volume; and (ix) Resize Volume.\n\nWith the exception of the Delete Volume and the Extract Volume operations,\nthe other volume Management operations return the same response that is com-\nposed by the id, account, attached, created, destroyed, deviceid, diskofferingdis-\nplaytext, diskofferingid, diskofferingname, domain, domainid, hypervisor, isex-\ntractable, name, project, projectid, serviceofferingdisplaytext, serviceofferingid,\nserviceofferingname, size, snapshotid, state, status, storage, storagetype, type,\nvirtualmachineid, vmdisplayname, vmname, vmstate, zoneid, zonename, tags,\njobid and jobstatus parameters. This common response will be referred as stand-\nard response parameter to simplify the extended list of parameters.\n\n4.4.5.1 Attach Volume\n\nThe Attach Volume operation attaches a disk volume to a CloudStack virtual\nmachine. This operation sends the id and virtualmachineid parameters as well\nas the optional deviceid parameter. The response returns the standard response\nparameter.\n\n4.4.5.2 Upload Volume\n\nThe Upload Volume operation uploads a data disk. This operation sends the\nformat, name, url, zoneid parameters as well as the optional account, checksum\nand domainid parameters. The response returns the standard response para-\nmeter.\n\n4.4.5.3 Detach Volume\n\nThe Detach Volume operation detaches a disk volume from a CloudStack virtual\nmachine. This operation sends the deviceid, id and virtualmachineid parameters.\nThe response returns the standard response parameter.\n\n4.4.5.4 Create Volume\n\nThe Create Volume operation creates a disk volume from a disk offering for\nfurther attachment. This operation sends the name parameter as well as the\noptional account, diskofferingid, domainid, projectid, size, snapshotid and zoneid\nparameters. The response returns the standard response parameter.\n\n\n\n4.4. CLOUDSTACK INTERFACE 125\n\n4.4.5.5 Delete Volume\n\nThe Delete Volume operation deletes a detached disk volume. This operation\nsends the id parameter and returns the displaytext and success parameters.\n\n4.4.5.6 List Volumes\n\nThe List Volumes operation list the available CloudStack volumes. This operation\nsends the optional account, domainid, hostid, id, isrecursive, keyword, listall,\nname, page, pagesize, podid, projectid, tags, type, virtualmachineid and zoneid\nparameters. The response returns the standard response parameter.\n\n4.4.5.7 Extract Volume\n\nThe Extract Volume operation extracts a specific volume. This operation sends\nthe id, mode, zoneid parameters as well as the optional url parameter. The\nresponse returns the extractvolumeresponse parameter.\n\n\u2022 The extractvolumeresponse parameter contains the id, accountid, created,\nextractId, extractMode, name, state, status, storagetype, uploadpercent-\nage, url, zoneid and zonename attributes.\n\n4.4.5.8 Migrate Volume\n\nThe Migrate Volume operation migrates a volume to other storage pool. This\noperation sends the storageid and volumeid parameters. The response returns\nthe standard response parameter.\n\n4.4.5.9 Resize Volume\n\nThe Resize Volume operation resizes a specific volume. This operation sends the\noptional diskofferingid, id, shrinkok and size parameters. The response returns\nthe standard response parameter.\n\nThe operations and parameters used by the Volume Management component\nfrom CloudStack interface are resumed in Table 4.15.\n\n\n\n126 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.15: Volume Management Table (Parameters vs Operations).\n\nOperations\nParameters A\n\nttach\nV\nolu\n\nm\ne\n\nU\np\nd\nate\n\nV\nolu\n\nm\ne\n\nD\netach\n\nV\nolu\n\nm\ne\n\nC\nreate\n\nV\nolu\n\nm\ne\n\nD\nelete\n\nV\nolu\n\nm\ne\n\nL\nist\n\nV\nolu\n\nm\nes\n\nE\nx\ntract\n\nV\nolu\n\nm\ne\n\nM\nigrate\n\nV\nolu\n\nm\ne\n\nR\nesize\n\nV\nolu\n\nm\ne\n\nSent Parameters\naccount x x x\nchecksum x\ndeviceid x x\ndiskofferingid x x\ndomainid x x x\nformat x\nhostid x\nid x x x x x x\nisrecursive x\nkeyword x\nlistall x\nmode x\nname x x x\npage x\npagesize x\npodid x\nprojectid x x\nshrinkok x\nsize x x\nsnapshotid x\nstorageid x\ntags x\ntype x\nurl x x\nviirtualmachineid x x x\nvolumeid x\nzoneid x x x x\n\nSent Parameters\nstandard response x x x x x x x\nextractvolumeresponse x\ndisplaytext x\nsuccess x\n\n4.5 PACI Interface\n\nThis interface is accessible through a base URL with the following syntax:\nhttps ://{ ip_address | hostname }: port / paci / version\n\nThe request URL is composed by the IP address or hostname and listening\nport of the PACI server, followed by the path containing the PACI string and the\nversion of the REST API. The PACI RESTful API allows the end user to manage\nthe system through lists of components available at [142]. This analysis will be\nfocussed on the common functionalities between PACI API and the API of the\nother IaaS platforms under comparison. In the case of PACI interface library,\n\n\n\n4.5. PACI INTERFACE 127\n\nthe common functionalities include the management of: (i) Servers; (ii) Firewall;\n(iii) Application Template Installations; and (vi) Images.\n\n4.5.1 Server Management\n\nThe Server Management component is used to obtain information and perform\nactions on individual servers through the following list of operations: (i) List Serv-\ners; (ii) Start/Stop a Server; (iii) Create Server; (iv) Create Server From Image;\n(v) Clone Server; (vi) Modify Server Configuration; (vii) Reset Server Adminis-\ntrator Password; (viii) Obtain Server Information; (ix) Obtain Server History;\n(x) Delete Server; (xi) Set Backup Schedule; (xii) Cancel Backup Schedule; (xiii)\nList Backup Schedule; and (xiv) Restore a Server.\n\n4.5.1.1 List Servers\n\nThe List Servers operation is used to obtain the list of servers owned by the\ncurrent user. This operation does not require a request body and sends the\nsubscription-id query parameter in the URL. The response returns the ve-info\nparameter for each listed server.\n\n\u2022 The ve-info parameter contains individual server information in the descrip-\ntion, state, name and subscription-id attributes.\n\n4.5.1.2 Start/Stop a Server\n\nStart/Stop a Server is used to start or stop a specified server. This operation\ndoes not require a request body sending the ve-name and the action parameters\nin the URI path. The response returns a text message describing the state of the\noperation, i.e., if the server is initiated or stopped.\n\n\u2022 The ve-name parameter specifies the server name;\n\n\u2022 The action parameter must be substituted by the action start or stop.\n\n4.5.1.3 Create Server\n\nThe Create Server operation is used to request the creation of a new server.\nThis operation send the name, description, subscription-id, cpu, ram-size, band-\nwidth, no-of-public-ip, no-of-public-ipv6, ve-disk, template-info, os-info, backup-\nschedule and admin parameters. The response returns a pwd-response paramet-\ners.\n\n\u2022 The name parameter is a string type and specifies the server name;\n\n\n\n128 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n\u2022 The description parameter is a string type and specifies the description of\nthe new server;\n\n\u2022 The subscription-id specifies the Subscription ID number (type: int) for\nwhich to create the new server and must be included and populated when\nthe customer account to which the user belongs has more than one sub-\nscription. It can be omitted otherwise;\n\n\u2022 The cpu parameter specifies information about the server CPU unit and\nincludes the number and power attributes do define the number of cores\nand their clock frequency in MHz;\n\n\u2022 The ram-size parameter specifies the quantity of RAM in megabytes the\nserver will contain;\n\n\u2022 The bandwidth parameter specifies the attributed network bandwidth in\nKib/s;\n\n\u2022 The no-of-public-ip specifies the number of desired public IPv4 addresses;\n\n\u2022 The no-of-public-ipv6 specifies the number of desired public IPv6 addresses;\n\n\u2022 The ve-disk specifies information about the server\u2019s hard disk and contains\nthe local, primary and size attributes to define if it is a local or network\nhard disk, if it should be a system disk and the storage capacity in GiB;\n\n\u2022 The template-info parameter provides information about the OS template\nand includes the name attribute of the OS template name;\n\n\u2022 The os-info parameter provides information about the OS and includes the\ntechnology and type attributes to define the virtualization technology to\nuse (virtual machine or container) and the OS type;\n\n\u2022 The backup-schedule is an optional parameter to provide information about\nthe backup schedule. It contains the name attribute to define de schedule\ntype from a provided list.\n\n\u2022 The admin parameter specifies the administrator credentials using the login\nand password attributes;\n\n\u2022 The pwd-response parameter contains the message and password attributes.\n\n4.5.1.4 Create Server From Image\n\nThe Create Server From Image operation is used to request the creation of a\nserver from an existing image. This operation does no require a request body\nand sends the ve-name, image-name and the subscription-id parameters in the\n\n\n\n4.5. PACI INTERFACE 129\n\nURI path. The response returns a text message describing the status of the\noperation.\n\n\u2022 The image-name parameter specifies the image name;\n\n4.5.1.5 Clone Server\n\nThe Clone Server operation is used to request the creation of an exact copy of\nan existing server. This operation does not require a request body and sends the\nve-name and new-server-name parameters in the URI path. The response returns\nthe pwd-response parameter.\n\n\u2022 The new-server-name parameter specifies the name of the cloned server;\n\n4.5.1.6 Modify Server Configuration\n\nThe Modify Server Configuration operation is used to request the modification\nof an existing server configuration. This operation sends the reconfigure-ipv4,\nreconfigure-ipv6, primary-disk-size parameters in the request body and the ve-\nname parameter in the URI path. The response returns a text message describing\nthe state of the operation.\n\n\u2022 The reconfigure-ipv4 and reconfigure-ipv6 parameters contain the descrip-\ntion, change-cpu, ram-size, bandwidth, add-ipand drop-ip elements;\n\n\u2022 The primary-disk-size parameter modifies the storage capacity in GiB.\n\n4.5.1.7 Reset Server Administrator Password\n\nThe Reset Server Administrator Password operation is used to request the server\nadministrator password regeneration. This operation does not require a request\nbody and sends the ve-name parameter in the URI path. The response returns\nthe pwd-response parameter.\n\n4.5.1.8 Obtain Server Information\n\nThe Obtain Server Information operation is used to request information about a\nspecified server. This operation does not require a request body and sends the\nve-name parameter in the URI path. The response returns the ve parameter.\n\n\u2022 The ve parameter contains the id, uuid, hnid, customer-id, name, descrip-\ntion, subscription-id, cpu, ram-size, bandwidth, ve-disk, platform, net-\nwork, backup-schedule, state, primary-disk-id, template-id, admin, last-\noperation-rc, app-info, load-balancer and steady-state elements.\n\n\n\n130 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.5.1.9 Obtain Server History\n\nThe Obtain Server History operation is used to request the modification history\nfor the specified server. This operation does not require a request body and sends\nthe ve-name, records, from-inclusive and to-exclusive parameters in the URI path.\nThe response returns the ve-history parameter.\n\n\u2022 The records parameter specifies the number of records (from the end) to\ninclude in the result set;\n\n\u2022 The from-inclusive and to-exclusive parameters are used to retrieve the\nrecords that were created during the specified date-time period;\n\n\u2022 The ve-hostory parameter is composed by a list of multiple ve-snapshot ele-\nments containing the cpu, ram, local-disk, nbd, bandwidth, backup-scheme,\nlast-operation-rc, last-touched-from, state, steady-state, last-changed-by,\nevent-timestamp, no-of-public-ip, no-of-public-ipv6, is-lb, private-incoming-\ntraffic, private-outgoing-traffic, public-incoming-traffic and public-outgoing-\ntraffic attributes.\n\n4.5.1.10 Delete Server\n\nThe Delete Server operation is used to permanently delete a server. This oper-\nation does not require a request body and sends the ve-name parameter in the\nURI path. The response returns a message describing the status of the operation.\n\n4.5.1.11 Set Backup Schedule\n\nThe Set Backup Schedule operation is used to assign a backup schedule to the\nspecified server. This operation does not require a request body and sends the\nve-name and schedule-name parameters in the URI path. The response returns\na text message describing the status of the operation.\n\n\u2022 The schedule-name parameter specifies the name of an existing backup\nschedule.\n\n4.5.1.12 Cancel Backup Schedule\n\nThe Cancel Server Backup Schedule operation is used to cancel a backup schedule\nassigned to a server. This operation does not require a request body and sends\nthe ve-name parameter in the URI path. The response returns a text message\ndescribing the status of the operation.\n\n\n\n4.5. PACI INTERFACE 131\n\n4.5.1.13 List Backup Schedule\n\nThe List Backups operation is used to list the available backups for the specified\nserver. This operation does not require a request body and sends the ve-name,\nfrom-inclusive and to-exclusive parameters in the URI path. The response returns\na ve-backups parameter.\n\n\u2022 The ve-backups parameter includes a list of backup elements constituted\nby the in-backup-id, cloud-backup-id, schedule-name, started, ended, suc-\ncessful, backup-size, backup-node-name and delta-of attributes.\n\n4.5.1.14 Restore a Server\n\nThe Restore a Server operation is used to restore a specified server from a specified\nbackup. This operation does not require a request body and sends the ve-name\nand cloud-backup-id parameters in the URI path. The response returns a text\nmessage describing the status of the operation.\n\n\u2022 The cloud-backup-id parameter specifies the backup ID.\n\nThe operations and parameters used by the Server Management component\nfrom PACI interface are resumed in Table 4.16.\n\n\n\n132 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nTable 4.16: Server Management Table (Parameters vs Operations).\n\nOperations\nParameters L\n\nist\nS\nervers\n\nS\ntart/S\n\ntop\na\nS\nerver\n\nC\nreate\n\nS\nerver\n\nC\nreate\n\nS\nerver\n\nfrom\nIm\n\nage\n\nM\no\nd\nify\n\nS\nerver\n\nC\non\n\nfi\ngu\n\nration\n\nC\nlon\n\ne\nS\nerver\n\nR\neset\n\nS\nerver\n\nA\nd\nm\nin\n.\nP\nassw\n\nord\n\nO\nb\ntain\n\nS\nerver\n\nIn\nfo.\n\nO\nb\ntain\n\nS\nerver\n\nH\nistory\n\nD\nelete\n\nS\nerver\n\nS\net\n\nB\nack\n\nu\np\nS\nch\ned\n\nu\nle\n\nC\nan\n\ncel\nB\nack\n\nu\np\nS\nch\ned\n\nu\nle\n\nL\nist\n\nB\nack\n\nu\np\ns\n\nR\nestore\n\nS\nerver\n\nSent Parameters\nname x\ndescription x\nsubscription-id x x x\ncpu x\nram-size x\nbandwidth x\nno-of-public-ip x\nno-of-public-ipv6 x\nve-disk x\ntemplate-info x\nos-info x\nbackup-schedule x\nadmin x\nprimary-disk-size x\nfrom-inclusive x\nto-exclusive x x\nrecords x x\nschedule-name x\nve-name x x x x x x x x x x x x\nimage-name x\nnew-server-name x\nreconfigure-ipv4 x\nreconfigure-ipv6 x\ncloud-backup-id x\n\nReceived Parameters\nve-info x\ntext message x x x x x x x\npwd-response x x x\nve x\nve-history x\nve-backups x\n\n4.5.2 Firewall Management\n\nThe Firewall Management component is used to manage the firewall rules of a\nvirtual machine through the following list of operations: (i) List Firewall Rules;\n(ii) Create Firewall Rule; (iii) Modify Firewall Rule; and (iv) Delete Firewall\nRule.\n\n\n\n4.5. PACI INTERFACE 133\n\n4.5.2.1 List Firewall Rules\n\nThe List Firewall Rules operation requests a list of existing firewall rules for the\nspecified server. This operation does not require a request body and sends the ve-\nname parameter in the URI path. The response returns the rule and remote-net\nparameters.\n\n\u2022 The rule parameter is constituted by the id, name, protocol, local-port and\nremote-port attributes containing the firewall information for each rule;\n\n\u2022 The remote-net parameter specifies the remote address and an optional\nmask.\n\n4.5.2.2 Create Firewall Rule\n\nThe Create Firewall Rules operation requests the creation of firewall rules for\nthe specified server. This operation has a rule and remote-net parameters in the\nrequest body as well as the ve-name parameter in the URI path. The response\nreturns a text message describing the operation status.\n\n4.5.2.3 Modify Firewall Rule\n\nThe Modify Firewall Rules operation requests the modification of the existing\nfirewall rules. This operation sends the rule and remote-net parameters in the\nrequest body as well as the ve-name parameter in the URI path. The response\nreturns a text message describing the operation status.\n\n4.5.2.4 Delete Firewall Rule\n\nThe Delete Firewall Rules operation is used to delete all existing firewall rules\nof a specified server. This operation does not require a request body and sends\nthe ve-name parameter in the URI path. The response returns a text message\ndescribing the status of the operation.\n\nThe operations and parameters used by the Firewall Management component\nfrom PACI interface are resumed in Table 4.17.\n\nTable 4.17: Firewall Management Table (Parameters vs Operations).\n\nOperations\nParameters List Firewall\n\nRules\nModify Firewall\n\nRule\nCreate Firewall\n\nRule\nDelete Firewall\n\nRule\nSent Parameters\n\nve-name x x x x\nrule x x\nremote-net x x\n\nReceived Parameters\ntext message x x x\nrule x\nremote-net x\n\n\n\n134 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.5.3 Application Template Management\n\nThe Application Template Management component is used to obtain information\nabout the available application templates and to install them in servers. An\napplication template represents a software application that can be installed in a\nserver. This component is created for a specific operating system that must be\ncompatible with the OS template used to create a server. The available operations\nare: (i) List Application Templates; (ii) Get Application Templates Information;\nand (iii) Install Application Templates.\n\n4.5.3.1 List Application Templates\n\nThe List Application Templates operation is used to list the available application\ntemplates. This operation has no request parameters. The response returns the\napplication-list parameter.\n\n\u2022 The application-list parameter contains a list of application-template ele-\nments with the id, name, active, c2u-version and for-os attributes.\n\n4.5.3.2 Get Application Templates Information\n\nThe Get Application Templates Information operation is used to obtain a detailed\ninformation about a specified application template. This operation does not\nrequire a request body and sends the name and for-os parameters in the URI.\nThe response returns the application-template parameter.\n\n\u2022 The application-template parameter contains the id, name, active, c2u-\nversion and for-os attributes.\n\n4.5.3.3 Install Application Templates\n\nThe Install Application Templates operation is used to install an application\ntemplate into a server. The application template must be compatible with the\nOS template installed in the target server. This operation does not require a\nrequest body and sends the ve-name and app-name parameters in the URI. The\nresponse returns a text message describing the status of the operation.\n\n\u2022 The app-name parameter specifies the application template name to be\ninstalled.\n\nThe operations and parameters used by the Application Template Manage-\nment component from PACI interface are resumed in Table 4.18.\n\n\n\n4.5. PACI INTERFACE 135\n\nTable 4.18: Application Template Management Table (Parameters vs Opera-\ntions).\n\nOperations\nParameters List Application\n\nTemplate\nGet Application\n\nTemplate\nInstall Application\n\nTemplate\nSent Parameters\n\napp-name x\nfor-os x\nid x\nve-name x\n\nReceived Parameters\napplication-list x\napplication-\ntemplate\n\nx\n\ntext message x\n\n4.5.4 Image Management\n\nThe Image Management component provides operations to manage server im-\nages. A server image is created from an existing server and can be used later to\ncreate new servers. The available operations are: (i) List Images; (ii) Get Image\nInformation; (iii) Create Image; and (iv) Delete Image.\n\n4.5.4.1 List Images\n\nThe List Images operation is used to obtain a list of the existing server images.\nThis operation has no request parameters and returns the image-list parameter.\n\n\u2022 The image-info parameter contains a list of image-info elements with the\nname, size, created, subscription-id, load-balancer, active and image-of at-\ntributes.\n\n4.5.4.2 Get Image Information\n\nThe Get Image Information operation is used to obtain a detailed information for\nthe specified server image. This operation does not require a request body and\nsends the image-name parameter in the URI. The response returns the ve-image\nparameter.\n\n\u2022 The image-name parameter specifies the name of the image;\n\n\u2022 The ve-image parameter contains the id, bnode-uuid, customer-id,\nsubscription-id, load-balancer, active, image-of, name, cpu-number, cpu-\npower, ram-size, bandwidth, login, template-id, primary-disk-id, image-\nsize, created, no-of-public-ip, no-of-publicipv6, description and disks at-\ntributes.\n\n\n\n136 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\n4.5.4.3 Create Image\n\nThe Create Image operation creates an image from an existing (stopped) server.\nThis operation does not require a request body and sends the ve-name, image-\nname and subscription-id parameters in the URI. The response returns a text\nmessage describing the status of the operation.\n\n4.5.4.4 Delete Image\n\nThe Delete Image operation deletes an existing server image. This operation does\nnot require a request body and sends the image-name parameter in the URI. The\nresponse returns a text message describing the status of the operation.\n\nThe operations and parameters used by the Image Management component\nfrom PACI interface are resumed in Table 4.19.\n\nTable 4.19: Image Management Table (Parameters vs Operations).\n\nOperations\nParameters List Images Get Image\n\nInformation\nCreate Image Delete Image\n\nSent Parameters\nimage-name x x x\nve-name x\nsubscription-id x\n\nReceived Parameters\ntext message x x\nve-image x\nimage-list x\n\n4.6 Conclusions\n\nThere are significant differences regarding the type and number of interfaces,\nthe level of customization, the organization of the groups of operations and the\nstructure of the request/response messages provided by the four IaaS platform\ninterface libraries.\n\nOpenStack and PACI rely on RESTful interfaces, while OpenNebula and\nCloudStack use XML-RPC and Query (RESTlike) interfaces, respectively.\nWhereas the last three solutions present a single API, OpenStack, due to the\nmodularity of the system, presents an API for each software component. The\nheterogeneity of the different interface types, in particular in the case on open\nsource solutions, hinders the communication between different software solutions,\nforcing the creation of additional software layers (increasing the system\u2019s com-\nplexity) to prevent the user lock-in phenomenon.\n\nThe way the management components are organised as well as the set of\nspecific system operations offered vary between IaaS platform. The provision of\n\n\n\n4.6. CONCLUSIONS 137\n\nadditional operations by some IaaS platforms enable end-users with an increased\nlevel of customization. This is the case of the CloudStack API since it provides\nthe end-user with a larger set of operations, including unique groups of opera-\ntions, e.g., the Virtual Private Cloud (VPC). On the other hand, the PACI API\nprovides less management functionalities to the end-user, limiting the common\nmanagement components of the four IaaS platform solutions to the Server and\nImage Management components. However, if only open source solutions are con-\nsidered, there are additional common management components, namely, Virtual\nNetworks, Volumes and VM Templates.\n\nThe organization of the operations differs between the four IaaS API. Some\nrely on one operation to provide multiple functionalities, while others define a\nspecific operation for each functionality. For example, OpenNebula uses the same\noperation to perform different actions to a VM, while the other solutions use one\noperation for each action. The structure of the operations differs significantly\nbetween IaaS platform. OpenNebula adopts a simpler request/response message\nstructure for the operations, exchanging a reduced set of parameters, while a\nCloudStack operation typically sends and retrieves a massive number of para-\nmeters.\n\nTable 4.20 shows the comparison between the exposed management compon-\nents.\n\nTable 4.20: Management Components Comparison.\n\nOpenNebula OpenStack CloudStack PACI\nServers\n\nVM Management x x x x\nImages\n\nImage Management x x x x\nStorage\n\nSnapshots Management x x x -\nVolumes Management x x x -\nObjects Management - x - -\n\nTemplates\nVM Templates\nManagement\n\nx x x -\n\nApplication Templates\nManagement\n\n- - - x\n\nVirtual Networks\nNetworks Management x x x -\nFirewall Management - -* x x\nNAT Management - -* x -\nVPN Management - -* x -\n\nOther Functionalities\nGroup Management x x x -\nProject Management - x x -\nZone Management - x x -\nAuto Scale\nManagement\n\n- x x -\n\nLoad Balancer - -* x x\nVPC - - x -\n\n\n\n138 CHAPTER 4. INTERFACE LIBRARIES COMPARISON\n\nIn the following Chapter, the identified common management components\nbetween the four platforms will be analysed according to the number of sim-\nilar operations and send/receive parameters in order to propose and develop an\nInteroperable Service.\n\n\n\nChapter 5\n\nInteroperable Interface Proposal\n\nThis chapter proposes an interoperable interface solution based on the study and\ncomparison of the different IaaS platforms performed in the previous chapter. The\ndevelopment of a new Interoperable Service with OpenNebula, OpenStack, Cloud-\nStack and PACI can be achieved by reusing existing cloud abstraction libraries or\nby designing and implementing a dedicated solution. These two approaches are\nanalysed and a final selection is performed.\n\n5.1 Dedicated Interoperable Service\n\nA Dedicated Interoperable Service (DIS) with the four studied IaaS platforms\ncan be created based on the identified common operations. This service can be\ndeveloped using the Java programming language and should be organized in three\nfundamental layers: (i) Interface; (ii) Abstraction; and (iii) Interaction. Figure\n5.1 illustrates a top level architecture that can be used in the development of\nsuch a service.\n\nThe knowledge representation of the overall process can be performed via a\ndedicated ontology, allowing the representation of the knowledge structure, cre-\nation of instances and inferencing. The Interface Layer knowledge models the\nexposed components features (operations, input and output parameters); the\nAbstraction Layer knowledge maps the Interface Layer knowledge with the In-\nteraction Layer knowledge; and the Interaction layer knowledge represents the\nidentified common components features (operations, input and output paramet-\ners) of the IaaS interface libraries.\n\n139\n\n\n\n140 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\nFigure 5.1: Dedicated Interoperable Service architecture.\n\n5.1.1 Interaction Layer\n\nThe Interaction Layer is responsible for the interaction with the four IaaS plat-\nforms API, behaving like a software client that complies with each interface lib-\nrary specifications. This way, the communication between the DIS and the dif-\nferent IaaS platforms can include the different API technologies (Query, REST\nor XML-RPC), messaging formats (XML or JSON) and defined parameters. The\nrequests are directly forwarded to the receiver IaaS platform API to be processed\nand the responses from the different IaaS platforms are delivered to the middle\nlayer - Abstraction Layer - for further processing.\n\n5.1.2 Abstraction Layer\n\nThe Abstraction Layer is responsible for processing the incoming messages and\noutgoing actions between the different cloud IaaS platforms and the exposed\nDedicated Interoperable Service, creating the necessary abstraction for the end\nuser. Since it is positioned between the Interaction Layer and the Interface Layer\nit is aware of the different IaaS platforms. This way, request messages are created\nmapping input parameters to the addressed IaaS API, retrieving any additional\nrequired information from a cache and translating the invoked actions to the\nspecific IaaS API operations. The incoming messages are processed in a similar\n\n\n\n5.1. DEDICATED INTEROPERABLE SERVICE 141\n\nway. The parameters from the response messages are analysed, cached for further\nutilization and selected to be delivered to the Interface Layer. The need for extra\nrequest operations is also evaluated, since the response messages provided by the\nIaaS platforms are significantly different and, in some cases, a single response to\na request operation may not be sufficient.\n\n5.1.3 Interface Layer\n\nThe Interface Layer interacts with the CLI and the Web dashboard through\na RESTful API, exposing a common list of component operations regarding the\ndifferent underlying IaaS platforms. The implementation of such an interoperable\nRESTful interface library can be supported by Apache CFX Framework [143],\nApache Wink [144] or Apache Axis2 [145]. According to the previous chapter, the\ncommon components supported by the studied IaaS platform interface libraries\nare restricted to: (i) Virtual Machine Management; and (ii) Image Management.\nThe following sections describe the specific list of operations exposed by such an\nDIS RESTful API.\n\n5.1.3.1 Virtual Machine Management\n\nThe Interoperable Server Management component provides the following group\nof operations to manage the virtual machines: (i) List Virtual Machines; (ii) Get\nVirtual Machine Information; (iii) Create Virtual Machine; (iv) Delete Virtual\nMachine; (v) Start Virtual Machine; and (vi) Stop Virtual Machine.\n\nList Virtual Machines:\n\nThe List Virtual Machines operation lists the available machines owned by\nan end user. This operation does not require a request body sending the state\nand platform parameters in the URI. The response returns the servers_list\nparameter.\n\n\u2022 The state parameter can be used to filter the available virtual machines by\nits state. The available values are ACTIVE, STOPPED or DELETED;\n\n\u2022 The platform parameter can filter the virtual machines available on a\nspecific IaaS platform. The available values are opennebula, openstack,\ncloudstack and paci;\n\n\u2022 The servers_list parameter is formed by a list of server elements contain-\ning the name, id, state and platform attributes of the virtual machines.\n\n\n\n142 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\nGet Virtual Machine Information:\n\nThe Get Virtual Machine Information operation retrieves detailed information\nfor a specific virtual machine. This operation does not require a request body and\nsends the id parameter in the URI. The response returns the server parameter.\n\n\u2022 The server parameter is composed by the id, name, image_id, tem-\nplate_id, network, state, description and platform attributes.\n\nCreate Virtual Machine:\n\nThe Create Virtual Machine operation generates a new virtual machine. This\noperation sends the name, template_id, image_id and description para-\nmeters. The response returns the server parameter.\n\nDelete Virtual Machine:\n\nThe Delete Virtual Machine operation deletes an existing virtual machine.\nThis operation does not require a request body and sends the id parameter in\nthe URI. The response returns the status parameter.\n\nStart Virtual Machine:\n\nThe Start Virtual Machine operation starts a paused virtual machine. This\noperation does not require a request body and sends the id parameter in the\nURI. The response returns the status parameter.\n\nStop Virtual Machine:\n\nThe Stop Virtual Machine operation pauses a running virtual machine. This\noperation does not require a request body and sends the id parameter in the\nURI. The response returns the status parameter.\n\n5.1.3.2 Image Management\n\nThe Image Management component provides the following group of operations\nto manage the images from the different IaaS platforms: (i) List Images; (ii) Get\nImage Information; (iii) Create Image; and (iv) Delete Image.\n\nList Images:\n\nThe List Images operation lists the images available to the end user. This\noperation does not require a request body and sends the type and platform\nparameters in the URI. The response returns the image_list parameter.\n\n\u2022 The type parameter filter the returned list to public or user created images.\nThe available values are public or user. If the parameter is omitted all the\nimages available to the user are returned;\n\n\n\n5.2. CLOUD ABSTRACTION INTERFACE SOLUTIONS 143\n\n\u2022 The platform parameter selects the images by the IaaS platform;\n\n\u2022 The image_list parameter is composed by a list of image elements with\nthe name, id, OS_name, OS_ARCH, description and platform at-\ntributes.\n\nGet Image Information:\n\nThe Get Image Information operation retrieves detailed information for a\nspecific image. This operation does not require a request body and sends the id\nparameter in the URI. The response returns the image parameter.\n\n\u2022 The image parameter is composed by the name, id, OS_name,\nOS_ARCH, description and platform attributes.\n\nCreate Image:\n\nThe Create Image operation generates a new image. This operation sends\nthe name, OS_option and description parameters. The response returns the\nimage parameter.\n\n\u2022 The OS_option parameter specifies the available OS image option to be\nused.\n\nDelete Image:\n\nThe Delete Image operation deletes an existing image. This operation does\nnot require a request body and sends the id parameter in the URI. The response\nreturns the status parameter.\n\n5.2 Cloud Abstraction Interface Solutions\n\nInterface abstraction libraries provide a collection of implementations of beha-\nviour for the development of middle-ware systems, that in a multiple IaaS plat-\nform environment, abstract the peculiarities of a specific IaaS platform API of-\nfering a standard and unique API for the management of multiple IaaS clouds.\nDeltacloud [146], jClouds [147] and Libcloud [148] are examples of existing cloud\nabstraction solutions.\n\n5.2.1 Deltacloud\n\nDeltacloud is an open source top-level project from the Apache Software Found-\nation developed by Red Hat that aims to abstract differences between IaaS cloud\nplatform interface libraries. Deltacloud is written in Ruby and, as illustrated\n\n\n\n144 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\non Figure 5.2, it is divided into an API server - it can be either the Deltacloud\nRESTful API, the Distributed Management Task Force (DMTF) open standard\nCloud Infrastructure Management Interface (CIMI) REST API [149] or the AWS\nEC2 API - and the drivers necessary for connecting with the cloud providers.\n\nFigure 5.2: Deltacloud architecture [150].\n\nDeltacloud also provides a Ruby client, a HTTP dashboard and a group of\nIaaS provider drivers (the entire list can be consulted at [151] that includes Open-\nNebula, OpenStack and Eucalyptus. It provides documentation for the develop-\nment of new provider drivers (written in Ruby) that, in the case of this project,\ncan be used to develop drivers for the Lunacloud IaaS platform PACI. Each driver\nexposes the list of supported Ruby collections that may differ from provider to\nprovider. These collections describe the abstractions used by Deltacloud API\nand each collection represents an entity in the back-end provider cloud, e.g., a\nrunning virtual server or a server image. The available collections are:\n\n\u2022 Realms - Distinct organizational units within back-end clouds, e.g., a data\ncentre. A realm may, but does not necessarily represent, the geographical\nlocation of the accessed computing resources;\n\n\u2022 Instances - Realized virtual servers running in given back-end clouds that\nare instantiated from server images;\n\n\u2022 Images - Virtual machine images (or raw OS images) from which instances\nare created. Each image defines the root partition and initial storage for\nthe instance OS;\n\n\u2022 Instance states - Representations of instance life-cycle (start, pending, run-\nning, stopped, shutting_down, finished);\n\n\u2022 Keys - Credentials used to access a running instance. Keys can take the\nform of key (e.g., an RSA key) or of password (i.e., with username and\npassword attributes);\n\n\n\n5.2. CLOUD ABSTRACTION INTERFACE SOLUTIONS 145\n\n\u2022 Storage volume - A virtual storage device that can be attached to an in-\nstance and mounted by the OS;\n\n\u2022 Storage snapshot - A copy of a storage volume at a specified time;\n\n\u2022 Bucket - A container for data . The organizational unit of a generic key/-\nvalue based on a data-store (e.g., Rackspace CloudFiles [152] or Amazon S3\n[86]). Individual data items are exposed as a sub-collection under a bucket;\n\n\u2022 Blob - A generic Binary Large Object (BLOB) data item that exists within\na specified bucket (e.g., an object in Amazon S3 [86] and Rackspace Cloud-\nFiles [152]);\n\n\u2022 Address - Represents an IP address. Depending on the back-end cloud\nprovider, the address can be public or private;\n\n\u2022 Load Balancer - A load balancer allows a distribution of ingress network\ntraffic received by a specified IP address to a number of instances;\n\n\u2022 Firewalls - Sets of rules that govern the accessibility of a running instance\nover the public Internet;\n\n\u2022 Metrics - Useful information about cloud resources, e.g., CPU utilization\nor network throughput.\n\nUsing the described collections, the Deltacloud framework exposes compute,\nnetwork and storage groups of operations. The server can respond to client\nrequests in various formats. The appropriate response format is determined by\nHTTP content negotiation and the primary format is XML. The output is also\navailable in JSON and HyperText Markup Language (HTML). Clients can also\nexplicitly request a specific response format by including the format= request\nparameter, as shown below:\n\nhttp://deltacloudserver.foo/api?format=xml\nhttp://deltacloudserver.foo/api?format=json\n\nDeltacloud uses basic HTTP authentication [126] to receive credentials from\nthe client and passes them to the particular back-end cloud. The credentials\nalways consist of a username and password and are never stored in the server.\nThe exact login credentials, and the place to find them, depends on the back-\nend cloud platform that the Deltacloud server API is interacting with. A list\nof existing drivers together with the authentication details is available at [151].\nThere is also a Ruby cloud service library named Fog [153] that provides an\naccessible entry point and facilitates cross service compatibility to IaaS services\nlike compute, DNS and storage. This library is included in Deltacloud.\n\n\n\n146 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\nDeltacloud interacts with multiple IaaS platforms by creating dedicated server\ninstances for each IaaS platform driver, i.e., provides a dedicated endpoint to\ninteract with each IaaS platform, or reuses a single server instance to interact\nwith all supported IaaS platform drivers.\n\n5.2.2 jClouds\n\nApache jclouds is an open source library, developed by Apache Software Found-\nation, for cloud enabling and inter IaaS cloud platforms communication that\nallows the provisioning and control of cloud resources. jClouds API offers both\nportable abstractions and cloud-specific features that enable the management of\nbuckets (BlobStore) and compute operations (ComputeService) with a compat-\nible list of cloud providers and IaaS platforms, including OpenStack, CloudStack\nand Eucalyptus (since AWS is also supported) [154].\n\njClouds enables developers to add the support for new IaaS platform pro-\nviders through the utilization of Maven archetypes. These archetypes supply the\nproject set-up, the necessary dependencies and some code samples. The exposed\nCompute API provides a basic abstraction across Compute interface libraries\nand also integrates popular tools such as Ant [155] and Maven [156]. It can man-\nage nodes as a set and address resources in any supported IaaS cloud platform\nwithout needing separate connections. The abstraction provided by the jClouds\nCompute API is based on the following vocabulary:\n\n\u2022 ComputeService - a service level API endpoint, e.g., EC2, that contains an\ninventory of hardware profiles, images and nodes, the ability to create and\ndestroy nodes and resolve templates;\n\n\u2022 Provider - runs a compute provisioning API, e.g., vCloud, EC2. It may\nrun multiple endpoints (e.g., west, east, north) and is generally bound to a\ncontext and a unique identity;\n\n\u2022 Image - a pre-configured operating system representing the base software\nfor a new node. It is a part of the template used to create nodes;\n\n\u2022 Hardware - a set of resource configurations that includes the input memory,\nCPU and disk required for the creation of a server;\n\n\u2022 Location - an assignable physical or logical location inside a provider where\nnodes can be launched. Location can be scoped as Provider, Region or\nZone. A location, which often refers to a virtual or physical data-centre, is\na mandatory input for the creation of a server;\n\n\u2022 TemplateOptions - options for creations of resources, including ports to\nopen, scripts to run at bootstrap;\n\n\n\n5.2. CLOUD ABSTRACTION INTERFACE SOLUTIONS 147\n\n\u2022 Template - composite of Image, Hardware, Location and TemplateOptions\nfor node creation. It allows for repeated creation of nodes;\n\n\u2022 Node - a named instance of a Template sometimes called server or virtual\nmachine. A node has, typically, login, IP and security metadata;\n\n\u2022 Group - A name that aggregates nodes and related incidental resources\nsuch as keys, so that groups of nodes can be controlled as a unit. As many\nclouds do not support multiple groups, a group implies a primary grouping.\n\nThe complete information about jClouds API packages, classes and methods\nis available at [157]. There is a similar open source abstraction Java library\nthat enables a Java developer to access functionality across a number of cloud\nproviders through a single interface named Dasein Cloud API [158]. This API\nhas a larger scope than jClouds, covering features such as networking and cloud\nPaaS functionalities.\n\n5.2.3 Libcloud\n\nApache Libcloud is a standard Python library that abstracts the differences\namong multiple cloud provider interface libraries. It was originally created by\nCloudkick (a start-up acquired by Rackspace) [159] and has grown into an inde-\npendent free software project licensed under the Apache License (2.0).\n\nThe current version allows users to manage four different cloud resources:\n\n\u2022 Cloud Servers - services such as Amazon EC2 [60];\n\n\u2022 Cloud Storage - services such as Amazon S3 [86] and Rackspace CloudFiles\n[152];\n\n\u2022 Load Balancer as a Service (LBaaS);\n\n\u2022 DNS as a Service (DNSaaS).\n\nThe complete list of IaaS cloud providers is available at [160]. The group\nof compute operations are: (i) List; (ii) Reboot; (iii) Create; (iv) Destroy; (v)\nImages; (vi) Sizes; and (vii) Deploy. These compute operations are supported by\nEucalyptus, OpenNebula, CloudStack and OpenStack. The compute terminology\nutilized to abstract the underlying differences between IaaS platforms is composed\nby the following terms:\n\n\u2022 Node - represents a cloud or virtual server;\n\n\n\n148 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\n\u2022 NodeSize - represents a node hardware configuration, including the available\nRAM, bandwidth, CPU speed and disk size and, in most cases, exposing\nan hourly price (in dollars);\n\n\u2022 NodeImage - represents an operating system image;\n\n\u2022 NodeLocation - represents a server physical location;\n\n\u2022 NodeState - represents a node state, which can be running, stopped, re-\nbooting, terminated, pending and unknown.\n\nThe support for other IaaS platforms can be provides through the creation of\nnew drivers (named third-party drivers by the Libcloud documentation). These\ndrivers can be developed according to the LibCloud documentation [161]. To use\na new driver it is necessary to register it using the provider.set_driver() function\nfrom the corresponding component. This function takes the provider_name,\npath.to.the.module and DriverClass arguments. An example of the registra-\ntion of a third-party can be obtained from [162].\n\n5.3 Conclusions\n\nThe IaaS cloud market is dynamically growing, but is supported by multiple non-\ninteroperable IaaS solutions. Cloud libraries that are capable of abstracting IaaS\nAPI differences are fundamental in the current context of cloud computing. Con-\nsumers interact with different IaaS providers according to the existing available\nofferings and, thus, need to deal with the diversity and specificity of the support-\ning IaaS platforms. The challenge is to overcome existing lack of interoperability,\nvendor lock-in, terminology issues and distinct authentication methods, while ex-\nposing a common API to manage resources across different IaaS clouds. There\nare two possible approaches to address this problem: the creation of dedicated\ninteroperable services or the adoption of existing abstraction solutions.\n\nCloud abstraction libraries or frameworks and dedicated interoperable solu-\ntions are commonly used to interact with different cloud infrastructures (with\ndifferent deployment models) in several R&amp;D cloud interoperability related pro-\njects. Some examples of such projects are:\n\n\u2022 Aeolus [163] - a project composed by a suite of open-source tools that help\nthe construction of custom clouds from both public and private resources.\nIt uses Deltacloud as a core component for the enabling of cross and hybrid\ncloud functionality;\n\n\n\n5.3. CONCLUSIONS 149\n\n\u2022 mOSAIC [164] - a 7th Framework Programme (FP7) funded project with\nthe goal to provide an open source API to support a multiple clouds plat-\nform. In terms of interfacing with exiting IaaS solutions, mOSAIC adopts\na specific type of component called Driver. These Drivers reuse Libcloud,\njclouds, DeltaCloud, PHP Simple Cloud [165] and Simple API for Grid Ap-\nplications (SAGA) [166]. Saga also uses the Libcloud abstraction library to\ninterface with the AWS EC2 API.\n\n\u2022 Contrail [167] - an European project, partially funded by FP7, with the ob-\njective to design, implement, evaluate and promote an open source system\nfor Cloud Federations. The Federated Cloud resources are highly heterogen-\neous in their hardware configuration and system-level organization, taking\nthe form of physical machines running the XtreemOS operating system, an\n6th Framework Programme (FP6) intellectual property project [168], vir-\ntual instances from external Clouds (using open standards such as OCCI\nand CIMI), virtual machines running XtreemOS or XtreemOS machines\nrunning virtualization software.\n\nThe goal of this project is to develop an open source solution for the integrated\nmanagement of OpenNebula, OpenStack, CloudStack and PACI IaaS platforms\nresources. There are two possible approaches: the design and development of\na dedicated interoperable Web service API or the reuse of existing abstraction\nsolutions (libraries and frameworks). The first approach, which is based on the\nlist of common IaaS platforms resource management operations, is specific and\nis not extensible. The latter, which is supported by existing an cloud abstraction\nlibraries and frameworks, provides the required functionalities, is reusable and\nextensible.\n\nDeltacloud, jClouds and Libcloud are among the most representative cloud\nIaaS abstraction solutions. Deltacloud, which provides by default three differ-\nent service API (native RESTful Deltacloud API, CIMI and AWS EC2), is a\nframework that includes a Ruby client, a Web dashboard and a driver develop-\nment environment to support the integration of further IaaS platforms. jClouds\nand Libcloud are standard programming libraries and, unlike Deltacloud, do not\nintegrate additional development tools. In terms of IaaS platform support, Lib-\ncloud provides official integration with the studied open source IaaS platforms\n(OpenNebula, Eucalyptus, OpenStack and CloudStack), jClouds supports Eu-\ncalyptus, CloudStack and OpenStack while Deltacloud supports OpenNebula,\nEucalyptus and OpenStack. None of these abstraction solutions provides sup-\nport for PACI. The Table 5.1 presents a comparison between these open-source\nabstraction solutions.\n\n\n\n150 CHAPTER 5. INTEROPERABLE INTERFACE PROPOSAL\n\nTable 5.1: Abstraction solutions comparison.\n\nOpen-source Abstraction Solutions\nCharacteristics Deltacloud jClouds Libcloud\nSolution type Framework Library Lybrary\nProgramming\nlanguage Ruby Java Python\n\nProviders\nsupported\n\n17 cloud\nproviders\n\n30 cloud\nproviders\n\n38 cloud\nproviders\n\nOperations\nsupported\n\nCompute, Storage,\nNetwork Compute, Storage\n\nCompute, Storage,\nNetwork\n\nIaaS platform\ninteraction\nform\n\nDrivers Mavendependencies Drivers\n\nAPI\nNative REST API,\nCIMI API,\nAWS API\n\n- -\n\nOther features Web dashboard,Ruby client - -\n\nAlthough Libcloud provides official support for the analysed open source\nIaaS platforms, there are also third-party drivers that integrate CloudStack with\nDeltacloud. Thus, the Deltacloud abstraction framework will be adopted because\nit provides additional development tools and Web services (e.g., the Ruby CLI,\nWeb Dashboard), exposes broadly used interface libraries (CIMI API and AWS\nEC2 API) that will make the overall development project more complete and\nprovide documentation for the development of Deltacloud drivers to integrate\nnew IaaS platforms. The PACI IaaS platform support can be added through the\ndevelopment of a third-party driver.\n\n\n\nChapter 6\n\nProject Development\n\nThis chapter presents the development environment, architecture, implementa-\ntion, deployment modes and functionalities of an Interoperable Interface Service\nfor the integrated management of the following IaaS platforms: PACI (LunaCloud\nIaaS proprietary platform) and OpenNebula, OpenStack and CloudStack open-\nsource platforms.\n\n6.1 Development Environment\n\nThe development environment is composed of the programming languages, back-\nend and front-end technologies as well as the development tools.\n\n6.1.1 Languages\n\nThe programming languages adopted are presented in the following sections ac-\ncording to its application domain.\n\n6.1.1.1 General Purpose Language\n\nThe Generic Purpose Language (GPL) adopted is the Ruby programming lan-\nguage [169]. This language, created by Yukihiro Matsumoto, is an object-oriented\nprogramming language with an easy-to-use interpreter, familiar syntax, complete\nobject-oriented functionality and powerful class libraries [170]. Ruby\u2019s current\nstable version is 2.1.1, but, due to compatibility reasons, the Ruby version used\nwas 1.9.1. The RubyGems [171] software (version 1.8.23) was installed together\nwith Ruby. RubyGems allows the download, installation and usage of Ruby soft-\nware packages. These software packages are called \u201cgems\u201d and contain Ruby\napplications or libraries.\n\n151\n\n\n\n152 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.1.1.2 Domain Specific Languages\n\nThe Domain Specific Language (DSL) used include XML and JSON for data\ntransfer as well as HTML Abstraction Markup Language (HAML) and JavaScript\n(jQuery mobile library) for the user interface.\n\n6.1.2 Back-end Technology\n\nThe Deltacloud framework constitutes the back-end technology of this project.\nIt acts as an abstraction middleware for OpenNebula, OpenStack, CloudStack\nand PACI IaaS platforms. The framework, as Figure 6.1 illustrates, is composed\nof three main layers: (i) Drivers Layer; (ii) Core Layer; and (iii) User Interface\nLayer.\n\nFigure 6.1: Deltacloud framework.\n\nThe Drivers Layer contains the individual drivers, written in Ruby, which in-\nteract with the specific back-end IaaS platforms and process the HTTP request-\ns/responses. Each driver uses external Ruby gems or cloud clients to interact\nwith the corresponding IaaS platform API. These libraries provide methods to\nperform API calls that comply with the IaaS API (protocol and operations). The\ndrivers rely on methods and attributes inherited by the BaseDriver superclass to\n\n\n\n6.1. DEVELOPMENT ENVIRONMENT 153\n\ncreate and access collections of objects used to expose and retrieve information\nfrom the IaaS platform via the User Interface Layer.\n\nThe middle layer - Core Layer - is composed of modules and classes that\nsupport the remaining two layers. It contains general Ruby libraries (i.e., Core\nExt.) as well as modules and classes that define the collection object type models,\nthe collections and the features that each API currently exposes and that each\ndriver can implement (represented as the Collections, Features and Models yellow\nfolders in Figure 6.1). It also implements the Web service interfaces (i.e., API\nfile) and the Web dashboard (i.e., Server file) as well as the HTTP body response\nformat (XML, JSON or HTML) exposed by the top layer. Whereas the Web\ndashboard pages are defined in HAML, the remaining components are written in\nRuby using Sinatra [172] and Sinatra\u2019s Rabbit extension [173].\n\nThe User Interface Layer is responsible for the exposure of the Deltacloud\nWeb services. These Web services, which reuse the Sinatra library and rely\non the Rack interface [174] for HTTP processing, are deployed in Thin [175],\nthe Deltacloud Web application server. End-users access via Thin the available\nDeltacloud interfaces (API and Web dashboard).\n\n6.1.2.1 Web Application Library\n\nSinatra is a Ruby-based DSL for building Web sites, Web services and Web applic-\nations [172]. It is a lightweight wrapper around Rack middleware that establishes\na close relationship between service endpoints and HTTP operations, making it\nsuitable for Web services. It emphasizes a minimalist approach to development,\nproviding only what is essential to handle HTTP requests and deliver responses\nto clients. Sinatra, by itself, is not a Web framework, i.e., it has no built-in\nORM tools or pre-fabricated configuration files (like other Web frameworks such\nas Rails [176]) and it does not require the implementation of specific software\nmodels [177]. Nevertheless, the Deltacloud framework uses Sinatra version 1.4.4\nand adopts a Model View Controller (MVC) approach.\n\n6.1.2.2 Web Application Server\n\nThin is a HTTP Ruby Web server [175] included with Deltacloud and is composed\nby Mongrel\u2019s HTTP parser, the network I/O Event Machine library and the Rack\nWeb server interface. The version used is 1.6.1.\n\n6.1.3 Front-end Technologies\n\nThe Front-end support technologies adopted were the Web browser and the cURL\ncommand line tool [178]. The Web browser is used to interact with Deltacloud\u2019s\nWeb dashboard and the cURL tool to access and invoke directly the Deltacloud\u2019s\nAPI. Alternatively, it is possible to use the Deltacloud Ruby Client library.\n\n\n\n154 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.1.4 Development Tools\n\nThe tools used on the development environment consist of the Unix text editor\n(e.g., VIM [179]), the Interactive Ruby Shell (IRB), the command line tool cURL\n[178] and the open-source packet analyser Wireshark [180]. While the Unix text\neditor and IRB are employed for programming and testing Ruby code, cURL and\nWireshark are used for interacting with and testing the Web API as well as to\nmeasure the data load and time response of the HTTP/TCP packets.\n\n6.2 Architecture\n\nThe Interoperable Service uses the Deltacloud abstraction framework as a mid-\ndleware between cloud users and IaaS platforms, permitting the management of\nmultiple IaaS platforms via a single service.\n\nFigure 6.2: Interoperable Service architecture.\n\nThe architecture of this Interoperable Service is composed, essentially, by\nthe back-end driver modules (OpenNebula, OpenStack, CloudStack and PACI\ndriver), the software daemon deltacloudd and the GUI and API service interfaces.\nFigure 6.2 illustrates the architecture of the Interoperable Service.\n\nThe back-end driver modules, composed of the OpenNebula, OpenStack,\nCloudStack and PACI drivers, are integrated and developed to enable the ab-\nstraction and interaction with the respective back-end IaaS platforms. These\ndrivers define, through method instantiation and implementation, the Deltacloud\noperations that the IaaS platform provides.\n\nThe software daemon deltacloudd is included in the deltacloud-core compon-\nent and defines the service start-up and deployment using a set of established\n\n\n\n6.3. INTEROPERABLE SERVICE API 155\n\noptions. Once instantiated, the deltacloudd starts the Thin Web server and\nloads the front-end Web services and defined driver modules.\n\nThe front-end Web services offer GUI and API service interfaces. The GUI\nservice presents a simple Web dashboard containing the driver implemented Col-\nlections and operations. The API service has a RESTful implementation that\nuses driver defined collections and operations to expose the cloud resources from\nthe IaaS platforms.\n\n6.3 Interoperable Service API\n\nThe Interoperable Service uses the Deltacloud API [181] to manage the compute\nand storage resources of the integrated IaaS platforms. The management of these\nresources is made via the Deltacloud defined collections. These collections are\nformed by groups of operations and represent the abstracted IaaS platform entity.\nThe existing collections are: (i) Realms; (ii) Hardware Profiles; (iii) Images; (iv)\nInstances; (v) Keys; (vi) Firewalls; (vii) Addresses; (viii) Load Balancers; (ix)\nVolumes; (x) Snapshots; and (xi) Blobs.\n\nThis section describes the collections and operations offered by Deltacloud.\n\n6.3.1 Realms\n\nThe Realms collection implements a boundary containing the resources of a spe-\ncific domain, e.g., a data centre. This collection is formed by the following oper-\nations: (i) List Realms; and (ii) Show Realm Information.\n\n6.3.1.1 List Realms\n\nThe List Realms operation lists all the available and implemented realms of the\nback-end IaaS platform. This operation accepts an optional architecture para-\nmeter in the request to filter the realms that support a specific architecture. The\nresponse returns a list of realm parameters containing the href and id attributes\nas well as the name and state sub-parameters for each realm.\n\n6.3.1.2 Show Realm Information\n\nThe Show Realm Information operation details the information of a specific realm.\nThis operation sends the id parameter of the realm in the request. The response\nreturns a realm parameter containing the href and id attributes of the realm\nas well as the name, state and limit sub-parameters.\n\n\n\n156 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.3.2 Hardware Profiles\n\nThe Hardware Profiles collection describes the attributes of a virtual machine,\ne.g., vCPU, RAM and Disk. This collection is formed by the following operations:\n(i) List Hardware Profiles; and (ii) Show Hardware Profile Information.\n\n6.3.2.1 List Hardware Profiles\n\nThe List Hardware Profiles operation lists all the available hardware profiles.\nThis operation does not require request parameters. The response returns a list\nof hardware_profile parameters containing the href and id attributes as well\nas the name and property sub-parameters. The property sub-parameters spe-\ncify the kind (\u2019fixed\u2019, \u2019ranged\u2019, \u2019enumeration\u2019), name (\u2019cpu\u2019, \u2019memory\u2019, \u2019storage\u2019,\n\u2019architecture\u2019), unit (\u2019count\u2019, \u2019MB\u2019, \u2019GB\u2019, \u2019label\u2019) and value attributes for each\nhardware profile property.\n\n6.3.2.2 Show Hardware Profile Information\n\nThe Show Hardware Profile Information details the information of a specific hard-\nware profile. This operation sends the id parameter of the hardware profile in\nthe request. The response returns a hardware_profile parameter containing\nthe href and id attributes as well as the name and property sub-parameters.\n\n6.3.3 Images\n\nThe Images collection describes image operations. The supported operations are:\n(i) List Images; (ii) Show Image Information; (iii) Create Image from Instance;\nand (iv) Delete Image.\n\n6.3.3.1 List Images\n\nThe List Images operation lists all the available images. This operation sends\noptionally the owner_id and architecture parameters. The response returns\na list of image parameters containing the href and id attributes as well as\nthe name, owner_id, description, architecture, state and actions sub-\nparameters.\n\n6.3.3.2 Show Image Information\n\nThe Show Image Information operation details the information of a specific image.\nThis operation sends the id parameter of the specified image. The response\nreturns an image parameter containing the href and id attributes as well as\nthe name, owner_id, description, architecture, state and actions sub-\nparameters.\n\n\n\n6.3. INTEROPERABLE SERVICE API 157\n\n6.3.3.3 Create Image from Instance\n\nThe Create Image from Instance operation creates an image from a specific in-\nstance. This operation sends the instance_id parameter as well as the optional\nname and description parameters in the request. The response returns the\nsame list of parameters as the Show Image Information operation.\n\n6.3.3.4 Delete Image\n\nThe Delete Image operation deletes a specific image. This operation sends the\nid parameter of the image in the request. The response returns a HTTP 204 No\nContent header.\n\n6.3.4 Instances\n\nThe Instances collection describes instance operations. The operations supported\nby this collection are: (i) List Instances; (ii) Show Instance Information; (iii)\nInstance Action; (iv) Create Instance; and (v) Delete Instance.\n\n6.3.4.1 List Instances\n\nThe List Instances operation enumerates all the available instances. This oper-\nation does not require request parameters. The response returns a list of in-\nstance parameters containing the href and id attributes as well as the name,\nowner_id, image, realm, state, hardware_profile, actions, launch_time,\npublic_addresses, private_address, firewalls and authentication\nsub-parameters.\n\n6.3.4.2 Show Instance Information\n\nThe Show Instance Information operation details the information of a specific in-\nstance. This operation sends the id parameter of the instance in the request. The\nresponse returns an instance parameter containing the href and id attributes\nas well as the name, owner_id, image, realm, state, hardware_profile,\nactions, launch_time, public_addresses, private_address, firewalls and\nauthentication sub-parameters.\n\n6.3.4.3 Instance Action\n\nThe Instance Action operation performs a start, stop and reboot action on a\nspecific instance. This operation sends the id and action parameters. The\nresponse returns the same list of parameters as the Show Instance Information\noperation.\n\n\n\n158 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.3.4.4 Create Instance\n\nThe Create Instance operation creates a new instance. This operation sends the\nrequired image_id parameter as well as the optional hwp_id, realm_id, and\nname parameters. The response returns the same list of parameters as the Show\nInstance Information operation.\n\n6.3.4.5 Delete Instance\n\nThe Delete Instance operation deletes the specified instance. This operation sends\nthe instance id parameter in the request. The response returns the HTTP 204\nNo Content header.\n\n6.3.5 Keys\n\nThe Keys collection capture the credentials required to access an Instance. This\ncollection is formed by the following operations: (i) List Keys; (ii) Show Key\nInformation; (iii) Create Key; and (iv) Delete Key.\n\n6.3.5.1 List Keys\n\nThe List Keys operation lists all the available keys. This operation does not\nrequire request parameters. The response returns a list of key parameters with\nthe href and id attributes as well as the action, fingerprint and state sub-\nparameters.\n\n6.3.5.2 Show Key Information\n\nThe Show Key Information operation details the information of a specific key.\nThis operation sends the id parameter in the request. The response returns the\nkey parameter with the href and id attributes as well as the action, fingerprint\nand state sub-parameters.\n\n6.3.5.3 Create Key\n\nThe Create Key operation generates a new key. This operation sends the name\nparameter in the request. The response returns the same parameter list as the\nShow Key Information operation.\n\n6.3.5.4 Delete Key\n\nThe Delete Key operation deletes an existing key. This operation sends the\nid parameter in the request. The response returns the HTTP 204 No Content\nheader.\n\n\n\n6.3. INTEROPERABLE SERVICE API 159\n\n6.3.6 Firewalls\n\nThe Firewalls collection manages sets of rules that govern the accessibility of a\nrunning instance over the public Internet. Currently, only the Amazon EC2 and\nFujitsu GCP support this Deltacloud collection. This collection is formed by\nthe following operations: (i) List Firewalls; (ii) Show Firewall Information; (iii)\nCreate Firewall; (iv) Delete Firewall; (v) Create Firewall Rule; and (vi) Delete\nFirewall Rule.\n\n6.3.6.1 List Firewalls\n\nThe List Firewalls operation lists all the available firewalls. This operation does\nnot require request parameters. The response returns a list of firewall paramet-\ners with the href and id attributes as well as the name, description, owner\nand rules sub-parameters.\n\n6.3.6.2 Show Firewall Information\n\nThe Show Firewall Information operation details the information of a specific\nfirewall. This operation sends the firewall_id parameter in the request. The\nresponse returns the firewall parameter with the href and id attributes as well\nas the name, description, owner and rules sub-parameters.\n\n6.3.6.3 Create Firewall\n\nThe Create Firewall operation creates a new firewall instance. This operation\nsends the name and description parameters in the request. The response re-\nturns the same list of parameters as the Show Firewall Information operation.\n\n6.3.6.4 Delete Firewall\n\nThe Delete Firewall operation deletes a specific firewall. This operation sends the\nid parameter in the request. The response returns the HTTP 204 No Content\nheader.\n\n6.3.6.5 Create Firewall Rule\n\nThe Create Firewall Rule adds a new rule to an existing firewall. This operation\nsends the firewall_id, protocol, port_from, port_to and sources paramet-\ners. The response returns the same list of parameters as the Show Firewall\nInformation operation.\n\n\n\n160 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.3.6.6 Delete Firewall Rule\n\nThe Delete Firewall Rule deletes a rule from an existing firewall. This operation\nsends the firewall_id and rule_id parameters in the request. The response\nreturns a HTTP 204 No Content header.\n\n6.3.7 Addresses\n\nThe Addresses collection allows IP address management. This collection is formed\nby the following operations: (i) List Addresses; (ii) Shows Address Information;\n(iii) Create Address; (iv) Delete Address; (v) Associate Address; and (vi) Disso-\nciate Address.\n\n6.3.7.1 List Addresses\n\nThe List Addresses operation lists all the available IP addresses. This operation\ndoes not require request parameters. The response returns the list of address\nparameters with the href and id attributes as well as the actions and ip sub-\nparameters.\n\n6.3.7.2 Shows Address Information\n\nThe Shows Address Information operation details the information of a specific\naddress. This operation sends the id parameter in the request. The response\nreturns the address parameter with the href and id attributes as well as the\nactions and ip sub-parameters.\n\n6.3.7.3 Create Address\n\nThe Create Address operation creates a new IP address. This operation does not\nrequire request parameters. The response returns the same list of parameters as\nthe Show Address Information operation.\n\n6.3.7.4 Delete Address\n\nThe Delete Address operation deletes a specific IP address. This operation sends\nthe id parameter in the request. The response returns the HTTP 204 No Content\nheader.\n\n6.3.7.5 Associate Address\n\nThe Associate Address operation associates an IP address to an instance. This\noperation sends the id and instance_id parameters in the request. The response\nreturns the HTTP 202 Accepted header.\n\n\n\n6.3. INTEROPERABLE SERVICE API 161\n\n6.3.7.6 Dissociate Address\n\nThe Dissociate Address operation, dissociates an IP address from an instance.\nThis operation sends the id parameter in the request. The response returns the\nHTTP 202 Accepted header.\n\n6.3.8 Load Balancers\n\nThe Load Balancers collection allows the distribution of ingress network traffic,\nreceived by a specified IP address, to a number of running instances. This col-\nlection is formed by the following operations: (i) List Load Balancers; (ii) Show\nLoad Balancer Information; (iii) Create Load Balancer; (iv) Delete Load Balan-\ncer; (v) Register Instance to Load Balancer; and (vi) Unregister Instance from\nLoad Balancer.\n\n6.3.8.1 List Load Balancers\n\nThe List Load Balancers operation returns the available load balancers. This\noperations does not require request parameters. The response returns the list\nof load_balancer parameters with a href and id attributes as well as the\nactions, public_addresses, created_at, realm, listeners and instances\nsub-parameters.\n\n6.3.8.2 Show Load Balancer Information\n\nThe Show Load Balancer Information operation details the information of a spe-\ncific load balancer. This operation sends the id parameter in the request. The\nresponse returns the load_balancer parameter with a href and id attributes\nas well as the actions, public_addresses, created_at, realm, listeners and\ninstances sub-parameters.\n\n6.3.8.3 Create Load Balancer\n\nThe Create Load Balancer operation creates a new load balancer. This operation\nsends the name, realm_id, listener_protocol, listener_balance_port and\nlistener_instance_port parameters in the request. The response returns the\nsame list of parameters as the Show Load Balancer Information operation.\n\n6.3.8.4 Delete Load Balancer\n\nThe Delete Load Balancer operation deletes a specified load balancer with no\nassociated instances. This operation sends the id parameter in the request. The\nresponse returns a HTTP 204 No Content header.\n\n\n\n162 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.3.8.5 Register Instance to Load Balancer\n\nThe Register Instance to Load Balancer operation attaches an instance to the\nload balancer. This operation sends the id and instance_id parameters in the\nrequest. The response returns a HTTP 204 No Content header.\n\n6.3.8.6 Unregister Instance from Load Balancer\n\nThe Unregister Instance from Load Balancer operation detaches an instance from\na load balancer. This operation sends the id and instance_id parameters in\nthe request. The response returns a HTTP 204 No Content header.\n\n6.3.9 Volumes\n\nThe Volumes collection manages storage volumes. This collection is formed by\nthe following operations: (i) List Volumes; (ii) Show Volume Information; (iii)\nCreate Volume; (iv) Delete Volume; (v) Attach Volume; and (vi) Detach Volume.\n\n6.3.9.1 List Volumes\n\nThe List Volumes operation lists all the available storage volumes. This opera-\ntion does not require request parameters. The response returns a list of stor-\nage_volume parameters with the href and id attributes as well as the created,\ncapacity, state and actions sub-parameters.\n\n6.3.9.2 Show Volume Information\n\nThe Show Volume Information operation details the information of a specific stor-\nage volume. This operation sends the id parameter in the request. The response\nreturns the storage_volume parameter with the href and id attributes as well\nas the created, capacity, realm, state, mount and actions sub-parameters.\n\n6.3.9.3 Create Volume\n\nThe Create Volume operation creates a new storage volume. This operation\nsends the required capacity parameter as well as the optional snapshot_id\nand realm_id parameters. The response returns the same list of parameters as\nthe Show Volume Information operation.\n\n6.3.9.4 Delete Volume\n\nThe Delete Volume operation deletes a specific storage volume. This operation\nsends the id parameter in the request. The response returns the HTTP 204 No\nContent header.\n\n\n\n6.3. INTEROPERABLE SERVICE API 163\n\n6.3.9.5 Attach Volume\n\nThe Attach Volume operation attaches an existing storage volume to an instance.\nThis operation sends the id, instance_id and device parameters. The response\nreturns the same list of parameters as the Show Volume Information operation.\n\n6.3.9.6 Detach Volume\n\nThe Detach Volume detaches a specific storage volume from an instance. This\noperation sends the id parameter in the request. The response returns the same\nlist of parameters as the Show Volume Information operation.\n\n6.3.10 Snapshots\n\nThe Snapshots collection creates and manages storage volumes backups. This\ncollection is formed by the following operations: (i) List Snapshots; (ii) Show\nSnapshot Information; (iii) Create Snapshot; and (iv) Delete Snapshot.\n\n6.3.10.1 List Snapshots\n\nThe List Snapshots operation returns all the available storage volume snapshots.\nThis operation does not require request parameters. The response returns the\nlist of storage_snapshot parameters with the href and id attributes as well\nas the created and storage_volume sub-parameters.\n\n6.3.10.2 Show Snapshot Information\n\nThe Show Snapshot Information operation details the information of a specific\nstorage volume snapshot. This operation sends the id parameter in the request.\nThe response returns the storage_snapshot parameter with the href and id\nattributes as well as the created and storage_volume sub-parameters.\n\n6.3.10.3 Create Snapshot\n\nThe Create Snapshot operation creates a new backup for a defined storage volume.\nThis operation sends the volume_id parameter in the request. The response\nreturns the same list of parameters as the Show Snapshot Information operation.\n\n6.3.10.4 Delete Snapshot\n\nThe Delete Snapshot operation deletes a specific snapshot. This operation sends\nthe id parameter in the request. The response returns the HTTP 204 No Content\nheader.\n\n\n\n164 CHAPTER 6. PROJECT DEVELOPMENT\n\n6.3.11 Blobs\n\nThe Blob collection manages storage objects. This collection is formed by the\nfollowing operations: (i) List Buckets; (ii) Show Bucket Information; (iii) Create\nBucket; (iv) Delete Bucket; (v) Show Blob Information; (vi) Create Blob; (vii)\nDelete Blob; (viii) Show Blob Metadata; and (ix) Update Blob Metadata.\n\n6.3.11.1 List Buckets\n\nThe List Buckets operation enumerates all the available buckets. This operation\ndoes not require request parameters. The response returns a list of bucket\nparameters with the href and id attributes as well as the name and size sub-\nparameters.\n\n6.3.11.2 Show Bucket Information\n\nThe Show Bucket Information operation details the information of a specific\nbucket. This operation sends the bucket_id parameter in the request. The\nresponse returns the bucket parameter with the href and id attributes as well\nas the name, size and blob sub-parameters.\n\n6.3.11.3 Create Bucket\n\nThe Create Bucket operation creates a new bucket. This operation sends the\nrequired name parameter as well as the optional location parameter. The\nresponse returns the same list of parameters as the Show Bucket Information\noperation.\n\n6.3.11.4 Delete Bucket\n\nThe Delete Bucket operation deletes a specific bucket. This operation sends the\nbucket_id parameter in the request. The response returns a HTTP 204 No\nContent header.\n\n6.3.11.5 Show Blob Information\n\nThe Show Blob Information details the information of a specific blob. This\noperation sends the id parameter in the request. The response returns the\nblob parameter with a href and id attributes as well as the bucket, con-\ntent_length, content_type, last_modified, user_metadata and content\nsub-parameters.\n\n\n\n6.4. INTEROPERABLE SERVICE GUI 165\n\n6.3.11.6 Create Blob\n\nThe Create Blob operation creates or updates a blob object in a specific bucket.\nThis operation sends the required bucket_id, name, content_length, data\nand metadata parameters as well as the optional content_type parameter in\nthe request. The response returns the same list of parameters as the Show Blob\nInformation operation.\n\n6.3.11.7 Delete Blob\n\nThe Delete Blob operation deletes a blob object from a specific bucket. This\noperation sends the bucket_id and blob_id parameters in the request. The\nresponse returns the HTTP 204 No Content header\n\n6.3.11.8 Show Blob Metadata\n\nThe Show Blob Metadata operation details the metadata information of a specific\nblob. This operation sends the bucket_id and blob_id parameters in the\nrequest. The response returns the HTTP 204 No Content header as well as the\nX-Deltacloud-Blobmeta headers.\n\n6.3.11.9 Update Blob Metadata\n\nThe Update Blob Metadata operation overwrites the metadata information of a\nspecific blob. This operation sends the bucket_id and blob_id parameters in\nthe request. The response returns the HTTP 204 No Content header as well as\nthe X-Deltacloud-Blobmeta headers.\n\n6.4 Interoperable Service GUI\n\nThe Interoperable Service GUI consists of the Deltacloud Web dashboard. This\ndashboard presents the collections implemented by the loaded driver and exposes\na simple graphic interface to manage the back-end IaaS platform resources. The\nGUI service implements also a mechanism to switch between back-end drivers\nto access other platforms collections. This latter mechanism only works if the\nauthentication and back-end IaaS platforms endpoints are specified in the driver\nor in a YAML Ain\u2019t Markup Language (YAML) configuration file (as explained\nin the next section). The Figure 6.3 presents snapshots of the GUI service loaded\nwith the PACI driver.\n\n\n\n166 CHAPTER 6. PROJECT DEVELOPMENT\n\nFigure 6.3: Deltacloud GUI service.\n\n6.5 Deployment Configurations\n\nThe Deltacloud daemon deltacloudd is responsible for the start-up and deploy-\nment of the front-end interface services (the GUI and API services). The daemon\ncan be launched via a Linux terminal using the following syntax:\n$ deltacloudd ?i [ driver ID ] ?p [ port number ] ?P [ cloud provider endpoint URL]\n\nDeltacloud only requires the specification of thedriver ID option on the dae-\nmon initiation. If the server port and cloud provider endpoint URL are not\ndefined, Deltacloud will use the default configured port i.e., 3001 and it will as-\nsume that the cloud provider endpoint URL is set within the used driver. The\ncomplete list of options that can be used with the Deltacloud daemon are defined\nin the Table 6.1.\n\nDepending on the configuration of the Deltacloud daemon, two different de-\nployments modes can be implemented: (i) Single Tenant Configuration; and (ii)\nMultiple Tenant Configuration. The following sections will address these two\nDeltacloud deployments.\n\n6.5.1 Single Tenant Configuration\n\nThe Single Tenant Configuration deployment mode uses a single Deltacloud dae-\nmon that loads a pre-defined YAML file containing the credentials and the cloud\nprovider URL endpoint for each driver module. This mode disables the authen-\ntication on the exposed services, forwarding all request to the back-end cloud\nwith the specified credentials. This way, the user can switch between different\n\n\n\n6.6. REUSED DRIVER MODULES 167\n\nTable 6.1: Deltacloud daemon Operations.\n\nOptions Description\n-i \u2013driver [Driver ID] Specifies the driver to load.\n-r \u2013hostname [Hostname] Binds to a host address. The default host is localhost.\n-p \u2013port [Port] Defines the server port. The default is 3001.\n-P \u2013provider [Endpoint URL] Sets the cloud provider endpoint URL. The default is set in the\n\ndriver.\n-f \u2013frontends [Frontends] Enable different front-end API (cimi, ec2, deltacloud).\n-c \u2013config [File] Uses the credentials and the cloud provider endpoint URL from\n\nthe HAML files.\n-e \u2013env [Env] Specifies an environment variable.\n-d \u2013daemon To run the daemon in the background.\n-u \u2013user [User] Defines the user to run daemon as. Use with -d (default:\n\n\u201cnobody\u201d).\n-g \u2013group [Group] Defines the group to run daemon as. Use with -d (default:\n\n\u201cnobody\u201d).\n-b \u2013pid [File] File to store the server Process ID (PID).\n\nThe default file is tmp/pids/thin.pid.\n-l \u2013drivers Lists all the available drivers.\n-L \u2013log [File] Saves the log requests to a file. Option disabled by default.\n-s \u2013ssl Enables Secure Sockets Layer (SSL). It is disabled by default.\n-k \u2013ssl-key [Key] Defines the SSL key file to use.\n-C \u2013ssl-cert [File] Defines the SSL certificate file to use.\n-t \u2013timeout [Timeout] Defines the time-out for a single request. The default is 60\n\nseconds.\n-V \u2013verbose Sets the verbose logging on.\n-w \u2013webrick Forces the use of Ruby WEBRick server.\n\u2013logdir [Log dir] Defines the directory for the log files.\n-h \u2013help Shows deltacloudd options.\n\ncloud provider drivers without the need to restart the server daemon (to specify\nthe endpoint URL for each driver) or to include new HTTP headers in the API\ncall. This approach is insecure since anybody with access to the server running\nthe API and GUI services can use the defined credentials to manage the back-end\ncloud resources.\n\n6.5.2 Multiple Tenant Configuration\n\nThe Multiple Tenant Configuration uses multiple server instances, containing\neach the GUI and API services defined by a specific back-end driver module.\nThis mode is set-up by instantiating the deltacloudd daemon multiple times with\ndifferent driver, port and cloud provider endpoint URL combinations. In this\nmode, the services initiated in the started servers are fixed and the users must\nswitch between servers to manage different back-end IaaS platforms. Using this\ndeployment mode, users need to authenticate with each server to access the cor-\nresponding services. Thus, this approach is safer and enables load balancing.\n\n6.6 Reused Driver Modules\n\nThe tested driver modules were OpenNebula, OpenStack, CloudStack and PACI\ndrivers. While the OpenNebula and OpenStack drivers are officially provided\n\n\n\n168 CHAPTER 6. PROJECT DEVELOPMENT\n\nby Deltacloud, the CloudStack driver is not officially included in the framework.\nNevertheless, a third-party CloudStack driver is available. In the following sec-\ntions, the OpenNebula, OpenStack and CloudStack driver are described.\n\n6.6.1 OpenNebula Driver\n\nThe OpenNebula driver was created to support OpenNebula 3.X versions using\nthe OCCI API [182] and, since the OCCI API specification (draft 0.8) continues\nto be supported by OpenNebula, is expected that the OpenNebula driver remains\ncompatible with the most recent versions of OpenNebula.\n\nThe driver module is formed by three Ruby files: (i) cloud_client.rb; (ii)\nocci_client.rb; and (iii) opennebula_driver.rb. The cloud_client.rb implements\na generic CloudClient library with methods and classes to authenticate, perform\nand monitor HTTP calls. The occi_client.rb file implements an OCCI Client\nclass, composed of OCCI operations methods, to interact with the OpenNebula\nOCCI API. Whereas the opennebula_driver.rb file defines the OpennebulaD-\nriver class, which resorts to the CloudClient library and OCCI Client class to\nimplement the supported collections and operations compliant with Deltacloud\u2019s\nspecifications.\n\nThe OpenNebula driver supports the Realms, Hardware Profiles, Images and\nInstances collections. The Realms collection is implemented with fixed values,\nsince the OCCI API does not provide support for this functionality. The Hard-\nware Profiles collection makes use of OpenNebula OCCI Instance Type collection\nto implement the defined compute resource templates (small, medium and large).\nThe Images collection offers the following Image operations: (i) List Images;\n(ii) Show Image Information; and (iii) Delete Image. The Instances collection\nprovides the following Instance operations: (i) List Instances; (ii) Show Instance\nInformation; (iii) Create Instance; (iv) Start Instance; (v) Stop Instance; (vi)\nReboot Instance; and (vii) Delete Instance.\n\n6.6.2 OpenStack Driver\n\nThe OpenStack driver consists of a single file. The driver is implemented by the\nOpenstackDriver class, defined in the openstack_driver.rb Ruby file, that uses\nthe openstack rubygem (library) version 1.1.2 [183] to interact with the Keystone,\nNova, Cinder and Swift projects.\n\nThis driver implements the Realms, Hardware Profiles, Images, Instances,\nKeys, Volumes, Snapshots and Blobs collections. Since the concept of Realm\nis undefined in the OpenStack projects, only the information about user limits\ncan be obtained from the Nova project API. This way, the Realms collection\nis implemented with the limits information obtained from the Nova project and\nfixed values for the name and identification definition of the realm. The Hardware\n\n\n\n6.7. DEVELOPED PACI DRIVER 169\n\nProfiles collection uses and exposes the OpenStack Nova \u201cflavors\u201d (VM resource\ntemplates). The Images collection exposes the following operations: (i) List\nImages; (ii) Show Image Information; (iii) Create Image from Instance; and (iv)\nDelete Image. The Instances collection offers the following operations: (i) List\nInstances; (ii) Show Instance Information; (iii) Create Instance; (iv) Reboot\nInstance and (v) Delete Instance. The Keys collection provides the following\noperations: (i) List Keys; (ii) Show Key Information; (iii) Create Key; and (iv)\nDelete Key. The Volumes and Snapshots collections interact with the Cinder\nproject API. The Volumes collection implements the following operations: (i) List\nVolumes; (ii) Show Volume Information; (iii) Create Volume; (iv) Delete Volume;\n(v) Attach Volume; and (vi) Detach Volume. The Snapshots collection includes\nthe following operations: (i) List Snapshots; (ii) Show Snapshot Information;\n(iii) Create Snapshot; and (iv) Delete Snapshot. The Blobs Collection interacts\nwith the Swift project and exposes the following operations: (i) List Buckets; (ii)\nShow Bucket Information; (iii) Create Bucket; (iv) Delete Bucket; (v) Show Blob\nInformation; (vi) Create Blob; (vii) Delete Blob; (viii) Obtain Blob Metadata;\nand (ix) Update Blob Metadata.\n\n6.6.3 CloudStack Driver\n\nThe Deltacloud 1.1.3 version does not include a CloudStack driver. However,\nChip Childers, a member of the Apache Software Foundation (ASF) involved in\nthe Apache CloudStack project, started the development of a CloudStack driver\nfor Deltacloud (available here [184]). This third-party driver was tested (Chapter\n8 - Test, Debugging and Validation) without success.\n\n6.7 Developed PACI Driver\n\nThere were no PACI driver implementations available for use with the Deltacloud\nframework. Therefore, to integrate Lunacloud\u2019s proprietary IaaS platform with\nDeltacloud, a dedicated PACI driver, compliant with the DeltaCloud framework,\nhad to be designed and developed. The PACI IaaS platform operations were not\ndefined in the existing rubygem libraries, meaning that a PACI client had also\nto be created to enable the communication with the back-end LunaCloud PACI\nAPI. This way, by relying on the available driver specification information [185]\nprovided by Deltacloud as well as by analysing the existing drivers code, a new\nPACI driver was developed.\n\nFollowing the OpenNebula driver module approach, the PACI driver imple-\nmentation consists of three Ruby files: (i) cloud_client.rb; (ii) paci_client.rb;\nand (iii) paci_driver.rb. The cloud_client.rb file, which was inspired on the\nOpenNebula driver module to exploit the already defined methods and classes,\nperforms and monitors the HTTP calls implemented by theCloudClient library.\n\n\n\n170 CHAPTER 6. PROJECT DEVELOPMENT\n\nThe paci_client.rb Ruby file was created to implement the PACI client, which\nconsists of a Client class with defined methods to interact with the Lunacloud\nPACI API. The paci_driver.rb Ruby file contains the PaciDriver class, which is\nresponsible for the implementation of the PACI driver. This class inherits the\nfeatures and properties (methods and attributes) from the BaseDriver superclass,\ndefined in the base_driver.rb file for the definition of Deltacloud collections and\noperations. Figure 6.4 represents a Unified Model Language (UML) class diagram\nof the developed PACI driver.\n\nFigure 6.4: UML classes diagram of the PACI driver.\n\nThe development of the PACI driver module was conducted in two main\nphases. The fist phase consisted on the development of the PACI client, while\nthe second phase focussed on the development and implementation of the PACI\ndriver. The PACI client was defined with methods to describe and implement\nthe PACI API HTTP requests, enabling the support for the following PACI\nAPI operations: (i) List Servers; (ii) Start/Stop Server; (iii) Create Server; (iv)\nCreate Server From Image; (v) Obtain Server Information; (vi) Delete Server;\n(vii) List Images; (viii) Get Image Information; (ix) Create Image From Server;\n(x) Delete Image; (xi) List Installed OS Templates; (xii) List Load Balancers;\n(xiii) Get Load Balancer Information; (xiv) Create Load Balancer; (xv) Delete\nLoad Balancer; (xvi) Attach Server to Load Balancer; and (xvii) Detach Server\nFrom Load Balancer.\n\n\n\n6.7. DEVELOPED PACI DRIVER 171\n\nThe methods that implement the described operations comply with the PACI\nRESTful API documentation [142]. The Code Snippet 6.1 presents the PACI\nclient Create Server operation, which is defined by the create_instance method:\n\nCode Snippet 6.1: PACI client create_instance method\n\n1 def create_instance (xml)\n2 post ( \u2019 /ve \u2019 , xml)\n3 end\n\nThe required parameters are passed through the method arguments. The\nmethod invokes the post private method responsible for processing the HTTP\nPOST verb. The PACI client, in the Client class, has private methods that\nimplement the processing of the GET, POST, DELETE and PUT HTTP verbs.\nThe Code Snippet 6.2 presents the description of the post method:\n\nCode Snippet 6.2: PACI client post method\n\n1 def post ( path , xml=n i l )\n2 url = URI . parse ( @endpoint+path )\n3 req = Net : :HTTP: : Post . new( url . path )\n4 i f ! xml . n i l ?\n5 req . content_type= \u2019 application /xml \u2019\n6 req . body=xml . to_s\n7 end\n8 do_request ( url , req )\n9 end\n\nThis method has two input arguments: the path argument containing the\npath for the operation and the xml argument with the description of the HTTP\nrequest body (since the PACI API uses the XML notation for the HTTP mes-\nsage body). The url object from the URI::HTTP class is created, using the\nURI.parse() method, by concatenating the endpoint and path strings. Then, the\nNET::HTTP::POST superclass is instantiated to define the HTTP request (req)\nobject. If the operation uses a HTTP body, the Multi-purpose Internet Mail\nExtension (MIME) type is specified using the Content-Type: application/xml\nHTTP header field and the xml argument string with the XML description is\nincluded to the HTTP request body. Once the request is created, the do_request\nmethod performs the request. The do_request private method code is presented\nin the Code Snippet 6.3:\n\nThe do_request invokes the http_start method from the CloudClient library\nin a Ruby block, passing the HTTP request information. The http_start method\nestablishes the HTTP connection, sending the HTTP request and collecting the\nHTTP server response. The HTTP connection is monitored using the Cloud-\nClient error class. The complete description of the CloudClient library and the\n\n\n\n172 CHAPTER 6. PROJECT DEVELOPMENT\n\nCode Snippet 6.3: PACI client do_request method\n\n1 def do_request ( url , req )\n2 req . basic_auth @auth [ 0 ] , @auth [ 1 ]\n3\n4 res = CloudClient : : http_start ( url , @timeout ) do | http |\n5 http . request ( req )\n6 end\n7\n8 i f CloudClient : : is_error ?( res )\n9 return res\n\n10 e l s e\n11 return res . body\n12 end\n13 end\n\nClient class from the PACI client are available in the Appendix A and Appendix\nB respectively.\n\nThe PACI driver implements the Realms, Hardware Profiles, Images, In-\nstances and Load Balancers collections and exposes the following operations:\n(i) List Realms; (ii) Show Realm Information; (iii) List Hardware Profiles; (iv)\nShow Hardware Profile Information; (v) List Instances; (vi) Show Instance In-\nformation; (vii) Create Instance; (viii) Start Instance; (ix) Stop Instance; (x)\nDelete Instance; (xi) List Images; (xii) Show Image Information; (xiii) List Load\nBalancers; (xiv) Show Load Balancer Information; (xv) Create Load Balancer;\n(xvi) Delete Load Balancer; (xvii) Attach Instance to Load Balancer; and (xviii)\nDetach Instance From Load Balancer. The operations described are implemented\nusing methods inherited from the BaseDriver superclass.\n\nThe Realms collection specifies Lunacloud existing domains (EU Central and\nEU West). Since the PACI API does not provide such operations, the val-\nues that define the Realms collection are static. Thus, PACI does not use re-\nsource templates. The virtual machine resources are defined in the XML de-\nscription request body of the Create Server operation. However, Deltacloud re-\nsorts to resource templates for the creation of instances. This way, the PACI\ndriver uses the define_hardware_profile methods from the Hardware Profiles\ncollection to implement resource templates. The code snippet 6.4 presents the\ndefine_hardware_profile method used to create a custom hardware profile:\n\nCode Snippet 6.4: PACI driver define_hardware_profile method\n\n1 define_hardware_profile ( \u2019Custom \u2019 ) do\n2 cpu (1 . . 8)\n3 memory (512 . . 192?512)\n4 storage (10 . . 2000)\n5 end\n\nThis hardware profile corresponds to an unspecified resource template that\n\n\n\n6.7. DEVELOPED PACI DRIVER 173\n\nallows the usage of custom VM CPU, RAM and Storage resources with pre-\ndefined size ranges. The range was implemented according to the Lunacloud\nlimits specifications for the CPU, RAM and Storage resources [186]. Other hard-\nware profiles were created, using the define_hardware_profile method, based on\nLunaCloud\u2019s resource configurations (e.g., PointFive, One, Two, Four, Eight and\nOneSix).\n\nThe Images collection is defined to use PACI OS templates. These OS tem-\nplates are responsible for the configuration and installation of the virtual machine\nOS at the time of the VM creation. The OS templates are implemented and man-\naged by the cloud provider (e.g., LunaCloud) and are available to multiple users\non the data centre. Thus, since the OS templates cannot be created or destroyed\nby normal users, only read operations can be implemented from the Deltacloud\nImages collection (i.e., the List Images and Show Image Information operations).\n\nThe Instances collection implements all instance operations with the excep-\ntion of the reboot operation since is not included in the PACI platform. The\nonly requirement for the creation of an instance using the Deltacloud is the in-\nstantiation of an OS Image. The hardware profile as well as the instance name,\nrealm and defined password are optional. However, these instantiations are insuf-\nficient to create a new VM in the PACI IaaS platform. The PACI API requires\nthat an additional set of parameters are specified in XML format. Therefore, to\nenable the creation of virtual machines in the LunaCloud PACI platform using\nDeltacloud, the following XML template (Code Snippet 6.5) was defined as a\nglobal VE_TEMPLATE string in the PACI driver class:\n\nCode Snippet 6.5: PACI driver XML server template\n\n1 VE_TEMPLATE = \\%q{\n2&lt;ve>\n3&lt;name><%=ve_name%></name>\n4&lt;cpu number=\"<%=opts [ : hwp_cpu]%>\" power=\"1600\"/>\n5&lt;ram?size><%=opts [ : hwp_memory ] . to_i%></ram?size >\n6&lt;bandwidth >10240</bandwidth>\n7&lt;no?of?public ?ip >1</no?of?public ?ip>\n8&lt;no?of?public ?ipv6 >0</no?of?public ?ipv6>\n9&lt;ve?disk l o c a l =\"true \" s i z e=\"<%=opts [ : hwp_storage]%>\"/>\n\n10&lt;platform>\n11&lt;template?i n f o name=\"<%=image_id%>\"/>\n12&lt;os?i n f o technology=\"<%=tech%>\" type=\"<%=type%>\"/>\n13&lt;/platform>\n14&lt;backup?schedule name=\"weekly\"/>\n15&lt;% i f opts [ : password ] %>\n16&lt;admin login =\"root \" password=\"<%=opts [ : password]%>\"/>\n17&lt;% e l s e %>\n18&lt;admin login =\"root \" />\n19&lt;% end %>\n20&lt;/ve>\n21 }\n\nThis template uses a XML structure, compliant with the PACI API docu-\nmentation, that contains embedded Ruby code. This approach allows the imple-\n\n\n\n174 CHAPTER 6. PROJECT DEVELOPMENT\n\nmentation of Deltacloud defined parameters (e.g., image_id and opts[] variables)\nas well as the inclusion of other parameters and attributes that must be specified\n(e.g., the cpu power attribute or the bandwidth parameter). The no-of-public-ip,\nno-of-public-ipv6, bandwidth and backup-schedule parameters as well as the ad-\nmin login attribute were implemented with default values since Deltacloud does\nnot support these features.\n\nThe Load Balancer collection implements all Load Balancer operations. How-\never, since the Deltacloud Load Balancer operations require more parameters\nthan those required by the PACI platform, the extra parameters are initialised\nwith mock values.\n\nThe PACI driver operations were defined by overriding the BaseDriver su-\nper class corresponding methods. For example, the Create Instance operation,\noverrides the create_instance(...) BaseDriver method. The implemented cre-\nate_instance(credentials, image_id, opts={}) method is presented in the Code\nSnippet 6.6:\n\nCode Snippet 6.6: PACI driver create_instance method\n\n1 def create_instance ( credentials , image_id , opts ={})\n2\n3 paci_client = new_client ( c r e d e n t i a l s )\n4\n5 i f opts [ : name ] &amp;&amp; opts [ : name ] . length >0\n6 ve_name= opts [ : name ]\n7 e l s e\n8 time=Time . now . to_s\n9 time=time . s p l i t ( \u2019+\u2019 ) . f i r s t\n\n10 time=time . gsub (/\\D/ , \u2019 \u2019 )\n11 ve_name= \" Server ?\"+time\n12 end\n13\n14 img_xml = treat_response ( paci_client . get_os_template ( image_id ) )\n15 buffer = REXML: : Document . new(img_xml . to_s ) . root . a t t r i b u t e s\n16 tech = buffer [ \u2019 technology \u2019 ]\n17 type = buffer [ \u2019 osType \u2019 ]\n18 req_xml = ERB. new(VE_TEMPLATE) . r e s u l t ( binding )\n19 treat_response ( paci_client . create_instance ( req_xml ) )\n20\n21 instance ( credentials , id : ve_name)\n22 end\n\nThis method obtains the parameters required to fill the PACI XML template,\nrequests the creation of an instance to the back-end PACI platform and shows\nthe created instance in front-end Deltacloud interfaces. Like all the other defined\nmethods that implement operations, this method instantiates the new_client\nprivate method to create a PaciClient object in order to use the PACI client\nmethods. Then, the create_instance method requests the PACI platform for\nthe information regarding a specific OS template, using treat_response private\nmethod for monitoring, and invokes the PACI client get_os_template(image_id)\nmethod to perform the HTTP call. The returned response is parsed to obtain\n\n\n\n6.7. DEVELOPED PACI DRIVER 175\n\nthe required attributes, using the Ruby Electric XML (REXML) superclass [187]\nand the Embedded Ruby (ERB) superclass [188], and the XML request body\nis created. Once again, the treat_response private method is used, with the\ncreate_instance(req_xml) PACI client method, to create a new VM. The re-\nsponse returned by the PACI API is discarded since it contains only informa-\ntion about the created password (Lunacloud sends this information to the user\ne-mail). Finally, in order to show the new instance, it is necessary to invoke\nthe instance(credentials, id: ve_name) method, passing the identification of the\ncreated instance. This latter method was defined (overriden) to implement the\nDeltacloud Show Instance Information operation.\n\nThe information exposed by Deltacloud in the front-end API and GUI inter-\nfaces is filtered from the PACI API returned responses. This process is imple-\nmented by private methods that receive the returned XML body and map its\nparameters and attributes. The Code Snippet 6.7 presents the convert_image\nmethod:\n\nCode Snippet 6.7: PACI driver convert_image method\n\n1 def convert_image (xml , c r e d e n t i a l s )\n2 buffer = REXML: : Document . new(xml . to_s ) . root\n3 i f buffer . a t t r i b u t e s [ \u2019 active \u2019]==\" f a l s e \"\n4 default_state = \"DISABLED\"\n5 e l s e\n6 default_state = \"ACTIVE\"\n7 end\n8\n9 Image . new({\n\n10 : id=>buffer . a t t r i b u t e s [ \u2019name \u2019 ] ,\n11 : name=>buffer . a t t r i b u t e s [ \u2019name \u2019 ] ,\n12 : de sc r ip ti on=>\"OS: \"+buffer . elements [ 2 ] . a t t r i b u t e s [ \u2019 value \u2019 ]+ \" ,\n13 V i r t u a l i z a t i o n type (VM/CT) : \"+buffer . a t t r i b u t e s [ \u2019 technology \u2019 ] ,\n14 : owner_id=>\" LunaCloud \" ,\n15 : state=>default_state ,\n16 : a r c h i t e c t u r e=>buffer . elements [ 1 ] . a t t r i b u t e s [ \u2019 value \u2019 ] ,\n17 : hardware_profiles=>hardware_profiles ( n i l )\n18 })\n19 end\n\nThe convert_image method, which retrieves image-related information, uses\nthe REXML superclass to parse the XML from the xml argument and to obtain its\nparameters and attributes. This information is, then, included in the Deltacloud\nImage objects. The Image objects are represented in hash tables containing\nthe id, name, description, owner_id, state, architecture and hardware_profiles\nkeys. The value defined in these keys is the information exposed by Deltacloud\ninterfaces.\n\nAnother important method defined in the PACI driver is the\ndefine_instance_states method that implements a state machine for the virtual\nmachine life-cycle of a PACI VM. This method is presented in the Code Snippet\n6.8:\n\n\n\n176 CHAPTER 6. PROJECT DEVELOPMENT\n\nCode Snippet 6.8: PACI driver define_instance_states method\n\n1 define_instance_states do\n2 s t a r t . to ( : pending ) . on ( : create )\n3 pending . to ( : stopped ) . automatically\n4 stopped . to ( : pending ) . on ( : destroy )\n5 pending . to ( : f i n i s h ) . automatically\n6 stopped . to ( : running ) . on ( : s t a r t )\n7 running . to ( : stopping ) . on ( : stop )\n8 stopping . to ( : stopped ) . automatically\n9 running . to ( : stopping ) . on ( : destroy )\n\n10 stopping . to ( : f i n i s h ) . automatically\n11 end\n\nWhen a VM is created, it is by default in the stopped state. Its state changes\nto pending when a destroy VM instruction is received, running when a start VM\ninstruction is received and stopping when a stop VM instruction is received. The\nstate changes from pending to stopped, pending to finish, stopping to stopped\nand stopping to finish automatically. The names for the VM states used by PACI\nare different from the ones defined by Deltacloud and needed to be changed to\ncomply with the Deltacloud defined states. Thus, the VE_STATES hash was\nimplemented - see Code Snippet 6.9.\n\nCode Snippet 6.9: PACI driver VE_STATES hash\n\n1 VE_STATES = {\n2 \"CREATE\" => \"START\" ,\n3 \"CREATION_IN_PROGRESS\" => \"PENDING\"\n4 \"CREATED\" => \"STOPPED\" ,\n5 \"START_IN_PROGRESS\" => \"RUNNING\" ,\n6 \"STARTED\" => \"RUNNING\" ,\n7 \"STOP_IN_PROGRESS\" => \"STOPPING\" ,\n8 \"STOPPED\" => \"STOPPED\" ,\n9 \"DELETE_IN_PROGRESS\" => \"STOPPING\" ,\n\n10 \"DELETED\" => \"FINISHED\"\n11 }\n\nUsing the state information of an instance and the defined state machine,\nDeltacloud can determine which instance actions can be performed. If the in-\nstance is STOPPED, the START action is \u201crecommended\u201d and, since the\ndefine_instance_states method is defined to change from start to stop and stop\nto destroy, an instance cannot be destroyed when its state is STARTED. The\nPaciDriver class containing the description of the PACI driver code is provided\nin the Appendix C.\n\nFinally, so that the PACI driver can be loaded by the Deltacloud daemon and\nrecognized by the front-end services, a paci.yaml YAML configuration file was\ndefined containing the following configuration:\n\n\n\n6.8. CONCLUSIONS 177\n\n???\n: paci :\n\n: name PACI\n\n6.8 Conclusions\n\nThe development environment provides support for the system back-end and\nfront-end. The back-end is provided by Deltacloud and the front-end consists of\nthe standard Web browser or the cURL CLI. The development support language\nis Ruby and the application layer protocol is HTTP. The dynamic Web pages\ntemplates are defined in HAML and the data output formats supported by API\ninclude XML, JSON and HTML.\n\nThe development of the Interoperable Service consisted on the integration of\nthe Deltacloud abstraction framework with the proprietary IaaS platform used\nby LunaCloud and the most popular open-source IaaS platforms: OpenNebula,\nOpenStack and CloudStack. The utilization of Deltacloud permitted the adoption\nof a single management syntax, using its RESTful API (or the CIMI and EC2\nAPI) and provided a simple Web dashboard to interact with different back-end\nIaaS platforms as well as a secure and managed access to the exposed services\nwith the possibility of a multiple tenant deployment.\n\nThe creation of the PACI driver followed the documentation provided by\nDeltacloud, which proved to be insufficient and outdated, and the code from\nother driver implementations, mainly the OpenNebula driver and the EC2 driver\ncode. Along with the PACI driver, it was also necessary to develop a PACI client\ncontaining the description of PACI API operations. From the analysis conducted\nbetween the offered Deltacloud collections and the group of operations exposed\nby the PACI API, it was decided to develop a driver with the Realms, Hardware\nProfiles, Images, Instances and Load Balancers collections. Since the Deltacloud\nAPI required parameters did not always match with the required PACI API\nparameters, e.g., when creating a new VM, it was necessary to perform multiple\ncalls to the back-end API as well as to define default and mock values for these\nparameters. This approach may lead to lack of front-end customization (using\nthe Deltacloud interfaces) and performance deterioration. Nevertheless, the de-\nveloped PACI driver is completely functional and can execute all the implemented\noperations.\n\n\n\n\n\nChapter 7\n\nTest, Debugging and Validation\n\nThis chapter describes the test bed set-up, the tests performed to evaluate the\ndeveloped Interoperable Service solution and the presentation, interpretation and\ndiscussion of the obtained results.\n\n7.1 Test Bed\n\nTo conduct the functional tests of the developed Interoperable Service, the Open-\nNebula, OpenStack and CloudStack IaaS platforms as well as the Deltacloud\nframework were installed and configured. For this purpose, a test bed containing\nthe following components was assembled:\n\n\u2022 OpenNebula cloud node;\n\n\u2022 OpenStack cloud node;\n\n\u2022 CloudStack cloud node;\n\n\u2022 Interoperable Service node;\n\n\u2022 Test network with access to the Internet.\n\nTo deploy the OpenNebula, OpenStack and CloudStack nodes two desktop\ncomputers with a Intel Pentium G3220 CPU (virtualization enabled), 4096 MB\nRAM, 500 GB of disk storage capacity and 1 NIC were used. Since there were\nno more platforms with enabled virtualization capabilities available, the hard\ndisk drive of one PC was partitioned to accommodate both the OpenStack and\nCloudStack IaaS platforms. To ensure identical deployment conditions for all\n\n179\n\n\n\n180 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nIaaS platforms, the Linux CentOS 6.5 OS was installed in the three systems.\nThe PACI IaaS platform node is provided and supported by Lunacloud.\n\nThe node containing the Interoperable Service was deployed in a laptop with\nthe Linux Mint 16 OS with the Deltacloud framework (containing the Open-\nNebula and OpenStack driver modules) and the PACI driver module. Since the\nlaptop was also used as the client platform to access the other set-up nodes,\nthe Deltacloud server was configured to run in the loopback network. All the\nnodes of the test bed were connected through a test network (192.168.1.0/24)\nusing a router that isolates the components from the laboratory network and\nprovides Internet access to the PACI IaaS platform. All nodes were connected\nto the router via Ethernet crossover cables and all computer clocks where con-\nfigured to synchronise with Lunacloud\u2019s Network Time Protocol (NTP) server\n(ntp.lunacloud.com). The Figure 7.1 illustrates the assembled test bed.\n\nFigure 7.1: Test bed platform.\n\nThe following sections describe the test bed installation procedures, including\nthe Interoperable service and the OpenNebula, OpenStack and CloudStack IaaS\nplatforms in the referred nodes.\n\n7.1.1 Interoperable Service Installation\n\nThe installation of the Interoperable service is divided in three steps: (i) Install-\nation of Deltacloud framework; (ii) Inclusion of the CloudStack driver module;\nand (iii) Inclusion of the PACI driver module.\n\n\n\n7.1. TEST BED 181\n\n7.1.1.1 Deltacloud Installation\n\nThe installation of Deltacloud consists of the installation of the required depend-\nencies to run the Deltacloud server and the installation of the Deltacloud service.\n\nDeltacloud requires the following dependencies:\n\n\u2022 Ruby, Ruby-devel and RubyGems;\n\n\u2022 GCC-C++, Libxml2, Libxml2-devel, Libxslt, Libxslt-devel, sqlite and sqlite-\ndevel libraries;\n\n\u2022 Thin, sinatra, rack-accept, rest-client, sinatra-content-for, and nokogiri gems.\n\nThe Ruby version 1.9.1, Ruby-devel and RubyGems can be installed with the\nfollowing command:\n# a p t ?g e t i n s t a l l r u b y 1 . 9 . 1 ? f u l l\n\nNext, the required libraries to build RubyGems with C extensions and for the\nCIMI persistence layer are installed:\n# a p t ?g e t i n s t a l l g++\n# a p t ?g e t i n s t a l l l i b x m l ++2.6? d e v l i b x m l 2 ?d e v\n# a p t ?g e t i n s t a l l l i b x s l t 1 . 1 l i b x s l t ?d e v\n# a p t ?g e t i n s t a l l s q l i t e l i b s q l i t e 3 ?d e v\n\nThen, the required gems are set using RubyGems:\n# gem i n s t a l l t h i n s i n a t r a r a c k ?a c c e p t r e s t ? c l i e n t \\\nsinatra ?content ?for nokogiri\n\nand a symbolic link is created for the service launcher:\n# l n ?s / d e l t a c l o u d ?c o r e ? 1 . 1 . 3 / b i n / d e l t a c l o u d d \\\n/ usr / bin / deltacloudd\n\nFinally, with all dependencies resolved, the Deltacloud is installed using Ruby-\nGems:\n# gem i n s t a l l d e l t a c l o u d ?c o r e\n\n7.1.1.2 CloudStack Driver Module Inclusion\n\nTo include the third-party CloudStack driver module in the Deltacloud frame-\nwork it was necessary to move the obtained cloudstack_driver.rb and cloud-\nstack_driver_cimi_methods.rb files to a created cloudstack folder inside the\nDeltacloud drivers folder:\n# mkdir / d e l t a c l o u d ?c o r e ? 1 . 1 . 3 / l i b / d e l t a c l o u d / d r i v e r s / c l o u d s t a c k\n# mv c l o u d s t a c k _ d r i v e r . r b c l o u d s t a c k _ d r i v e r _ c i m i _ m e t h o d s . r b \\\n/ deltacloud ?core ?1.1.3/ l i b / deltacloud / d r i v e r s / cloudstack /\n\n\n\n182 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nThen, it was necessary the creation of a cloudstack.yaml file containing the\nfollowing information:\n???\n: cloudstack :\n\n: name Cloudstack\n\nFinally, the cloudstack.yaml file needed to be moved to the drivers configur-\nation folder:\n# mv c l o u d s t a c k . yaml / d e l t a c l o u d ?c o r e ? 1 . 1 . 3 / c o n f i g / d r i v e r s /\n\n7.1.1.3 PACI Driver Module Inclusion\n\nTo include the developed PACI driver module in the Deltacloud framework it was\nnecessary to move the cloud_client.rb, paci_client.rb and paci_driver.rb files to\na created paci folder inside the Deltacloud drivers folder:\n# mkdir d e l t a c l o u d ?c o r e ? 1 . 1 . 3 / l i b / d e l t a c l o u d / d r i v e r s / p a c i\n# mv c l o u d _ c l i e n t . r b p a c i _ c l i e n t . r b p a c i _ d r i v e r . r b \\\n/ deltacloud ?core ?1.1.3/ l i b / deltacloud / d r i v e r s / paci /\n\nThen, the paci.yaml configuration file needed to be moved to the Deltacloud\ndrivers configuration folder:\n# mv p a c i . yaml / d e l t a c l o u d ?c o r e ? 1 . 1 . 3 / c o n f i g / d r i v e r s /\n\n7.1.2 OpenNebula Installation\n\nThe OpenNebula installation is divided in four main procedures: (i) OS Config-\nuration; (ii) OpenNebula Services Installation; (iii) KVM Hypervisor Installation\nand (iv) Virtual Resources Allocation.\n\n7.1.2.1 OS Configuration\n\nThe OS Configuration procedure prepares the host OS for the OpenNebula in-\nstallation. This procedure configures the OS network service for the OpenNebula\nusage and sets the OpenNebula repository to download and install automatically\nthe required software packages.\n\nThe OS NIC eth0 has to be configured as a network bridge (e.g, br0) that\nwill act as the public interface for the KVM virtual machines.\n\nThe eth0 interface can be configured by replacing /etc/sysconfig/network-scripts/\nifcfg-eth0 file with:\nDEVICE=eth0\nBOOTPROTO=none\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Ethernet\nBRIDGE=br0\n\n/etc/sysconfig/network-scripts/ifcfg-eth0\n/etc/sysconfig/network-scripts/ifcfg-eth0\n\n\n7.1. TEST BED 183\n\nThen, a new ifcfg-br0 file needs be added to the /etc/sysconfig/network-scripts/\nnetwork-scripts/ folder with the following configuration:\n# c a t&lt;&lt;EOT > / e t c / s y s c o n f i g / network?s c r i p t s / network?s c r i p t s / i f c f g ?b r 0\nDEVICE=br0\nTYPE=Bridge\nIPADDR=192.168.1.16\nNETMASK=255.255.255.0\nONBOOT=yes\nBOOTPROTO=s t a t i c\nNM_CONTROLLED=no\nEOT\n\nAfter the network bridge is configured, the OS network service is restarted\nfor the changes to take effect:\n# s e r v i c e n e t w o r k r e s t a r t\n\nOnce the OS network service configured, the OpenNebula repository can be\ncreated to get and install the necessary software packages for the installation of\nOpenNebula:\n# c a t&lt;&lt;EOT > / e t c /yum . r e p o s . d/ o p e n n e b u l a . r e p o\n[ opennebula ]\nname=opennebula\nbaseurl=http :// downloads . opennebula . org / repo /CentOS/6/ stable /\\ $basearch\nenabled=1\ngpgcheck=0\nEOT\n\n7.1.2.2 OpenNebula Services Installation\n\nThe OpenNebula Services packages consist of the opennebula-server, opennebula-\nsunstone and opennebula-ruby. The opennebula-server package contains the\nmain OpenNebula daemon, scheduler and other core functions. The opennebula-\nsunstone package contains the Sunstone Web dashboard as well as the EC2 and\nOCCI interface servers. The opennebula-ruby package contains the necessary\nRuby bindings for the OpenNebula usage. These services can be installed with\nthe following command:\n# yum i n s t a l l o p e n n e b u l a?s e r v e r o p e n n e b u l a?s u n s t o n e o p e n n e b u l a?r u b y\n\nThe installation creates the oneadmin user with a random password located\nat ~/.one/one_auth as well as a SSH key.\n\nThe SSH key allows the oneadmin user to access the virtualization nodes\nwithout inserting the password. This is achieved by granting the user with read\nand write privileges over the ~/.ssh/config file:\n# chmod 600 ~ / . s s h / c o n f i g\n\nand by specifying the following ~/.ssh/config file configuration:\n\n/etc/sysconfig/network-scripts/network-scripts/\n/etc/sysconfig/network-scripts/network-scripts/\n~/.one/one_auth\n~/.ssh/config\n~/.ssh/config\n\n\n184 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nHost ?\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/ null\n\nOpenNebula also provides a script that installs any required Ruby gems as\nwell as some development libraries packages. This script can be executed from\nthe /usr/share/one/ directory :\n# cd / u s r / s h a r e / one\n# . / i n s t a l l _ g e m s\n\nThe Sunstone and OCCI servers listen, by default, to the loopback inter-\nface. To access these services from other machines in the network it is ne-\ncessary to edit /etc/one/sunstone-server.conf and /etc/one/occi-server.\nconf server configuration files and change the :host: 127.0.0.1 loopback address\nto the defined fixed IP address, e.g., :host: 192.168.1.16. Then, to enable the\naccess to these services it is necessary to add the following iptables rules:\n# i p t a b l e s ?I INPUT ?p t c p ??d p o r t 9869 ?m s t a t e \\\n??state=NEW,ESTABLISHED,RELATED ?j ACCEPT\n# i p t a b l e s ?I INPUT ?p t c p ??d p o r t 4567 ?m s t a t e \\\n??state=NEW,ESTABLISHED,RELATED ?j ACCEPT\n\n# s e r v i c e i p t a b l e s s a v e\n# s e r v i c e i p t a b l e s r e s t a r t\n\nAfter installing and configuring OpenNebula\u2019s services, it is necessary to\nlaunch them and to configure boot startup:\n# s e r v i c e o p e n n e b u l a s t a r t\n# s e r v i c e m e s s a g e b u s s t a r t\n# s e r v i c e o p e n n e b u l a?s u n s t o n e s t a r t\n# o c c i ?s e r v e r s t a r t\n# c h k c o n f i g o p e n n e b u l a on\n# c h k c o n f i g m e s s a g e b u s on\n# c h k c o n f i g m e s s a g e b u s o p e n n e b u l a?s u n s t o n e on\n\n7.1.2.3 KVM Hypervisor Installation\n\nThe KVM Hypervisor Installation procedure is done by installing the opennebula-\nnode-kvm package:\n# yum l o c a l i n s t a l l o p e n n e b u l a?node?kvm\n\nThis package installs the KVM hypervisor and the necessary configuration\nfiles to work with the oneadmin user. After the installation and configuration\nof the hypervisor, it is necessary to start and configure the libvirt service boot\nstartup:\n# s e r v i c e l i b v i r t d s t a r t\n# c h k c o n f i g l i b v i r t d on\n\n/etc/one/sunstone-server.conf\n/etc/one/occi-server.conf\n/etc/one/occi-server.conf\n\n\n7.1. TEST BED 185\n\n7.1.2.4 Virtual Resources Allocation\n\nPrior to running virtual machines with OpenNebula, it is necessary to create a\nvirtual network, an image and a virtual machine template.\n\nThe OpenNebula virtual networks are based on Linux bridging and they are\ndivided into Fixed and Ranged. A Fixed virtual network has fixed IP addresses\nwhile a Ranged virtual network has a pre-defined network address and number\nof IP addresses. A Fixed virtual network with ten IP addresses is created, using\na network template, i.e., a template file (MyNetwork.one), with the following\nconfiguration:\n# c a t&lt;&lt;EOT > MyNetwork . one\nNAME = \" private \"\nTYPE = FIXED\n\nBRIDGE = br0\n\nLEASES = [ IP =192.168.1.110 ]\nLEASES = [ IP =192.168.1.111 ]\nLEASES = [ IP =192.168.1.112 ]\nLEASES = [ IP =192.168.1.113 ]\nLEASES = [ IP =192.168.1.114 ]\nLEASES = [ IP =192.168.1.115 ]\nLEASES = [ IP =192.168.1.116 ]\nLEASES = [ IP =192.168.1.117 ]\nLEASES = [ IP =192.168.1.118 ]\nLEASES = [ IP =192.168.1.119 ]\n\nEOT\n\nOnce the virtual network template has been defined, the virtual network can\nbe created by the oneadmin user using Opennebula CLI command onevnet:\n$ onevnet create mynetwork . one\n\nThe OS image used is a CentOS-6.4_x86_64 OS image in the QEMU QEMU\nCopy on Write Version 2 (QCOW2) image format, which is supported by the\nQuick Emulator (QEMU) virtual machine monitor. This OS image was registered\nwith OpenNebula in the standard Linux FS using the CLI command oneimage\nas oneadmin user:\n$ oneimage create \\\n\n??name \"CentOS?6.4_x86_64\" \\\n??path \" http :// us . cloud . centos . org / i /one/c6?x86_64 ?20130910 ?1.qcow2 . bz2 \" \\\n??driver qcow2 \\\n??datastore default\n\nThe image, which is stored in the provided path, remains in locked state until\nit is ready for use.\n\nThe virtual machine template defines the virtual machine with the specified\nname, CPU, RAM and also attaches the disk (in this case the registered image),\nthe network as well as other useful parameters that the virtual machine requires\n(e.g., SSH keys and network contextualization). The virtual machine template\ncan be created by the oneadmin user using the CLI command onetemplate :\n\n\n\n186 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n$ onetemplate create \\\n??name \"CentOS?6.4 \" \\\n??cpu 1 ??vcpu 1 ??memory 512 \\\n??arch x86_64 ??disk \"CentOS?6.4_x86_64\" ??nic \" private \" ??vnc \\\n??ssh\n\nWith the virtual machine template defined, the virtual machine can be gen-\nerated by the oneadmin user using the CLI command onetemplate:\n$ onetemplate i n s t a n t i a t e \"CentOS?6.4 \" ??name \" Test?VM\"\n\nThe allocation of virtual resources was conducted without any problems, con-\nfirming that the OpenNebula IaaS platform was successfully installed and con-\nfigured. For troubleshooting, OpenNebula provides logs at /var/log/one.\n\n7.1.3 OpenStack Installation\n\nOpenStack installation is divided in five procedures : (i) OS Configuration; (ii)\nKeystone Installation; (iii) Glance Installation; (iv) Nova Installation and (v)\nVirtual Resources Allocation.\n\n7.1.3.1 OS Configuration\n\nThe OS Configuration procedure sets the required software packages and OS\nconfigurations required for the installation of OpenStack.\n\nLike OpenNebula, the NIC eth0 has to be configured to use a network bridge\nbr0, replacing /etc/sysconfig/network-scripts/ifcfg-eth0 file parameters\nwith:\nDEVICE=eth0\nBOOTPROTO=none\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Ethernet\nBRIDGE=br0\n\nand the ifcfg-br0 file that contains the network bridge information needs also\nto be created with a public static IP address:\n# c a t&lt;&lt;EOT > / e t c / s y s c o n f i g / network?s c r i p t s / network?s c r i p t s / i f c f g ?b r 0\nDEVICE=br0\nTYPE=Bridge\nIPADDR=192.168.1.17\nNETMASK=255.255.255.0\nONBOOT=yes\nBOOTPROTO=s t a t i c\nNM_CONTROLLED=no\nEOT\n\nThe host name osnode is set to simplify the services configuration process. It\nis defined using the hostname CLI command:\n# hostname o s n o d e\n\n/etc/sysconfig/network-scripts/ifcfg-eth0\n\n\n7.1. TEST BED 187\n\nThen, the host name must be specified accordingly in the configuration file\n/etc/sysconfig/network to ensure that the change persists after reboot. This\nis done by changing the line starting with HOSTNAME=:\nHOSTNAME=osnode\n\nFinally, the host name and public static IP address is added to the /etc/hosts\nfile:\n192.168.1.17 osnode\n\nAt this stage, the network service needs to be restarted for changes to take\neffect:\n# s e r v i c e n e t w o r k r e s t a r t\n\nThe OpenStack services require a database to store information. This way,\nbefore proceeding with OpenStak packages installation, it is necessary to install a\ndatabase management system. The database system adopted is MySQL, which is\nan open-source relational database management system. To install MySQL it is\nnecessary to install the MySQL client and server packages as well as the Python\nlibrary:\n# yum i n s t a l l m y s q l mysql?s e r v e r MySQL?p y t h o n\n\nThen, in order to to set the bind-address with the IP address of the machine,\nthe following statement is added to the /etc/my.cnf file:\n[ mysqld ]\n. . .\nbind?address = 192.168.1.17\n\nNext, the MySQL database server has to be started and configured to launch\nautomatically when the system boots:\n# s e r v i c e m y s q l d s t a r t\n# c h k c o n f i g m y s q l d on\n\nFinally, it is also necessary to define a root password for the MySQL database\nand erase the anonymous users that are created when the database is first started.\nThis is done by following the instructions of mysql_secure_installation command:\n# m y s q l _ i n s t a l l _ d b\n# m y s q l _ s e c u r e _ i n s t a l l a t i o n\n\nThe first time the root password is set for the MySQL Relational Data-\nbase Management System (RDBMS), the system asks for a password, which\nis not defined yet. This step can be skipped by pressing the enter key. The\nmysql_secure_installation CLI command presents a number of other options to\nsecure the database installation. All these prompts can be answered with yes.\n\n/etc/sysconfig/network\n/etc/hosts\n/etc/my.cnf\n\n\n188 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nWith MySQL configured, the system is ready for OpenStack installation.\nThe required OpenStack packages are available on the RDO repository. This\nrepository provides packages that work on Red Hat Enterprise Linux (RHEL)\n6, compatible versions of CentOS and Fedora 19. The RDO repository access is\nenabled by downloading and installing the rdo-release-havana package:\n# yum i n s t a l l h t t p : / / r e p o s . f e d o r a p e o p l e . o r g / r e p o s / o p e n s t a c k /\nopenstack?havana/rdo?release ?havana ?6. noarch . rpm\n\nTogether with the rdo-release-havana package it is necessary to install the\nopenstack-utils and openstack-selinux packages:\n# yum i n s t a l l o p e n s t a c k ? u t i l s o p e n s t a c k ?s e l i n u x\n\nWhile the rdo-release-havana package contains utility programs that make\ninstallation and configuration easier, the openstack-selinux package includes the\npolicy files that are required to configure the Security-Enhanced Linux (SELinux)\nduring the OpenStack installation.\n\nNext, it is necessary to upgrade and reboot the system:\n# yum u p g r a d e\n# r e b o o t\n\nAfter the system reboot, the next step is to install the messaging queue server\nQpid (RabbitMQ and ZeroMQ can also be used):\n# yum i n s t a l l q p i d ?cpp?s e r v e r memcached\n\nand disable the Qpid authentication by editing /etc/qpidd.conf file and\nchanging the auth option to no:\nauth=no\n\nAfter disabling the default Qpid authentication mechanism, the Qpid ser-\nvice can be started and configured to launch at system boot using the following\ncommands:\n# s e r v i c e q p i d d s t a r t\n# c h k c o n f i g q p i d d on\n\n7.1.3.2 Keystone Installation\n\nKeystone is the OpenStack identity service and enables user management as well\nas a catalogue of available services together with their API endpoints. It can be\ninstalled using the following command:\n# yum i n s t a l l o p e n s t a c k ?k e y s t o n e python?k e y s t o n e c l i e n t\n\n/etc/qpidd.conf\n\n\n7.1. TEST BED 189\n\nSince the identity service stores the identity data in the MySQL database,\nit is necessary to specify the location of the database with the Keystone user\nname and KEYSTONE_DBPASS password in the /etc/keystone/keystone.\nconf configuration file:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / k e y s t o n e / k e y s t o n e . c o n f \\\n\nsql connection mysql :// keystone :KEYSTONE_DBPASS@osnode/ keystone\n\nThen, the openstack-db command is used to create the Keystone database\nand user (keystone user and KEYSTONE_DBPASS password):\n# o p e n s t a c k ?db ?? i n i t ??s e r v i c e k e y s t o n e ??p a s s w o r d KEYSTONE_DBPASS\n\nNext, it is necessary to define an authorization token to be used as a shared\nsecret between the identity service and other OpenStack services. Openssl is\nused to generate and stores the token in the configuration file /etc/keystone/\nkeystone.conf:\n# ADMIN_TOKEN=$ ( o p e n s s l rand ?h e x 1 0 )\n# e c h o $ADMIN_TOKEN\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / k e y s t o n e / k e y s t o n e . c o n f DEFAULT \\\n\nadmin_token $ADMIN_TOKEN\n\nBy default, Keystone uses Public Key Infrastructure (PKI) tokens for signing\nkeys and certificates:\n# k e y s t o n e ?manage p k i _ s e t u p ??k e y s t o n e ?u s e r k e y s t o n e ??k e y s t o n e ?g r o u p k e y s t o n e\n# chown ?R k e y s t o n e : k e y s t o n e / e t c / k e y s t o n e /? / v a r / l o g / k e y s t o n e / k e y s t o n e . l o g\n\nTo apply this configuration, Keystone must be started and configured to auto-\nmatically start at system boot:\n# s e r v i c e o p e n s t a c k ?k e y s t o n e s t a r t\n# c h k c o n f i g o p e n s t a c k ?k e y s t o n e on\n\nWith Keystone installed and running, the users, tenants and roles must be\ncreated to allow access to services and endpoints. Since, by default, no users\nare defined, the authorization token created earlier must be used to interact\nwith Keystone. To simplify the process, the OS_SERVICE_TOKEN and the\nOS_SERVICE_ENDPOINT environment variables can be set to contain the\nADMIN_TOKEN and the identity service endpoint:\n# e x p o r t OS_SERVICE_TOKEN=$ADMIN_TOKEN\n# e x p o r t OS_SERVICE_ENDPOINT=h t t p : / / o s n o d e : 3 5 3 5 7 / v2 . 0\n\nThen, is necessary to create a tenant for the administrative user and a tenant\nfor the remaining OpenStack services:\n# k e y s t o n e t e n a n t ?c r e a t e ??name=admin ??d e s c r i p t i o n =\"Admin Tenant \"\n# k e y s t o n e t e n a n t ?c r e a t e ??name=s e r v i c e ??d e s c r i p t i o n =\" S e r v i c e Tenant \"\n\nIn the case of the administrative user, the user credentials (admin with pass-\nword ADMIN_PASS) and email address (e.g.,1050758@isep.ipp.pt) must be set:\n\n/etc/keystone/keystone.conf\n/etc/keystone/keystone.conf\n/etc/keystone/keystone.conf\n/etc/keystone/keystone.conf\n\n\n190 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n# k e y s t o n e u s e r ?c r e a t e ??name=admin ??p a s s=ADMIN_PASS \\\n??email =1050758 @isep . ipp . pt\n\nThis is followed by the definition of a role for the administrative tasks (also\ncalled admin):\n# k e y s t o n e r o l e ?c r e a t e ??name=admin\n\nand by the assignment of the admin role and the admin tenant to the admin\nuser:\n# k e y s t o n e u s e r ?r o l e ?add ??u s e r=admin ??t e n a n t=admin ??r o l e=admin\n\nAfter creating the admin user, tenant and role it is necessary to register the\ncorresponding Keystone service and API endpoint:\n# k e y s t o n e s e r v i c e ?c r e a t e ??name=k e y s t o n e ??t y p e=i d e n t i t y \\\n\n??de s cr ip ti on=\" Keystone? Identity ? Service \"\n\nUsing the returned service ID, the API endpoint for Keystone can be specified\n(the_service_id must be substituted by the returned service ID):\n# k e y s t o n e e n d p o i n t ?c r e a t e \\\n\n??service ?id=the_service_id \\\n??publ icurl=http :// osnode :5000/ v2 .0 \\\n??i n t e r n a l u r l=http :// osnode :5000/ v2 .0 \\\n??adminurl=http :// osnode :35357/ v2 .0\n\nWith the Keystone service API endpoints configured, the \u2013os- variables (\u2013os-\nusername, \u2013os-password, \u2013os-tenant-name and \u2013os-auth-url) can be set in the\nenvironment to simplify command-line usage. This procedure can be made by\ncreating a openrc.sh file containing the values of the \u2013os-* variables in environ-\nment variables:\n# c a t&lt;&lt;EOT > o p e n r c . s h\nexport OS_USERNAME=admin\nexport OS_PASSWORD=ADMIN_PASS\nexport OS_TENANT_NAME=admin\nexport OS_AUTH_URL=http :// osnode :35357/ v2 .0\nEOT\n\n# s o u r c e o p e n r c . s h\n\nAfter sourcing this file, the Keystone CLI commands can be instantiated\nwithout using the \u2013os- variables. Finally, the access from other machines in the\nnetwork to the Keystone API endpoints can be granted by adding the following\niptables rules:\n# i p t a b l e s ?I INPUT ?p t c p ?m m u l t i p o r t ??d p o r t s 5 0 0 0 , 3 5 3 5 7 ? j ACCEPT\n\n# s e r v i c e i p t a b l e s s a v e\n# s e r v i c e i p t a b l e s r e s t a r t\n\n\n\n7.1. TEST BED 191\n\n7.1.3.3 Glance Installation\n\nGlance is the OpenStack image service and enables users to discover, register and\nretrieve virtual machine images. It can be installed using the following command:\n# yum i n s t a l l o p e n s t a c k ?g l a n c e\n\nLike OpenNebula, this service uses the Linux FS (/var/lib/glance/images/)\nto store images. The image service provides the glance-api and glance-registry\nservices, each with its own configuration file that must include the location of the\nMySQL RDBMS (GLANCE_DBPASS is the Image Service database password):\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f \\\n\nDEFAULT sql_connection mysql :// glance :GLANCE_DBPASS@osnode/ glance\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f \\\n\nDEFAULT sql_connection mysql :// glance :GLANCE_DBPASS@osnode/ glance\n\nThen, the Image Service database and the database user are created using\nthe openstack-db CLI command:\n# o p e n s t a c k ?db ?? i n i t ??s e r v i c e g l a n c e ??p a s s w o r d GLANCE_DBPASS\n\nThe glance user is created in a similar way to the keystone admin user and\nassociated with the service tenant and admin role:\n# k e y s t o n e u s e r ?c r e a t e ??name=g l a n c e ??p a s s=GLANCE_PASS \\\n\n??email=glance@example . com\n# k e y s t o n e u s e r ?r o l e ?add ??u s e r=g l a n c e ??t e n a n t=s e r v i c e ??r o l e=admin\n\nAt this stage, is necessary to configure Glance (both glance-api and glance-\nregistry configuration files) to use the identity service for authentication:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nauth_uri http :// osnode :5000\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nauth_host osnode\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_tenant_name s e r v i c e\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_user glance\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_password GLANCE_PASS\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?a p i . c o n f p a s t e _ d e p l o y \\\n\nf l a v o r keystone\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nauth_uri http :// osnode :5000\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nauth_host osnode\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_tenant_name s e r v i c e\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_user glance\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f k e y s t o n e _ a u t h t o k e n \\\n\nadmin_password GLANCE_PASS\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / g l a n c e / g l a n c e ?r e g i s t r y . c o n f p a s t e _ d e p l o y \\\n\nf l a v o r keystone\n\nTo grant access to the Glance database, the Glance credentials must be added\nto the /etc/glance/glance-api-paste.ini and /etc/glance/glance-registry-paste.\nini files. First, these files need to be copied to the correct location:\n\n/etc/glance/glance-api-paste.ini\n/etc/glance/glance-registry-paste.ini\n/etc/glance/glance-registry-paste.ini\n\n\n192 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n# cp / u s r / s h a r e / g l a n c e / g l a n c e ?a p i ?d i s t ?p a s t e . i n i \\\n/ etc / glance / glance ?api?paste . i n i\n# cp / u s r / s h a r e / g l a n c e / g l a n c e ?r e g i s t r y ?d i s t ?p a s t e . i n i \\\n/ etc / glance / glance ?registry ?paste . i n i\n\nThen, each file is edited to set the following options in the [filter:authtoken]\nsection (any other existing option should remain as it is):\n[ f i l t e r : authtoken ]\npaste . f i l t e r _ f a c t o r y=keystoneclient . middleware . auth_token : f i l t e r _ f a c t o r y\nauth_host=osnode\nadmin_user=glance\nadmin_tenant_name=s e r v i c e\nadmin_password=GLANCE_PASS\n\nThe next step is to register Glance with Keystone so that other OpenStack\nservices can locate it. This is made by registering the service and creating its\nendpoint:\n# k e y s t o n e s e r v i c e ?c r e a t e ??name=g l a n c e ??t y p e=image \\\n\n??de s cr ip ti on=\" Glance?Image? Service \"\n\nThe ID returned by the service-create CLI command is used to create the\nendpoint (the the_service_id must be substituted by the provided ID):\n# k e y s t o n e e n d p o i n t ?c r e a t e \\\n\n??service ?id=the_service_id \\\n??publ icurl=http :// osnode :9292 \\\n??i n t e r n a l u r l=http :// osnode :9292 \\\n??adminurl=http :// osnode :9292\n\nFinally, the glance-api and glance-registry services have to be started and\nconfigured for automatic launch at system boot:\n# s e r v i c e o p e n s t a c k ?g l a n c e ?a p i s t a r t\n# s e r v i c e o p e n s t a c k ?g l a n c e ?r e g i s t r y s t a r t\n# c h k c o n f i g o p e n s t a c k ?g l a n c e ?a p i on\n# c h k c o n f i g o p e n s t a c k ?g l a n c e ?r e g i s t r y on\n\nTo enable the access to the Glance endpoint from other machines in the\nnetwork, the firewall must be configured using the following iptables rules:\n# i p t a b l e s ?I INPUT ?p t c p ?m m u l t i p o r t ??d p o r t s 9292 ? j ACCEPT\n\n# s e r v i c e i p t a b l e s s a v e\n# s e r v i c e i p t a b l e s r e s t a r t\n\n7.1.3.4 Nova Installation\n\nNova is the OpenStack compute service used to host and manage cloud computing\nresources. It can be installed using the following command:\n# yum i n s t a l l o p e n s t a c k ?nova python?n o v a c l i e n t\n\n\n\n7.1. TEST BED 193\n\nFirst, similarly to the previous installed OpenStack services, the location of\nMySQL RDBMS must be set in /etc/nova/nova.conf file using the nova user\nand NOVA_DBPASS password:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f \\\n\ndatabase connection mysql :// nova :NOVA_DBPASS@osnode/nova\n\nThis file (/etc/nova/nova.conf) must, then, be configured to use the Qpid\nmessage broker:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f \\\n\nDEFAULT rpc_backend nova . openstack . common . rpc . impl_qpid\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT qpid_hostname o s n o d e\n\nNext, using the openstack-db command, the Nova service database and user\nare created:\n# o p e n s t a c k ?db ?? i n i t ??s e r v i c e nova ??p a s s w o r d NOVA_DBPASS\n\nThen, the my_ip, vncserver_listen and vncserver_proxyclient_address op-\ntions need to be configured to use the IP address of the machine:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nmy_ip 192.168.1.17\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nvncserver_listen 192.168.1.17\n\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nvncserver_proxyclient_address 192.168.1.17\n\nThe next step is to create a nova user in the Keystone service associated with\nthe service tenant and the admin role:\n# k e y s t o n e u s e r ?c r e a t e ??name=nova ??p a s s=NOVA_PASS ??e m a i l=nova@example . com\n# k e y s t o n e u s e r ?r o l e ?add ??u s e r=nova ??t e n a n t=s e r v i c e ??r o l e=admin\n\nThen, the Nova service has to be configured to use the Keystone credentials\nand the host that runs Glance:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nauth_strategy keystone\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nauth_host osnode\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nauth_protocol http\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nauth_port 35357\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nadmin_user nova\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nadmin_tenant_name s e r v i c e\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f k e y s t o n e _ a u t h t o k e n \\\nadmin_password NOVA_PASS\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT g l a n c e _ h o s t \\\nosnode\n\nAt this stage, the api_paste_config=/etc/nova/api-paste.ini option must be\ndefined in the /etc/nova/nova.conf file. Then, the Nova credentials must be\nincluded to the /etc/nova/api-paste.ini file by adding the following options\nto the [filter:authtoken] section:\n\n/etc/nova/nova.conf\n/etc/nova/nova.conf\n/etc/nova/nova.conf\n/etc/nova/api-paste.ini\n\n\n194 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n[ f i l t e r : authtoken ]\npaste . f i l t e r _ f a c t o r y = keystoneclient . middleware . auth_token : f i l t e r _ f a c t o r y\nauth_host = osnode\nauth_port = 35357\nauth_protocol = http\nauth_uri = http :// osnode :5000/ v2 .0\nadmin_tenant_name = s e r v i c e\nadmin_user = nova\nadmin_password = NOVA_PASS\n\nOnce the Nova credentials have been set, it is necessary to register the Nova\ncompute service with the Keystone identity service:\n# k e y s t o n e s e r v i c e ?c r e a t e ??name=nova ??t y p e=compute \\\n\n??de s cr ip ti on=\"Nova?Compute? s e r v i c e \"\n\nUsing the ID property returned, the Nova service endpoint can be created\nusing the following command:\n# k e y s t o n e e n d p o i n t ?c r e a t e \\\n\n??service ?id=the_service_id \\\n??publ icurl=http :// osnode :8774/ v2/%\\(tenant_id \\) s \\\n??i n t e r n a l u r l=http :// osnode :8774/ v2/%\\(tenant_id \\) s \\\n??adminurl=http :// osnode :8774/ v2/%\\(tenant_id \\) s\n\nTo implement a simple network configuration based on Linux bridging, the\nlegacy Nova network service that implements DHCP (FlatDHCPManager) is\nused:\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\n\nnetwork_manager nova . network . manager . FlatDHCPManager\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\n\nf i r e w a l l _ d r i v e r nova . v i r t . l i b v i r t . f i r e w a l l . IptablesFirewallDriver\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nnetwork_size 254\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nallow_same_net_traffic False\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nsend_arp_for_ha True\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nshare_dhcp_address True\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nforce_dhcp_release True\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nf l a t _ i n t e r f a c e eth0\n\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\nflat_network_bridge br0\n# o p e n s t a c k ?c o n f i g ??s e t / e t c / nova / nova . c o n f DEFAULT \\\npublic_interface eth0\n\n\n\n7.1. TEST BED 195\n\nThen, Nova can be started and configured to automatically launch at system\nboot:\n# s e r v i c e o p e n s t a c k ?nova?a p i s t a r t\n# s e r v i c e o p e n s t a c k ?nova?c e r t s t a r t\n# s e r v i c e o p e n s t a c k ?nova?c o n s o l e a u t h s t a r t\n# s e r v i c e o p e n s t a c k ?nova?s c h e d u l e r s t a r t\n# s e r v i c e o p e n s t a c k ?nova?c o n d u c t o r s t a r t\n# s e r v i c e o p e n s t a c k ?nova?n o v n c p r o x y s t a r t\n# s e r v i c e o p e n s t a c k ?nova?compute s t a r t\n# s e r v i c e o p e n s t a c k ?nova?n e t w o r k s t a r t\n# s e r v i c e l i b v i r t d s t a r t\n# s e r v i c e m e s s a g e b u s s t a r t\n# c h k c o n f i g o p e n s t a c k ?nova?a p i on\n# c h k c o n f i g o p e n s t a c k ?nova?c e r t on\n# c h k c o n f i g o p e n s t a c k ?nova?c o n s o l e a u t h on\n# c h k c o n f i g o p e n s t a c k ?nova?s c h e d u l e r on\n# c h k c o n f i g o p e n s t a c k ?nova?c o n d u c t o r on\n# c h k c o n f i g o p e n s t a c k ?nova?n o v n c p r o x y on\n# c h k c o n f i g o p e n s t a c k ?nova?compute on\n# c h k c o n f i g o p e n s t a c k ?nova?n e t w o r k on\n# c h k c o n f i g l i b v i r t d on\n# c h k c o n f i g m e s s a g e b u s on\n\nTo enable access to the VNC proxy and the Nova API, the firewall must be\nconfigured using the following iptables rules:\n# i p t a b l e s ?I INPUT ?p t c p ?m m u l t i p o r t ??d p o r t s 6080 ? j ACCEPT\n# i p t a b l e s ?I INPUT ?p t c p ?m m u l t i p o r t ??d p o r t s 8774 ? j ACCEPT\n\n# s e r v i c e i p t a b l e s s a v e\n# s e r v i c e i p t a b l e s r e s t a r t\n\n7.1.3.5 Virtual Resources Allocation\n\nWith Keystone, Glance and Nova services installed, configured and running, the\nOpenstack IaaS platform is up with the minimum set of components that allows\nthe creation of virtual resources and provision of virtual machines.\n\nThe virtual network for the virtual machines can be created using Nova\nnetwork-create CLI command:\n# nova network?c r e a t e vmnet ??f i x e d ?range?v4 = 1 9 2 . 1 6 8 . 2 . 0 / 2 8 \\\n\n??bridge=br0\n\nA small test image that is often used for testingOpenStack deployments, the\n64-bit CirrOS QCOW2 image, is downloaded and placed in the created ~/images/\nfolder:\n$ mkdir ~/ images\n$ cd ~/ images/\n$ wget http :// cdn . download . cirros ?cloud . net /0.3.1/ cirros ?0.3.1 ?x86_64?disk . img\n\nThen, the image is uploaded to the image service Glance using the image-\ncreate CLI command:\n\n~/images/\n\n\n196 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n# g l a n c e image?c r e a t e \\\n??name=\" CirrOS? 0 . 3 . 1 \" \\\n??disk ?format=qcow2 \\\n??container ?format=bare \\\n??is ?public=true &lt;cirros ?0.3.1 ?x86_64?disk . img\n\nThe image ID can be consulted using the Glance image-list CLI command:\n$ glance image? l i s t\n\nTo provide virtual machine password-less SSH access, a key-pair must be\ncreated:\n$ ssh?keygen\n$ cd . ssh\n$ nova keypair ?add ??pub_key id_rsa . pub mykey\n\nTo launch an instance, a flavour ID must be specified. A flavour is a resource\nallocation profile that specifies the number of virtual CPU as well as the disk\nand RAM size that the instances will have. OpenStack provides the following\nprofiles:\n\n\u2022 m1.tiny - 512 MB of RAM, 1 vCPU and 1 GB of disk storage capacity;\n\n\u2022 m1.small - 2048 MB of RAM, 1 vCPU and 20 GB of disk storage capacity;\n\n\u2022 m1.medium - 4096 MB of RAM, 2 vCPU and 40 GB of disk storage capacity;\n\n\u2022 m1.large - 8192 MB of RAM, 4 vCPU and 80 GB of disk storage capacity;\n\n\u2022 m1.xlarge - 16384 MB of RAM, 8 vCPU and 160 GB of disk storage capa-\ncity.\n\nThe Nova compute service uses security groups, which are sets of IP filter\nrules, to be applied to the networking instances. The security groups rules are\nconfigured to enable SSH and ping:\n$ nova secgroup?add?rule default tcp 22 22 0 . 0 . 0 . 0 / 0\n$ nova secgroup?add?rule default icmp ?1 ?1 0 . 0 . 0 . 0 / 0\n\nOnce the virtual resources are created, a virtual machine can be launched\nusing the m1.tiny flavor, a key-pair mykey for password-less access to the VM\nwhen using SSH, the cirrOS image (ID = caede980-890f-477d-8856-4cc7ba2c2f39),\nthe default security group profile that enables SSH and ping to the VM and finally\nthe VM name cirrOS:\n$ nova boot ??f l a v o r 1 ??key_name mykey \\\n??image caede980 ?890f ?477d?8856?4cc7ba2c2f39 \\\n??security_group default cirrOS\n\n\n\n7.1. TEST BED 197\n\nThe allocation of virtual resources was conducted without any problems, con-\nfirming that the Keystone, Glance and Nova projects deployed in the OpenStack\nIaaS platform were successfully installed and configured. For troubleshooting,\nOpenStack provides logs at /var/log/keystone for Keystone, /var/log/glance for\nGlance and /var/log/nova for Nova.\n\n7.1.4 CloudStack Installation\n\nThe CloudStack installation is composed of five main procedures: (i) OS Config-\nuration; (ii) Management Server Installation; (iii) KVM Hypervisor Installation;\n(iv) Cloud Infrastructure Configuration and (v) Virtual Resources Allocation.\n\n7.1.4.1 OS Configuration\n\nThe OS Configuration procedure configures OS network for CloudStack usage and\nsets the CloudStack repository and required services for CloudStack installation.\n\nThe NIC eth0 has to be configured to use a network bridge cloudbr0, replacing\n/etc/sysconfig/network-scripts/ifcfg-eth0 file parameters with:\nDEVICE=eth0\nBOOTPROTO=none\nNM_CONTROLLED=no\nONBOOT=yes\nTYPE=Ethernet\nBRIDGE=cloudbr0\n\nThe ifcfg-cloudbr0 file, that specifies the network bridge information, needs\nalso to be created with a public static IP address:\n# c a t&lt;&lt;EOT > / e t c / s y s c o n f i g / network?s c r i p t s / network?s c r i p t s / i f c f g ?c l o u d b r 0\nDEVICE=cloudbr0\nTYPE=Bridge\nIPADDR=192.168.1.18\nNETMASK=255.255.255.0\nGATEWAY=192.168.1.1\nONBOOT=yes\nBOOTPROTO=s t a t i c\nNM_CONTROLLED=no\nEOT\n\nThe host name csnode is set to simplify the services configuration process. It\nis defined using the hostname CLI command:\n# hostname c s n o d e\n\nTo ensure that this host name change persists, the configuration file /etc/\nsysconfig/network is edited and the line starting with HOSTNAME= is changed:\nHOSTNAME=csnode\n\nThen, the host name with the corresponding public fixed IP address is added\nto the /etc/hosts file:\n\n/etc/sysconfig/network-scripts/ifcfg-eth0\n/etc/sysconfig/network\n/etc/sysconfig/network\n/etc/hosts\n\n\n198 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n192.168.1.18 csnode\n\nAt this stage, the network service needs to be restarted for changes to take\neffect:\n# s e r v i c e n e t w o r k r e s t a r t\n\nThe SELinux security module must be set to permissive. The SELinux system\nsecurity level can be set to permissive by running the following command:\n# s e t e n f o r c e 0\n\nTo ensure that it remains in that state after a system boot, the file /etc/\nselinux/config has to be configured to reflect the permissive state:\nSELINUX=permissive\n\nTo add the CloudStack repository, it is necessary to create the file /etc/yum.\nrepos.d/cloudstack.repo with the following information:\n# c a t&lt;&lt;EOT > / e t c /yum . r e p o s . d/ c l o u d s t a c k . r e p o\n[ cloudstack ]\nname=cloudstack\nbaseurl=http :// cloudstack . apt?get . eu/ rhel /4.3/\nenabled=1\ngpgcheck=0\nEOT\n\nCloudStack uses NFS for both primary and secondary storage. This way, two\nNFS shares need to be set-up. First, nfs-utils has to be installed:\n# yum i n s t a l l n f s ? u t i l s\n\nThen, the two directories must be created:\n# mkdir / p r i m ar y\n# mkdir / s e c o n d a r y\n\nTo configure the new directories as NFS exports, the /etc/exports file has\nto be edited:\n/primary ?(rw , async , no_root_squash )\n/ secondary ?(rw , async , no_root_squash )\n\nThen, it is necessary to uncomment the following configuration values in the\n/etc/sysconfig/nfs file:\nLOCKD_TCPPORT=32803\nLOCKD_UDPPORT=32769\nMOUNTD_PORT=892\nRQUOTAD_PORT=875\nSTATD_PORT=662\nSTATD_OUTGOING_PORT=2020\n\nand edit the iptables rules to allow incoming NFS connections:\n\n/etc/selinux/config\n/etc/selinux/config\n/etc/yum.repos.d/cloudstack.repo\n/etc/yum.repos.d/cloudstack.repo\n/etc/exports\n/etc/sysconfig/nfs\n\n\n7.1. TEST BED 199\n\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p udp ??dport 111 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 111 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 2049 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 32803 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p udp ??dport 32769 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 892 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p udp ??dport 892 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 875 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p udp ??dport 875 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p tcp ??dport 662 ?j ACCEPT\n# i p t a b l e s ?A INPUT ?s 1 9 2 . 1 6 8 . 1 . 0 / 2 4 ?m s t a t e \\\n??state NEW ?p udp ??dport 662 ?j ACCEPT\n\n# s e r v i c e i p t a b l e s s a v e\n# s e r v i c e i p t a b l e s r e s t a r t\n\nThe NFS service needs to be initiated and configured to start on boot:\n# s e r v i c e r p c b i n d s t a r t\n# s e r v i c e n f s s t a r t\n# c h k c o n f i g r p c b i n d on\n# c h k c o n f i g n f s on\n\nCloudStack also requires the usage of a database. Like in the case of Open-\nstack, the RDBMS installed was MySQL:\n# yum ?y i n s t a l l mysql?s e r v e r\n\nThen, the following options need to be added to the [mysqld] section of the\n/etc/my.cnf file:\ninnodb_rollback_on_timeout=1\ninnodb_lock_wait_timeout=600\nmax_connections=350\nlog ?bin=mysql?bin\nbinlog ?format = \u2019ROW\u2019\n\nWith MySQL properly configured, it can be started and configured to start\non boot:\n# s e r v i c e m y s q l d s t a r t\n# c h k c o n f i g m y s q l d on\n\nFinally, it is necessary to define a root password (DB_PASS) for the MySQL\nRDBMS and erase the anonymous users created by default by running the\nmysql_secure_installation daemon:\n# m y s q l _ i n s t a l l _ d b\n# m y s q l _ s e c u r e _ i n s t a l l a t i o n\n\n/etc/my.cnf\n\n\n200 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n7.1.4.2 Management Server Installation\n\nThe Management Server can be installed by executing the following command:\n# yum ?y i n s t a l l c l o u d ?c l i e n t\n\nThen, it is necessary to create the cloud user in the MySQL RDBMS:\n# c l o u d s t a c k ?s e t u p ?d a t a b a s e s c l o u d : DB_PASS@csnode \\\n??deploy?as=root\n\nAt this stage, the Management Server can be started:\n# c l o u d s t a c k ?s e t u p ?management\n\nCloudStack uses a number of system virtual machines to provide access the\nthe virtual machines, providing various networking services and managing various\nstorage features. This way, a system VM template must be downloaded and\ndeployed to the mounted NFS /export/secondary share:\n# / u s r / s h a r e / c l o u d s t a c k ?common/ s c r i p t s / s t o r a g e / s e c o n d a r y / c l o u d ?i n s t a l l ?s y s ?t m p l t\n?m / secondary ?u \\\nhttp :// download . cloud . com/ templates / acton /acton?systemvm ?02062012.qcow2 . bz2 \\\n?h kvm ?F\n\n7.1.4.3 KVM Hypervisor Installation\n\nThe KVM agent installation is done by installing the cloud-agent package:\n# yum ?y i n s t a l l c l o u d ?a g e n t\n\nThen, libvirt and QEMU need to be configured. The QEMU configuration\nenables the VNC server and it is done by editing /etc/libvirt/qemu.conf and\nensuring that the following line is present and uncommented:\nvnc_listen =0.0.0.0\n\nLibvirt is used by Cloudstack for managing virtual machines. Its configuration\nis performed in /etc/libvirt/libvirtd.conf and /etc/sysconfig/libvirtd\nfiles so that it listens for unsecured TCP connections for live migration and to\nturn off libvirts attempt to use multicast DNS advertising. The /etc/libvirt/\nlibvirtd.conf file is configured by setting the following parameters:\nl i s t e n _ t l s = 0\nlisten_tcp = 1\ntcp_port = \" 16059 \"\nauth_tcp = \" none \"\nmdns_adv = 0\n\nAdditionally, the following line of the /etc/sysconfig/libvirtd file must\nbe uncommented:\n#LIBVIRTD_ARGS=\"?? l i s t e n \"\n\n/etc/libvirt/qemu.conf\n/etc/libvirt/libvirtd.conf\n/etc/sysconfig/libvirtd\n/etc/libvirt/libvirtd.conf\n/etc/libvirt/libvirtd.conf\n/etc/sysconfig/libvirtd\n\n\n7.1. TEST BED 201\n\nFinally, the libvirt is restarted:\n# s e r v i c e l i b v i r t d r e s t a r t\n\nThe KVM agent is properly configured and running.\n\n7.1.4.4 Cloud Infrastructure Configuration\n\nThe IaaS platform is implemented using CloudStack\u2019s Web interface basic set-\nup option. This Web interface is accessible through the Web browser using the\nhttp://192.168.1.18:8080/client URL. Then, using the default admin au-\nthentication credentials (username admin and password password), the configur-\nation process can be started by choosing the Continue with Basic Setup option.\n\nThe cloud configuration process is divided in six sections: (i) Admin password\nmodification; (ii) Zone creation; (iii) Pod creation; (iv) Cluster configuration; (v)\nPrimary storage configuration; and (vi) Secondary storage configuration.\n\nThe first section requires the admin password modification. This way the\npassword for the admin user was modified to CS_PASS.\n\nThe Zone creation section allows the specification of CloudStack\u2019s Zone Name\nand public and private DNS. Only Public and Internal DNS 1 are mandatory.\nThe Internal DNS is assumed to be capable of resolving internal-only host names,\nwhile the Public DNS is provided to the guest virtual machines to resolve public\nIP addresses. In this specific set-up, no names will be attributed for internal\nresources. As a result, the Public DNS were configured to use Google\u2019s public\nDNS (8.8.8.8 and 8.8.4.4) and the private DNS were configured to use the local\nnetwork DNS (193.136.63.3 and 193.136.60.2):\n\nCloudStack Zone:\n\n1. Name: zone1;\n\n2. Public DNS 1: 8.8.8.8;\n\n3. Public DNS 2: 8.8.4.4;\n\n4. Internal DNS 1: 193.136.63.3;\n\n5. Internal DNS 2: 193.136.60.2.\n\nNext, the Pod configuration section allows setting the network for the system\nvirtual machines and the guest virtual machines. In the first page the Pod\u2019s name\npod1 and the service network were configured as described:\n\n1. Name: pod1;\n\nhttp://192.168.1.18:8080/client\n\n\n202 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n2. Gateway: 192.168.1.1;\n\n3. Netmask: 255.255.255.0;\n\n4. IP Range: 192.168.1.30 \u2013192.168.1.39.\n\nThen, on the second page, the guest network was specified:\n\n1. Gateway: 192.168.1.1;\n\n2. Netmask: 255.255.255.0;\n\n3. IP Range: 192.168.1.40 \u2013192.168.1.49.\n\nThe Cluster configuration section allows the definition of the name and the\nhypervisor used by a cluster (in this case, cluster1):\n\n1. Name: cluster1;\n\n2. Hypervisor: KVM.\n\nThe second page configures the Linux root account access to the physical ma-\nchine where CloudStack is installed, using the IP address of the physical machine,\nthe root username and password:\n\n1. Host Name: 192.168.1.18;\n\n2. Username: root;\n\n3. Password: $CSnode2014.\n\nWith the Cluster configured, the user is directed to a page to set the primary\nstorage, including the NFS protocol, the primary1 name, the address (192.168.1.18)\nto physical machine and the location path of the primary storage:\n\n1. Name: primary1;\n\n2. protocol: NFS;\n\n3. Server: 192.168.1.18;\n\n4. Path: /primary.\n\nThe Secondary storage section allows the configuration of the secondary stor-\nage, including the NFS server address and the location path for the secondary\nstorage:\n\n\n\n7.1. TEST BED 203\n\n1. NFS Server: 192.168.1.18;\n\n2. Path: /secondary.\n\n7.1.4.5 Virtual Resources Allocation\n\nAfter CloudStack installation and configuration, the physical machine is ready\nfor the provision of virtual machines. Each VM creation involves the CloudStack\nservice offerings, i.e., the virtual resources available, consisting of compute, disk,\nnetwork and templates offerings. By default, CloudStack has two compute offer-\nings options:\n\n\u2022 Small - with 1 vCPU with 500 MHz and 512 MB of RAM;\n\n\u2022 Medium - with 1 vCPU with 1000 MHz and 1024 MB of RAM.\n\nand four disk offerings options:\n\n\u2022 Small - with 5 GB of storage capacity;\n\n\u2022 Medium - with 20 GB of storage capacity;\n\n\u2022 Large - with 100 GB of of storage capacity;\n\n\u2022 Custom - with no defined value of capacity.\n\nThe default network offering uses the Pod network configuration and a Cen-\ntOS template image for the guest virtual machines. ISO files, security groups,\nusers, groups, zones and projects are also available features that can be associated\nwith the virtual machines.\n\nTo test the CloudStack installation, an Ubuntu 12.04 server 64 bit ISO was\nregistered and used to create a VM via the CloudStack set-up GUI.\n\nThe ISO registration procedure is done by clicking on the Templates option\nin the left navigation bar, selecting ISO images from the Select View and clicking\non the Add ISO button. Then, in the Add ISO screen, the following configuration\nwas set:\n\n1. Name: ubuntu-12.04.2-server-amd64;\n\n2. Description: ubuntu-12.04.2-server-amd64;\n\n3. URL: http://releases.ubuntu.com/precise/ubuntu-12.04.4-server-amd64.\niso;\n\n4. Zones: All zones;\n\nhttp://releases.ubuntu.com/precise/ubuntu-12.04.4-server-amd64.iso\nhttp://releases.ubuntu.com/precise/ubuntu-12.04.4-server-amd64.iso\n\n\n204 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n5. Bootable: checked;\n\n6. OS: Ubuntu 12.04 (64-bit);\n\n7. Extractable: unchecked;\n\n8. Public: checked;\n\n9. Featured: checked.\n\nNext, after clicking on the OK button, the Management Server downloads\nthe ISO image from the provided URL and the ISO status column will display\n\u201cReady\u201d once it has been successfully downloaded into the secondary storage.\n\nThe \u201cReady\u201d ISO image can, then, be associated with a VM. The VM can be\ncreated by clicking on the Instances option in the left navigation bar and choosing\nthe Add Instance option. This procedure launches the VM creation wizard that\nwas configured with the following options:\n\n1. Select a zone: zone1;\n\n2. Select ISO or template: ISO;\n\n3. ubuntu-12.04.2-server-amd64;\n\n4. Compute Offering: Small Instance;\n\n5. Data Disk Offering: Small;\n\n6. security group(s): default;\n\n7. Name: CS-vm.\n\nThe allocation of virtual resources was conducted without any problems, con-\nfirming that the CloudStack IaaS platform was successfully installed and con-\nfigured. For troubleshooting, the CloudStack logs are available at /var/log/cloud-\nstack.\n\n7.2 Testing and Evaluation\n\nOnce the test bed was assembled, the OpenNebula, OpenStack, CloudStack and\nPACI driver modules were tested and evaluated in terms of functionality and\ninteroperability performance. These tests consisted of a comparison between the\nintermediary utilization of the Deltacloud API and the direct use of the dedicated\nIaaS platforms API operations in terms of time response per operation (i.e., the\ntotal amount of time required to perform a HTTP request to the API and obtain\n\n\n\n7.2. TESTING AND EVALUATION 205\n\nthe returned response) as well as the HTTP request packet length and HTTP\nresponse content length. The execution of the API operations (either to the\nDeltacloud API and the dedicated IaaS platform API) and the measurement of\nits time response was performed with the cURL command line tool, using the\nfollowing syntax and options:\n$ curl ??verbose ?X&lt;custom request method> ?H&lt;header> \\\n?d \u2019&lt;request data >\u2019 ?u \u2019<user : pass >\u2019 \u2019URL\u2019 ?w \u2019&lt;variables >\u2019\n\n\u2022 The \u2013v, \u2013verbose option shows additional information (e.g., HTTP header\ndata sent and received by curl) useful for debugging;\n\n\u2022 The \u2013X, \u2013request option specifies the custom request method to use, which\nin HTTP are the HTTP verbs (e.g., \u2013X POST);\n\n\u2022 The \u2013H, \u2013header option specifies extra header to include in the HTTP\nrequest (e.g., \u2013H \u201cContent-Type: application/xml\u201d);\n\n\u2022 The \u2013d, \u2013data option specifies the data to send by the HTTP POST request;\n\n\u2022 The \u2013u, \u2013user option specifies the user credentials for server authentication.\nIf only the user is provided, curl will prompt for a password. This option\ncan be combined with the \u2013digest option to use an authentication scheme\nthat prevents the password from being sent in clear text;\n\n\u2022 The URL formed by the API endpoint and the path of the resource (e.g.,\nhttp://192.168.1.16:4567/compute/9\u2019);\n\n\u2022 The \u2013w, \u2013write_out option enables the combination of plain text with\ndefined curl variables, using a string syntax, to display messages after a\ncompleted and successful operation. This operation was used to obtain\nthe time response for the tested operations (e.g., \u2013w \u2019\\n total time of the\noperation: %{time_total} seconds\u2019).\n\nA complete list of cURL options is available at [189]. The HTTP request\npacket length and response payload were measured using the Wireshark soft-\nware, analysing the local loopback interface lo (127.0.0.1 IP address) and the\ntest network interface eth0 (192.168.1.105 IP address) of the laptop computer\nfor the Deltacloud API request operations and the dedicated IaaS platforms API\nrequest operations, respectively. A filter formed by the HTTP protocol and the\nendpoint IP address (e.g., http&amp;&amp;ip.addr==192.168.1.16) was adopted to catch\nand inspect the desired HTTP request and response packets. For the sake of\nthese tests, the HTTP Secure Sockets Layer (SSL) encryption security procedure\nwas purposely disregarded.\n\nhttp://192.168.1.16:4567/compute/9'\n\n\n206 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nThe following sections present the tests performed with each driver module,\nincluding the analysis of the driver operation as well as the obtained results\nconstituted by the mean and standard deviation of ten runs per operation.\n\n7.2.1 OpenNebula Interaction Tests and Results\n\nTo test the interaction with the OpenNebula IaaS platform, the Deltacloud server\nwas started with the OpenNebula driver and the endpoint URL of the OCCI\nserver API using the localhost 3001 port:\n$ deltacloudd ?i opennebula ?P \" http ://192.168.1.16:4567 \" ?p 3001\nStarting Deltacloud API : : opennebula : :\nhttp ://192.168.1.16:4567 : : http :// l o c a l h o s t :3001/ api\nThin web server ( v1 . 6 . 1 codename Death Proof )\nDebugging ON\nMaximum connections set to 1024\nListening on l o c a l h o s t :3001 , CTRL+C to stop\n\nThe OCCI server implemented in the OpenNebula IaaS platform uses by de-\nfault a HTTP basic authentication [126] formed by the user identification (onead-\nmin) and a Secure Hash Algorithm 1 (SHA-1) hashed password (defined in the\n~/.one/one_auth file) separated by a colon (\u201c:\u201d). However, since the Open-\nNebula driver uses an authentication method (defined in the Client class of the\nocci_client.rb file) that implements the SHA-1 hash function on the password,\nthe authentication with the Deltacloud server used the unprotected plain text\npassword of the OpenNebula account.\n\nAlong with this security problem (that can be resolved by the adoption of\ncryptographic protocols such as SSL), the driver supplied with the Deltacloud\nframework had two minor bugs that had to be corrected. The first problem was\nrelated with an id argument mismatch in the destroy_image method that was\nincluded in the opennebula_driver.rb file. The second problem was associated\nwith the instantiation of an unused xmlfile argument in the delete method, which\nwas specified in the occi_client.rb file. Both problems were corrected and repor-\nted. Additionally, with this test confirmed that the driver works with the current\nversion of OpenNebula (version 4.4).\n\nTo maximize the execution of the interoperable tests and to simulate a normal\nusage of the resources, the operations tested were executed in the following order:\n\n1. List collections;\n\n2. List Images;\n\n3. Show Image Information;\n\n4. List Hardware Profiles;\n\n5. Create Instance;\n\n~/.one/one_auth\n\n\n7.2. TESTING AND EVALUATION 207\n\n6. List instances;\n\n7. Show Instance Information;\n\n8. Stop Instance;\n\n9. Start Instance;\n\n10. Reboot Instance;\n\n11. Delete Instance;\n\n12. Delete Image.\n\nSince each operation was executed ten times, it was necessary to allocate ten\nimages for each interaction (in fact, for the Deltacloud API and the dedicated\nOCCI API tests, twenty images were allocated). This way, and to save time, a\nttylinux - kvm image, downloaded from the OpenNebula marketplace (accessed\nvia Sunstone Web dashboard)[190] with a tiny footprint (40 MB), was used.\nIn the Create Instance operation the smallest hardware profile configuration (1\nvirtual CPU, 512 MB of RAM and 40 MB of disk size) was used in order to\nguarantee that the deployed OpenNebula node could handle the creation of ten\nconsecutive virtual machines (instances).\n\nThe time response test results, displaying the mean and standard deviation\nper operations and for both the Deltacloud and the direct API interactions, are\npresented in the chart of Figure 7.2.\n\nFigure 7.2: OpenNebula time response comparison.\n\n\n\n208 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nThis chart shows that the intermediary interaction with the Deltacloud API,\nusing the OpenNebula driver module, increases the operations time response,\nparticularly the operations that return lists of information i.e., the List Instances,\nList Images and List Hardware Profiles operations. It is also possible to observe\nthat the Delete Image and Create Instance operations have almost the same\naverage response time, and the collected samples from the last referred operation\n(Create Instance operation) varied significantly.\n\nThe results of the HTTP request packet length and returned payload per\noperation in both cases are presented in the chart of Figure 7.3. They reinforce\nthe interpretation of the time response results from Figure 7.2.\n\nFigure 7.3: OpenNebula HTTP request/response length comparison.\n\nFigure 7.3 shows that the HTTP requests packets length of the OpenNeb-\nula OCCI API operations are slightly bigger than the ones of the Deltacloud\nAPI operations. This can be observer mainly in the Create Instance, Stop In-\nstance, Start Instance and Reboot Instance operations. On the other hand, the\nlength of the HTTP response payloads varies and is bigger for the responses of\nDeltacloud API List Collections, List Instances, List Images, Show Image In-\nformation and List Hardware Profiles operations (being the List Instances and\nList Images the responses containing the greater measured values), identical in\nthe Create Instance operation and larger for the responses of the OpenNebula\nOCCI API Show Instance, Stop Instance, Start Instance and Reboot Instance\noperations. In the case of the Delete Instance and Delete Image operations, the\nreturned payload length is nil since these are responses without HTTP body.\n\n\n\n7.2. TESTING AND EVALUATION 209\n\n7.2.2 OpenStack Interaction Tests and Results\n\nTo test the interaction with the OpenStack IaaS platform, the Deltacloud server\nwas started with the OpenStack driver and the endpoint URL of the OpenStack\nIdentity Service (Keystone) API using the localhost 3001 port:\n$ deltacloudd ?i openstack ?P \" http ://192.168.1.17:5000/ v2 .0/ \" ?p 3001\nStarting Deltacloud API : : openstack\n: : http ://192.168.1.17:5000/ v2 .0/ : : http :// l o c a l h o s t :3001/ api\nThin web server ( v1 . 6 . 1 codename Death Proof )\nDebugging ON\nMaximum connections set to 1024\nListening on l o c a l h o s t :3001 , CTRL+C to stop\n\nTo authenticate with the OpenStack services, it was necessary to first per-\nform an authentication request to the OpenStack identity service (Keystone) API.\nThis authentication request consists of a HTTP request body, usually formatted\nin JSON, containing the credentials of the user (the user name, password, and op-\ntionally, the name or ID of the tenant). The following curl command exemplifies\na HTTP authentication request to the Keystone API:\ncurl ?H \" Content?Type : ? application / json \" ?X POST \\\n\n?d \u2019{ \" auth \" : { \" tenantName \" : \" admin \" , \" passwordCredentials \" :\n{ \" username \" : \" admin \" , \" password \" : \"ADMIN_PASS\" }}} \u2019 \\\n\u2019 http ://192.168.1.17:5000/ v2 .0/ tokens \u2019\n\nThe returned HTTP response contained the authentication token and the\ntenant ID (which is required to access OpenStack resources). The authentication\ntoken is valid for one day and has to be included in the HTTP request headers\n(using the X-Auth-Token header) of all HTTP requests operations.\n\nIn the case of the authentication with the Deltacloud Server API, the HTTP\nbasic authentication was used, exposing the credentials and providing the ten-\nant name concatenated with the user name separated by a plus character, i.e.,\nuser+tenant_name:password.\n\nAs in the case of the OpenNebula driver, the OpenStack driver exposes the\ncredentials if the connection to the Deltacloud Server is not secure (using cryp-\ntographic protocols). The connection to the OpenStack Keystone server is also\ninsecure, although the credentials are only exchanged once in 24 hours.\n\nAlthough the OpenStack driver is fully functional (with the Keystone, Nova\nand Glance API), since it lacks a start and stop VM operations in the OpenStack\nrubygem, the VM life-cycle used by the OpenStack driver differs from the one\nprovided by the Nova project. Moreover, for an unexplained reason, the method\nthat implements the Delete Instance operation was defined as an alias of the Stop\nInstance operation method. Thus, this implementation causes the destruction of a\nstarted VM when the Stop Instance operation is invoked. More recently, a Polish\ncompany named Cybercom proposed an OpenStack driver implementation with\nseparate Start Instance and Stop Instance operations [191]. This newer version\n\n\n\n210 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nof the driver was only announced after the execution of these tests and, for this\nreason, it was not used.\n\nThe OpenStack node deployed in the test bed was only configured with the\nKeystone, Glance and Nova OpenStack services, meaning that the Swift and\nCinder implemented collections were not tested. This way, the OpenStack driver\noperations tested were the following:\n\n1. List Images;\n\n2. Show Image Information;\n\n3. List Hardware Profiles;\n\n4. Create Instance;\n\n5. List Instances;\n\n6. Show Instance Information;\n\n7. Reboot Instance;\n\n8. Delete Instance;\n\n9. Delete Image.\n\nOnce again, ten images were created for each interaction. This time, the same\nimage used to test the Glance service installation - the 64-bit CirrOS QCOW2\nimage - using the same CLI command (the glance image-create CLI command)\nwas allocated. The smallest hardware profile configuration (1 virtual CPU, 512\nMB of RAM and 10 GB of Disk) was used for the Create Instance operation so\nthat the OpenStack node could handle the creation of ten consecutive virtual\nmachines (instances).\n\nThe time response results are presented in the chart of Figure 7.4. Since\nthe authentication request is performed in each Deltacloud API operation when\nusing the OpenStack driver, the mean value of HTTP authentication request\ntime response was added to the mean value of the OpenStack services (Nova and\nGlance) API operations time response.\n\n\n\n7.2. TESTING AND EVALUATION 211\n\nFigure 7.4: OpenStack time response comparison.\n\nAs expected, the time response from the Deltacloud API operations is signi-\nficantly higher than the time response of the OpenStack services API (Keystone,\nNova and Glance) operations. This is clearly observed in every listed operation\nwith the exception of the Delete Image and List Hardware Profiles operations. In\nfact, the time response mean of some operations like the List Instances, Create\nInstance and Reboot Instance Deltacloud API operations reached values higher\nthan the operations performed to the Deltacloud API using the OpenNebula\ndriver. There are three main reasons for this behaviour: (i) the instantiation\nof multiple OpenStack operations in the OpenStack driver to execute a single\nDeltacloud operation; (ii) the incremental time response of the OpenStack API\noperations in comparison with the OpenNebula OCCI API operations time re-\nsponse, e.g., the List Instances OpenNebula OCCI API operation has a time\nresponse mean of approximately 0,120 s while the same OpenStack API oper-\nation has a time response value mean of approximately 0,230 s; and (iii) the\nHTTP response payload length, that is illustrated in the chart of Figure 7.5,\nwhich presents the results of the HTTP request packet length and returned pay-\nload for each executed operations in both cases (using the Deltacloud API and\nthe OpenStack services API).\n\n\n\n212 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nFigure 7.5: OpenStack HTTP request/response length comparison.\n\nThe authentication procedure (authentication token) used by OpenStack is\nreflected in the values of the HTTP request packet length of the OpenStack API\noperations, which is substantially bigger than the values of the corresponding\nDeltacloud API operations. The HTTP response payload, on the other hand,\nis not so linear. The OpenStack API List Instance and Show Instance Inform-\nation operations return bigger payloads than the corresponding Deltacloud API\noperations, the Delete Image operation returns the same payload in both cases\n(a void HTTP response body) and the remaining operations return a smaller\npayload than the Deltacloud API counterparts. In the case of the Reboot In-\nstance and Delete Instance operations, the HTTP responses of the OpenStack\nAPI do not provide a HTTP body. Usually, the HTTP response of the Delete\nInstance operation defined by the Deltacloud API does not provide a HTTP body\nalso. However, since the OpenStack driver defined the destroy_instance method\nas an alias of the stop_instance method, the deletion of an OpenStack instance\nwith the Deltacloud API stops the instance. In fact it sends the delete instance\ninstruction, but returns the Stop Instance operation information.\n\n7.2.3 CloudStack Interaction Tests and Results\n\nThe third party CloudStack driver was added to Deltacloud in order to integrate\nThe CloudStack IaaS platform, but it did not work. From the analysis of the\ndriver implementation, it was possible to conclude that, although the driver is\nbased on the Deltacloud mock driver, it is incomplete and, thus, non functional.\nThe driver implementation includes a Ruby CloudStack client that can be used\nto interact successfully with the CloudStack IaaS platform, but it does not imple-\n\n\n\n7.2. TESTING AND EVALUATION 213\n\nment the Deltacloud collections. Since the development of the CloudStack driver\nfalls out of the scope of this project, the integration of the CloudStack platform\nwas dropped.\n\n7.2.4 PACI Interaction Tests and Results\n\nThe interaction with the PACI IaaS platform was tested by starting the Deltacloud\nserver with the PACI driver and the URL endpoint of the Lunacloud PACI server\nAPI, using the localhost 3001 port:\n$ deltacloudd ?i paci ?P \" http :// apicontrol . lunacloud . com:4465/ paci /v1 .0/ \" ?p 3001\nStarting Deltacloud API : : paci : : http :// apicontrol . lunacloud . com:4465/ paci /v1 .0/\n: : http :// l o c a l h o s t :3001/ api\nThin web server ( v1 . 6 . 1 codename Death Proof )\nDebugging ON\nMaximum connections set to 1024\nListening on l o c a l h o s t :3001 , CTRL+C to stop\n\nThe PACI API uses a HTTP basic authentication method without any hash\nfunction applied to the user\u2019s password. Nevertheless, Lunacloud uses crypto-\ngraphic protocols to protect the HTTP communications between the users and the\nPACI IaaS platform via the https://apicontrol.lunacloud.com:4463/paci/v1.0/\nendpoint. The PACI API endpoint used to test the PACI driver interaction did\nnot use these cryptographic protocols in order to allow the visualization of the\nHTTP request/response packets with Wireshark.\n\nThe developed PACI driver is fully functional and the interaction with the\nDeltacloud API showed that, in the case of the Create Load Balancer operations,\nDeltacloud requires additional parameters, e.g., realm_id,\nlistener_protocol, listener_balancer_port and listener_instance_port, that are\nnot specified by the PACI API Create Load Balancer operation, which requires\nonly a name parameter. Thus, to implement this operation with the Deltacloud\nAPI some mock default values were attributed to the realm_id, listener_protocol,\nlistener_balancer_port and listener_instance_port parameters.\n\nContrary to the open source IaaS platforms (OpenNebula, OpenStack and\nCloudStack), which were in the same test network as the laptop used to perform\nthe tests, the PACI IaaS platform was in Lunacloud\u2019s network, thirteen hoops\naway from the created test network:\n\n\n\n214 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\n$ traceroute apicontrol . lunacloud . com\ntraceroute to apicontrol . lunacloud . com\n(80.172.242.108) , 30 hops max, 60 byte packets\n1 192.168.1.1 ( 1 9 2 . 1 6 8 . 1 . 1 ) 0.691 ms 1.637 ms 1.982 ms\n2 10.0.16.250 ( 1 0 . 0 . 1 6 . 2 5 0 ) 7.498 ms 7.625 ms 7.744 ms\n3 salvador . core . ipp . pt (193.136.60.250) 12.569 ms 12.747 ms 12.867 ms\n4 193.136.61.2 ( 1 9 3 . 1 3 6 . 6 1 . 2 ) 7.720 ms 7.856 ms 7.974 ms\n5 r2isep . wan . ipp . pt (193.136.56.62) 14.483 ms 17.741 ms 23.407 ms\n6 00.wan . ipp . pt ( 1 9 3 . 1 3 6 . 5 6 . 1 ) 31.001 ms 32.701 ms 30.747 ms\n7 border . wan . ipp . pt (193.136.56.254) 31.673 ms 31.774 ms 31.859 ms\n8 193.136.4.177 (193.136.4.177) 31.073 ms 31.174 ms 31.252 ms\n9 router3 .10 ge .dwdm. l i s b o a . fccn . pt ( 1 9 3 . 1 3 6 . 1 . 1 ) 32.247 ms 2.363 ms ?\n\n10 router4 .10 ge . cr2 . l i s b o a . fccn . pt ( 1 9 3 . 1 3 7 . 0 . 2 0 ) 36.660 ms 37.314 ms 37.800 ms\n11 claranet . as8426 . gigapix . pt (193.136.251.5) 38.569 ms 39.566 ms 40.703 ms\n12 176.111.111.31 (176.111.111.31) 60.652 ms 61.151 ms 61.785 ms\n13 80.172.242.108 (80.172.242.108) 64.270 ms 63.101 ms 29.623 ms\n\nAs a result, in the case of the tests performed with PACI, the network latency\nwas measured (using the Linux command line ping) and deducted from the time\nresponse measurements. Figure 7.6 displays the registered network latency.\n\nFigure 7.6: PACI IaaS platform API connection latency values.\n\nThe list of operations tested in both cases (Deltacloud API and PACI API)\nwas ordered as follows:\n\n1. List Images;\n\n2. Show Image Information;\n\n3. Create Instance;\n\n4. List Instances;\n\n5. Show Instance Information;\n\n6. Start Instance;\n\n\n\n7.2. TESTING AND EVALUATION 215\n\n7. Create Load Balancer;\n\n8. Associate Instance to Load Balancer;\n\n9. List Load Balancers;\n\n10. Show Load Balancer Information;\n\n11. Dissociate Instance from Load Balancer;\n\n12. Delete Load Balancer;\n\n13. Stop Instance;\n\n14. Delete Instance.\n\nThe CentOS-62-x86_64-vm image was selected from the 103 images (OS tem-\nplates) available for the PACI IaaS platform. The Create Instance operation used\na modest hardware profile (1 virtual CPU core, 512 MB of RAM and 10 GB of\ndisk) in order to maintain identical test conditions. The PACI IaaS platform,\nunlike the deployed OpenNebula, OpenStack and CloudStack nodes, is a pub-\nlic cloud platform configured for high availability and performance and, thus,\nprovides physical resources with a different scale (it uses multiple PACI nodes\nwith clusters of 24 CPU cores and 96 GB RAM).\n\nThe results of the time response tests are presented in the charter of Figure\n7.7.\n\nFigure 7.7: PACI time response comparison.\n\nThe analysis of the obtained results shows that, despite the registered latency,\nthe time response values of the PACI API operations are lower than the values\n\n\n\n216 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\nregistered for the OpenNebula OCCI API and OpenStack API operations, with\nthe exception of the List Images operation. Although, the List Image operation\nlists 103 Images in comparison with the 10 images that were listed by the same\noperations of the OpenNebula OCCI API and OpenStack API. In comparison,\nthe results of the interaction with the Deltacloud API, using the PACI driver,\nshow a significantly time response increase, mainly with the List Instances, List\nImages, List Load Balancers and Show Load Balancers Information operations.\nThe List Images operation presents the highest time response value, since the\ndriver has to process the information of 103 returned images. On the other\nhand, the time response results for the remaining operations is justified by the\nnecessity to perform additional calls to the back-end PACI API and to process\nthe returned information. The refinement of this methodology may improve the\nobserved time response. Other Deltacloud API operations, e.g., Show Instance\nInformation, Stop Instance, Start Instance, Delete Instance and Show Image In-\nformation present better time response results than the corresponding operations\nvia the OpenNebula and OpenStack drivers.\n\nThe results of the HTTP request packet length and returned payload for the\nlisted operations in both cases are presented in Figure 7.8.\n\nFigure 7.8: PACI HTTP request/response length comparison.\n\nSince the List Images operation presents, in comparison to the remaining op-\nerations, a much higher received payload, the chart scale is not appropriate to\nanalyse the overall payload results. The complete set of results is provided in the\nadjoining table. The length of the HTTP request packets is bigger for the PACI\nAPI operations with the exception of the Create Load Balancer, Associate Load\nBalancer and Dissociate Load Balancer operations. These Deltacloud API op-\n\n\n\n7.3. CONCLUSIONS 217\n\nerations require more parameters than the corresponding PACI API operations.\nThe Deltacloud API operations return a larger payload than the direct API calls\nwith the exception of the Show Load Balancer Information, Delete Instance and\nDelete Load Balancer operations. In the case of the last two operations, the\nDeltacloud API does not send a HTTP response body.\n\n7.3 Conclusions\n\nThis chapter presented the interoperable service test bed, including the deploy-\nment of all components, the test and evaluation of the driver modules used. The\ntests involved direct API calls (IaaS platforms API) and calls via the implemen-\nted interoperable service (Deltacloud API and driver modules) and consisted of\nthe measurement of the time response and data payload for the implemented\noperations.\n\nThe installation procedures of the OpenNebula, OpenStack and CloudStack\nIaaS platforms were described. The OpenNebula and CloudStack deployments\nconsist of monolithic software installations, while the OpenStack deployment is\na modular installation process (composed by the installation of the Keystone,\nGlance and Nova projects). Thus, the OpenNebula and CloudStack deployments\nwere simpler in comparison to the OpenStack deployment, which was more com-\nplex. The centralized management of the OpenNebula and CloudStack platforms\nsimplifies also the cloud user interaction with the platforms resources, i.e., the\nusers can interact with and manage the available resources via a single API, in\ncontrast with OpenStack that uses an API for each installed project.\n\nFrom integration of OpenNebula, OpenStack, CloudStack and PACI via driver\nmodules with Deltacloud showed that the OpenNebula and OpenStack drivers,\nofficially included with Deltacloud, were functional and managed to operate with\nrecent released versions of its respective IaaS platforms after minor corrections\nand improvements. The third-party CloudStack driver however, was incomplete\nand, therefore, non functional. Thus, the integration of CloudStack IaaS plat-\nform with the Deltacloud framework was abandoned. The PACI driver, which\nwas totally developed in order to integrate the PACI IaaS platform with the\nDeltacloud abstraction framework, was fully functional.\n\nThe tests results proved that Deltacloud (and the integrated driver modules)\nintroduces a time response delay, observable when compared to the direct IaaS\nplatforms API calls. The reason for this occurrence is explained by the abstrac-\ntion processing and the re-transmission of the HTTP request, performed by the\nincluded driver modules, to the back-end IaaS platforms API. This is particu-\nlarly visible with the Deltacloud API operations whose drivers need to perform\nmultiple HTTP calls to the back-end IaaS platform API to obtain the necessary\ninformation, e.g., List Images, List Instances, List Load Balancers operations in\n\n\n\n218 CHAPTER 7. TEST, DEBUGGING AND VALIDATION\n\ngeneral and Show Load Balancer Information in particular. This last operation\nrequests internally a list of available instances to the back-end PACI IaaS plat-\nform API in order to show the list of dissociated instances for a specific Load\nBalancer.\n\nThe packet request and response payload results displayed, in the majority of\nthe operations tested, a smaller HTTP request packet length for the Deltacloud\nAPI operations. This is explained by the syntax adopted by the Deltacloud API\nthat, in the majority of the operations, uses less parameters than the operations of\nthe dedicated IaaS platforms API. The results of the HTTP response payload were\nprominently bigger for the Deltacloud API List Instance, List Image, List Load\nBalancer and List Hardware Profiles operations, meaning that the Deltacloud\nAPI exposes more information when listing the available resources.\n\nIn the case of the PACI tests, and although the platform was located at the\nLunacloud premises, the time response for the direct interaction with the PACI\nIaaS platform was faster than for the direct interaction with the OpenNebula\nand OpenStack IaaS platforms. This was also reflected when interacting via the\nDeltacloud API (and the developed PACI driver), which performed better in\nterms of time response than via the other drivers. The exception was the List\nInstances operation that registered the highest time response and payload values,\ndue to the fact that it listed 103 images.\n\n\n\nChapter 8\n\nConclusions\n\nThis chapter presents the conclusions regarding the integrated management of\nIaaS cloud computing resources selected, installed and tested.\n\n8.1 Discussion\n\nIn order to propose and develop an interoperable service for the integrated man-\nagement of cloud resources provisioned by the IaaS platforms used by Lunacloud,\na survey was conducted to characterise and compare the features of the most\npopular open source IaaS platforms - OpenNebula, OpenStack, CloudStack and\nEucalyptus - and the proprietary IaaS platform used by Lunacloud - PACI. This\nsurvey concluded that the open-source IaaS platforms expose similar function-\nalities. Although, between them, the architecture, interface library operations\nand governance models are significantly different. The proprietary solution does\nnot support directly the interaction with other IaaS platforms and originates, on\npurpose, the vendor lock-in problem to monetize new products and paid support\nservices. It was also observed that the proprietary IaaS platform has a smaller\ngroup of functionalities in comparison to the open source IaaS platforms studied.\n\nEucalyptus, which is essentially a private IaaS platform to test AWS imple-\nmentations, was discarded and the client interface API of the OpenNebula, Open-\nStack, CloudStack and PACI IaaS platforms were studied in detail to identify\ntheir common and distinctive features. The conclusion was that there are signi-\nficant differences regarding the type and number of interfaces, the level of cus-\ntomization, the organization of the groups of operations and the structure of\nthe request/response messages provided by the four IaaS platform interface lib-\nraries studied. The identified common management components between the\nfour platforms were analysed according to the number of similar operations and\n\n219\n\n\n\n220 CHAPTER 8. CONCLUSIONS\n\nsend/receive parameters in order to propose and develop an interoperable inter-\nface solution that meets the main objective of this dissertation.\n\nA second survey on existing IaaS abstraction solutions resulted in the selec-\ntion and comparison of the Deltacloud framework and the jClouds and Libcloud\nlibraries. The comparison between these abstraction solutions resulted in the se-\nlection of the Deltacloud framework since it provides natively more services (Web\ndashboard, multiple API, a Ruby client application) and allows the integration\nof new IaaS platforms through the development of new driver modules.\n\nThe decision between the development of a new interoperable interface and\nthe adoption of Deltacloud, which included the implementation of a new driver\nmodule, resulted in the latter. While the first option is specific and not extensible\nto other implementations, the Deltacloud abstraction framework already provides\nmany of the desired functionalities and includes several IaaS platform driver\nmodules.\n\nThe Deltacloud framework was installed and the officially supported Open-\nNebula and OpenStack drivers as well a third-party CloudStack driver were re-\nused. The PACI driver did not exist and had to be developed to integrate the\nPACI IaaS platform with the Deltacloud framework. To test the drivers and\nthe overall interoperable service, a test bed composed of the OpenNebula, Open-\nStack and CloudStack IaaS platforms was assembled. The PACI IaaS platform\nprovisioned by Lunacloud was adopted.\n\nThe test bed was then used to verify the integration and correct operation of\nthe OpenNebula, OpenStack, CloudStack and PACI driver modules. These tests\nshowed that the CloudStack driver module was incomplete and non functional\nand, for this reason, the integration of the CloudStack platform was dropped.\nThe OpenNebula and OpenStack drivers were functional and managed to op-\nerate with recent released versions of its respective IaaS platforms after minor\ncorrections and improvements. The PACI driver, on the other hand, did not\nmanifest any malfunctioning and it was fully functional. A second set of tests\nwas conducted involving the integrated platforms (OpenNebula, OpenStack and\nPACI) to compare the time response and data payload between direct (via the\nOpenNebula, OpenStack and PACI API) and Deltacloud (via the OpenNebula,\nOpenStack and PACI driver modules) operations. The results showed that the\nuse of Deltacloud (and the integrated driver modules) to access the IaaS plat-\nform resources introduces an expected time response delay when compared with\nthe direct IaaS platforms API calls. In the majority of the operations with the\nthree platforms, the HTTP request packet length of the Deltacloud API was\nlower and the results of the HTTP response payload were prominently bigger for\nthe Deltacloud API operations that list multiple resources (List Instance, List\nImage, List Load Balancer and List Hardware Profiles operations). In general,\n\n\n\n8.2. ENVISAGED USE CASES 221\n\nthe abstraction brought by the Deltacloud framework lowers the HTTP response\ndetail to the fundamental information and reduces, in most cases, the number of\nrequired operation parameters. The PACI platform results, despite being located\nat the Lunacloud premises, for the direct interaction were lower than the results\nof a direct interaction with the OpenNebula and OpenStack IaaS platforms. Al-\nthough the PACI driver performed according to the results of the other drivers,\nit can be refined to enhance the time response of certain operations. Future\nimprovements to the Deltacloud API may also enhance the performance of the\nincluded drivers.\n\nThe solution adopted for the integrated management and provision of IaaS\nresources from PACI, OpenNebula and OpenStack IaaS platforms fulfils the spe-\ncified requirements, i.e., provides Lunacloud with the ability to expand the range\nof adopted IaaS platforms and offers a Web dashboard and a REST API for\nthe integrated management. The contributions of this work include the surveys\nand comparisons made, the selection of the abstraction framework, the testbed\nplatform to test the interoperable service proposed and, last but not the least,\nthe PACI driver developed. The PACI driver was shared with the Deltacloud\ncommunity and the observed OpenNebula and OpenStack driver malfunctions\nwere also reported.\n\n8.2 Envisaged Use Cases\n\nThere are two main use cases:\n\n\u2022 IaaS resource consumer case - this approach is intended to deliver end\nusers with seamless access to IaaS resources from different IaaS platforms.\nFurthermore, it empowers the IaaS resource consumer with the ability to\nmanage his resources via a single, common interface without needing to\nknow any of the specificities of the underlying cloud computing IaaS plat-\nform API. In such a scenario, an user may contract, consume and free IaaS\nresources from any IaaS platform with an implemented Deltacloud driver.\n\n\u2022 IaaS resource provider case - this solution exposes a single, common inter-\nface, allows the integration of multiple IaaS platforms and contributes to\nincrease the number of offered resources and to reduce the vendor lock-in\nproblem. Moreover, if several IaaS providers adopt this approach, federa-\ntions of resource providers may be established, strengthening the visibility\nand increasing the market share of the federated providers.\n\n\n\n222 CHAPTER 8. CONCLUSIONS\n\n8.3 Future Developments\n\nThe PACI driver information processing can be refined by adopting Ruby libraries\nthat parse the information returned from the back-end PACI IaaS platform API\nfaster, e.g., changing the REXML Ruby library by the Nokogiri Ruby library\n[192]. The performance of the driver could also improve significantly if a newer\nPACI API version containing more information per operation could be adopted\nto decrease the number of necessary HTTP requests that have to be executed\ninternally to obtain the desired information.\n\nThe addition of new collections and the redefinition of the required API para-\nmeters within Deltacloud would increase the range of functionalities provided\nby the integrated IaaS platforms and resolve some problems, e.g., the need to\ninstantiate mock parameters when using the Create Load Balancer operation to\ncomply with the Deltacloud API requirements.\n\nThe development of an additional GUI module to provide end-users with a\nfriendly intuitive interface to manage their resources from different back-end IaaS\nplatforms in a highly abstracted way.\n\nDo a survey on existing standards to develop a resource migration service\nbetween the integrated IaaS platforms.\n\nThe development of a fully functional CloudStack driver would allow the integ-\nration of another IaaS platform to the Deltacloud framework and, consequently,\nto the proposed interoperable service.\n\n\n\nAppendices\n\n223\n\n\n\n\n\nAppendix A\n\nCloud Client Library\n\n1\n2 require \u2019 rubygems \u2019\n3 require \u2019 uri \u2019\n4 require \u2019 net / http \u2019\n5 require \" rexml/document \"\n6\n7 begin\n8 require \u2019 rexml/ formatters / pretty \u2019\n9 REXML_FORMATTERS=true\n\n10 rescue LoadError\n11 REXML_FORMATTERS=f a l s e\n12 end\n13\n14 begin\n15 require \u2019 net / http / post / multipart \u2019\n16 MULTIPART_LOADED=true\n17 rescue LoadError\n18 MULTIPART_LOADED=f a l s e\n19 end\n20\n21 ########## The CloudClient module contains general\n22 # f u n c t i o n a l i t i e s for connection and error management\n23 ###########################################################################\n24\n25 module CloudClient\n26\n27 ##########################################################################\n28 # Starts an http connection and c a l l s the block provided . SSL f l a g\n29 # i s set i f needed .\n30 ##########################################################################\n31\n32 def s e l f . http_start ( url , timeout , &amp;block )\n33 http = Net : :HTTP. new( url . host , url . port )\n34 i f timeout\n35 http . read_timeout = timeout . to_i\n36 end\n37 i f url . scheme==\u2019 https \u2019\n38 http . use_ssl = true\n39 http . verify_mode=OpenSSL : : SSL : :VERIFY_NONE\n40 end\n41 begin\n42 res = http . s t a r t do | connection |\n43 block . c a l l ( connection )\n44 end\n\n225\n\n\n\n226 APPENDIX A. CLOUD CLIENT LIBRARY\n\n45 rescue Errno : :ECONNREFUSED => e\n46 s t r = \" Error connecting to server (#{e . to_s }).\\ n\"\n47 s t r&lt;&lt;\" Server : #{url . host }:#{ url . port } \"\n48 return CloudClient : : Error . new( str , \" 503 \" )\n49 rescue Errno : :ETIMEDOUT => e\n50 s t r = \" Error timeout connecting to server (#{e . to_s }).\\ n\"\n51 s t r&lt;&lt;\" Server : #{url . host }:#{ url . port } \"\n52 return CloudClient : : Error . new( str , \" 504 \" )\n53 rescue Timeout : : Error => e\n54 s t r = \" Error timeout while connected to server (#{e . to_s }).\\ n\"\n55 s t r&lt;&lt;\" Server : #{url . host }:#{ url . port } \"\n56 return CloudClient : : Error . new( str , \" 504 \" )\n57 end\n58 i f res . is_a ?( Net : : HTTPSuccess)\n59 res\n60 e l s e\n61 CloudClient : : Error . new( res . body , res . code )\n62 end\n63 end\n64\n65 # #########################################################################\n66 # The Error Class represents a generic error in the Cloud Client\n67 # l i b r a r y . It contains a readable representation of the error .\n68 # #########################################################################\n69 c l a s s Error\n70 attr_reader : message\n71 attr_reader : code\n72\n73 # message+ a d es cr ip ti o n of the error\n74 def i n i t i a l i z e ( message=nil , code=\" 500 \" )\n75 @message=message\n76 @code=code\n77 end\n78 def to_s ()\n79 @message\n80 end\n81 end\n82\n83 # #########################################################################\n84 # Returns true i f the object returned by a method of the PACI\n85 # l i b r a r y i s an Error\n86 # #########################################################################\n87 def s e l f . is_error ?( value )\n88 value . c l a s s==CloudClient : : Error\n89 end\n90 end\n\n\n\nAppendix B\n\nPACI Client\n\n1 require \u2019 rubygems \u2019\n2 require \u2019 rexml/document \u2019\n3 require \u2019 uri \u2019\n4 require \u2019 deltacloud / d r i v e r s / paci / cloud_client \u2019\n5\n6 module PACIClient\n7\n8 #####################################################################\n9 # Client Library to i n t e r f a c e with the P a r a l l e l s PACI Service\n\n10 #####################################################################\n11 c l a s s Client\n12\n13 attr_accessor : endpoint\n14\n15 ######################################################################\n16 # I n i t i a l i z e PACI l i b r a r y\n17 ######################################################################\n18 def i n i t i a l i z e ( endpoint_str=nil , user=nil , pass=nil ,\n19 timeout=nil , debug_flag=true )\n20 @debug = debug_flag\n21 @timeout = timeout\n22\n23 # endpoint processement\n24 # LunaCloud \u2019 s endpoint i s e i t h e r\n25 # \" http :// apicontrol . lunacloud . com:4465/ paci /v1 . 0 \" or\n26 # \" https :// apicontrol . lunacloud . com:4463/ paci /v1 . 0 \"\n27 i f endpoint_str[?1]== \u2019 / \u2019\n28 @endpoint = endpoint_str . gsub (/\\/$ / , \u2019 \u2019 )\n29 e l s e\n30 @endpoint = endpoint_str\n31 end\n32\n33 i f ! @endpoint | | @endpoint==\" \"\n34 r a i s e \" Endpoint URL not configured !\n35 Client needs to set \\ \u2019X?Deltacloud ?Provider \\ \u2019\n36 HTTP request header , or , Deltacloud server\n37 administrator must set the API_PROVIDER\n38 environment variable \"\n39 end\n40\n41 # Autentication\n42 i f user &amp;&amp; pass\n43 @auth = [ user , pass ]\n44 # include here c a l l for ENV. Variables auth i f\n45 # necessary ( t h i s method needs to be implemented in\n\n227\n\n\n\n228 APPENDIX B. PACI CLIENT\n\n46 # the cloud_client . rb f i l e )\n47 end\n48\n49 i f ! @auth\n50 r a i s e \"No authorization data present \"\n51 end\n52 # lunacloud performs SHA?1 on server side , uncomment\n53 # the following l i n e in case SHA?1 hash encryption i s needed\n54 # @auth [ 1 ] = Digest : : SHA1. hexdigest (@auth [ 1 ] )\n55 end\n56\n57\n58 #################################\n59 # Instance Request Methods #\n60 #################################\n61\n62 ######################################################################\n63 # Retrieve the l i s t of the a v a i l a b l e PACI Servers\n64 ######################################################################\n65 def get_instances ()\n66 get ( \u2019 /ve \u2019 )\n67 end\n68\n69 ######################################################################\n70 # Retrieve the l i s t of the a v a i l a b l e PACI Servers by subscription\n71 ######################################################################\n72 def get_instances ( subscription_id . to_s )\n73 get ( \u2019 /ve /? subscription=\u2019+subscription_id )\n74 end\n75\n76 ######################################################################\n77 # Obtain Server Information\n78 ######################################################################\n79 def get_instance (ve_name)\n80 get ( \u2019 /ve/ \u2019+ve_name . to_s )\n81 end\n82\n83 ######################################################################\n84 # Create a Server\n85 ######################################################################\n86 def create_instance (xml)\n87 post ( \u2019 /ve \u2019 , xml)\n88 end\n89\n90 ######################################################################\n91 # Create a Server from Image\n92 ######################################################################\n93 def create_instance_from_image (ve_name , image_name)\n94 post ( \u2019 /ve/ \u2019+ve_name . to_s+\u2019 /from/ \u2019+image_name . to_s )\n95 end\n96\n97 ######################################################################\n98 # Delete Server\n99 ######################################################################\n\n100 def delete_instance (ve_name)\n101 delete ( \u2019 /ve/ \u2019+ ve_name . to_s )\n102 end\n103\n104\n105 ######################################################################\n106 # Start the Server\n107 ######################################################################\n108 def start_instance (ve_name)\n109 put ( \u2019 /ve/ \u2019+ve_name . to_s+\u2019 / s t a r t \u2019 )\n110 end\n111\n112\n113 ######################################################################\n114 # Stop the Server\n115 ######################################################################\n\n\n\n229\n\n116 def stop_instance (ve_name)\n117 put ( \u2019 /ve/ \u2019+ve_name . to_s+\u2019 / stop \u2019 )\n118 end\n119\n120\n121 #################################\n122 # Images Request Methods #\n123 #################################\n124\n125 ######################################################################\n126 # Create Image from Server\n127 ######################################################################\n128 def create_image (ve_name , image_name)\n129 post ( \u2019 /image/ \u2019+ve_name . to_s+\u2019 / create / \u2019+image_name . to_s )\n130 end\n131\n132 ######################################################################\n133 # Create Image from Server ( subscription )\n134 ######################################################################\n135 def create_image ( subscription_id . to_s , ve_name . to_s , image_name . to_s )\n136 post ( \u2019 /image/ \u2019+ve_name+\u2019 / \u2019+subscription_id+\u2019 / create / \u2019+image_name)\n137 end\n138\n139 ######################################################################\n140 # List Images\n141 ######################################################################\n142 def get_images ()\n143 get ( \u2019 /image \u2019 )\n144 end\n145\n146 ######################################################################\n147 # Get Image Information\n148 ######################################################################\n149 def get_image (image_name)\n150 get ( \u2019 /image/ \u2019+image_name)\n151 end\n152\n153 ######################################################################\n154 # Delete Image\n155 ######################################################################\n156 def delete_image ( image_id )\n157 delete ( \u2019 /image/ \u2019+image_id )\n158 end\n159\n160 ######################################################################\n161 # Get OS Template Information\n162 ######################################################################\n163 def get_os_template ( template_name )\n164 get ( \u2019 / template / \u2019+template_name . to_s )\n165 end\n166\n167 ######################################################################\n168 # List I n s t a l l e d OS Templates\n169 ######################################################################\n170 def get_os_templates ()\n171 get ( \u2019 / template \u2019 )\n172 end\n173\n174 #################################\n175 # Load Balancer Request Methods #\n176 #################################\n177\n178 ######################################################################\n179 # List Load Balancers\n180 ######################################################################\n181 def get_load_balancers ()\n182 get ( \u2019 /load?balancer \u2019 )\n183 end\n184\n185\n\n\n\n230 APPENDIX B. PACI CLIENT\n\n186 ######################################################################\n187 # Get Load Balance Information\n188 ######################################################################\n189 def get_load_balancer (lb_name)\n190 get ( \u2019 /load?balancer / \u2019+lb_name . to_s )\n191 end\n192\n193 #####################################################################\n194 # Create Load Balancer\n195 ######################################################################\n196 def create_load_balancer (lb_name)\n197 post ( \u2019 /load?balancer / create / \u2019+lb_name . to_s )\n198 end\n199\n200 #####################################################################\n201 # Create Load Balancer ( subscription )\n202 ######################################################################\n203 def create_load_balancer ( subscription_id , lb_name)\n204 post ( \u2019 load?balancer / \u2019+subscription_id . to_s+\u2019 / create / \u2019+lb_name . to_s )\n205 end\n206\n207 ######################################################################\n208 # Delete Load Balancer\n209 ######################################################################\n210 def delete_load_balancer (lb_name)\n211 delete ( \u2019 /load?balancer / \u2019+lb_name . to_s )\n212 end\n213\n214 ######################################################################\n215 # Attache Server To Load Balancer\n216 ######################################################################\n217 def register_instance (lb_name , ve_name)\n218 post ( \u2019 /load?balancer / \u2019+lb_name . to_s+\u2019 / \u2019+ve_name . to_s )\n219 end\n220\n221 ######################################################################\n222 # Detache Server From Load Balancer\n223 ######################################################################\n224 def unregister_instance (lb_name , ve_name)\n225 delete ( \u2019 /load?balancer / \u2019+lb_name . to_s+\u2019 / \u2019+ve_name . to_s )\n226 end\n227\n228\n229 private\n230\n231 def get ( path )\n232 url = URI . parse ( @endpoint+path )\n233 req = Net : :HTTP: : Get . new( url . path )\n234 do_request ( url , req )\n235 end\n236\n237 def post ( path , xml=n i l )\n238 url = URI . parse ( @endpoint+path )\n239 req = Net : :HTTP: : Post . new( url . path )\n240 i f ! xml . n i l ?\n241 req . body=xml . to_s\n242 req . content_type= \u2019 application /xml \u2019\n243 end\n244 do_request ( url , req )\n245 end\n246\n247 def delete ( path , xml=n i l )\n248\n249 url = URI . parse ( @endpoint+path )\n250 req = Net : :HTTP: : Delete . new( url . path )\n251 i f ! xml . n i l ?\n252 req . body=xml . to_s\n253 req . content_type= \u2019 application /xml \u2019\n254 end\n255 do_request ( url , req )\n\n\n\n231\n\n256 end\n257\n258 def put ( path , xml=n i l )\n259 url = URI . parse ( @endpoint+path )\n260 req = Net : :HTTP: : Put . new( url . path )\n261 i f ! xml . n i l ?\n262 req . body=xml . to_s\n263 req . content_type= \u2019 application /xml \u2019\n264 end\n265 do_request ( url , req )\n266 end\n267\n268 def do_request ( url , req )\n269 req . basic_auth @auth [ 0 ] , @auth [ 1 ]\n270\n271 res = CloudClient : : http_start ( url , @timeout ) do | http |\n272 http . request ( req )\n273 end\n274\n275 i f CloudClient : : is_error ?( res )\n276 return res\n277 e l s e\n278 return res . body\n279 end\n280 end\n281 end\n282 end\n\n\n\n\n\nAppendix C\n\nPACI Driver\n\n1 require \u2019 deltacloud / d r i v e r s / paci / paci_client \u2019\n2 require \u2019 erb \u2019\n3 require \u2019 rexml/document \u2019\n4\n5 module Deltacloud\n6 module Drivers\n7 module Paci\n8\n9 c l a s s PaciDriver &lt;Deltacloud : : BaseDriver\n\n10\n11 feature : instances , : user_name\n12 feature : instances , : authentication_password\n13 feature : images , : user_name\n14\n15 DEFAULT_REGION = \u2019EU West \u2019\n16\n17 ########################\n18 # Hardware P r o f i l e s ( LunaCloud )\n19 ###########################################\n20\n21 define_hardware_profile ( \u2019 PointFive \u2019 ) do\n22 cpu (1 . . 1)\n23 memory (512 . . 512)\n24 storage (10 . . 10)\n25 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n26 end\n27\n28 define_hardware_profile ( \u2019One \u2019 ) do\n29 cpu (1 . . 1)\n30 memory (1024 . . 1024)\n31 storage (50 . . 50)\n32 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n33 end\n34\n35 define_hardware_profile ( \u2019Two \u2019 ) do\n36 cpu (2 . . 2)\n37 memory (2048 . . 2048)\n38 storage (100 . . 100)\n39 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n40 end\n41\n42 define_hardware_profile ( \u2019 Four \u2019 ) do\n43 cpu (2 . . 2)\n44 memory (4096 . . 4096)\n45 storage (250 . . 250)\n\n233\n\n\n\n234 APPENDIX C. PACI DRIVER\n\n46 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n47 end\n48\n49 define_hardware_profile ( \u2019 Eight \u2019 ) do\n50 cpu (2 . . 2)\n51 memory (8192 . . 8192)\n52 storage (500 . . 500)\n53 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n54 end\n55\n56 define_hardware_profile ( \u2019 OneSix \u2019 ) do\n57 cpu (4 . . 4)\n58 memory (16384 . . 16384)\n59 storage (1000 . . 1000)\n60 #a r c h i t e c t u r e [ \u2019 x86_64 \u2019 , \u2019 i386 \u2019 ]\n61 end\n62\n63 define_hardware_profile ( \u2019Custom \u2019 ) do\n64 cpu (1 . . 8)\n65 memory (512 . . 192?512)\n66 storage (10 . . 2000)\n67 end\n68\n69 ########################\n70 # Realms\n71 #######################\n72\n73 (REALMS = [ Realm . new({\n74 : id => \u2019EU_West \u2019 ,\n75 : name => \u2019EU_West \u2019 ,\n76 : l i m i t => \u2019Unknown \u2019 ,\n77 : state => \u2019AVAILABLE \u2019\n78 }) ,\n79 Realm . new({\n80 : id => \u2019 EU_Central \u2019 ,\n81 : name => \u2019 EU_Central \u2019 ,\n82 : l i m i t => \u2019Unknown \u2019 ,\n83 : state => \u2019AVAILABLE \u2019\n84 })\n85 ] ) unless defined ?( REALMS )\n86\n87 def realms ( credentials , opts ={})\n88 return REALMS i f ( opts . n i l ? )\n89 r e s u l t s = REALMS\n90 # r e s u l t s = f il te r_o n ( results , : id , opts )\n91 # r e s u l t s\n92 end\n93\n94 ########################\n95 # Instances\n96 #######################\n97\n98\n99 # cpu power i s always 1600 MHz, default bandwidth set\n\n100 # to 10240 (min value ) but i t can go to 102400.\n101 # bandwidth , no?of?public ?ip , and no?of?public ?ipv6\n102 # are s t a t i c because no deltacloud instance f e a t u r e s\n103 # are a v a i l a b l e for these parameters .\n104\n105 # VM XML Template :\n106 VE_TEMPLATE = %q{\n107&lt;ve>\n108&lt;name><%=ve_name%></name>\n109&lt;cpu number=\"<%=opts [ : hwp_cpu]%>\" power=\"1600\"/>\n110&lt;ram?size><%=opts [ : hwp_memory ] . to_i%></ram?size >\n111&lt;bandwidth >10240</bandwidth>\n112&lt;no?of?public ?ip >1</no?of?public ?ip>\n113&lt;no?of?public ?ipv6 >0</no?of?public ?ipv6>\n114&lt;ve?disk l o c a l =\"true \" s i z e=\"<%=opts [ : hwp_storage]%>\"/>\n115&lt;platform>\n\n\n\n235\n\n116&lt;template?i n f o name=\"<%=image_id%>\"/>\n117&lt;os?i n f o technology=\"<%=tech%>\" type=\"<%=type%>\"/>\n118&lt;/platform>\n119&lt;backup?schedule name=\"weekly\"/>\n120&lt;% i f opts [ : password ] %>\n121&lt;admin login =\"root \" password=\"<%=opts [ : password]%>\"/>\n122&lt;% e l s e %>\n123&lt;admin login =\"root \" />\n124&lt;% end %>\n125&lt;/ve>\n126 }\n127\n128 VE_STATES = {\n129 \"CREATE\" => \"START\" ,\n130 \"CREATION_IN_PROGRESS\" => \"PENDING\" ,\n131 \"CREATED\" => \"STOPPED\" ,\n132 \"START_IN_PROGRESS\" => \"RUNNING\" ,\n133 \"STARTED\" => \"RUNNING\" ,\n134 \"STOP_IN_PROGRESS\" => \"STOPPING\" ,\n135 \"STOPPED\" => \"STOPPED\" ,\n136 \"DELETE_IN_PROGRESS\" => \"STOPPING\" ,\n137 \"DELETED\" => \"FINISHED\"\n138 }\n139\n140 define_instance_states do\n141 s t a r t . to ( : pending ) . on ( : create )\n142 pending . to ( : stopped ) . automatically\n143 stopped . to ( : pending ) . on ( : destroy )\n144 pending . to ( : f i n i s h ) . automatically\n145 stopped . to ( : running ) . on ( : s t a r t )\n146 running . to ( : stopping ) . on ( : stop )\n147 stopping . to ( : stopped ) . automatically\n148 running . to ( : stopping ) . on ( : destroy )\n149 stopping . to ( : f i n i s h ) . automatically\n150 end\n151\n152\n153 def instance ( credentials , opts ={})\n154 paci_client = new_client ( c r e d e n t i a l s )\n155 ve_name=opts [ : id ]\n156 xml = treat_response ( paci_client . get_instance (ve_name) )\n157 convert_instance (xml , c r e d e n t i a l s )\n158 end\n159\n160 def instances ( credentials , opts ={})\n161 paci_client = new_client ( c r e d e n t i a l s )\n162 xml = treat_response ( paci_client . get_instances () )\n163 instances = REXML: : Document . new(xml ) . root . elements .map do | d |\n164 convert_instances (d , c r e d e n t i a l s )\n165 end\n166 end\n167\n168 def create_instance ( credentials , image_id , opts ={})\n169 paci_client = new_client ( c r e d e n t i a l s )\n170\n171 # Processing name\n172 i f opts [ : name ] &amp;&amp; opts [ : name ] . length >0\n173 ve_name= opts [ : name ]\n174 e l s e\n175 time=Time . now . to_s\n176 time=time . s p l i t ( \u2019+\u2019 ) . f i r s t\n177 time=time . gsub (/\\D/ , \u2019 \u2019 )\n178 ve_name= \" Server ?\"+time\n179 end\n180\n181 # Mapping OS template i n f o\n182 img_xml = treat_response ( paci_client . get_os_template ( image_id ) )\n183 buffer = REXML: : Document . new(img_xml . to_s ) . root . a t t r i b u t e s\n184 tech=buffer [ \u2019 technology \u2019 ]\n185 type=buffer [ \u2019 osType \u2019 ]\n\n\n\n236 APPENDIX C. PACI DRIVER\n\n186\n187 # Building VE POST XML body\n188 req_xml = ERB. new(VE_TEMPLATE) . r e s u l t ( binding )\n189\n190 # Send/ Receive ( no need for a variable because\n191 # the returned message i s not used by Deltacloud )\n192 treat_response ( paci_client . create_instance ( req_xml ) )\n193\n194 # Show Instance XML ( Deltacloud XML)\n195 instance ( credentials , id : ve_name)\n196 end\n197\n198 def start_instance ( credentials , id )\n199 paci_client = new_client ( c r e d e n t i a l s )\n200 treat_response ( paci_client . start_instance ( id ) )\n201 ve_xml = treat_response ( paci_client . get_instance ( id ) )\n202 convert_instance (ve_xml , c r e d e n t i a l s )\n203 end\n204\n205 def stop_instance ( credentials , id )\n206 paci_client = new_client ( c r e d e n t i a l s )\n207 treat_response ( paci_client . stop_instance ( id ) )\n208 ve_xml = treat_response ( paci_client . get_instance ( id ) )\n209 convert_instance (ve_xml , c r e d e n t i a l s )\n210 end\n211\n212 def destroy_instance ( credentials , id )\n213 paci_client = new_client ( c r e d e n t i a l s )\n214 treat_response ( paci_client . delete_instance ( id ) )\n215 # 204 HTTP code returned\n216 end\n217\n218 ########################\n219 # Images\n220 #\n221 #\n222 ########################\n223 def image ( credentials , opts ={})\n224 paci_client = new_client ( c r e d e n t i a l s )\n225 xml = treat_response ( paci_client . get_os_template ( opts [ : id ] ) )\n226 convert_image (xml , c r e d e n t i a l s )\n227 end\n228\n229 def images ( credentials , opts ={})\n230 paci_client = new_client ( c r e d e n t i a l s )\n231 xml = treat_response ( paci_client . get_os_templates () )\n232 images = REXML: : Document . new(xml ) . root . elements .map do | d |\n233 convert_image (d , c r e d e n t i a l s )\n234 end\n235 end\n236\n237 # Since the OS Templates are owned by the cloud provider\n238 # the user can not delete them .\n239 # def destroy_image ( credentials , id )\n240 # paci_client = new_client ( c r e d e n t i a l s )\n241 # treat_response ( paci_client . delete_image ( id ) )\n242 # end\n243\n244 ########################\n245 # Load Balancers\n246 #######################\n247\n248 def load_balancer ( credentials , opts ={})\n249 paci_client = new_client ( c r e d e n t i a l s )\n250 xml=treat_response ( paci_client . get_load_balancer ( opts [ : id ] ) )\n251 convert_load_balancer (xml , c r e d e n t i a l s )\n252 end\n253\n254 def load_balancers ( credentials , opts ={})\n255 paci_client = new_client ( c r e d e n t i a l s )\n\n\n\n237\n\n256 xml=treat_response ( paci_client . get_load_balancers () )\n257 load_balancers=REXML: : Document . new(xml ) . root . elements .map do | d |\n258 convert_load_balancers (d , c r e d e n t i a l s )\n259 end\n260 end\n261\n262 def create_load_balancer ( credentials , opts ={})\n263 paci_client = new_client ( c r e d e n t i a l s )\n264 lb_name= opts [ \u2019name \u2019 ]\n265 i f lb_name . n i l ?\n266 time=Time . now . to_s\n267 time=time . s p l i t ( \u2019+\u2019 ) . f i r s t\n268 time=time . gsub (/\\D/ , \u2019 \u2019 )\n269 lb_name= \"LB?\"+time\n270 end\n271 treat_response ( paci_client . create_load_balancer (lb_name) )\n272 load_balancer ( credentials , id : lb_name )\n273\n274 end\n275\n276 def destroy_load_balancer ( credentials , id )\n277 paci_client = new_client ( c r e d e n t i a l s )\n278 treat_response ( paci_client . delete_load_balancer ( id ) )\n279 # 204 HTTP code returned\n280 end\n281\n282 def lb_register_instance ( credentials , opts ={})\n283 paci_client = new_client ( c r e d e n t i a l s )\n284 lb_name= opts [ : id ]\n285 instance_name= opts [ \u2019 instance_id \u2019 ]\n286 treat_response ( paci_client . register_instance (lb_name , instance_name ) )\n287 load_balancer ( credentials , id : lb_name)\n288 end\n289\n290 def lb_unregister_instance ( credentials , opts ={})\n291 paci_client = new_client ( c r e d e n t i a l s )\n292 lb_name= opts [ : id ]\n293 instance_name= opts [ \u2019 instance_id \u2019 ]\n294 treat_response ( paci_client . unregister_instance (lb_name , instance_name ) )\n295 load_balancer ( credentials , id : lb_name)\n296 end\n297\n298\n299 private\n300\n301 def new_client ( c r e d e n t i a l s )\n302 PACIClient : : Client . new( api_provider , c r e d e n t i a l s . user , c r e d e n t i a l s . password )\n303 end\n304\n305 ########################\n306 # Mapping Parameters\n307 ########################\n308\n309 # This method mapps only OS Templates\n310 def convert_image (xml , c r e d e n t i a l s )\n311 buffer = REXML: : Document . new(xml . to_s ) . root\n312 i f buffer . a t t r i b u t e s [ \u2019 active \u2019]==\" f a l s e \"\n313 default_state = \"DISABLED\"\n314 e l s e\n315 default_state = \"ACTIVE\"\n316 end\n317\n318 #mapping\n319 Image . new({\n320\n321 : id=>buffer . a t t r i b u t e s [ \u2019name \u2019 ] ,\n322 : name=>buffer . a t t r i b u t e s [ \u2019name \u2019 ] ,\n323 : de sc r ip ti on=>\"OS: \"+\n324 buffer . elements [ 2 ] . a t t r i b u t e s [ \u2019 value \u2019 ]+\n325 \" , V i r t u a l i z a t i o n type (VM/CT) : \"+\n\n\n\n238 APPENDIX C. PACI DRIVER\n\n326 buffer . a t t r i b u t e s [ \u2019 technology \u2019 ] ,\n327 : owner_id=>\" LunaCloud \" ,\n328 : state=>default_state ,\n329 : a r c h i t e c t u r e=>buffer . elements [ 1 ] . a t t r i b u t e s [ \u2019 value \u2019 ] ,\n330 : hardware_profiles=>hardware_profiles ( n i l )\n331 })\n332 end\n333\n334 # Convert PACI VM parameters to Deltacloud\n335 def convert_instance (xml , c r e d e n t i a l s )\n336 buffer = REXML: : Document . new(xml . to_s ) . root . elements\n337\n338 i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 1 \" &amp;&amp;\n339 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 512 \" &amp;&amp;\n340 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 10 \"\n341 instance_profile=\u2019 PointFive \u2019\n342 e l s i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 1 \" &amp;&amp;\n343 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 1024 \" &amp;&amp;\n344 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 50 \"\n345 instance_profile=\u2019One \u2019\n346 e l s i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 2 \" &amp;&amp;\n347 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 2048 \" &amp;&amp;\n348 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 100 \"\n349 instance_profile=\u2019Two \u2019\n350 e l s i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 2 \" &amp;&amp;\n351 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 4096 \" &amp;&amp;\n352 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 250 \"\n353 instance_profile=\u2019 Four \u2019\n354 e l s i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 2 \" &amp;&amp;\n355 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 8192 \" &amp;&amp;\n356 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 500 \"\n357 instance_profile=\u2019 Eight \u2019\n358 e l s i f buffer [ \u2019 cpu \u2019 ] . a t t r i b u t e s [ \u2019number \u2019]==\" 4 \" &amp;&amp;\n359 buffer [ \u2019ram?s i z e \u2019 ] . text==\" 16384 \" &amp;&amp;\n360 buffer [ \u2019 ve?disk \u2019 ] . a t t r i b u t e s [ \u2019 s i z e \u2019]==\" 1000 \"\n361 instance_profile=\u2019 OneSix \u2019\n362 e l s e\n363 instance_profile=\u2019Custom \u2019\n364 end\n365\n366 private_ip =[]\n367 public_ip =[]\n368\n369 i f buffer [ \u2019 network/ public ?ip \u2019 ]\n370 buffer . each ( \u2019 network/ public ?ip \u2019 ) { | ip | public_ip\n371&lt;&lt;InstanceAddress . new( ip . a t t r i b u t e s [ \u2019 address \u2019 ] . s p l i t ( \u2019 / \u2019 ) . f i r s t ,\n372 : type => : ipv4 )}\n373 end\n374\n375 i f buffer [ \u2019 network/ public ?ipv6 \u2019 ]\n376 buffer . each ( \u2019 network/ public ?ipv6 \u2019 ) { | ip | public_ip\n377&lt;&lt;InstanceAddress . new( ip . a t t r i b u t e s [ \u2019 address \u2019 ] . s p l i t ( \u2019 / \u2019 ) . f i r s t ,\n378 : type => : ipv6 )}\n379 end\n380\n381 private_ip\n382&lt;&lt;InstanceAddress . new( buffer [ \u2019 network \u2019 ] . a t t r i b u t e s [ \u2019 private ?ip \u2019 ] . s p l i t ( \u2019 / \u2019 ) . f i r s t ,\n383 : type => : ipv4 )\n384\n385 Instance . new( {\n386 : id=>buffer [ \u2019name \u2019 ] . text ,\n387 : owner_id=>buffer [ \u2019 subscription ?id \u2019 ] . text ,\n388 : name=>buffer [ \u2019name \u2019 ] . text ,\n389 : image_id=>buffer [ \u2019 platform \u2019 ] . elements [ \u2019 template?i n f o \u2019 ] . a t t r i b u t e s [ \u2019name \u2019 ] ,\n390 : instance_profile=>I n s t a n c e P r o f i l e . new( instance_profile ) ,\n391 : realm_id=>DEFAULT_REGION,\n392 : state=>VE_STATES[ buffer [ \u2019 state \u2019 ] . text ] ,\n393 : public_addresses=>public_ip ,\n394 : private_addresses=>private_ip ,\n395 : username=>buffer [ \u2019 admin \u2019 ] . a t t r i b u t e s [ \u2019 login \u2019 ] ,\n\n\n\n239\n\n396 : password=>buffer [ \u2019 admin \u2019 ] . a t t r i b u t e s [ \u2019 password \u2019 ] ,\n397 : actions=> instance_actions_for ( VE_STATES[ buffer [ \u2019 state \u2019 ] . text ] ) ,\n398 : storage_volumes =>[],\n399 : launch_time=> \"Unknown\"\n400 } )\n401\n402 end\n403\n404 # hack to provide vm l i s t s with more d e t a i l s ( uses the returned vm_list\n405 # XML to query each vm at a time reusing convert_instance method\n406 def convert_instances (xml , c r e d e n t i a l s )\n407 paci_client = new_client ( c r e d e n t i a l s )\n408 ve_name=REXML: : Document . new(xml . to_s ) . root . a t t r i b u t e s [ \u2019name \u2019 ]\n409 vexml=treat_response ( paci_client . get_instance (ve_name) )\n410 convert_instance ( vexml , c r e d e n t i a l s )\n411 end\n412\n413 ########\n414 # Mapping Load Balancers\n415 #######\n416\n417 def convert_load_balancer (xml , c r e d e n t i a l s )\n418 buffer = REXML: : Document . new(xml . to_s ) . root . elements\n419 addresses =[]\n420 i f buffer [ \u2019 network/ public ?ip \u2019 ]\n421 buffer . each ( \u2019 network/ public ?ip \u2019 ) {| ip | addresses\n422&lt;&lt;InstanceAddress . new( ip . a t t r i b u t e s [ \u2019 address \u2019 ] . s p l i t ( \u2019 / \u2019 ) . f i r s t ,\n423 : type=> : ipv4 )}\n424 end\n425\n426 i f buffer [ \u2019 network/ public ?ipv6 \u2019 ]\n427 buffer . each ( \u2019 network/ public ?ipv6 \u2019 ) {| ip | addresses\n428&lt;&lt;InstanceAddress . new( ip . a t t r i b u t e s [ \u2019 address \u2019 ] . s p l i t ( \u2019 / \u2019 ) . f i r s t ,\n429 : type=> : ipv6 )}\n430 end\n431\n432 balancer=LoadBalancer . new({\n433 : id=> buffer [ \u2019name \u2019 ] . text ,\n434 : created_at=> \"Unknown\" ,\n435 : public_addresses=> addresses ,\n436 : realms=> REALMS,\n437 })\n438\n439 balancer . l i s t e n e r s = [ ]\n440\n441 buffer . each ( \u2019 used?by \u2019 ) do |vm|\n442 balancer . add_listener ({\n443 : protocol => \u2019HTTP\u2019 ,\n444 : load_balancer_port => \u2019Unknown \u2019 ,\n445 : instance_port => \u2019Unknown \u2019\n446 })\n447 end\n448\n449 balancer . instances = [ ]\n450\n451 buffer . each ( \u2019 used?by \u2019 ) do |vm|\n452 balancer . instances&lt;&lt;instance ( credentials , id : vm. a t t r i b u t e s [ \u2019 ve?name \u2019 ] )\n453 end\n454 balancer\n455\n456 end\n457\n458 def convert_load_balancers (xml , c r e d e n t i a l s )\n459 paci_client = new_client ( c r e d e n t i a l s )\n460 lb_name=REXML: : Document . new(xml . to_s ) . root . a t t r i b u t e s [ \u2019name \u2019 ]\n461 lbxml=treat_response ( paci_client . get_load_balancer ( lb_name) )\n462 convert_load_balancer ( lbxml , c r e d e n t i a l s )\n463 end\n464\n465 ########\n\n\n\n240 APPENDIX C. PACI DRIVER\n\n466 # Errors and returned messages process\n467 #######\n468\n469\n470 # function to process returned messages\n471 def treat_response ( res )\n472 s a f e l y do\n473 i f CloudClient . is_error ?( res )\n474 r a i s e case res . code\n475 when \" 401 \" then \" AuthenticationFailure \"\n476 when \" 404 \" then \" ObjectNotFound \"\n477 e l s e res . message\n478 end\n479 end\n480 end\n481 res\n482 end\n483\n484 # error treatment seen by Deltacloud\n485 exceptions do\n486 on / AuthenticationFailure / do\n487 status 401\n488 end\n489\n490 on /ObjectNotFound/ do\n491 status 404\n492 end\n493\n494 on // do\n495 status 502\n496 end\n497 end\n498 end\n499 end\n500 end\n501 end\n\n\n\nBibliography\n\n[1] Amazon. (2013) Amazon Web Services. [Online]. Available: http:\n//aws.amazon.com [cited in p. 1, 16]\n\n[2] Google. (2013) Google App Engine. [Online]. Available: https:\n//developers.google.com/appengine/ [cited in p. 1, 17]\n\n[3] Spotify. About Us. [Online]. Available: https://www.spotify.com/pt/\nabout\\T1\\textendashus/contact/ [cited in p. 1]\n\n[4] Edstrom, D. (2012, nov) The Network Is The Computer. [Online]. Avail-\nable: http://www.imts.com/show/newsletter/insider/article.cfm?aid=547\n[cited in p. 1]\n\n[5] Furht, B. and Escalante, A., Handbook of Cloud Computing. Springer US,\n2010. [cited in p. 7, 8]\n\n[6] Foster, I. and Kesselman, C., The grid: blueprint for a new computing\ninfrastructure. Morgan Kaufmann Publishers, 1999. [cited in p. 7]\n\n[7] Chetty, M. and Buyya, R., \u201cWeaving computational grids: how analogous\nare they with electrical grids?\u201d Computing in Science Engineering, vol. 4,\nno. 4, pp. 61\u201371, 2002. [cited in p. 7]\n\n[8] Foster, I et al., \u201cCloud Computing and Grid Computing 360-Degree Com-\npared,\u201d in Grid Computing Environments Workshop, 2008. GCE \u201908, 2008,\npp. 1\u201310. [cited in p. 8, 9]\n\n[9] Stanoevska-Slabeva, K. et al., Grid and Cloud Computing. Springer Berlin\nHeidelberg, 2010. [cited in p. 8, 31]\n\n[10] Patidar, S. et al., \u201cA Survey Paper on Cloud Computing,\u201d in Advanced\nComputing Communication Technologies (ACCT), 2012 Second Interna-\ntional Conference on, 2012, pp. 394\u2013398. [cited in p. 8]\n\n241\n\nhttp://aws.amazon.com\nhttp://aws.amazon.com\nhttps://developers.google.com/appengine/\nhttps://developers.google.com/appengine/\nhttps://www.spotify.com/pt/about\\T1\\textendash us/contact/\nhttps://www.spotify.com/pt/about\\T1\\textendash us/contact/\nhttp://www.imts.com/show/newsletter/insider/article.cfm?aid=547\n\n\n242 BIBLIOGRAPHY\n\n[11] Vaquero, Luis M. et al., \u201cA break in the clouds: towards a cloud definition,\u201d\nSIGCOMM Comput. Commun. Rev., vol. 39, no. 1, pp. 50\u201355, Dec 2008.\n[cited in p. 8]\n\n[12] Cloud Expo. (2008) Twenty-One Experts Define Cloud Comput-\ning. [Online]. Available: http://cloudcomputing.sys-con.com/node/612375\n[cited in p. 8]\n\n[13] Gartner, Inc. (NYSE: IT). (2013). [Online]. Available: http://www.\ngartner.com/technology/home.jsp [cited in p. 8]\n\n[14] Bank of America Corporation, Merrill Lynch &amp; Co., Inc. (2013). [Online].\nAvailable: http://www.ml.com/index.asp?id=7695_15125 [cited in p. 8]\n\n[15] Gartner. (2008, September) Gartner Says Contrasting Views on Cloud\nComputing Are Creating Confusion, Gartner press release. [Online].\nAvailable: http://www.gartner.com/newsroom/id/766215 [cited in p. 8]\n\n[16] Rangan, K. et al., \u201cThe cloud wars: $100 + billion at stake,\u201d Merrill Lynch,\nTechnical report, 2008. [cited in p. 8]\n\n[17] Armbrust et al., \u201cAbove the Clouds: A Berkeley View of Cloud Comput-\ning,\u201d EECS Department, University of California, Berkeley, Tech. Rep.,\nFeb 2009. [cited in p. 9]\n\n[18] Buyya, R. et al., \u201cMarket-Oriented Cloud Computing: Vision, Hype, and\nReality for Delivering IT Services as Computing Utilities,\u201d in High Per-\nformance Computing and Communications, 2008. HPCC \u201908. 10th IEEE\nInternational Conference on, 2008, pp. 5\u201313. [cited in p. 9]\n\n[19] Mell, P. , Grace, T., \u201cThe NIST Definition of Cloud Computing,\u201d Na-\ntional Institute of Standards and Technology (NIST), Tech. Rep., Sep 2011.\n[cited in p. 9, 10, 11, 17, 18]\n\n[20] Fingar, P. (2010, April) A BPTrends column: Enterprise as\na Service (EaaS)-That\u2019s where BPM Comes In. [Online]. Avail-\nable: http://bptrends.com/publicationfiles/EIGHT%2004-10-COL-EXT%\n20COMPETITION-Enterprise%20as%20Svc-Fingar-final1.pdf [cited in p. 9]\n\n[21] Daconta, M. (2012, March) GCN commentary: Why NIST\u2019s cloud\ndefinition is fatally flawed. [Online]. Available: http://gcn.com/Articles/\n2012/04/02/Reality-Check-NIST-flawed-cloud-framework.aspx [cited in p. 9]\n\n[22] YUNG CHOU. (2012, January) Cloud Expo Article: An Inconvenient\nTruth of the NIST Definition of Cloud Computing. [Online]. Available:\nhttp://cloudcomputing.sys-con.com/node/2131995 [cited in p. 9]\n\nhttp://cloudcomputing.sys-con.com/node/612375\nhttp://www.gartner.com/technology/home.jsp\nhttp://www.gartner.com/technology/home.jsp\nhttp://www.ml.com/index.asp?id=7695_15125\nhttp://www.gartner.com/newsroom/id/766215\nhttp://bptrends.com/publicationfiles/EIGHT%2004-10-COL-EXT%20COMPETITION-Enterprise%20as%20Svc-Fingar-final1.pdf\nhttp://bptrends.com/publicationfiles/EIGHT%2004-10-COL-EXT%20COMPETITION-Enterprise%20as%20Svc-Fingar-final1.pdf\nhttp://gcn.com/Articles/2012/04/02/Reality-Check-NIST-flawed-cloud-framework.aspx\nhttp://gcn.com/Articles/2012/04/02/Reality-Check-NIST-flawed-cloud-framework.aspx\nhttp://cloudcomputing.sys-con.com/node/2131995\n\n\nBIBLIOGRAPHY 243\n\n[23] NIST, Information Technology Laboratory. (2011, October) Final Version\nof NIST Cloud Computing Definition Published. [Online]. Available:\nhttp://www.nist.gov/itl/csd/cloud-102511.cfm [cited in p. 9]\n\n[24] Liu, F. et al., \u201cNIST Cloud Computing Reference Architecture,\u201d National\nInstitute of Standards and Technology (NIST), Tech. Rep., Sep 2011.\n[cited in p. 10, 11, 12, 13, 14, 15, 16]\n\n[25] Buyya, R. et al., Cloud Computing: Principles and Paradigms. Wiley,\n2011. [cited in p. 16, 36, 37]\n\n[26] LunaCloud. (2013) Lunacloud Server. [Online]. Available: http://www.\nlunacloud.com/en/cloud-server [cited in p. 16]\n\n[27] Google. (2013) Google Apps for Business. [Online]. Available: http:\n//www.google.com/enterprise/apps/business/ [cited in p. 17]\n\n[28] Salesforce. (2013) Sales-cloud. [Online]. Available: http://www.salesforce.\ncom/sales-cloud/overview/ [cited in p. 17]\n\n[29] Velte, T. et al., Cloud Computing, A Practical Approach. Mcgraw-hill,\n2009. [cited in p. 17]\n\n[30] Gong, C. et al., \u201cThe Characteristics of Cloud Computing,\u201d in Parallel\nProcessing Workshops (ICPPW), 2010 39th International Conference on,\n2010, pp. 275\u2013279. [cited in p. 17]\n\n[31] Jadeja, Y. and Modi, K., \u201cCloud computing - concepts, architecture and\nchallenges,\u201d in Computing, Electronics and Electrical Technologies (IC-\nCEET), 2012 International Conference on, 2012, pp. 877\u2013880. [cited in p. 17]\n\n[32] Hashizume, K. et al., \u201cAn analysis of security issues for cloud computing,\u201d\nJournal of Internet Services and Applications, vol. 4, no. 1, pp. 1\u201313, 2013.\n[Online]. Available: http://dx.doi.org/10.1186/1869-0238-4-5 [cited in p. 20,\n21]\n\n[33] Li, Wenjuan and Ping, Lingdi, \u201cTrust Model to Enhance Security and\nInteroperability of Cloud Environment,\u201d in Cloud Computing, ser. Lecture\nNotes in Computer Science, Jaatun, MartinGilje and Zhao, Gansen and\nRong, Chunming, Ed. Springer Berlin Heidelberg, 2009, vol. 5931, pp. 69\u2013\n79. [Online]. Available: http://dx.doi.org/10.1007/978-3-642-10665-1_7\n[cited in p. 20]\n\n[34] Madrid International Conference. (2009, November) 31st International\nConference of Data Protection and Privacy. [Online]. Available: http://\nwww.privacyconference2009.org/home/index-iden-idweb.html [cited in p. 21]\n\nhttp://www.nist.gov/itl/csd/cloud-102511.cfm\nhttp://www.lunacloud.com/en/cloud-server\nhttp://www.lunacloud.com/en/cloud-server\nhttp://www.google.com/enterprise/apps/business/\nhttp://www.google.com/enterprise/apps/business/\nhttp://www.salesforce.com/sales-cloud/overview/\nhttp://www.salesforce.com/sales-cloud/overview/\nhttp://dx.doi.org/10.1186/1869-0238-4-5\nhttp://dx.doi.org/10.1007/978-3-642-10665-1_7\nhttp://www.privacyconference2009.org/home/index-iden-idweb.html\nhttp://www.privacyconference2009.org/home/index-iden-idweb.html\n\n\n244 BIBLIOGRAPHY\n\n[35] IBM Systems Magazine. (2012, December) 40 Years of\nVM Community\u2014and Going Strong! [Online]. Avail-\nable: http://www.ibmsystemsmag.com/mainframe/trends/z-vm/vm_\ncommunity/ [cited in p. 22]\n\n[36] Oracle. (2011) VM User\u2019s Guide: Brief History of Virtualization. [Online].\nAvailable: http://docs.oracle.com/cd/E20065_01/doc.30/e18549/intro.\nhtm [cited in p. 22]\n\n[37] Antonopoulos, N. and Gillam, L., Cloud Computing: Principles, Systems\nand Applications. Springer London, 2010. [cited in p. 22, 24, 25, 35]\n\n[38] Popek, Gerald J. and Goldberg, Robert P., \u201cFormal requirements for vir-\ntualizable third generation architectures,\u201d Commun. ACM, vol. 17, no. 7,\npp. 412\u2013421, jul 1974. [cited in p. 22]\n\n[39] IBM. (2005, December) IBM Systems Software Informa-\ntion Center: Virtual systems overview. [Online]. Avail-\nable: http://publib.boulder.ibm.com/infocenter/eserver/v1r2/index.jsp?\ntopic=/eicay/eicayvservers.htm [cited in p. 23]\n\n[40] Creasy, R. J., \u201cThe Origin of the VM/370 Time-Sharing System,\u201d IBM\nJournal of Research and Development, vol. 25, no. 5, pp. 483\u2013490, 1981.\n[cited in p. 23]\n\n[41] Intel. Intel Virtualization Technology List: About Intel Virtualiz-\nation Technology. [Online]. Available: http://ark.intel.com/products/\nvirtualizationtechnology [cited in p. 24]\n\n[42] AMD. AMD Virtualization. [Online]. Available: http://www.amd.com/us/\nsolutions/servers/virtualization/Pages/virtualization.aspx#2 [cited in p. 24]\n\n[43] VMware. virtualization software company. [Online]. Available: http:\n//www.vmware.com/#eu [cited in p. 24, 40, 42, 47, 51, 58]\n\n[44] KVM. virtualization solution. [Online]. Available: http://www.linux-kvm.\norg/page/Main_Page [cited in p. 24, 40, 47, 51, 58]\n\n[45] Xen Server. Open Source Virtualization. [Online]. Available: http:\n//www.xenserver.org/ [cited in p. 25, 40, 47, 58]\n\n[46] OpenVZ. container-based virtualization for Linux. [Online]. Available:\nhttp://openvz.org/Main_Page [cited in p. 26]\n\n[47] Parallels. Parallels Virtuozzo Containers. [Online]. Available: http:\n//www.parallels.com/products/pvc/ [cited in p. 26]\n\nhttp://www.ibmsystemsmag.com/mainframe/trends/z-vm/vm_community/\nhttp://www.ibmsystemsmag.com/mainframe/trends/z-vm/vm_community/\nhttp://docs.oracle.com/cd/E20065_01/doc.30/e18549/intro.htm\nhttp://docs.oracle.com/cd/E20065_01/doc.30/e18549/intro.htm\nhttp://publib.boulder.ibm.com/infocenter/eserver/v1r2/index.jsp?topic=/eicay/eicayvservers.htm\nhttp://publib.boulder.ibm.com/infocenter/eserver/v1r2/index.jsp?topic=/eicay/eicayvservers.htm\nhttp://ark.intel.com/products/virtualizationtechnology\nhttp://ark.intel.com/products/virtualizationtechnology\nhttp://www.amd.com/us/solutions/servers/virtualization/Pages/virtualization.aspx#2\nhttp://www.amd.com/us/solutions/servers/virtualization/Pages/virtualization.aspx#2\nhttp://www.vmware.com/#eu\nhttp://www.vmware.com/#eu\nhttp://www.linux-kvm.org/page/Main_Page\nhttp://www.linux-kvm.org/page/Main_Page\nhttp://www.xenserver.org/\nhttp://www.xenserver.org/\nhttp://openvz.org/Main_Page\nhttp://www.parallels.com/products/pvc/\nhttp://www.parallels.com/products/pvc/\n\n\nBIBLIOGRAPHY 245\n\n[48] Marston, Sean et al., \u201cCloud computing - The business perspective,\u201d Decis.\nSupport Syst., vol. 51, no. 1, pp. 176\u2013189, apr 2011. [cited in p. 26, 28]\n\n[49] \u201cCloud computing \u2014business models, value creation dynamics and advant-\nages for customers,\u201d White Paper, Siemens, 2010. [cited in p. 27]\n\n[50] Microsoft. [Online]. Available: http://www.microsoft.com/ [cited in p. 28]\n\n[51] Google. About Google. [Online]. Available: https://www.google.pt/intl/\nen\\T1\\textendashUS/about/ [cited in p. 28]\n\n[52] Facebook. [Online]. Available: https://www.facebook.com/facebook\n[cited in p. 28]\n\n[53] RightScale. [Online]. Available: http://www.rightscale.com/ [cited in p. 29]\n\n[54] Vordel. [Online]. Available: http://www.vordel.com/ [cited in p. 29]\n\n[55] Wieder, Philipp et al., Service Level Agreements for Cloud Computing.\nSpringer, 2011. [cited in p. 30, 31]\n\n[56] Gangadharan, G.R. and Parrilli, DavideMaria, \u201cService Level Agreements\nin Cloud Computing: Perspectives of Private Consumers and Small-to-\nMedium Enterprises,\u201d in Cloud Computing for Enterprise Architectures.\nSpringer London, 2011, pp. 207\u2013225. [cited in p. 31]\n\n[57] ETSI, \u201cSLAs for Cloud services,\u201d European Telecommunications\nStandards Institute (ETSI), Tech. Rep., Nov 2012. [Online]. Avail-\nable: http://www.etsi.org/deliver/etsi_tr/103100_103199/103125/01.01.\n01_60/tr_103125v010101p.pdf [cited in p. 31]\n\n[58] Andrieux, A. et al., \u201cWeb Services Agreement Specification (WS-\nAgreement),\u201d Open Grid Forum (OGF), Tech. Rep., Oct 2006. [Online].\nAvailable: http://redmin.ogf.org/Public_Comment_Docs/Documents/\nOct-2006/WS-AgreementSpecificationDraftFinal_sp_tn_jpver_v2.pdf\n[cited in p. 31, 32]\n\n[59] Ludwig, H. et al., \u201cWeb service level agreement (WSLA) language specific-\nation,\u201d IBM Corporation, pp. 815\u2013824, 2003. [cited in p. 31]\n\n[60] Amazon. Amazon Web Services: Amazon EC2. [Online]. Available:\nhttp://aws.amazon.com/ec2/ [cited in p. 36, 147]\n\n[61] Sotomayor, B. and Montero, Ruben S. and Llorente, I.M. and Foster, I.,\n\u201cVirtual Infrastructure Management in Private and Hybrid Clouds,\u201d Inter-\nnet Computing, IEEE, vol. 13, no. 5, pp. 14\u201322, Sept 2009. [cited in p. 36]\n\nhttp://www.microsoft.com/\nhttps://www.google.pt/intl/en\\T1\\textendash US/about/\nhttps://www.google.pt/intl/en\\T1\\textendash US/about/\nhttps://www.facebook.com/facebook\nhttp://www.rightscale.com/\nhttp://www.vordel.com/\nhttp://www.etsi.org/deliver/etsi_tr/103100_103199/103125/01.01.01_60/tr_103125v010101p.pdf\nhttp://www.etsi.org/deliver/etsi_tr/103100_103199/103125/01.01.01_60/tr_103125v010101p.pdf\nhttp://redmin.ogf.org/Public_Comment_Docs/Documents/Oct-2006/WS-AgreementSpecificationDraftFinal_sp_tn_jpver_v2.pdf\nhttp://redmin.ogf.org/Public_Comment_Docs/Documents/Oct-2006/WS-AgreementSpecificationDraftFinal_sp_tn_jpver_v2.pdf\nhttp://aws.amazon.com/ec2/\n\n\n246 BIBLIOGRAPHY\n\n[62] C12G Labs. OPEN\u2014SOURCE ENTERPRISE CLOUD SIMPLIFIED.\n[Online]. Available: http://opennebula.org [cited in p. 36]\n\n[63] OpenStack Foundation. Open source software for building private and\npublic clouds. [Online]. Available: http://www.openstack.org [cited in p. 36,\n42]\n\n[64] Apache Software Foundation. Apache CloudStack: Open Source Cloud\nComputing. [Online]. Available: http://cloudstack.apache.org/index.html\n[cited in p. 36]\n\n[65] Eucalyptus Systems Inc. Eucalyptus: Open Source Private Cloud Software.\n[Online]. Available: https://www.eucalyptus.com/eucalyptus-cloud/iaas\n[cited in p. 36, 54]\n\n[66] Parallels. Parallels Automation for Cloud Infrastructure. [Online].\nAvailable: http://www.parallels.com/products/paci/ [cited in p. 36]\n\n[67] Distributed Systems Architecture (DSA). Research and Innovation in\nCloud Computing. [Online]. Available: http://dsa-research.org/doku.php\n[cited in p. 36]\n\n[68] Toraldo, G., OpenNebula 3 Cloud Computing, ser. Community experience\ndistilled. Packt Publishing\u201e 2012. [cited in p. 37, 40]\n\n[69] C12G Labs. [Online]. Available: http://c12g.com/ [cited in p. 37]\n\n[70] SQLite. About SQLite. [Online]. Available: http://www.sqlite.org/about.\nhtml [cited in p. 37, 40, 44, 47]\n\n[71] MySQL. The world\u2019s most popular open source database. [Online].\nAvailable: http://www.mysql.com/ [cited in p. 37, 40, 44, 47, 52]\n\n[72] OpenNebula Community wiki. OpenNebula Ecosystem. [Online]. Available:\nhttp://community.opennebula.org/ecosystem [cited in p. 38]\n\n[73] OpenNebula Archive. Scalable Architecture and APIs 4.0. [Online].\nAvailable: http://archives.opennebula.org/documentation:archives:rel4.0:\nintroapis [cited in p. 39]\n\n[74] Amazon Elastic Compute Cloud. API Reference. [Online].\nAvailable: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/\nWelcome.html [cited in p. 39]\n\n[75] Amazon Web Services. Amazon EC2 API Tools. [Online]. Available:\nhttp://aws.amazon.com/developertools/Amazon-EC2/351 [cited in p. 39]\n\nhttp://opennebula.org\nhttp://www.openstack.org\nhttp://cloudstack.apache.org/index.html\nhttps://www.eucalyptus.com/eucalyptus-cloud/iaas\nhttp://www.parallels.com/products/paci/\nhttp://dsa-research.org/doku.php\nhttp://c12g.com/\nhttp://www.sqlite.org/about.html\nhttp://www.sqlite.org/about.html\nhttp://www.mysql.com/\nhttp://community.opennebula.org/ecosystem\nhttp://archives.opennebula.org/documentation:archives:rel4.0:introapis\nhttp://archives.opennebula.org/documentation:archives:rel4.0:introapis\nhttp://docs.aws.amazon.com/AWSEC2/latest/APIReference/Welcome.html\nhttp://docs.aws.amazon.com/AWSEC2/latest/APIReference/Welcome.html\nhttp://aws.amazon.com/developertools/Amazon-EC2/351\n\n\nBIBLIOGRAPHY 247\n\n[76] Open Grid Forum (OGF). Open Forum - Open Standards. [Online].\nAvailable: https://www.ogf.org/dokuwiki/doku.php [cited in p. 39]\n\n[77] OGF. OCCI: About. [Online]. Available: http://occi-wg.org/about/\n[cited in p. 39]\n\n[78] OpenNebula Archive. OpenNebula OCCI User Guide 4.0. [Online].\nAvailable: http://archives.opennebula.org/documentation:archives:rel4.0:\nocciug [cited in p. 39]\n\n[79] IEEE, \u201cIEEE Standard for Local and metropolitan area networks\u2013\u2013Media\nAccess Control (MAC) Bridges and Virtual Bridged Local Area Networks,\u201d\nIEEE Std 802.1Q-2011 (Revision of IEEE Std 802.1Q-2005), pp. 1\u20131365,\nAug 2011. [cited in p. 41]\n\n[80] Open vSwitch. An Open Virtual Switch. [Online]. Available: http:\n//openvswitch.org/ [cited in p. 41, 44]\n\n[81] Housley, R. et al., \u201cInternet X.509 Public Key Infrastructure Certificate\nand CRL Profile,\u201d January 1999. [Online]. Available: http://www.ietf.org/\nrfc/rfc2459.txt [cited in p. 42]\n\n[82] THE RACKSPACE BLOG &amp; NEWSROOM. Opening The Rack-\nspace Cloud. [Online]. Available: http://www.rackspace.com/blog/\nopening-the-rackspace-cloud/ [cited in p. 42]\n\n[83] NASA. NASA Nebula Cloud Computing Platform. [Online]. Available:\nhttp://www.nasa.gov/open/plan/nebula.html [cited in p. 42]\n\n[84] OpenStack. The OpenStack Open Source Cloud Mission. [Online].\nAvailable: https://wiki.openstack.org/wiki/Main_Page [cited in p. 42]\n\n[85] OpenStack Compute Administration Guide. Grizzly, Concep-\ntual Architecture. [Online]. Available: http://docs.openstack.org/\ngrizzly/openstack-compute/admin/content/conceptual-architecture.html\n[cited in p. 43]\n\n[86] AWS. Amazon S3. [Online]. Available: https://aws.amazon.com/s3/\n[cited in p. 44, 55, 145, 147]\n\n[87] Ceph. Block Storage. [Online]. Available: http://ceph.com/ceph-storage/\nblock-storage/ [cited in p. 44]\n\n[88] Mongo DB. GridFS. [Online]. Available: http://docs.mongodb.org/\nmanual/core/gridfs/ [cited in p. 44]\n\nhttps://www.ogf.org/dokuwiki/doku.php\nhttp://occi-wg.org/about/\nhttp://archives.opennebula.org/documentation:archives:rel4.0:occiug\nhttp://archives.opennebula.org/documentation:archives:rel4.0:occiug\nhttp://openvswitch.org/\nhttp://openvswitch.org/\nhttp://www.ietf.org/rfc/rfc2459.txt\nhttp://www.ietf.org/rfc/rfc2459.txt\nhttp://www.rackspace.com/blog/opening-the-rackspace-cloud/\nhttp://www.rackspace.com/blog/opening-the-rackspace-cloud/\nhttp://www.nasa.gov/open/plan/nebula.html\nhttps://wiki.openstack.org/wiki/Main_Page\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/conceptual-architecture.html\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/conceptual-architecture.html\nhttps://aws.amazon.com/s3/\nhttp://ceph.com/ceph-storage/block-storage/\nhttp://ceph.com/ceph-storage/block-storage/\nhttp://docs.mongodb.org/manual/core/gridfs/\nhttp://docs.mongodb.org/manual/core/gridfs/\n\n\n248 BIBLIOGRAPHY\n\n[89] Mongo DB. Agile and Scalable. [Online]. Available: http://www.mongodb.\norg/ [cited in p. 44]\n\n[90] Open Networking Foundation. OpenFlow. [Online]. Avail-\nable: https://www.opennetworking.org/sdn-resources/onf-specifications/\nopenflow [cited in p. 44]\n\n[91] Cisco. [Online]. Available: http://www.cisco.com/ [cited in p. 44]\n\n[92] AWS. Amazon EBS. [Online]. Available: http://aws.amazon.com/ebs/\n[cited in p. 45, 56]\n\n[93] OpenStack Compute Administration Guide. Grizzly, Logical Ar-\nchitecture. [Online]. Available: http://docs.openstack.org/grizzly/\nopenstack-compute/admin/content/logical-architecture.html [cited in p. 46]\n\n[94] OpenStack. OCCI. [Online]. Available: https://wiki.openstack.org/wiki/\nOcci#Summary [cited in p. 46]\n\n[95] OpenStack. Configuring Object Storage with the S3 API. [Online].\nAvailable: http://docs.openstack.org/grizzly/openstack-object-storage/\nadmin/content/configuring-openstack-object-storage-with-s3_api.html\n[cited in p. 47]\n\n[96] RabbitMQ. Messaging that just works. [Online]. Available: http:\n//www.rabbitmq.com/ [cited in p. 47]\n\n[97] Apache. Apache Qpid. [Online]. Available: http://qpid.apache.org/\n[cited in p. 47]\n\n[98] OpenStack. Block Storage Service Administration Guide. [Online]. Avail-\nable: http://docs.openstack.org/grizzly/openstack-block-storage/admin/\ncontent/index.html [cited in p. 47]\n\n[99] Microsoft. Hyper\u2014V. [Online]. Available: http://technet.microsoft.com/\nen-us/windowsserver/dd448604.aspx [cited in p. 47]\n\n[100] OpenStack Compute Administration Guide. Grizzly, Selecting\na Hypervisor. [Online]. Available: http://docs.openstack.org/\ngrizzly/openstack-compute/admin/content/selecting-a-hypervisor.html\n[cited in p. 47]\n\n[101] SQLAlchemy. The Python SQL Toolkit and Object Relational Mapper.\n[Online]. Available: http://www.sqlalchemy.org/ [cited in p. 47]\n\n[102] OpenStack Manuals. Architecture: Physical network diagram. [Online].\nAvailable: http://docs.openstack.org/grizzly/basic-install/yum/content/\nbasic-install_architecture.html [cited in p. 48]\n\nhttp://www.mongodb.org/\nhttp://www.mongodb.org/\nhttps://www.opennetworking.org/sdn-resources/onf-specifications/openflow\nhttps://www.opennetworking.org/sdn-resources/onf-specifications/openflow\nhttp://www.cisco.com/\nhttp://aws.amazon.com/ebs/\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/logical-architecture.html\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/logical-architecture.html\nhttps://wiki.openstack.org/wiki/Occi#Summary\nhttps://wiki.openstack.org/wiki/Occi#Summary\nhttp://docs.openstack.org/grizzly/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html\nhttp://docs.openstack.org/grizzly/openstack-object-storage/admin/content/configuring-openstack-object-storage-with-s3_api.html\nhttp://www.rabbitmq.com/\nhttp://www.rabbitmq.com/\nhttp://qpid.apache.org/\nhttp://docs.openstack.org/grizzly/openstack-block-storage/admin/content/index.html\nhttp://docs.openstack.org/grizzly/openstack-block-storage/admin/content/index.html\nhttp://technet.microsoft.com/en-us/windowsserver/dd448604.aspx\nhttp://technet.microsoft.com/en-us/windowsserver/dd448604.aspx\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/selecting-a-hypervisor.html\nhttp://docs.openstack.org/grizzly/openstack-compute/admin/content/selecting-a-hypervisor.html\nhttp://www.sqlalchemy.org/\nhttp://docs.openstack.org/grizzly/basic-install/yum/content/basic-install_architecture.html\nhttp://docs.openstack.org/grizzly/basic-install/yum/content/basic-install_architecture.html\n\n\nBIBLIOGRAPHY 249\n\n[103] Apache. The Apache Software Foundation. [Online]. Available: http:\n//www.apache.org/foundation/ [cited in p. 49]\n\n[104] The Channel. (2010, May) Cloud.com takes on virty infrastructure.\n[Online]. Available: http://www.channelregister.co.uk/2010/05/04/cloud_\ncom_launch/ [cited in p. 49]\n\n[105] Citrix. Citrix Systems, Inc. [Online]. Available: http://www.citrix.com/\n[cited in p. 49]\n\n[106] Apache. The Apache Software Foundation Incubator. [Online]. Available:\nhttp://incubator.apache.org/ [cited in p. 49]\n\n[107] Sabharwal, N., Apache CloudStack Cloud Computing, ser. Community ex-\nperience distilled. Packt Publishing\u201e 2013. [cited in p. 50, 52]\n\n[108] Citrix. XenServer. [Online]. Available: http://www.citrix.com/products/\nxenserver/overview.html [cited in p. 51]\n\n[109] Citrix. CloudPlatform. [Online]. Available: http://www.citrix.com/\nproducts/cloudplatform/overview.html [cited in p. 51]\n\n[110] Soheil Eizadi. (2013, August) CloudStack, Development 101. [Online].\nAvailable: https://cwiki.apache.org/confluence/display/CLOUDSTACK/\nDevelopment+101 [cited in p. 51]\n\n[111] Citrix. NetScaler Application Delivery Controller. [Online]. Available: http:\n//www.citrix.com/products/netscaler-application-delivery-controller/\noverview.html [cited in p. 51]\n\n[112] Apache. Apache Tomcat. [Online]. Available: http://tomcat.apache.org/\n[cited in p. 52]\n\n[113] Joe Brockmeier. (2013, March) Deploying Apache CloudStack from\nAPI to UI. [Online]. Available: http://www.slideshare.net/jzb/\ncloud-stack-from-api-to-ui [cited in p. 53]\n\n[114] Eucalyptus. The Eucalyptus Story. [Online]. Available: https://www.\neucalyptus.com/about/story [cited in p. 54]\n\n[115] Eucalyptus. Eucalyptus Cloud Computing Architecture. [Online]. Avail-\nable: https://www.eucalyptus.com/eucalyptus-cloud/iaas/architecture\n[cited in p. 55]\n\n[116] Eucalyptus. Eucalyptus Components. [Online]. Avail-\nable: https://www.eucalyptus.com/docs/eucalyptus/3.2/ig/euca_\ncomponents.html#euca_components [cited in p. 56, 58]\n\nhttp://www.apache.org/foundation/\nhttp://www.apache.org/foundation/\nhttp://www.channelregister.co.uk/2010/05/04/cloud_com_launch/\nhttp://www.channelregister.co.uk/2010/05/04/cloud_com_launch/\nhttp://www.citrix.com/\nhttp://incubator.apache.org/\nhttp://www.citrix.com/products/xenserver/overview.html\nhttp://www.citrix.com/products/xenserver/overview.html\nhttp://www.citrix.com/products/cloudplatform/overview.html\nhttp://www.citrix.com/products/cloudplatform/overview.html\nhttps://cwiki.apache.org/confluence/display/CLOUDSTACK/Development+101\nhttps://cwiki.apache.org/confluence/display/CLOUDSTACK/Development+101\nhttp://www.citrix.com/products/netscaler-application-delivery-controller/overview.html\nhttp://www.citrix.com/products/netscaler-application-delivery-controller/overview.html\nhttp://www.citrix.com/products/netscaler-application-delivery-controller/overview.html\nhttp://tomcat.apache.org/\nhttp://www.slideshare.net/jzb/cloud-stack-from-api-to-ui\nhttp://www.slideshare.net/jzb/cloud-stack-from-api-to-ui\nhttps://www.eucalyptus.com/about/story\nhttps://www.eucalyptus.com/about/story\nhttps://www.eucalyptus.com/eucalyptus-cloud/iaas/architecture\nhttps://www.eucalyptus.com/docs/eucalyptus/3.2/ig/euca_components.html#euca_components\nhttps://www.eucalyptus.com/docs/eucalyptus/3.2/ig/euca_components.html#euca_components\n\n\n250 BIBLIOGRAPHY\n\n[117] Eucalyptus. Overview of Euca2ools. [Online]. Avail-\nable: https://www.eucalyptus.com/docs/euca2ools/3.0/euca2ools-guide/\neuca2ools_oview.html#euca2ools] [cited in p. 56]\n\n[118] DeKoenigsberg, Greg, \u201cEucalyptus Architecture,\u201d 2012. [Online].\nAvailable: https://github.com/eucalyptus/eucalyptus/wiki/documents/\neucalyptus-detailed-architecture-v1.6.2.pdf [cited in p. 57, 59]\n\n[119] \u201cParallels Automation for Cloud Infrastructure (PACI),\u201d Data Sheet, Par-\nallels, November 2011. [cited in p. 60, 62]\n\n[120] \u201cParallels Automation for Communication Service Providers,\u201d\nData Sheet, Parallels, January 2013. [Online]. Avail-\nable: http://www.parallels.com/fileadmin/media/hcap/pa/documents/\nPA_Datasheet_for_Carriers_Ltr_EN_01092013.pdf [cited in p. 61]\n\n[121] Parallels. Supported Third-Party Products in Parallels Operations Auto-\nmation. [Online]. Available: http://kb.parallels.com/en/8769 [cited in p. 62]\n\n[122] \u201cParallels Automation 5.4: Hardware Requirements,\u201d Data Sheet, Parallels,\nFebruary 2013. [cited in p. 63]\n\n[123] Roy Thomas Fielding, \u201cArchitectural Styles and the Design of Network-\nbased Software Architectures,\u201d Ph.D. dissertation, UNIVERSITY OF\nCALIFORNIA, IRVINE, 2000. [cited in p. 67]\n\n[124] Berners-Lee, T. et al., \u201cUniform Resource Identifier (URI): Generic\nSyntax,\u201d January 2005. [Online]. Available: http://www.ietf.org/rfc/\nrfc3986.txt [cited in p. 68]\n\n[125] Masse, M., REST API Design Rulebook. O\u2019Reilly Media, 2011. [cited in p. 68]\n\n[126] Fielding, R. et al., \u201cHypertext Transfer Protocol \u2013 HTTP/1.1,\u201d June\n1999. [Online]. Available: http://www.ietf.org/rfc/rfc2616.txt [cited in p. 68,\n145, 206]\n\n[127] Laurent, S.S. and Johnston, J. and Dumbill, E., Programming Web Services\nwith XML-RPC. O\u2019Reilly Media, Incorporated, 2001. [cited in p. 69]\n\n[128] OpenNebula. XML-RPC API 4.2. [Online]. Available: http://opennebula.\norg/documentation:rel4.2:api [cited in p. 70]\n\n[129] OpenNebula. Virtual Machine Definition File 4.2. [Online]. Available:\nhttp://opennebula.org/documentation:rel4.2:template [cited in p. 71, 72]\n\n[130] OpenNebula. Image Definition Template 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:img_template [cited in p. 78]\n\nhttps://www.eucalyptus.com/docs/euca2ools/3.0/euca2ools-guide/euca2ools_oview.html#euca2ools]\nhttps://www.eucalyptus.com/docs/euca2ools/3.0/euca2ools-guide/euca2ools_oview.html#euca2ools]\nhttps://github.com/eucalyptus/eucalyptus/wiki/documents/eucalyptus-detailed-architecture-v1.6.2.pdf\nhttps://github.com/eucalyptus/eucalyptus/wiki/documents/eucalyptus-detailed-architecture-v1.6.2.pdf\nhttp://www.parallels.com/fileadmin/media/hcap/pa/documents/PA_Datasheet_for_Carriers_Ltr_EN_01092013.pdf\nhttp://www.parallels.com/fileadmin/media/hcap/pa/documents/PA_Datasheet_for_Carriers_Ltr_EN_01092013.pdf\nhttp://kb.parallels.com/en/8769\nhttp://www.ietf.org/rfc/rfc3986.txt\nhttp://www.ietf.org/rfc/rfc3986.txt\nhttp://www.ietf.org/rfc/rfc2616.txt\nhttp://opennebula.org/documentation:rel4.2:api\nhttp://opennebula.org/documentation:rel4.2:api\nhttp://opennebula.org/documentation:rel4.2:template\nhttp://opennebula.org/documentation:rel4.2:img_template\nhttp://opennebula.org/documentation:rel4.2:img_template\n\n\nBIBLIOGRAPHY 251\n\n[131] OpenNebula. Virtual Network Definition File 4.2. [Online]. Available:\nhttp://opennebula.org/documentation:rel4.2:vnet_template [cited in p. 81]\n\n[132] OpenNebula. The LVM Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:lvm_ds [cited in p. 84]\n\n[133] OpenNebula. The VMFS Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:vmware_ds [cited in p. 84]\n\n[134] OpenNebula. The Ceph Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:ceph_ds [cited in p. 84]\n\n[135] OpenNebula. The System Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:system_ds [cited in p. 84]\n\n[136] OpenNebula. The Filesystem Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:fs_ds [cited in p. 84]\n\n[137] OpenNebula. The iSCSI Datastore 4.2. [Online]. Available: http:\n//opennebula.org/documentation:rel4.2:iscsi_ds [cited in p. 84]\n\n[138] OpenNebula. The Kernels &amp; Files Datastore 4.2. [Online]. Available:\nhttp://opennebula.org/documentation:rel4.2:file_ds [cited in p. 84]\n\n[139] OpenStack. OpenStack API Complete Reference. [Online]. Available:\nhttp://api.openstack.org/api-ref.html [cited in p. 86]\n\n[140] Apache CloudStack. CloudStack API Developer\u2019s Guide. [Online]. Avail-\nable: http://cloudstack.apache.org/docs/en-US/Apache_CloudStack/4.0.\n2/html/API_Developers_Guide/index.html [cited in p. 102]\n\n[141] Apache CloudStack. Apache CloudStack API Documentation (v4.1.0).\n[Online]. Available: http://cloudstack.apache.org/docs/api/apidocs-4.1/\nTOC_User.html [cited in p. 102]\n\n[142] \u201cParallels Operations Automation 5.4: PACI RESTful API Pro-\ngrammer\u2019s Guide,\u201d Data Sheet, Parallels, January 2013. [Online].\nAvailable: http://download.pa.parallels.com/poa/5.4/doc/pdf/POA%\n20RESTful%20API%20Guide/paci-restful-api-guide-5.4.pdf [cited in p. 126,\n171]\n\n[143] Apache. Apache CXF: An Open-Source Services Framework. [Online].\nAvailable: http://cxf.apache.org/ [cited in p. 141]\n\n[144] Apache. Apache Wink. [Online]. Available: http://wink.apache.org/\n[cited in p. 141]\n\nhttp://opennebula.org/documentation:rel4.2:vnet_template\nhttp://opennebula.org/documentation:rel4.2:lvm_ds\nhttp://opennebula.org/documentation:rel4.2:lvm_ds\nhttp://opennebula.org/documentation:rel4.2:vmware_ds\nhttp://opennebula.org/documentation:rel4.2:vmware_ds\nhttp://opennebula.org/documentation:rel4.2:ceph_ds\nhttp://opennebula.org/documentation:rel4.2:ceph_ds\nhttp://opennebula.org/documentation:rel4.2:system_ds\nhttp://opennebula.org/documentation:rel4.2:system_ds\nhttp://opennebula.org/documentation:rel4.2:fs_ds\nhttp://opennebula.org/documentation:rel4.2:fs_ds\nhttp://opennebula.org/documentation:rel4.2:iscsi_ds\nhttp://opennebula.org/documentation:rel4.2:iscsi_ds\nhttp://opennebula.org/documentation:rel4.2:file_ds\nhttp://api.openstack.org/api-ref.html\nhttp://cloudstack.apache.org/docs/en-US/Apache_CloudStack/4.0.2/html/API_Developers_Guide/index.html\nhttp://cloudstack.apache.org/docs/en-US/Apache_CloudStack/4.0.2/html/API_Developers_Guide/index.html\nhttp://cloudstack.apache.org/docs/api/apidocs-4.1/TOC_User.html\nhttp://cloudstack.apache.org/docs/api/apidocs-4.1/TOC_User.html\nhttp://download.pa.parallels.com/poa/5.4/doc/pdf/POA%20RESTful%20API%20Guide/paci-restful-api-guide-5.4.pdf\nhttp://download.pa.parallels.com/poa/5.4/doc/pdf/POA%20RESTful%20API%20Guide/paci-restful-api-guide-5.4.pdf\nhttp://cxf.apache.org/\nhttp://wink.apache.org/\n\n\n252 BIBLIOGRAPHY\n\n[145] Apache. Welcome to Apache Axis2/Java. [Online]. Available: http:\n//axis.apache.org/axis2/java/core/ [cited in p. 141]\n\n[146] Deltacloud. Deltacloud Framework. [Online]. Available: http://deltacloud.\napache.org/ [cited in p. 143]\n\n[147] jClouds. The Java Multi-Cloud Toolkit. [Online]. Available: http:\n//jclouds.apache.org/ [cited in p. 143]\n\n[148] Apache Libcloud. One Interface To Rule Them All. [Online]. Available:\nhttp://libcloud.apache.org/ [cited in p. 143]\n\n[149] DMTF. Cloud Management Initiative. [Online]. Available: http:\n//dmtf.org/standards/cloud [cited in p. 144]\n\n[150] Deltacloud. About Deltacloud. [Online]. Available: http://deltacloud.\napache.org/about.html [cited in p. 144]\n\n[151] Deltacloud. Deltacloud drivers. [Online]. Available: http://deltacloud.\napache.org/drivers.html#drivers [cited in p. 144, 145]\n\n[152] Rackspace. Rackspace Cloud Files. [Online]. Available: http://www.\nrackspace.com/cloud/files/ [cited in p. 145, 147]\n\n[153] Fog. The Ruby cloud services library. [Online]. Available: http://fog.io/\n[cited in p. 145]\n\n[154] jClouds. Providers. [Online]. Available: http://jclouds.apache.org/\nreference/providers/ [cited in p. 146]\n\n[155] Apache. Apache Ant. [Online]. Available: http://ant.apache.org/\n[cited in p. 146]\n\n[156] Apache. Apache Maven Project. [Online]. Available: http://maven.apache.\norg/ [cited in p. 146]\n\n[157] jClouds. Apache jclouds 1.6.2\u2013incubating API. [Online].\nAvailable: http://demobox.github.io/jclouds-maven-site-1.6.2/1.6.\n2-incubating/jclouds/apidocs/ [cited in p. 147]\n\n[158] Dasein. DASEIN\u2013CLOUD\u2013CORE. [Online]. Available: http://www.\ndasein.org/api/dasein-cloud/ [cited in p. 147]\n\n[159] Williams, A. (2010, December) Another Giant Gets Another\nSexy Startup: Rackspace Acquires Cloudkick. [Online]. Avail-\nable: http://readwrite.com/2010/12/15/another-giant-buys-another-sex#\nawesm=~oE40TX0mteRPFf [cited in p. 147]\n\nhttp://axis.apache.org/axis2/java/core/\nhttp://axis.apache.org/axis2/java/core/\nhttp://deltacloud.apache.org/\nhttp://deltacloud.apache.org/\nhttp://jclouds.apache.org/\nhttp://jclouds.apache.org/\nhttp://libcloud.apache.org/\nhttp://dmtf.org/standards/cloud\nhttp://dmtf.org/standards/cloud\nhttp://deltacloud.apache.org/about.html\nhttp://deltacloud.apache.org/about.html\nhttp://deltacloud.apache.org/drivers.html#drivers\nhttp://deltacloud.apache.org/drivers.html#drivers\nhttp://www.rackspace.com/cloud/files/\nhttp://www.rackspace.com/cloud/files/\nhttp://fog.io/\nhttp://jclouds.apache.org/reference/providers/\nhttp://jclouds.apache.org/reference/providers/\nhttp://ant.apache.org/\nhttp://maven.apache.org/\nhttp://maven.apache.org/\nhttp://demobox.github.io/jclouds-maven-site-1.6.2/1.6.2-incubating/jclouds/apidocs/\nhttp://demobox.github.io/jclouds-maven-site-1.6.2/1.6.2-incubating/jclouds/apidocs/\nhttp://www.dasein.org/api/dasein-cloud/\nhttp://www.dasein.org/api/dasein-cloud/\nhttp://readwrite.com/2010/12/15/another-giant-buys-another-sex#awesm=~oE40TX0mteRPFf\nhttp://readwrite.com/2010/12/15/another-giant-buys-another-sex#awesm=~oE40TX0mteRPFf\n\n\nBIBLIOGRAPHY 253\n\n[160] Libcloud. Supported Providers. [Online]. Available: http://libcloud.\napache.org/supported_providers.html [cited in p. 147]\n\n[161] Libcloud . Drivers. [Online]. Available: https://github.com/StratusLab/\nlibcloud-drivers [cited in p. 148]\n\n[162] Libcloud. Registering a third party driver. [On-\nline]. Available: https://libcloud.readthedocs.org/en/latest//other/\nregistering-a-third-party-driver.html [cited in p. 148]\n\n[163] Aeolus. Manage Your Cloud Deployments with Ease. [Online]. Available:\nhttp://www.aeolusproject.org/ [cited in p. 148]\n\n[164] mOSAIC. Open source API and platform for multiple clouds. [Online].\nAvailable: http://www.mosaic-cloud.eu/ [cited in p. 149]\n\n[165] IBM. The Simple Cloud API. [Online]. Available: http://www.ibm.com/\ndeveloperworks/library/os-simplecloud/ [cited in p. 149]\n\n[166] RADICAL Research. SAGA Project. [Online]. Available: http://\nsaga-project.github.io/ [cited in p. 149]\n\n[167] Contrail. Open computing infrastructures for elastic services. [Online].\nAvailable: http://contrail-project.eu/ [cited in p. 149]\n\n[168] OGF. XtreemOS. [Online]. Available: http://www.xtreemos.eu/\n[cited in p. 149]\n\n[169] Ruby. [Online]. Available: https://www.ruby-lang.org/en/ [cited in p. 151]\n\n[170] Matsumoto, Y., Ruby in a Nutshell. O\u2019Reilly Media, 2001. [cited in p. 151]\n\n[171] RubyGems. [Online]. Available: http://rubygems.org/ [cited in p. 151]\n\n[172] Blake Mizerany. Sinatra. [Online]. Available: http://www.sinatrarb.com/\nabout.html [cited in p. 153]\n\n[173] Sinatra-rabbit. [Online]. Available: http://rubydoc.info/gems/\nsinatra-rabbit/1.1.6/frames [cited in p. 153]\n\n[174] Rack. Rack Documentation. [Online]. Available: http://rack.rubyforge.\norg/doc/ [cited in p. 153]\n\n[175] Thin. Thin: A fast and very simple Ruby web server. [Online]. Available:\nhttp://code.macournoyer.com/thin/ [cited in p. 153]\n\n[176] David Heinemeier Hansson. Rails. [Online]. Available: http://rubyonrails.\norg/ [cited in p. 153]\n\nhttp://libcloud.apache.org/supported_providers.html\nhttp://libcloud.apache.org/supported_providers.html\nhttps://github.com/StratusLab/libcloud-drivers\nhttps://github.com/StratusLab/libcloud-drivers\nhttps://libcloud.readthedocs.org/en/latest//other/registering-a-third-party-driver.html\nhttps://libcloud.readthedocs.org/en/latest//other/registering-a-third-party-driver.html\nhttp://www.aeolusproject.org/\nhttp://www.mosaic-cloud.eu/\nhttp://www.ibm.com/developerworks/library/os-simplecloud/\nhttp://www.ibm.com/developerworks/library/os-simplecloud/\nhttp://saga-project.github.io/\nhttp://saga-project.github.io/\nhttp://contrail-project.eu/\nhttp://www.xtreemos.eu/\nhttps://www.ruby-lang.org/en/\nhttp://rubygems.org/\nhttp://www.sinatrarb.com/about.html\nhttp://www.sinatrarb.com/about.html\nhttp://rubydoc.info/gems/sinatra-rabbit/1.1.6/frames\nhttp://rubydoc.info/gems/sinatra-rabbit/1.1.6/frames\nhttp://rack.rubyforge.org/doc/\nhttp://rack.rubyforge.org/doc/\nhttp://code.macournoyer.com/thin/\nhttp://rubyonrails.org/\nhttp://rubyonrails.org/\n\n\n254 BIBLIOGRAPHY\n\n[177] Harris, A. and Haase, K., Sinatra: Up and Running. O\u2019Reilly Media, 2011.\n[cited in p. 153]\n\n[178] cURL. [Online]. Available: http://curl.haxx.se/ [cited in p. 153, 154]\n\n[179] VIM. [Online]. Available: http://www.vim.org/ [cited in p. 154]\n\n[180] Wireshark. [Online]. Available: http://www.wireshark.org/ [cited in p. 154]\n\n[181] Deltacloud. Deltacloud API. [Online]. Available: https://deltacloud.\napache.org/rest-api.html#rest [cited in p. 155]\n\n[182] OpenNebula. OpenNebula: How to Interact with OpenNebula Using\nDeltacloud. [Online]. Available: http://wiki.opennebula.org/deltacloud\n[cited in p. 168]\n\n[183] Deltacloud. OpenStack RubyGem. [Online]. Available: https://rubygems.\norg/gems/openstack/versions/1.1.2 [cited in p. 168]\n\n[184] Childers, C. CloudStack Driver for Deltacloud. [Online]. Avail-\nable: https://github.com/chipchilders/deltacloud/tree/cloudstack-driver/\nserver/lib/deltacloud/drivers/cloudstack [cited in p. 169]\n\n[185] Deltacloud. Deltacloud: Write a provider driver. [Online]. Available:\nhttps://deltacloud.apache.org/write-new-driver.html [cited in p. 169]\n\n[186] Lunacloud. Lunacloud: Cloud server pricing. [Online]. Available:\nhttp://www.lunacloud.com/en/cloud-server-pricing [cited in p. 173]\n\n[187] REXML. Overview. [Online]. Available: http://www.germane-software.\ncom/software/rexml/ [cited in p. 175]\n\n[188] ERB. [Online]. Available: http://ruby-doc.org/stdlib-2.1.1/libdoc/erb/\nrdoc/ERB.html [cited in p. 175]\n\n[189] cURL. The man page. [Online]. Available: http://curl.haxx.se/docs/\nmanpage.html [cited in p. 205]\n\n[190] OpenNebula. OpenNebula Market place: ttylinux - kvm. [Online]. Avail-\nable: http://marketplace.c12g.com/appliance/4fc76a938fb81d3517000003\n[cited in p. 207]\n\n[191] Cybercom. OpenStack driver for Deltacloud. [Online]. Avail-\nable: http://mail-archives.apache.org/mod_mbox/deltacloud-dev/\n201404.mbox/browser [cited in p. 209]\n\n[192] Aaron Patterson. Nokogiri. [Online]. Available: http://nokogiri.org/\n[cited in p. 222]\n\nhttp://curl.haxx.se/\nhttp://www.vim.org/\nhttp://www.wireshark.org/\nhttps://deltacloud.apache.org/rest-api.html#rest\nhttps://deltacloud.apache.org/rest-api.html#rest\nhttp://wiki.opennebula.org/deltacloud\nhttps://rubygems.org/gems/openstack/versions/1.1.2\nhttps://rubygems.org/gems/openstack/versions/1.1.2\nhttps://github.com/chipchilders/deltacloud/tree/cloudstack-driver/server/lib/deltacloud/drivers/cloudstack\nhttps://github.com/chipchilders/deltacloud/tree/cloudstack-driver/server/lib/deltacloud/drivers/cloudstack\nhttps://deltacloud.apache.org/write-new-driver.html\nhttp://www.lunacloud.com/en/cloud-server-pricing\nhttp://www.germane-software.com/software/rexml/\nhttp://www.germane-software.com/software/rexml/\nhttp://ruby-doc.org/stdlib-2.1.1/libdoc/erb/rdoc/ERB.html\nhttp://ruby-doc.org/stdlib-2.1.1/libdoc/erb/rdoc/ERB.html\nhttp://curl.haxx.se/docs/manpage.html\nhttp://curl.haxx.se/docs/manpage.html\nhttp://marketplace.c12g.com/appliance/4fc76a938fb81d3517000003\nhttp://mail-archives.apache.org/mod_mbox/deltacloud-dev/201404.mbox/browser\nhttp://mail-archives.apache.org/mod_mbox/deltacloud-dev/201404.mbox/browser\nhttp://nokogiri.org/\n\n\tContents\n\tList of Figures\n\tList of Tables\n\tList of Code Snippets\n\tAcronyms\n\tIntroduction\n\tMotivation\n\tThesis\n\tObjectives\n\tFunctional Tests\n\tExpected Results\n\tContributions\n\tWork Plan\n\tStructure of the Dissertation\n\n\tCloud Computing\n\tDefinition\n\tDeployment Models\n\tPrivate Clouds\n\tCommunity Clouds\n\tPublic Clouds\n\tHybrid Clouds\n\n\tService Models\n\tInfrastructure as a Service\n\tPlatform as a Service\n\tSoftware as a Service\n\n\tCharacteristics\n\tOn-demand Self-service\n\tBroad Network Access\n\tResource Pooling\n\tRapid Elasticity\n\tMeasured Service\n\n\tConcerns\n\tUnpredictable Performance\n\tSecurity Risks\n\tData Privacy\n\n\tVirtualization Technologies\n\tFull Virtualization\n\tPara-virtualization\n\tOS-level Virtualization\n\n\tBusiness Perspective\n\tIaaS Business Model\n\tPaaS Business Model\n\tSaaS Business Model\n\tStakeholders\n\tService Life-Cycle\n\tService Level Agreements\n\n\tConclusions\n\n\tCloud Infrastructure Platforms\n\tOpenNebula\n\tArchitecture\n\tInterfaces\n\tPlatform Deployment\n\tAuthentication and Authorization\n\n\tOpenStack\n\tArchitecture\n\tInterfaces\n\tPlatform Deployment\n\tAuthentication and Authorization\n\n\tCloudStack\n\tArchitecture\n\tInterfaces\n\tPlatform Deployment\n\tAuthentication and Authorization\n\n\tEucalyptus\n\tArchitecture\n\tInterfaces\n\tPlatform Deployment\n\tAuthentication and Authorization\n\n\tParallels Automation for Cloud Infrastructure\n\tArchitecture\n\tInterfaces\n\tPlatform Deployment\n\tAuthentication and Authorization\n\n\tConclusion\n\n\tInterface Libraries Comparison\n\tInterface Types\n\tRESTful API\n\tXML-RPC API\n\tQuery API\n\n\tOpenNebula Interface\n\tServer Management\n\tAllocate VM\n\tVM Actions\n\tSave Disk\n\tAttach Disk\n\tDetach Disk\n\tAttach NIC\n\tDetach NIC\n\tChange VM Ownership\n\tRename VM\n\tCreate Snapshot\n\tRevert Snapshot\n\tDelete Snapshot\n\tResize VM\n\tUpdate VM\n\tVM Information\n\tVM Pool Information\n\tVM Monitoring\n\tVM Pool Monitoring\n\n\tTemplate Management\n\tAllocate Template\n\tClone Template\n\tDelete Template\n\tInstantiate Template\n\tUpdate Template\n\tChange Template Ownership\n\tRename Template\n\tTemplate Information\n\tTemplate Pool Information\n\n\tImage Management\n\tAllocate Image\n\tClone Image\n\tDelete Image\n\tEnable Image\n\tPersistent\n\tChange Image Type\n\tUpdate Image\n\tChange Image Ownership\n\tRename Image\n\tImage Information\n\tImage Pool Information\n\n\tNetwork Management\n\tAllocate VN\n\tDelete VN\n\tAdd VN Leases\n\tRemove VN Leases\n\tHold VN Lease\n\tRelease VN Lease\n\tUpdate VN\n\tChange VN Ownership\n\tRename VN\n\tVN Information\n\tVN Pool Information\n\n\tData-Store Management\n\tAllocate Data-store\n\tDelete Data-store\n\tUpdate Data-store\n\tChange Data-store Ownership\n\tData-store Information\n\tData-store Pool Information\n\n\n\tOpenStack Interface\n\tServer Management\n\tList Servers\n\tCreate Server\n\tGet Server Details\n\tUpdate Server\n\tDelete Server\n\tList Addresses\n\tList Addresses by Network\n\tChange Administrator Password\n\tReboot Server\n\tRebuild Server\n\tResize Server\n\tConfirm Resized Server\n\tRevert Resized Server\n\tCreate Image\n\n\tFlavour Management\n\tList Flavours\n\tGet Flavour Details\n\n\tImage Management\n\tCreate Image\n\tList Images\n\tGet Image Details\n\tUpdate Image\n\tDelete Image\n\tUpload Image\n\tDownload Image\n\tAdd Image Tag\n\tDelete Image Tag\n\n\tNetwork Management\n\tList Networks\n\tShow Network\n\tCreate Network\n\tUpdate Network\n\tDelete Network\n\tList Subnets\n\tShow Subnet\n\tCreate Subnet\n\tUpdate Subnet\n\tDelete Subnet\n\tList Ports\n\tShow Port\n\tCreate Port\n\tUpdate Port\n\tDelete Port\n\n\tVolume Management\n\tCreate Volume\n\tList Volumes\n\tList Volume Details\n\tShow Volume\n\tUpdate Volume\n\tDelete Volume\n\n\n\tCloudStack Interface\n\tServer Management\n\tDeploy Virtual Machine\n\tDestroy Virtual Machine\n\tReboot Virtual Machine\n\tStart Virtual Machine\n\tStop Virtual Machine\n\tReset Password For Virtual Machine\n\tReset SSH Key For Virtual Machine\n\tUpdate Virtual Machine\n\tList Virtual Machines\n\tGet VM Password\n\tRestore Virtual Machine\n\tChange Service For Virtual Machine\n\tAdd NIC To Virtual Machine\n\tRemove NIC From Virtual Machine\n\tUpdate Default NIC For Virtual Machine\n\n\tTemplate Management\n\tCreate Template\n\tUpdate Template\n\tCopy Template\n\tDelete Template\n\tList Templates\n\tUpdate Template Permissions\n\tList Template Permissions\n\tExtract Template\n\n\tImage Management\n\tAttach ISO\n\tDetach ISO\n\tList ISO\n\tUpdate ISO\n\tDelete ISO\n\tCopy ISO\n\tUpdate ISO Permissions\n\tList ISO Permissions\n\tExtract ISO\n\n\tNetwork Management\n\tList Port Forwarding Rules\n\tCreate Port Forwarding Rule\n\tDelete Port Forwarding Rule\n\tUpdate Port Forwarding Rule\n\tCreate Firewall Rule\n\tDelete Firewall Rule\n\tList Firewall Rules\n\tCreate Egress Firewall Rule\n\tDelete Egress Firewall Rule\n\tList Egress Firewall Rules\n\tCreate Network\n\tDelete Network\n\tList Networks\n\tRestart Network\n\tUpdate Network\n\n\tVolume Management\n\tAttach Volume\n\tUpload Volume\n\tDetach Volume\n\tCreate Volume\n\tDelete Volume\n\tList Volumes\n\tExtract Volume\n\tMigrate Volume\n\tResize Volume\n\n\n\tPACI Interface\n\tServer Management\n\tList Servers\n\tStart/Stop a Server\n\tCreate Server\n\tCreate Server From Image\n\tClone Server\n\tModify Server Configuration\n\tReset Server Administrator Password\n\tObtain Server Information\n\tObtain Server History\n\tDelete Server\n\tSet Backup Schedule\n\tCancel Backup Schedule\n\tList Backup Schedule\n\tRestore a Server\n\n\tFirewall Management\n\tList Firewall Rules\n\tCreate Firewall Rule\n\tModify Firewall Rule\n\tDelete Firewall Rule\n\n\tApplication Template Management\n\tList Application Templates\n\tGet Application Templates Information\n\tInstall Application Templates\n\n\tImage Management\n\tList Images\n\tGet Image Information\n\tCreate Image\n\tDelete Image\n\n\n\tConclusions\n\n\tInteroperable Interface Proposal\n\tDedicated Interoperable Service\n\tInteraction Layer\n\tAbstraction Layer\n\tInterface Layer\n\tVirtual Machine Management\n\tImage Management\n\n\n\tCloud Abstraction Interface Solutions\n\tDeltacloud\n\tjClouds\n\tLibcloud\n\n\tConclusions\n\n\tProject Development\n\tDevelopment Environment\n\tLanguages\n\tGeneral Purpose Language\n\tDomain Specific Languages\n\n\tBack-end Technology\n\tWeb Application Library\n\tWeb Application Server\n\n\tFront-end Technologies\n\tDevelopment Tools\n\n\tArchitecture\n\tInteroperable Service API\n\tRealms\n\tList Realms\n\tShow Realm Information\n\n\tHardware Profiles\n\tList Hardware Profiles\n\tShow Hardware Profile Information\n\n\tImages\n\tList Images\n\tShow Image Information\n\tCreate Image from Instance\n\tDelete Image\n\n\tInstances\n\tList Instances\n\tShow Instance Information\n\tInstance Action\n\tCreate Instance\n\tDelete Instance\n\n\tKeys\n\tList Keys\n\tShow Key Information\n\tCreate Key\n\tDelete Key\n\n\tFirewalls\n\tList Firewalls\n\tShow Firewall Information\n\tCreate Firewall\n\tDelete Firewall\n\tCreate Firewall Rule\n\tDelete Firewall Rule\n\n\tAddresses\n\tList Addresses\n\tShows Address Information\n\tCreate Address\n\tDelete Address\n\tAssociate Address\n\tDissociate Address\n\n\tLoad Balancers\n\tList Load Balancers\n\tShow Load Balancer Information\n\tCreate Load Balancer\n\tDelete Load Balancer\n\tRegister Instance to Load Balancer\n\tUnregister Instance from Load Balancer\n\n\tVolumes\n\tList Volumes\n\tShow Volume Information\n\tCreate Volume\n\tDelete Volume\n\tAttach Volume\n\tDetach Volume\n\n\tSnapshots\n\tList Snapshots\n\tShow Snapshot Information\n\tCreate Snapshot\n\tDelete Snapshot\n\n\tBlobs\n\tList Buckets\n\tShow Bucket Information\n\tCreate Bucket\n\tDelete Bucket\n\tShow Blob Information\n\tCreate Blob\n\tDelete Blob\n\tShow Blob Metadata\n\tUpdate Blob Metadata\n\n\n\tInteroperable Service GUI\n\tDeployment Configurations\n\tSingle Tenant Configuration\n\tMultiple Tenant Configuration\n\n\tReused Driver Modules\n\tOpenNebula Driver\n\tOpenStack Driver\n\tCloudStack Driver\n\n\tDeveloped PACI Driver\n\tConclusions\n\n\tTest, Debugging and Validation\n\tTest Bed\n\tInteroperable Service Installation\n\tDeltacloud Installation\n\tCloudStack Driver Module Inclusion\n\tPACI Driver Module Inclusion\n\n\tOpenNebula Installation\n\tOS Configuration\n\tOpenNebula Services Installation\n\tKVM Hypervisor Installation\n\tVirtual Resources Allocation\n\n\tOpenStack Installation\n\tOS Configuration\n\tKeystone Installation\n\tGlance Installation\n\tNova Installation\n\tVirtual Resources Allocation\n\n\tCloudStack Installation\n\tOS Configuration\n\tManagement Server Installation\n\tKVM Hypervisor Installation\n\tCloud Infrastructure Configuration\n\tVirtual Resources Allocation\n\n\n\tTesting and Evaluation\n\tOpenNebula Interaction Tests and Results\n\tOpenStack Interaction Tests and Results\n\tCloudStack Interaction Tests and Results\n\tPACI Interaction Tests and Results\n\n\tConclusions\n\n\tConclusions\n\tDiscussion\n\tEnvisaged Use Cases\n\tFuture Developments\n\n\tAppendices\n\tCloud Client Library\n\tPACI Client\n\tPACI Driver\n\tBibliography"}]}}}