{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.10927"}, {"@name": "filename", "#text": "16130_348525.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE SANTA CATARINA\n\nDEPARTAMENTO DE ENGENHARIA MECA?NICA\n\nGabriel Silva de Matos\n\nLASER TRIANGULATION SENSOR WITH\n\nREFRACTION MODELLING FOR UNDERWATER 3D\n\nMEASUREMENT\n\nFloriano?polis\n\n2017\n\n\n\n\n\nGabriel Silva de Matos\n\nLASER TRIANGULATION SENSOR WITH\n\nREFRACTION MODELLING FOR UNDERWATER 3D\n\nMEASUREMENT\n\nDissertac?a?o submetida ao Programa\n\nde Po?s-Graduac?a?o em Engenharia Meca?nica\n\npara a obtenc?a?o do Grau de Mestre\n\nem Engenharia Meca?nica.\n\nOrientador: Tiago Loureiro Figaro da\n\nCosta Pinto, Dr. Eng.\n\nFloriano?polis\n\n2017\n\n\n\n\n\nGabriel Silva de Matos\n\nLASER TRIANGULATION SENSOR WITH\n\nREFRACTION MODELLING FOR UNDERWATER 3D\n\nMEASUREMENT\n\nEsta Dissertac?a?o foi julgada aprovada para a obtenc?a?o do T??tulo\nde \u201cMestre em Engenharia Meca?nica\u201d, e aprovada em sua forma final\npelo Programa de Po?s-Graduac?a?o em Engenharia Meca?nica.\n\nFloriano?polis, 8 de Maio 2017.\n\nJonny Carlos da Silva, Dr. Eng.\nCoordenador\n\nTiago Loureiro Figaro da Costa Pinto, Dr. Eng.\nOrientador\n\nBanca Examinadora:\n\nTiago Loureiro Figaro da Costa Pinto, Dr. Eng.\nPresidente\n\nArmando Albertazzi Gonc?alves Ju?nior, Dr. Eng.\n\nMarcelo Ricardo Stemmer, Dr.-Ing.\n\nWalter Antonio Kapp, Dr. Eng.\n\nMarco Antonio Martins Cavaco, PhD.\n\n\n\n\n\nACKNOWLEDGEMENTS\n\nTo my parents, Daniel and Anesia, who have always supported\nme and taught me the importance of education.\n\nTo Marcella, my life partner, thank you for all the love, trust\nand companionship.\n\nTo Prof. Tiago Pinto, for believing in my capabilities and for\nthe valuable advice, availability and cooperation throughout this work.\n\nTo all the LABMETRO staff for their assistance throughout the\nprogress of this work. Especially, to Pedro for sharing the technical\ndiscussions, to Fabio for guidance during the mechanical design and\nRosana, who was always ready to help.\n\nTo the Brazilian Institute of Oil Gas and Biofuels (IBP), who\nprovided the funding for this work.\n\nAnd to all my family and friends for accompanying me along this\npath.\n\n\n\n\n\nRESUMO\n\nMedic?o?es tridimensionais em ambientes subaqua?ticos sa?o u?teis\nem diversas aplicac?o?es. Por exemplo, a indu?stria de petro?leo e ga?s\npossui muitos equipamentos utilizados na extrac?a?o de petro?leo que ne-\ncessitam de manutenc?a?o constante, a biologia tem grande interesse\nem investigar a vida submarina e investigac?o?es de naufra?gios podem\nfornecer dados u?teis. No entanto, nem sempre e? uma tarefa fa?cil obter\ndados subaqua?ticos confia?veis, porque a luz espalha-se, e? absorvida e\nrefrata e o som nem sempre pode fornecer uma boa resoluc?a?o em curtas\ndista?ncias. Estes obsta?culos devem ser considerados para alcanc?ar os\nmelhores resultados de medic?a?o poss??veis.\n\nEste trabalho esta? dividido em quatro partes: uma revisa?o das\nte?cnicas de digitalizac?a?o 3D subaqua?ticas dispon??veis, o projeto meca?nico\ne o?ptico do sistema para atender aos objetivos descritos, os algoritmos\nde calibrac?a?o e de medic?a?o utilizados com o sensor e uma avaliac?a?o final\ncom comparac?o?es entre o me?todo proposto e um me?todo de refere?ncia.\n\nAtualmente, diversos me?todos sa?o utilizados para realizar a dig-\nitalizac?a?o destes ambientes, alguns destes sa?o: Sound Navigation and\nRanging (sonar) , Light Detection And Ranging (LiDAR), visa?o este?reo\ne triangulac?a?o laser. O sonar e? amplamente utilizado em embarcac?o?es\nmar??timas para obter informac?o?es sobre a profundidade e o fundo do\nmar e para a localizac?a?o e mapeamento simulta?neos (SLAM) associa-\ndos a ve??culos operados remotamente (ROVs). Mas eles tambe?m podem\nser usados em medic?o?es tridimensionais sendo por vezes chamados como\nca?meras acu?sticas. Eles sa?o baseados na emissa?o de um pulso sonoros\ne na medic?a?o do tempo de vo?o do sinal. Sendo eles divididos em u?nico\nfeixe, que fornece informac?a?o de azimute e dista?ncia; mu?ltiplos feixes,\nque fornece mu?ltiplos azimutes e dista?ncias e de feixe lateral, que uti-\nliza uma onda em forma de cone para obter uma imagem acu?stica.\nUma das grandes vantagens dos sonares e? a capacidade de obter da-\ndos confia?veis mesmo em a?guas muito turvas. LiDAR tambe?m e? uma\nte?cnica de tempo de vo?o, entretanto ele utiliza a luz para obter o tempo\nde retorno do sinal.\n\nA fotogrametria e? uma te?cnica que utiliza imagens para obter\ninformac?o?es como dista?ncias e geometrias da cena. Exemplos de fo-\ntogrametria sa?o a visa?o este?reo e a triangulac?a?o laser. Entretanto, ela\npossui limitac?o?es em ambientes subaqua?ticos, porque a luz sofre diver-\nsas influe?ncias da a?gua. Apesar disto, em medic?o?es de curta dista?ncia,\n\n\n\nate? 10 m, elas possuem as menores incertezas entre as outras te?cnicas\napresentadas. Uma considerac?a?o comum na fotogrametria e? a consid-\nerac?a?o da ca?mera pinhole: todos os raios de luz passam pelo centro\nde pupila da ca?mera e projetam-se no plano imagem. Neste modelo a\ncalibrac?a?o consiste em obter a matriz de projec?a?o da ca?mera, a matriz\nque projeta um ponto 3D qualquer em um ponto na imagem.\n\nTe?cnicas de visa?o este?reo utilizam duas ou mais ca?meras para\nobter a nuvem de pontos tridimensionais. Nelas a matriz de projec?a?o\ne? utilizada para projetar retas de pontos homo?logos, mesmo ponto 3D\nrepresentado em diferentes imagens. O cruzamento destas retas resulta\nno ponto 3D medido. Para obter estes pontos homo?logos a cena pode\nser iluminada com projec?a?o de luz estruturada ou iluminac?a?o passiva.\nUm exemplo de luz estruturada e? a projec?a?o de franjas senoidais, pos-\nsibilitando a assinatura u?nica dos diversos pontos 3D. Assim, a busca\nde pontos homo?logos resume-se em buscar pontos na imagem com o\nmesmo valor de fase.\n\nA triangulac?a?o laser pode ser considerada uma forma de fo-\ntogrametria com iluminac?a?o estruturada, ja? que um laser e? utilizado\npara iluminar a cena e uma ca?mera observa-o com certo a?ngulo. Os\nprincipais para?metros que definem um sistema de triangulac?a?o laser\nsa?o: o baseline (dista?ncia entre o centro do pinhole da ca?mera e o eixo\ndo laser), o a?ngulo de triangulac?a?o (a?ngulo entre o eixo da ca?mera e o\neixo do laser), o foco da lente, o tamanho do pixel da ca?mera e a res-\noluc?a?o da ca?mera. Dois me?todos de calibrac?a?o do sistema sa?o demon-\nstrados. No primeiro, polino?mios sa?o ajustados para correlacionar o\npico laser detectado com o ponto 3D medido. No segundo, o modelo\npinhole e? utilizado em conjunto com um ajuste matema?tico do plano\nlaser, tornando, assim, poss??vel trac?ar uma reta da ca?mera que cruza\ncom o plano laser ajustado.\n\nEmbaixo d?a?gua, a luz sofre diversas alterac?a?o quando com-\nparada com seu comportamento no ar. As principais alterac?o?es sa?o:\nabsorc?a?o, espalhamento e refrac?a?o. A absorc?a?o acontece principal-\nmente nos comprimentos de onda pro?ximos do vermelho (650 nm) . Ja?\no espalhamento tem uma maior influe?ncia nos comprimentos de onda\nazuis (450 nm), resultando na cor dos oceanos. Assim, uma soluc?a?o de\ncompromisso e? necessa?ria, sendo, geralmente, lasers pro?ximos do verde\nescolhidos para medic?o?es em dista?ncias maiores. A refrac?a?o resulta em\numa \u201ddistorc?a?o 3D\u201d, pois o desvio do raio depende da dista?ncia do\nobjeto. Entretanto, conhecendo-se o eixo de refrac?a?o, as dista?ncias en-\ntre as interfaces de refrac?a?o e o primeiro raio de projec?a?o da ca?mera e?\nposs??vel determinar o raio final da refrac?a?o.\n\n\n\nO sistema proposto possui os seguintes objetivos: o sensor deve\nser projetado para medir um quinto da secc?a?o transversal exterior de\num tubo de 300 mm de dia?metro, a incerteza de medic?a?o final deve ser\npro?xima de de?cimos de mil??metros, a dista?ncia do sistema de medic?a?o a?\nsuperf??cie de interesse deve ser suficiente para garantir a seguranc?a do\nequipamento, por fim, o sistema tambe?m deve ser modular, para per-\nmitir mudanc?as futuras no volume de medic?a?o, com alterac?o?es na base-\nline do sistema e no a?ngulo de triangulac?a?o. Apo?s diversos ca?lculos e\nsimulac?o?es o sensor que cumpre estes requisitos tem as seguintes carac-\nter??sticas: baseline de 265 mm, a?ngulo de triangulac?a?o de 35?, dista?ncia\nfocal da lente de 12.5 mm, resoluc?a?o do sensor de 1280x1024 pixels e\ntamanho do pixel de 5.5 ?m. Esta configurac?a?o garante uma profun-\ndidade de medic?a?o de 220 mm com uma dista?ncia m??nima de medic?a?o\nde 200 mm e um comprimento de 180 mm ao longo da linha laser no\nstand-off. Ale?m do sistema, uma bancada para medic?a?o e calibrac?a?o\ntambe?m foi constru??da. Ela consiste em uma mesa de deslocamento\nlinear, um tanque para medic?a?o, um tanque de armazenamento e de\num sistema hidra?ulico, permitindo, assim, a medic?a?o tanto no ar como\nna a?gua em todo o volume de medic?a?o.\n\nAlgoritmos de medic?a?o e calibrac?a?o foram desenvolvidos. A\nmedic?a?o consiste em trac?ar o caminho inverso do raio de luz, da ca?mera\nate? o plano laser. O primeiro passo e? obter o raio saindo do pico laser\ndetectado na imagem utilizando a matriz de calibrac?a?o da ca?mera.\nDepois, o cruzamento deste raio com o plano de refrac?a?o e? medido\nutilizando a dista?ncia do centro o?ptico ate? a janela de refrac?a?o. O\nraio e?, enta?o, refratado e o cruzamento deste raio com o plano laser\ne? computado. Para obter a dista?ncia do centro o?ptico ate? a janela de\nrefrac?a?o uma calibrac?a?o e? proposta. Nesta calibrac?a?o, uma aquisic?a?o\nsubaqua?tica de um degrau e? realizada e a altura do degrau e? medida\nutilizando-se dista?ncias de janela pro?ximas a?s do projeto meca?nico. A\ndista?ncia com a menor diferenc?a para com a altura real e? a utilizada\ndurante as medic?o?es.\n\nPara avaliar o sistema diversas medic?o?es foram realizadas de di-\nversos objetos. O principal objeto de avaliac?a?o sa?o duas esferas com\ndista?ncia entre seus centros calibrada. A bancada foi utilizada para\ncomparar os resultados do ajuste polinomial e dos algoritmos desen-\nvolvidos tanto no ar quanto na a?gua. Foram avaliados tambe?m a consid-\nerac?a?o de apenas uma refrac?a?o ou duas refrac?o?es na janela dos sistema\ne a utilizac?a?o de uma ou mais aquisic?o?es do degrau para otimizac?a?o da\ndista?ncia da janela. A considerac?a?o de uma refrac?a?o provou-se superior\na? de duas refrac?o?es e ao menos tre?s imagens do degrau fora necessa?rias\n\n\n\npara calibrar o sistema na melhor dista?ncia de janela. Os resultado do\nme?todo proposto e do ajuste polinomial foram bastante semelhantes\ntanto no ar quanto na a?gua, sendo as medic?o?es na a?gua com menores\nerros do que as medic?o?es no ar, conseque?ncia da diminuic?a?o do cone de\nvisa?o no ambiente subaqua?tico, resultando em uma melhor resoluc?a?o.\n\nAssim, o sistema desenvolvido, composto tanto dos componentes\nf??sicos quanto dos algoritmos provou-se capaz de realizar as medic?o?es\nem laborato?rio, tendo resultados pro?ximos ao me?todo de refere?ncia.\nPoss??veis trabalhos futuros incluem o acoplamento do sensor a mo?dulos\nde movimentac?a?o, como ROVs, para a realizac?a?o de medic?o?es fora do\nlaborato?rio.\n\nPalavras-chave: Digitalizac?a?o subaqua?tica. Sensores de trian-\ngulac?a?o laser. Modelo de refrac?a?o. Projeto o?ptico.\n\n\n\nABSTRACT\n\nUnderwater tridimensional measurement has many applications, for ex-\nample, to control underwater equipment during optimized maintenance\nprocedures in the oil and gas industry. Systems with laser triangulation\nsensors (LTS) are being used underwater at present and some under-\nwater problems with LTS have already been discussed. Among these\nchallenging obstacles are poor image quality, due to light absorption\nand backscattering, and refraction, due to optical window interfaces\nbetween water and air inside the camera chamber. The refraction ef-\nfect can be predicted knowing the distance from the camera pinhole\ncenter to the surface of refraction, the axis of refraction, the refractive\nindex of the mediums and the thickness of the optical window. This\nwork analyses two methods for underwater LTS calibration using real\nexperiments with a built LTS, in-air and underwater. The first method\nuses a polynomial adjustment correlating to the laser peak for each\ncamera image line with a 3D point. This method needs a complete\ncalibration in the underwater environment. The second method pro-\nposed is based on the pinhole camera model and a fitted mathematical\nplane for the projected laser light plane. In air measurements, a line\ncan be defined through the lens center using the pinhole projection\nmatrix for each laser peak detected in the image. The intersection of\nthis line with the laser mathematical plane leads to a measured 3D\npoint. For high quality underwater LTS measurements, it is necessary\nto additionally consider the refraction effect on the window and water\ninterface. Considering the optical window normal to the camera axis,\na ray path is defined on the plane of refraction according to the Snell?s\nlaw to intercept the laser plane, defining a 3D point. The calibration\nfor underwater measurement needs to estimate the window distance\nfrom the camera. In the proposed method, after the in-air calibration,\na step standard is measured underwater and the window distance from\nthe camera is optimized. The method is evaluated according to the\nguidelines of VDI/VDE 2634 and multiple objects were evaluated.\nKeywords: Underwater digitalization. Laser triangulation sensors.\nRefraction modelling. Optical design.\n\n\n\n\n\nLIST OF FIGURES\n\nFigure 1 Images from different sonar types. (HORNER et al., 2009;\nCOIRAS; PETILLOT; LANE, 2007). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\nFigure 2 Acoustic lens schematic. (ROSENBLUM; KAMGAR-PARSI,\n1992) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\nFigure 3 Echoscope equipment and measurement result. (CO-\nDAOCTOPUS. . . , 2017; CRISP. . . , 2017) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\nFigure 4 BlueView equipment and measurement result.(BLUVIEW. . . ,\n2017; BLUEVIEW. . . , 2017). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\nFigure 5 Scopos Investigator equipment, measurement result and\nsubject. (SCOPOS. . . , 2017; SUBSEA. . . , 2017) . . . . . . . . . . . . . . . . . . . . . 33\n\nFigure 6 STIL technique. (MCLEAN, 1999) . . . . . . . . . . . . . . . . . . . . . . 34\n\nFigure 7 Pinhole camera schematic. (PINHOLE. . . , 2017). . . . . . . . . 34\n\nFigure 8 Pinhole mathematical model. Adapted from Hartley and\nZisserman (2004). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\nFigure 9 Stereo system. (CHOOSING. . . , 2017) . . . . . . . . . . . . . . . . . . . 39\n\nFigure 10 Homologous points and epipolar geometry in stereo vi-\nsion. Source: Author\u2019s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\nFigure 11 Commercial active stereo systems. Source: Author\u2019s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\nFigure 12 Accuity laser. (COMPACT. . . , 2017) . . . . . . . . . . . . . . . . . . . . 41\n\nFigure 13 2G Robotics underwater laser triangulation sensor and\nmeasurement example. (ULS-500. . . , 2017; OFFSHORE. . . , 2017) . . . . 42\n\nFigure 14 Laser triangulation model. Adapted from Gan and Tang\n(2011). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\nFigure 15 Uncertainty limit for a LTS.(DORSCH; HA?USLER; HER-\nRMANN, 1994) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\nFigure 16 Scheimpflug configuration in a LTS. Adapted from Gan\nand Tang (2011) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n\nFigure 17 The standoff of a LTS is in the middle of the measurement\nrange. Source: Author?s own work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\nFigure 18 Occlusion during measurement. Source: Author?s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n\nFigure 19 Laser plane fitting measurement. (SANTOLARIA et al.,\n2009) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n\n\n\nFigure 20 Light absorption in different oceans. 1 is in the Central\nPacific; a very clear, deep-sea water, 6 is in the Atlantic ocean,\n(WOZNIAK; DERA, 2007). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n\nFigure 21 Image of the same pattern in the same position, with and\nwithout water, respectively. The images were captured through a\nflat glass and the optical axis was normal to the glass. Source:\nAuthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n\nFigure 22 Refraction model. Adapted from Glassner (1989) . . . . . . 52\n\nFigure 23 Dome window correcting the refraction. (NEWTON; BALD-\nWIN; FRYER, 1989) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n\nFigure 24 Non-SVP while looking through a flat interface. (TREIB-\nITZ; SCHECHNER; SINGH, 2008) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\nFigure 25 Multi-layer flat refractive geometry. (AGRAWAL et al.,\n2012) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\nFigure 26 Designed sensor. Source: Author?s own work. . . . . . . . . . . 60\n\nFigure 27 Inscribed pentagon side sized in a 300 mm diameter cir-\ncle. Source: Author?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n\nFigure 28 LTS 3D model optimization with in-air values. Source:\nAuthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n\nFigure 29 Angled wall water tank used in final evaluation. Source:\nAuthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\nFigure 30 Camera positioning.(a) The positioning ?U? shape.(b)\nThe camera fixed on the ?U? shapes. (c) The auxiliary fixing el-\nement positioning. (d) The camera mounted on the auxiliary ele-\nment. (e) The auxiliary positioning element with respect to the lid.\n(f) The camera mounted on the lid. Source: Author?s own work . . 64\n\nFigure 31 Laser positioning. Source: Author?s own work . . . . . . . . . 65\n\nFigure 32 Details of the back flanges. Source: Author?s own work 66\n\nFigure 33 Clamped and unclassified configurations.(HARRIS, 1999) 66\n\nFigure 34 Window resistance values. (HARRIS, 1999) . . . . . . . . . . . . . 67\n\nFigure 35 Sensor fixing during calibration. Source: Author?s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\nFigure 36 Back and superior fixing holes for auxiliary moving sys-\ntems. Source: Author?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n\nFigure 37 Hardware used to build the LTS. (MQ013MG-E2, 2017;\nHF12. . . , 2017; MINI. . . , 2017) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n\nFigure 38 Built LTS. (a) is the camera module and (b) is the laser\n\n\n\nmodule. Source: Author?s own work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\nFigure 39 Complete measuring bench: displacement system (a),\nhydraulic system (b) and mounting structure (c). Source: Author?s\nown work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\nFigure 40 The linear slide and the motor. Source: Author?s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\nFigure 41 Mounting structure details. Source: Author?s own work 72\n\nFigure 42 Measurement model. Source: Author?s own work . . . . . . 73\n\nFigure 43 Measurement procedure. Source: Author?s own work. . . 74\n\nFigure 44 Resulting image from the step standard used during win-\ndow distance calibration. Source: Author?s own work. . . . . . . . . . . . . 76\n\nFigure 45 3D point result for different window distances d. Source:\nAuthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n\nFigure 46 Calibration procedure. Source: Author?s own work. . . . . 77\n\nFigure 47 Evaluation objects: two fixed spheres, welding beads, a\nplane and a dummy head. Source: Author?s own work. . . . . . . . . . . . 80\n\nFigure 48 Calibration Standard. Source: Author?s own work . . . . . 82\n\nFigure 49 Step Standard. Source: Author?s own work . . . . . . . . . . . . 82\n\nFigure 50 Calibration layout used to adjust the laser and the cam-\nera, also showing the OCS. Source: Author?s own work . . . . . . . . . . . 83\n\nFigure 51 Calibration layout used to acquire the calibration images.\nFirst the table is aligned (a), then the LTS is aligned (b), both with\na spirit level. Source: Author?s own work. . . . . . . . . . . . . . . . . . . . . . . . . 84\n\nFigure 52 Acquisition layout. Source: Author?s own work . . . . . . . . 86\n\nFigure 53 First, the original acquired image, then the processing\nresult and the labels. Source: Author?s own work. . . . . . . . . . . . . . . . . 87\n\nFigure 54 Laser image processing to peak search. First, the original\nimage, then, the filter and the laser peak position (not shown with\nsubpixel resolution). Source: Author?s own work . . . . . . . . . . . . . . . . . 87\n\nFigure 55 Sphere acquisitions.(a-c) images shown in-air. (d-f) im-\nages underwater. Respectively close to the system, on the stand off\nand farther from the system. Source: Author?s own work . . . . . . . . . 88\n\nFigure 56 (a) (c) and (e) are in-air images. (b) (d) and (f) are un-\nderwater images. Respectively from the dummy head, the welding\nbeads and the PVC tube. Source: Author?s own work . . . . . . . . . . . . 90\n\nFigure 57 Sensor movement direction during acquisitions. Source:\nAuthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n\nFigure 58 Z(v) polynomials examples from one of the top, middle\n\n\n\nand bottom image lines, in-air and underwater. Source: Author?s\nown work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\nFigure 59 Y(Z) polynomials examples from one of the top, middle\nand bottom image lines, in-air and underwater. Source: Author?s\nown work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n\nFigure 60 Dummy head measurements in-air and underwater using\nthe polynomial adjustment. Source: Author?s own work . . . . . . . . . . 94\n\nFigure 61 Welding bead measurements in-air and underwater using\nthe polynomial adjustment. Source: Author?s own work . . . . . . . . . . 95\n\nFigure 62 300 mm diameter pipe measurements in-air and under-\nwater with the polynomial method. Source: Author?s own work . . 95\n\nFigure 63 Dummy head measurements in-air (a) and underwater\n(b) with the proposed method. Source: Author?s own work . . . . . . . 100\n\nFigure 64 Welding bead measurements in-air and underwater using\nthe proposed method. Source: Author?s own work . . . . . . . . . . . . . . . . 100\n\nFigure 65 300 mm diameter pipe measurements in-air (a) and un-\nderwater (b) with the proposed method. Source: Author?s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n\nFigure 66 Comparison of all the measurement results. Source: Au-\nthor?s own work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n\nFigure 67 Y direction measurement volume. Source: Author?s own\nwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n\n\n\nLIST OF TABLES\n\nTable 1 System parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n\nTable 2 Underwater sphere measurement errors for the polyno-\nmial adjustment. The worst measurement for each parameter is\nhighlighted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n\nTable 3 in-air and underwater plane measurement errors for the\npolynomial adjustment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n\nTable 4 Relation between the number of step standard images\nused during calibration and the refractive window distance . . . . . . . 97\n\nTable 5 Measurement errors considering one or two refractions\nduring spheres evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n\nTable 6 Underwater sphere measurement errors for the proposed\nmethod. The worst measurement for each parameter is highlighted 99\n\nTable 7 Underwater plane measurement errors for the proposed\nmethod.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\nTable 8 Spheres spacing percent error. . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n\nTable 9 Proposed method underwater uncertainty evaluation. . . . 104\n\n\n\n\n\nLIST OF ABBREVIATIONS\n\nAUV Autonomous Underwater Vehicle\n\nCCS Camera Coordinate System\n\nCMM Coordinate Measuring Machine\n\nDLT Direct Linear Transformation\n\nFOV Field of View\n\nLTS Laser Triangulation System\n\nOCS Object Coordinate System\n\nPC Personal Computer\n\nPE Probing Error\n\nPVC Polyvinyl chloride\n\nROV Remotely Operated System\n\nSLAM Simultaneous Localization and mapping\n\nSonar Sound Navigation and Ranging\n\nSSE Sphere Spacing Error\n\nSVP Single View Point\n\nToF Time of Flight\n\nUSB Universal Serial Bus\n\n\n\n\n\nLIST OF SYMBOLS\n\nX\n\u00af\n\n3D coordinate in OCS\n\nx\n\u00af\n\nImage coordinate\n\nC\n\u00af\n\nCamera pinhole center\n\nP\n\u00af\n\nProjection matrix\n\nK Intrinsic parameters matrix\n\nR Rotation matrix\n\nt Translation matrix\n\n?t Triangulation angle\n\nf Focal length\n\nx?, y? Undistorted image coordinates\n\nk1, k2 Radial distortion coefficients\n\n? Image plane\n\nB Baseline\n\n?t Triangulation angle\n\nsx, sy Pixel size\n\n? Light wavelength\n\n?s Scheimpflug condition angle\n\nk Lens magnification factor\n\n? Medium index of refraction\n\nvn Vector during refraction on layer n\n\nd Distance from the camera pinhole center point to the plane\nof refraction\n\nT Transmitted ray\n\nI Incidence ray\n\nN Refraction interface normal vector\n\n?T Transmitted angle\n\n?I Incidence angle\n\ntw Window thickness\n\nAw Unsupported aperture diameter\n\n?Pw Pressure differential\n\nSf Fracture strength\n\nN Refraction interface normal vector\n\n\n\n? Degrees of freedom\n\n\n\nCONTENTS\n\n1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n\n1.1 WORK PURPOSES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n1.2 WORK OVERVIEW . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n2 UNDERWATER MEASUREMENTS. . . . . . . . . . . . . . . . 29\n\n2.1 MAIN METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n2.1.1 Sonars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n2.1.2 LiDAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\n2.1.3 Photogrammetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n2.1.4 Laser Triangulation Systems . . . . . . . . . . . . . . . . . . . . . . . 41\n\n2.1.4.1 Laser Triangulation Principle . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n2.1.4.2 Laser Plane Fitting Calibration . . . . . . . . . . . . . . . . . . . . . . . 47\n\n2.1.4.3 Polynomial Regression Calibration . . . . . . . . . . . . . . . . . . . . 49\n\n2.2 INFLUENCES ON UNDERWATER IMAGES . . . . . . . . . . . . . 50\n\n2.3 UNDERWATER CAMERA CALIBRATION . . . . . . . . . . . . . . 55\n\n3 UNDERWATER DEVELOPED SENSOR . . . . . . . . . . . 59\n\n3.1 MECHANICAL DESIGN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n\n3.1.1 Design Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n\n3.1.2 System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\n3.1.3 Measuring Bench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n3.2 DEVELOPED ALGORITHMS . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n\n3.2.1 Measurement Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n\n3.2.2 Refraction Calibration Algorithm . . . . . . . . . . . . . . . . . . 76\n\n4 SYSTEM EVALUATION . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n4.1 EVALUATION OBJECTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n4.2 CALIBRATION STANDARDS . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n4.3 CALIBRATION LAYOUT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n\n4.4 ACQUISITION LAYOUT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n\n4.5 IMAGES PROCESSING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n\n4.6 ACQUIRED IMAGES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n\n4.7 POLYNOMIAL CALIBRATION . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\n4.7.1 Polynomial Adjustment Results . . . . . . . . . . . . . . . . . . . . 93\n\n4.8 PROPOSED CALIBRATION . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n\n4.8.1 Number of Underwater Images to Calibrate d . . . . . . 96\n\n4.8.2 Refraction between Glass and Air . . . . . . . . . . . . . . . . . . 97\n\n4.8.3 Proposed Method Results . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n\n4.8.4 Results Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n\n\n\n4.8.5 Uncertainty Evaluation for the Proposed Method\n\nUnderwater . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n4.8.6 Y Direction Measurement Length . . . . . . . . . . . . . . . . . . 105\n5 CONCLUSIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.1 FUTURE WORKS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\nReferences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n\n\n\n25\n\n1 INTRODUCTION\n\nUnderwater tridimensional measurement has many applications,\nin the oil and gas industry. The area of interest of the present work,\nfor example, are the applications on the underwater equipment control\nand maintenance procedure optimization. Other applications include\narcheology (DRAP et al., 2007) and biology(GIBSON; ATKINSON; GOR-\nDON, 2016) investigation.\n\nMultiple techniques can be used in these environments to ac-\nquire distances and geometrical information, such as Sound Navigation\nand Ranging (sonar) and Light Detection and Ranging (LiDAR). How-\never, the optical techniques are, still, the most suitable ones for close\nrange measuring, up to 10 m. Among the optical techniques, some\nthat can be adapted for underwater measurements are stereo vision, si-\nmultaneous localization and mapping (SLAM), and laser triangulation\nsensors (LTS). All of these techniques have advantages and disadvan-\ntages, mainly that acoustic techniques, such as the sonar, have the best\nperformance for long range measurements, greater than 10 m and the\noptical methods have the smallest uncertainties for close range mea-\nsurements.\n\nThe LTS is a very known technique in air (BESL, 1988; JI; LEU,\n1989a) and some underwater problems with the LTS have already been\ndiscussed in other works (CACCIA, 2006; CHANTLER; CLARK; UMA-\nSUTHAN, 1997), a few of these obstacles are light absorption, light\nscattering, and refraction.\n\nOvercoming these obstacles is a fundamental step to obtain reli-\nable 3D information with a LTS. Light absorption and scattering can be\ndiminished, for example, by selecting the right wavelength and the re-\nfraction can be considered in the measurement procedure. These steps\ncan take the underwater measurement uncertainties closer to the in-air\nuncertainties.\n\nThis work, then, proposed a LTS design and a method for its\nmeasuring and calibrating procedures. The LTS was built in order to\nachieve some objectives, such as measuring a 300 mm diameter pipe\nwith an uncertainty of a tenth of a millimeter as well as maintaining a\nsafe distance from the measured subject.\n\nAlong with the built sensor, a method for calibration and mea-\nsuring was proposed and evaluated. This method is based on the esti-\nmation of the refractive window distance from the camera center. This\ndistance is ray traced from the camera to the laser plane and creates\n\n\n\n26\n\nthe point cloud. The method is compared with a reference calibration\nto evaluate its effectiveness.\n\n1.1 WORK PURPOSES\n\nThe main objective of this work is to design, build and validate a\nlight sheet laser triangulation system for underwater environment oper-\nation up to 5 m deep. This sensor must consider the additional effects\ndue to the refraction and it must be able to acquire quality underwater\nimages, resulting in a reliable three-dimensional point cloud.\n\nThe specifics objectives of this work are:\n\n? Develop calibration and measurement algorithms specific to un-\nderwater measurements with laser triangulation systems, taking\ninto account the refraction effect.\n\n? Flexible, compact and modular mechanical design for underwater\napplications. The systems design must consider the attachment to\nmeasurement robots, linear guides, ROVs or underwater robots.\n\n? Extract quantitative parameters from scenes and objects of in-\nterest, through three-dimensional measurements and be able to\nconsider the system movement to allow point cloud concatena-\ntion.\n\n? Evaluate the developed system errors.\n\n1.2 WORK OVERVIEW\n\nThis work is divided into five main chapters: introduction, un-\nderwater measurements, underwater developed sensor, system evalua-\ntion and conclusion.\n\nThe introduction presents the motivation to build an underwater\nLTS, the objectives to achieve the desired sensor and the work overview.\n\nUnderwater measurements presents the main methods to achieve\nunderwater digitalization and the challenges in underwater environ-\nments. The main methods covered are sonar, LiDAR and photogram-\nmetry techniques. A more detailed description is given to the laser\ntriangulation method and the main obstacles are focused in underwa-\nter images.\n\n\n\n27\n\nUnderwater developed sensor presents the designed and built\nLTS and its algorithms. All the calculations and simulations to achieve\nthe chosen sensor characteristics are described. Followed by the descrip-\ntion of the algorithms applied to the calibration and measurement.\n\nSystem evaluation presents the evaluation to estimate the sen-\nsor errors with the proposed method and a reference polynomial ad-\njustment method, both underwater and in air. Multiple objects were\nmeasured to evaluate the capabilities of the sensor.\n\nIn the conclusion, the methods are compared and some obser-\nvations are made along with proposed future works for the designed\nsensor.\n\n\n\n28\n\n\n\n29\n\n2 UNDERWATER MEASUREMENTS\n\nThree dimensional measurements of underwater environments\nare useful on many applications. The oil and gas industry utilizes\na lot of equipment for oil extraction that need constant maintenance\n(DEY; OGUNLANA; NAKSUKSAKUL, 2004), biologist have great interest\nin investigating the underwater life abundance (GIBSON; ATKINSON;\nGORDON, 2016) and archaeological sites provide historical knowledge\n(DRAP et al., 2007). Usually light and sound are used as means to\ngather the measurement information. However, it is not always an\neasy task to obtain reliable underwater point clouds, because the light\nscatters, is absorbed and refracts and the sound cannot always provide\na good resolution. These obstacles must be considered to reach optimal\nmeasurement results.\n\nThere are a few methods used nowadays in a varying number of\napplications. A brief review of the main methods will be made showing\ntheir results, advantages and disadvantages. After this review, the laser\ntriangulation method, the focus of this work, will be explained more\ndeeply.\n\n2.1 MAIN METHODS\n\nUnderwater 3D reconstruction methods can be divided in time of\nflight (ToF) techniques and triangulation techniques (MASSOT-CAMPOS;\nOLIVER-CODINA, 2015). The ToF methods are, for example, sonar and\nLiDAR and some of the triangulation methods are stereo vision and\nlaser triangulation.\n\n2.1.1 Sonars\n\nSonar is a widely used ToF technique in maritime vessels to\nobtain depths and sea floor information. They are also used in SLAM\napplications, associated with remotely operated vehicles(ROVs) (RIBAS\net al., 2006). Nonetheless, they can also be used in three-dimensional\nmeasurements being, sometimes, called acoustic chambers or acoustic\ncameras (TAO et al., 2003; ROSENBLUM; KAMGAR-PARSI, 1992).\n\nSonar systems create a pulse of sound and wait for its reflection.\nThe time of reflection is then correlated with the object distance. Since\n\n\n\n30\n\nsound propagates well underwater, they can reach distances as far as\nkilometers and can operate with little to zero visibility conditions. How-\never, they also have higher uncertainties in close range measurements,\nwhen compared with other digitalization method, such as optical ones\n(MASSOT-CAMPOS; OLIVER-CODINA, 2015).\n\nBecause the measurement is dependent on the sound wave prop-\nagation speed, this is a source of uncertainties. The propagation speed\nunderwater is determined by the medium pressure, which changes with\ndepth and density, which changes with temperature and salinity. (DI-\nVISION; DIVISION; ECKART, 1946)\n\nThe simplest sonar is the single forward looking beam. After\nthe signal generation, the beam is shaped within the desired angle and\ndirection. Then, the resulting image has information of azimuth and\ndistance, but the elevation cannot be acquired with a single acquisition.\n\nHowever, aside from using a single beam (GUO, 2013; HORNER et\nal., 2009) a sonar can also use multiple beams (PATHAK; BIRK; VASKE-\nVICIUS, 2010) or side beams (COIRAS; PETILLOT; LANE, 2007; ROSEN-\nBLUM; KAMGAR-PARSI, 1992) to gather data. To create a point cloud,\nthe single beam must be moved with a known distance and orientation,\nbut the multiple beams can create a 3D reconstruction with a single\nacquisition. The side beam sonar uses a beam in the form of cone\nthat can later be processed to acquire a three dimensional point cloud\n(COIRAS; PETILLOT; LANE, 2007).\n\nThe acquisition result of a single beam sonar is shown in Figure\n1 a. The elevation is lost during acquisition, the distance is the Y\naxis and the azimuth is the angle. A side scan sonar image is shown\nin Figure 1 b. The image is equivalent to a photo, because the lost\ninformation is the depth.\n\nSonar with low frequency can survey wide areas of the ocean\nfloor, but can only distinguish features larger than tens of meters. On\nthe other hand, the resolution is proportional to the frequency and side\nscan sonar can produce dozens of millimeters of resolution, in exchange\nfor a loss in range.\n\nThe acoustic lens are a set of highly directional acoustic trans-\nducers attached to a shell forming a retina, shown in Figure 2. . This\nway, the delay has the range information and the transducer position\nestimation that provides the azimuth and elevation, allowing a full 3D\nreconstruction.\n\nOne example of a commercial 3D sonar is the Echoscope in Fig-\nure 3, an acoustic chamber ultrasound 3D inspection system with a\nresolution of 30 mm and a maximum reach of 120 m.\n\n\n\n31\n\n(a) Single beam sonar image (b) Side scan sonar image\n\nFigure 1: Images from different sonar types. (HORNER et al., 2009;\nCOIRAS; PETILLOT; LANE, 2007)\n\nFigure 2: Acoustic lens schematic. (ROSENBLUM; KAMGAR-PARSI,\n1992)\n\nAnother example of sonar is the BV5000 in Figure 4 manufac-\ntured by BlueView. It also makes 3D measurements and enables inte-\ngration with laser triangulation systems. It has a resolution of 10 mm\nand a maximum reach of 10 m.\n\n2.1.2 LiDAR\n\nLight detecting and ranging (LiDAR) is a ToF technique, such\nas the sonar, but it uses light signals. Some in-air systems can reach\nresolutions of 0.8 mm in a 10 m range (SCAN. . . , 2017). ). Consider-\n\n\n\n32\n\nFigure 3: Echoscope equipment and measurement result. (CODAOCTO-\nPUS. . . , 2017; CRISP. . . , 2017)\n\nFigure 4: BlueView equipment and measurement result.(BLUVIEW. . . ,\n2017; BLUEVIEW. . . , 2017)\n\n\n\n33\n\nFigure 5: Scopos Investigator equipment, measurement result and sub-\nject. (SCOPOS. . . , 2017; SUBSEA. . . , 2017)\n\ning underwater applications, most LiDAR systems are used as airborne\nscanners being able to recover both the surface and the sea bottom\nresulting in a high range reconstruction (MASSOT-CAMPOS; OLIVER-\nCODINA, 2015). LiDAR systems are also used in underwater ranging\napplications, where only the distance is obtained (WALDRON; MULLEN,\n2009). However, one commercial system built for underwater measure-\nments is the Scopos Investigaton 5, which has two ranges: 250 m and\n1500 m and the resolutions are not given by the manufacturer.\n\nAnother LiDAR technique is the Streak Tube Imaging LiDAR\n(STIL), which is very similar to the single-beam sonar technique (MCLEAN,\n1999). A cylindrical lens is used to form a fan beam and a photocathode\nwith a slit and a sweep plate is used to resolve the range and azimuth\ndata. The process is repeated as the aircraft moves forward and a full\n3D image is obtained as shown in Figure 6. The sensor can reach a 10\nm range with 2.6 mm resolution.\n\n2.1.3 Photogrammetry\n\nIn the photogrammetry techniques, images are used to create\na 3D point cloud. A camera or set of cameras and passive or ac-\ntive techniques are used to obtain the three-dimensional points. While\n\n\n\n34\n\nFigure 6: STIL technique. (MCLEAN, 1999)\n\nunderwater, the light is highly attenuated, meaning that the range\nof measurement for these kinds of sensors is much smaller than the\nsonar. However, much smaller uncertainties can be achieved using these\ntechniques (MASSOT-CAMPOS; OLIVER-CODINA, 2015). Stereo vision,\nstructure from motion and laser triangulation are some of the most\ncommonly used photogrammetry techniques. Since the main focus of\nthis work is the laser triangulation technique, it will be presented in a\nseparate section.\n\nCamera calibration is an important component for various pho-\ntogrammetry techniques. The main camera calibration method consid-\ners a pinhole model, in which all rays pass through a small hole in a\nclosed box. These rays form, on the opposite side of the box an upside\ndown image, as shown in Figure 7. (HARTLEY; ZISSERMAN, 2004)\n\nFigure 7: Pinhole camera schematic. (PINHOLE. . . , 2017)\n\nTo simplify this model the image plane is considered between the\n\n\n\n35\n\npinhole center and the object. So, considering Figure 8, a point X in\n3D object coordinate system (OCS) being observed by a camera with a\nprojection center or pupil C and an image plane. The X projection on\nthe image plane will be x, which is the result of the crossing between\nthe line from X through the pinhole center C and the image plane.\n\nC\n\ny\n\nx\n\nz\n\nCamera Axis\n\nImage Plane\n\nf\n\nX\n\nY\n\nZ\n\nOCS\n\nX\n\nx\n\nFigure 8: Pinhole mathematical model. Adapted from Hartley and\nZisserman (2004).\n\nIn this model, the calibration parameter is the projection ma-\ntrix, the matrix that projects the 3D point X in OCS to its respective\nprojected camera pixel x. The projection matrix P is a 3x4 matrix\nthat contains both the extrinsic and intrinsic parameters of the cam-\nera. The extrinsic parameters are the rotation and translation matrices\n[R t] that transform the 3D coordinates from the OCS to the CCS.\nThe intrinsic parameters are the focal point f and the principal sensor\npoint, coordinates x0 and y0, which allows the CCS shift to the center\nof the imaging sensor.(HARTLEY; ZISSERMAN, 2004)\n\nx = PX (2.1)\n\nWhere P = K[R t], K is a 3 \u00d7 3 matrix and [R t] is a 3 \u00d7 4\nmatrix that contains the rotation, R in a 3\u00d73 matrix and the translation\nin a 3 \u00d7 1 matrix.\n\nMatrix K contains the intrinsic parameters and is as follows:\n\nK =\n\n?\n\n?\n\nfx 0 x0\n0 fy y0\n0 0 1\n\n?\n\n?\n\n\n\n36\n\nWhere fx and fy are the camera?focal length and x0 and y0 are\nthe camera?s principal point in CCS.\n\nThere are many ways to obtain the camera projection matrix,\ntwo main methods will be presented. Both methods need a set of 3D\npoints and its image correspondence. The problem with finding the\nprojection matrix with these conditions is called the resection problem.\nThe first approach is the direct linear transformation (DLT) method\nand the other uses plane homographies. After the applied method, some\noptimization techniques are applied, such as the Levenberg-Marquardt\nalgorithm (LOURAKIS, 2005).\n\nThe DLT approach formulates a homogeneous, usually overde-\ntermined, linear system of equations and solves this by finding an ap-\nproximate null space. Let pi, i = 1, 2, 3 be the 4 \u00d7 1 vectors containing\nthe rows of projection matrix P , that is\n\nP =\n\n?\n\n?\n\npT\n1\n\npT\n2\n\npT\n3\n\n?\n\n?\n\nAnd PX can be written as\n\nPX =\n\n?\n\n?\n\npT\n1\nX\n\npT\n2\nX\n\npT\n3\nX\n\n?\n\n?\n\nSince x = PX the cross product x \u00d7 PX is zero. Considering\nx = (x, y, 1), since x is in homogeneous coordinates, this cross product\ncan be written explicitly as\n\nx \u00d7 PX =\n\n?\n\n?\n\ny pT\n3\nX ? pT\n\n2\nX\n\npT\n1\nX ? x pT\n\n3\nX\n\nx pT\n2\nX ? y pT\n\n1\nX\n\n?\n\n?\n\nThen, since pTi X = X\nT pi and isolating the variable pi\n\n?\n\n?\n\n0 ?X y X\nX 0 ?x X\ny X x X 0\n\n?\n\n?\n\n?\n\n?\n\np1\np2\np3\n\n?\n\n? = 0\n\nWhere each 0 represents a 1\u00d74 null matrix, resulting in a linear\nsystem Ax = 0. In addition, because the third row can be written as a\ncombination of the first two rows, matrix A is singular, thus the linear\nequations to be solved are\n\n\n\n37\n\n[\n\n0 ?X y X\nX 0 ?x X\n\n]\n\n?\n\n?\n\np1\np2\np3\n\n?\n\n? = 0\n\nBecause the matrix can be scaled, there are 8 variables and since\neach point leads to two equations, a 4 point set leads to an exact so-\nlution. However, since the system is affected by image noise, more\npoints leading to an overdetermined system is a more viable solution.\nOne way to find the A matrix null space approximation is through the\neigenvector with the smallest eigenvector of AT A. But, this is a poorly\nconditioned matrix since the third column will be orders of magnitude\nhigher than the other columns, considering that the image coordinates\ncan be as large as thousands of pixels. This can be solved normalizing\nthe data before matrix A is built. The image points can be normalized\nby translating every point so the centroid of the points is the system\ncoordinate center and scaled so the average points distance to the cen-\nter is\n\n?\n2 resulting in average point of (1,1). The 3D points can be\n\nnormalized the same way but with an origin center distance of\n?\n3 so\n\nthe average point is (1,1,1). This is a suitable normalization for most\nconfigurations. (HARTLEY; ZISSERMAN, 2004)\n\nIt follows that, the full projection matrix is obtained. Neverthe-\nless, this method requires three dimensional standards, because the use\nof planar standard results in one coordinate always being null, which\nleads to a singular A matrix.\n\nA second common way to calibrate a camera is through plane\nhomographs (ZHANG, 1999). This method does not require a three di-\nmensional standard, just a set of planar standard images with different\nrotations and translations is enough to provide the system calibration\nparameters. This method considers the Z coordinate to be null, thus,\n\ns\n\n?\n\n?\n\nx\ny\n1\n\n?\n\n? = K\n[\n\nr1 r2 r3 t\n]\n\n?\n\n?\n\n?\n\n?\n\nX\nY\n0\n1\n\n?\n\n?\n\n?\n\n?\n\n= K\n[\n\nr1 r2 t\n]\n\n?\n\n?\n\nX\nY\n1\n\n?\n\n?\n\nBeing s a scalar factor. Considering a homography H = [h1h2h3]\nis related to the projection matrix by\n\n[\n\nh1 h2 h3\n]\n\n= ?K\n[\n\nr1 r2 t\n]\n\nWhere ? is an arbitrary scalar. Since r1 and r2 are orthonormal\n\n\n\n38\n\nhT\n1\nK?T K?1h2 = 0 (2.2)\n\nhT\n1\nK?T K?1h1 = h\n\nT\n2\nK?T K?1h2 (2.3)\n\nConsidering B = A?T A?1 and the vector b = [B11, B12, B22, B23, B33]\nT\n\nAfter some equations adjustments and considering\n\nvij = [hi1hj1, hi1hj2+hi2hj1, hi2hj2, hi3hj1+hi1hj3, hi3hj2+hi2hj3, hi3hj3]\nT\n\n(2.4)\nThe equation\n\n[\n\nvT\n2\n\n(v11 ? v22)T\n]\n\nb = 0\n\nResults in a likewise Ax = 0 equation system\n\nVb = 0 (2.5)\n\nFrom the projection matrix, the intrinsic and extrinsic parame-\nters can be separated. The translation is the fourth projection matrix\ncolumn. The intrinsic parameters and the rotation are obtained from\nthe three remaining columns. This is done by RQ-factorization, which\nseparates one matrix into one right triangular and another as orthog-\nonal, being the camera matrix as right triangular and the rotation as\northogonal. After this, the matrices must be scaled so that the third\nline, third element of the intrinsic parameters is one.\n\nBesides the pinhole model, the lens distortions can also be es-\ntimated during calibration (BROWN, 1971). The lens distortions can\nbe divided in radial and tangential distortions. In today\u2019s commercial\nlenses, the radial distortion is the most influential during image forma-\ntion and usually just two coefficients are enough to correct the distorted\nimage. (ZHANG, 1999). The optimization, then, has to find the optimal\nvalues for the equations\n\nx? = x + x[k1(x\n2 + y2) + k2(x\n\n2 + y2)2] (2.6)\n\ny? = y + y[k1(x\n2 + y2) + k2(x\n\n2 + y2)2] (2.7)\n\nWhere x? and y? and the normalized corrected pixel coordinates\nand k1 and k2 are the distortion coefficients.\n\nAfter this estimation, the alternation between estimation of the\n\n\n\n39\n\nother parameters and the distortion coefficients are usually capable of\nevaluating the initial guesses to initialize the non-linear optimization.\nAnd finally, the calibration parameters are estimated to minimize the\nequations of both the projection matrix and the distortion coefficients.\n\nThe stereo vision techniques use two or more fixed cameras, as\nshown in Figure 9, to capture the scene and construct the point cloud.\nThrough camera calibration, the position of one camera in respect to\nthe others is solved and triangulation is possible using homologous\npoints.\n\nFigure 9: Stereo system. (CHOOSING. . . , 2017)\n\nThe homologous points are image points that represent the same\n3D point in each different captured image of the scene, in Figure 10\nthe homologous points are x and x?, respectively on the image planes\n? and ??. These image points are projected into lines from the image\ncenters C and C? by applying the camera projection matrix. The cross\nbetween these lines is the measured 3D point X. In practice, the lines\ndo not cross due to image noise, so an approximation is made. The\nprocedure is then repeated until a full point cloud is constructed.\n\nIn photogrammetry techniques, the homologous points used in\ntriangulation can be obtained using passive or active techniques. The\nactive methods use light with some kind of information projected onto\nthe surface of interest while the passive illumination techniques do not\nproject any additional information to the scene.\n\nThe fringe projection technique is an example of active pho-\ntogrametry. The projected image has a sinusoidal pattern that assign\na phase value for each image line or column. The use of calibrated cam-\neras allows the epipolar geometry to be used. The epipolar geometry\nis built uniting the two pinhole centers generating the triangle shown\nin Figure 10. Then, the same phase is searched on the other images\nalong the l line. The process is repeated for each image pixel and the\n\n\n\n40\n\n???\n\nCC?\n\nX\n\nxx?\nl\n\nFigure 10: Homologous points and epipolar geometry in stereo vision.\nSource: Author\u2019s own work\n\npoint cloud is built. In Figure 11 a commercial system that uses fringe\nprojection is presented. (HARTLEY; ZISSERMAN, 2004)\n\nFigure 11: Commercial active stereo systems. Source: Author\u2019s own\nwork\n\nThe passive illumination methods do not project any additional\ninformation on the surface of interest, for example the structure from\nmotion technique (JORDT, 2014). In this technique, a single camera is\nmoved whilst observing the surface of interest. After some image pro-\ncessing, the homologous points are established along with the camera\nposition in each acquired image and the 3D mesh is constructed. These\nsystems can easily be implemented in underwater environments, since\nall it needs is a moving camera that can be mounted, for example, on\na ROV. All the image processing can be done after a video recording.\n\nStereo vision techniques are present in underwater measurements\n(BIANCO et al., 2013; BRUNO et al., 2011; BUSCHINELLI et al., 2016). The\nunderwater environmental influences on images are further explored in\nthe next sections.\n\n\n\n41\n\n2.1.4 Laser Triangulation Systems\n\nA laser triangulation sensor (LTS) uses a laser projector and a\ncamera to obtain required information, being, therefore, an active op-\ntical technique. Such information can be, for example, displacements,\ndistances and three-dimensional points. The main principle behind LTS\nis the geometrical triangulation principle.\n\nOne example of commercial laser triangulation measurement sen-\nsor is the profiler scanCONTROL 2600-50 manufactured by Micro-\nEpsilon, presented in Figure 12, which presents a resolution of 4 ?m\nand a measurement range of 60 mm.\n\nFigure 12: Accuity laser. (COMPACT. . . , 2017)\n\nLaser triangulation is one of the most used 3D underwater mea-\nsurement methods. They have one of the lowest uncertainties among\nthe presented methods and are used for short range measurements (up\nto 10 m), mainly due to light signal degradation.(MASSOT-CAMPOS;\nOLIVER-CODINA, 2015)\n\nThe company 2G Robotics has a LTS for underwater uses, shown\nin Figure 13, but the uncertainty is unspecified. The range goes from\n0.13 m to 10 m and the resolution goes from 0.03 mm up to 0.31 mm,\nrespectively.\n\n2.1.4.1 Laser Triangulation Principle\n\nThe LTS uses some form of laser projection fixed with a certain\ndistance and angle from a camera (JI; LEU, 1989b; GAN; TANG, 2011).\nThe laser is, then, projected on a surface of interest and suffers some\nlevel of diffuse reflection. Some of the light rays from the surface pass\nthrough the focal point of the lens and the result of the intersection\n\n\n\n42\n\nFigure 13: 2G Robotics underwater laser triangulation sensor and mea-\nsurement example. (ULS-500. . . , 2017; OFFSHORE. . . , 2017)\n\nbetween the laser and the surface is captured by an image sensor. In\nthe captured image, the detected laser peak pixel can be associated\nwith the desired dimension.\n\nThe simplest LTS is composed of a laser dot projector and a\ncamera observing it with an angled view. As the camera observes an\nobject getting closer to the laser, the laser image on the camera also\nmoves along the image columns or lines. After proper calibration, the\nassociation between the distance from the laser and the image pixel\nposition containing the laser peak can be done, resulting in distance\nmeasurement.\n\nSystem characteristics such as, the measurement volume and the\nsystem resolution are defined by some parameters. The main system\nparameters, shown in Figure 14, are: the baseline B, the triangulation\nangle ?t, the pinhole center point C given by the lens focal length f,\nthe pixel size and the sensor resolution. One very important initial\nobservation is that the relation between system resolution and mea-\nsurement volume is inversely proportional considering the same sensor\nbeing used.\n\nThe baseline is the distance between the pinhole center and the\nlaser projector. The triangulation angle is the angle between the camera\naxis and the laser optical axis. The pixel size and the resolution are\ncharacteristics of the sensor, and the pinhole center point depends on\nthe selected lens. A relation between these characteristics and the\nsystem resolution can be derived, considering the sensor pixel size sx\nand u the number of pixel from the image center and the CCS presented\nin 14. By triangle similarity (GAN; TANG, 2011)\n\n\n\n43\n\nLaser\nImage Plane\n\nC\n\nf\n\n?t\n\nB\n\nX\n\nx\n\nZ\n\nX\n\nFigure 14: Laser triangulation model. Adapted from Gan and Tang\n(2011)\n\nZ =\nBf\n\nf tan ?t ? sxu\n(2.8)\n\nAfter the Eq. 2.8 derivative, the sensor resolution can be cal-\nculated, i.e. how the distance and pixel variation displacement are\nrelated. The derivative is (GAN; TANG, 2011)\n\n?Z =\nBfsx\n\n(f tan ?t ? sxu)2\n?u (2.9)\n\nThis equation can be applied to estimate the sensor resolution\nalong the Z coordinate. Since sx is very small compared to f the factor\ninverse of (f tan ?t)\n\n2 is the most influential. The same procedure can\nbe made in the Y direction, along the laser line in the captured image.\nUsing the same similarity principle, Y is\n\nY =\nB cos ?tfsyv\n\nf tan ?t ? sxu\n(2.10)\n\nWhich has a derivative\n\n?Y =\nvB cos ?tsysx\n\n(f tan ?t ? sxB cos ?t)2\n?u +\n\nsyB cos ?t\nf tan ?t ? sxu\n\n?v (2.11)\n\nThe defined object coordinate system is arbitrary and in this\n\n\n\n44\n\ncase the X coordinate is always null.\nThe laser peak detection has a great influence on the measure-\n\nment uncertainty. There are a few approaches to reach a sub pixel\ndetection level. The center of mass, the zero crossing and the curve\nadjustment are some examples. There are some comparisons between\nmethods with some advantage for the zero crossing method (NAIDU;\nFISHER, 1991).\n\nThe theoretical uncertainty limit is defined by the laser speckle\n(DORSCH; HA?USLER; HERRMANN, 1994). The speckle occurs when co-\nherent light is projected onto a diffuse surface. The coherence enables\nthe constructive and destructive interference between the various laser\nreflections on the surface and a granular pattern is formed. The smaller\nthe size of the speckle grain, the smaller the uncertainty limit, because\nthe peak detection can be more exact. The speckle grain size charac-\nteristics depends on the wave length, the lens aperture and the distance\nbetween the camera and the measured surface. One way to make the\ngrain smaller is to open the camera aperture. However, this also makes\nthe depth of focus smaller, which makes the laser peak detection more\ninaccurate. Therefore, there is a compromised relation between the\nspeckle grain size and the depth of focus length. In Figure 15 the rela-\ntion between the detectable peak difference ?xcog, the light wavelength\n? and the system angle of aperture u is shown.\n\nFigure 15: Uncertainty limit for a LTS.(DORSCH; HA?USLER; HER-\nRMANN, 1994)\n\nThe depth of focus influence can be minimized by applying the\nScheimpflug principle which is widely used in LTS, with some consider-\n\n\n\n45\n\nLaser\n\nImageP lane\n\n?t\n\nLaser\n\nImageP lane\n\n?t\n\n?s\n\nFigure 16: Scheimpflug configuration in a LTS. Adapted from Gan and\nTang (2011)\n\nations (MIKS; NOVAK; NOVAK, 2013). The Scheimpflug condition is met\nwhen the angle between the image plane and the lens axis, as shown in\nFigure 16, ?s is\n\ntan ?t = k tan ?s (2.12)\n\nWhere k is the magnification factor. This is also called tilt and\nshift in photography. When the angle ?s, changes, the plane of focus\nalso changes. Then, it is possible to always maintain the laser plane in\nfocus, therefore, optimizing the measurements for all the measurement\nvolume, since the portion of the image with the laser line will always\nbe in focus. The sensor tilt enables a wider lens aperture, decreasing\nthe speckle influence on the measurement.\n\nInstead of projecting a dot, a line or multiple lines can be pro-\njected on the surface of interest. The line can capture more 3D points\nfrom the scene within a single image acquisition, since each laser peak\nfor each image line or column is a 3D point. The multiple line projec-\ntion has the advantage of measuring the sensor orientation with respect\nto the object with only one acquisition, since a plane can be adjusted\nwith the captured information, but it requires multiple lasers or special\noptics.\n\nA fundamental procedure to create a dense point cloud with a\nLTS is to move the sensor, because in one acquisition, only a small part\nof the object of interest is measured, only where the laser is hitting the\n\n\n\n46\n\nobject. Therefore, the sensor must be mounted on equipment with\nknown displacement, which can be a moving table, a robot or, in un-\nderwater cases, a ROV or autonomous underwater vehicle (AUV). This\nmounting requires another calibration to locate the sensor coordinate\nsystem in respect to the moving equipment coordinate system. One\nexample of this calibration is to use an auxiliary object as a reference\ncoordinate system (SANTOLARIA et al., 2009). This object has its coor-\ndinate system defined both by the LTS and an auxiliary machine, such\nas a coordinate measuring machine (CMM) Hence, the transformation\nbetween the LTS and the coupling between the sensor and the moving\nequipment coordinate system coordinate systems are possible.\n\nAnother important system characteristic is the stand-off, the op-\ntimal distance from the system to perform the measurement. It is the\ndistance between the camera and the center of the measurement vol-\nume, as shown in Figure 17. The system parameters, such as lens and\nlaser focus are adjusted for this distance. Accordingly, this is the best\narea in the measurement volume to acquire images.\n\nLaser\n\nImage Plane\n\nC\n\nStandoff\nMeasurement\n\nRange\n\nFigure 17: The standoff of a LTS is in the middle of the measurement\nrange. Source: Author?s own work\n\nSome limitations in regards to the surface being measured and\nsurface discontinuities appear during measurements with a LTS. Since\nthe laser must be observed by the camera from an angled view, the\nsurface must have some level of diffuse reflection, so the laser can be\nseen in the image. When the surface has low diffuse reflection, the\nlaser power can be raised to get more light into the sensor. Surface\ndiscontinuities may cause occlusions during measurement, this occurs\n\n\n\n47\n\nwhen the own surface appears as an obstacle between the camera and\nthe laser, as shown in Figure18. Therefore, when occlusions occur, the\nlaser peak cannot be detected. Occlusions can be avoided by measuring\nthe surface in different orientations.\n\nLaser\nImageP lane\n\nC\n\nX\n\nx\n\nDiscontinuity\n\nFigure 18: Occlusion during measurement. Source: Author?s own work\n\nThe calibration of laser sheet sensors allows the acquired im-\nage to be transformed into three-dimensional points. Two calibration\nmethods will be used in this work, both considering one laser line being\nprojected. The first method considers a pinhole camera model and a\nmathematical plane (SANTOLARIA et al., 2009, 2011), in which it is nec-\nessary to calibrate both the camera and the laser plane. This method\nwill be adapted to consider a refraction during measurement. The sec-\nond method is a model-less method, with polynomials fitted for each\nline, relating the laser peak and the 3D point, this method will be used\nas a reference method.\n\n2.1.4.2 Laser Plane Fitting Calibration\n\nThis calibration needs a calibrated camera, i.e. the intrinsic and\nextrinsic parameters must be estimated in a previous step. After the\ncamera calibration, the laser plane fitting calibration is performed. It is\nbased on approximating the laser plane as a mathematical plane with\nthe plane coefficients according to the plane equation: Ax+By +Cz +\nD = 0. The measurement is based on projecting the detected laser\npeak, by applying the camera calibration parameters, and crossing this\n\n\n\n48\n\nprojection with the mathematical plane, as shown in Figure 19.\n\nFigure 19: Laser plane fitting measurement. (SANTOLARIA et al., 2009)\n\nTo obtain the laser plane coefficients, a set of 3D points on the\nlaser plane in various locations in the measurement volume is used to\nadjust a best fitted plane.\n\nThe plane point cloud can be obtained with a simple procedure,\nconsidering the OCS with X direction normal to the laser plane,the Z\ndirection parallel to the laser projector axis and the origin at the mea-\nsurement volume center. First, a plane with a known Z OCS coordinate\non the XY plane containing the laser line is captured by the camera.\nAfter the image distortion correction, for each image line, the laser peak\nis detected and for each detected peak, a line is back projected to the\nOCS using the projection matrix. The projection provides two equa-\ntions for a three variable system, which can be solved by the known\nZ coordinate. The process is then repeated for several Z coordinates\nuntil a laser plane point cloud is built and the mathematical plane is\nadjusted. The process can be improved by using the same image to cal-\nibrate the laser and the camera. Thus, ensuring that the plane images\nused are contained on the XY plane in OCS, since the OCS is defined\nduring the camera calibration. Therefore, the plane movement during\nplane calibration is only in the Z direction.\n\nSome sources of uncertainties arise during this kind of calibra-\ntion. The moved plane flatness influences the measured points to con-\nstruct the laser plane, because all the laser peaks are considered to be\non the same Z coordinate. The plane displacement uncertainty leads to\na different Z coordinate used in calibration. Also, since the laser plane\n\n\n\n49\n\ncalibration and the measurement procedure use the projection matrix,\nthe camera calibration errors have a direct influence on uncertainty\nmeasurement. Finally, the measurement quality is influenced by the\nlaser light sheet quality, i.e. how ?flat?is the projected laser plane, this\nerror can be minimized changing the adjusted surface to an equation\nthat better represents the projected surface.\n\n2.1.4.3 Polynomial Regression Calibration\n\nThis method correlates the laser peak and the 3D point through\npolynomial equations and is based on Trucco, Fisher and Fitzgibbon\n(1994). This is a ?black-box? approach, therefore the errors from cam-\nera and laser calibration, presented in the previous calibration, are\nabsorbed into the adjusted equations. In this calibration, the ZY plane\nin OCS is considered to be on the laser plane, so the Y coordinate is\naligned with the observed laser line and the X coordinate is always null.\nIt is considered that the laser line captured on the image is along the\nimage columns, therefore the peaks are established for each line.\n\nA set of known 3D points and the images column with the laser\npeak are the variables and two polynomial equations are fitted for each\nimage line.\n\nThe first polynomial equation correlates the image column pixel\nto the laser peak to the Z coordinate Pu(v) = Z and the second\npolynomial equation correlates the Z coordinate to the Y coordinate\nPu(Z) = Y . Polynomial equations must be adjusted since the sensitiv-\nity changes throughout the measurement volume, so the non-linearity\nis compensated. During the measurement, after the laser peak column\nis obtained, the Z coordinate is calculated with the first equation, then\nthe Z is applied to the second equation. After that, the process is\nrepeated for each image line.\n\nThe second equation is needed because of the camera amplifica-\ntion factor. Since the number of image lines is always the same, the\nY variation for each line is bigger when the object is farther from the\nlaser, so the Z distance for each line influences the Y coordinate.\n\nThe utilised 3D points to adjust the equations are the main\nsource of uncertainties in this kind of calibration. If a moving plane is\nused to obtain the 3D points the plane flatness affects the considered\n3D point errors, thus affecting the measurement uncertainty.\n\n\n\n50\n\n2.2 INFLUENCES ON UNDERWATER IMAGES\n\nUnderwater images taken with a camera inside housing are dif-\nferent than in air images, as the objects appear to be closer than they\nare and the observable distance is much lower. Light absorption, light\nscattering and refraction are the three main image quality and distor-\ntion influences when taking underwater images. They are dependent on\nmany factors, some of which are, water salt concentration, particles in\nsuspension and travelling light wavelength, thus all these factors have\nsome kind of influence on the measurement result.\n\nLight absorption occurs when the photons are absorbed while\ninteracting with different particles in suspension in the water (WOZ-\nNIAK; DERA, 2007). Increasing the light source power can help reach a\nlonger distance when this has a large influence. As shown in Figure 20,\nthe light wave-length with the least underwater absorption is around\n450 nm to 550 nm, the green and blue range. This is the reason why\nthe ocean color is blue, because the most absorbed range is on the red\nspectrum.\n\nFigure 20: Light absorption in different oceans. 1 is in the Central Pa-\ncific; a very clear, deep-sea water, 6 is in the Atlantic ocean, (WOZNIAK;\nDERA, 2007)\n\n\n\n51\n\nLight scatter is divided in backscatter and forward scatter (SCHECH-\nNER; KARPEL, 2005). They both occur when the light is deflected when\ncolliding with particles in water. The deflection can occur guiding the\nlight back to the receptor, backscattering, or to the object of interest,\nforward scattering. They have a higher influence on light propagation\nthe more turbid the water is. When the light is backscattering, the\nincrease in light power can decrease the image quality, since more light\ncan come from scattering on the water itself than from the object of\ninterest. The backscatter leads to a loss of contrast and the forward\nscatter causes a loss of sharpness. The use of higher wave-lengths re-\nduces the scattering effect.\n\nThe light absorption and scattering are, then, on opposite sides.\nThe least absorbed wave length is on the green and blue spectrum,\nbut the least scattered is in the red spectrum. Generally, the green is\nchosen as a middle ground between the two, although for small ranges,\nthe absorption does not play a major role, so the red spectrum is also\nusable.\n\nRefraction is the light ray deflection when the transmission medium\nindex of refraction changes. In underwater images, the refraction oc-\ncurs on the interfaces between water, glass and air. The effect of a\nrefraction in underwater images can be seen in Fig 21. The same pat-\ntern on the same distance and position from the camera is shown in\nboth images, but the first one is underwater and the second in air. The\nrefraction depends on the index of refraction of the medium, in the\ncase of water, its index is dependent mainly on pressure, temperature,\nwavelength and salt concentration (QUAN; FRY, 1995).\n\nFigure 21: Image of the same pattern in the same position, with and\nwithout water, respectively. The images were captured through a flat\nglass and the optical axis was normal to the glass. Source: Author?s\nown work\n\n\n\n52\n\nThe refraction can be modelled (GLASSNER, 1989) as shown in\nFigure 22. The incidence ray I on a medium with index of refraction\n?1 reaches a surface with an angle of incidence ?i with respect to the\nvector N, which is normal to the interface of refraction. The ray is\ntransmitted to the medium with index of refraction ?2 in the direction\nT with an angle of ?t.\n\n?1\n\n?2\n\nN\n\n-N\u2019\n\nI\n\nT?t\n\n?i\n\nFigure 22: Refraction model. Adapted from Glassner (1989)\n\nBecause the transmitted ray is in the same plane as the normal\nand the incidence ray it can be written as a combination of both\n\nT = ?I + ?N (2.13)\n\nThe Snell? s Law states\n\nsin ?t\nsin ?i\n\n=\n?i\n?t\n\n= ?it (2.14)\n\nFrom squaring both sides of Eq. 2.14\n\nsin2 ?i ?\n2\n\nit = sin\n2 ?t (2.15)\n\nAnd because sin2 ? + cos2 ? = 1 and (N \u00b7 N) = 1\n\n\n\n53\n\n(1 ? cos2 ?i) ? 1 = cos2 ?t\n= [?N \u00b7 T]2\n\n= [?N \u00b7 (?I + ?T)]2\n\n= [?(?N \u00b7 I) + ?(?N \u00b7 N)]2\n\n= [? cos ?1 ? ?]2\n\n(2.16)\n\nBecause T has unit length\n\n1 = T \u00b7 T\n= (?I + ?N) \u00b7 (?I + ?N)\n= ?2(I \u00b7 I) + 2??(I \u00b7 N) + ?2(N \u00b7 N)\n= ?2 ? 2?? cos ?i + ?2\n\n(2.17)\n\nBy solving Eq.2.16 and Eq.2.17 for ? and ? and selecting the\nvector on the forth quadrant, the transmitted ray equation is\n\nT = ?itI + (?it(N \u00b7 ?I) ?\n?\n\n(1 + ?2it((N \u00b7 ?I)2 ? 1))N (2.18)\n\nSlight variations in the refractive index can cause a substantial\ndifference in the acquired image, since an angle variation can represent\nmany pixels in image coordinates. The refraction causes a problem de-\nscribed as a 3D distortion on the image (TREIBITZ; SCHECHNER; SINGH,\n2008). It happens because the apparent focal length varies depending\non the light ray incidence angle, thus the pinhole model, in which all\nthe rays pass through a single point, which is not a physically valid\nmodel. One way around the refraction is the dome windows (NEWTON;\nBALDWIN; FRYER, 1989), as seen in Figure 23, they minimize the re-\nfraction effect acting as a slightly positive lens. They must be mounted\nso that the dome center of curvature is coincident with the pinhole\ncenter, which makes the dome specific for the designed lens and casing\nset.\n\nAll these factors have many impacts on both the designing un-\nderwater sensors and on the measurement method. The light source\nchoice and considering the refraction during the measurement are some\nof the actions to minimize the various water effects. In the LTS case, i.e.\nshort ranges, refraction is one of the most impacting effects, therefore,\n\n\n\n54\n\nFigure 23: Dome window correcting the refraction. (NEWTON; BALD-\nWIN; FRYER, 1989)\n\na calibration that takes this matter into account is essential.\n\n\n\n55\n\n2.3 UNDERWATER CAMERA CALIBRATION\n\nThe main effect to be considered during underwater camera cal-\nibration is refraction. The ray deflection caused by it can reflect into a\nseveral pixels position error. Refraction depends on the kind of housing\nbeing applied, domes and flat windows are the most commonly used.\nThe flat window is the easiest to build, but the most influenced by the\nrefraction. In this work, the main focus is to use the flat window and\nto predict the refraction using the measurement software, thus reduc-\ning the building costs. The camera in underwater environments inside\na flat window housing is affected by a focal length variation effect, as\nshown in Figure 24, the objects appears to be bigger than when they\nare in-air, because of the reduced field of view (FOV). There are two\nmain ways to counteract this effect: to consider the camera axial or\nto approximate this effect to a radial distortion also leading to a new\nsystem focal length.\n\nFigure 24: Non-SVP while looking through a flat interface. (TREIBITZ;\nSCHECHNER; SINGH, 2008)\n\nThe first possible solution is to consider the camera as axial\n(AGRAWAL et al., 2012), i.e., the focal point is variable with the light\nray angle of incidence. This is a more physically accurate method, since\neach ray is traced following its actual path. However, in this model,\nnew calibration parameters must be estimated, thus the calibration\nbecomes more complex. Even though the camera is considered axial,\nall the rays still physically pass through the pinhole center point.\n\nThe underwater camera model used on the axial camera con-\n\n\n\n56\n\nsideration is as follows, shown in Figure 25. Considering a pinhole\nmodel camera watching a 3D point through n, perfectly flat windows,\ni = 1, 2, ..., n, with an angle between the camera axis and the window\nnormal vector. Building the light path from the camera to the object\npoints, the first ray is v0 in the medium with refractive index \u00b50, this\nray reaches the first interface of refraction on the point q0 with a dis-\ntance d0 from the pinhole center point. The ray is, then, refracted on\nthe plane of refraction that contains the vector v0 and the axis of refrac-\ntion and the process is repeated for each layer. Thus, ray tracing the\nlight ray from the camera is possible knowing the axis z1,the distance\nbetween each interface di and each refraction index \u00b5i. These are the\nnew calibration parameters, apart from the other camera parameters\nto be considered on this model. All these parameters can be obtained\nwith only one underwater planar standard acquisition, for instance a\ncheck board. When considering a camera housing, watching the un-\nderwater environment through a flat window, some simplifications can\nbe made. For example the glass width can be measured with a mi-\ncrometer, which can have negligible errors or only one refraction can\nbe considered (between water and air) instead of the two that actually\nhappen (between water and glass and glass and air).\n\nFigure 25: Multi-layer flat refractive geometry. (AGRAWAL et al., 2012)\n\nA second possible way to consider the refraction during the mea-\nsurement is to approximate the underwater distortion as a 2D radial\ndistortion (KANG; WU; YANG, 2012). This technique can have good re-\nsults in certain conditions where the refraction does not have a large\ninfluence, such as a smaller than 30? FOV. However, this also requires\n\n\n\n57\n\na focal length approximation, since each light ray converges to a dif-\nferent point, depending on the angle of incidence. In this method, a\nset of 3D points is acquired on the measurement conditions, i.e. the\ncamera mounted inside the housing and underwater. This set of points\nare used to calibrate the camera using the same methods used in-air.\nThe results will be very different from the in-air ones, the focal length\nand the distortion coefficients will both be larger. This, then, requires\na set of underwater acquisitions on the measurement environment.\n\nBoth methods have advantages and disadvantages. Mainly that\nthe axial camera is more physically accurate, thus more flexible for\ndifferent system configurations, but it is harder to implement than the\nSVP approximation. In this work the focus will then be the more phys-\nically accurate method, since the developed software will be applicable\nfor different configurations.\n\n\n\n58\n\n\n\n59\n\n3 UNDERWATER DEVELOPED SENSOR\n\nThe built LTS and the algorithms developed for measurement\nand calibration methods are described in this section. The designed\nLTS is optimized for underwater measurements using both the algo-\nrithms.\n\n3.1 MECHANICAL DESIGN\n\nThe sensor was designed and was built by following the decisions\ndescribed here. First, some design requirements are listed aiming at the\noil and gas industry, then the system features to reach these goals are\nchosen and implemented in the built sensor.\n\n3.1.1 Design Requirements\n\nThe sensor is aimed at the oil and gas industries need to verify\nunderwater equipment. These procedures are essential to grant the\nwhole safety and maintenance of the system. One of the most usual\npieces of equipment are oil and gas ducts with a typical duct diameter\nof 300 mm.\n\nThus, the main LTS requirements are:\n\n? The sensor must be designed to measure one fifth of a 300 mm\ndiameter tube transversal section. This is a common procedure\nin the oil and gas industry to estimate the equipment condition.\n\n? The final measurement uncertainty must be close to tenths of\nmillimeters to gather the required object state.\n\n? The distance from the measurement system to the surface of in-\nterest must ensure the equipment safety. Here, the considered\ndistance is of at least 150 mm.\n\n? The system must also be modular, to enable future changes in\nthe measurement volume by changing the system baseline and\ntriangulation angle.\n\nThe requirements are used to fulfill the equations used to model\nthe LTS and decisions made during designing. The procedures and\nequations used during the process are described in the next section.\n\n\n\n60\n\n3.1.2 System Design\n\nThe system design consists of choosing the right system features\nto satisfy the design requirements and build the system in accordance\nwith the safety rules. The LTS features to be chosen are triangulation\nangle and baseline. The hardware to be chosen is a, image sensor, a\nlens, a laser and laser line projection optics. Then, the housing must\nbe designed to seal and fix the sensor components for underwater use.\nThe final designed sensor is presented in Figure 26.\n\nFigure 26: Designed sensor. Source: Author?s own work\n\nThe first tests were made using a laser triangulation mathemati-\ncal model. The triangulation angle, camera and lens choices were made\nbased on this model. Using a water tank to simulate underwater mea-\nsurements the base line was later adjusted to fulfill the requirements.\n\nThree requirements were initially considered: measuring one fifth\nof a 300 mm diameter pipe transversal section, tenths of millimiters\nuncertainty sensor and the required distance from the sensor to the\nmeasured surface to allow the system safety. The uncertainty here is\napproximated as a sensor resolution, i.e. the mm variation per pixel.\nThis is a reasonable initial approximation, because, even though the\nfinal uncertainty is bigger than the sensor resolution, the laser peak is\ndetected with sub-pixel resolution.\n\nThe measuring of a 300 mm diameter tube in 5 divisions requires\na minimum of 176.3 mm along the laser line, as shown in Figure 27.\n\nTo estimate the Y measurement range, the equation 2.11 is used.\nThen, the result is multiplied by the number of pixels in the corre-\nsponding Y direction. Because the Y resolution changes within the\nmeasurement volume, the stand-off was used.\n\nThe Equation 2.9 was used to estimate the LTS resolution on\nthe Z direction.\n\n\n\n61\n\n1\n7\n6\n.3\n\nm\nm3\n\n0\n0\nm\nm\n\nFigure 27: Inscribed pentagon side sized in a 300 mm diameter circle.\nSource: Author?s own work\n\n?Z =\nB cos ?tfsx\n\n(f tan ?t ? sxB cos ?t)2\n?u (3.1)\n\nThe initial desired ?Z is 0.2 mm per sensor pixel considered the\nimage center. The first choices were the triangulation angle and the\nimage sensor.\n\nTriangulation angles can vary from 15? up to 80?, an initial es-\ntimate is, the bigger the angle, the smaller the measurement volume\nand consequently, better the resolution. Since the sensor is aimed at\nmid range measurements, the initial considered triangulation angle was\n35?.\n\nThe selected camera must have a global shutter, because the\nwhole laser line must be captured in the same moment. This way if\nthe sensor is moving during a measurement, for example, attached to\nan ROV, the measurement results are not affected. The chosen camera\nresolution is 1.3 mega pixels as an array of 1280 pixel columns and\n1024 pixel lines and a pixel size of 5.3 ?m. This is also a camera with\nmedium resolution among the machine vision cameras.\n\nThe baseline and focus were left as variables. The considered mid\nFOV lenses were 8 mm and 12.5 mm focal lengths. Another variable was\nthe camera position in respect to the laser line: the laser line along the\nsensor line or the laser line along the sensor columns, which allow a\nbigger measurement length along the Z or Y directions, respectively.\n\nThe last requirement is the distance from the sensor to prevent\naccidents, the considered safety distance was of at least 150 mm from\nthe beginning of the measurement volume. This distance can be geo-\nmetrically approximated by drawing the current configuration. After\napplying the equations, and the geometrical approximation the 8 mm\n\n\n\n62\n\nlens could not reach the required Y range within the safety distance, so\nthe 12.5 mm lens was chosen. The baseline was set, as a first approx-\nimation to 240 mm with the laser line along the sensor columns. The\nunderwater influence was approximated to make the resolution 1.33\nhigher, i.e. a magnification of 1.33. After choosing the camera, the\nlens and the laser, the free variables are the triangulation angle and\nthe baseline.\n\nThe FOV lens with the selected camera was obtained by posi-\ntioning a graph paper at known distances from the lenses. The FOV\nwas used to simulate the final measurement volume in a 3D model soft-\nware, as shown in Figure 28. The desired Y direction distance along\nthe laser and the object distance to the sensor, were also optimized\nduring this procedure. The desired requirements were obtained with\nan adjustment to the baseline.\n\nFigure 28: LTS 3D model optimization with in-air values. Source:\nAuthor?s own work\n\nSubsequently, the camera and laser were positioned outside of a\nwater tank, shown in Figure 29, with an angled wall to reproduce the\nfinal sensor positioning. In this set-up, the baseline was finally chosen.\n\nConsequently, the system equipment and characteristics that\nmeet the desired requirements are shown in Tab. 1.\n\nThe next step is to build the sensor housing and positioning\nstructure. The sensor consists of the camera module, the laser module\nand the positioning structure. The modules are positioned by two\naluminum sheets and four parallel pins, two in each module. These\naluminum sheets also have holes to mount possible systems for auxiliary\n\n\n\n63\n\nFigure 29: Angled wall water tank used in final evaluation. Source:\nAuthor?s own work\n\nTable 1: System parameters\n\nBaseline 265 mm\n\nTriangulation Angle 35?\n\nResolution 1.3 mega pixels\n\nPixel Size 5.5 \u00b5m\n\nOrientation Laser along the sensor columns\n\nFocal length 12.5 mm\n\nmovement. This module set-up was chosen since, after changing the\naluminum sheets, the modules can be positioned in different distances\nand angles, enabling a measurement volume variation.\n\nThe camera and laser were fully positioned without theoretical\nredundant alignments. Both the camera and the laser are positioned\nwith respect to the superior aluminum sheet that permits the specified\nsensor baseline and triangulation angle.\n\nThe camera positioning and mounting is shown in Figure 30. The\ncamera is fixed by two ?U shapes?, but is only positioned by the superior\n?U shape?, which has a smaller inner space. The positioning features\nare: the plane behind the shape, one side of the ?U shapes?interior and\nthe superior plane inside the ?U?. The ?U? shapes are fixed into another\nauxiliary element and positioned by three planes on this element. The\n\n\n\n64\n\nauxiliary element is then positioned on the lid by an internal cylinder,\na plane and a positioning pin. The lid is positioned on the aluminum\nsheet support by a plane and two pins. The support is again positioned\nby two pins and a plane on the aluminum sheet.\n\n(a) (b) (c)\n\n(d) (e) (f)\n\n(g) (h)\n\nFigure 30: Camera positioning.(a) The positioning ?U? shape.(b) The\ncamera fixed on the ?U? shapes. (c) The auxiliary fixing element po-\nsitioning. (d) The camera mounted on the auxiliary element. (e) The\nauxiliary positioning element with respect to the lid. (f) The camera\nmounted on the lid. Source: Author?s own work\n\nThe laser axis must be perpendicular to the window and coin-\ncident with the lid axis, this positioning is shown in Figure 31. The\nlaser is fixed by a claw mechanism that is also centered in the window\ncenter. Then, the laser claw mechanism has an inside cylinder that fits\n\n\n\n65\n\ninside the front lid. Finally the laser lid is fixed to the aluminum sheet\nin the same way as the camera lid.\n\nFigure 31: Laser positioning. Source: Author?s own work\n\nBesides the camera and laser positioning, the system is also fixed\nby another aluminum sheet under the sensor and the back lids also have\nadditional holes for fastening the aluminum sheets.\n\nThe sensor, therefore, has four lids, two for each module. The\nlids have O-ring grooves for both the window and the housing. Addi-\ntionally, the camera lid has reserved open space for the lens, so the focus\nand aperture can be adjusted while the camera is in the measurement\nposition.\n\nThe modules housing are pneumatic cylinders and their sizes\nwere selected by the camera and laser sizes. The camera has a diago-\nnal length of 37.33 mm, models were created with each commercially\navailable cylinder and the 63 mm diameter was chosen for the camera\nand the 50 mm for the laser.\n\nThe sensor sealing was made according to ISO 3601 (FLITNEY,\n2011). The system uses two axial tandem O-rings in each window and\ntwo more radial O-rings in each of the frontal and back lids in the laser\nand camera modules. The aluminum tubes were measured in a CMM\nso that the O-ring squeeze is in the designed range. The camera FOV\nwas used to optimize the spaces reserved for the O-rings.\n\nThe cables were connected to the inside of the housings by hoses\n\n\n\n66\n\nand sealed by hose barbs and O-rings on the flanges, as shown in Figure\n32. Since the sensor is designed to dive to a maximum depth of 5 m,\nthe hose barb is enough to grant the sealing. The hose diameter is\n31.75 mm in the camera module and 12.7 mm in the laser module. The\ncamera hose diameter is big enough to allow an USB 3.0 connector to\nmove freely. The laser hose diameter is enough to insert the power\nsupply cable.\n\nFigure 32: Details of the back flanges. Source: Author?s own work\n\nThe windows were projected to resist a pressure differential of\n1000 kPa aiming for the safety of the system.\n\nConsidering the window configurations shown in Figure 33.\n\nFigure 33: Clamped and unclassified configurations.(HARRIS, 1999)\n\nWhere tw is the window thickness, Aw is the unsupported aper-\nture diameter and ?Pw is the pressure differential. The window thick-\nness tw must be greater than (HARRIS, 1999)\n\ntw = 0.5Aw\n\n[\n\nKwfs\n?Pw\nSf\n\n]1/2\n\n(3.2)\n\n\n\n67\n\nWhere fs is the safety factor (usually 4) and Sf is the fracture\nstrength. Examples of fracture strength values are found in Figure 34.\n\nFigure 34: Window resistance values. (HARRIS, 1999)\n\nThe resulting thickness for the designed housing is 5.53 mm con-\nsidering the camera module as the weaker window since it is the one\nwith the larger unsupported aperture. Hence, the mounted window\nthickness is 6 mm, the closest available commercial thickness.\n\nIt is important to notice that the aluminum sheets are fixed to\nthe front lid of each module and the auxiliary fixing element is used\nto fix the sheet to the lid, as shown in Figure 35. This way the sensor\ncalibration can be made in the same configuration where the measure-\nment will occur. Accordingly, adjustments, for example, in camera and\nlaser focus, can be made in measurement positioning.\n\nFigure 35: Sensor fixing during calibration. Source: Author?s own work\n\nThe aluminum sheets also have auxiliary holes to mount the\nsystem into a moving system. The holes are to fit in a ABB robot. The\nsensor can be fixed from above or from behind, as shown in Figure 36.\n\n\n\n68\n\nFigure 36: Back and superior fixing holes for auxiliary moving systems.\nSource: Author?s own work\n\nThe system hardware, shown in Figure 37, is composed of a\ncamera, a lens, a laser and line generator optics. The camera is a\nXimea MQ013MG-E2, as mentioned, it has a resolution of 1.3 mega\npixels and a pixel size of 5.3 ?m. The lens is a Fujinon HF12.5HA-1B,\nwhich has a focal length of 12.5 mm and adjustable focus and aperture.\nThe laser is a mini structured light laser diode modules with 635 nm\nwavelength, it uses line formation optics to project a line with an angle\nof aperture of 45? and has power of 7 mW.\n\n(a) Camera (b) Lens (c) Laser\n\nFigure 37: Hardware used to build the LTS. (MQ013MG-E2, 2017;\nHF12. . . , 2017; MINI. . . , 2017)\n\nIn Figure 38, the built LTS is shown.\nThe resulting ?s angle, to change the camera plane of focus to the\n\nlaser plane is 87.9?, thus the Scheimpflug condition was not considered.\nThe sensor speckle limit resolution considering the f number of\n\n4, at the standoff of 250 mm and the focal length of 12.5 mm is 16 ?m.\n\n\n\n69\n\nFigure 38: Built LTS. (a) is the camera module and (b) is the laser\nmodule. Source: Author?s own work\n\n\n\n70\n\n3.1.3 Measuring Bench\n\nThe LTS must be displaced to build a dense point cloud. The\nsensor displacement and the new orientation must be known, allowing\nthe point clouds concatenation. In this case, the displacement system is\nused both during calibration and measurement. The measuring bench\nallows this displacement to be controlled with good alignment between\nthe elements.\n\nThe measuring bench is composed of a displacement system, a\nhydraulic system and a mounting structure. The complete measuring\nbench can be seen in Figure 39.\n\nFigure 39: Complete measuring bench: displacement system (a), hy-\ndraulic system (b) and mounting structure (c). Source: Author?s own\nwork\n\n\n\n71\n\nIn this work, the displacement system used to move the LTS is\na linear slide, shown in Figure 41. It has a length of 750 mm and it\nis ball screw driven. The table is driven by the bipolar step motor\nAK23/15F6FN1.8, the driver AKDMP164.2A and a 42 V DC power\nsource with 4 A peak current. The system is controlled by an Arduino\nmicro controller which communicates via serialization with a personal\ncomputer. Apart from the slider control, the Arduino is also used along\nwith a transistor to control the laser.\n\nFigure 40: The linear slide and the motor. Source: Author?s own work\n\nThe threaded rod has a lead of 5 mm and the motor has 400 steps\nper revolution, which results in a displacement resolution of 0.0125 mm\nper step. The driver allows motor micro steps, but this feature was not\nused.\n\nThe hydraulic system has a measuring tank, a storage tank, a\nwater pump and the connecting pipes. During underwater measure-\nments, the measure tank is filled by the height difference. Then, for\nin-air measurements, the water pump is turned on to drain out the wa-\nter from the measurement tank to the storage tank. The system also\nhas valves to allow a water change in the storage tank.\n\nThe mounting structure is made of aluminum profiles and it\nfastens the displacement table, the LTS and the calibration standard on\nthe measuring tank. The LTS can be mounted facing downward (during\nmeasurement) or horizontally (during calibration). More details of both\npositions are presented in the evaluation section.\n\n\n\n72\n\nFigure 41: Mounting structure details. Source: Author?s own work\n\n3.2 DEVELOPED ALGORITHMS\n\nAlgorithms for measurement and calibration were developed.\nThe calibration model considers one refraction between water and air\nto estimate the distance between the interface of refraction and the\npinhole center. This distance, along with the camera calibration pa-\nrameters, is used to ray trace the crossing between the laser peak ray\nwith the laser plane.\n\nTo perform the underwater calibration, the system must already\nbe calibrated in-air. The camera projection matrix, the distortion co-\nefficients and the laser plane coefficients are, therefore, known.\n\nFirst, the measurement procedure will be shown. This is a more\nlogical sequence because the calibration procedure uses a very similar\nalgorithm as the one used during measurement.\n\n3.2.1 Measurement Algorithm\n\nThe measurement method considers a refraction, between air\nand water to back trace the crossing point between the detected laser\npeak and the laser plane. The following approximations where made :\n\n? The camera is calibrated with the protective window, so the re-\nfraction on the glass is not considered, being approximated as a\n\n\n\n73\n\nradial distortion.\n\n? The camera axis is considered normal to the refractive window,\nso that the axis of refraction is coincident with the camera axis.\n\n? The laser plane is perpendicular to the protective window, there-\nfore its refraction is not considered.\n\nThe influence of these considerations was also weighted during\nsystem evaluation.\n\nThe following model, shown in Figure 42, was used. A camera\nwith pinhole center C and distance d from the interface of refraction\nobserves a scene. An arbitrary ray I from the pinhole center reaches\nthe interface of refraction at the point q, then it is refracted and trans-\nmitted on the T direction . The crossing between the ray T and the\nlaser plane is the 3D point X.\n\nZ\n\n?it Water\n\nAir\n\nN\n\n-N?\n\nC\n\nI\n\nT\n\nq\n\nd\n\nX\n\nFigure 42: Measurement model. Source: Author?s own work\n\nThe back trace from the image laser peak to the crossing with the\nlaser plane is used to perform the measurement. During the ray tracing,\neach ray from each image line detected peak is refracted separately.\nThe whole measurement procedure is shown in Figure 43. All the\nmeasurements are made in the CCS.\n\nBefore starting the measurement, the system must have all in-\nair parameters shown in Figure 43 estimated: distortion coefficients,\ncamera matrix, window distance, refraction index and laser plane coef-\nficients.\n\nThe first step is to acquire an image of the laser line projected\non the surface of interest. After the acquisition, the image is corrected\n\n\n\n74\n\nAcquires an im-\nage of the laser line\n\nUndistort the acquired image\n\nFind the laser peak\nfor each image line\n\nDistortion\nCoefficients\n\nDefine vectors I\nfrom laser peaks\n\nCamera\nMatrix\n\nFind the points of intersec-\ntion of the projected rays with\n\nthe interface of refraction\n\nWindow\nDistance d\n\nRay refraction\nRefraction\n\nIndex\n\nFind the 3D point of intersection X of\nthe refracted ray with the laser plane\n\nLaser Plane\nCoefficients\n\nFigure 43: Measurement procedure. Source: Author?s own work\n\nfrom its distortion with the distortion coefficients. Then, the image is\nprocessed to extract the image coordinate containing laser peak for each\nimage line. The first directional vector, v, which is also the incidence\nvector I, is obtained from the laser peak pixel and the inverse of the\nintrinsic parameters matrix K\n\nK?1x = I (3.3)\n\nWhere x is the laser peak pixel coordinate and I is the refraction\nincidence vector. After this, the point of intersection between this\nvector and the refractive window is found. The refractive window is\nconsidered a plane parallel to the image sensor, hence with normal, N,\npointing in the Z direction in CCS, i.e. with coordinates (0, 0, 1). The\ndistance from the pinhole center to the refractive window is used in the\ninterface of refraction plane equation. Point p\n\n0\nis, then, (0, 0, d) where\n\nd is the distance from the pinhole center to the window of refraction\n\n\n\n75\n\nand the plane equation for a point p on the window of refraction is\n\np ? (0, 0, d) \u00b7 N = 0 (3.4)\n\nAnd the crossing between the line with direction I from the origin\nand the plane of refraction is the point q\n\nq =\nd\n\nI \u00b7 N\nI (3.5)\n\nThe directional vector I is refracted on the plane of refraction\nand the transmitted ray T direction is given by the Eq. 2.18, presented\nagain below\n\nT = ?itI + (?it(N \u00b7 ?I) ?\n?\n\n(1 + ?2it((N \u00b7 ?I)2 ? 1))N (3.6)\n\nThe refracted ray has the equation\n\npt = kT + q (3.7)\n\nWhere k is a scalar in IR.\nThe final step is to compute the crossing between the refracted\n\nray equation and the laser plane, which is a plane with coefficients A,\nB, C, D.\n\nThen, its normal direction vector is Nl = (A, B, C) and one\npoint on the plane is P0l = (0, 0, ?D/C) which results in the plane\nequation for a general laser plane point Pl\n\n(Pl ? P0l) \u00b7 Nl = 0 (3.8)\n\nAnd the crossing between the laser plane and the transmitted\nray T is\n\nX =\n(P0l ? q) \u00b7 Nl\n\nT \u00b7 Nl\nT + q (3.9)\n\nResulting in the 3D point X. The whole process is repeated for\neach image line forming the 3D point cloud for this sensor position.\nThe point cloud is then transformed to the OCS, where the X vector\nis normal to the laser plane, to make the later concatenation of point\nclouds easier.\n\nAfter this, the sensor is moved in the direction normal to the laser\nplane, which is the direction \u00b1 X in OCS, with a known displacement\nvalue. A new point cloud is obtained and, after the transformation to\n\n\n\n76\n\nOCS, the X coordinate is added to the displacement value, accomplish-\ning the concatenation. The results from each acquisition are united,\nmaking the final three-dimensional point cloud.\n\nMost of the parameters used during the measurements are ob-\ntained during camera calibration and laser calibration. The refraction\nindex can be estimated with refractometers or some previous knowl-\nedge. So, the remaining window distance is the aim during the new\ndeveloped calibration process.\n\n3.2.2 Refraction Calibration Algorithm\n\nThe main idea of the refractive calibration algorithm is to mea-\nsure some known underwater feature to optimize the window distance\nparameter used during measurement. Then, the proposed method con-\nsists, first, of a complete in-air calibration, i.e. intrinsic and extrinsic\ncamera parameters, distortion coefficients and laser plane coefficients.\nAfter this calibration, some underwater acquisitions are taken from\na standard with a known characteristic. The standard must have a\nfeature possible to evaluate with only one line along the object, for\nexample, here, the height between steps of a standard. The result of\none acquisition of this object is shown in Figure 44. This underwater\nacquisition is used to estimate the distance from the pinhole center to\nthe window of refraction, the distance d.\n\nFigure 44: Resulting image from the step standard used during window\ndistance calibration. Source: Author?s own work\n\nThe measured 3D point depends on the refraction interface dis-\n\n\n\n77\n\ntance to the pinhole center and has an influence in the final result, as\nshown in Figure 45. However, this is a distance dependent on the cam-\nera and lens configuration, manufacturing tolerances and mechanical\ndesign, which is not always easy to estimate.\n\n?it\n\nAir\nWater\n\nC\n\nI\n\nT\n\nd1\n\nX\n\n?it\n\nAir\nWater\n\nC\n\nI\n\nT\n\nd2\n\nX\n\nFigure 45: 3D point result for different window distances d. Source:\nAuthor?s own work\n\nThe main objective of the proposed underwater calibration method\nis to estimate the window distance that minimizes the measurement er-\nror. The whole calibration procedure is shown in Figure 46.\n\nIn-air Camera and\nLaser Calibration\n\nUnderwater Standard Acquisition\n\nStandard measurements\nwith window distances d\n\nWindow dis-\ntances d to test\n\nSelection of the window\ndistance d with minimized error\n\nFigure 46: Calibration procedure. Source: Author?s own work\n\n\n\n78\n\nAfter the in-air system calibration, some acquisitions of a stan-\ndard are made underwater. This standard must allow the measurement\nof some feature with only one laser line. In the step case, it can have\nits height estimated. The standard must have the evaluated feature\nmeasured with a negligible uncertainty compared with the LTS uncer-\ntainty.\n\nThe underwater acquisition of the standard is used to repeat the\nwhole measuring algorithm for a set of window distances to test. The\nresult is a point cloud for each tested window distance with one acqui-\nsition. After this, the point cloud of each tested window is separately\nevaluated. In the step case, a line is adjusted in one of the steps and\nthe mean distance from the points on the other step to the adjusted\nline is the measured height for this window distance. The calibration\nresults is the window?s distance d that minimized the evaluated error\nand is used in the measurement model.\n\n\n\n79\n\n4 SYSTEM EVALUATION\n\nThe evaluation was made using both the developed method and\nthe polynomial adjustment, the latter used as a reference. All the re-\nsults are compared with measurements in-air and underwater. The\npolynomial adjustment requires all the acquisitions to be underwater\nand the proposed method requires an in-air calibration and some un-\nderwater images of a standard in any non-previously known position.\n\nTo evaluate the system, two fixed calibrated spheres, a plane, a\ndummy head, welding beads and a 300 mm diameter pipe were mea-\nsured. The spheres were calibrated with a CMM; the dummy head and\nthe welding beads were measured with a commercial fringe projection\nsystem.\n\nThe camera and laser calibration were performed with a circle\npattern standard. Additionally, to estimate the distance between the\npinhole center and the refraction layer, a step standard was used, min-\nimizing the measured height error.\n\nThe system mounting and alignments during calibration and ac-\nquisitions are also described in this section. They are aimed to minimize\nthe influence of first order errors.\n\nThe calibration results using one or multiple underwater step\nimages to estimate distance d were compared and the considerations of\none or two refractions were evaluated. In the two refraction case the\nsystem was calibrated without the window and two refractions were\nconsidered during measurement.\n\nAfter each calibration, in-air and underwater, the sensor is fixed\nin a linear displacement table to acquire images of all the described\nobjects, then the images are processed using either the polynomial ad-\njustment method or the proposed method.\n\n4.1 EVALUATION OBJECTS\n\nAfter all the calibrations, a few objects were measured to evalu-\nate the system errors. They were measured both underwater and in-air\nusing the polynomial method and the proposed method. Some objects\nwere used to evaluate the LTS errors inside the various measurement\nvolume portions, since the resolution varies along the volume, and other\nobjects are presented as measurement examples.\n\nThe objects used to evaluate the method and the system, shown\n\n\n\n80\n\nFigure 47: Evaluation objects: two fixed spheres, welding beads, a\nplane and a dummy head. Source: Author?s own work\n\nin Figure 47, are: two fixed spheres, welding beads, a glass plane, a\ndummy head and a 300 mm diameter PVC pipe. All the objects, except\nfor the PVC pipe, were painted white to provide a diffuse surface.\n\nThe sphere standard is the main evaluation object, following\nthe VDI/VDE 2634 (OPTISCHE. . . , 2000), the guidelines for optical\n3D measuring systems. It consists of two fixed spheres with calibrated\ndiameters and distance between its centers. The spheres were calibrated\nin a CMM. The sphere 1 has a diameter of 50.12 \u00b1 0.01 mm and the\nsphere 2 has a diameter of 50.14 \u00b1 0.01 mm. The distance between\nits centers is 99.18 \u00b1 0.01 mm. The main parameters evaluated in\nthe spheres with the LTS are the sphere spacing error (SSE) and the\nprobing error (PE). The SSE is the difference between the measured\ndistance between the spheres centers by the sensor and the calibrated\ndistance. The range of distances, i.e. the peak and valley difference, of\nthe measured 3D point to the fitted spheres is the PE. The standard\ndeviation (STD) of these distances were also evaluated.\n\n\n\n81\n\nThe spheres images were acquired in six positions along the mea-\nsurement volume. Two acquisitions closer to the system, two on the\nstand-off and two on the end of the measurement volume. In each po-\nsition, 38 images were taken, each moving the sensor 1 mm in the X\ndirection in OCS.\n\nThe plane images were also taken closer to the system, on the\nstand-off and farther from the system. In each position, the plane was\npositioned both perpendicular to the laser plane and rotated around\nthe Y axis at approximately 30?. Afterwards, 150 images were taken\nin each configuration. After each plane point cloud construction the\nrange error (RE) is estimated. The RE is the range of distances, i.e.\nthe peak and valley difference, of the measured 3D point to the best\nfitted plane. The standard deviation (STD) of these distances were also\nevaluated.\n\nAs measurement examples, a dummy head, welding bead and\na 300 mm diameter PVC pipe were also measured with the presented\ntechniques. The dummy head presents challenges for a LTS as the nose\ngenerates occlusions and discontinuities, which pose an obstacle to the\nlaser peak search. The welding bead measurement aims to evaluate\nthe capability of the sensor to measure smaller details. The 300 mm\ndiameter pipe is the main sensor application objective as described in\nthe mechanical design section. One set of acquisitions for each object\non the stand-off was made.\n\n4.2 CALIBRATION STANDARDS\n\nThe calibration standards are the objects used to calibrate the\nparameters needed during measurement. Two calibration standards\nwere used for this work. One to calibrate the camera and the laser, a\nplane with a circle pattern, and the other to calibrate the refraction\nwindow distance, a step attached to a mirror.\n\nTo obtain the 3D points in the OCS used in the calibrations, a\ncalibration standard, shown in Figure 48 is employed. The calibration\nstandard consists of a glass plane with circular patterns. The OCS\nduring calibration is defined by the circles and the table movement\ndirection. Three circles at the center are black and define the origin\nand the X and Y coordinate directions. The Z coordinate is the vector\nnormal to both directions. The X and Y variations in OCS are obtained\nfrom the distances between the circle centers, which were measured with\na measuring microscope and are 9.375 mm in X direction and 9.372\n\n\n\n82\n\nmm in the Y direction. The Z coordinate was measured by known\ndisplacements of the linear slider.\n\nFigure 48: Calibration Standard. Source: Author?s own work\n\nThe step, shown in Figure 49, is formed by glued steel parallelo-\ngrams on a mirror. The distance between steps was measured in mul-\ntiple positions with a microscope and a linear stage and has a height\nof 23.17 \u00b1 0.02. The mirror is important to minimize the influence of a\nrotation around the X axis during the acquisition, which introduces a\nfirst order error in the step measurement.\n\nFigure 49: Step Standard. Source: Author?s own work\n\n\n\n83\n\nWhile the step standard can be aligned using only the mirror,\nthe circular pattern must be moved by the slider and properly aligned\nwith respect to the LTS. This requires some calibration layouts, as well\nas a layout to adjust the lens and the laser.\n\n4.3 CALIBRATION LAYOUT\n\nThe calibration layouts are the methods to mount and to align\nthe planar standard and the LTS. Two calibration layouts were used:\nin the first, shown in Figure 50, the laser and camera are adjusted and\nin the second, shown in Figure 51, the planar standard images used\nduring calibration are acquired.\n\nX\n\nY\n\nFigure 50: Calibration layout used to adjust the laser and the camera,\nalso showing the OCS. Source: Author?s own work\n\nIn the first layout, the laser focus and the camera aperture and\nfocus are adjusted, since the sensor is without the housing. They are\nboth adjusted with the standard at the standoff distance. The first\nstep is to focus the laser, after this, both the camera focus and the\naperture are adjusted. The aperture is set to fully open and the focus\nis adjusted, then the standard is moved to the end of the measurement\nvolume and the aperture is closed up until the image is in focus. This\nallows the speckle to be as small as possible while the laser line is in\n\n\n\n84\n\n(a) Slide alignment\n\n(b) LTS alignment\n\nFigure 51: Calibration layout used to acquire the calibration images.\nFirst the table is aligned (a), then the LTS is aligned (b), both with a\nspirit level. Source: Author?s own work\n\n\n\n85\n\nfocus. Next, the laser is positioned perpendicularly to the top sensor\nfixing aluminum sheet, this element will be used as reference to fix the\nLTS later during measurement. Hence, the LTS is aligned with the\nlinear slide movement direction.\n\nIn the second calibration layout, the calibration standard is fixed\non the measuring tank mounting structure and the LTS is moved by\nthe linear slide. The aim of this positioning is to minimize the error\nfrom the misalignment between the table movement direction and the\nZ coordinate from the LTS OCS.\n\nThe calibration standard is fixed on the aluminum structure with\nthick acrylic adhesive double sided tape and a square is used to align\nthe standard with the profile. Then, the structure with the standard is\nfixed on the linear slider mounting structure.\n\nA spirit level is used to align the laser with the table displacement\nduring calibration.\n\nThe OCS used during calibration is based on the calibration\nstandard. This coordinate system does not have the YZ plane on the\nlaser plane. Later, during measurement, the OCS will be changed so\nthat the X coordinate is mathematically perpendicular to the laser\nplane, i.e. the table displacement direction, thus minimizing the error\nfrom this misalignment. Although, even with these considerations, this\nmisalignment will still be a source of uncertainty for the measurement\nresult.\n\nThe calibration standard is moved 10 mm between acquisitions\nand there is is a wait time of 1 second, both underwater and in-air. The\nstandard covered a distance of 300 mm in-air and 220 mm underwater\nin the Z direction.\n\n4.4 ACQUISITION LAYOUT\n\nIn the acquisition layout, shown in Figure 52, all images used\nlater during the measurement were acquired. Here, the LTS is moved\nby the linear slide and the measured object is fixed.\n\nThe OCS is defined so that the X direction is perpendicular to\nthe laser plane. The LTS is aligned based on the linear slider with a\nspirit level, so the laser plane is perpendicular to the direction of move-\nment. This results in a displacement vector close to the X direction,\nthus making the concatenation easier and more accurate.\n\nDuring the measurements, the LTS is moved 1 mm between im-\nage acquisitions. To avoid error due to vibrations, there is a one second\n\n\n\n86\n\nFigure 52: Acquisition layout. Source: Author?s own work\n\nwait after the movement. Then, the images are processed using either\nthe polynomial adjustment or plane adjustment calibrations. The same\npositioning was used in all the acquisitions.\n\n4.5 IMAGES PROCESSING\n\nThe image processing is an essential step during a LTS measure-\nment and calibration. All the acquired images are processed to better\nfind both the circle centers of the calibration standard and the laser\npeaks. Even though the image is separated in pixels, the aim of the\nimage processing is to reach the sub-pixel level through image process-\ning operations and interpolations.\n\nThe image processing to estimate the circles centers is shown\nin Figure 53. The following sequence of processing operations is ap-\nplied: median filter with a size of 5 x 5 and 12th order, threshold with\nbackground correction, circle hole filling, elimination of particles on the\nborder , removal of small particles and estimation of each ellipse center\nby the function IMAQ Fit Ellipse 2 in LabVIEW. The ellipses?center\ncoordinates are used to find the circles with a black center on the origi-\nnal image and the coordinate system X and Y direction is built. Finally,\nthe ellipse centers are given 3D coordinates using the real distance be-\ntween centers in mm.\n\nThe laser peaks are estimated after a threshold with background\ncorrection and a 5 x 5 size low pass filter. Then, quadratic equations\n\n\n\n87\n\nFigure 53: First, the original acquired image, then the processing result\nand the labels. Source: Author?s own work\n\nFigure 54: Laser image processing to peak search. First, the original\nimage, then, the filter and the laser peak position (not shown with\nsubpixel resolution). Source: Author?s own work\n\nare adjusted every few columns along the line being evaluated and the\npeaks are estimated. The process is shown in Figure 54.\n\nThe processing of circle centers is used as references to calibrate\nthe camera and the laser. The processing of the laser peaks is used dur-\ning both calibration and measurement. The detection of these features\nis an important source of uncertainty.\n\n4.6 ACQUIRED IMAGES\n\nThe image acquisition process is a great source of uncertainty.\nSince the sensor must be displaced to create a dense point cloud, the\n\n\n\n88\n\nacquisition direction and the distance displaced by the slider are some\nof these sources. Some acquisition images are presented along with the\nsensor movement direction. All the presented images are still distorted.\n\nThe spheres were positioned in three different distances from the\nsensor, as shown in Figure 55, in-air and underwater.\n\n(a) (b) (c)\n\n(d) (e) (f)\n\nFigure 55: Sphere acquisitions.(a-c) images shown in-air. (d-f) images\nunderwater. Respectively close to the system, on the stand off and\nfarther from the system. Source: Author?s own work\n\nThe plane, the dummy head, the welding beads and the PVC\ntube image examples are shown in Figure 56. The main difference\nbetween in-air and underwater images is the measurement volume. For\nexample, the laser line is completely captured during underwater PVC\ntube measurement, but in-air, the line does not appear in all image\nlines.\n\n\n\n89\n\nThe sensor movement direction for the sphere, the dummy head\nand the welding beads are presented in Figure 57. These are examples\nof meshes created by the LTS. The sensor movement direction aims to\nminimize the linear slider influence on the measurement result.\n\n\n\n90\n\n(a) (b)\n\n(c) (d)\n\n(e) (f)\n\nFigure 56: (a) (c) and (e) are in-air images. (b) (d) and (f) are under-\nwater images. Respectively from the dummy head, the welding beads\nand the PVC tube. Source: Author?s own work\n\n\n\n91\n\n(a) Spheres (b) Dummy head (c) Welding beads\n\nFigure 57: Sensor movement direction during acquisitions. Source:\nAuthor?s own work\n\n\n\n92\n\n4.7 POLYNOMIAL CALIBRATION\n\nAs a comparison with the proposed method, a modeless calibra-\ntion was also adopted. This is considered a reference method, because\nall the considerations made in the proposed method are absorbed dur-\ning the polynomial adjustment.\n\nThe calibration with the polynomial equations were made in-\nair and underwater. For each line, two sixth-degree polynomials were\nfitted: one relating the laser peak column v with the Z coordinate Z(v)\nand another relating the Z coordinate with the Y coordinate Y(Z).\nAfter the laser peak detection, the detected column v is applied to the\nZ(v) equation and the resulting Z coordinate is applied to the Y(Z)\nequation. Resulting in the Z and Y coordinates for the detected peak.\nThe process is repeated for each line.\n\nThese polynomial are, then, correlated with the image peaks\ndetected from the acquisitions. The Z(v) polynomial equations have a\nmean residual of 0.06 mm in-air and underwater. The Y(Z) polynomial\nequations have a negligible mean residual error.\n\nExamples of the adjusted polynomial equations are plotted in\nFigure 58 and Figure 59. The plotted polynomial equation are from one\nof the first image lines (top), from the image center (middle) and from\none of last lines of the image (bottom). The distinct slope is noticeable\nwhen comparing the Z(v) polynomial equations in-air and underwater,\nbecause the underwater resolution is higher than in-air. The Y(Z)\npolynomial equations show the camera FOV in the Y direction as noted\nin the different equations for the top, middle and bottom image lines.\n\n200 400 600 800\n\n?150\n\n?100\n\n?50\n\n50\n\n100\n\n150\n\nv(pixel)\n\nZ\n(m\n\nm\n)\n\nZ(v) in Air\n\nBottom\nMiddle\nTop\n\n200 400 600 800\n\n?150\n\n?100\n\n?50\n\n50\n\n100\n\n150\n\nv(pixel)\n\nZ\n(m\n\nm\n)\n\nZ(v) Underwater\n\nBottom\nMiddle\nTop\n\nFigure 58: Z(v) polynomials examples from one of the top, middle and\nbottom image lines, in-air and underwater. Source: Author?s own work\n\n\n\n93\n\n?80 ?30 20 70\n\n?150\n\n?100\n\n?50\n\n50\n\n100\n\nZ(mm)\n\nY\n(m\n\nm\n)\n\nY(Z) in Air\n\nTop\nMiddle\nBottom\n\n?80 ?30 20 70\n\n?150\n\n?100\n\n?50\n\n50\n\n100\n\nZ(mm)\n\nY\n(m\n\nm\n)\n\nY(Z) Underwater\n\nTop\nMiddle\nBottom\n\nFigure 59: Y(Z) polynomials examples from one of the top, middle and\nbottom image lines, in-air and underwater. Source: Author?s own work\n\n4.7.1 Polynomial Adjustment Results\n\nPolynomial equations were adjusted in-air and underwater and\nthe results for each measured sphere standard and plane position are\nshown in Table 2 and in Table 3, respectively.\n\nTable 2: Underwater sphere measurement errors for the polynomial\nadjustment. The worst measurement for each parameter is highlighted\n\nPE (mm) STD (mm) SSE (mm)\n\nin-air Underwater in-air Underwater in-air Underwater\n\nCloser 1 0.58 1.08 0.10 0.17 0.17 0.06\n\nCloser 2 0.47 0.97 0.09 0.17 0.33 0.15\n\nStand-off 1 0.57 0.86 0.10 0.14 0.28 0.02\n\nStand-off 2 0.49 0.69 0.08 0.13 0.32 0.11\n\nFarther 1 0.50 0.66 0.09 0.11 0.20 0.25\n\nFarther 2 0.48 0.65 0.08 0.11 0.39 0.10\n\nThe results in Table 2 show that the PE is bigger underwater and\nthe SSE is bigger in-air. The underwater PE is worse in more distant\nacquisitions, one of the possible causes for this is the light absorption,\n\n\n\n94\n\nwhich intensifies the farther the object is. The SSE is expected to be\nbigger in-air, since the LTS has a better resolution underwater.\n\nTable 3: in-air and underwater plane measurement errors for the poly-\nnomial adjustment.\n\nRE (mm) STD (mm)\n\nIn-air Underwater In-air Underwater\n\nCloser 0.36 0.55 0.05 0.07\n\nCloser Rotated 0.80 0.75 0.08 0.12\n\nStandoff 0.34 0.62 0.06 0.10\n\nStandoff Rotated 0.49 0.41 0.09 0.06\n\nFarther 0.33 0.62 0.05 0.10\n\nFarther Rotated 0.36 0.51 0.06 0.08\n\nThe measurement results using the polynomial method for the\ndummy head, welding beads and PVC tube are presented in Figures\n60, 61 and 62. All the color map scales are presented with two standard\ndeviations.\n\nFigure 60: Dummy head measurements in-air and underwater using\nthe polynomial adjustment. Source: Author?s own work\n\n\n\n95\n\nFigure 61: Welding bead measurements in-air and underwater using\nthe polynomial adjustment. Source: Author?s own work\n\n(a) In Air (b) Underwater\n\nFigure 62: 300 mm diameter pipe measurements in-air and underwater\nwith the polynomial method. Source: Author?s own work\n\nThe dummy head and the welding beads have very similar color\nmaps in-air and underwater. This shows the sensor?s capability to\nmeasure more complex forms. These results are taken as a reference\n\n\n\n96\n\nfor this LTS configuration.\n\n4.8 PROPOSED CALIBRATION\n\nThe proposed calibration has two steps: one in-air sensor cali-\nbration and some underwater acquisitions. First, the intrinsic camera\ncalibration parameters are obtained and, after, the window of refraction\ndistance to the pinhole center is estimated.\n\nIn the first step, the same pattern used in the polynomial cali-\nbration is used for the in-air sensor calibration and the 3D points were\nobtained in the same way as in the polynomial calibration. First, the\nparameters of the camera are calibrated by direct optimization. Then,\nfor each planar pattern position, an image with the laser turned on\nis also captured. For each laser peak in image coordinates, a line is\nprojected using the camera matrix and its intersection with the planar\nstandard at a known Z position is computed. The procedure is repeated\nfor each standard position and a 3D point cloud that defines the laser\nplane is acquired.\n\nAfter the in-air calibration, a step standard was used to calibrate\nthe refractive window distance to the pinhole center. To validate this\nstep, the nominal distance from the window to the pinhole center is\napproximated by combining the camera calibrated focal length, i.e., the\ndistance from the camera imaging sensor to the lens focal point, and\nthe distance between the sensor and the window from the mechanical\ndesign. The distance from the CMOS to the inner side window is\n75.8 mm. According to the calibration, the focal length is 13.0 mm.\nSubtracting the mechanical distance by the focal length results in a\nrefractive window distance of 62,8 mm.\n\n4.8.1 Number of Underwater Images to Calibrate d\n\nThe estimated window distance variation with one and multiple\nimages were also evaluated. Up to 9 images were used for comparison:\nthree images from each position inside the measurement volume.\n\nThe results were compared using the spheres SSE parameter\nmean for all the acquired sphere positions. The step images were ac-\nquired closer to the sensor, at the stand-off and farther from the sensor.\nThe resulting window distance when combining only the images from\none region were also compared. The results are presented in Table 4.\n\n\n\n97\n\nTable 4: Relation between the number of step standard images used\nduring calibration and the refractive window distance\n\nD (mm) SSE (mm) Mean (mm) SSE (mm)\n\n46.3 0.09\n\nCloser 42.6 0.23 46.2 0.09\n\n46.2 0.09\n\n47.1 0.12\n\nStand-off 43.4 0.17 45.6 0.09\n\n45.6 0.09\n\n45.1 0.10\n\nFarther 47.8 0.16 47.8 0.16\n\n47.9 0.16\n\nThe result when using all the images is a window distance of\n46.3 mm, this is a 16.5 mm difference from the mechanical project\nestimation. The regions closer to the sensor and at the standoff have\nthe best estimation with the least number of images and three images\nfrom these areas were enough to estimate the optimal distance.\n\n4.8.2 Refraction between Glass and Air\n\nAlthough only one refraction is considered in the developed method,\ntwo refractions happen during acquisitions: between the air inside the\ncamera housing and the window glass and between the glass and the\nwater outside of the housing. One refraction approximation allows the\nsystem to be calibrated with the glass, bringing the calibration condi-\ntion closer to the measurement condition.\n\nTo measure the influence of this consideration, the spheres were\nalso measured considering two refractions. To consider this additional\nrefraction, the system was calibrated without the glass and the window\nthickness was measured. The window thickness was measured with a\nmicrometer in multiple positions and is 5.93\u00b10.02 mm. The glass index\nof refraction was considered 1.52.(III; VASCOTT, 2005)\n\nThe same nine step acquisitions used during the calibration with\n\n\n\n98\n\nrefraction were used to calibrate the glass window distance, resulting\nin a new distance d of 40.7 mm.\n\nAfter the calibrations, the spheres were measured and the results\ncomparison are on Table 5.\n\nTable 5: Measurement errors considering one or two refractions during\nspheres evaluation.\n\nOne Refraction Two Refractions\n\nPE (mm) SSE (mm) PE (mm) SSE (mm)\n\nCloser 1 0.07 0.11 0.07 0.05\n\nCloser 2 0.08 0.01 0.08 0.05\n\nStand-off 1 0.04 0.04 0.04 0.12\n\nStand-off 2 0.06 0.02 0.07 0.04\n\nFarther 1 0.06 0.19 0.06 0.29\n\nFarther 2 0.10 0.16 0.10 0.26\n\nMean 0.07 0.09 0.07 0.14\n\nThe mean resulting SSE for the one refraction consideration is\n36% smaller than the two refraction consideration. A probable cause\nis the addition of new variables that were not optimized during cali-\nbration: the window thickness and the glass index of refraction. These\nmay have been absorbed during the calibration in-air with the window.\n\n4.8.3 Proposed Method Results\n\nThe final window distance after a calibration using all nine step\nstandard images is 46.3 mm. The sphere and plane measurement errors\nare presented respectively in Table 6 and Table 7.\n\nThe results in Table 6 show a slightly worse PE underwater and\na worse SSE in-air. These results are similar to the polynomial adjust-\nment results.\n\nThe measurement results using the proposed method for the\ndummy head, welding beads and PVC tube are presented in Figures\n63, 64 and 65. The color maps are scaled to two standard deviations.\n\nSimilar to the polynomial results the color map of the dummy\n\n\n\n99\n\nTable 6: Underwater sphere measurement errors for the proposed\nmethod. The worst measurement for each parameter is highlighted\n\nPE (mm) STD (mm) SSE (mm)\n\nIn-air Underwater In-air Underwater In-air Underwater\n\nCloser 1 0.48 0.69 0.09 0.12 0.29 0.08\n\nCloser 2 0.47 0.66 0.08 0.14 0.34 0.06\n\nStand-off 1 0.51 0.51 0.08 0.12 0.35 0.09\n\nStand-off 2 0.44 0.51 0.08 0.11 0.45 0.02\n\nFarther 1 0.47 0.46 0.08 0.10 0.35 0.23\n\nFarther 2 0.36 0.48 0.07 0.10 0.44 0.18\n\nTable 7: Underwater plane measurement errors for the proposed\nmethod.\n\nRE (mm) STD (mm)\n\nIn-air Underwater In-air Underwater\n\nCloser 0.47 0.62 0.09 0.10\n\nCloser Rotated 0.52 0.59 0.08 0.10\n\nStandoff 0.37 0.50 0.07 0.09\n\nStandoff Rotated 0.62 0.45 0.12 0.08\n\nFarther 0.33 0.50 0.06 0.09\n\nFarther Rotated 0.42 0.47 0.08 0.08\n\nhead and the welding beads are very related.\n\n\n\n100\n\n(a) In-air (b) Underwater\n\nFigure 63: Dummy head measurements in-air (a) and underwater (b)\nwith the proposed method. Source: Author?s own work\n\nFigure 64: Welding bead measurements in-air and underwater using\nthe proposed method. Source: Author?s own work\n\n\n\n101\n\n(a) In-air (b) Underwater\n\nFigure 65: 300 mm diameter pipe measurements in-air (a) and under-\nwater (b) with the proposed method. Source: Author?s own work\n\n\n\n102\n\n4.8.4 Results Comparison\n\nThe full comparison is presented in Figure 66. The SSE and\nPE results are from the spheres. The values of the plane, dummy\nhead, welding beads (WB) and the tube PE are one standard deviation\nfor the measured object with the indicated method in the indicated\nenvironment.\n\n0 0.2 0.4 0.6 0.8 1\n\nSSE\n\nSpheres PE\n\nSpheres STD\n\nPlane RE\n\nPlane STD\n\nDummy\n\nWB\n\nPVC\n\nmm\n\nResults Comparison\n\nUnderwater Proposed\nUnderwater Polynomial\n\nIn Air Proposed\nIn Air Polynomial\n\nFigure 66: Comparison of all the measurement results. Source: Au-\nthor?s own work\n\nThe highest errors are presented in-air both with the polynomial\nand proposed methods while evaluating the SSE, however when com-\nparing the probing error for all the objects, the in-air measurements\nhad better results. Since in-air and underwater measurements have\ndifferent measurement volumes, the errors must be compared with the\n\n\n\n103\n\nvolume variation. In-air, the measurement length in the Z direction is\n330 mm and underwater is 220 mm. The percent error related to the\nmeasurement volume is presented in Table 8, which is the division of\nthe error by respective measurement length in the Z direction.\n\nTable 8: Spheres spacing percent error\n\nIn air Underwater\n\nPolynomial Proposed Polynomial Proposed\n\nSSE 0.12% 0.14% 0.12% 0.08%\n\nNevertheless, the uncertainty is more than two times higher in-\nair than underwater for the proposed method. This is hardly an ac-\nquisition error, since the probing error is lower in-air. One possible\ninfluence for this result for the proposed method is the underwater\nstep acquisition and subsequent calibration, which adds another vari-\nable to minimize the errors. The residual errors from the camera and\nlaser calibration are some of the strongest influences for these results.\n\n4.8.5 Uncertainty Evaluation for the Proposed Method Un-\n\nderwater\n\nTo further evaluate the proposed method underwater, the sources\nof uncertainty contributions to the error were weighted. In the proposed\nmethod, the temperature affects the water index of refraction and the\ndistance between the spheres? center. The sphere standard also has\na calibration uncertainty and the developed system has a resolution\nbetween object displacement and pixel displacement. Finally, since six\nmeasurements were made, the repeatability of these measurements were\nevaluated.\n\n? U? The temperature during measurements was considered to be\n25 \u00b1 5?C. This range causes an index of refraction variation of\n1.3315\u00b10.0005 (BASHKATOV; GENINA, 2003). This causes a vari-\nation of 0.06 mm in the distance between center measurements\nat the stand-off. This source is considered to have a uniform\ndistribution.\n\n? Ute This temperature range also causes a variation of \u00b10.011 mm\nin the standard dimension due to thermal expansion. This source\n\n\n\n104\n\nis considered to have a uniform distribution.\n\n? Res The resolution of a pixel in the Z direction at the stand-off\ndistance is 0.22 mm. In the Z direction, the sensor has a subpixel\npeak search, therefore the effective resolution is considered to be\none tenth of the pixel resolution.\n\n? Ustd The spheres were calibrated in a CMM, which has an un-\ncertainty of \u00b10.004mm for a 98.28 mm distance.\n\n? SSE The repeatability was considered to be the standard devia-\ntion of the 6 measurements. The correction is the mean distance\nerror.\n\nAll the results are shown in Table 9. C is the correction, U is the\nuncertainty, DoF is the degrees of freedom and total % in the source\nweight in the final value. The table was adjusted to 20?C since the\nmeasurements took place at the mean temperature of 25?C.\n\nTable 9: Proposed method underwater uncertainty evaluation\n\nSources of Uncertainty mm\n\nSymbol Uncertainty (\u00b1) C Distr. u ? Total %\n\nU? Index of refraction(?) 0.030 0.060 Uni. 0.017 ? 0.9%\n\nUte Standard Expansion 0.010 0.011 Uni. 0.006 ? 0.1%\n\nRes Resolution 0.011 - Uni. 0.006 ? 0.1%\n\nUstd Standard Calibration 0.002 - Norm. 0.002 ? -\n\nSSE Sphere Spacing Error 0.110 0.040 Norm. 0.110 5 37.6%\n\nPE Probing Error 0.140 - Norm. 0.140 ? 61.2%\n\nThe combination of these sources results in 35 degrees of freedom.\nThe expanded uncertainty is \u00b1 0.37 mm and the combined correction\nis +0.11 mm resulting in a maximum error of \u00b1 0.48 mm.\n\nThe cylindricity of the PVC tube and the real diameter were\nnot evaluated with another system. Therefore, the obtained diameters\ncannot be compared with a calibrated value.\n\nOverall, the proposed method and the polynomial adjustment\nmethod have very close underwater measurement uncertainties, but the\nproposed method has the advantage of fewer underwater acquisitions,\nbesides being more physically accurate.\n\n\n\n105\n\n4.8.6 Y Direction Measurement Length\n\nThe minimum measurement length in the Y direction which al-\nlows five measurements to cover a 300 mm diameter tube is 176.3 mm.\nTo confirm this designed LTS capability, the 300 mm diameter PVC\ntube was measured at the stand-off distance. The result is shown in\nFigure 67, which shows a length of 185.27 mm.\n\nFigure 67: Y direction measurement volume. Source: Author?s own\nwork\n\n\n\n106\n\n\n\n107\n\n5 CONCLUSIONS\n\nIn this work, a laser triangulation sensor (LTS) for underwater\nmeasurements was built aiming to fulfill the oil and gas industry?s need\nto measure underwater equipment. The main objective was to measure\na 300 mm diameter pipe with an uncertainty of half millimeter. The\nsensor was then built and two algorithms for calibration and measure-\nment were developed and evaluated by multiple object measurements.\n\nThe sensor characteristics are: a lens focal length of 12.5 mm,\n265 mm baseline, 35? triangulation angle, the measurement length in\nthe Z direction starts at 200 mm from the sensor and goes up to 420 mm\nand at the stand-off, the laser line measures 184 mm. This permits the\nmeasurement of one fifth of a 300 mm diameter tube transversal section\nand a safety distance from the object of interest. It is also modular,\nallowing a measurement volume change by replacing the positioning\nsheets.\n\nTwo methods for calibration and measurement with LTS un-\nderwater were compared: the polynomial equation adjustment and a\nproposed two step method.\n\nThe polynomial adjustment is based on a correlation between\nthe detected laser peak with the real 3D point. This requires a full un-\nderwater calibration with know standard displacements, since multiple\nnon-planar 3D points are required.\n\nThe second, proposed method requires an in-air calibration and\nsome underwater acquisitions of a step pattern. These underwater ac-\nquisitions are used to calibrate the window of refraction distance from\nthe camera pinhole center, thus allowing the ray tracing from the cam-\nera to the laser plane, resulting in a measured 3D point.\n\nThe methods were evaluated by measuring multiple objects re-\nsulting in both having very similar underwater results. A standard\nwith two spheres with calibrated distance between centers was the main\nevaluated object. After six acquisitions, both methods had a mean un-\nderwater distance between their centers error of 0.08 mm along the\nmeasurement and the proposed method had a maximum error of 0.48\nmm. Apart from the spheres, multiple objects with varying shapes were\nmeasured having very similar results for both in air and underwater.\n\nWhen comparing the in-air and underwater results, both meth-\nods also had similar results: better probing errors in-air, but better\nsphere spacing errors underwater. One cause for this effect is the mea-\nsurement volume variation, since the measurement volume is smaller\n\n\n\n108\n\nunderwater, leading to a higher resolution. Another probable cause for\nthis result considering the proposed method is the additional images\nused during underwater calibration.\n\nUltimately, the designed sensor and the algorithms have proven\nto be suitable for the designed application. The proposed method accu-\nrately performs the ray tracing being applicable in other configurations.\n\n5.1 FUTURE WORKS\n\nBoth the system and algorithms still have room for improvement.\nConsidering the proposed method, more parameters can be cal-\n\nibrated in situ within the underwater acquisition, such as the water\nindex of refraction, lowering the sources of uncertainties from this esti-\nmation.\n\nThe step standard itself can be improved, because the built qual-\nity influences the refraction window distance estimation. A standard\nwith a small parallelism error between the two steps may result in a\nmore reliable estimation and may not need several images to grant the\noptimum distance.\n\nThe sensor can be mounted to a displacement system such as an\nROV and algorithms of integration between the ROV locating system\ncan be developed resulting in an out-of-laboratory measurement.\n\nAdditionally, since the sensor is modular, new configurations of\nthe baseline, lens, camera, laser and triangulation angle can be tested\nfor further evaluations.\n\n\n\n109\n\nREFERENCES\n\nAGRAWAL, A.; RAMALINGAM, S.; TAGUCHI, Y.; CHARI, V. A\ntheory of multi-layer flat refractive geometry. In: Computer Vision\nand Pattern Recognition (CVPR), 2012 IEEE Conference on. [S.l.:\ns.n.], 2012. p. 3346\u20133353. ISSN 1063-6919.\n\nBASHKATOV, A. N.; GENINA, E. A. Water refractive index in\ndependence on temperature and wavelength: a simple approximation.\nIn: INTERNATIONAL SOCIETY FOR OPTICS AND PHOTONICS.\nSaratov Fall Meeting 2002: Optical Technologies in Biophysics and\nMedicine IV. [S.l.], 2003. p. 393\u2013395.\n\nBESL, P. J. Active, optical range imaging sensors. Machine vision and\napplications, Springer, v. 1, n. 2, p. 127\u2013152, 1988.\n\nBIANCO, G.; GALLO, A.; BRUNO, F.; MUZZUPAPPA, M.\nA comparative analysis between active and passive techniques\nfor underwater 3d reconstruction of close-range objects. Sensors,\nMultidisciplinary Digital Publishing Institute, v. 13, n. 8, p.\n11007\u201311031, 2013.\n\nBLUEVIEW BV5000 Harbor Scan. 2017. https://www.youtube.\ncom/watch?v=MWL3mBOUk-I. Accessed: 2017-04-05.\n\nBLUVIEW 3D Multibeam Scanning Sonar. 2017. http://www.\nblueview.com/products/3d-multibeam-scanning-sonar/3/.\nAccessed: 2017-04-05.\n\nBROWN, D. C. Close-range camera calibration. PHOTOGRAMMET-\nRIC ENGINEERING, v. 37, n. 8, p. 855\u2013866, 1971.\n\nBRUNO, F.; BIANCO, G.; MUZZUPAPPA, M.; BARONE, S.;\nRAZIONALE, A. Experimentation of structured light and stereo vision\nfor underwater 3d reconstruction. ISPRS Journal of Photogrammetry\nand Remote Sensing, Elsevier, v. 66, n. 4, p. 508\u2013518, 2011.\n\nBUSCHINELLI, P. D.; MATOS, G.; PINTO, T.; ALBERTAZZI, A.\nUnderwater 3d shape measurement using inverse triangulation through\ntwo flat refractive surfaces. In: IEEE. OCEANS 2016 MTS/IEEE\nMonterey. [S.l.], 2016. p. 1\u20137.\n\n\n\n110\n\nCACCIA, M. Laser-triangulation optical-correlation sensor for rov\nslow motion estimation. IEEE Journal of Oceanic Engineering, IEEE,\nv. 31, n. 3, p. 711\u2013727, 2006.\n\nCHANTLER, M. J.; CLARK, J.; UMASUTHAN, M. Calibration and\noperation of an underwater laser triangulation sensor: the varying\nbaseline problem. Optical Engineering, International Society for Optics\nand Photonics, v. 36, n. 9, p. 2604\u20132611, 1997.\n\nCHOOSING a 3D vision system for automated robotics\napplications. 2017. http://www.vision-systems.com/\narticles/print/volume-19/issue-11/features/\n\nchoosing-a-3d-vision-system-for-automated-robotics-applications.\n\nhtml. Accessed: 2017-04-05.\n\nCODAOCTOPUS Sound Underwater Inteligence. 2017. http:\n//www.codaoctopus.com/product-list/3d. Accessed: 2017-04-05.\n\nCOIRAS, E.; PETILLOT, Y.; LANE, D. M. Multiresolution 3-d\nreconstruction from side-scan sonar images. IEEE Transactions on\nImage Processing, IEEE, v. 16, n. 2, p. 382\u2013390, 2007.\n\nCOMPACT laser scanner for profile transmission. 2017.\nhttp://www.micro-epsilon.com/2D_3D/laser-scanner/\n\nscanCONTROL-compact/. Accessed: 2017-04-05.\n\nCRISP real-time 3D sonar imagery. 2017. http://www.codaoctopus.\ncom/products/use. Accessed: 2017-04-05.\n\nDEY, P. K.; OGUNLANA, S. O.; NAKSUKSAKUL, S. Risk-based\nmaintenance model for offshore oil and gas pipelines: a case study.\nJournal of Quality in Maintenance Engineering, Emerald Group\nPublishing Limited, v. 10, n. 3, p. 169\u2013183, 2004.\n\nDIVISION, U. of California (1868-1952). Division of W. R.\nS. D.; DIVISION, C. U. D. of W. R. S. D.; ECKART, C.\nPrinciples of Underwater Sound. Research Analysis Group,\nCommittee on Undersea Warfare, National Research Council,\n1946. (Summary technical report of the National Defense Research\nCommittee: Summary technical report of Division 6, NDRC).\n<https://books.google.com.br/books?id=9kQrAAAAYAAJ>.\n\nDORSCH, R. G.; HA?USLER, G.; HERRMANN, J. M. Laser\ntriangulation: fundamental uncertainty in distance measurement.\n\n\n\n111\n\nApplied Optics, Optical Society of America, v. 33, n. 7, p. 1306\u20131314,\n1994.\n\nDRAP, P.; SEINTURIER, J.; SCARADOZZI, D.; GAMBOGI, P.;\nLONG, L.; GAUCH, F. Photogrammetry for virtual exploration\nof underwater archeological sites. In: Proceedings of the 21st\ninternational symposium, CIPA. [S.l.: s.n.], 2007. p. 1e6.\n\nFLITNEY, R. K. Seals and sealing handbook. [S.l.]: Elsevier, 2011.\n\nGAN, Z.; TANG, Q. Visual Sensing and Its Applica-\ntions: Integration of Laser Sensors to Industrial Robots.\nZhejiang University Press, 2011. (Advanced topics in\nscience and technology in China). ISBN 9787308080514.\n<https://books.google.com.br/books?id=nmYemwEACAAJ>.\n\nGIBSON, R.; ATKINSON, R.; GORDON, J. A review of underwater\nstereo-image measurement for marine biology and ecology applications.\nOceanography and marine biology: an annual review, CRC Press,\nv. 47, p. 257\u2013292, 2016.\n\nGLASSNER, A. S. (Ed.). An Introduction to Ray Tracing. London,\nUK, UK: Academic Press Ltd., 1989. ISBN 0-12-286160-4.\n\nGUO, Y. 3d underwater topography rebuilding based on single beam\nsonar. In: IEEE. Signal Processing, Communication and Computing\n(ICSPCC), 2013 IEEE International Conference on. [S.l.], 2013.\np. 1\u20135.\n\nHARRIS, D. C. Materials for infrared windows and domes: properties\nand performance. [S.l.]: SPIE press, 1999.\n\nHARTLEY, R. I.; ZISSERMAN, A. Multiple View Geometry in\nComputer Vision. Second. [S.l.]: Cambridge University Press, ISBN:\n0521540518, 2004.\n\nHF12.5HA-1B. 2017. http://www.fujifilmusa.com/products/\noptical_devices/machine-vision/2-3-15/hf125ha-1b/. Accessed:\n2017-04-05.\n\nHORNER, D.; MCCHESNEY, N.; MASEK, T.; KRAGELUND, S.\n3D Reconstruction with an AUV Mounted Forward-Looking Sonar.\n[S.l.], 2009.\n\n\n\n112\n\nIII, T. P. S.; VASCOTT, T. High temperature glass melt property\ndatabase for process modeling. [S.l.]: Wiley-American Ceramic Society,\n2005.\n\nJI, Z.; LEU, M.-C. Design of optical triangulation devices. Optics &amp;\nLaser Technology, Elsevier, v. 21, n. 5, p. 339\u2013341, 1989.\n\nJI, Z.; LEU, M.-C. Design of optical triangulation devices. Optics &amp;\nLaser Technology, Elsevier, v. 21, n. 5, p. 339\u2013341, 1989.\n\nJORDT, A. Underwater 3D reconstruction based on physical models\nfor refraction and underwater light propagation. [S.l.]: Citeseer, 2014.\n\nKANG, L.; WU, L.; YANG, Y.-H. Experimental study of the influence\nof refraction on underwater three-dimensional reconstruction using\nthe svp camera model. Appl. Opt., OSA, v. 51, n. 31, p. 7591\u20137603,\nNov 2012.&lt;http://ao.osa.org/abstract.cfm?URI=ao-51-31-7591>.\n\nLOURAKIS, M. I. A brief description of the levenberg-marquardt\nalgorithm implemented by levmar. Foundation of Research and\nTechnology, v. 4, n. 1, 2005.\n\nMASSOT-CAMPOS, M.; OLIVER-CODINA, G. Optical sensors and\nmethods for underwater 3d reconstruction. Sensors, Multidisciplinary\nDigital Publishing Institute, v. 15, n. 12, p. 31525\u201331557, 2015.\n\nMCLEAN, J. W. High-resolution 3d underwater imaging. In:\nINTERNATIONAL SOCIETY FOR OPTICS AND PHOTONICS.\nSPIE\u2019s International Symposium on Optical Science, Engineering, and\nInstrumentation. [S.l.], 1999. p. 10\u201319.\n\nMIKS, A.; NOVAK, J.; NOVAK, P. Analysis of imaging for laser\ntriangulation sensors under scheimpflug rule. Optics express, Optical\nSociety of America, v. 21, n. 15, p. 18225\u201318235, 2013.\n\nMINI Structured Light Laser Diode Modules. 2017. https:\n//www.edmundoptics.com/lasers/laser-diode-modules/\n\nmini-structured-light-laser-diode-modules/2273. Accessed:\n2017-04-05.\n\nMQ013MG-E2. 2017. https://www.ximea.com/en/products/\nusb3-vision-cameras-xiq-line/mq013mg-e2. Accessed:\n2017-04-05.\n\n\n\n113\n\nNAIDU, D.; FISHER, R. B. A comparative analysis of algorithms for\ndetermining the peak position of a stripe to sub-pixel accuracy. In:\nBMVC91. [S.l.]: Springer, 1991. p. 217\u2013225.\n\nNEWTON, I.; BALDWIN, R.; FRYER, J. Underwater\nphotogrammetry. Non-Topographic Photogrammetry, American\nSociety for Photogrammetry and Remote Sensing: Bethesda, MD,\nUSA, p. 147\u2013166, 1989.\n\nOFFSHORE Laser Scanning for Asset Management. 2017.\nhttps://www.2grobotics.com/offshore/. Accessed: 2017-04-05.\n\nOPTISCHE 3D-Messsysteme: VDI-VDE-Handbuch\nMe\u00dftechnik. Bildgebende Systeme mit fla?chenhafter An-\ntastung : VDI-VDE 2634, Blatt 2 Entwurf. Blatt\n2. 2634,2E. Beuth, 2000. (VDI-VDE-Richtlinien).\n<https://books.google.com.br/books?id=mlrmZwEACAAJ>.\n\nPATHAK, K.; BIRK, A.; VASKEVICIUS, N. Plane-based registration\nof sonar data for underwater 3d mapping. In: IEEE. Intelligent Robots\nand Systems (IROS), 2010 IEEE/RSJ International Conference on.\n[S.l.], 2010. p. 4880\u20134885.\n\nPINHOLE Camera. 2017. https://commons.wikimedia.org/wiki/\nFile:Pinhole-camera.png. Accessed: 2017-04-05.\n\nQUAN, X.; FRY, E. S. Empirical equation for the index of refraction\nof seawater. Applied Optics, Optical Society of America, v. 34, n. 18,\np. 3477\u20133480, 1995.\n\nRIBAS, D.; RIDAO, P.; NEIRA, J.; TARDOS, J. D. Slam using an\nimaging sonar for partially structured underwater environments. In:\nIEEE. Intelligent Robots and Systems, 2006 IEEE/RSJ International\nConference on. [S.l.], 2006. p. 5040\u20135045.\n\nROSENBLUM, L.; KAMGAR-PARSI, B. 3d reconstruction of\nsmall underwater objects using high-resolution sonar data. In:\nIEEE. Autonomous Underwater Vehicle Technology, 1992. AUV\u201992.,\nProceedings of the 1992 Symposium on. [S.l.], 1992. p. 228\u2013235.\n\nSANTOLARIA, J.; AGUILAR, J.-J.; GUILLOMA?A, D.;\nCAJAL, C. A crenellated-target-based calibration method\nfor laser triangulation sensors integration in articulated\nmeasurement arms. Robotics and Computer-Integrated\nManufacturing, v. 27, n. 2, p. 282 \u2013 291, 2011. ISSN 0736-5845.\n\n\n\n114\n\nTranslational Research a??\u201c Where Engineering Meets Medicine.\n<http://www.sciencedirect.com/science/article/pii/S0736584510000876>.\n\nSANTOLARIA, J.; PASTOR, J. J.; BROSED, F. J.; AGUILAR,\nJ. J. A one-step intrinsic and extrinsic calibration method for\nlaser line scanner operation in coordinate measuring machines.\nMeasurement Science and Technology, v. 20, n. 4, p. 045107, 2009.\n<http://stacks.iop.org/0957-0233/20/i=4/a=045107>.\n\nSCAN Station P20. 2017. http://w3.leica-geosystems.com/\ndownloads123/hds/hds/ScanStation_P20/brochures-datasheet/\n\nLeica_ScanStation_P20_DAT_en.pdf. Accessed: 2017-04-05.\n\nSCHECHNER, Y. Y.; KARPEL, N. Recovery of underwater visibility\nand structure by polarization analysis. IEEE Journal of oceanic\nengineering, IEEE, v. 30, n. 3, p. 570\u2013587, 2005.\n\nSCOPOS Tailormade Solutions. 2017. http://www.scopos.no/\nindex.php/en/products/investigator.html. Accessed: 2017-04-05.\n\nSUBSEA, Deepwater Camera For Underwater Measurements.\n2017. https://www.youtube.com/watch?v=YqyKJCPxi7g. Accessed:\n2017-04-05.\n\nTAO, L.; CASTELLANI, U.; FUSIELLO, A.; MURINO, V. 3d\nacoustic image segmentation by a ransac-based approach. In: IEEE.\nOCEANS 2003. Proceedings. [S.l.], 2003. v. 2, p. 1098\u20131101.\n\nTREIBITZ, T.; SCHECHNER, Y. Y.; SINGH, H. Flat refractive\ngeometry. In: Computer Vision and Pattern Recognition, 2008. CVPR\n2008. IEEE Conference on. [S.l.: s.n.], 2008. p. 1\u20138. ISSN 1063-6919.\n\nTRUCCO, E.; FISHER, R. B.; FITZGIBBON, A. W. Direct\ncalibration and data consistency in 3-D laser scanning. [S.l.]:\nDepartment of Artificial Intelligence, University of Edinburgh, 1994.\n\nULS-500 PRO. 2017. https://www.2grobotics.com/products/\nunderwater-laser-scanner-uls-500/. Accessed: 2017-04-05.\n\nWALDRON, D. L.; MULLEN, L. Underwater optical ranging: A\nhybrid lidar-radar approach. In: IEEE. OCEANS 2009, MTS/IEEE\nBiloxi-Marine Technology for Our Future: Global and Local\nChallenges. [S.l.], 2009. p. 1\u20137.\n\nWOZNIAK, B.; DERA, J. Light absorption in sea water. [S.l.]:\nSpringer, 2007.\n\n\n\n115\n\nZHANG, Z. Flexible camera calibration by viewing a plane from\nunknown orientations. In: IEEE. Computer Vision, 1999. The\nProceedings of the Seventh IEEE International Conference on. [S.l.],\n1999. v. 1, p. 666\u2013673."}]}}}