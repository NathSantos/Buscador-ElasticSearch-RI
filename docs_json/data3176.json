{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.08379"}, {"@name": "filename", "#text": "13215_TacianaKisakiOliveiraShimizu_revisada.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "Taciana Kisaki Oliveira Shimizu \nTese de Doutorado do Programa Interinstitucional de P\u00f3s-Gradua\u00e7\u00e3o \nem Estat\u00edstica (PIPGES) \n\nPenalized regression methods for compositional data \n\n\n\nSERVI\u00c7O DE P\u00d3S-GRADUA\u00c7\u00c3O DO ICMC-USP\n\nData de Dep\u00f3sito:\n\nAssinatura: ______________________\n\nTaciana Kisaki Oliveira Shimizu\n\nPenalized regression methods for compositional data\n\nDoctoral dissertation submitted to the Instituto de\n\nCi\u00eancias Matem\u00e1ticas e de Computa\u00e7\u00e3o \u2013 ICMC-\n\nUSP and to the Departamento de Estat\u00edstica \u2013 DEs-\n\nUFSCar, in partial fulfillment of the requirements for\n\nthe degree of the Doctorate joint Graduate Program in\n\nStatistics DEs-UFSCar/ICMC-USP. FINAL VERSION\n\nConcentration Area: Statistics\n\nAdvisor: Prof. Dr. Francisco Louzada Neto\n\nUSP \u2013 S\u00e3o Carlos\n\nFebruary 2019\n\n\n\nFicha catalogr\u00e1fica elaborada pela Biblioteca Prof. Achille Bassi \ne Se\u00e7\u00e3o T\u00e9cnica de Inform\u00e1tica, ICMC/USP, \n\ncom os dados inseridos pelo(a) autor(a)\n\n                                       Bibliotec\u00e1rios respons\u00e1veis pela estrutura de cataloga\u00e7\u00e3o da publica\u00e7\u00e3o de acordo com a AACR2: \n                                       Gl\u00e1ucia Maria Saia Cristianini - CRB - 8/4938 \n                                       Juliana de Souza Moraes - CRB - 8/6176\n\nS555p\nShimizu, Taciana Kisaki Oliveira\n   Penalized regression methods for compositional\ndata / Taciana Kisaki Oliveira Shimizu; orientador\nFrancisco Louzada. -- S\u00e3o Carlos, 2019.\n   95 p.\n\n   Tese (Doutorado - Programa Interinstitucional de\nP\u00f3s-gradua\u00e7\u00e3o em Estat\u00edstica) -- Instituto de Ci\u00eancias\nMatem\u00e1ticas e de Computa\u00e7\u00e3o, Universidade de S\u00e3o\nPaulo, 2019.\n\n   1. Compositional data. 2. Regression model. 3.\nIsometric logratio coordinates. 4. Variable\nselection. I. Louzada, Francisco, orient. II.\nT\u00edtulo. \n\n\n\nTaciana Kisaki Oliveira Shimizu\n\nM\u00e9todos de regress\u00e3o penalizados para dados\n\ncomposicionais\n\nTese apresentada ao Instituto de Ci\u00eancias\n\nMatem\u00e1ticas e de Computa\u00e7\u00e3o \u2013 ICMC-USP e\n\nao Departamento de Estat\u00edstica \u2013 DEs-UFSCar,\n\ncomo parte dos requisitos para obten\u00e7\u00e3o do t\u00edtulo\n\nde Doutora em Estat\u00edstica \u2013 Interinstitucional de\n\nP\u00f3s-Gradua\u00e7\u00e3o em Estat\u00edstica. VERS\u00c3O REVISADA\n\n\u00c1rea de Concentra\u00e7\u00e3o: Estat\u00edstica\n\nOrientador: Prof. Dr. Francisco Louzada Neto\n\nUSP \u2013 S\u00e3o Carlos\n\nFevereiro de 2019\n\n\n\n\n\nFor my husband, Marcelo (Hiro), who has always been a constant source of support and\nencouragement during the challenges of my life!\n\n\n\n\n\nACKNOWLEDGEMENTS\n\nFirstly, I would like to thank God for the gift of my life, providing protection and wisdom\nin all the moments of my life.\n\nTo my parents who devoted their love and care to my education. To my brothers who\nalways demonstrated love and fraternal fellowship.\n\nTo my husband Hiro, who has always supported me unconditionally in my personal and\nprofessional projects with love and patience.\n\nI am especially grateful to my supervisor Dr. Francisco Louzada for his patience,\ncomprehension, encouragement, professionalism and support in this study. I am also grateful to\nProfessor Adriano Kamimura Suzuki who always patient and willing to help in moments that I\nneeded.\n\nI am thankful to my family and my husband\u2019s family who helped us in crucial moments.\n\nI express my sincere gratitude to Elizabeth Mie Hashimoto who always supported me\nand encouraged me to go ahead.\n\nI would also like to thank the Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Estado de S\u00e3o Paulo\n(FAPESP) - processo no 2014/16147-3, for the financial support, which enabled me to carry out\nthe project.\n\n\n\n\n\n\u201cO destino \u00e9 uma quest\u00e3o de escolha.\u201d\n(Augusto Cury)\n\n\n\n\n\nRESUMO\n\nSHIMIZU, T. K. O. M\u00e9todos de regress\u00e3o penalizados para dados composicionais. 2019.\n95 p. Doctoral dissertation (Doctorate Candidate joint Graduate Program in Statistics DEs-\nUFSCar/ICMC-USP) \u2013 Instituto de Ci\u00eancias Matem\u00e1ticas e de Computa\u00e7\u00e3o, Universidade de\nS\u00e3o Paulo, S\u00e3o Carlos \u2013 SP, 2019.\n\nDados composicionais consistem em vetores conhecidos como composi\u00e7\u00f5es cujos componentes\ns\u00e3o positivos e definidos no intervalo (0,1) representando propor\u00e7\u00f5es ou fra\u00e7\u00f5es de um \u201ctodo\u201d,\nsendo que a soma desses componentes totalizam um. Tais dados est\u00e3o presentes em diferentes\n\u00e1reas, como na geologia, ecologia, economia, medicina entre outras. Desta forma, h\u00e1 um\ngrande interesse em ampliar os conhecimentos acerca da modelagem de dados composicionais,\nprincipalmente quando h\u00e1 a influ\u00eancia de covari\u00e1veis nesse tipo de dado. Nesse contexto, a\npresente tese tem por objetivo propor uma nova abordagem de modelos de regress\u00e3o aplicada\nem dados composicionais. A ideia central consiste no desenvolvimento de um m\u00e9todo balizado\npor regress\u00e3o penalizada, em particular Lasso, do ingl\u00eas least absolute shrinkage and selection\noperator, elastic net e Spike-e-Slab Lasso (SSL) para a estima\u00e7\u00e3o dos par\u00e2metros do modelo.\nEm particular, visionamos o desenvolvimento dessa modelagem para dados composicionais,\ncom o n\u00famero de vari\u00e1veis explicativas excedendo o n\u00famero de observa\u00e7\u00f5es e na presen\u00e7a de\ngrandes bases de dados, e al\u00e9m disso, quando h\u00e1 restri\u00e7\u00e3o na vari\u00e1vel resposta e nas covari\u00e1veis.\n\nPalavras-chave: Dados composicionais, modelo de regress\u00e3o, coordenadas log-raz\u00e3o isom\u00e9tri-\ncas, sele\u00e7\u00e3o de vari\u00e1veis.\n\n\n\n\n\nABSTRACT\n\nSHIMIZU, T. K. O. Penalized regression methods for compositional data. 2019. 95 p. Doc-\ntoral dissertation (Doctorate Candidate joint Graduate Program in Statistics DEs-UFSCar/ICMC-\nUSP) \u2013 Instituto de Ci\u00eancias Matem\u00e1ticas e de Computa\u00e7\u00e3o, Universidade de S\u00e3o Paulo, S\u00e3o\nCarlos \u2013 SP, 2019.\n\nCompositional data consist of known vectors such as compositions whose components are\npositive and defined in the interval (0,1) representing proportions or fractions of a \u201cwhole\u201d,\nwhere the sum of these components must be equal to one. Compositional data is present in\ndifferent areas, such as in geology, ecology, economy, medicine, among many others. Thus,\nthere is great interest in new modeling approaches for compositional data, mainly when there\nis an influence of covariates in this type of data. In this context, the main objective of this\nthesis is to address the new approach of regression models applied in compositional data. The\nmain idea consists of developing a marked method by penalized regression, in particular the\nLasso (least absolute shrinkage and selection operator), elastic net and Spike-and-Slab Lasso\n(SSL) for the estimation of parameters of the models. In particular, we envision developing\nthis modeling for compositional data, when the number of explanatory variables exceeds the\nnumber of observations in the presence of large databases, and when there are constraints on the\ndependent variables and covariates.\n\nKeywords: Compositional data, regression model, isometric log-ratio coordinates, variable\nselection.\n\n\n\n\n\nLIST OF FIGURES\n\nFigure 1 \u2013 Estimation picture for the Lasso (left) and ridge regression (right). . . . . . 34\nFigure 2 \u2013 Spike and Slab distributions for l0 = 1,2,3 and l1 = 0.1. . . . . . . . . . . 36\nFigure 3 \u2013 ICMS series disaggregated in three economic sectors. . . . . . . . . . . . . 42\nFigure 4 \u2013 The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y1).\n\nThe colored points on the solution path represent the estimated values of the\ncoefficients. The vertical line (D) and (E) corresponds to the optimal model\nLasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 45\n\nFigure 5 \u2013 The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y2).\nThe colored points on the solution path represent the estimated values of the\ncoefficients. The vertical line (D) and (E) corresponds to the optimal model\nLasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 46\n\nFigure 6 \u2013 The parameter estimation averaged over 1000 replicates assuming r = 0.2\nfor the covariance matrix (n = 100, p = 1000). . . . . . . . . . . . . . . . . 51\n\nFigure 7 \u2013 The parameter estimation averaged over 1000 replicates assuming r = 0.5\nfor the covariance matrix (n = 100, p = 1000). . . . . . . . . . . . . . . . . 52\n\nFigure 8 \u2013 The SSL solution paths (A, B, C). . . . . . . . . . . . . . . . . . . . . . . . 53\nFigure 9 \u2013 The Lasso solution path (D) and elastic net path (E). . . . . . . . . . . . . . 58\nFigure 10 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for healthy\n\npatients. The colored points on the solution path represent the estimated\nvalues of the coefficients. The vertical line (D) and (E) corresponds to the\noptimal model lasso and elastic net (cross-validation), respectively. . . . . . 59\n\nFigure 11 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for patients with\npathologies. The colored points on the solution path represent the estimated\nvalues of the coefficients. The vertical line (D) and (E) corresponds to the\noptimal model lasso and elastic net (cross-validation), respectively. . . . . . 60\n\nFigure 12 \u2013 The SSL solution paths (A, B, C) (for ilr(y1)). . . . . . . . . . . . . . . . . 68\nFigure 13 \u2013 The lasso solution path (D) and elastic net path (E) (for ilr(y1)). . . . . . . . 69\nFigure 14 \u2013 The SSL solution paths (A, B, C) (for ilr(y2)). . . . . . . . . . . . . . . . . 70\nFigure 15 \u2013 The lasso solution path (D) and elastic net path (E) (for ilr(y2)). . . . . . . . 71\nFigure 16 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y1).\n\nThe colored points on the solution path represent the estimated values of the\ncoefficients. The vertical line (D) and (E) corresponds to the optimal model\nlasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 72\n\n\n\nFigure 17 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y2).\nThe colored points on the solution path represent the estimated values of the\ncoefficients. The vertical line (D) and (E) corresponds to the optimal model\nlasso and elastic net (cross-validation), respectively. . . . . . . . . . . . . . 73\n\n\n\nLIST OF TABLES\n\nTable 1 \u2013 Example: Volleyball game score. . . . . . . . . . . . . . . . . . . . . . . . . 23\nTable 2 \u2013 Elementary logistic transformations of SD for RD?1. . . . . . . . . . . . . . 25\nTable 3 \u2013 Averages of some performance measures for penalized methods with compo-\n\nsitional response variable (ilr(y1)). . . . . . . . . . . . . . . . . . . . . . . . 40\nTable 4 \u2013 Averages of some performance measures for penalized methods with compo-\n\nsitional response variable (ilr(y2)). . . . . . . . . . . . . . . . . . . . . . . . 41\nTable 5 \u2013 Averages of some performance measures for penalized methods with compo-\n\nsitional covariates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\nTable 6 \u2013 Averages of some performance measures for penalized methods with compo-\n\nsitional dependent variables and covariates (ilr(y1)). . . . . . . . . . . . . . . 63\nTable 7 \u2013 Averages of some performance measures for penalized methods with compo-\n\nsitional dependent variables and covariates (ilr(y2)). . . . . . . . . . . . . . . 64\n\n\n\n\n\nLIST OF ABBREVIATIONS AND ACRONYMS\n\nalr additive logratio\n\nBMI body mass index\n\nclr centered logratio\n\nEM Expectation-Maximization\n\nFN false negative\n\nFP false positive\n\nHAM Hamming\n\nilr isometric logratio\n\nLARS Least Angle Regression\n\nMCMC Markov Chain Monte Carlo\n\nMSE Mean Square Error\n\nOLS Ordinary Least Squares\n\nPA phase angle\n\nSCAD Smoothly Clipped Absolute Deviations\n\nSSL Spike-and-Slab Lasso\n\nWHO World Health Organization\n\n\n\n\n\nCONTENTS\n\n1 INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n2 PRELIMINARIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.1 Basic Concepts for compositional data . . . . . . . . . . . . . . . . . 27\n2.1.1 Principles of compositional analysis . . . . . . . . . . . . . . . . . . . 28\n2.1.1.1 Scale invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.1.1.2 Permutation invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.1.1.3 Subcompositional coherence . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.1.2 The Aitchison geometry . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n2.1.3 Logratio coordinates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n2.2 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.2.1 Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.2.1.1 Orthonormal design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.2.1.2 K-fold Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.2.1.3 Coordinate descent algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.2.2 Elastic net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.2.3 Spike-and-Slab Lasso . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n3 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL RE-\nSPONSE VARIABLES . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n3.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.2 Real data application - ICMS dataset . . . . . . . . . . . . . . . . . . 40\n3.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n4 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL CO-\nVARIATES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n\n4.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.2 Artificial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.3 Real data application - Brazilian children malnutrition dataset . . . 54\n4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\n5 PENALIZED REGRESSION MODEL FOR COMPOSITIONAL RE-\nSPONSE VARIABLES AND COVARIATES . . . . . . . . . . . . . . 61\n\n5.1 Simulation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n\n\n\n5.2 Toy example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5.3 Real data application - ICMS dataset . . . . . . . . . . . . . . . . . . 65\n5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n6 CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\nBIBLIOGRAPHY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n\nAPPENDIX A APPENDIX 1 . . . . . . . . . . . . . . . . . . . . . . . 81\n\n\n\n23\n\nCHAPTER\n\n1\n\nINTRODUCTION\n\nAppropriate study of the compositional data theory has been developed since the 1970s\nfrom the contributions of Aitchison and Shen (1980) and Aitchison (1982a). Since then, its\napplications have grown in different areas of knowledge, some examples include mineral com-\npositions of rocks or sediment compositions such as (sand, silt, clay) compositions in geology,\nspecies compositions of biological communities in ecology, household budget compositions in\neconomy, blood and urine compositions in medicine.\n\nCompositional data are vectors of proportions that specify D fractions as a whole.\nTherefore, for z = (z1,z2,...,zD)> to be a compositional vector, zi > 0 for i = 1,...,D and\nz1 + z2 + ...+ zD = 1.\n\nIn order to exemplify, we can describe compositional data as follows in the Table 1. The\nvector z in the simplex sample space as a composition (rows of the Table 1 - % attack, % block,\n% serve and % opponent\u2019s error of each match), the elements of such vector as components\n(columns of the Table 1) and the set of these vectors represent compositional data (Table 1)\n(AITCHISON, 1982b). Such data often result from the normalization of raw data or obtaining\n\nTable 1 \u2013 Example: Volleyball game score.\n\nMatch % attack % block % serve % opponent\u2019s error\n1 48.00 12.00 2.67 37.33\n2 53.06 14.29 7.14 25.51\n3 44.00 13.33 8.00 34.67\n4 52.63 14.74 7.37 25.26\n5 56.00 8.00 5.33 30.67\n6 65.63 10.16 2.34 21.88\n...\n\n...\n...\n\n...\n...\n\ndata as proportions of a certain heterogeneous quantity. Standard methods for multivariate\ndata analysis under the usual assumption of multivariate normal distribution (see, for example,\n\n\n\n24 Chapter 1. Introduction\n\n(JOHNSON; WICHERN, 1998)) are not appropriate for compositional data, due to compositional\nrestrictions.\n\nDifferent models have been adopted for the analysis of compositional data analysis. The\nfirst was the Dirichlet distribution; however, it requires the correlation structure to be wholly\nnegative, a fact that is not observed for compositional data, in which some correlations are\npositive (see, for example, (AITCHISON, 1982a; AITCHISON, 1986)).\n\nAn alternative for the analysis of compositional data was proposed by Aitchison Aitchison\n(1986), who considered suitable transformations from restricted sample space Simplex to well-\ndefined real sample space. More specifically, Aitchison and Shen Aitchison and Shen (1980)\ndeveloped the logistic-Normal class of distributions transforming the D component vector x into\na vector y in RD?1 and considering the additive logratio (alr) function.\n\nThe alr and centered logratio (clr) transformations were introduced by Aitchison (1986)\nin order to solve the constant sum constraint. These transformations are coordinates with respect\nto the Aitchison geometry (PAWLOWSKY-GLAHN; EGOZCUE; TOLOSANA-DELGADO,\n2015). However, a remarkable disadvantage about the clr transformation is that the variance\nmatrix of its transformed composition is unique. Furthermore, the alr coordinates are non-\nisometric and asymmetric and the clr coordinates are isometric and symmetric (CHEN; ZHANG;\nLI, 2017). Another transformation for compositional data is proposed by Egozcue et al. (2003)\ncalled isometric logratio (ilr), which is calculated with respect to a given orthogonal basis,\nallowing a simple manipulation of the geometric elements in the simplex sample space. Such a\ntransformation preserves all metric properties in the real coordinates. According to the Hron,\nFilzmoser and Thompson (2012), the ilr transformation provides a way to obtain an interpretation\nof the unknown parameters for regression model without constraints on the parameters.\n\nTable 2 presents other elementary transformations: multiplicative logistic and hybrid\nlogistic, besides the alr mentioned above.\n\nMore recently, some contributions about the theory and applications of compositional\ndata have been developed, for example, in Pawlowsky-Glahn and Buccianti (2011), Boogaart\nand Tolosana-Delgado (2013), Pawlowsky-Glahn, Egozcue and Tolosana-Delgado (2015). Hijazi\nand Jernigan (2009) made a comparison between the Dirichlet regression model and the alr\ntransformation to verify which one is better in the presence of the covariate.\n\nAitchison and Egozcue (2005) reported a bibliography review about statistical model-\ning for compositional data in the last twenty years, where one of the tantalizing problems in\ncompositional data was how to deal with the presence of components equal to zero. One of the\nfew articles which addressed this situation was proposed by Mart\u00edn-Fernandez, Barcel\u00f3-Vidal\nand Pawlowsky-Glahn (2003) who considered non-parametric imputation. Hijazi (2011) pro-\nposed a novel technique based on the Expectation-Maximization (EM) algorithm to replace the\ncomponents containing zeros.\n\n\n\n25\n\nTable 2 \u2013 Elementary logistic transformations of SD for RD?1.\n\nTransformations Inverses\n\nalr yi = ln\n?\n\nzi\nzD\n\n?\n\nmultiplicative logistic yi = ln\n\n0\n\nBBB\n@\n\nzi\n\n1?\ni\n\n\u00c2\nk=1\n\nzk\n\n1\n\nCCC\nA\n\nhybrid logistic\ny1 = ln\n\n?\nz1\n\n1?z1\n\n?\n;\n\nyi = ln\n\n2\n\n666\n4\n\nzi0\n\n@1?\ni?1\n\u00c2\nk=1\n\nzk\n\n1\n\nA\n\n0\n\n@1?\ni\n\n\u00c2\nk=1\n\nzk\n\n1\n\nA\n\n3\n\n777\n5\n\n, i = 2,...,D?1\n\nOn the other hand, the increase in large datasets whose dimensionality is much larger\nthan the sample size establishes new challenges for current methodology of compositional data.\nThe fact that there is a situation of a low relation between the number of dependent variables and\nthe sample size makes the standard analysis unsuitable for a regression model with compositional\ndata. According to this situation, the high collinearity among the covariates can also be seen,\nwhich is restricted by the dependence on each other. Whenever one of the above situations is\nconsidered, a problem of poor conditioning is observed, and a contraction can be considered in\norder to overcome this problem.\n\nThe advance of regularization techniques for variable selection and estimation in linear\nregression have received much attention from many authors to handle high-dimensional datasets\nand colinearity between the covariates of the model. Among the most popular penalization\napproaches are the Lasso (TIBSHIRANI, 1996), the elastic net (ZOU; HASTIE, 2005), the\nSmoothly Clipped Absolute Deviations (SCAD) (FAN; LI, 2001), among others and more\nrecently, the SSL (ROCKOV\u00c1; GEORGE, 2018).\n\nThe l1 regularization or Lasso, and its extensions, has become a popular method because\nit achieves a sparse solution (TIBSHIRANI, 1996). This method shrinks many coefficients\nexactly to zero through the fast optimization algorithms called Least Angle Regression (LARS)\nand the cyclic coordinate descent proposed by Efron et al. (2004) and Friedman, Hastie and\nTibshirani (2010), respectively. Recently, Lin et al. (2014) have proposed an l1 regularization\nmethod for variable selection and estimation in high-dimensional linear models with constraints\nin the covariates, combining coordinate descent with the method of multipliers. Similar to Lasso,\nZou and Hastie (2005) proposed a new regularization and variable selection method that deal\nwith strong correlations among the covariates. The best advantage of the elastic net is that\nit incorporates the ridge penalty with the Lasso penalty. This combination performs feature\n\n\n\n26 Chapter 1. Introduction\n\nselection and works with multicollinearity in the dataset together, which reveals important\nattributes for the analysis where the number of observations is smaller than the number of\ncovariates in the model. The combination of ridge and Lasso performs feature selection and\nhandles multicollinearity within the dataset, which are important characteristics for analyzing\ndatasets with large numbers of features (many of which could be collinear) and relatively smaller\nnumber of observations.\n\nOn the other hand, a Bayesian alternative is to adapt the amount of shrinkage applied to\nthe hierarchical model with mixture Spike-and-Slab priors (GEORGE; MCCULLOCH, 1993).\nIn this context, the Spike-and-Slab prior has been an important tool for most Bayesian variable\nselection (CHIPMAN, 1996; ROCKOV\u00c1; GEORGE, 2014). Some studies have applied Spike-\nand-Slab variable selection approaches using the mixture normal priors on coefficients by\nMarkov Chain Monte Carlo (MCMC) algorithms (see for example, Ishwaran and Rao (2005);\nShelton et al. (2015); among others). Although widely practical, the MCMC methods have a high\ncomputational cost. Moreover, such methods cannot perform a variable selection, and the mixture\nof normal priors does not shrink coefficients towards zero. An EM algorithm was developed\nby Rockov\u00e1 and George (2014) to apply in large-scale linear models with the mixture normal\npriors. Recently, Rockov\u00e1 and George (2018) developed a new structure for high-dimensional\nnormal linear models; the so-called SSL. Under this model, a new prior was applied to the\ncoefficients, that is, the Spike-and-Slab mixture double-exponential distribution. The SSL is a\nfast-computable approximation to mode detection under the Spike-and-Slab mixture of a point\nmass at 0 and ensures significant theoretical and practical properties. The SSL method applied to\nCox models and generalized linear models have received some attention in the literature, as can\nbe seen in Tang et al. (2017b) and Tang et al. (2017a), respectively. However, to the best of our\nknowledge, the shrinkage methodology for compositional data needs to be developed further.\n\nIn this context, the main objective of this thesis is to introduce a proposal for regression\nmodels based on regularization methods such as Lasso, elastic net and SSL, where responses\nand/or covariates have a compositional character. It is worth pointing out that the study with\nusing these data is challenging due to the dependence and absence of parametric classes in the\nsimplex sample space.\n\nThe remainder of this work is organized as follows. Chapter 2 introduces the prelimi-\nnaries of some important topics of compositional data and the shrinkage methods adopted for\nthe analysis. Chapter 3 presents the penalized regression model for compositional dependent\nvariables under the application of Lasso, elastic net and SSL methods for penalization. Chapter 4\npresents a penalized regression model when the restriction of compositional data exists in the\ncovariates. Chapter 5 presents the case of compositional constraints on both dependent variables\nand covariates where the regularization methods applied are Lasso, elastic net and SSL. Finally,\nChapter 6 draws some general conclusions and possible extensions of this current work.\n\n\n\n27\n\nCHAPTER\n\n2\n\nPRELIMINARIES\n\nIn this Chapter, we present a literature review of some important topics and properties\nabout basic operations in the methodology of compositional data, logratio coordinates and some\nregularization methods which were applied in the proposed models.\n\n2.1 Basic Concepts for compositional data\nInitially, we start by defining compositional data. According to Pawlowsky-Glahn,\n\nEgozcue and Tolosana-Delgado (2015), a column vector zzz = (z1,z2,...,zD)> is a D?part com-\nposition when all the components are positive real numbers and carry only relative information.\n\nAn important operation called closure assigns a constant sum representative to a compo-\nsition. It divides each component of a vector by the sum of the components, rescaling the initial\nvector to the constant sum k. In mathematical terms, the definition is given by\n\nDefinition 1 (Closure). For any vector of D strictly positive real components, zzz = [z1,...,zD] 2\nRD+, zi > 0 for all i = 1,...,D, the closure of zzz to c > 0 is defined as\n\nC(zzz) =\n?\n\ncz1\n\u00c2Di=1 zi\n\n,...,\nczD\n\n\u00c2Di=1 zi\n\n?\n,\n\nwhere c is an arbitrary positive real number and is usually 1 (proportions) or 100 (%) depending\non the units of measurement.\n\nAn appropriate scaling factor can be used to represent compositional data as proportions.\nConsequently, we can assume compositional data as proportions, that is, as vectors of constant\nsum c.\n\nThe sample space of compositional data called simplex is denoted by\n\nSD = {zzz = (z1,z2,...,zD)> : zi > 0,i = 1,2,...,D;\nD\n\n\u00c2\ni=1\n\nzi = c},\n\n\n\n28 Chapter 2. Preliminaries\n\nMore specifically, we can define a vector zzz in the simplex sample space as a composition,\nthe elements of such vector as components and the set of these vectors represent compositional\ndata (AITCHISON, 1982a).\n\n2.1.1 Principles of compositional analysis\nThe definition of compositional data follows the natural principles of compositions and\n\nthey are called: scale invariance, permutation invariance and subcompositional coherence.\n\n2.1.1.1 Scale invariance\n\nScale invariance refers to when a composition has information only about relative\nvalues. According to Aitchison and Egozcue (2005), the concept is easily formalized into a\nstatement that all meaningful functions of a composition can be expressed in terms of a set of\ncomponent ratios. In other words, if a composition changes from parts per unit to percentages, for\nexample, the information carried is completely equivalent (PAWLOWSKY-GLAHN; EGOZCUE;\nTOLOSANA-DELGADO, 2015).\n\nDefinition 2 (Scale invariance). Let f (.) be a function defined on RD+. Such a function is scale\ninvariant if for any positive real value n 2 R+ and for any composition zzz 2 SD it satisfies\nf (n zzz) = f (zzz), that is, it yields the same result for all compositionally equivalent vectors.\n\n2.1.1.2 Permutation invariance\n\nThe concept of permutation invariance is that it provides the same results when the\ncomponents in the composition are changed (see Pawlowsky-Glahn, Egozcue and Tolosana-\nDelgado (2015) for a detailed discussion).\n\n2.1.1.3 Subcompositional coherence\n\nFinally, a definition for subcomposition is given by a subset of components or parts of\na composition. Thus, the subcompositional coherence can be summarized as: if we have two\ncompositions, in which one has full compositions and the other one a subcomposition of these\nfull compositions, the inference about the relations within the common parts should be the same\nresults, i.e., the scale invariance of the results is preserved within arbitrary subcompositions, that\nis, the ratios between any parts in the subcomposition are equal to the corresponding ratios in the\noriginal composition.\n\n2.1.2 The Aitchison geometry\nIn Euclidian geometry, we work with operations in vectors in real space. This geometry\n\nis familiar with its geometric structure because the real space is a linear vector space with a\nmetric structure. However, this geometry is not suitable for analyzing compositional data. A way\n\n\n\n2.1. Basic Concepts for compositional data 29\n\nto illustrate this statement is to consider four compositions [5,55,40], [15,45,40], [40,30,30],\n[50,20,30]. The difference between [5,55,40] and [15,45,40] is not the same as the difference\nbetween [40,30,30] and [50,20,30]. The Euclidean distance between them is the same, there is\na difference of 10 units both between the first and second components respectively. While in\nthe first case, the proportion in the first component is triplicated, in the second case, the relative\nincrease is about 25%. To describe compositional variability, it is more interesting to consider\nthis relative difference.\n\nThis is one of the reasons for dispensing the Euclidian geometry as an appropriate tool for\nanalyzing compositional data. Other problems might occur, such as those where outcomes finish\nup outside the sample space simplex, or when translating compositional vectors, or determining\njoint confidence regions for random compositions under assumptions of normality.\n\nA wise geometry is needed to deal with compositional data. Indeed, it is possible to\nobtain two operations that provide the simplex of a vector space structure. They are defined as\nperturbation and powering. The first one is like an addition in real space and the second one is\nlike a multiplication by a scalar in real space. These basic operations required for a vector space\nstructure of the simplex are defined below.\n\nDefinition 3 (Perturbation). Consider the compositions zzz,yyy 2 SD. The perturbation of zzz with yyy\nis given by\n\nzzz?yyy = C[z1y1,z2y2,...,zDyD] 2 SD.\n\nwhere C[.] is defined in Definition 1.\n\nDefinition 4 (Powering). Consider the compositions zzz,yyy 2 SD. The powering of zzz by a constant\na 2 R as the composition is given by\n\na ?zzz = C[za1 ,z\na\n2 ,...,z\n\na\nD] 2 S\n\nD.\n\nTo obtain a Euclidian vector space structure (PAWLOWSKY-GLAHN; EGOZCUE,\n2001), we take the following inner product with its related norm ||.||a and Aitchison distance\n(the subindex a stands for Aitchison).\n\nDefinition 5 (Aitchison inner product). Inner product of zzz,yyy 2 SD,\n\nhzzz,yyyia =\nD\n\n\u00c2\ni=1\n\nln\n?\n\nzi\ngm(zzz)\n\n??\nln\n\nyi\ngm(yyy)\n\n?\n,\n\nwhere gm(zzz) denotes the geometric mean of the components of zzz.\n\nDefinition 6 (Aitchison norm).\n||zzz||a =\n\np\nhzzz,zzzia.\n\n\n\n30 Chapter 2. Preliminaries\n\nDefinition 7 (Aitchison distance).\n\nda(zzz,yyy) = ||zzz yyy||a,\n\nwhere zzz yyy is equal to the perturbation zzz?((?1)?yyy).\n\n2.1.3 Logratio coordinates\nAitchison (1986) proposed transformations based on ratios, including alr transformation\n\nand clr transformation. By the Aitchison\u2019s approach, it is possible to give an algebraic-geometric\nfoundation and based on this framework, a transformation of coefficients is equivalent to ex-\npress observations in a different coordinate system (PAWLOWSKY-GLAHN; EGOZCUE;\nTOLOSANA-DELGADO, 2015). The principal logratio coordinates (alr, clr and ilr) are defined\nbelow. The alr transformation is defined as follows.\n\nDefinition 8 (Additive logratio coordinates). Let zzz = [z1,z2,...,zD] be a composition in SD and\nconsider zD as a reference part. Its alr transformation into RD?1 is\n\nalr(zzz) =\n?\n\nln\nz1\nzD\n\n,ln\nz2\nzD\n\n,...,ln\nzD?1\nzD\n\n?\n= zzz .\n\nTo recover zzz from zzz = [z1,z2,...,zD?1], the inverse alr transformation is given through by\nclosure definition\n\nzzz = alr?1(zzz ) = C[exp(z1),exp(z2),...,exp(zD?1),1].\n\nAs the reference part zD is in the denominator of the components logratio, the alr\ntransformation is not symmetric in the components. Another option of the reference part can be\nchosen, conducting it to different alr-transformations. On the other hand, alr coordinates cannot\ncompute the Aitchison inner products or distances in the standard Euclidean way, that is, the alr\ndoes not supply an isometry between SD and RD?1. Each part of the composition except for the\npart in the denominator of the alr is\n\nzi =\nexp(zi)\n\n1 + \u00c2D?1j=1 exp(zi)\n,\n\nwhere the denominator is the effect of the closure. Its term additive comes from the denominator,\nwhich is the sum of the exponentials.\n\nThe clr coordinates give the expression of a composition in centered logratio coefficients\n\nclr(zzz) =\n?\n\nln\nz1\n\ngm(zzz)\n,ln\n\nz2\ngm(zzz)\n\n,...,ln\nzD\n\ngm(zzz)\n\n?\n= xxx .\n\nThe clr transformation is symmetric in the components, but the sum of the components\nis zero. In addition, the covariance matrix of clr(zzz) is singular, that is, the determinant is zero.\n\n\n\n2.1. Basic Concepts for compositional data 31\n\nMoreover, the clr coefficients are not subcompositionally coherent, because the geometric mean\nof the parts of a subcomposition gm(zzz) is not necessarily equal to that of the full composition. A\nformal definition of the clr coefficients is given as follows.\n\nDefinition 9 (Centered logratio coefficients). For a composition zzz 2 SD, the clr coefficients are\nthe components of the unique vector xxx = [x1,x2,...,xD] = clr(zzz), satisfying the two conditions\n\nzzz = clr?1(xxx ) = C(exp(xxx )) and\nD\n\n\u00c2\ni=1\n\nxxx i = 0.\n\nThe ith clr coefficient is\nxi = ln\n\nzi\ngm(zzz)\n\n.\n\nThe more recent logratio coordinates are ilr coordinates. The main idea of ilr coordinates\nis to obtain an orthonormal basis on the simplex, and to apply the new coordinates in a linear\nregression model. There are many ways to construct such a basis (CHEN; ZHANG; LI, 2017).\nSpecifically, an example of a basis for compositional data is called sequential binary partitioning\n(HRON; FILZMOSER; THOMPSON, 2012). We obtain coordinates which are interpreted in\nterms of the included compositional parts. For a given matrix\n\nWWW D?(D?1) = (www1,www2,...,wwwD?1) =\n\n0\n\nBBBBBBBBBB\n@\n\nq\nD?1\n\nD 0 ... 0\n\n? 1p\nD(D?1)\n\nq\nD?2\nD?1 ... 0\n\n...\n...\n\n. . .\n...\n\n? 1p\nD(D?1)\n\n? 1p\n(D?1)(D?2)\n\n... 1p\n2\n\n? 1p\nD(D?1)\n\n? 1p\n(D?1)(D?2)\n\n... ? 1p\n2\n\n1\n\nCCCCCCCCCC\nA\n\n, (2.1)\n\nand\neeei = C(exp wwwi), i = 1,...,D?1,\n\nis the corresponding orthonormal basis eee1,...,eeeD?1 and then the transformation of the compo-\nsition zzz = (z1,...,zD)> 2 SD to the ilr coordinates ilr(zzz) = (ilr(zzz)1,...,ilr(zzz)D?1)> 2 RD?1 is\nobtained by\n\nvi = ilr(zzz)i =\nr\n\nD?i\nD?i + 1\n\nln\n\n0\n\n@ zi\nD?i\nq\n\n\u2019Dj=i+1 z j\n\n1\n\nA, i = 1,...,D?1. (2.2)\n\nThe inverse ilr transformation of (2.2) is given by\n\nz1 = exp\n\n(r\nD?1\n\nD\nv1\n\n)\n,\n\nzi = exp\n\n(\n?\n\ni?1\n\u00c2\nj=1\n\n1\np\n\n(D? j + 1)(D? j)\nv j +\n\nr\nD?i\n\nD?i + 1\nvi\n\n)\n, i = 2,...,D?1, (2.3)\n\nzD = exp\n\n(\n?\n\nD?1\n\u00c2\nj=1\n\n1\np\n\n(D? j + 1)(D? j)\nv j\n\n)\n.\n\n\n\n32 Chapter 2. Preliminaries\n\nConsidering the scale invariance property, the composition zzz (2.3) can be represented\nby vectors with a chosen constant sum constraint. An important relationship between ilr and clr\ncoordinates of composition zzz (CHEN; ZHANG; LI, 2017) is defined by\n\nilr(zzz) = WWW>D clr(zzz) = WWW\n>\nD log(zzz),\n\nwhere WWW D is defined in (2.1). Specifically, the first coordinates of ilr(zzz) and clr(zzz) present a\nlinear relation as\n\nilr(zzz)1 =\nr\n\nD\nD?1\n\nclr(zzz)1 =\n1\n\nD(D?1)\n\n?\nln\n?\n\nz1\nz2\n\n?\n+ ...+ ln\n\n?\nz1\nzD\n\n??\n.\n\nThe coordinate ilr(z)1 extracts all relative information regarding z1 and obtains the\nrelative contribution of z1 respecting all the other parts (HRON; FILZMOSER; THOMPSON,\n2012). Some properties of the ilr coordinates are expressed below to explain their potential\napplication and computation. Let the function ilr: SD ! RD?1 an isometry of vector spaces and\nthe asterisk (?) denotes coordinates in an orthonormal basis.\n\nProperty 1. Consider zzzh 2 SD, h = 1,2 and real constants a,g ,\n\n(a) ilr(a ?zzz1 ?g ?zzz2) = a.ilr(zzz1)+ g.ilr(zzz2) = a.zzz?1 + g.zzz\n?\n2;\n\n(b) hzzz1,zzz2ia = hilr(zzz1),ilr(zzz2)i = hzzz?1,zzz\n?\n2i;\n\n(c) ||zzz1||a = ||ilr(zzz1)|| = ||zzz?1||;\n\n(d) da(zzz1,zzz2) = d(ilr(zzz1),ilr(zzz2)) = d(zzz?1,zzz\n?\n2).\n\nIn this thesis, we applied the ilr coordinates to the compositional data in order to avoid\nnumerical problems in the context of linear models. For clr and ilr coordinates, the scalar product\nis preserved and they are isometric, but this fact is different with alr coordinates, that is,\n\nhzzz,yyyi = clr(zzz).clr>(yyy) = ilr(zzz).ilr>(yyy) 6= alr(zzz).alr>(yyy).\n\nA disadvantage of using the alr coordinates is that they should not be applied when there are\ndistances, angles and shapes involved. In addition, the clr coordinates provide singular covariance\nmatrices, a problem for estimation in linear models (BOOGAART; TOLOSANA-DELGADO,\n2013).\n\n2.2 Shrinkage Methods\nThe current section considers the three penalization methods used to develop the proposed\n\nmodels for compositional data. This involves the Lasso, elastic net and SSL.\n\n\n\n2.2. Shrinkage Methods 33\n\nFirst, we consider the generic and classical linear regression model\n\nyyy = XXX bbb + eee, (2.4)\n\nwhere yyy 2 Rn is a vector of responses, XXX 2 Rn?p is a regression matrix of p predictors, bbb =\n(b1,...,bp)> 2 Rp is a vector of unknown regression coefficients and eee 2 Rn is the independent\nnoise vector distributed as Nn(0,s 2IIIn) being IIIn an identity matrix with dimension n. To solve the\nestimation problem, the ordinary least squares (OLS) method is often used where the parameters\nare estimated by the minimization of the residual sum of squares ||yyy?XXX bbb ||22. This method can\nbe applied under specific conditions, that is, XXX>XXX is nonsingular and consequently we obtain\nb?bb = (XXX>XXX)?1XXX>yyy. However, problems with high-dimensional regression are common in a wide\nrange of applications, that is, when we have a large number of covariates p to a response of\ninterest, which exceeds the number of observations n, p > n or even p >> n.\n\n2.2.1 Lasso\nTibshirani (1996) proposed Lasso (least absolute shrinkage and selection operator)\n\nmethod or l1 regularization which has become very popular for high-dimensional estimation\nproblems taking into account its statistical accuracy for prediction and variable selection jointly\nwith its computational feasibility. Furthermore, its theoretical properties in high-dimensional\nregression are well-understood (LIN et al., 2014).\n\nLasso is known as a penalized likelihood approach that develops the methodology for\nl1-penalization in high-dimensional settings with desirable properties for p >> n problems. In\nsuch problems, lasso demonstrated its superiority compared to other existing methods.\n\nAssuming the regression model (2.4), the convex optimization problem with the applica-\ntion the Lasso is defined as\n\nb?bb = argmin\nb\n\n?\n1\n\n2n\n||yyy?XXX bbb ||22 + l ||bbb ||1\n\n?\n, (2.5)\n\nwhere l is a tuning parameter dealing with the amount of shrinkage, and ||.||2 and ||.||1 are the\nl2 and l1 norms, respectively. Not only does the l1 penalty shrink the coefficients toward zero, but\nit also has some advantages in relation to the classical Ordinary Least Squares (OLS) methods\nsuch as some criteria for model selection, resulting in convexity of the optimization problem and\nsolving large problems efficiently (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015).\n\nWhen the lasso estimates regression coefficients to zero, it is creating a sparse solution,\nthat is, only few of the regression coefficients are nonzero. This performance is important due to\nthe variable selection that determines relevant covariates showing the strongest effects. The Lasso\nresults in sparsity and ridge penalty is not sparse are presented in Figure ?? in a geometrical\npicture when there are only two parameters, which is given by the constraint interpretation of\ntheir penalties. We can observe that the Lasso estimate can be set to zero.\n\n\n\n34 Chapter 2. Preliminaries\n\nFigure 1 \u2013 Estimation picture for the Lasso (left) and ridge regression (right).\n\nSource: (HASTIE; TIBSHIRANI; FRIEDMAN, 2009).\n\n2.2.1.1 Orthonormal design\n\nFollowing Buhlmann and Geer (2011), the lasso estimator can be derived for an orthonor-\nmal design case. Assuming uncorrelated variables implies that XXX>i XXX j = 0 for each i 6= j and\n1\nn XXX\n\n>XXX = III p. Thus, the lasso estimator is given by\n\nb?bb = S(b?bb\nLSE\n\n; l ), (2.6)\n\nwhere S(.; l ) is the soft-thresholding operator\n\nS(t; l ) = sgn(t)(|t|?l )+ =\n\n8\n>>><\n\n>>>:\n\nt ?l , if t > l ,\n\n0, if |t| ? l ,\n\nt + l , if t &lt;?l ,\n\nwhere sgn denotes the sign of its argument (\u00b11), (t)+ = max(t,0) denotes the positive part and\nb?bb\n\nLSE\nis the OLS estimator for bbb .\n\n2.2.1.2 K-fold Cross-validation\n\nThe k-fold cross-validation scheme is commonly used to select a reasonable tuning\nparameter l for the Lasso estimator. First, we randomly divide the observations into k groups.\nOne group is fixed as the test set, and the k?1 groups are designated as a training set. The model\nis fitted to the training data for a range of values of l in a grid, and we predict the responses in\nthe test set based on each fitted model, saving the mean-squared prediction errors. We repeat\nthis process k times, where the k groups have a chance to be test data, in relation to k ?1 groups\nused as a training set. Thus, we capture k different estimates of the prediction error over a range\nof values of l (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015). The choice of k is usually 5 or\n10, where k = 10 is very common in the field of applied machine learning.\n\n\n\n2.2. Shrinkage Methods 35\n\n2.2.1.3 Coordinate descent algorithm\n\nBased on the estimator (2.5), there is no closed form expression for the estimates for\nthe lasso. Indeed, the optimization problem become a convex problem with inequality con-\nstraints (FRIEDMAN et al., 2007). Since the seminal work of Tibshirani (1996), computational\ndevelopments have been approached to obtain efficiency and solutions to solve the lasso problem.\n\nThe LARS algorithm was proposed by Efron et al. (2004), which is a useful and less\ngreedy version of traditional forward selection methods.\n\nOn the other hand, another fast and popular approach used for estimation in regularization\nmethods is the coordinate descent algorithm (FU, 1998), which has shown to be a strong\ncompetitor to the LARS algorithm (FRIEDMAN et al., 2007). The idea of the algorithm is to\nfix the penalty parameter l in the Lagrangian form (2.5) and optimize successively over each\nparameter, keeping the other parameters fixed at their actual values. For more details, see for\nexample Hastie, Tibshirani and Friedman (2009).\n\n2.2.2 Elastic net\nThe elastic net approach was proposed by Zou and Hastie (2005). According to the\n\nauthors, the method is similar to the lasso, in view of being a variable selection and continuous\nshrinkage. Therefore, this method selects groups of correlate variables.\n\nThus, the elastic net combines the ridge (HOERL; KENNARD, 1970) and lasso penalties\nto solve the following convex problem\n\nb?bb = argmin\nb\n\n?\n1\nn\n||yyy?XXX bbb ||22 + l\n\n?\n1\n2\n(1?a)||bbb ||2 + a||bbb ||1\n\n??\n,\n\nwhere a 2 [0,1] is an elastic net tuning parameter that controls the mixing between the l1 and\nl2 penalties. There are many alternatives of algorithms to solve the elastic net problem. Within\nthem, the coordinate descent is efficient due to the fact that the updates will be a simple extension\nof lasso (HASTIE; TIBSHIRANI; WAINWRIGHT, 2015).\n\n2.2.3 Spike-and-Slab Lasso\nUnder a Bayesian perspective, Rockov\u00e1 and George (2018) proposed the SSL for high-\n\ndimensional normal linear models, and showed that it has important properties.\n\nThe general form of the penalized likelihood approach estimates bbb is given by\n\nb?bb = arg max\nb2RD\n\n?\n?\n\n1\n2\n||yyy?XXX bbb ||22 + penl (bbb )\n\n?\n,\n\nwhere penl (bbb ) is a penalty function that prioritizes suitable solutions.\n\nThe SSL involves placing a mixture prior on the regression coefficients bbb , where each\nb j, j = 1,...,D is assumed a priori to be drawn from either a Laplacian \u201cSpike\u201d concentrated\n\n\n\n36 Chapter 2. Preliminaries\n\naround zero (and hence is considered negligible), or a diffuse Laplacian \u201cSlab\u201d (and hence may\nbe large). Thus, the hierarchical prior over bbb and the latent indicator variables ggg = (g1,...,gD) is\ngiven by\n\np(bbb |ggg) =\nD\n\n\u2019\nj=1\n\n[g jy1(bi)+(1?g j)y0(b j)], ggg ? p(ggg), (2.7)\n\np(ggg|q ) =\nD\n\n\u2019\nj=1\n\nq g j (1?q )1?g j , and qqq ? Beta(a,b),\n\nwhere y1(b j) = (l1/2)e?|b j|l1 is the Slab distribution, y0(b j) = (l0/2)e?|b j|l0 is the Spike\ndistribution (l1&lt;&lt;l0) and the beta-binomial prior has been used for the latent indicators. The\nFigure 2 illustrates the spike and slab distribution for different values of l0. In this thesis, we\n\nFigure 2 \u2013 Spike and Slab distributions for l0 = 1,2,3 and l1 = 0.1.\n\napplied two types of SSL penalties studied in Rockov\u00e1 and George (2018): separable SSL and\nnon-separable SSL. The first one is the separable SSL penalty that arises from an independent\nproduct prior (2.7), assuming q known, that is, it is fixed. Its definition is given below.\n\npenS(bbb |q ) =\nD\n\n\u00c2\nj=1\n\nr(b j|q ) = ?l1|b j|+\nD\n\n\u00c2\nj=1\n\nlog\n?\n\np?q (0)\np?q (b j)\n\n?\n,\n\nwhere\np?q (b j) =\n\nq y1(b j)\nq y1(b j)+(1?q )y0(b j)\n\nand\np?q (0) =\n\nq y1(0)\nq y1(0)+(1?q )y0(0)\n\n=\nq l1\n\nq (l1 ?l0)+ l0\n.\n\nAnother one is the non-separable SSL penalty with unknown variance proposed by\nMoran, Rockov\u00e1 and George (2018). This penalty treats the q as a random, avoiding the need\nfor cross-validation over q . Thus, the non-separable SSL penalty with q ? p(q ) is defined by\n\npenNS(bbb ) = log\n?\n\np(bbb )\np(000p)\n\n?\n= ?l1|bbb |+ log\n\n2\n\n4\n\nR q p\n\u2019pj=1 p\n\n?\nq (b j)\n\ndp(q )\nR q p\n\n\u2019pj=1 p\n?\nq (0)\n\ndp(q )\n\n3\n\n5.\n\nAll the penalized methods presented in this section were implemented in the software R\n(R Core Team, 2017). Some examples of the routines for each penalized method used in this\nwork were in Appendix A.\n\n\n\n37\n\nCHAPTER\n\n3\n\nPENALIZED REGRESSION MODEL FOR\nCOMPOSITIONAL RESPONSE VARIABLES\n\nIn this Chapter, we present the penalized regression model with restrictions in the\nresponse variable, that is, in the vector yyy.\n\nThe model into a multivariate regression problem with compositional response is defined\nas\n\nyyy = XXX bbb + eee, (3.1)\n\nwhere yyy is a vector (D ? 1) of compositional response variables, XXX is a matrix (D ? p) of p\ncovariates, where D is the number of the components, bbb = (b1,...,bp)> is a vector (p?1) of\nunknown parameters and eee is the noise vector with distribution ND(000,ID), with a known variance\ns2 = 1. The intercept of the model is not included, since the response and predictor variables\ncan be centered.\n\nBased on the principle of working in coordinates, we can rewrite the model (3.1) as\n\nyyy = XXX ?bbb + eee\n\nilr(yyy) = XXX bbb + ilr(eee), (3.2)\n\nwhere eee ? N(000D?1,Silr).\n\nHere, we assume the following estimators for bbb of the regression models with composi-\ntional responses focused on regularization methods presented in Section 2. We considered the\nLasso, elastic net, SSL with separable and non-separable penalty approaches, respectively, for\nthe model (3.2) as follows.\n\n1. Lasso:\nb?bb = argmin\n\nbbb\n\n?\n||ilr(yyy)?XXX bbb ||22/n + l ||bbb ||1\n\n?\n(3.3)\n\nwhere ||ilr(yyy)?XXX bbb ||22 = \u00c2\nn\ni=1(ilr(yyy)?XXX bbb )\n\n2 and ||bbb ||1 = \u00c2\np\nj=1 |b j|.\n\n\n\n38 Chapter 3. Penalized Regression Model for compositional response variables\n\n2. Elastic net:\n\nb?bb = argmin\nbbb\n\n?\n1\nn\n||ilr(yyy)?XXX bbb ||22 + l\n\n?\n1\n2\n(1?a)||bbb ||22 + a||bbb ||1\n\n??\n. (3.4)\n\n3. SSL with separable penalty (known variance):\n\nb?bb = argmax\nbbb2RD?1\n\n(\n?\n\n1\n2\n||ilr(yyy)?XXX bbb ||22 +\n\n\"\n?l1|bbb |+\n\np\n\n\u00c2\nj=1\n\nlog\n?\n\np?q (0)\np?q (b j)\n\n?#)\n. (3.5)\n\n4. SSL with non-separable penalty (unknown variance):\n\nb?bb = argmax\nbbb2RD?1\n\n8\n<\n\n:\n?\n\n1\n2\n||ilr(yyy)?XXX bbb ||22 +\n\n0\n\n@?l1|bbb |+ log\n\n2\n\n4\n\nR q p\n\u2019pj=1 p\n\n?\nq (b j)\n\ndp(q )\nR q p\n\n\u2019pj=1 p\n?\nq (0)\n\ndp(q )\n\n3\n\n5\n\n1\n\nA\n\n9\n=\n\n;\n. (3.6)\n\nFor the estimation of the bbb \u2019s, we implemented the estimators (3.3) and (3.4) through by\nR package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The algorithm used to find\nthe minimum was cyclical coordinate descent. Such algorithm computes a grid of possible value\nof l and a sequence of models related to the loss function is provided as output. One advantage\nof this algorithm is that it can implemented for generalized linear model. The estimators (3.5)\nand (3.6) were obtained through by R package SSLASSO (MORAN; ROCKOV\u00c1; GEORGE,\n2018). The coordinate descent algorithm is used to fit the sequence of models indexed by the\nregularization parameter l0.\n\n3.1 Simulation Analysis\nHere, we provided the simulation studies to investigate the efficacy of the penalized\n\nmethods for the regression model with compositional responses variables. We replicated the\nsimulation 1000 times and the results were summarized based on these replicates (Table 3 for\nilr(y1) and Table 4 for ilr(y2)). We generated a data matrix XXX from a normal distribution with\nmean 0 and s = 2 for each element of matrix XXX . The compositional response variable is generated\naccording to model (3.1), from a logistic normal distribution with mean 000D and covariance matrix\nS = (r|i? j|), with r = 0.2 and r = 0.5, for j = 1,...,D. We assume D = 3, that is, we have\n3 components (y1,y2,y3) of a composition. The fixed values for the parameters bbb\n\n?\n= (b?1 ,b\n\n?\n2 )\n\nwere b?1 = (?2,?1.5,?1,0,1,1.5,2,0,...,0)\n> and b?2 = (2,?1,?2.5,0,1,?1,0.5,0,...,0)\n\n>\n\nto q = 6 random directions (non-zero coefficients).\n\nWe assumed three scenarios with a different number of sample sizes and covariates:\n(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-\nparisons were the Mean Square Error (MSE) given by MSE(b?bb?) = Var(b?bb?)+(Bias(b?bb?)), where\nVar(b?bb?) is the variance of the estimates of bbb? and Bias(b?bb?) = b?bb?? bbb?; the number of false\npositive (FP), the number of false negative (FN), where positive and negative refer to nonzero\n\n\n\n3.1. Simulation Analysis 39\n\nand zero coefficients, respectively; and the Hamming (HAM) distance between the support of\nthe estimated bbb and the true bbb?, that is, suppose two vectors x = (1,0,0) and w = (0,1,0), the\nHAM distance d(x,w) (number of different elements) between this two vectors, being that in\nthis case d(x,w) = 2 because the first and second elements of these vectors are different from\neach other. In this way, lower values of HAM indicate better performance of the method. The\nTables 3 and 4 report the averages of these performance measures for the five penalized methods\nadopted in this work.\n\nAccording to the results in Tables 3 and 4, we can see that, in general, the SSL Separable\n(SSL(1,6/p) with s = 1 fixed) performs better than other methods in all settings based on the\nHAM distance (lower values), for p = 30,200 and 1000 covariates. Therefore, this method tends\nto select fewer FP compared with other penalized methods. Among the approaches studied, the\nelastic net has a worse performance in almost all settings.\n\n\n\n40 Chapter 3. Penalized Regression Model for compositional response variables\n\nTable 3 \u2013 Averages of some performance measures for penalized methods with compositional response\nvariable (ilr(y1)).\n\n(n, p) Method MSE FP FN HAM\nr =0.2\n\nSSL (l1, q )\n(50, 30) SSL (1, 0.8) with s =1 fixed 0.4845 0.1850 5.9550 6.2020\n\nSSL (1, 6/30) with s =1 fixed 0.4834 0.0020 6.0000 6.0020\nSSL (1, 6/30) with unknown s 0.4834 0.0030 5.0190 6.0030\nLasso 0.4854 4.1180 5.7020 10.3090\nElastic Net 0.4850 6.3010 5.4880 12.6020\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0733 1.9700 5.9540 7.9780\nSSL (1, 6/200) with s = 1 fixed 0.0725 0.0020 6.0000 6.0020\nSSL (1, 6/200) with unknown s 0.0769 31.9060 6.0000 38.0670\nLasso 0.0726 9.9130 5.7020 15.9760\nElastic Net 0.0726 16.4320 5.4880 22.5360\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0149 4.7620 5.9750 10.6770\nSSL (1, 6/1000) with s = 1 fixed 0.0145 0.0000 6.0000 6.0010\nSSL (1, 6/1000) with unknown s 0.0153 0.0160 6.0000 6.0160\nLasso 0.0145 12.2770 5.9280 18.2910\nElastic Net 0.0145 23.3260 5.8590 32.9320\n\nr =0.5\nSSL (l1, q )\n\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4645 1.6550 5.5600 7.7260\nSSL (1, 6/30) with s = 1 fixed 0.4523 0.1620 5.9530 6.1680\nSSL (1, 6/30) with unknown s 0.4500 0.0140 5.9960 6.0140\nLasso 0.4849 3.8260 5.0460 9.9790\nElastic Net 0.4551 7.4290 4.1100 13.7340\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0742 4.0100 5.8640 10.0330\nSSL (1, 6/200) with s = 1 fixed 0.0725 0.0090 6.0000 6.0090\nSSL (1, 6/200) with unknown s 0.0725 0.0090 6.0000 6.0090\nLasso 0.0726 9.0780 5.7120 15.1410\nElastic Net 0.0726 15.1490 5.5680 21.2220\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0152 7.2320 5.9640 13.2410\nSSL (1, 6/1000) with s = 1 fixed 0.0145 0.0010 6.0000 6.0010\nSSL (1, 6/1000) with unknown s 0.0145 0.0030 6.0000 6.0030\nLasso 0.0145 11.1680 5.9370 17.1780\nElastic Net 0.0145 21.5300 5.8740 27.5530\n\n3.2 Real data application - ICMS dataset\n\nThe following dataset was made available by the Secretaria da Fazenda of Sao Paulo\nState. The dataset consists of ICMS (Imposto sobre Circula\u00e7\u00e3o de Mercadorias e Presta\u00e7\u00e3o de\nServi\u00e7os), which is the main revenue source for the Brazilian states.\n\nThe challenge with this dataset is the development of models which are disaggregated in\nthree economic sectors: industry (y1), commerce (y2) and administered prices (y3). These sectors\n\n\n\n3.2. Real data application - ICMS dataset 41\n\nTable 4 \u2013 Averages of some performance measures for penalized methods with compositional response\nvariable (ilr(y2)).\n\n(n, p) Method MSE FP FN HAM\nr =0.2\n\nSSL (l1, q )\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4645 1.6550 5.5600 7.7260\n\nSSL (1, 6/30) with s = 1 fixed 0.4523 0.1620 5.9530 6.1680\nSSL (1, 6/30) with unknown s 0.4500 0.0140 5.9960 6.0140\nLasso 0.4547 5.5720 4.5270 11.8300\nElastic Net 0.4551 7.4290 4.1100 13.7340\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0749 11.9850 5.6220 18.0480\nSSL (1, 6/200) with s = 1 fixed 0.0677 0.1440 5.9950 6.1450\nSSL (1, 6/200) with unknown s 0.0723 8.3520 5.7350 14.3940\nLasso 0.0679 11.5980 5.6290 17.6650\nElastic Net 0.0678 19.5060 5.3710 25.6010\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0150 12.3520 5.9200 18.3610\nSSL (1, 6/1000) with s = 1 fixed 0.0135 0.0840 6.0000 6.0850\nSSL (1, 6/1000) with unknown s 0.0135 0.0000 6.0000 6.0000\nLasso 0.0136 14.8520 5.9120 20.8660\nElastic Net 0.0135 26.9050 5.8120 32.9320\n\nr =0.5\nSSL (l1, q )\n\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4525 0.4250 5.8870 6.4460\nSSL (1, 6/30) with s = 1 fixed 0.4502 0.0240 5.9960 6.0260\nSSL (1, 6/30) with unknown s 0.4501 0.0060 5.9980 6.0060\nLasso 0.4530 3.8230 4.9620 9.9770\nElastic Net 0.4541 8.4840 3.7240 14.8560\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0694 4.1730 5.8720 10.1910\nSSL (1, 6/200) with s = 1 fixed 0.0675 0.0040 5.9990 6.0040\nSSL (1, 6/200) with unknown s 0.0675 0.0050 5.9990 6.0050\nLasso 0.0678 13.9800 5.5570 20.0600\nElastic Net 0.0677 23.4400 5.3080 29.5410\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.0142 7.1470 5.9520 13.1520\nSSL (1, 6/1000) with s = 1 fixed 0.0135 0.0000 6.0000 6.0000\nSSL (1, 6/1000) with unknown s 0.0135 0.0010 6.0000 6.0010\nLasso 0.0136 17.7410 5.8700 23.7620\nElastic Net 0.0135 33.1620 5.7680 39.2000\n\nrepresent a linear combination, that is, they are defined as compositional data. The importance\nof disaggregating the sectors is to allow the government to forecast potential decreases in tax\ncollection and plan efficient actions. The data were extracted from August 2007 to April 2018,\nthat is, the sample size is n = 128 months. The Figure 3 presents the evolution of the ICMS\nseries disaggregated in these three economic sectors: industry, commerce and administered price.\n\n\n\n42 Chapter 3. Penalized Regression Model for compositional response variables\n\nComposition ICMS given the 3 main sectors\n\nTime\n\n0 20 40 60 80 100 120\n\n15\n00\n\n20\n00\n\n25\n00\n\n30\n00\n\n35\n00\n\n40\n00\n\n45\n00\n\nIndustry\nCommerce\nADM_Prices\n\nFigure 3 \u2013 ICMS series disaggregated in three economic sectors.\n\nThe exogenous variables or covariates provided by research institutes are:\n\n\u2022 log of Monthly Industrial Survey (IBGE) - X1;\n\n\u2022 log of Monthly Trade Survey - PMC/IBGE - X2;\n\n\u2022 log of Monthly energy consumption in Sao Paulo State - X3;\n\n\u2022 Index of Economic Activity of the Central Bank - X4;\n\n\u2022 IGP-DI/FGV \u2013 General Price Index - X5.\n\nBesides the covariates mentioned above, we also considered the lagged covariate in\nthe period of 12 months of the proportion of ICMS in the industry (X6) and commerce (X7)\nand 6 months of the proportion of ICMC in the industry (X8) and commerce (X9). The total\nof covariates in the model is p = 9. Figures 4 and 5 present the solution path by the SSL\n(non-adaptative choice (separable), fixed q ; non-adaptative oracle choice (separable); adaptative\nchoice, q ? B(1, p) (non-separable)), Lasso and elastic net methods for modeling the ICMS\ndisaggregated in 3 parts: industry, commerce and administered prices. These sectors represent\ncompositional data, once they are dependent on each other. Thereby, the results showed the same\nperformance for ilr(y1) and ilr(y2) when the SSL with separable penalties (Figures 4A, 4B, 5A\nand 5B), that is, these methods did not select any significant covariate for the model. On the other\nhand, SSL non-separable presented three significant covariates (X6, X7 and X8). The optimal\nl calculated by the 10-fold cross-validation were 0.0036 (ilr(y1)) and 0.0033 (ilr(y2)) for the\n\n\n\n3.3. Discussion 43\n\nLasso method and 0.0076 (ilr(y1)) and 0.0078 (ilr(y2)) for the elastic net method (vertical line in\nFigures 4D, 4E, 5D and 5E). Moreover, Lasso method selected the covariates X1, X2 and X5 and\nelastic net method besides these covariates also selected X3 considering the response variable\nilr(y1). These results present the significant exogenous variables when the approached methods\nare applied considering compositional restriction on the response variable.\n\nThe models for each applied method is given by\n\n1. SSL non-separable:\n\ny1 =0.268?ICMSindustry12months + 0.280?ICMScommerce12months\n\n+ 0.500?ICMSindustry6months\n\ny2 =?0.242?%ICMSindustry12months + 0.118?%ICMScommerce6months\n\n2. Lasso:\n\ny1 =0.002?MontlyIndustrialSurvey + 0.001?MontlyTradeSurvey?0.002?IGP?DI\n\ny2 =0.001?MontlyIndustrialSurvey + 0.004?MontlyTradeSurvey\n\n+ 0.003?Montlyenergyconsumption + 0.002?IGP?DI\n\n3. Elastic net:\n\ny1 =0.002?MontlyIndustrialSurvey + 0.002?MontlyTradeSurvey\n\n+ 0.003?Montlyenergyconsumption?0.002?IGP?DI\n\ny2 =0.003?MontlyTradeSurvey + 0.004?Montlyenergyconsumption + 0.003?IGP?DI\n\n3.3 Discussion\nIn this chapter, we presented a compositional regression model with restriction in the\n\nresponse variables under five penalties methods. We applied the ilr coordinates on the response\nvariables to remove the dependence among the components.\n\nA simulation study for the proposed model (3.2) showed that the model with SSL non-\nadaptative oracle choice (separable) performs better in terms of estimation if compared with the\nother penalized methods. It is noteworthy that this situation occurs in moderate dimensionality\nas in high-dimensionality.\n\nIn the case of application, the real data set involves the ICMC tax. As this data set is\nconsidered with moderate dimensionality, the SSL non-separable showed a better performance\nin relation to the other SSL penalties. The Lasso and elastic net estimators presented similar\nresults, with only a little difference between the optimal l . Based on these results, the SSL\nnon-separable method considered the lagged covariates X6, X7 and X8 significant, that is, the\n\n\n\n44 Chapter 3. Penalized Regression Model for compositional response variables\n\nproportion of ICMS in the industry, commerce in the period of 12 months and the proportion\nof ICMS in the industry in the period of 6 months are relevant to explain the response variable\nilr(y1) (proportion of the ICMS in the industry). On the other hand, the Lasso method considered\nonly exogenous covariates significant, which are Monthly Industrial Survey, Monthly Trade\nSurvey and IGP-DI/FGV General Price Index and for the elastic net method, besides these\ncovariates mentioned above, including also the covariate monthly energy comsuption in Sao\nPaulo State.\n\n\n\n3.3. Discussion 45\n\nFigure 4 \u2013 The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y1). The colored points\non the solution path represent the estimated values of the coefficients. The vertical line (D) and\n(E) corresponds to the optimal model Lasso and elastic net (cross-validation), respectively.\n\n\n\n46 Chapter 3. Penalized Regression Model for compositional response variables\n\nFigure 5 \u2013 The solution path SSL (A, B, C), Lasso (D) and elastic net (E) for ilr(y2). The colored points\non the solution path represent the estimated values of the coefficients. The vertical line (D) and\n(E) corresponds to the optimal model Lasso and elastic net (cross-validation), respectively.\n\n\n\n47\n\nCHAPTER\n\n4\n\nPENALIZED REGRESSION MODEL FOR\nCOMPOSITIONAL COVARIATES\n\nIn this section, we present the penalized regression model with compositional constraints\nin the covariates. The regression model based on methodology of compositional data is given by\n\nyyy = ilr(XXX)bbb + eee, (4.1)\n\nwhere yyy is a vector (l ? 1) of response variables, XXX is a matrix (l ? D) of D compositional\ncovariates, where l = 1,...,L and D is the number of the components, bbb = (b1,...,bD)> is a\nvector (D?1) of unknown parameters and eee is the noise vector with distribution Nl(000,Ip), with\na known variance s 2 = 1. The intercept of the model is not included, equal to model 3.1.\n\nBased on the principle of working in coordinates, we can rewrite the model (4.1) as\n\ny = hbbb ,XXXiA + eee\n\n= (ilr(bbb ),ilr(XXX))+ eee\n\n=\nD?1\n\u00c2\nk=1\n\nilr(b )kilrk(X)+ eee\n\n=\nD?1\n\u00c2\nk=1\n\nbkilrk(X)+ eee, (4.2)\n\nwith a vector of parameters bbb = bk that afterwards might be mapped back to a composition\nthrough the inverse ilr transformation.\n\nNow, we considered the following estimators for bbb of the regression models with com-\npositional covariates focused on regularization methods presented in Section 2. We considered\nthe lasso, elastic net, SSL with separable and non-separable penalty approaches, respectively, for\nthe model (4.2) as follows.\n\n\n\n48 Chapter 4. Penalized Regression Model for compositional covariates\n\n1. Lasso:\n\nb?bb = argmin\nbbb\n\n \n||yyy?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22/n + l ||bbb ||1\n\n!\n, (4.3)\n\nwhere ||yyy?\u00c2D?1k=1 bkilrk(Xi)||\n2\n2 = \u00c2\n\nn\ni=1(yi ?\u00c2\n\nD?1\nk=1 bkilrk(Xi))\n\n2 and ||bbb ||1 = \u00c2D?1j=1 |b j|.\n\n2. Elastic Net:\n\nb?bb = argmin\nbbb\n\n \n1\nn\n||yyy?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22 + l\n?\n\n1\n2\n(1?a)||bbb ||22 + a||bbb ||1\n\n?!\n. (4.4)\n\n3. SSL with separable penalty (known variance):\n\nb?bb = argmax\nbbb2RD?1\n\n(\n?\n\n1\n2\n||yyy?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22 +\n\"\n?l1|bbb |+\n\nD?1\n\u00c2\nj=1\n\nlog\n?\n\np?q (0)\np?q (b j)\n\n?#)\n. (4.5)\n\n4. SSL with non-separable penalty (unknown variance):\n\nb?bb = argmax\nbbb2RD?1\n\n8\n><\n\n>:\n?\n\n1\n2\n||yyy?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22 +\n\n0\n\nB\n@?l1|bbb |+ log\n\n2\n\n6\n4\n\nR q D?1\n\u2019D?1j=1 p\n\n?\nq (b j)\n\ndp(q )\nR q D?1\n\n\u2019D?1j=1 p\n?\nq (0)\n\ndp(q )\n\n3\n\n7\n5\n\n1\n\nC\nA\n\n9\n>=\n\n>;\n.\n\n(4.6)\n\nFor the estimation of the bbb \u2019s, we implemented the estimators (4.3) and (4.4) through\nby R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The estimators (4.5) and\n(4.6) were obtained through by R package SSLASSO (MORAN; ROCKOV\u00c1; GEORGE, 2018).\n\n4.1 Simulation Analysis\nWe provided the simulation studies to investigate the efficacy of the penalized methods\n\nfor a regression model with compositional covariates. We replicated the simulation 1000 times\nand the results were summarized based on these replicates (Table 5). We generated a data\ncompositional matrix XXX of covariates from a logistic normal distribution with mean 000D and S =\n(r|i? j|) with r = 0.2 and r = 0.5. The response is generated according to model (4.1) with b? =\n\n1p\n3\n(?2,?1.5,?1,0,1,1.5,2,0,...,0)> to q = 6 random directions (non-zeros coefficients).\n\nWe assumed three scenarios with a different number of sample size and covariates:\n(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-\nparisons were the MSE, the number of FP, the number of FN, and the HAM measure between\nthe support of the estimated b and the true b?. Table 5 reports the averages of these performance\nmeasures for the five methods adopted.\n\nAs can be seen in Table 5, the SSL Separable Oracle (1,6/p) for p = 30,200,1000,\nperforms better than other methods in all settings based on the mean squared error and false\npositives. The SSL Separable tends to select fewer false negatives in high dimensions, which is\n\n\n\n4.1. Simulation Analysis 49\n\nacceptable because the omission of important variables is more relevant than the inclusion of\nshrunk variables.\n\nFigures 6 and 7 present the estimates of coefficients over 1,000 replicates and the red\ncircle represents the true value of coefficients. For both settings, Figures 6B and 7B show better\naccurate estimations.\n\n\n\n50 Chapter 4. Penalized Regression Model for compositional covariates\n\nTable 5 \u2013 Averages of some performance measures for penalized methods with compositional covariates.\n\n(n, p) Method MSE FP FN HAM\nr =0.2\n\nSSL (l1, q )\n(50, 30) SSL Separable (1, 0.8) 0.0115 0.6190 0.0030 6.5620\n\nSSL Separable Oracle (1, 6/30) 0.0070 0.0290 0.0240 6.0250\nSSL (1, 6/30) with unknown s 0.0079 0.1260 0.0210 6.1290\nLasso 0.0258 4.1020 0.0030 10.1020\nElastic Net 0.0262 6.4000 0.0000 12.3990\n\n(100, 200) SSL Separable (1, 0.8) 0.0032 4.7320 0.0000 10.7320\nSSL Separable Oracle (1, 6/200) 0.0004 0.0030 0.0000 6.0030\nSSL (1, 6/200) with unknown s 0.0150 0.0180 1.7050 6.0180\nLasso 0.0030 7.0350 0.0000 13.0350\nElastic Net 0.0030 11.5890 0.0000 17.5890\n\n(100, 1000) SSL Separable (1, 0.8) 0.0010 7.4650 0.0000 13.4650\nSSL Separable Oracle (1, 6/1000) 0.0001 0.0000 0.0030 6.0000\nSSL (1, 6/1000) with unknown s 0.0096 0.0000 4.4750 6.0000\nLasso 0.0010 11.0290 0.0020 17.0290\nElastic Net 0.0010 16.8400 0.0020 22.8390\n\nr =0.5\nSSL (l1, q )\n\n(50, 30) SSL Separable (1, 0.8) 0.0184 0.5920 0.0430 6.5920\nSSL Separable Oracle (1, 6/30) 0.0165 0.0440 0.1840 6.0440\nSSL (1, 6/30) with unknown s 0.0192 0.1430 0.2130 6.1430\nLasso 0.0420 3.7790 0.0400 9.7790\nElastic Net 0.0407 6.0690 0.0200 12.0690\n\n(100, 200) SSL Separable (1, 0.8) 0.0050 4.6700 0.0000 10.6700\nSSL Separable Oracle (1, 6/200) 0.0007 0.0030 0.0100 6.0030\nSSL (1, 6/200) with unknown s 0.0092 0.0400 0.8090 6.0400\nLasso 0.0049 6.9680 0.0010 12.9680\nElastic Net 0.0049 11.9650 0.0000 17.9650\n\n(100, 1000) SSL Separable (1, 0.8) 0.0017 7.5630 0.0130 13.5630\nSSL Separable Oracle (1, 6/1000) 0.0002 0.0010 0.0510 6.0010\nSSL (1, 6/1000) with unknown s 0.0078 0.0000 3.6610 6.0000\nLasso 0.0015 10.6440 0.0250 16.6440\nElastic Net 0.0016 16.6880 0.0140 22.6880\n\n\n\n4.1. Simulation Analysis 51\n\n?=0.2\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?2.8 ?1.8 ?0.8 0.2 1.2 2.2\n \n\n \n\nSSL Separable (?=0.8)A\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?2.8 ?1.8 ?0.8 0.2 1.2 2.2\n \n\n \n\nSSL Separable (?=6/1000)B\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?2.8 ?1.8 ?0.8 0.2 1.2 2.2\n \n\n \n\nSSL Non?SeparableC\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?2.8 ?1.8 ?0.8 0.2 1.2 2.2\n \n\n \n\nLassoD\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?2.8 ?1.8 ?0.8 0.2 1.2 2.2\n \n\n \n\nElastic netE\n\nFigure 6 \u2013 The parameter estimation averaged over 1000 replicates assuming r = 0.2 for the covariance\nmatrix (n = 100, p = 1000).\n\n\n\n52 Chapter 4. Penalized Regression Model for compositional covariates\n\n?=0.5\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?3 ?2 ?1 0 1 2\n \n\n \n\nSSL Separable (?=0.8)A\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?3 ?2 ?1 0 1 2\n \n\n \n\nSSL Separable (?=6/1000)B\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?3 ?2 ?1 0 1 2\n \n\n \n\nSSL Non?SeparableC\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?3 ?2 ?1 0 1 2\n \n\n \n\nLassoD\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?\n\n?1\n\n?2\n\n?3\n\n?4\n\n?5\n\n?6\n\n?7\n\n?3 ?2 ?1 0 1 2\n \n\n \n\nElastic netE\n\nFigure 7 \u2013 The parameter estimation averaged over 1000 replicates assuming r = 0.5 for the covariance\nmatrix (n = 100, p = 1000).\n\n\n\n4.2. Artificial Data 53\n\n4.2 Artificial Data\n\nA way of illustrating the proposed model (4.2) to compare the SSL, Lasso and elastic\nnet penalties, we considered the following example. For n = 100 individuals with D = 1000\ncompositional covariates, we considered one generated sample of the simulation study presented\nin the last subsection.\n\nWe compared the SSL with a fixed variance with three settings: (i) separable choice\nq = 0.8, to verify the over-estimating of the true non-zero fraction 6/1000, (ii) separable oracle\nchoice q = 6/1000 and (iii) non-adaptative choice q ? B(1,D). The slab parameter was set to\nl1 = 0.1 and we used a ladder l0 2 I = {1,2,...,50} for the spike parameter. In addition, we\napplied the generated data to the lasso and elastic net penalties implemented in the R package\nglmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). For this approach, an optimal value of l\nwas selected by 10-fold cross-validation. According to Figures 8 and 9, we can see the solution\n\nFigure 8 \u2013 The SSL solution paths (A, B, C).\n\npaths for the five settings when we have the structure of compositional covariates. Each line\nrepresents a single regression coefficient and the horizontal dotted lines corresponds to the levels\nof true coefficients. The true coefficients are in blue and zero coefficients are in red. We can\n\n\n\n54 Chapter 4. Penalized Regression Model for compositional covariates\n\nobserve that when q is too large, there are some false positives (Figure 8A). Comparing oracle\nchoice and when q with distribution Binomial (1, p), the solution path is similar between them.\nMoreover, the Spike-and-Slab lasso will keep the larger coefficients in the models. On the other\nhand, irrelevant coefficients are removed. Compared to Figure 9D, the lasso model included 6\nnonzero coefficients (false negatives) when the optimal l was 0.285. For the elastic net penalty\nfor the model (4.2), we can see that it presents the same behaviour as the lasso penalty.\n\n4.3 Real data application - Brazilian children malnutri-\ntion dataset\n\nThe SSL approach for the regression model with compositional covariates was applied to\nanalyze the nutritional status of children treated at a tertiary university hospital. Our focus is to\nverify children with some types of pathology, including the following: osteogenesis imperfecta,\ncardiopathy, cystic fibrosis, respiratory disease and tumor who were hospitalized at the University\nHospital Medical School in Ribeiro Preto/SP, Brazil. Basically, the study is based on knowledge\nof the prevalence of child malnutrition, where some information about the hospitalized children,\nsuch as sex, age, weight, height, gestational age, body mass index (BMI), bioelectrical impedance,\namong others. The BMI can be classified by cutoff points to age (BMI/A) that are determined\naccording to the Z-score of the World Health Organization (WHO) table of parameters, where\n+2 means overweight and -2 means undernutrition. Through some measures, the phase angle\nbased on resistance and reactance values was also obtained. The phase angle (PA) in children is\na useful tool for evaluating nutritional assessment of body cell mass in stable pediatric patients\nand an important alternative method for predicting malnutrition (low PA value) (PILEGGI et al.,\n2016)). More information about these measures can be found in Pileggi et al. (2016).\n\nThe motivation of this study has been to compare the prevalence of malnutrition in the\npediatric wards based on the average of phase angle of patients with some specific diseases\n(University hospital). The evaluation of the children was between February 2008 and February\n2009. We were able to use the data from 93 pediatric ward patients. However, our sample size is\nn = 12 months of search.\n\nThe predictors analyzed in the model were the number of patients: with Z-score BMI (di-\nvided into 3 classes: underweight, normal weight and overweight, has a compositional structure)\n(Z1, Z2, Z3), where these covariates became ilr coordinates (X1 and X2); birth normal weight\n(X3); gestational age less than 37 weeks (X4); male (X5); with age less than 5 years (X6); cesarean\nbirth (X7). The PA was used for the response of the model. Figures 10 and 11 present the\nsolution path by the SSL model (non-adaptative choice (separable), fixed q ; non-adaptative\noracle choice (separable); adaptative choice, q ? Binomial(1, p) (non-separable)), Lasso and\nelastic net penalties for modeling healthy patients and patients with some disease, respectively.\nFor the group of healthy children, the three settings of SSL (Figure 10 A, B and C) obtained\n\n\n\n4.3. Real data application - Brazilian children malnutrition dataset 55\n\nsimilar results, that is, the predictor number of patients who were born with normal weight (X3)\nand number of patients who had gestational age less than 37 weeks (X4) were significant. This\nresult showed that the healthy children who did not have malnutrition are those who have a\nnormal weight at birth and gestational age less than 37 weeks (coefficients with positive values).\nOn the other hand, the Lasso and elastic net methods included X1 and X5 covariates in the model,\nwhere it presented optimal l = 0.824 by a 10-fold cross-validation (Figure 10D), that is, the\nhealthy children tend to have malnutrition when there are underweight at birth and are female\n(negative estimative of X5).\n\nFor the group of children with some diseases, the solution paths for q = 0.5 when it is\nfixed (Figure 11A) and q is set to the separable penalty choice 2/7 (Figure 11B), the predictor is\nthe number of patients who had cesarean births, which was included in the model (coefficient\nwith negative value). However, the SSL with the non-separable (adaptative) choice (Figure 11C)\ndid not include no coefficient in the model. The Lasso and elastic net methods included the\nnumber of patients who had cesarean births (X7) in the model, assuming the optimal l = 1.017\ncalculated by the 10-fold cross-validation (Figure 11D). If we observe the Figures (Figure 11A,\n11B, 11D and 11E), the children with diseases who were not born by cesarean have prevalence\nto a malnutrition based on the PA measure. This is an important fact to analyze the remarkable\nquestion of malnutrition between healthy children and who have some type of pathology. The\nsame result can be seen in Figures 11D and 11E.\n\nThe models for each applied method (healthy children group) is given by\n\n1. SSL separable (Figure 10A):\n\ny = 10.040?birthnormalweight + 6.479?gestationalagelessthan37weeks\n\n2. SSL separable (Figure 10B):\n\ny = 10.940?birthnormalweight + 6.479?gestationalagelessthan37weeks\n\n3. SSL non-separable (Figure 10C):\n\ny = 10.940?birthnormalweight + 6.479?gestationalagelessthan37weeks\n\n4. Lasso:\n\ny = 0.589?ZscoreBMIunderweight ?0.197?male\n\n5. Elastic net:\n\ny = 0.275?ZscoreBMIunderweight ?1.084?male\n\nThe models for each applied method (children group with some pathologies) is given by\n\n\n\n56 Chapter 4. Penalized Regression Model for compositional covariates\n\n1. SSL separable (Figure 10A):\n\ny = ?6.725?cesareanbirth\n\n2. SSL separable (Figure 10B):\n\ny = ?6.725?cesareanbirth\n\n3. Lasso:\n\ny = ?1.190?cesareanbirth\n\n4. Elastic net:\n\ny = ?2.158?cesareanbirth\n\n\n\n4.4. Discussion 57\n\n4.4 Discussion\nIn this chapter, we applied a new methodology for regression model with compositional\n\ncovariates for child malnutrition data. The SSL, Lasso and elastic net penalties were applied\nin the model with constraint covariates assuming dependence among them. Such a modelling\napproach had a motivation based on a real data set that focused on the nutrition status of children\nwith some pathologies and a control group of healthy children by some measures defined by the\nWHO.\n\nThe main key is to apply such penalties in the regression model with compositional\nconstraints when n&lt;&lt;p. This methodology yields good solutions by the fact of removing\nirrelevant predictors and keeping the larger coefficients, thus obtaining accuracy of coefficient\nestimation. We compared the SSL method with Lasso and elastic net, which is similar when we do\nnot have the slab component, and the performance of Lasso and elastic net were different from the\nSSL method for the healthy children, showing that this approach moves more coefficients toward\nzero, even if we adopt a strong penalty. On the other hand, for the group of children with some\npathologies, the SSL methods (except SSL non-separable), Lasso and elastic net incorporated\nthe same significant covariate (X7) in the model, that is, children with some pathologies tend to\nhave malnutrition when they were not born by cesarean (negative value of estimative).\n\n\n\n58 Chapter 4. Penalized Regression Model for compositional covariates\n\nFigure 9 \u2013 The Lasso solution path (D) and elastic net path (E).\n\n\n\n4.4. Discussion 59\n\nFigure 10 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for healthy patients. The colored\npoints on the solution path represent the estimated values of the coefficients. The vertical\nline (D) and (E) corresponds to the optimal model lasso and elastic net (cross-validation),\nrespectively.\n\n\n\n60 Chapter 4. Penalized Regression Model for compositional covariates\n\nFigure 11 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for patients with pathologies.\nThe colored points on the solution path represent the estimated values of the coefficients.\nThe vertical line (D) and (E) corresponds to the optimal model lasso and elastic net (cross-\nvalidation), respectively.\n\n\n\n61\n\nCHAPTER\n\n5\n\nPENALIZED REGRESSION MODEL FOR\nCOMPOSITIONAL RESPONSE VARIABLES\n\nAND COVARIATES\n\nIn this section, we present the penalized regression model with compositional response\nand covariates. The regression model based on the methodology of compositional data is given\nby\n\nyyy = XXX bbb + eee, (5.1)\n\nwhere yyy is a vector (D ? 1) of compositional response variables, XXX is a matrix (D ? D) of D\ncompositional covariates, where D is the number of components, bbb = (b1,...,bD)> is a vector\n(D?1) unknown parameters and eee is the noise vector with distribution ND(000,Ip), with a known\nvariance s 2 = 1. The intercept of the model is not included, equal to models (3.1) and (4.1).\n\nBased on the principle of working in coordinates, we can rewrite the model (5.1) as\n\nilr(yyy) = hbbb ,XXXiA + eee, (5.2)\n\n=\nD?1\n\u00c2\nk=1\n\nbkilrk(X)+ eee,\n\nwhere eee ? N(000D?1,Silr) and with a vector of parameters bbb = (bk) that afterwards might be\nmapped back to a composition through the inverse ilr transformation.\n\nConsidering the same scheme of Chapter 3 and Chapter 4, we have the following\nestimators for bbb of the regression models with compositional responses and covariates focused\non regularization methods presented in Section 2. We considered the Lasso, elastic net, SSL with\nseparable and non-separable penalty approaches, respectively, for model (5.2) as follows.\n\n1. Lasso:\n\nb?bb = argmin\nbbb\n\n \n||ilr(yyy)?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22/n + l ||bbb ||1\n\n!\n, (5.3)\n\n\n\n62 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\nwhere ||ilr(yyy)?\u00c2D?1k=1 bkilrk(Xi)||\n2\n2 = \u00c2\n\nn\ni=1(ilr(yyy)?\u00c2\n\nD?1\nk=1 bkilrk(X))\n\n2 and ||bbb ||1 = \u00c2Dj=1 |b j|.\n\n2. Elastic Net:\n\nb?bb = argmin\nbbb\n\n \n1\nn\n||ilr(yyy)?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||22 + l\n?\n\n1\n2\n(1?a)||bbb ||22 + a||bbb ||1\n\n?!\n. (5.4)\n\n3. SSL with separable penalty (known variance):\n\nb?bb = argmax\nbbb2RD?1\n\n(\n?\n\n1\n2\n||ilr(yyy)?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||2 +\n\"\n?l1|bbb |+\n\nD\n\n\u00c2\nj=1\n\nlog\n?\n\np?q (0)\np?q (b j)\n\n?#)\n. (5.5)\n\n4. SSL with non-separable penalty (unknown variance):\n\nb?bb = argmax\nbbb2RD?1\n\n8\n><\n\n>:\n?\n\n1\n2\n||ilr(yyy)?\n\nD?1\n\u00c2\nk=1\n\nbkilrk(X)||2 +\n\n0\n\nB\n@?l1|bbb |+ log\n\n2\n\n6\n4\n\nR q D\n\u2019Dj=1 p\n\n?\nq (b j)\n\ndp(q )\nR q D\n\n\u2019Dj=1 p\n?\nq (0)\n\ndp(q )\n\n3\n\n7\n5\n\n1\n\nC\nA\n\n9\n>=\n\n>;\n.\n\n(5.6)\n\nFor the estimation of the bbb \u2019s, we implemented the estimators (5.3) and (5.4) through\nby R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). The estimators (5.5) and\n(5.6) were obtained through by R package SSLASSO (MORAN; ROCKOV\u00c1; GEORGE, 2018).\n\n5.1 Simulation Analysis\nWe provided the simulation studies to investigate the efficacy of the penalized methods\n\nfor a regression model with compositional response variable and covariates. We replicated the\nsimulation 1000 times and the results were summarized based on these replicates (Tables 6\nand 7). We generated compositional data matrix XXX from a logistic normal distribution with\nmean 000D and covariance matrix S = (r|i? j|) with r = 0.2 and r = 0.5, for i, j = 1,...,D.\nMoreover, the compositional response variable is generated according to model (5.2), from\na logistic normal distribution with mean 000D and S = (r|i? j|) with r = 0.2 and r = 0.5. We\nassume D = 3, that is, we have 3 components (y1,y2,y3) of a composition. The fixed values\nfor the parameters bbb? = (b?1 ,b\n\n?\n2 ) were b\n\n?\n1 = (?2,?1.5,?1,0,1,1.5,2,0,...,0)\n\n> and b?2 =\n(2,?1,?2.5,0,1,?1,0.5,0,...,0)> to q = 6 random directions (non-zero coefficients).\n\nWe assumed three scenarios with different number of sample sizes and covariates:\n(n, p) = (50,30),(100,200) and (100,1000). The adopted performance measures for our com-\nparisons were the MSE, FP, FN, HAM measure. Tables 6 and 7 report the averages of these\nperformance measures for the five regularization methods adopted. As can be seen in Tables 6\nand 7, similar results are presented for all the settings. It is worth highlighting that the lasso and\nelastic net estimator perform slightly better than SSL penalties in high dimensions according to\nthe HAM measure.\n\n\n\n5.2. Toy example 63\n\nTable 6 \u2013 Averages of some performance measures for penalized methods with compositional dependent\nvariables and covariates (ilr(y1)).\n\n(n, p) Method MSE FP FN HAM\nr =0.2\n\nSSL (l1, q )\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.5007 0.1820 5.9610 6.1820\n\nSSL (1, 6/30) with s = 1 fixed 0.5001 0.0060 5.9990 6.0060\nSSL (1, 6/30) with unknown s 0.5001 0.0070 5.9990 6.0070\nLasso 0.5039 1.5170 5.3930 7.5170\nElastic Net 0.5028 1.9220 5.2400 7.9220\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0733 1.7710 5.9540 7.7710\nSSL (1, 6/200) with s = 1 fixed 0.0729 0.0010 6.0000 6.0010\nSSL (1, 6/200) with unknown s 0.0729 0.0080 6.0000 6.0080\nLasso 0.1465 1.1470 5.8790 7.1470\nElastic Net 0.0730 3.8840 5.7570 9.8840\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1465 9.0870 5.4270 15.0870\nSSL (1, 6/1000) with s = 1 fixed 0.1465 7.0500 5.5620 13.0500\nSSL (1, 6/1000) with unknown s 0.1465 15.3680 5.0480 21.3680\nLasso 0.1465 0.7290 5.9390 6.7290\nElastic Net 0.1465 0.7270 5.9390 6.7270\n\nr =0.5\nSSL (l1, q )\n\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.5017 0.4320 5.9050 6.4320\nSSL (1, 6/30) with s = 1 fixed 0.5001 0.0200 5.9970 6.0200\nSSL (1, 6/30) with unknown s 0.5000 0.0040 5.9980 6.0040\nLasso 0.4675 1.5170 5.3590 7.5170\nElastic Net 0.5027 1.6720 5.4050 7.6720\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0740 3.9720 5.8860 9.9720\nSSL (1, 6/200) with s = 1 fixed 0.0729 0.0010 6.0000 6.0010\nSSL (1, 6/200) with unknown s 0.0729 0.0050 6.0000 6.0050\nLasso 0.0730 2.2810 5.8380 8.2810\nElastic Net 0.0730 3.2900 5.7940 9.2900\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1466 12.7020 5.1950 18.7020\nSSL (1, 6/1000) with s = 1 fixed 0.1465 10.3150 5.3290 16.3150\nSSL (1, 6/1000) with unknown s 0.1465 11.0040 5.3050 17.0040\nLasso 0.0145 2.8290 5.9610 8.8290\nElastic Net 0.1465 0.6470 5.9470 6.6470\n\n5.2 Toy example\n\nA way of illustrating the proposed model (5.2) to compare the SSL, lasso and elastic\nnet penalties, we considered the following example. For n = 100 individuals with D = 1000\ncompositional covariates, we considered one generated sample of the simulation study presented\nin the last section.\n\nWe compared the SSL with fixed variance s 2 = 1 with three settings: (i) separable\n\n\n\n64 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\nTable 7 \u2013 Averages of some performance measures for penalized methods with compositional dependent\nvariables and covariates (ilr(y2)).\n\n(n, p) Method MSE FP FN HAM\nr =0.2\n\nSSL (l1, q )\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4747 1.5680 5.5900 7,5680\n\nSSL (1, 6/30) with s = 1 fixed 0.4671 0.1890 5.9460 6.1890\nSSL (1, 6/30) with unknown s 0.4656 0.0100 5.9970 6.0100\nLasso 0.4675 1.5170 5.3590 7.5170\nElastic Net 0.4673 1.8500 5.3250 7.8500\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0725 12.1800 5.6400 18.1800\nSSL (1, 6/200) with s = 1 fixed 0.0680 0.1500 5.9960 6.1500\nSSL (1, 6/200) with unknown s 0.0678 0.0040 6.0000 6.0040\nLasso 0.1364 1.0600 5.8780 7.0600\nElastic Net 0.0679 3.5070 5.8180 9.5070\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1366 21.7880 4.6050 27.7880\nSSL (1, 6/1000) with s = 1 fixed 0.1364 18.9530 4.7720 24.9530\nSSL (1, 6/1000) with unknown s 0.1364 4.4560 5.6820 10.4560\nLasso 0.1364 0.5970 5.9460 6.5970\nElastic Net 0.1364 0.5920 5.9460 6.5920\n\nr =0.5\nSSL (l1, q )\n\n(50, 30) SSL (1, 0.8) with s = 1 fixed 0.4673 0.3940 5.8750 6.3940\nSSL (1, 6/30) with s = 1 fixed 0.4657 0.0150 5.9960 6.0150\nSSL (1, 6/30) with unknown s 0.4655 0.0030 5.9990 6.0030\nLasso 0.4692 1.8110 5.2710 7.8110\nElastic Net 0.4683 2.2020 5.1620 8.2020\n\n(100, 200) SSL (1, 0.8) with s = 1 fixed 0.0690 4.0420 5.8800 10.0420\nSSL (1, 6/200) with s = 1 fixed 0.0679 0.0050 5.9980 6.0050\nSSL (1, 6/200) with unknown s 0.0678 0.0080 5.9990 6.0080\nLasso 0.0679 2.8000 5.7780 8.8000\nElastic Net 0.0679 4.0810 5.7780 10.0810\n\n(100, 1000) SSL (1, 0.8) with s = 1 fixed 0.1364 12.6170 5.1660 18.6170\nSSL (1, 6/1000) with s = 1 fixed 0.1364 10.2980 5.3290 16.2980\nSSL (1, 6/1000) with unknown s 0.1364 11.0540 5.2670 17.0540\nLasso 0.0135 4.1300 5.9380 10.1300\nElastic Net 0.0135 0.6440 5.9360 6.6440\n\nchoice q = 0.8, to verify the over-estimating of the true non-zero fraction 6/1000, (ii) separable\noracle choice q = 6/1000 and (iii) non-separable (adaptative) choice q ? Binomial(1,D). The\nslab parameter was set to l1 = 0.1 and we used a ladder l0 2 I = {1,2,...,50} for the spike\nparameter. In addition, we applied the generated data to the Lasso and elastic net penalties\nimplemented in the R package glmnet (FRIEDMAN; HASTIE; TIBSHIRANI, 2010). For this\napproach, an optimal value of l was selected by 10-fold cross-validation. According to Figures\n12, 13, 14 and 15, we can see the solution paths for the five settings when we have the structure of\n\n\n\n5.3. Real data application - ICMS dataset 65\n\ncompositional response variables and covariates together. Each line represents a single regression\ncoefficient and the horizontal dotted lines corresponds to the levels of true coefficients. The true\ncoefficients are in blue and zero coefficients are in red. We can observe that when q is too large,\nthere are more false positives than false negatives (Figures 12A, 12B and 12C; 14A, 14B and\n14C). In comparison with the Figures 13D, 13E, 15D and 15E, the lasso and elastic net presented\nmore false negatives, that is, the model includes unimportant variables with shrunk coefficients.\n\n5.3 Real data application - ICMS dataset\n\nThe description of the applied dataset is in Chapter 3. The focus on this Chapter is the\nrestriction in the regression model with compositional response and covariates.\n\nIn this case, we considered the same three economic sectors: industry (y1), commerce (y2)\nand administered prices (y3), defined as compositional data. Besides the covariates mentioned\nin Chapter 3, we add the lagged compositional covariate in the period of 12 months of the\nproportion of ICMC in the industry (X6), commerce (X7) and administered prices (X8) and 6\nmonths of the proportion of ICMC in the industry (X9), commerce (X10) and administered prices\n(X11).\n\nFigures 16 and 17 present the solution path by the SSL (non-adaptative choice (separable),\nfixed q ; non-adaptative oracle choice (separable); adaptative choice, q ? Binomial(1, p) (non-\nseparable)), lasso and elastic net methods for modeling the ICMS disaggregated in 3 parts:\nindustry, commerce and administered prices considering compositional covariates (X6 to X11).\nThereby, the results showed the same performance for ilr(y1) and ilr(y2) when the SSL with\nseparable penalties (Figures 16A, 16B, 17A and 17B), that is, these methods did not select\nany significant covariate for the model. On the other hand, SSL non-separable presented three\nsignificant covariates (ilr(X6), ilr(X7) and ilr(X8)). The optimal l calculated by the 10-fold\ncross-validation were 0.0043 (ilr(y1)) and 0.0020 (ilr(y2)) for the lasso method and 0.0069\n(ilr(y1)) and 0.0054 (ilr(y2)) for the elastic net method (vertical line in Figures 16D, 16E, 17D\nand 17E).\n\nThe models for each applied method is given by\n\n\n\n66 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\n1. SSL non-separable:\n\ny1 =0.004?MonthlyIndustrialSurvey + 0.001?MonthlyTradeSurvey\n\n?0.002?IndexEconomicActivity?0.001?IGP?DI/F GV + 0.080?ICMSindustry12months\n\n+ 0.062?ICMScommerce12months + 0.116?ICMSadm12months\n\n?0.007?ICMScommerce12months\n\ny2 =?0.001?MonthlyIndustrialSurvey + 0.005?MonthlyTradeSurvey\n\n+ 0.001?Monthlyenergyconsumption + 0.001?IGP?DI/F GV\n\n?0.075?ICMSindustry12months + 0.0357?ICMSindustry6months\n\n2. Lasso:\n\ny1 =0.002?MonthlyIndustrialSurvey + 0.002?MonthlyTradeSurvey\n\n+ 0.003?Monthlyenergyconsumption?0.001?IndexEconomicActivity?0.002?IGP?DI/F GV\n\ny2 =0.001?MonthlyIndustrialSurvey + 0.003?MonthlyTradeSurvey\n\n+ 0.004?Monthlyenergyconsumption?0.003?IndexEconomicActivity + 0.003?IGP?DI/F GV\n\n3. Elastic net:\n\ny1 =0.002?MonthlyIndustrialSurvey + 0.001?MonthlyTradeSurvey\n\n+ 0.003?Monthlyenergyconsumption?0.001?IndexEconomicActivity?0.002?IGP?DI/F GV\n\ny2 =0.001?MonthlyIndustrialSurvey + 0.004?MonthlyTradeSurvey\n\n+ 0.004?Monthlyenergyconsumption?0.003?IndexEconomicActivity + 0.003?IGP?DI/F GV\n\n5.4 Discussion\nIn this chapter, we presented a compositional regression model with restriction in the\n\nresponse variables and covariates under five regularization methods presented in Chapter 2. We\napplied the ilr coordinates on the response variables and covariates simultaneously to remove the\ndependence among the components.\n\nA simulation study for the proposed model (5.2) showed that the model with lasso and\nelastic net estimators perform better in terms of estimation if comparable to the other penalized\nmethods in high-dimensions.\n\nIn order to illustrate the methodology, a toy example was presented. When the lasso and\nelastic net estimators are applied, there are many more false negatives if compared with SSL\nestimators. Clearly, for the ilr(y2), the SSL estimators obtained a performance better than lasso\nand elastic net. Therefore, SSL non-separable (Figure 14C) has superior performance compared\nwith the other SSL estimators (separable).\n\n\n\n5.4. Discussion 67\n\nIn the case of application, the real data set involves the ICMC tax as in Chapter 3. The\nSSL non-separable showed a better performance in relation to the other SSL penalties. The\nlasso and elastic net estimators presented similar results, with only a little difference between\nthe optimal l . Based on these results, the SSL non-separable method considered the lagged\ncovariates administered prices ilr(X6), ilr(X7) and ilr(X8) significant, that is, the proportion\nof ICMS in the industry, commerce and administered prices in the period of 12 months are\nrelevant to explain the response variable ilr(y1) (proportion of the ICMS in the industry). On\nthe other hand, the lasso method considered only exogenous covariates significant, which are\nMonthly Industrial Survey, Monthly Trade Survey and IGP-DI/FGV General Price Index and for\nthe elastic net method, besides these covariates mentioned above, including also the covariate\nMonthly energy comsuption in Sao Paulo State. We observe that these results were similar with\nobtained in Chapter 3.\n\n\n\n68 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\nFigure 12 \u2013 The SSL solution paths (A, B, C) (for ilr(y1)).\n\n\n\n5.4. Discussion 69\n\nFigure 13 \u2013 The lasso solution path (D) and elastic net path (E) (for ilr(y1)).\n\n\n\n70 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\nFigure 14 \u2013 The SSL solution paths (A, B, C) (for ilr(y2)).\n\n\n\n5.4. Discussion 71\n\nFigure 15 \u2013 The lasso solution path (D) and elastic net path (E) (for ilr(y2)).\n\n\n\n72 Chapter 5. Penalized Regression Model for compositional response variables and covariates\n\nFigure 16 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y1). The colored points\non the solution path represent the estimated values of the coefficients. The vertical line (D)\nand (E) corresponds to the optimal model lasso and elastic net (cross-validation), respectively.\n\n\n\n5.4. Discussion 73\n\nFigure 17 \u2013 The solution path SSL (A, B, C), lasso (D) and elastic net (E) for ilr(y2). The colored points\non the solution path represent the estimated values of the coefficients. The vertical line (D)\nand (E) corresponds to the optimal model lasso and elastic net (cross-validation), respectively.\n\n\n\n\n\n75\n\nCHAPTER\n\n6\n\nCONCLUSION\n\nIn this thesis, we considered penalized regression methods, in particular the Lasso (least\nabsolute shrinkage and selection operator), elastic net and Spike-and-Slab lasso when there are\ncompositional restrictions in the response variable, covariates or both of them.\n\nOne of the principal constraints of compositional data is the nature of dependence among\nthe components, which cannot be ignored in order to obtain accurate inferences. Thus, the\nincrease in large datasets, whose dimensionality is much larger than sample size, poses new\nchallenges to the current methodology of compositional data.\n\nFor the context of regression models, we presented three novel models based on compo-\nsitional data with an application of different regularization methods. We note that considering the\npenalized model with compositional response variables, the simulation studies and application in\na real data set proved that the SSL estimators (oracle separable and non-separable) performed\nbetter than the other regularization methods.\n\nConsidering the penalized regression model with compositional covariates, the analysis\nunder this approach in the child malnutrition data is an important contribution to the present\nstudy. These data focus on the nutrition status of children with some pathologies with some\nmeasures defined by the WHO. For this model, the SSL estimators presented good solutions\nby the fact of removing irrelevant predictors and keeping the larger coefficients, thus obtaining\naccuracy of coefficient estimation.\n\nFinally, the last penalized regression model considered restrictions for both the response\nvariables and covariates. The lasso and elastic net estimators perform better if compared with the\nother penalized methods in high-dimensions in the simulation study.\n\nFor the further development, there are several extensions of this current work. In particu-\nlar, we can consider longitudinal and spatio-temporal longitudinal models with compositional\nrestriction under regularization methods, semiparametric or non-parametric approaches for re-\ngression model with log-contrast, where new methods of the regularized estimation could be\n\n\n\n76 Chapter 6. Conclusion\n\ndeveloped. Another extension could be to study appropriate models in the presence of zero in\ncompositional data in a high-dimensional setting.\n\n\n\n77\n\nBIBLIOGRAPHY\n\nAITCHISON, J. The statistical analysis of compositional data. Journal of the Royal Statistical\nSociety. Series B (Methodological), v. 44, n. 2, p. 139\u2013177, 1982. Citations on pages 23, 24,\nand 28.\n\n. The statistical analysis of compositional data. Journal of the Royal Statistical Society.\nSeries B (Methodological), p. 139\u2013177, 1982. Citation on page 23.\n\n. The statistical analysis of compositional data. [S.l.]: Chapman and Hall, 1986. Citations\non pages 24 and 30.\n\nAITCHISON, J.; EGOZCUE, J. J. Compositional data analysis: Where are we and where should\nwe be heading? Mathematical Geology, v. 37, n. 7, p. 829\u2013850, 2005. Citations on pages 24\nand 28.\n\nAITCHISON, J.; SHEN, S. M. Logistic-normal distributions: Some properties and uses.\nBiometrika, v. 67, n. 2, p. 261\u2013272, 1980. Citations on pages 23 and 24.\n\nBOOGAART, K. G. van den; TOLOSANA-DELGADO, R. Analyzing Compositional\nData with R. [S.l.]: Springer Publishing Company, Incorporated, 2013. ISBN 3642368085,\n9783642368080. Citations on pages 24 and 32.\n\nBUHLMANN, P.; GEER, S. van de. Statistics for High-Dimensional Data: Methods, Theory\nand Applications. 1st. ed. [S.l.]: Springer Publishing Company, Incorporated, 2011. ISBN\n3642201911, 9783642201912. Citation on page 34.\n\nCHEN, J.; ZHANG, X.; LI, S. Multiple linear regression with compositional response and\ncovariates. Journal of Applied Statistics, v. 44, n. 12, p. 2270\u20132285, 2017. Citations on pages\n24, 31, and 32.\n\nCHIPMAN, H. Bayesian variable selection with related predictions. Canadian Journal of\nStatistics, v. 24, n. 1, p. 17\u201336, 1996. Citation on page 26.\n\nEFRON, B.; HASTIE, T.; JOHNSTONE, I.; TIBSHIRANI, R. Least angle regression. The\nAnnals of Statistics, v. 32, n. 2, p. 407\u2013499, 2004. Citations on pages 25 and 35.\n\nEGOZCUE, J. J.; PAWLOWSKY-GLAHN, V.; MATEU-FIGUERAS, G.; BARCEL\u00d3-VIDAL,\nC. Isometric logratio transformations for compositional data analysis. Mathematical Geology,\nv. 35, n. 3, p. 279\u2013300, 2003. Citation on page 24.\n\nFAN, J.; LI, R. Variable selection via nonconcave penalized likelihood and its oracle properties.\nJournal of the American Statistical Association, Taylor &amp; Francis, v. 96, n. 456, p. 1348\u20131360,\n2001. Citation on page 25.\n\nFRIEDMAN, J.; HASTIE, T.; H\u00f6FLING, H.; TIBSHIRANI, R. Pathwise coordinate optimization.\nThe Institute of Mathematical Statistics, v. 1, n. 2, p. 302\u2013332, 12 2007. Available:&lt;https:\n//doi.org/10.1214/07-AOAS131>. Citation on page 35.\n\nhttps://doi.org/10.1214/07-AOAS131\nhttps://doi.org/10.1214/07-AOAS131\n\n\n78 Bibliography\n\nFRIEDMAN, J.; HASTIE, T.; TIBSHIRANI, R. Regularization paths for generalized linear\nmodels via coordinate descent. Journal of Statistical Software, v. 33, n. 1, p. 1\u201322, 2010.\nCitations on pages 25, 38, 48, 53, 62, and 64.\n\nFU, W. J. Penalized regressions: The bridge versus the lasso. Journal of Computational and\nGraphical Statistics, v. 7, n. 3, p. 397\u2013416, 1998. Citation on page 35.\n\nGEORGE, E. I.; MCCULLOCH, R. E. Variable selection via gibbs sampling. Journal of the\nAmerican Statistical Association, v. 88, n. 423, p. 881\u2013889, 1993. Citation on page 26.\n\nHASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. The elements of statistical learning: data\nmining, inference and prediction. 2. ed. [S.l.]: Springer, 2009. Citations on pages 34 and 35.\n\nHASTIE, T.; TIBSHIRANI, R.; WAINWRIGHT, M. Statistical Learning with Sparsity:\nThe Lasso and Generalizations. [S.l.]: Chapman &amp; Hall/CRC, 2015. ISBN 1498712169,\n9781498712163. Citations on pages 33, 34, and 35.\n\nHIJAZI, R. An em-algorithm based method to deal with rounded zeros in compositional data\nunder dirichlet models. In: PROCEEDINGS OF THE 1TH INTERNATIONAL WORKSHOP\nON COMPOSITIONAL DATA ANALYSIS, 4. Giron, 2011. Citation on page 24.\n\nHIJAZI, R.; JERNIGAN, R. W. Modelling compositional data using dirichlet regression models.\nJournal of Applied Probability &amp; Statistics, v. 4, n. 1, p. 77\u201391, 2009. Citation on page 24.\n\nHOERL, A. E.; KENNARD, R. W. Ridge regression: Biased estimation for nonorthogonal\nproblems. Technometrics, v. 12, p. 55\u201367, 1970. Citation on page 35.\n\nHRON, K.; FILZMOSER, P.; THOMPSON, K. Linear regression with compositional explanatory\nvariables. Journal of Applied Statistics, v. 39, n. 5, p. 1115\u20131128, 2012. Citations on pages\n24, 31, and 32.\n\nISHWARAN, H.; RAO, J. S. Spike and slab gene selection for multigroup microarray data.\nJournal of the American Statistical Association, v. 100, n. 471, p. 764\u2013780, 2005. Citation\non page 26.\n\nJOHNSON, R.; WICHERN, D. Applied multivariate statistical analysis. [S.l.]: New Jersey:\nPrentice Hall, 1998. Citation on page 24.\n\nLIN, W.; SHI, P.; FENG, R.; LI, H. Variable selection in regression with compositional covariates.\nBiometrika, v. 101, n. 4, p. 1\u201313, 2014. Citations on pages 25 and 33.\n\nMART\u00edN-FERNANDEZ, J.; BARCEL\u00f3-VIDAL, C.; PAWLOWSKY-GLAHN, V. Dealing with\nzeros and missing values in compositional data sets using nonparametric imputation. Mathe-\nmatical Geology, v. 35, n. 3, p. 253\u2013278, 2003. Citation on page 24.\n\nMORAN, G. E.; ROCKOV\u00c1, V.; GEORGE, E. I. On variance estimation for Bayesian variable\nselection. ArXiv e-prints, Jan. 2018. Citations on pages 36, 38, 48, and 62.\n\nPAWLOWSKY-GLAHN, V.; BUCCIANTI, A. Compositional data analysis: Theory and\napplications. [S.l.]: John Wiley &amp; Sons, 2011. Citation on page 24.\n\nPAWLOWSKY-GLAHN, V.; EGOZCUE, J. Geometric approach to statistical analysis on the\nsimplex. Stochastic Environmental Research and Risk Assessment, v. 15, p. 384\u2013398, 2001.\nCitation on page 29.\n\n\n\nBibliography 79\n\nPAWLOWSKY-GLAHN, V.; EGOZCUE, J. J.; TOLOSANA-DELGADO, R. Modeling and\nanalysis of compositional data. [S.l.]: John Wiley &amp; Sons, 2015. Citations on pages 24, 27,\n28, and 30.\n\nPILEGGI, V. N.; MONTEIRO, J. P.; MARGUTTI, A. V. B.; JR., J. S. C. Prevalence of child\nmalnutrition at a university hospital using the world health organization criteria and bioelectrical\nimpedance data. Brazilian Journal of Medical and Biological Research, v. 49, n. 3, 2016.\nCitation on page 54.\n\nR Core Team. R: A Language and Environment for Statistical Computing. Vienna, Austria,\n2017. Available:&lt;https://www.R-project.org/>. Citation on page 36.\n\nROCKOV\u00c1, V.; GEORGE, E. I. Emvs: The em approach to bayesian variable selection. Journal\nof the American Statistical Association, v. 109, n. 506, p. 828\u2013846, 2014. Citation on page\n26.\n\n. The spike-and-slab lasso. Journal of the American Statistical Association, v. 113, n. 521,\np. 431\u2013444, 2018. Citations on pages 25, 26, 35, and 36.\n\nSHELTON, J. A.; SHEIKH, A. S.; BORNSCHEIN, J.; STERNE, P.; LUCKE, J. Nonlinear\nspike-and-slab sparse coding for interpretable image encoding. PLoS One, v. 10, p. e0124088,\n2015. Citation on page 26.\n\nTANG, Z.; SHEN, Y.; ZHANG, X.; YI, N. The spike-and-slab lasso cox model for survival\nprediction and associated genes detection. Bioinformatics, v. 33, n. 18, p. 2799\u20132807, 2017.\nCitation on page 26.\n\n. The spike-and-slab lasso generalized linear models for prediction and associated genes\ndetection. Genetics, v. 205, n. 1, p. 77\u201388, 2017. Citation on page 26.\n\nTIBSHIRANI, R. Regression shrinkage and selection via the lasso. Journal of the Royal\nStatistical Society B, v. 58, p. 267\u2013288, 1996. Citations on pages 25, 33, and 35.\n\nZOU, H.; HASTIE, T. Regularization and variable selection via the elastic net. Journal of the\nRoyal Statistical Society, Series B, v. 67, p. 301\u2013320, 2005. Citations on pages 25 and 35.\n\nhttps://www.R-project.org/\n\n\n\n\n81\n\nAPPENDIX\n\nA\n\nAPPENDIX 1\n\nComputational Routines for estimation of parameters -\nsoftware R\n\n1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n2 # S i m u l a t i o n C h a p t e r 3\n3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n4 rm ( l i s t = l s ( ) )\n5\n6 # g e n e r a t e c o m p o s i t i o n a l d a t a\n7 r c o m p o s&lt;? f u n c t i o n ( n , l , p , b e t a , s i g m a ) {\n8 y = m a t r i x (NA, nrow =n , n c o l = l )\n9 Z = m a t r i x (NA, nrow =n , n c o l =p )\n\n10 Y = m a t r i x (NA, nrow =n , n c o l = l )\n11\n12 f o r ( j i n 1 : p ) {\n13 Z [ , j ]&lt;? r n o r m ( n , 2 )\n14 }\n15\n16 mu = Z%?%b e t a\n17 mean = a p p l y ( mu , 2 , mean )\n18 y = rcompnorm ( n , mean , s i g m a , t y p e = \" a l r \" )\n19\n20 r e t u r n ( l i s t ( y , Z ) )\n21 }\n22\n23 o u t p u t 1 . 1 = l i s t ( )\n\n\n\n82 APPENDIX A. Appendix 1\n\n24 o u t p u t 1 . 2 = l i s t ( )\n25 o u t p u t 1 . 3 = l i s t ( )\n26 o u t p u t 2 . 1 = l i s t ( )\n27 o u t p u t 2 . 2 = l i s t ( )\n28 o u t p u t 2 . 3 = l i s t ( )\n29 o u t p u t 3 . 1 = l i s t ( )\n30 o u t p u t 3 . 2 = l i s t ( )\n31 o u t p u t 3 . 3 = l i s t ( )\n32 e s t 1 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n33 e s t 1 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n34 e s t 1 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n35 e s t 2 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n36 e s t 2 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n37 e s t 2 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n38 e s t 3 . 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n39 e s t 3 . 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n40 e s t 3 . 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n41 e s t . l a s s o 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n42 e s t . l a s s o 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n43 e s t . l a s s o 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n44 e s t . e l a s t i c 1 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n45 e s t . e l a s t i c 2 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n46 e s t . e l a s t i c 3 = m a t r i x ( 0 , nrow =p , n c o l =S?1)\n47 ham . s s l 1 . 1 = c ( )\n48 ham . s s l 1 . 2 = c ( )\n49 ham . s s l 1 . 3 = c ( )\n50 ham . s s l 2 . 1 = c ( )\n51 ham . s s l 2 . 2 = c ( )\n52 ham . s s l 2 . 3 = c ( )\n53 ham . s s l 3 . 1 = c ( )\n54 ham . s s l 3 . 2 = c ( )\n55 ham . s s l 3 . 3 = c ( )\n56 ham . l a s s o 1 = c ( )\n57 ham . l a s s o 2 = c ( )\n58 ham . l a s s o 3 = c ( )\n59 ham . e l a s t i c 1 = c ( )\n60 ham . e l a s t i c 2 = c ( )\n61 ham . e l a s t i c 3 = c ( )\n62\n\n\n\n83\n\n63 # # # # GENERATE COMPOSITIONAL DATA MATRIX ###\n64 s e t . s e e d ( 2 0 1 8 )\n65 w h i l e ( j &lt;S ) {\n66 Y&lt;? r c o m p o s ( n , l , p , b e t a , s i g m a )\n67 y&lt;? a s . m a t r i x ( p i v o t C o o r d (Y [ [ 1 ] ] ) )\n68 Z&lt;? a s . m a t r i x (Y [ [ 2 ] ] )\n69 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n70 ## SSL\n71 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n72 l a m b d a 1&lt;? 1\n73 l a m b d a 0&lt;? s e q ( l a m b d a 1 , 5 0 , l e n g t h = 1 0 )\n74 L&lt;? l e n g t h ( l a m b d a 0 )\n75\n76 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )\n77 r e s u l t 1 . 1&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" s e p a r a b l e \" , v a r i a n c e\n\n= \" known \" ,\n78 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n79 t h e t a = 0 . 8 )\n80\n81 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )\n82 r e s u l t 1 . 2&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" s e p a r a b l e \" , v a r i a n c e\n\n= \" known \" ,\n83 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n84 t h e t a = 6 / p )\n85\n86 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )\n87 r e s u l t 1 . 3&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" a d a p t i v e \" , v a r i a n c e =\n\n\" unknown \" ,\n88 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n89 t h e t a = 6 / p )\n90\n91 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n92 ## L a s s o\n93 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n94 r e s u l t 1 . 4 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = 1 ,\n\ns t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )\n95 c v . l a s s o . m o d t o t a l 1 . 4 = c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" ,\n\na l p h a = 1 )\n96 b e s t l a m . l a s s o 1 . 4 = c v . l a s s o . m o d t o t a l 1 . 4 $ l a m b d a . 1 s e\n\n\n\n84 APPENDIX A. Appendix 1\n\n97\n98 r e s u l t 1 . 4 . o p t = g l m n e t ( Z , y [ , 1 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE , l a m b d a =\n\nb e s t l a m . l a s s o 1 . 4 , i n t e r c e p t =FALSE )\n99 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )\n\n100 c o e f . l a s s o 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 4 . o p t ) [ ?1 , ] , nrow =p ) , 4 )\n101\n102 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n103 ## E l a s t i c N e t\n104 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n105 a&lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )\n106 s e a r c h&lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {\n107 c v&lt;? c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , n f o l d = 1 0 ,\n\nt y p e . m e a s u r e = \" d e v i a n c e \" , p a r a l l e = TRUE , a l p h a = i )\n108 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a . 1\n\ns e = c v $ l a m b d a . 1 s e , a l p h a = i )\n109 }\n110 c v 1&lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]\n111\n112 r e s u l t 1 . 5 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = c v 1 $ a l p h a ,\n\nl a m b d a = c v 1 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )\n113\n114 c o e f . e l a s t i c 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 5 ) [ ?1 , ] , nrow =p ) , 4 )\n\n1 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n2 # S i m u l a t i o n C h a p t e r 4\n3 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n4 # # # # GENERATE COMPOSITIONAL DATA MATRIX\n5 s e e d =2018\n6 s e t . s e e d ( 2 0 1 8 )\n7\n8 w h i l e ( j &lt;S ) {\n9 mean&lt;? c ( r e p ( 0 , p?1) ) # means f o r e a c h c o m p o n e n t\n\n10 s i g m a&lt;? m a t r i x ( 0 . 2 , nrow =p?1 , n c o l =p?1)\n11 d i a g ( s i g m a )&lt;? 1\n12\n13 X&lt;? rcompnorm ( n , m=mean , s = s i g m a , t y p e = \" a l r \" )\n14 X . mean = a p p l y ( X , 2 , mean )\n15 i n d&lt;? o r d e r (X . mean , d e c r e a s i n g =T ) [ 1 : 3 ]\n16 X . new&lt;? ( c b i n d (X [ , i n d ] , X[ ,? i n d ] ) )\n\n\n\n85\n\n17\n18 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n19 ## SSL\n20 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n21 ### GENERATE RESPONSE VECTOR Y\n22 b e t a = c ( ?2 , ?1 . 5 , ?1 , 0 , 1 , 1 . 5 , 2 , r e p ( 0 , p?8) )\n23\n24 y = Z [ , 1 ] ? b e t a [ 1 ] + Z [ , 2 ] ? b e t a [ 2 ] + Z [ , 3 ] ? b e t a [ 3 ] + Z [ , 4 ] ? b e t a [ 4 ] + Z\n\n[ , 5 ] ? b e t a [ 5 ] + Z [ , 6 ] ? b e t a [ 6 ] + Z [ , 7 ] ? b e t a [ 7 ] + r n o r m ( n )\n25\n26\n27 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )\n28 r e s u l t 1&lt;? SSLASSO ( Z , y , p e n a l t y = \" s e p a r a b l e \" , v a r i a n c e = \"\n\nknown \" ,\n29 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n30 t h e t a = 0 . 8 )\n31\n32 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )\n33 r e s u l t 2&lt;? SSLASSO ( Z , y , p e n a l t y = \" s e p a r a b l e \" , v a r i a n c e = \"\n\nknown \" ,\n34 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n35 t h e t a = 6 / p )\n36\n37 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )\n38 r e s u l t 3&lt;? SSLASSO ( Z , y , p e n a l t y = \" a d a p t i v e \" , v a r i a n c e = \"\n\nunknown \" ,\n39 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n40 t h e t a = 6 / p )\n41\n42 o u t p u t 1 [ [ j ] ] = r e s u l t 1 $ b e t a [ , 1 0 ]\n43 o u t p u t 2 [ [ j ] ] = r e s u l t 2 $ b e t a [ , 1 0 ]\n44 o u t p u t 3 [ [ j ] ] = r e s u l t 3 $ b e t a [ , 1 0 ]\n45\n46 e s t 1 [ , j ] = a s . m a t r i x ( o u t p u t 1 [ [ j ] ] )\n47 e s t 2 [ , j ] = a s . m a t r i x ( o u t p u t 2 [ [ j ] ] )\n48 e s t 3 [ , j ] = a s . m a t r i x ( o u t p u t 3 [ [ j ] ] )\n49\n50 m e d i a s 1 = a p p l y ( e s t 1 , 1 , mean )\n51 m e d i a s 2 = a p p l y ( e s t 2 , 1 , mean )\n\n\n\n86 APPENDIX A. Appendix 1\n\n52 m e d i a s 3 = a p p l y ( e s t 3 , 1 , mean )\n53\n54 v a r 1 = a p p l y ( e s t 1 , 1 , v a r )\n55 v a r 2 = a p p l y ( e s t 2 , 1 , v a r )\n56 v a r 3 = a p p l y ( e s t 3 , 1 , v a r )\n57\n58 s d 1 = a p p l y ( e s t 1 , 1 , s d )\n59 s d 2 = a p p l y ( e s t 2 , 1 , s d )\n60 s d 3 = a p p l y ( e s t 3 , 1 , s d )\n61\n62 # # # # # d i a g n o s t i c s s t a t i s t i c s\n63 b i a s 1 = m e d i a s 1?b e t a\n64 b i a s 2 = m e d i a s 2?b e t a\n65 b i a s 3 = m e d i a s 3?b e t a\n66\n67 mse1 = mean ( v a r 1 + ( b i a s 1 ^ 2 ) )\n68 mse2 = mean ( v a r 2 + ( b i a s 2 ^ 2 ) )\n69 mse3 = mean ( v a r 3 + ( b i a s 3 ^ 2 ) )\n70\n71 ## e r r o r p r e d i c t i o n\n72 p e 1 = sum ( y?Z%?%e s t 1 [ , j ] ) ^ 2 / n\n73 p e 2 = sum ( y?Z%?%e s t 2 [ , j ] ) ^ 2 / n\n74 p e 3 = sum ( y?Z%?%e s t 3 [ , j ] ) ^ 2 / n\n75\n76 j = j +1\n77 c a t ( j , \" \" , i t e r + 1 , \" \\ n \" )\n78 i t e r&lt;? i t e r +1\n79 }\n80\n81 ## FP = f a l s e p o s i t i v e n u m b e r\n82 f p 1 = sum ( e s t 1 [ 4 , ] ! = 0 , e s t 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n83 f p 2 = sum ( e s t 2 [ 4 , ] ! = 0 , e s t 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n84 f p 3 = sum ( e s t 3 [ 4 , ] ! = 0 , e s t 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n85\n86 ## FP = f a l s e n e g a t i v e n u m b e r\n87 f n 1 = sum ( e s t 1 [ 1 : 3 , ] == 0 , e s t 1 [ 5 : 7 , ] == 0 ) / ( S?1)\n88 f n 2 = sum ( e s t 2 [ 1 : 3 , ] == 0 , e s t 2 [ 5 : 7 , ] == 0 ) / ( S?1)\n89 f n 3 = sum ( e s t 3 [ 1 : 3 , ] == 0 , e s t 3 [ 5 : 7 , ] == 0 ) / ( S?1)\n90\n\n\n\n87\n\n91 ## Hamming d i s t a n c e\n92 ham1 = sum ( e s t 1 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)\n93 ham2 = sum ( e s t 2 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)\n94 ham3 = sum ( e s t 3 [ 1 : ( p?1) , ] ! = b e t a [ 1 : ( p?1) ] ) / ( S?1)\n95\n96 d i a g n = r o u n d ( r b i n d ( mse1 , mse2 , mse3 , ham1 , ham2 ,\n97 ham3 , f p 1 , f p 2 , f p 3 , f n 1 , f n 2 , f n 3 , pe1 , pe2 , p e 3 )\n\n, 5 )\n98 d i a g n\n\n1\n2 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n3 # S i m u l a t i o n C h a p t e r 5\n4 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n5 rm ( l i s t = l s ( ) )\n6 # g e n e r a t e c o m p o s i t i o n a l d a t a\n7 r c o m p o s&lt;? f u n c t i o n ( n , l , p , b e t a , s i g m a ) {\n8 y = m a t r i x (NA, nrow =n , n c o l = l )\n9 Z = m a t r i x (NA, nrow =n , n c o l =p )\n\n10 Y = m a t r i x (NA, nrow =n , n c o l = l )\n11\n12 mean1&lt;? c ( r e p ( 0 , p?1) ) # means f o r e a c h c o m p o n e n t\n13 s i g m a&lt;? m a t r i x ( 0 . 4 , nrow =p?1 , n c o l =p?1)\n14 d i a g ( s i g m a )&lt;? 2\n15\n16 X&lt;? rcompnorm ( n , m=mean1 , s = s i g m a , t y p e = \" a l r \" )\n17 X . mean = a p p l y ( X , 2 , mean )\n18 i n d&lt;? o r d e r (X . mean , d e c r e a s i n g =T ) [ 1 : 3 ] ## E x t r a i n d o o s 5\n\nm a i o r e s c o m p o n e n t e s\n19 X . new&lt;? ( c b i n d (X [ , i n d ] , X[ ,? i n d ] ) )\n20\n21 Z&lt;? a s . m a t r i x ( p i v o t C o o r d (X . new ) )\n22\n23 mu = Z%?%b e t a\n24 mean2 = a p p l y ( mu , 2 , mean )\n25 y = rcompnorm ( n , mean2 , s i g m a c , t y p e = \" a l r \" )\n26\n27 r e t u r n ( l i s t ( y , Z ) )\n28 }\n\n\n\n88 APPENDIX A. Appendix 1\n\n29\n30 # # # # # Y1 # # # # #\n31 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n32 ## SSL\n33 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n34 l a m b d a 1&lt;? 1\n35 l a m b d a 0&lt;? s e q ( l a m b d a 1 , 5 0 , l e n g t h = 1 0 )\n36 L&lt;? l e n g t h ( l a m b d a 0 )\n37\n38 o u t p u t 1 . 1 = l i s t ( )\n39 o u t p u t 1 . 2 = l i s t ( )\n40 o u t p u t 1 . 3 = l i s t ( )\n41 o u t p u t 2 . 1 = l i s t ( )\n42 o u t p u t 2 . 2 = l i s t ( )\n43 o u t p u t 2 . 3 = l i s t ( )\n44 o u t p u t 3 . 1 = l i s t ( )\n45 o u t p u t 3 . 2 = l i s t ( )\n46 o u t p u t 3 . 3 = l i s t ( )\n47 e s t 1 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n48 e s t 1 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n49 e s t 1 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n50 e s t 2 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n51 e s t 2 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n52 e s t 2 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n53 e s t 3 . 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n54 e s t 3 . 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n55 e s t 3 . 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n56 e s t . l a s s o 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n57 e s t . l a s s o 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n58 e s t . l a s s o 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n59 e s t . e l a s t i c 1 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n60 e s t . e l a s t i c 2 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n61 e s t . e l a s t i c 3 = m a t r i x ( 0 , nrow =p?1 , n c o l =S?1)\n62 ham . s s l 1 . 1 = c ( )\n63 ham . s s l 1 . 2 = c ( )\n64 ham . s s l 1 . 3 = c ( )\n65 ham . s s l 2 . 1 = c ( )\n66 ham . s s l 2 . 2 = c ( )\n67 ham . s s l 2 . 3 = c ( )\n\n\n\n89\n\n68 ham . s s l 3 . 1 = c ( )\n69 ham . s s l 3 . 2 = c ( )\n70 ham . s s l 3 . 3 = c ( )\n71 ham . l a s s o 1 = c ( )\n72 ham . l a s s o 2 = c ( )\n73 ham . l a s s o 3 = c ( )\n74 ham . e l a s t i c 1 = c ( )\n75 ham . e l a s t i c 2 = c ( )\n76 ham . e l a s t i c 3 = c ( )\n77\n78 # # # # GENERATE COMPOSITIONAL DATA MATRIX ###\n79 w h i l e ( j &lt;S ) {\n80 j j = j j +1\n81 s e t . s e e d ( j j )\n82\n83 Y&lt;? r c o m p o s ( n , l , p , b e t a , s i g m a )\n84 y&lt;? a s . m a t r i x ( p i v o t C o o r d (Y [ [ 1 ] ] ) )\n85 Z&lt;? a s . m a t r i x (Y [ [ 2 ] ] )\n86\n87 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )\n88 r e s u l t 1 . 1&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" s e p a r a b l e \" ,\n\nv a r i a n c e = \" known \" ,\n89 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n90 t h e t a = 0 . 8 )\n91\n92 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )\n93 r e s u l t 1 . 2&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" s e p a r a b l e \" ,\n\nv a r i a n c e = \" known \" ,\n94 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n95 t h e t a = 6 / p )\n96\n97 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )\n98 r e s u l t 1 . 3&lt;? SSLASSO ( Z , y [ , 1 ] , p e n a l t y = \" a d a p t i v e \" , v a r i a n c e\n\n= \" unknown \" ,\n99 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n\n100 t h e t a = 6 / p )\n101\n102 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n103 ## L a s s o\n\n\n\n90 APPENDIX A. Appendix 1\n\n104 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n105 r e s u l t 1 . 4 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = 1 ,\n\ns t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )\n106 c v . l a s s o . m o d t o t a l 1 . 4 = c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" ,\n\na l p h a = 1 )\n107 b e s t l a m . l a s s o 1 . 4 = c v . l a s s o . m o d t o t a l 1 . 4 $ l a m b d a . 1 s e\n108\n109 r e s u l t 1 . 4 . o p t = g l m n e t ( Z , y [ , 1 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE ,\n\nl a m b d a = b e s t l a m . l a s s o 1 . 4 , i n t e r c e p t =FALSE )\n110 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )\n111 c o e f . l a s s o 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 4 . o p t ) [ ?1 , ] , nrow =p?1)\n\n, 4 )\n112\n113 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n114 ## E l a s t i c n e t\n115 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n116 a&lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )\n117 s e a r c h&lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {\n118 c v&lt;? c v . g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , n f o l d = 1 0 ,\n\nt y p e . m e a s u r e = \" d e v i a n c e \" , p a r a l l e = TRUE , a l p h a = i )\n119 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a\n\n. 1 s e = c v $ l a m b d a . 1 s e , a l p h a = i )\n120 }\n121 c v 1&lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]\n122\n123 r e s u l t 1 . 5 = g l m n e t ( Z , y [ , 1 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = c v 1 $ a l p h a ,\n\nl a m b d a = c v 1 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )\n124\n125 c o e f . e l a s t i c 1 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 1 . 5 ) [ ?1 , ] , nrow =p?1)\n\n, 4 )\n126\n127 # # # # # Y2 # # # # #\n128 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n129 ## SSL\n130 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n131 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e )\n132 r e s u l t 2 . 1&lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = \" s e p a r a b l e \" ,\n\nv a r i a n c e = \" known \" ,\n133 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n\n\n\n91\n\n134 t h e t a = 0 . 8 )\n135\n136 # O r a c l e SSLASSO w i t h known v a r i a n c e ( S e p a r a b l e O r a c l e )\n137 r e s u l t 2 . 2&lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = \" s e p a r a b l e \" ,\n\nv a r i a n c e = \" known \" ,\n138 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n139 t h e t a = 6 / p )\n140\n141 # O r a c l e SSLASSO w i t h unknown v a r i a n c e ( Non?S e p a r a b l e )\n142 r e s u l t 2 . 3&lt;? SSLASSO ( Z , y [ , 2 ] , p e n a l t y = \" a d a p t i v e \" , v a r i a n c e\n\n= \" unknown \" ,\n143 l a m b d a 1 = l a m b d a 1 , l a m b d a 0 = l a m b d a 0 ,\n144 t h e t a = 6 / p )\n145\n146 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n147 ## L a s s o\n148 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n149 r e s u l t 2 . 4 = g l m n e t ( Z , y [ , 2 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = 1 ,\n\ns t a n d a r d i z e =TRUE , i n t e r c e p t =FALSE )\n150 c v . l a s s o . m o d t o t a l 2 . 4 = c v . g l m n e t ( Z , y [ , 2 ] , f a m i l y = \" g a u s s i a n \" ,\n\na l p h a = 1 )\n151 b e s t l a m . l a s s o 2 . 4 = c v . l a s s o . m o d t o t a l 2 . 4 $ l a m b d a . 1 s e\n152\n153 r e s u l t 2 . 4 . o p t = g l m n e t ( Z , y [ , 2 ] , a l p h a = 1 , s t a n d a r d i z e =TRUE ,\n\nl a m b d a = b e s t l a m . l a s s o 2 . 4 , i n t e r c e p t =FALSE )\n154 # p r i n t ( c o e f ( r e s u l t 1 . o p t ) , d i g i t = 3 )\n155 c o e f . l a s s o 2 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 2 . 4 . o p t ) [ ?1 , ] , nrow =p?1)\n\n, 4 )\n156\n157 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n158 ## E l a s t i c n e t\n159 # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n160 a&lt;? s e q ( 0 . 1 , 0 . 9 , 0 . 0 5 )\n161 s e a r c h&lt;? f o r e a c h ( i = a , . c o m b i n e = r b i n d ) %d o p a r% {\n162 c v&lt;? c v . g l m n e t ( Z , y [ , 2 ] , f a m i l y = \" g a u s s i a n \" , n f o l d = 1 0 ,\n\nt y p e . m e a s u r e = \" d e v i a n c e \" , p a r a l l e = TRUE , a l p h a = i )\n163 d a t a . f r a m e ( cvm = cv$cvm [ c v $ l a m b d a == c v $ l a m b d a . 1 s e ] , l a m b d a\n\n. 1 s e = c v $ l a m b d a . 1 s e , a l p h a = i )\n164 }\n\n\n\n92 APPENDIX A. Appendix 1\n\n165 c v 2&lt;? s e a r c h [ s e a r c h $ c v m == min ( s e a r c h $ c v m ) , ]\n166\n167 r e s u l t 2 . 5 = g l m n e t ( Z , y [ , 2 ] , f a m i l y = \" g a u s s i a n \" , a l p h a = c v 2 $ a l p h a ,\n\nl a m b d a = c v 2 $ l a m b d a . 1 s e , i n t e r c e p t =FALSE , s t a n d a r d i z e =TRUE )\n168\n169 c o e f . e l a s t i c 2 = r o u n d ( m a t r i x ( c o e f ( r e s u l t 2 . 5 ) [ ?1 , ] , nrow =p?1)\n\n, 4 )\n170\n171 # # # # # # # # # # # OUTPUTS # # # # # # # # # # #\n172 o u t p u t 1 . 1 [ [ j ] ] = r e s u l t 1 . 1 $ b e t a [ , 1 0 ]\n173 o u t p u t 1 . 2 [ [ j ] ] = r e s u l t 1 . 2 $ b e t a [ , 1 0 ]\n174 o u t p u t 1 . 3 [ [ j ] ] = r e s u l t 1 . 3 $ b e t a [ , 1 0 ]\n175 o u t p u t 2 . 1 [ [ j ] ] = r e s u l t 2 . 1 $ b e t a [ , 1 0 ]\n176 o u t p u t 2 . 2 [ [ j ] ] = r e s u l t 2 . 2 $ b e t a [ , 1 0 ]\n177 o u t p u t 2 . 3 [ [ j ] ] = r e s u l t 2 . 3 $ b e t a [ , 1 0 ]\n178\n179 e s t 1 . 1 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 1 [ [ j ] ] )\n180 e s t 1 . 2 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 2 [ [ j ] ] )\n181 e s t 1 . 3 [ , j ] = a s . m a t r i x ( o u t p u t 1 . 3 [ [ j ] ] )\n182 e s t 2 . 1 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 1 [ [ j ] ] )\n183 e s t 2 . 2 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 2 [ [ j ] ] )\n184 e s t 2 . 3 [ , j ] = a s . m a t r i x ( o u t p u t 2 . 3 [ [ j ] ] )\n185 e s t . l a s s o 1 [ , j ] = c o e f . l a s s o 1\n186 e s t . l a s s o 2 [ , j ] = c o e f . l a s s o 1\n187 e s t . e l a s t i c 1 [ , j ] = c o e f . e l a s t i c 1\n188 e s t . e l a s t i c 2 [ , j ] = c o e f . e l a s t i c 2\n189\n190 m e d i a s 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , mean )\n191 m e d i a s 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , mean )\n192 m e d i a s 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , mean )\n193 m e d i a s 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , mean )\n194 m e d i a s 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , mean )\n195 m e d i a s 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , mean )\n196 m e d i a s . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , mean )\n197 m e d i a s . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , mean )\n198 m e d i a s . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , mean )\n199 m e d i a s . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , mean )\n200\n201 v a r 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , v a r )\n\n\n\n93\n\n202 v a r 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , v a r )\n203 v a r 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , v a r )\n204 v a r 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , v a r )\n205 v a r 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , v a r )\n206 v a r 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , v a r )\n207 v a r . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , v a r )\n208 v a r . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , v a r )\n209 v a r . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , v a r )\n210 v a r . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , v a r )\n211\n212 s d 1 . 1 = a p p l y ( e s t 1 . 1 , 1 , s d )\n213 s d 1 . 2 = a p p l y ( e s t 1 . 2 , 1 , s d )\n214 s d 1 . 3 = a p p l y ( e s t 1 . 3 , 1 , s d )\n215 s d 2 . 1 = a p p l y ( e s t 2 . 1 , 1 , s d )\n216 s d 2 . 2 = a p p l y ( e s t 2 . 2 , 1 , s d )\n217 s d 2 . 3 = a p p l y ( e s t 2 . 3 , 1 , s d )\n218 s d . l a s s o 1 = a p p l y ( e s t . l a s s o 1 , 1 , s d )\n219 s d . l a s s o 2 = a p p l y ( e s t . l a s s o 2 , 1 , s d )\n220 s d . e l a s t i c 1 = a p p l y ( e s t . e l a s t i c 1 , 1 , s d )\n221 s d . e l a s t i c 2 = a p p l y ( e s t . e l a s t i c 2 , 1 , s d )\n222\n223 # # # # # d i a g n o s t i c s s t a t i s t i c s\n224 b i a s 1 . 1 = m e d i a s 1 .1? b e t a 1\n225 b i a s 1 . 2 = m e d i a s 1 .2? b e t a 1\n226 b i a s 1 . 3 = m e d i a s 1 .3? b e t a 1\n227 b i a s 2 . 1 = m e d i a s 2 .1? b e t a 2\n228 b i a s 2 . 2 = m e d i a s 2 .2? b e t a 2\n229 b i a s 2 . 3 = m e d i a s 2 .3? b e t a 2\n230 b i a s . l a s s o 1 = m e d i a s . l a s s o 1 ?b e t a 1\n231 b i a s . l a s s o 2 = m e d i a s . l a s s o 2 ?b e t a 2\n232 b i a s . e l a s t i c 1 = m e d i a s . e l a s t i c 1 ?b e t a 1\n233 b i a s . e l a s t i c 2 = m e d i a s . e l a s t i c 2 ?b e t a 2\n234\n235 mse1 . 1 = mean ( v a r 1 . 1 + ( b i a s 1 . 1 ^ 2 ) )\n236 mse1 . 2 = mean ( v a r 1 . 2 + ( b i a s 1 . 2 ^ 2 ) )\n237 mse1 . 3 = mean ( v a r 1 . 3 + ( b i a s 1 . 3 ^ 2 ) )\n238 mse2 . 1 = mean ( v a r 2 . 1 + ( b i a s 2 . 1 ^ 2 ) )\n239 mse2 . 2 = mean ( v a r 2 . 2 + ( b i a s 2 . 2 ^ 2 ) )\n240 mse2 . 3 = mean ( v a r 2 . 3 + ( b i a s 2 . 3 ^ 2 ) )\n\n\n\n94 APPENDIX A. Appendix 1\n\n241 mse . l a s s o 1 = mean ( v a r . l a s s o 1 + ( b i a s . l a s s o 1 ^ 2 ) )\n242 mse . l a s s o 2 = mean ( v a r . l a s s o 2 + ( b i a s . l a s s o 2 ^ 2 ) )\n243 mse . e l a s t i c 1 = mean ( v a r . e l a s t i c 1 + ( b i a s . e l a s t i c 1 ^ 2 ) )\n244 mse . e l a s t i c 2 = mean ( v a r . e l a s t i c 2 + ( b i a s . e l a s t i c 2 ^ 2 ) )\n245\n246 ## e r r o r p r e d i c t i o n\n247 p e 1 . 1 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 1 [ , j ] ) ^ 2 / n\n248 p e 1 . 2 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 2 [ , j ] ) ^ 2 / n\n249 p e 1 . 3 = sum ( y [ , 1 ] ?Z%?%e s t 1 . 3 [ , j ] ) ^ 2 / n\n250 p e 2 . 1 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 1 [ , j ] ) ^ 2 / n\n251 p e 2 . 2 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 2 [ , j ] ) ^ 2 / n\n252 p e 2 . 3 = sum ( y [ , 2 ] ?Z%?%e s t 2 . 3 [ , j ] ) ^ 2 / n\n253 p e . l a s s o 1 = sum ( y [ , 1 ] ?Z%?% e s t . l a s s o 1 [ , j ] ) ^ 2 / n\n254 p e . l a s s o 2 = sum ( y [ , 2 ] ?Z%?% e s t . l a s s o 2 [ , j ] ) ^ 2 / n\n255 p e . e l a s t i c 1 = sum ( y [ , 1 ] ?Z%?% e s t . e l a s t i c 1 [ , j ] ) ^ 2 / n\n256 p e . e l a s t i c 2 = sum ( y [ , 2 ] ?Z%?% e s t . e l a s t i c 2 [ , j ] ) ^ 2 / n\n257\n258 j = j +1\n259 c a t ( j , \" \" , i t e r + 1 , \" \\ n \" )\n260 i t e r&lt;? i t e r +1\n261 }\n262\n263 ## FP = f a l s e p o s i t i v e n u m b e r\n264 f p 1 . 1 = sum ( e s t 1 . 1 [ 4 , ] ! = 0 , e s t 1 . 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n265 f p 1 . 2 = sum ( e s t 1 . 2 [ 4 , ] ! = 0 , e s t 1 . 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n266 f p 1 . 3 = sum ( e s t 1 . 3 [ 4 , ] ! = 0 , e s t 1 . 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n267 f p 2 . 1 = sum ( e s t 2 . 1 [ 4 , ] ! = 0 , e s t 2 . 1 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n268 f p 2 . 2 = sum ( e s t 2 . 2 [ 4 , ] ! = 0 , e s t 2 . 2 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n269 f p 2 . 3 = sum ( e s t 2 . 3 [ 4 , ] ! = 0 , e s t 2 . 3 [ 8 : ( p?1) , ] ! = 0 ) / ( S?1)\n270 f p . l a s s o 1 = sum ( e s t . l a s s o 1 [ 4 , ] ! = 0 , e s t . l a s s o 1 [ 8 : ( p?1) , ] ! = 0 )\n\n/ ( S?1)\n271 f p . l a s s o 2 = sum ( e s t . l a s s o 2 [ 4 , ] ! = 0 , e s t . l a s s o 2 [ 8 : ( p?1) , ] ! = 0 )\n\n/ ( S?1)\n272 f p . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 4 , ] ! = 0 , e s t . e l a s t i c 1 [ 8 : ( p?1) , ]\n\n! = 0 ) / ( S?1)\n273 f p . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 4 , ] ! = 0 , e s t . e l a s t i c 2 [ 8 : ( p?1) , ]\n\n! = 0 ) / ( S?1)\n274\n275 ## FP = f a l s e n e g a t i v e n u m b e r\n\n\n\n95\n\n276 f n 1 . 1 = sum ( e s t 1 . 1 [ 1 : 3 , ] == 0 , e s t 1 . 1 [ 5 : 7 , ] == 0 ) / ( S?1)\n277 f n 1 . 2 = sum ( e s t 1 . 2 [ 1 : 3 , ] == 0 , e s t 1 . 2 [ 5 : 7 , ] == 0 ) / ( S?1)\n278 f n 1 . 3 = sum ( e s t 1 . 3 [ 1 : 3 , ] == 0 , e s t 1 . 3 [ 5 : 7 , ] == 0 ) / ( S?1)\n279 f n 2 . 1 = sum ( e s t 2 . 1 [ 1 : 3 , ] == 0 , e s t 2 . 1 [ 5 : 7 , ] == 0 ) / ( S?1)\n280 f n 2 . 2 = sum ( e s t 2 . 2 [ 1 : 3 , ] == 0 , e s t 2 . 2 [ 5 : 7 , ] == 0 ) / ( S?1)\n281 f n 2 . 3 = sum ( e s t 2 . 3 [ 1 : 3 , ] == 0 , e s t 2 . 3 [ 5 : 7 , ] == 0 ) / ( S?1)\n282 f n . l a s s o 1 = sum ( e s t . l a s s o 1 [ 1 : 3 , ] == 0 , e s t . l a s s o 1 [ 5 : 7 , ] == 0 ) / (\n\nS?1)\n283 f n . l a s s o 2 = sum ( e s t . l a s s o 2 [ 1 : 3 , ] == 0 , e s t . l a s s o 2 [ 5 : 7 , ] == 0 ) / (\n\nS?1)\n284 f n . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 1 : 3 , ] == 0 , e s t . e l a s t i c 1 [ 5 : 7 , ]\n\n== 0 ) / ( S?1)\n285 f n . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 1 : 3 , ] == 0 , e s t . e l a s t i c 2 [ 5 : 7 , ]\n\n== 0 ) / ( S?1)\n286\n287 ## Hamming d i s t a n c e\n288 ham . s s l 1 . 1 = sum ( e s t 1 . 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)\n289 ham . s s l 1 . 2 = sum ( e s t 1 . 2 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)\n290 ham . s s l 1 . 3 = sum ( e s t 1 . 3 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)\n291 ham . s s l 2 . 1 = sum ( e s t 2 . 1 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)\n292 ham . s s l 2 . 2 = sum ( e s t 2 . 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)\n293 ham . s s l 2 . 3 = sum ( e s t 2 . 3 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)\n294 ham . l a s s o 1 = sum ( e s t . l a s s o 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S?1)\n295 ham . l a s s o 2 = sum ( e s t . l a s s o 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S?1)\n296 ham . e l a s t i c 1 = sum ( e s t . e l a s t i c 1 [ 1 : ( p?1) , ] ! = b e t a 1 [ 1 : ( p?1) ] ) / ( S\n\n?1)\n297 ham . e l a s t i c 2 = sum ( e s t . e l a s t i c 2 [ 1 : ( p?1) , ] ! = b e t a 2 [ 1 : ( p?1) ] ) / ( S\n\n?1)"}]}}}