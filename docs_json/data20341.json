{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.25054"}, {"@name": "filename", "#text": "9629_Santos_ThiagoDiasdos_M.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA CIVIL,  \n\nARQUITETURA E URBANISMO \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nAplica\u00e7\u00e3o de Redes Neurais Artificiais em Simula\u00e7\u00e3o  \n\nNum\u00e9rica do Acoplamento Po\u00e7o-Reservat\u00f3rio \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nThiago Dias dos Santos \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \nCampinas \n\n 2012 \n\n\n\n i \n\nUNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA CIVIL, ARQUITETURA E URBANISMO \n \n\n \n\n \n\n \n\nAplica\u00e7\u00e3o de Redes Neurais Artificiais em Simula\u00e7\u00e3o Num\u00e9rica do \n\nAcoplamento Po\u00e7o-Reservat\u00f3rio \n \n\n \n\nThiago Dias dos Santos \n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\n \n\n \n\nDisserta\u00e7\u00e3o apresentada \u00e0 Comiss\u00e3o de \n\nP\u00f3s-gradua\u00e7\u00e3o da Faculdade de Engenharia \n\nCivil, Arquitetura e Urbanismo da \n\nUniversidade Estadual de Campinas, como \n\nparte dos requisitos para obten\u00e7\u00e3o do t\u00edtulo \n\nde Mestre em Engenharia Civil, na \u00e1rea de \n\nconcentra\u00e7\u00e3o de Estruturas. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nOrientador: Prof. Dr. Philippe Remy Bernard Devloo \n \n\n \n\n \n\n \n\n \nCampinas \n\n2012 \n\n\n\n \n\n ii  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFICHA  CATALOGR\u00c1FICA  ELABORADA  PELA  \n\n  BIBLIOTECA  DA  \u00c1REA  DE  ENGENHARIA  E  ARQUITETURA  -  BAE  -  UNICAMP \n\n \n\n \n\n \n\n \n\n    Sa59a \n\n \n\nSantos, Thiago Dias dos \n\n     Aplica\u00e7\u00e3o de redes neurais artificiais em simula\u00e7\u00e3o \n\nnum\u00e9rica do acoplamento po\u00e7o-reservat\u00f3rio / Thiago \n\nDias dos Santos. --Campinas, SP: [s.n.], 2012. \n\n \n\n     Orientador: Philippe Remy Bernard Devloo. \n\n     Disserta\u00e7\u00e3o de Mestrado - Universidade Estadual de \n\nCampinas, Faculdade de Engenharia Civil, Arquitetura e \n\nUrbanismo. \n\n \n\n     1. Redes neurais artificiais.  2. Engenharia de \n\npetr\u00f3leo.  3. Metodos de simula\u00e7\u00e3o.  4. Simula\u00e7\u00e3o por \n\ncomputador.  5. Analise num\u00e9rica.  I. Devloo, Philippe \n\nRemy Bernard.  II. Universidade Estadual de Campinas. \n\nFaculdade de Engenharia Civil, Arquitetura e \n\nUrbanismo.  III. T\u00edtulo. \n\n \n\n \n\nT\u00edtulo em Ingl\u00eas: Artificial neural networks applied to the numerical simulation of \n\nwell-reservoir coupling \n\nPalavras-chave em Ingl\u00eas: Artificial neural network, Petroleum engineering, \n\nSimulation methods, Computer simulation, Numerical \n\nanalysis \n\n\u00c1rea de concentra\u00e7\u00e3o: Estruturas \n\nTitula\u00e7\u00e3o: Mestre em Engenharia Civil \n\nBanca examinadora: Mario Conrado Cavichia, Romis Ribeiro de Faissol Attux \n\nData da defesa: 31-01-2012 \n\nPrograma de P\u00f3s Gradua\u00e7\u00e3o: Engenharia Civil \n\n\n\n\n\niv\n\n\n\n\u00c0 minha fam\u00edlia, exemplo de amor,\n\nhumildade e, principalmente, f\u00e9:\n\nGerv\u00e1sio Bento dos Santos,\n\nPh.D. na arte de doar a vida pela fam\u00edlia.\n\nBenedita Lopes Dias dos Santos,\n\nPh.D. na arte da obedi\u00eancia e da humildade.\n\nJacqueline Marques do Santos,\n\ndoutoranda na arte de conquistar pessoas.\n\nKevin Caio Marques dos Santos,\n\nmestrando na arte de aprender e de ser companheiro.\n\nv\n\n\n\nvi\n\n\n\nAgradecimentos\n\nAgrade\u00e7o a Deus, \"eterna verdade, verdadeira caridade e querida eternidade!\" (Sto Agostinho).\n\nAgrade\u00e7o especialmente aos enormes companheiros de batalha, os quais me ensinaram n\u00e3o so-\nmente o conhecimento t\u00e9cnico, mas tamb\u00e9m peculiaridades que somente a vida pode nos dar. S\u00e3o\neles:\n\n\u2022 Orientador Philippe Remy Bernard Devloo, engem\u00e1tico de ideias e de solu\u00e7\u00f5es n-dimensionais\ncomplexas, ou perplexas, dependendo do ponto de vista operacional;\n\n\u2022 Professora Silvana Bastos, UFPE, pelas sugest\u00f5es e aux\u00edlio na pesquisa em redes neurais\nartificiais;\n\n\u2022 Professor Francisco Antonio Menezes, grande amigo que tem me acompanhado na carreira\nacad\u00eamica desde as primeiras inicia\u00e7\u00f5es cient\u00edficas e grande exemplo profissional;\n\n\u2022 Amigos de estudo e trabalho do LabMeC: Jorge Calle, professora S\u00f4nia Maria Gomes, Tiago\nForti, Edimar Rylo, Gustavo Longhin, Alaor Rosa, Maur\u00edcio Souza, Agnaldo Farias, Denise\nSiqueira, Jo\u00e3o Gon\u00e7alves, Caju, Diogo Cec\u00edlio, Nathan Shauer, Mariane Montibeller Silva,\nAna Clara Galindo Rosa, Tiago Almeida, Jos\u00e9 Antonio Silverio, Tito Rezende;\n\n\u2022 Pessoal do LAPLA, pela amizade, solidariza\u00e7\u00e3o e cafezinhos compartilhados. Especial-\nmente: Isadora Salviano e Dani Lins.;\n\n\u2022 Amigos de todas as bandas e quantas. Especialmente Giovanna Trevizan, pela amizade e\npela m\u00fatua solidariza\u00e7\u00e3o frente \u00e0s dificuldades acad\u00eamicas; pessoal do TLC, por toda for\u00e7a e\nf\u00e9 ao longo dos \u00faltimos meses; ex-companheiros da FEC, pelo incentivo ao desenvolvimento\nprofissional e acad\u00eamico;\n\n\u2022 FEC - UNICAMP;\n\n\u2022 CENPES - PETROBRAS, ANP;\n\n\u2022 SimWorx.\n\nvii\n\n\n\nviii\n\n\n\nix\n\nDe um calculista que quase ficou doid\u00e3o\n\nQuando era criancinha acreditava em\n\nPapai Noel, Sacizinho e Bicho Pap\u00e3o.\n\nJ\u00e1 na mocidade aprendi na faculdade,\n\ncom dificuldade, a tal Singularidade.\n\nDepois, como balela, vi com uma tabela\n\nCrescimento de Ferros Dobrados a M\u00e3o\n\nTamb\u00e9m l\u00e1, os mestres me ensinaram at\u00e9\n\nTombamento de Muro em Torno do seu P\u00e9.\n\nE pra me graduar tive que tolerar,\n\nem um serm\u00e3o, que Viga Trabalha a Tor\u00e7\u00e3o\n\n40 anos depois...\n\nHoje com um bom lampi\u00e3o e sagacidade,\n\nca\u00e7o a singularidade e a viga a tor\u00e7\u00e3o.\n\nPor\u00e9m uma n\u00e3o resiste \u00e0 realidade\n\ne a outra n\u00e3o tolera deforma\u00e7\u00e3o.\n\nDa balela dos ferros que crescem na m\u00e3o,\n\ntem quem gosta mas n\u00e3o servem pra constru\u00e7\u00e3o.\n\nDos muros tombados, fora os que escorregaram\n\nbem apoiados sobre estacas em seu p\u00e9,\n\nsobram os que o seu bom sub-solo esmagaram,\n\npois t\u00e3o duro ch\u00e3o s\u00f3 mesmo com muita f\u00e9.\n\nFinalizando, saber com seriedade\n\ns\u00f3 se adquire com muita maturidade.\n\nFl\u00e1vio de Oliveira Costa\n\n\n\nx\n\n\n\nResumo\n\nNo presente trabalho, desenvolveu-se uma biblioteca para gera\u00e7\u00e3o de redes neurais artificiais (Neu-\nralLib) e aplicou-se a mesma para aproxima\u00e7\u00e3o do acoplamento de escoamento em po\u00e7os horizon-\ntais com reservat\u00f3rio. A biblioteca NeuralLib foi desenvolvida em linguagem C++. A arquitetura\nde rede gerada e utilizada foi a Multilayer Perceptron (MLP) com uma \u00fanica camada oculta. Optou-\nse em gerar 3 arquiteturas com diferentes n\u00fameros de neur\u00f4nios ocultos com objetivo de analisar\no comportamento das MLPs. O algoritmo de treinamento adotado foi o de retropropaga\u00e7\u00e3o ou\nbackpropagation.\n\nA rede neural foi utilizada para mapear o fluxo do reservat\u00f3rio tridimensional para o po\u00e7o\nhorizontal. O escoamento no po\u00e7o \u00e9 simulado utilizando leis constitutivas turbulentas e lamina-\nres. Foi elaborada uma t\u00e9cnica para gerar os conjuntos de padr\u00f5es para o processo de treinamento\ndas MLPs, utilizando para tal as curvas de fluxo do reservat\u00f3rio para o po\u00e7o provenientes de um\nmodelo tridimensional. As MLPs treinadas foram utilizadas na resolu\u00e7\u00e3o de um modelo unidimen-\nsional fornecendo valores de um par\u00e2metro de fluxo do reservat\u00f3rio. Nesse processo, o modelo\nunidimensional produziu curvas de fluxo no po\u00e7o semelhantes aos gerados pelo modelo tridimensi-\nonal. Os resultados s\u00e3o avaliados com rela\u00e7\u00e3o ao processo de treinamento das MLPs e com rela\u00e7\u00e3o\n\u00e0s curvas de fluxo e vaz\u00e3o total de produ\u00e7\u00e3o dos po\u00e7os.\n\nPalavras chave: Redes Neurais Artificiais, Engenharia de Petr\u00f3leo, M\u00e9tdos de Simula\u00e7\u00e3o, Simula\u00e7\u00e3o por\nComputador, An\u00e1lise Num\u00e9rica.\n\nxi\n\n\n\nxii\n\n\n\nAbstract\n\nIn this work, an object-oriented library was developed which implements neural networks (Neural-\n\nLib). The library was used to model the coupling of the fluid flow in a three-dimensional reservoir\n\nwith a one-dimensional well model. The architecture of the neural network is the Multilayer Per-\n\nceptron (MLP) with a single hidden layer. Three different architectures with varying number of\n\nhidden neurons were tested to evaluate the behaviour of the MLP. The backpropagation algorithm\n\nwas used to train the network.\n\nThe neural network was applied to estimate the mass flux from a three dimensional reservoir\n\nto a horizontal well. The fluid flow in the horizontal well uses laminar and turbulent constitutive\n\nmodels. A technique was developed to generate a set of patterns which were used to train the\n\nMLP\u2019s. The MLP\u2019s output data is a function which represents the mass flux from the reservoir to\n\nthe one dimensional well. Using the mass flux function, the pressure function in the horizontal well\n\nand well flux were very close to the pressure and flux computed using the three dimensional model.\n\nThe effectiveness of the neural network was evaluated by comparing cases which were not included\n\nin the original training set.\n\nKeywords: Artificial Neural Networks, Petroleum Engineering, Simulation Methods, Computer Simulation,\nNumerical Analysis.\n\nxiii\n\n\n\nxiv\n\n\n\nSum\u00e1rio\n\nSum\u00e1rio xv\n\nLista de Figuras xvii\n\nLista de Tabelas xix\n\nLista de S\u00edmbolos xxi\n\n1 Introdu\u00e7\u00e3o 1\n1.1 Apresenta\u00e7\u00e3o: Redes Neurais Artificiais . . . . . . . . . . . . . . . . . . . . . . . 1\n\n1.2 Motiva\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n\n1.3 Objetivos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.4 Organiza\u00e7\u00e3o do Trabalho . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n2 Revis\u00e3o Bibliogr\u00e1fica 5\n2.1 Redes Neurais Artificiais . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n2.1.1 Defini\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n2.1.2 Neur\u00f4nio Artificial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n2.1.3 Arquiteturas de Redes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n2.1.4 Processos de Aprendizagem . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n2.2 Acoplamento Po\u00e7o-Reservat\u00f3rio . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n2.2.1 Modelo Tridimensional do Acoplamento Po\u00e7o-Reservat\u00f3rio . . . . . . . . 21\n\n2.2.2 Escoamento no Reservat\u00f3rio . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n2.2.3 Escoamento no Po\u00e7o Horizontal . . . . . . . . . . . . . . . . . . . . . . . 24\n\n2.2.4 Curvas de Fluxo do Modelo Tridimensional . . . . . . . . . . . . . . . . . 25\n\n3 Metodologia 29\n3.1 Descri\u00e7\u00e3o Geral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n3.2 Ferramentas Computacionais Utilizadas . . . . . . . . . . . . . . . . . . . . . . . 30\n\nxv\n\n\n\nxvi SUM\u00c1RIO\n\n3.3 Acoplamento Po\u00e7o-Reservat\u00f3rio: Unidimensional . . . . . . . . . . . . . . . . . . 31\n3.4 An\u00e1lise da Resistividade K(x) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n3.4.1 Adimensionaliza\u00e7\u00e3o da Resistividade K(x) . . . . . . . . . . . . . . . . . 34\n3.4.2 Representa\u00e7\u00e3o de K(x) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n3.5 Aprendizagem da Rede Neural Artificial . . . . . . . . . . . . . . . . . . . . . . . 38\n3.5.1 Conjunto de Treinamento . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.5.2 Arquitetura de Rede Neural Utilizada . . . . . . . . . . . . . . . . . . . . 41\n3.5.3 Processo de Aprendizagem . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n3.6 MLP e o Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6.1 Resolu\u00e7\u00e3o do Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . 43\n3.6.2 An\u00e1lise dos Resultados: Medida de Erros . . . . . . . . . . . . . . . . . . 44\n\n4 Resultados 47\n4.1 NeuralLib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n4.2 Aprendizagem da Rede Neural . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n\n4.2.1 Arquitetura 1: MLP-5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.2.2 Arquitetura 2: MLP-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2.3 Arquitetura 3: MLP-15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.2.4 Compara\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n\n4.3 Redes Neurais e o Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . . 61\n4.3.1 MLP-5 e Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . . . 61\n4.3.2 MLP-10 e Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . . 63\n4.3.3 MLP-15 e Modelo Unidimensional . . . . . . . . . . . . . . . . . . . . . 65\n4.3.4 Compara\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n5 Conclus\u00e3o 77\n\nRefer\u00eancias Bibliogr\u00e1ficas 79\n\nA NeuralLib 83\nA.1 Descri\u00e7\u00e3o das Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\nA.2 Caso de Uso . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\nA.3 Arquivo Treinamento . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n\n\n\nLista de Figuras\n\n2.1 Modelo simplificado do neur\u00f4nio de McCulloch e Pitts. . . . . . . . . . . . . . . . 7\n\n2.2 Fun\u00e7\u00e3o de limiar. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n2.3 Fun\u00e7\u00e3o linear por partes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n2.4 Fun\u00e7\u00e3o log\u00edstica. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n2.5 Fun\u00e7\u00e3o tangente hiperb\u00f3lica. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n2.6 Exemplo de fun\u00e7\u00e3o de base radial: fun\u00e7\u00e3o Gaussiana. . . . . . . . . . . . . . . . . 11\n\n2.7 Rede alimentada adiante com uma camada de entrada e uma de sa\u00edda. . . . . . . . 12\n\n2.8 Rede alimentada adiante com uma camada oculta de neur\u00f4nios. . . . . . . . . . . . 13\n\n2.9 Rede recorrente autoalimentada. . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n2.10 Rede recorrente com entrada e sa\u00edda. . . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n2.11 Po\u00e7o horizontal e reservat\u00f3rio el\u00edpitco: elementos tridimensionais curvos. . . . . . 22\n\n2.12 Wireframe da malha do po\u00e7o horizontal e reservat\u00f3rio el\u00edpitco. . . . . . . . . . . . 22\n\n2.13 Curva de press\u00e3o no po\u00e7o horizontal. . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n2.14 Curva de vaz\u00e3o no po\u00e7o horizontal. . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n2.15 Curva de distribui\u00e7\u00e3o de vaz\u00e3o no po\u00e7o horizontal. . . . . . . . . . . . . . . . . . 27\n\n2.16 Valor do n\u00famero de Reynolds no po\u00e7o horizontal. . . . . . . . . . . . . . . . . . . 27\n\n3.1 Arranjo das redes neurais e modelo 1D do acomplamento po\u00e7o-resevat\u00f3rio . . . . . 30\n\n3.2 Modelo simplificado unidimensional do acoplamento po\u00e7o-reservat\u00f3rio. . . . . . . 31\n\n3.3 Exemplo da curva K(x). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n4.1 Fun\u00e7\u00e3o Peaks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n\n4.2 Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP. . . . . . . . . . . . . . 48\n\n4.3 Fun\u00e7\u00e3o descont\u00ednua. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n\n4.4 Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP. . . . . . . . . . . . . . 50\n\n4.5 Fun\u00e7\u00e3o Rastringin. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n\n4.6 Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP. . . . . . . . . . . . . . 51\n\n4.7 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas. . . . . 53\n\nxvii\n\n\n\nxviii LISTA DE FIGURAS\n\n4.8 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas. . . . . 53\n4.9 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas. . . . . 55\n4.10 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas. . . . . 55\n4.11 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas. . . . . 57\n4.12 Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas. . . . . 57\n4.13 Compara\u00e7\u00e3o das energias totais dos erros do conjunto A para as tr\u00eas MLPs. . . . . 59\n4.14 Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva Qhw(x). . . . . . . 67\n4.15 Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva p(x). . . . . . . . . 67\n4.16 Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva dQhw(x)/dx. . . . 68\n4.17 Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva d p(x)/dx. . . . . . 68\n4.18 Compara\u00e7\u00e3o norma L1 dos erros do conjunto A para as tr\u00eas MLPs - Qhw. . . . . . . 69\n4.19 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva Qhw(x). . . . . . . . 72\n4.20 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva p(x). . . . . . . . . 72\n4.21 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva dQhw(x)/dx. . . . . 73\n4.22 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva d p(x)/dx(x). . . . . 73\n4.23 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva Qhw(x). . . . . . . . . 74\n4.24 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva p(x). . . . . . . . . . 74\n4.25 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva dQhw(x)/dx. . . . . . 75\n4.26 Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva d p(x)/dx(x). . . . . . 75\n\n\n\nLista de Tabelas\n\n4.1 Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate. . . . . . . . . . . . . . . 54\n4.2 Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate. . . . . . . . . . . . . . . 56\n4.3 Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate. . . . . . . . . . . . . . . 58\n4.4 Energia m\u00e9dia do erro para o subconjunto AT . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Energia m\u00e9dia do erro para o subconjunto Av. . . . . . . . . . . . . . . . . . . . . 60\n4.6 Energia m\u00e9dia do erro para o subconjunto Ate. . . . . . . . . . . . . . . . . . . . . 60\n4.7 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x). . . . . . . . . . . . . . . 61\n4.8 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x). . . . . . . . . . . . . . . . . 61\n4.9 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx. . . . . . . . . . . . 62\n4.10 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx. . . . . . . . . . . . . . 62\n4.11 Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw. . . . . . . . . . . . . . . 62\n4.12 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x). . . . . . . . . . . . . . . 63\n4.13 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x). . . . . . . . . . . . . . . . . 63\n4.14 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx. . . . . . . . . . . . 63\n4.15 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx. . . . . . . . . . . . . . 63\n4.16 Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw. . . . . . . . . . . . . . . 64\n4.17 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x). . . . . . . . . . . . . . . 65\n4.18 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x). . . . . . . . . . . . . . . . . 65\n4.19 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx. . . . . . . . . . . . 65\n4.20 Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx. . . . . . . . . . . . . . 65\n4.21 Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw. . . . . . . . . . . . . . . 66\n4.22 Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto AT . . . . . . . 70\n4.23 Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto Av. . . . . . . 70\n4.24 Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto Ate. . . . . . . 71\n\nA.1 Trecho de um arquivo de treinamento de uma rede neural. . . . . . . . . . . . . . . 90\n\nxix\n\n\n\nxx LISTA DE TABELAS\n\n\n\nLista de S\u00edmbolos\n\nMLP (def. p\u00e1g. 2) Multilayer Perceptron\n\n?(\u00b7) (def. p\u00e1g. 8) fun\u00e7\u00e3o de ativa\u00e7\u00e3o de um neur\u00f4nio\nuk (def. p\u00e1g. 8) sa\u00edda do combinador linear do neur\u00f4nio k\n\nyk (def. p\u00e1g. 8) sa\u00edda do neur\u00f4nio k\n\nwk j (def. p\u00e1g. 8) peso sin\u00e1ptico do neur\u00f4nio k para o sinal de entrada j\n\nx j (def. p\u00e1g. 8) sinal de entrada j\n\nbk (def. p\u00e1g. 8) bias do neur\u00f4nio k\n\nvk (def. p\u00e1g. 8) campo local induzido do neur\u00f4nio k\n\nr (def. p\u00e1g. 11) raio de abertura da fun\u00e7\u00e3o de base radial\n\n~y (def. p\u00e1g. 16) vetor sinal de sa\u00edda da rede\n~d (def. p\u00e1g. 16) vetor sa\u00edda esperada da rede\n\ndi (def. p\u00e1g. 16) valor de sa\u00edda esperado do neur\u00f4nio i\n\n~e (def. p\u00e1g. 16) vetor sinal de erro da rede\n\nei (def. p\u00e1g. 16) sinal de erro do neur\u00f4nio i\n\n? (def. p\u00e1g. 16) valor instant\u00e2neo da energia do erro da rede neural\n\n?med (def. p\u00e1g. 17) valor m\u00e9dio da energia do erro da rede neural\n\nT (def. p\u00e1g. 17) dimens\u00e3o da amostra de treinamento\n\n4w ji (def. p\u00e1g. 18) fator de corre\u00e7\u00e3o do peso sin\u00e1ptico i do neur\u00f4nio j\n? (def. p\u00e1g. 18) taxa de aprendizagem da rede neural\n\n? j (def. p\u00e1g. 18) gradiente local para o neur\u00f4nio j\n\n?\n?\nj(\u00b7) (def. p\u00e1g. 18) derivada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o do neur\u00f4nio j\n\n? (def. p\u00e1g. 20) constante de momento da regra delta generalizada\n\nQ (def. p\u00e1g. 23) fluxo de fluido\n\n\u00b5 (def. p\u00e1g. 23) viscosidade do \u00f3leo\n??\nK (def. p\u00e1g. 23) tensor de permeabilidade do meio poroso\n\nxxi\n\n\n\nxxii LISTA DE S\u00cdMBOLOS\n\np (def. p\u00e1g. 23) press\u00e3o do fluido\n\n~n (def. p\u00e1g. 23) vetor normal a uma face\n\n?0 (def. p\u00e1g. 23) fluxo normal conhecido numa face (c.c. Neumann)\n\np0 (def. p\u00e1g. 23) press\u00e3o conhecida numa face ou regi\u00e3o (c.c. Dirichlet)\n\nx (def. p\u00e1g. 24) coordenada do eixo longitudinal do po\u00e7o\n\nf (def. p\u00e1g. 24) fator de fric\u00e7\u00e3o\n\nV (def. p\u00e1g. 24) velocidade m\u00e9dia do fluxo na se\u00e7\u00e3o circular do po\u00e7o\n\n? (def. p\u00e1g. 24) massa espec\u00edfica do fluido\n\nD (def. p\u00e1g. 24) di\u00e2metro do po\u00e7o\n\nQhw (def. p\u00e1g. 24) vaz\u00e3o de fluido no po\u00e7o horizontal\n\nql (def. p\u00e1g. 24) fluxo por unidade de comprimento\n\nQheel (def. p\u00e1g. 25) vaz\u00e3o de fluido no po\u00e7o horizontal (no heel)\n\np(x) (def. p\u00e1g. 25) press\u00e3o ao longo do po\u00e7o\n\nQhw(x) (def. p\u00e1g. 26) vaz\u00e3o ao longo do po\u00e7o\ndQhw(x)\n\ndx (def. p\u00e1g. 27) distribui\u00e7\u00e3o de vaz\u00e3o ao longo do po\u00e7o\n\nRey(x) (def. p\u00e1g. 27) n\u00famero de Reynolds ao longo do po\u00e7o\nd p(x)\n\ndx (def. p\u00e1g. 27) distribui\u00e7\u00e3o de press\u00e3o ao longo do po\u00e7o\n\nK(x) (def. p\u00e1g. 32) resistividade entre os fluxos do reservat\u00f3rio e do po\u00e7o\n\npres (def. p\u00e1g. 32) press\u00e3o no reservat\u00f3rio (far field)\n\nxheel (def. p\u00e1g. 33) coordenada relativa ao calcanhar do po\u00e7o horizontal\n\nxtoe (def. p\u00e1g. 33) coordenada relativa ao ded\u00e3o do po\u00e7o horizontal\n\npheel (def. p\u00e1g. 33) press\u00e3o no calcanhar do po\u00e7o horizontal\n\nQtoe (def. p\u00e1g. 33) vaz\u00e3o no ded\u00e3o do po\u00e7o horizontal\n\nQw (def. p\u00e1g. 34) vaz\u00e3o total do po\u00e7o vertical (equa\u00e7\u00e3o po\u00e7o vertical)\n\nk (def. p\u00e1g. 34) permeabilidade meio poroso (equa\u00e7\u00e3o po\u00e7o vertical)\n\nh (def. p\u00e1g. 34) altura do reservat\u00f3rio (equa\u00e7\u00e3o po\u00e7o vertical)\n\npw (def. p\u00e1g. 34) press\u00e3o no po\u00e7o vertical (equa\u00e7\u00e3o po\u00e7o vertical)\n\nrres (def. p\u00e1g. 34) raio externo do reservat\u00f3rio (equa\u00e7\u00e3o do po\u00e7o vertical)\n\nrw (def. p\u00e1g. 34) raio do po\u00e7o vertical (equa\u00e7\u00e3o do po\u00e7o vertical)\n\nqlw (def. p\u00e1g. 34) fluxo por unidade de comprimento (po\u00e7o vertical)\n\nKvw (def. p\u00e1g. 34) resistividade para o po\u00e7o vertical\n\nK?vw (def. p\u00e1g. 35) resistividade adimensional para o po\u00e7o vertical\n\n\n\nLISTA DE S\u00cdMBOLOS xxiii\n\nlhw (def. p\u00e1g. 35) comprimento total do po\u00e7o horizontal\n\nBres (def. p\u00e1g. 35) largura do reservat\u00f3rio\n\nHres (def. p\u00e1g. 35) altura do reservat\u00f3rio\n\n?(x) (def. p\u00e1g. 35) fator de corre\u00e7\u00e3o\n\nKhw (def. p\u00e1g. 35) resistividade constante para o po\u00e7o horizontal\n\nK?(x) (def. p\u00e1g. 36) resistividade adimensional para o po\u00e7o horizontal\n\n?(x) (def. p\u00e1g. 36) fun\u00e7\u00e3o peso do polin\u00f4mio de Legendre\n\nK?p(x) (def. p\u00e1g. 36) resistividade adimensional aproximada por polin\u00f4mios\n\nan (def. p\u00e1g. 36) n-\u00e9simo coeficiente do n-\u00e9simo polin\u00f4mio de Legendre\n\nLn (def. p\u00e1g. 36) n-\u00e9simo polin\u00f4mio de Legendre\n\nE7 (def. p\u00e1g. 38) fun\u00e7\u00e3o objetivo da equa\u00e7\u00e3o de quadrados m\u00ednimos\n\n~a (def. p\u00e1g. 38) vetor de sa\u00edda esperado das MLPs (? ~d)\n~? (def. p\u00e1g. 39) vetor sinal de entrada das MLPs\n\nnt (def. p\u00e1g. 39) n\u00famero total de casos gerados para compor o conjunto A\n\nA (def. p\u00e1g. 39) conjunto que cont\u00e9m todos os casos para treinamento\n\nAT (def. p\u00e1g. 40) conjunto de treinamento (ajustes dos pesos sin\u00e1pticos)\n\nAv (def. p\u00e1g. 40) conjunto de valida\u00e7\u00e3o\n\nAte (def. p\u00e1g. 40) conjunto de teste\n\nptoe (def. p\u00e1g. 43) press\u00e3o no ded\u00e3o do po\u00e7o horizontal\n\n4p (def. p\u00e1g. 43) diferencial de press\u00e3o, usado no Runge-Kutta\nmi (def. p\u00e1g. 43) fatores do algoritmo de Runge-Kutta\n\nk (def. p\u00e1g. 43) ponto no dom\u00ednio do po\u00e7o para o algoritmo de Runge-Kutta\n\nL2 (def. p\u00e1g. 44) norma L2\n\nE f (def. p\u00e1g. 44) erro em norma L2 para an\u00e1lise das curvas no po\u00e7o\n\nL1 (def. p\u00e1g. 45) norma L1\n\nEQ (def. p\u00e1g. 45) erro em norma L2 da curva Qhw(x)\n\nEdQdx (def. p\u00e1g. 45) erro em norma L2 da curva\ndQhw(x)\n\ndx\n\nEp (def. p\u00e1g. 45) erro em norma L2 da curva p(x)\n\nEd pdx (def. p\u00e1g. 45) erro em norma L2 da curva\nd p(x)\n\ndx\n\nE L\n1\n\nQhw\n(def. p\u00e1g. 45) erro em norma L1da vaz\u00e3o do po\u00e7o Qhw\n\nEQhw (def. p\u00e1g. 45) erro em norma L\n1alterada da vaz\u00e3o do po\u00e7o Qhw\n\n? (def. p\u00e1g. 45) desvio padr\u00e3o amostral\n\n\n\nCap\u00edtulo 1\n\nIntrodu\u00e7\u00e3o\n\nO uso de redes neurais artificiais em diversas \u00e1reas do conhecimento humano tem sido propagado\nnas \u00faltimas d\u00e9cadas devido aos grandes avan\u00e7os no desenvolvimento de computadores, fato que\nreestimulou as pesquisas na \u00e1rea de Intelig\u00eanica Artificial (IA) e suas ramifica\u00e7\u00f5es.\n\nO avan\u00e7o na capacidade de processamento de dados tamb\u00e9m estimulou pesquisas e desenvolvi-\nmento de simuladores num\u00e9ricos envolvendo problemas f\u00edsicos mais complexos de forma a obter\nsimula\u00e7\u00f5es mais realistas e datalhadas. Al\u00e9m do mais, a busca por simula\u00e7\u00f5es eficientes e r\u00e1pidas,\nprincipalmente em aplica\u00e7\u00f5es industriais, tem estimulado a intera\u00e7\u00e3o entre as duas \u00e1reas: redes\nneurais artificiais e simula\u00e7\u00e3o num\u00e9rica.\n\nO acoplamento po\u00e7o-reservat\u00f3rio \u00e9 um problema muito comum na engenharia de petr\u00f3leo\nquando se trata de po\u00e7os perfurados de forma horizontal (po\u00e7os horizontais). Embora existam\nalgumas equa\u00e7\u00f5es anal\u00edticas e semi-anal\u00edticas que o descrevem, como apresentado no trabalho de\nJoshi (1991), o acoplamento po\u00e7o-reservat\u00f3rio tem sido simulado usando m\u00e9todos num\u00e9ricos tais\ncomo elementos finitos, diferen\u00e7as finitas, volumes finitos etc. Em geral, cada autor prop\u00f5e me-\ntodologias diferentes para acoplar as formula\u00e7\u00f5es de fluxo no reservat\u00f3rio com as do po\u00e7o, de\nacordo com as necessidades de cada um, mas todos procuram obter informa\u00e7\u00f5es e detalhes sobre o\ncomportamento dos fluxos no interior do po\u00e7o.\n\nNeste contexto, o presente trabalho tem por objetivo aplicar redes neurais artificiais na simula-\n\u00e7\u00e3o num\u00e9rica do acoplamento po\u00e7o-reservat\u00f3rio, buscando uma metodologia que possa ser aplicada\nem outras \u00e1reas e problemas da engenharia.\n\n1.1 Apresenta\u00e7\u00e3o: Redes Neurais Artificiais\n\nAs pesquisas em Intelig\u00eancia Artificial (IA) nas \u00faltimas d\u00e9cadas proporcionaram o surgimento de\ndiversos tipos e arquiteturas de redes neurais, cada uma com caracter\u00edsticas pr\u00f3prias e aplica\u00e7\u00f5es\n\n1\n\n\n\n2 CAP\u00cdTULO 1. INTRODU\u00c7\u00c3O\n\nvariadas. Um tipo de rede neural artificial muito comum \u00e9 o perceptron de m\u00faltiplas camadas\n(MLP - Multilayer Perceptron). Este tipo de rede neural \u00e9 amplamente utilizado para mapeamento\nde fun\u00e7\u00f5es devido \u00e0 sua capacidade de \"aprender\" dados existentes e de generalizar valores. Assim,\napresentado um conjunto de padr\u00f5es ou dados de treinamento para a MLP, ap\u00f3s o processo de trei-\nnamento, a rede poder\u00e1, idealmente, ser capaz de generalizar valores relativos \u00e0 fun\u00e7\u00e3o associada\ncom os dados de treinamento. Em termos matem\u00e1ticos, um conjunto de dados de treinamento A \u00e9\ndefinido por:\n\nA ={(~xi,~yi) : ~yi = f (~xi), i = 1,..., n, n ? N},\n\nem que f (~xi) \u00e9 uma rela\u00e7\u00e3o definida sobre A.\n\nA fun\u00e7\u00e3o f (~xi) pode ser uma express\u00e3o anal\u00edtica, um modelo num\u00e9rico ou mesmo uma rela\u00e7\u00e3o\nobservada a partir de experimentos. Espera-se que a rede MLP treinada a partir de A gere valores\n~? j tais que ~? j ?=~y j = f (~x j), j? N, para j 6= i e j = i, descrevendo assim a capacidade da MLP de\nmapear a fun\u00e7\u00e3o f (~xi).\n\nO trabalho utilizou o modelo tridimensional de acoplamento po\u00e7o-reservat\u00f3rio desenvolvido\npor Devloo et al. (2009) para gerar um conjunto de dados do tipo A para treinamento de uma MLP.\n\u00c9 apresentada uma t\u00e9cnica desenvolvida para gerar o conjunto A de forma que a MLP treinada bus-\nque fornecer um par\u00e2metro de fluxo do reservat\u00f3rio para um modelo simplificado unidimensional\ndo problema po\u00e7o-reservat\u00f3rio. A ideia \u00e9 que esse modelo simplificado consiga gerar resultados\nsemelhantes ao modelo tridimensional.\n\n1.2 Motiva\u00e7\u00e3o\n\nRedes neurais artificiais podem ser aplicadas em diversos problemas f\u00edsicos da engenharia, com-\npondo desde controle e automa\u00e7\u00e3o de processos at\u00e9 mapeamento de fun\u00e7\u00f5es. O grupo de pesquisa\ndo Laborat\u00f3rio de Mec\u00e2nica Computacional - LabMeC, da Faculdade de Engenharia Civil, Ar-\nquitetura e Urbanismo da UNICAMP, tem interesse no dom\u00ednio de tal tecnologia para uso nas\npesquisas relacionadas \u00e0 mec\u00e2nica computacional e \u00e0 simula\u00e7\u00e3o num\u00e9rica. Foi definido como caso\nde uso um problema num\u00e9rico de acoplamento po\u00e7o-reservat\u00f3rio, objeto de projetos de pesquisa\ndo laborat\u00f3rio e de interesse da ind\u00fastria e engenharia de petr\u00f3leo.\n\nA motiva\u00e7\u00e3o se faz em utilizar as redes neurais juntamente com um modelo simplificado de\nforma a substituir o modelo de elementos finitos quando do estudo pr\u00e9vio dos par\u00e2metros que\nregem o problema f\u00edsico. Explica-se: o modelo de elementos finitos demanda um certo tempo para\nsimular as equa\u00e7\u00f5es tridimensionais dos fluxos; por\u00e9m, para um estudo pr\u00e9vio de projeto de po\u00e7o\n\n\n\n1.3. OBJETIVOS 3\n\nou mesmo para entender o comportamento de fluxos quando se alteram os par\u00e2metros, um tempo\nlongo \u00e9 demandado para obter todas as respostas. Para agilizar esse processo de projeto pr\u00e9vio e do\nestudo do comportamento do problema f\u00edsico, surgiu a ideia de se utilizar um modelo substituto que\npudesse gerar resultados semelhantes ao do modelo original, por\u00e9m em um tempo extremamente\ncurto. O modelo substituto deveria ser capaz de interagir com o projetista ou engenheiro de forma\nque esse poderia variar um par\u00e2metro qualquer, por meio de uma barra de rolagem, por exemplo,\ne o modelo geraria os resultados instantaneamente. Diante disso, optou-se em utilizar a tecnologia\nde redes neurais artificiais como composi\u00e7\u00e3o desse modelo substituto.\n\n1.3 Objetivos\n\nA teoria de redes neurais artificiais vem se consolidando, mundialmente, como uma nova e efi-\nciente ferramenta para lidar com a ampla classe dos assim chamados problemas complexos, em\nque extensas massas de dados devem ser modelados e analisados em um contexto multidisciplinar,\nenvolvendo, simultaneamente, tanto os aspectos estat\u00edsticos e computacionais como os din\u00e2micos\ne de otimiza\u00e7\u00e3o (Kov\u00e1cs, 2006). Em vista disto, o uso de redes neurais artificiais nos mais diversos\nproblemas da engenharia moderna n\u00e3o seria nada mais do que um pequeno passo neste processo de\nevolu\u00e7\u00e3o de ferramentas num\u00e9ricas associadas com intelig\u00eancia artificial e teorias conexionistas.\n\nNesse contexto, o trabalho tem por objetivo justamente estudar, desenvolver e utilizar das ca-\nracter\u00edsticas de uma rede neural do tipo perceptron de m\u00faltiplas camadas (MLP) num problema\nde simula\u00e7\u00e3o num\u00e9rica da engenharia de petr\u00f3leo: o acoplamento po\u00e7o-reservat\u00f3rio. Por se tratar\nde um problema complexo, o trabalho visar\u00e1 analisar o uso da MLP num modelo num\u00e9rico j\u00e1 de-\nsenvolvido do acoplamento (Devloo et al., 2009), observando as potencialidades e limita\u00e7\u00f5es da\nrede neural no presente tema. Al\u00e9m do mais, a metologia desenvolvida n\u00e3o tem a inten\u00e7\u00e3o de se\nrestringir a esse problema, mas sim de deixar potencialidade de aplica\u00e7\u00e3o em outros campos da\nengenharia.\n\n1.4 Organiza\u00e7\u00e3o do Trabalho\n\nO trabalho \u00e9 organizado em 4 cap\u00edtulos e um ap\u00eandice, a saber: Revis\u00e3o Bibliogr\u00e1fica, Metodo-\nlogia, Resultados, Conlus\u00e3o e Ap\u00eandice A. A seguir, s\u00e3o descritos resumidamente o conte\u00fado de\ncada um.\n\nEm Revis\u00e3o Bibliogr\u00e1fica s\u00e3o apresentados conceitos sobre redes neurais artificiais e sobre o\nmodelo de acomplamento po\u00e7o-reservat\u00f3rio. Na se\u00e7\u00e3o sobre redes neurais, descreve-se o neur\u00f4-\nnio artificial, algumas arquiteturas de redes e o processo de aprendizagem utilizado no trabalho\n\n\n\n4 CAP\u00cdTULO 1. INTRODU\u00c7\u00c3O\n\n(retropropaga\u00e7\u00e3o). O acomplamento po\u00e7o-reservat\u00f3rio \u00e9 descrito de acordo com o modelo tridi-\nmensional desenvolvido por Devloo et al. (2009). S\u00e3o descritas as equa\u00e7\u00f5es de escoamento no\nreservat\u00f3rio e no po\u00e7o, assim como as curvas de fluxo no interior do po\u00e7o.\n\nNo cap\u00edtulo Metodologia, s\u00e3o descritas as ferramentas computacionais utilizadas, o modelo\nsimplificado unidimensional do acomplamento po\u00e7o-reservat\u00f3rio, a an\u00e1lise de um par\u00e2metro de\nfluxo (resistividade K (x)), o processo de aprendizagem das redes neurais geradas (MLPs) e o ar-\nranjo das MLPs com o modelo unidimensional.\n\nO cap\u00edtulo Resultados apresenta alguns exemplos de valida\u00e7\u00e3o da biblioteca produzida para\ngera\u00e7\u00e3o de redes neurais (NeuralLib). S\u00e3o apresentados tamb\u00e9m os resultados referentes \u00e0 apren-\ndizagem das MLPs utilizadas e os resultados referentes ao arranjo do modelo unidimensinal com\nessas MLPs. Gr\u00e1ficos e tabelas comparativas foram produzidas como forma de analisar a aprendi-\nzagem de cada MLP e a resolu\u00e7\u00e3o do modelo unidimensional.\n\nNo cap\u00edtulo Conclus\u00e3o s\u00e3o apresentados discuss\u00f5es sobre o trabalho como todo, ressaltando\nalguns detalhes observados em Resultados.\n\nO Ap\u00eandice A descreve em maiores detalhes a biblioteca NeuralLib. S\u00e3o descritas as classes\nimplementadas, um caso de uso para gera\u00e7\u00e3o de uma rede tipo MLP e uma descri\u00e7\u00e3o sobre o\nformato do arquivo de treinamento.\n\n\n\nCap\u00edtulo 2\n\nRevis\u00e3o Bibliogr\u00e1fica\n\nProcurou-se elencar os princ\u00edpios e conceitos fundamentais dos dois assuntos-chave deste trabalho:\nredes neurais artificias e acomplamento po\u00e7o-reservat\u00f3rio. A revis\u00e3o bibliogr\u00e1fica descreve cada\nassunto num \u00e2mbito mais generalista e detalha os t\u00f3picos que de fato s\u00e3o empregados no presente\ntrabalho.\n\nAlguns exemplos de aplica\u00e7\u00e3o de redes neurais artificiais como ferramenta auxiliar em mo-\ndelagens num\u00e9ricas relacionados \u00e0 ind\u00fastria do petr\u00f3leo podem ser encontrados nos trabalhos de\nAminzadeh et al. (2000), Nguyen et al. (2002), Alrumah (2003), Pimentel et al. (2005) e Villanueva\n(2008).\n\n2.1 Redes Neurais Artificiais\n\nDescrever-se-\u00e1 brevemente sobre redes neurais artificiais, contextualizando os princ\u00edpios funda-\nmentais que regem seu funcionamento, potencialidades e tipologias de redes. Inicialmente explicar-\nse-\u00e1 a unidade b\u00e1sica de processamento \u00e0 semelhan\u00e7a do c\u00e9rebro humano: o neur\u00f4nio artificial.\nFormas de arranjo de tais neur\u00f4nios (arquitetura de redes) e tipos de treinamento ser\u00e3o descritos\nem sequ\u00eancia. Um algoritmo de aprendizagem tamb\u00e9m \u00e9 descrito ao decorrer da se\u00e7\u00e3o. A revi-\ns\u00e3o \u00e9 baseada principalmente nos trabalhos de Haykin (2001), de Braga et al. (2007) e de Zuben\n(1996). Outros trabalhos tamb\u00e9m serviram como base te\u00f3rica nesse assunto: Fausett (1994), Has-\nsoun (1995) e Galushkin (2007).\n\n2.1.1 Defini\u00e7\u00e3o\n\nRedes neurais artificiais, bem conhecidas tamb\u00e9m como redes neurais, s\u00e3o modelos matem\u00e1ticos\nque tentam descrever o funcionamento de um c\u00e9rebro quanto ao processamento de informa\u00e7\u00f5es.\n\n5\n\n\n\n6 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nO c\u00e9rebro \u00e9 um computador (sistema de processamento de informa\u00e7\u00e3o) altamente complexo, n\u00e3o-\nlinear e paralelo, capaz de organizar seus constituintes estruturais, conhecidos como neur\u00f4nios, de\nforma que processamentos complexos sejam realizados numa velocidade muito maior do que os\ncomputadores digitais atualmente existentes. Essa capacidade do c\u00e9rebro de realizar tarefas com-\nplexas, apenas usando unidades simples de processamento (neur\u00f4nios), motiva os estudos e avan-\n\u00e7os sobre redes neurais artificiais na tentativa de modelar tal capacidade utilizando-se componentes\neletr\u00f4nicos ou mesmo programa\u00e7\u00e3o em computadores digitais.\n\n2.1.2 Neur\u00f4nio Artificial\n\nOs neur\u00f4nios artificiais s\u00e3o unidades de processamento de informa\u00e7\u00e3o fundamentais para o funcio-\nnamento das redes neurais artificiais. Tais unidades de processamento nada mais s\u00e3o que modelos\nartificiais de um neur\u00f4nio biol\u00f3gico. Assim, os avan\u00e7os nos estudos sobre o funcionamento do\nc\u00e9rebro e, consequentemente, dos neur\u00f4nios biol\u00f3gicos, ocorridos nas primeiras d\u00e9cadas do s\u00e9culo\npassado t\u00eam motivado fisiologistas, f\u00edsicos e matem\u00e1ticos a encontrarem um modelo que assim des-\ncrevesse seus comportamentos. Uma breve descri\u00e7\u00e3o sobre o funcionamento do neur\u00f4nio biol\u00f3gico\n\u00e9 realizada na sequ\u00eancia.\n\nNeur\u00f4nio Biol\u00f3gico\n\nOs neur\u00f4nios biol\u00f3gicos, segundo Kov\u00e1cs (1997), assim como qualquer c\u00e9lula biol\u00f3gica, s\u00e3o de-\nlimitados por uma fina membrana celular a qual, al\u00e9m de sua fun\u00e7\u00e3o biol\u00f3gica normal, possui\ndeterminadas propriedades que s\u00e3o essenciais para o funcionamento el\u00e9trico dessas c\u00e9lulas nervo-\nsas.\n\nEstruturalmente, os neur\u00f4nios biol\u00f3gicos s\u00e3o divididos, de forma geral, em tr\u00eas se\u00e7\u00f5es: o corpo\ncelular, os dendritos e o ax\u00f4nio. Cada se\u00e7\u00e3o possui fun\u00e7\u00f5es espec\u00edficas que, em conjunto, ca-\nracterizam o funcionamento do neur\u00f4nio. O corpo celular apresenta-se como a se\u00e7\u00e3o de menor\ndimens\u00e3o, da ordem de mil\u00e9simos de mil\u00edmetro. Os dendritos s\u00e3o estruturas com alguns mil\u00edme-\ntros, e o ax\u00f4nio j\u00e1 pode ser mais longo. Um neur\u00f4nio pode possuir muitos dendritos, mas possui\nsomente um ax\u00f4nio. Basicamente, os dendritos t\u00eam a fun\u00e7\u00e3o de receber sinais el\u00e9tricos provindos\nde outros neur\u00f4nios, de forma que o corpo celular, ao processar tais sinais, envia-os para outros\nneur\u00f4nios atrav\u00e9s do ax\u00f4nio. Assim, o fluxo de sinais el\u00e9tricos num neur\u00f4nio inicia pela recep\u00e7\u00e3o\nnos dentritos, sendo processado no corpo celular e transmitido pelo ax\u00f4nio para outros dendritos\nde outros neur\u00f4nios, reinicializando o ciclo. O ponto de contato entre o ax\u00f4nio de um neur\u00f4nio e\no dendrito de outro \u00e9 conhecido como sinapse, e \u00e9 atrav\u00e9s das sinapses que os neur\u00f4nios se unem\nfuncionalmente, compondo as redes neurais biol\u00f3gicas.\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 7\n\nO tipo mais comum de sinapse \u00e9 a sinapse qu\u00edmica. O processo a ela subjacente consiste\nna convers\u00e3o de um sinal el\u00e9trico provindo de um ax\u00f4nio (pr\u00e9-sin\u00e1ptico) em um sinal qu\u00edmico o\nqual, por sua vez, \u00e9 convertido num outro sinal el\u00e9trico (p\u00f3s-sin\u00e1ptico), que \u00e9 transmitido para o\ndendrito de outro neur\u00f4nio. Assume-se que a sinapse \u00e9 uma conex\u00e3o simples que pode impor ao\nneur\u00f4nio receptivo excita\u00e7\u00e3o ou inibi\u00e7\u00e3o, mas nunca as duas. Os sinais p\u00f3s-sin\u00e1pticos provindos\ndos diversos dendritos s\u00e3o combinados no corpo celular, o qual, dependendo do percentual de\nexcita\u00e7\u00e3o, emite um impulso atrav\u00e9s do ax\u00f4nio para os neur\u00f4nios seguintes. Este percentual de\nexcita\u00e7\u00e3o (ou potencial de a\u00e7\u00e3o) \u00e9 conhecido como limiar de disparo (Kov\u00e1cs, 1997), e separa o\nestado do neur\u00f4nio em repouso ou em disparo. Ap\u00f3s a gera\u00e7\u00e3o de um impulso (disparo), o neur\u00f4nio\nentra em um per\u00edodo de refra\u00e7\u00e3o absoluta (na qual fica incapaz de emitir outro sinal), seguido de um\nper\u00edodo de refra\u00e7\u00e3o relativa, correspondente a uma eleva\u00e7\u00e3o no limiar de disparo (Kov\u00e1cs, 1997).\n\nModelos de um Neur\u00f4nio\n\nO primeiro modelo de um neur\u00f4nio biol\u00f3gico foi proposto pelo fisiologista Warren McCulloch jun-\ntamente com Walter Pitts (McCulloch e Pitts, 1943), em 1943, e tratava-se de um modelo simples\nque descrevia um funcionamento booleano da c\u00e9lula (Kov\u00e1cs, 1997). Este modelo consistia em m\nterminais de entrada (dendritos) que recebem m valores x1, x2, ..., xm (que representam as ati-\nva\u00e7\u00f5es dos neur\u00f4nios anteriores) e apenas um terminal de sa\u00edda y (representando o ax\u00f4nio). As\nsinapses foram representadas por pesos w1, w2, ..., wm, acoplados nos terminais de entrada. Tais\npesos podem possuir valores positivos ou negativos. Assim, o efeito da sinapse de um neur\u00f4nio i\npr\u00e9-sin\u00e1ptico que emite um sinal xi \u00e9 wixi. O percentual de excita\u00e7\u00e3o de tal modelo \u00e9 feito somando\ntodas as sinapses correspondentes ? wixi. O disparo ou n\u00e3o do neur\u00f4nio ent\u00e3o se d\u00e1 comparando o\nresultado dessa soma com o limiar de ativa\u00e7\u00e3o, a partir de uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Por se tratar de\num modelo bin\u00e1rio, a sa\u00edda do neur\u00f4nio de McCulloch e Pitts pode ser pulso (1) ou n\u00e3o-pulso (0).\n\nO diagrama abaixo ilustra o modelo proposto de McCulloch e Pitts:\n\nFigura 2.1: Modelo simplificado do neur\u00f4nio de McCulloch e Pitts.\n\n\n\n8 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nA fun\u00e7\u00e3o de ativa\u00e7\u00e3o, tamb\u00e9m conhecida como fun\u00e7\u00e3o restritiva, tem por objetivo limitar a\namplitude do sinal de sa\u00edda do neur\u00f4nio. Para o modelo de McCulloch e Pitts, a fun\u00e7\u00e3o de ativa\u00e7\u00e3o\n\u00e9 a fun\u00e7\u00e3o de limiar, representada matematicamente como:\n\n?(v) =\n\n???\n??\n\n1 se v ? 0\n\n0 se v &lt;0\n. (2.1)\n\nO modelo de McCulloch e Pitts pode ser esquematizado em uma forma mais geral de forma\na possibilitar diversas aplica\u00e7\u00f5es: pode-se inserir um polarizador ou bias na entrada do neur\u00f4nio\ne alterar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o ?(\u00b7), utilizando para isso outras fun\u00e7\u00f5es. O bias tem o efeito de\naumentar ou diminuir a entrada l\u00edquida da fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Quatro elementos b\u00e1sicos podem ser\nidentificados nesse modelo mais geral, a saber:\n\n1. Sinapses.\n\n2. Somador.\n\n3. Fun\u00e7\u00e3o de Ativa\u00e7\u00e3o.\n\n4. Bias (polarizador).\n\nO funcionamento do neur\u00f4nio da Figura 2.1, acrescido do bias, pode ser descrito de forma mate-\nm\u00e1tica como:\n\nuk =\nm\n\n?\nj=1\n\nwk jx j (2.2)\n\ne\n\nyk = ?(uk + bk) (2.3)\n\nem que x1, x2, ..., xm s\u00e3o sinais de entrada; wk1, wk2, ..., wkm s\u00e3o os pesos sin\u00e1pticos do neur\u00f4nio\nk; uk \u00e9 a sa\u00edda do combinador linear; bk \u00e9 o bias ou polarizador; ?(\u00b7) \u00e9 a fun\u00e7\u00e3o de ativa\u00e7\u00e3o; yk\n\u00e9 o sinal de sa\u00edda do neur\u00f4nio. O termo uk + bk \u00e9 chamado tamb\u00e9m de campo local induzido ou\npotencial de ativa\u00e7\u00e3o, definido como:\n\nvk = uk + bk. (2.4)\n\nA fun\u00e7\u00e3o de ativa\u00e7\u00e3o estipula a sa\u00edda do neur\u00f4nio em fun\u00e7\u00e3o do campo local induzido, e pode\napresentar diversas topologias de acordo com as fun\u00e7\u00f5es que a definem. Em geral, as fun\u00e7\u00f5es de\nativa\u00e7\u00e3o restrigem a amplitude de sa\u00edda do neur\u00f4nio em um intervalo unit\u00e1rio fechado [0, 1] ou\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 9\n\n[?1, 1]. Abaixo seguem algumas fun\u00e7\u00f5es de ativa\u00e7\u00e3o apresentadas em Haykin (2001) e em Braga\net al. (2007):\n\n\u2022 Fun\u00e7\u00e3o de Limiar. Esta fun\u00e7\u00e3o apresenta a seguinte forma matem\u00e1tica:\n\n?(v) =\n\n???\n??\n\n1 se v ? 0\n\n0 se v &lt;0\n. (2.5)\n\nA Figura 2.2 ilustra o gr\u00e1fico dessa fun\u00e7\u00e3o:\n\nFigura 2.2: Fun\u00e7\u00e3o de limiar.\n\n\u2022 Fun\u00e7\u00e3o Linear por Partes. Tal fun\u00e7\u00e3o, cujo gr\u00e1fico \u00e9 apresentado na Figura 2.3, pode ser\ndescrita como:\n\n?(v) =\n\n??????\n?????\n\n1, v ?+1/2\n\nv + 1/2, +1/2 > v >?1/2\n\n0, v ??1/2\n\n. (2.6)\n\n\n\n10 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nFigura 2.3: Fun\u00e7\u00e3o linear por partes.\n\n\u2022 Fun\u00e7\u00e3o Sigm\u00f3ide. Esta fun\u00e7\u00e3o apresenta um gr\u00e1fico cont\u00ednuo e suave, sendo tamb\u00e9m dife-\nrenci\u00e1vel. A fun\u00e7\u00e3o sigmoidal pode ser representada pela fun\u00e7\u00e3o log\u00edstica ou pela fun\u00e7\u00e3o\ntangente hiperb\u00f3lica. Fundamentalmente, a diferen\u00e7a entre elas est\u00e1 nos intervalos nos quais\ns\u00e3o delimitadas. As Figuras 2.4 e 2.5 ilustram os gr\u00e1ficos dessas fun\u00e7\u00f5es. As express\u00f5es da\nfun\u00e7\u00e3o log\u00edstica e da fun\u00e7\u00e3o tangente hiperb\u00f3lica s\u00e3o representadas por 2.7 e 2.8, respectiva-\nmente:\n\n?l(v) = a\u00b7\n1\n\n1 + exp(?bv)\n(2.7)\n\n?th(v) = a\u00b7\nexp(bv)?exp(?bv)\nexp(bv)+ exp(?bv)\n\n. (2.8)\n\nFigura 2.4: Fun\u00e7\u00e3o log\u00edstica.\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 11\n\nFigura 2.5: Fun\u00e7\u00e3o tangente hiperb\u00f3lica.\n\n\u2022 Fun\u00e7\u00e3o de Base Radial. Um exemplo deste tipo de fun\u00e7\u00e3o \u00e9 a fun\u00e7\u00e3o Gaussiana. A Figura\n2.6 ilustra o gr\u00e1fico gerado por 2.9:\n\n?(v) = exp(\n?(v?u)2\n\nr2\n) (2.9)\n\nFigura 2.6: Exemplo de fun\u00e7\u00e3o de base radial: fun\u00e7\u00e3o Gaussiana.\n\nem que u \u00e9 o ponto no qual a fun\u00e7\u00e3o est\u00e1 centrada e r \u00e9 dispers\u00e3o da fun\u00e7\u00e3o ao longo do eixo v. Na\nFigura 2.6, u = 0 e r = 1.\n\n\n\n12 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\n2.1.3 Arquiteturas de Redes\n\nA arquitetura de uma rede neural \u00e9 a maneira como seus neur\u00f4nios est\u00e3o organizados. Essa or-\nganiza\u00e7\u00e3o entre neur\u00f4nios est\u00e1 intimamente ligada com o algoritmo de aprendizagem usado para\ntreinamento da rede. De forma geral, Haykin (2001) identifica tr\u00eas classes de arquiteturas diferen-\ntes:\n\nRedes Alimentadas Adiante com Camada \u00danica\n\nUma forma de organiza\u00e7\u00e3o de redes \u00e9 em camadas, ou seja, o neur\u00f4nios se estruturam na forma\nde camadas. Na forma mais simples de uma rede em camadas, tem-se uma camada de entrada de\nn\u00f3s de fonte que se projeta sobre uma camada de sa\u00edda de neur\u00f4nios, mas n\u00e3o vice-versa. Este tipo\nde arquitetura \u00e9 estritamente do tipo alimentada adiante ou ac\u00edclica. O fato de ser referida como\nrede de camada \u00fanica \u00e9 que somente a camada de sa\u00edda realiza \"c\u00e1lculo\" sobre os dados, ou seja, a\ncamada de entrada somente \"reproduz\" os sinais de entrada para cada neur\u00f4nio da camada de sa\u00edda.\nA Figura 2.7 ilustra esse tipo de arquitetura:\n\nFigura 2.7: Rede alimentada adiante com uma camada de entrada e uma de sa\u00edda.\n\nRedes Alimentadas Diretamente com M\u00faltiplas Camadas\n\nEste tipo de arquitetura tem a caracter\u00edstica de apresentar uma ou mais camadas intermedi\u00e1rias\nentre as camadas de entrada e de sa\u00edda. Essas camadas intermed\u00edarias s\u00e3o referidas como ocultas,\nnas quais os neur\u00f4nios nelas presentes s\u00e3o referenciados como neur\u00f4nios ocultos. A fun\u00e7\u00e3o dos\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 13\n\nneur\u00f4nios ocultos \u00e9 intervir entre a entrada externa e a sa\u00edda da rede de uma maneira \u00fatil. Neste\narranjo de estrutura, os n\u00f3s da camada de entrada distribuem o vetor de entradas (sinais de entrada\nda rede neural) para os neur\u00f4nios da segunda camada. Os sinais de sa\u00edda da segunda camada s\u00e3o\npassados para a terceira camada como sinais de entrada, e assim se repetir\u00e1 at\u00e9 a camada de sa\u00edda\nda rede neural, sendo que o conjunto de sinais dos neur\u00f4nios desta \u00faltima camada constituir\u00e1 a\nresposta global da rede para o padr\u00e3o de ativa\u00e7\u00e3o fornecido pela camada de entrada. A Figura 2.8\nilustra uma rede com uma camada oculta:\n\nFigura 2.8: Rede alimentada adiante com uma camada oculta de neur\u00f4nios.\n\nRedes Recorrentes\n\nUma rede recorrente se distingue de uma rede neural alimentada adiante por ter pelo menos um\nla\u00e7o de realimenta\u00e7\u00e3o. Uma rede desse tipo pode apresentar neur\u00f4nios ocultos ou n\u00e3o. A presen\u00e7a\nde la\u00e7os de realimenta\u00e7\u00e3o tem um impacto profundo na capacidade de aprendizagem da rede e no\nseu desempenho. Somado a isso, esses la\u00e7os de realimenta\u00e7\u00e3o envolvem o uso de ramos particu-\nlares compostos de elementos de atraso unit\u00e1rio, os quais realimentam alguns neur\u00f4nios da rede\ncom sa\u00eddas processadas no passo de tempo anterior. Essas realimenta\u00e7\u00f5es conferem \u00e0 rede um\ncomportamento din\u00e2mico n\u00e3o-linear. As figuras 2.9 e 2.10 mostram duas redes distintas, mas que\napresentam pelo menos um la\u00e7o de realimenta\u00e7\u00e3o:\n\n\n\n14 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nFigura 2.9: Rede recorrente autoalimentada.\n\nFigura 2.10: Rede recorrente com entrada e sa\u00edda.\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 15\n\n2.1.4 Processos de Aprendizagem\n\nA habilidade de aprender a partir de seu ambiente \u00e9 uma propriedade fundamental para uma rede\nneural. O processo de aprendizagem destaca, al\u00e9m disso, um papel importante para melhoria de\ndesempenho da rede segundo algum crit\u00e9rio preestabelecido. Em geral, uma rede neural aprende\nacerca de seu ambiente a partir de um processo de ajustes sobre seus pesos sin\u00e1pticos e biases.\nIdealmente, a rede se torna mais instru\u00edda sobre o seu ambiente ap\u00f3s cada itera\u00e7\u00e3o do processo de\naprendizagem (Haykin, 2001).\n\nDentre diversas defini\u00e7\u00f5es ou atividades relacionadas ao conceito de aprendizagem como todo,\n\u00e9 apresentada uma defini\u00e7\u00e3o contextualizada no \u00e2mbito das redes neurais, conforme descrito em\nHaykin (2001):\n\nAprendizagem \u00e9 um processo pelo qual os par\u00e2metros livres de uma rede neural s\u00e3o\n\nadaptados atrav\u00e9s de um processo de estimula\u00e7\u00e3o pelo ambiente no qual a rede est\u00e1\n\ninserida. O tipo de aprendizagem \u00e9 determinado pela maneira pela qual a modifica\u00e7\u00e3o\n\ndos par\u00e2metros ocorre.\n\nSegundo Braga et al. (2007), os processos de aprendizagem podem ser agrupados em dois pa-\nradigmas principais: aprendizado supervisionado e aprendizado n\u00e3o-supervisionado. Aprendizado\nsupervisionado implica a exist\u00eancia de um supervisor ou professor, o qual \u00e9 respons\u00e1vel por esti-\nmular as entradas da rede por meio de padr\u00f5es de entrada e observar a sa\u00edda da rede calculada pela\nmesma, comparando-a com a sa\u00edda desejada. Esse tipo de aprendizado \u00e9 aplicado em problemas em\nque se deseja mapear padr\u00f5es de entrada e sa\u00edda. No aprendizado n\u00e3o-supervisionado n\u00e3o existe\na figura do supervisor ou professor e, em geral, apenas os padr\u00f5es de entrada est\u00e3o dispon\u00edveis.\nO processo de aprendizagem se d\u00e1 pela exist\u00eancia de regularidades e redund\u00e2ncias nos padr\u00f5es\nde entrada apresentados \u00e0 rede. Esse processo se aplica em problemas que visam \u00e0 descoberta de\ncaracter\u00edsticas estat\u00edsticas nos dados.\n\nPode-se listar alguns processos de aprendizagem, elencando-os dentro desses dois paradigmas,\nde acordo com Braga et al. (2007):\n\nAprendizado supervisionado:\n\n\u2022 Aprendizagem por corre\u00e7\u00e3o de erro.\n\n\u2022 Aprendizagem por refor\u00e7o.\n\n\n\n16 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nAprendizado n\u00e3o supervisionado:\n\n\u2022 Aprendizagem Hebbiana.\n\n\u2022 Aprendizagem competitiva.\n\nMaiores detalhes sobre cada processo, ou mesmo sobre outros, podem ser encontrados em Fausett\n(1994), Hassoun (1995), Haykin (2001), Braga et al. (2007) e Galushkin (2007).\n\nOs processos de aprendizagem podem, em geral, serem compostos por diferentes procedimen-\ntos ou algoritmos de aprendizagem. Algoritmo de aprendizagem pode ser definido como con-\njunto de regras bem-definidas utilizadas para resolu\u00e7\u00e3o de um problema de aprendizagem (Haykin,\n2001). Diversos algoritmos existem e, em geral, diferem um do outro pela forma como ajustam os\npesos sin\u00e1ptios dos neur\u00f4nios. No presente trabalho, foi utilizado um algoritmo conhecido como\nretropropaga\u00e7\u00e3o ou backpropagation, o qual faz parte do processo de aprendizado supervisionado,\nmais especificamente por corre\u00e7\u00e3o de erro. Ser\u00e3o descritos, na sequ\u00eancia, o processo de aprendi-\nzagem por corre\u00e7\u00e3o de erro e o algoritmo de retropropaga\u00e7\u00e3o ou backpropagation.\n\nAprendizagem por corre\u00e7\u00e3o de erro\n\nEste processo baseia-se em alterar os valores dos pesos sin\u00e1pticos dos neur\u00f4nios em fun\u00e7\u00e3o da\ndiferen\u00e7a entre os sinais de sa\u00edda da rede ~y(n, k) = (y1(n, k), ..., yi(n, k), ..., ym(n, k)) e os va-\nlores esperados ~d (k) = (d1 (k), ..., di (k), ..., dm (k)), em que o \u00edndice n indica a n-\u00e9sima ite-\nra\u00e7\u00e3o no processo de aprendizagem, k \u00e9 a k-\u00e9sima amostra do conjunto de treinamento e m \u00e9\no n\u00famero de neur\u00f4nios na camada de sa\u00edda. Essa diferen\u00e7a recebe o nome de sinal de erro\n~e(n, k) = (e1(n, k), ..., ei(n, k), ..., em(n, k)), definido como:\n\n~e(n, k) = ~d(k)?~y(n, k). (2.10)\n\nDe forma geral, o sinal de erro ~e(n, k) aciona um mecanismo de controle, cujo prop\u00f3sito \u00e9\naplicar uma sequ\u00eancia de ajustes corretivos aos pesos sin\u00e1pticos de cada neur\u00f4nio da camada de\nsa\u00edda e tamb\u00e9m das camadas ocultas. Os ajustes corretivos s\u00e3o projetados para aproximar passo a\npasso o sinal de sa\u00edda~y(n, k) da resposta desejada ~d(k). Isto \u00e9 realizado atrav\u00e9s da minimiza\u00e7\u00e3o de\numa fun\u00e7\u00e3o de custo ? (n, k), definida como:\n\n? (n, k) =\n1\n2\n(~e(n, k)\u00b7~e(n, k)) (2.11)\n\nsendo ? (n, k) o valor instant\u00e2neo da energia do erro para a n-\u00e9sima itera\u00e7\u00e3o ou \u00e9poca, para a k-\n\u00e9sima amostra do conjunto de treinamento. Destaca-se que para contabilizar uma itera\u00e7\u00e3o ou \u00e9poca\n\u00e9 necess\u00e1rio que todas as amostras (ou casos) do conjunto de treinamento sejam apresentadas \u00e0\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 17\n\nrede para o processo de treinamento. Assim, na n-\u00e9sima itera\u00e7\u00e3o ou \u00e9poca, todas as amostras do\nconjunto de treinamento foram apresentadas pelo menos n?1 vezes \u00e0 rede; ao final da n-\u00e9sima\nitera\u00e7\u00e3o, todas as amostras foram apresentadas n vezes.\n\nA energia m\u00e9dia do erro quadrado \u00e9 obtida somando-se os ? (n, k) de todo o conjunto de trei-\nnamento. Considerando T como sendo a dimens\u00e3o da amostra de treinamento, tem-se a energia\nm\u00e9dia do erro quadrado para n-\u00e9sima itera\u00e7\u00e3o ou \u00e9poca:\n\n?med(n) =\n1\nT\n\nT\n\n?\nk=1\n\n? (n, k). (2.12)\n\nOs ajustes passo a passo dos pesos sin\u00e1pticos de cada neur\u00f4nio continuam at\u00e9 o sistema atin-\ngir um estado est\u00e1vel, isto \u00e9, os pesos sin\u00e1pticos est\u00e3o de tal forma estabilizados que o valor da\nenergia do erro situa-se num ponto de m\u00ednimo (local ou global) na superf\u00edcie de erro. Idealmente,\no processo de treinamento deveria se encerrar quando a energia do erro atingisse seu m\u00ednimo glo-\nbal. Por\u00e9m, isso nem sempre \u00e9 poss\u00edvel devido \u00e0s n\u00e3o-linearidades presentes na rede. Apesar disso,\nexistem t\u00e9cnicas de treinamento que buscam levar a rede a se aproximar do ponto de m\u00ednimo global.\n\nA minimiza\u00e7\u00e3o da fun\u00e7\u00e3o de custo ? (n, k) resulta na regra de aprendizagem normalmente refe-\nrida como regra delta ou regra de Widrow-Hoff. A minimiza\u00e7\u00e3o da fun\u00e7\u00e3o de custo \u00e9 dependende\ndo processo ou algoritmo de aprendizado utilizado. Na sequ\u00eancia, ser\u00e1 descrito maiores detalhes\ndessa minimiza\u00e7\u00e3o, de acordo com o algoritmo de retropropaga\u00e7\u00e3o ou backpropagation.\n\nAlgoritmo de retropropaga\u00e7\u00e3o (backpropagation)\n\nA energia instant\u00e2nea do erro ? (n, k) e, consequentemente, a energia m\u00e9dia do erro ?med(n), s\u00e3o\nfun\u00e7\u00f5es de todos os pesos sin\u00e1pticos e biases dos neur\u00f4nios da rede. Para um dado conjunto de\ntreinamento, ?med(n) representa a fun\u00e7\u00e3o de custo como uma medida do desempenho de aprendi-\nzagem. O objetivo do processo de aprendizagem \u00e9 ajustar os par\u00e2metros livres (pesos sin\u00e1pticos\ne biases) para minimizar ?med(n). Os ajustes dos pesos s\u00e3o realizados de acordo com os respec-\ntivos erros calculados para cada padr\u00e3o apresentado \u00e0 rede. A m\u00e9dia aritm\u00e9tica destas altera\u00e7\u00f5es\nindividuais de peso sobre o conjunto de treinamento \u00e9, portanto, uma estimativa da altera\u00e7\u00e3o real\nque resultaria da modifica\u00e7\u00e3o dos peso baseada na minimiza\u00e7\u00e3o da fun\u00e7\u00e3o de custo ?med sobre o\nconjunto de treinamento inteiro. Ser\u00e1 descrito a seguir a express\u00e3o de corre\u00e7\u00e3o dos pesos sin\u00e1pticos\ne a sequ\u00eancia de passos para realiz\u00e1-la. Destaca-se que a express\u00e3o foi deduzida para uma amostra\ngeral do conjunto de treinamento. Portanto, o \u00edndice k utilizado para referenciar a k-\u00e9sima amostra\nn\u00e3o ser\u00e1 utilizado para esse fim.\n\nA corre\u00e7\u00e3o 4w ji(n) aplicada ao peso sin\u00e1ptico w ji(n) \u00e9 definida pela regra delta:\n\n\n\n18 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\n4w ji(n) =??\n? ? (n)\n\n? w ji(n)\n(2.13)\n\nem que ? \u00e9 o par\u00e2metro da taxa de aprendizagem e j representa o j-\u00e9simo neur\u00f4nio da camada\nde sa\u00edda cujo i-\u00e9simo peso sin\u00e1ptico est\u00e1 sendo corrigido. O sinal negativo indica a descida do\ngradiente no espa\u00e7o de pesos. O ajuste do peso w ji \u00e9 realizado como:\n\nw ji(n + 1) = w ji(n)+4w ji(n). (2.14)\n\nA Equa\u00e7\u00e3o 2.13 pode ser escrita como:\n\n4w ji(n) = ? ? j(n)yi(n) (2.15)\n\nem que ? j(n) \u00e9 chamado gradiente local e yi(n) \u00e9 o sinal de sa\u00edda do i-\u00e9simo neur\u00f4nio pr\u00e9-sin\u00e1ptico\nao neur\u00f4nio j. O gradiente local ? j(n) \u00e9 definido como:\n\n? j(n) = e j(n)?\n?\nj(v j(n)). (2.16)\n\nem que e j(n) \u00e9 o sinal de erro do neur\u00f4nio j e ??j(v j(n)) \u00e9 a derivada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o em\nrela\u00e7\u00e3o \u00e0 v j(n). Maiores detalhes sobre a defini\u00e7\u00e3o do gradiente local s\u00e3o encontrados em Haykin\n(2001).\n\nO gradiente local \u00e9 um estimador que aponta para uma poss\u00edvel dire\u00e7\u00e3o de modifica\u00e7\u00f5es ne-\ncess\u00e1rias a serem efetuados sobre os pesos sin\u00e1pticos. De acordo com a Equa\u00e7\u00e3o 2.16, o gradiente\nlocal ? j(n) para o neur\u00f4nio de sa\u00edda j \u00e9 igual ao produto do sinal de erro e j(n) com derivada\n?\n?\nj(v j(n)) da fun\u00e7\u00e3o de ativa\u00e7\u00e3o associada a esse neur\u00f4nio.\n\nNota-se pelas Equa\u00e7\u00f5es 2.15 e 2.16 que um fator chave envolvido no c\u00e1lculo do ajuste de peso\n?w ji(n) \u00e9 o sinal de erro e j(n) na sa\u00edda do neur\u00f4nio j. Neste contexto, identificam-se dois casos\ndistintos dependendo de onde o neur\u00f4nio j est\u00e1 localizado. No caso 1, o neur\u00f4nio j \u00e9 um n\u00f3 de\nsa\u00edda. No caso 2, o neur\u00f4nio j \u00e9 um n\u00f3 oculto. Para cada caso, o c\u00e1lculo de ?w ji(n) assume formas\ndiferentes, as quais s\u00e3o descritas abaixo:\n\n1. Caso 1. Quando o neur\u00f4nio j est\u00e1 localizado na camada de sa\u00edda da rede, ele \u00e9 suprido com\numa resposta desejada particular. Podemos utilizar a Equa\u00e7\u00e3o 2.10 para calcular o sinal de\nerro e j(n) associado a esse neur\u00f4nio. Tendo-se determinado e j(n), calcula-se diretamente o\ngradiente local ? j(n), usando a Equa\u00e7\u00e3o 2.16.\n\n2. Caso 2. Quando o neur\u00f4nio j est\u00e1 localizado em uma camada oculta da rede, n\u00e3o existe\numa resposta desejada para aquele neur\u00f4nio. Sendo assim, para c\u00e1lculo do gradiente local,\n\u00e9 necess\u00e1rio retropropagar os sinais de erros atrav\u00e9s da rede. Haykin (2001) desenvolve\n\n\n\n2.1. REDES NEURAIS ARTIFICIAIS 19\n\no c\u00e1lculo do gradiente local ? j(n) partindo da fun\u00e7\u00e3o de custo definida na Equa\u00e7\u00e3o 2.11,\nchegando em:\n\n? j(n) = ?\n?\nj(v j(n))\n\nm\n\n?\nk=1\n\n?k(n)wk j(n) (2.17)\n\nem que k \u00e9 o k-\u00e9simo neur\u00f4nio da camada subsequente \u00e0 camada do neur\u00f4nio j (ou seja, k s\u00e3o os\nneur\u00f4nios que recebem o sinal de sa\u00edda do neur\u00f4nio j) e m \u00e9 o n\u00famero total de neur\u00f4nios presentes\nnessa camada (os quais est\u00e3o ligados ao neur\u00f4nio j por meio dos pesos wk j(n)).\n\nO algoritmo de retropropaga\u00e7\u00e3o \u00e9 definido ent\u00e3o como a sequ\u00eancia de dois passos principais:\npropaga\u00e7\u00e3o e retropropaga\u00e7\u00e3o. S\u00e3o descritos a seguir:\n\nPropaga\u00e7\u00e3o\n\nNa propaga\u00e7\u00e3o, os pesos sin\u00e1pticos se mant\u00eam inalterados em toda a rede e os sinais funcionais s\u00e3o\ncalculados individualmente, neur\u00f4nio a neur\u00f4nio. Ou seja, a fase de propaga\u00e7\u00e3o come\u00e7a desde a\ncamada de entrada, passando pelas camadas ocultas (quando essas existem), e termina na camada\nde sa\u00edda, onde s\u00e3o calculados o sinais de erro.\n\nRetropropaga\u00e7\u00e3o\n\nO passo de retropropaga\u00e7\u00e3o come\u00e7a na camada de sa\u00edda, a partir da qual os ajustes dos pesos\ns\u00e3o enviados para as camadas ocultas atrav\u00e9s dos c\u00e1lculos dos gradientes locais ? j(n) para cada\nneur\u00f4nio oculto. Todos os pesos sin\u00e1pticos dos neur\u00f4nios s\u00e3o ent\u00e3o ajustados de acordo com as\nexpress\u00f5es definidas pela regra delta.\n\nAp\u00f3s t\u00e9rmino do passo de retropropaga\u00e7\u00e3o, inicializa-se o passo de propaga\u00e7\u00e3o, recome\u00e7ando\nassim o algoritmo. Esse processo recursivo permite que os pesos sin\u00e1pticos sofram modifica\u00e7\u00f5es\nde acordo com a regra delta. Destaca-se que o passo de propaga\u00e7\u00e3o e retropropaga\u00e7\u00e3o podem ser\nrealizados amostra por amostra do conjunto de treinamento, e as express\u00f5es acima foram deduzidas\na partir disso. Essa maneira de realizar os ajustes dos pesos sin\u00e1pticos \u00e9 conhecida como modo\nsequencial de aprendizagem. Uma outra maneira de aplicar o algoritmo \u00e9 realizar o passo de\npropaga\u00e7\u00e3o sobre todas as amostras do conjunto de treinamento e somente depois realizar o passo\nde retropropaga\u00e7\u00e3o. Essa forma \u00e9 conhecida como modo por lote de treinamento e apresenta poucas\ndiferen\u00e7as no c\u00e1lculo da corre\u00e7\u00e3o 4w ji(n) em rela\u00e7\u00e3o ao modo sequencial. Maiores detalhes sobre\no modo por lote s\u00e3o encontrados em Haykin (2001).\n\nUm termo que pode ser acrescentado na Equa\u00e7\u00e3o 2.14 com intuito de aumentar a taxa de apren-\ndizagem evitando, por\u00e9m, instabilidades na converg\u00eancia do m\u00e9todo, \u00e9 um termo de momento,\napresentado na sequ\u00eancia:\n\n\n\n20 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nw ji(n + 1) = w ji(n)+4w ji(n)+ ? w ji(n?1) (2.18)\n\nem que ? \u00e9 usualmente um n\u00famero positivo chamado de constante de momento. A Equa\u00e7\u00e3o 2.18\n\u00e9 conhecida como regra delta generalizada.\n\n2.2 Acoplamento Po\u00e7o-Reservat\u00f3rio\n\nO problema de acoplamento po\u00e7o-reservat\u00f3rio \u00e9 um assunto estudado durante d\u00e9cadas por diver-\nsos pesquisadores e profissionais da engenharia de petr\u00f3leo, principalmente quando da simula\u00e7\u00e3o\nnum\u00e9rica do escoamento no meio poroso com o escoamento no po\u00e7o horizontal. A diferen\u00e7a na\nnatureza dos fluxos no reservat\u00f3rio e no po\u00e7o requer o desenvolvimento de diversos modelos mate-\nm\u00e1ticos que, a partir de algumas simplifica\u00e7\u00f5es, apresentam-se na forma de uma equa\u00e7\u00e3o anal\u00edtica\nou semi-anal\u00edtica para tal problema. Por\u00e9m, tais modelos anal\u00edticos e semi-anal\u00edticos podem n\u00e3o\nser interessantes quando deseja-se ter mais detalhes do fluxo no interior do po\u00e7o ou mesmo do\nreservat\u00f3rio. Desta forma, pesquisadores e profissionais recorrem aos simuladores num\u00e9ricos de\nreservat\u00f3rios, geralmente conhecidos como simuladores num\u00e9ricos de fluxo (Rosa et al., 2006), os\nquais apresentam maior complexidade em termos de modelagem matem\u00e1tica do problema f\u00edsico.\nAssim, v\u00e1rios s\u00e3o os modelos de acoplamento po\u00e7o-reservat\u00f3rio existentes, cada um concentrando-\nse em determinadas caracter\u00edsticas e complexidades de acordo com as finalidades para as quais\nforam desenvolvidos. Pode-se listar os trabalhos de Gomes (1990), Lemos (1993), Dickstein et al.\n(1997), Vicente (2000), Arrieta (2004), Shirdel e Sepehrnoori (2009) e Devloo et al. (2009) como\nexemplos de modelos num\u00e9ricos e os trabalhos de Joshi (1991) e, mais recentemente, de Escobar e\nMontealegre (2008) como exemplos de modelos anal\u00edticos e de correla\u00e7\u00f5es.\n\nOs avan\u00e7os no desenvolvimento e aperfei\u00e7oamento na tecnologia de perfura\u00e7\u00e3o de po\u00e7os per-\nmitiram que, nos \u00faltimos anos, uma grande quantidade de po\u00e7os horizontais fosse perfurada no\nmundo (Arturo et al., 2007; Vicente, 2000). De acordo com Fernandes et al. (2006), a Petrobras,\ndesde a d\u00e9cada de 90, vem desenvolvendo v\u00e1rios campos com po\u00e7os horizontais para drenar desde\ncarbonatos de baixa permeabilidade at\u00e9 arenitos altamente fri\u00e1veis. Al\u00e9m disto, v\u00e1rios po\u00e7os hori-\nzontais t\u00eam sido perfurados e completados para inje\u00e7\u00e3o de \u00e1gua, predominantemente em arenitos\nda Bacia de Campos (Fernandes et al., 2006). O prop\u00f3sito dos po\u00e7os horizontais \u00e9 justamente\naumentar o contato com o reservat\u00f3rio e, portanto, aumentar a produtividade do po\u00e7o. As vaz\u00f5es\npara po\u00e7os horizontais podem ser de 2 a 5 vezes maiores que para po\u00e7os verticais n\u00e3o estimulados\n(Arturo et al., 2007).\n\nDevido ao crescente interesse pr\u00e1tico, a tecnologia de po\u00e7os horizontais tem recebido not\u00e1vel\naten\u00e7\u00e3o e consider\u00e1vel quantidade de pesquisa. Segundo Vicente (2000), embora diversas fer-\n\n\n\n2.2. ACOPLAMENTO PO\u00c7O-RESERVAT\u00d3RIO 21\n\nramentas num\u00e9ricas e anal\u00edticas tenham sido desenvolvidas para investigar o comportamento do\nfluxo e predizer a performance de po\u00e7os horizontais, as dificuldades inerentes de realizar essas\ntarefas desafiadoras s\u00e3o enormes. Assim, optou-se em utilizar no presente trabalho um modelo\nexistente, desenvolvido por Devloo et al. (2009), e o qual apresenta not\u00e1vel grau de qualidade no\nquesito comportamento de fluxo no interior do po\u00e7o. Descrever-se-\u00e3o as formula\u00e7\u00f5es e os m\u00e9todos\nnum\u00e9ricos utilizados por Devloo et al. (2009), detalhando posteriormente os fluxos no reservat\u00f3rio\ne no po\u00e7o e a maneira como foram acoplados. Outros detalhes sobre simula\u00e7\u00e3o num\u00e9rica em en-\ngenharia de petr\u00f3leo podem ser encontrados em Crichlow (1977) e em Aziz e Settari (1983). Uma\nvis\u00e3o geral sobre engenharia de petr\u00f3leo pode ser encontrada no trabalho de Thomas (2001).\n\n2.2.1 Modelo Tridimensional do Acoplamento Po\u00e7o-Reservat\u00f3rio\n\nO modelo utiliza o m\u00e9todo de elementos finitos juntamente com a aplica\u00e7\u00e3o de algumas tecnologias\navan\u00e7adas, tais como elementos geom\u00e9tricos de mapeamentos curvos, refinamento direcional de\nmalha, adaptatividade hp, redu\u00e7\u00e3o dimensional e combina\u00e7\u00e3o dos m\u00e9todos de elementos finitos\ntradicional e de Galerkin descont\u00ednuo.\n\nO modelo assume escoamento monof\u00e1sico isot\u00e9rmico e implementa as equa\u00e7\u00f5es de escoamento\ntanto no reservat\u00f3rio quanto no po\u00e7o, as quais est\u00e3o descritas em 2.2.2 e em 2.2.3. A geometria do\nreservat\u00f3rio contempla \u00e1reas de drenagem como sendo retangulares ou el\u00edpticas. As condi\u00e7oes de\ncontorno aplicadas s\u00e3o de fluxo nulo nas camadas confinantes (imperme\u00e1veis), press\u00e3o declarada\nna fronteira da \u00e1rea de drenagem e no calcanhar (heel) do po\u00e7o horizontal. As permeabilidades\nhorizontal e vertical do reservat\u00f3rio s\u00e3o consideradas de forma a contemplar sua anisotropia.\n\nA malha desenvolvida baseou-se no mapeamento transfinito capaz de mapear exatamente o\ndom\u00ednio do problema (Lucci, 2009). Desta forma, foram geradas malhas que representavam exata-\nmente as geometrias dos po\u00e7os e \u00e1reas de drenagem. O uso de refinamento direcional e seletividade\nde ordem de aproxima\u00e7\u00e3o em cima dessas malhas permitiu melhorar a qualidade da solu\u00e7\u00e3o sem\ncomprometer o desempenho da simula\u00e7\u00e3o. As Figuras 2.11 e 2.12 esbo\u00e7am o uso de elementos\ncurvos para descrever a geometria e o detalhe dos refinamentos direcionais, respectivamente:\n\n\n\n22 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\nFigura 2.11: Po\u00e7o horizontal e reservat\u00f3rio el\u00edpitco: elementos tridimensionais curvos.\n\nFigura 2.12: Wireframe da malha do po\u00e7o horizontal e reservat\u00f3rio el\u00edpitco.\n\nA formula\u00e7\u00e3o do escoamento no reservat\u00f3rio \u00e9 linear. J\u00e1 a formula\u00e7\u00e3o do escoamento no po\u00e7o\nhorizontal \u00e9 n\u00e3o-linear. A resolu\u00e7\u00e3o do problema acoplado po\u00e7o-reservat\u00f3rio requer, portanto, a so-\nlu\u00e7\u00e3o de um sistema de equa\u00e7\u00f5es n\u00e3o-lineares. A sua resolu\u00e7\u00e3o foi realizada atrav\u00e9s do m\u00e9todo de\n\n\n\n2.2. ACOPLAMENTO PO\u00c7O-RESERVAT\u00d3RIO 23\n\nNewton-Raphson com busca unidimensional (line search) do tipo segmento \u00e1ureo. O modelo assu-\nmiu o escoamento no po\u00e7o horizontal como unidimensional. Assim, os elementos tridimensionais\nque descrevem o po\u00e7o horizontal foram substitu\u00eddos por elementos unidimensionais atrav\u00e9s da sis-\ntem\u00e1tica da redu\u00e7\u00e3o dimensional, o que permitiu maior rapidez nas itera\u00e7\u00f5es de Newton-Raphson.\n\nOs dados de entrada (inputs) para simula\u00e7\u00e3o no modelo tridimensional para po\u00e7os horizontais\ns\u00e3o: geometria da \u00e1rea de drenagem do reservat\u00f3rio (retangular ou el\u00edptico), comprimento do po\u00e7o,\nraio de drenagem, afastamento vertical, permeabilidade horizontal, permeabilidade vertical, altura\ndo reservat\u00f3rio, press\u00e3o no bordo do reservat\u00f3rio (far field), press\u00e3o no calcanhar (heel) do po\u00e7o,\ndi\u00e2metro do po\u00e7o, viscosidade do \u00f3leo, massa espec\u00edfica do \u00f3leo, fator skin ou de dano, op\u00e7\u00f5es de\ncompleta\u00e7\u00e3o do po\u00e7o (\"po\u00e7o aberto\" ou tela/liner), dados do liner ou tela (quando a completa\u00e7\u00e3o \u00e9\ndo tipo tela/liner).\n\nNas pr\u00f3ximas subse\u00e7\u00f5es, ser\u00e3o descritas as formula\u00e7\u00f5es para o escoamento no reservat\u00f3rio e\nno po\u00e7o, assim como as curvas de fluxo resultantes do modelo.\n\n2.2.2 Escoamento no Reservat\u00f3rio\n\nO escoamento no reservat\u00f3rio \u00e9 assumido como sendo monof\u00e1sico e descrito pela lei de Darcy.\nA conserva\u00e7\u00e3o de massa no reservat\u00f3rio, em regime permanente de fluido incompress\u00edvel, implica\nem:\n\ndivQ = 0 (2.19)\n\ncom\nQ =?\n\n1\n\u00b5\n\n?\nK?p (2.20)\n\nem que\n?\nK \u00e9 o tensor de permeabilidade do meio poroso, \u00b5 \u00e9 a viscosidade do \u00f3leo e Q \u00e9 o fluxo de\n\nfluido (vaz\u00e3o).\n\nAs condi\u00e7\u00f5es de contorno tipo Neumann s\u00e3o definidas como\n\nQ\u00b7\n?\nn =?\n\n1\n\u00b5\n\n?\nK?p\u00b7\n\n?\nn = ?0 (2.21)\n\nisso \u00e9, o fluxo normal Q\u00b7\n?\nn \u00e9 um valor conhecido (?0).\n\nPodem-se aplicar, tamb\u00e9m, condi\u00e7\u00f5es tipo Dirichlet, isto \u00e9, imposi\u00e7\u00e3o de valores de press\u00f5es\nconhecidos p = p0.\n\n\n\n24 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\n2.2.3 Escoamento no Po\u00e7o Horizontal\n\nO escoamento turbulento em tubos \u00e9 descrito pela equa\u00e7\u00e3o de Fanning, de acordo com Fernandes\net al. (2006):\n\nd p\ndx\n\n=\n2 f V 2?\n\nD\n(2.22)\n\nem que V \u00e9 a velocidade m\u00e9dia do fluxo na se\u00e7\u00e3o circular do po\u00e7o, ? \u00e9 a massa espec\u00edfica do fluido,\nD \u00e9 o di\u00e2metro do tubo e f \u00e9 o fator de fric\u00e7\u00e3o, que \u00e9 fun\u00e7\u00e3o do n\u00famero de Reynolds.\n\nUma vez que a vaz\u00e3o no po\u00e7o horizontal \u00e9 descrita pela equa\u00e7\u00e3o\n\nQhw = V\n? D2\n\n4\n(2.23)\n\ntem-se que:\nd p\ndx\n\n=\n32 f ? Q2hw\n\n? 2D5\n(2.24)\n\nou\n\nQhw =?\n\n?\n? 2D5\n\n32 f ?\nd p\ndx\n\n. (2.25)\n\nO sinal negativo indica que a vaz\u00e3o tem sentido do ponto de maior press\u00e3o para o de menor\npress\u00e3o. Pela lei de conserva\u00e7\u00e3o de massa, tem-se que:\n\ndQhw\ndx\n\n+ ql = 0 (2.26)\n\nem que ql \u00e9 o fluxo de \u00f3leo entrando no po\u00e7o.\n\nO fator de fric\u00e7\u00e3o f \u00e9 dado em fun\u00e7\u00e3o do n\u00famero de Reynolds e do tipo de regime de escoa-\nmento. O n\u00famero de Reynolds \u00e9 um n\u00famero adimensional, obtido pela express\u00e3o:\n\nReynolds =\n?V D\n\n\u00b5\n(2.27)\n\nem que V \u00e9 a velocidade m\u00e9dia, ? \u00e9 a massa espec\u00edfica do fluido, D \u00e9 o di\u00e2metro do tubo e \u00b5 a\nviscosidade do fluido.\n\nNa literatura cl\u00e1ssica, escoamentos com n\u00famero de Reynolds inferiores a 2100 s\u00e3o considerados\nescoamentos laminares e, acima de 4000, turbulentos. Desta forma, para escoamentos laminares, o\nfator de fric\u00e7\u00e3o pode ser calculado como:\n\nflaminar =\n16\n\nReynolds\n(2.28)\n\n\n\n2.2. ACOPLAMENTO PO\u00c7O-RESERVAT\u00d3RIO 25\n\ne, para escoamentos turbulentos (f\u00f3rmula de Blasius), como:\n\nfturbulento =\n0,0791\n\nReynolds0,25\n. (2.29)\n\nUma vez que o escoamento em po\u00e7os horizontais pode apresentar regimes diferentes, conforme\na se\u00e7\u00e3o analisada do po\u00e7o, poder-se-ia adotar o fator de fric\u00e7\u00e3o como sendo:\n\nf =\n\n???\n??\n\nflaminar, Reynolds ? 2100\n\nfturbulento, Reynolds > 2100\n. (2.30)\n\nEssa express\u00e3o, entretanto, conduziria a uma descontinuidade no valor do fator de fric\u00e7\u00e3o no\nponto de Reynolds igual a 2100, o que seria bastante desfavor\u00e1vel para a qualidade do resultado da\nsimula\u00e7\u00e3o num\u00e9rica. Com a inten\u00e7\u00e3o de evitar esta inconsist\u00eancia, Devloo et al. (2009) adotaram a\nseguinte express\u00e3o:\n\nf =\n\n???\n??\n\nflaminar, Reynolds ? 1187,38\n\nfturbulento, Reynolds > 1187,38\n, (2.31)\n\nque \u00e9 cont\u00ednua.\n\nSubstituindo as Equa\u00e7\u00f5es 2.28 e 2.29 na Equa\u00e7\u00e3o 2.31, e esta na Equa\u00e7\u00e3o 2.25, tem-se:\n\nQhw =\n\n???\n??\n? ? D\n\n4\n\n128\u00b5\nd p\ndx , Reynolds ? 1187,38\n\n?2,252610888D\n19/7\n\n\u00b5\n1/7?\n\n3/7\n\n(\nd p\ndx\n\n)4/7\n, Reynolds > 1187,38\n\n. (2.32)\n\nPara composi\u00e7\u00e3o do modelo, as Equa\u00e7\u00f5es 2.19 e 2.32 s\u00e3o reescritas na forma fraca, de acordo\ncom o m\u00e9todo dos elementos finitos, e implementadas nos elementos espec\u00edficos da malha: ele-\nmentos de reservat\u00f3rio e elementos de po\u00e7o, respectivamente.\n\n2.2.4 Curvas de Fluxo do Modelo Tridimensional\n\nOs resultados do modelo envolvem um valor num\u00e9rico conhecido como IP (\u00edndice de produtivi-\ndade), muito utilizado para verificar a viabilidade econ\u00f4mica do po\u00e7o, o valor da vaz\u00e3o total Qhw,\ncorrespondente \u00e0 vaz\u00e3o no calcanhar (heel) do po\u00e7o (ou seja, Qheel ), e curvas de fluxo, geradas a\npartir do p\u00f3s-processamento das solu\u00e7\u00f5es provenientes do c\u00e1lculo de elementos finitos. As curvas\nmostram dados sobre o fluxo no interior do po\u00e7o e s\u00e3o descritas na sequ\u00eancia:\n\n1. p(x). Esbo\u00e7a a press\u00e3o ao longo do eixo do po\u00e7o (Figura 2.13);\n\n\n\n26 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\n2. Qhw(x). Esbo\u00e7a a vaz\u00e3o no interior do po\u00e7o (Figura 2.14);\n\n3. dQhw(x)dx . Esbo\u00e7a a distribui\u00e7\u00e3o do fluxo ao longo do po\u00e7o (Figura 2.15);\n\n4. Rey(x). Esbo\u00e7a o valor do n\u00famero de Reynolds ao longo po\u00e7o (Figura 2.16).\n\nRessalta-se que a coordenada x coincide com o eixo do po\u00e7o. As figuras abaixo ilustram as curvas\nde fluxo geradas, nas quais a origem do sistema de coordenadas situa-se no heel do po\u00e7o.\n\nFigura 2.13: Curva de press\u00e3o no po\u00e7o horizontal.\n\nFigura 2.14: Curva de vaz\u00e3o no po\u00e7o horizontal.\n\n\n\n2.2. ACOPLAMENTO PO\u00c7O-RESERVAT\u00d3RIO 27\n\nFigura 2.15: Curva de distribui\u00e7\u00e3o de vaz\u00e3o no po\u00e7o horizontal.\n\nFigura 2.16: Valor do n\u00famero de Reynolds no po\u00e7o horizontal.\n\nPor se tratar de um modelo num\u00e9rico baseado no m\u00e9todo dos elementos finitos, estas curvas\nn\u00e3o apresentam uma descri\u00e7\u00e3o anal\u00edtica, mas s\u00e3o definidas a partir de pares de pontos (posi\u00e7\u00e3o x no\n\n\n\n28 CAP\u00cdTULO 2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\npo\u00e7o, dado num\u00e9rico de fluxo) correspondentes aos elementos finitos e suas fun\u00e7\u00f5es de forma. O\np\u00f3s-processamento opera sobre esses pares de pontos para esbo\u00e7ar as curvas de forma suave. Uma\noutra curva que pode ser obtida no p\u00f3s-processamento \u00e9 a d p(x)dx , a qual \u00e9 \u00fatil para analisar as perdas\nde carga ao longo da extens\u00e3o do po\u00e7o.\n\nAs curvas possibilitam o entendimento do comportamento do fluxo no interior do po\u00e7o, al\u00e9m\nde auxiliar na preven\u00e7\u00e3o de breakthrough de \u00e1gua ou g\u00e1s precocemente, o que poderiam trazer\naspectos adversos \u00e0 explora\u00e7\u00e3o de petr\u00f3leo em po\u00e7os horizontais (Junior et al., 2007). Al\u00e9m do\nmais, quando se busca a uniformiza\u00e7\u00e3o do fluxo ao longo do po\u00e7o (Fernandes et al., 2006) ou\num estudo de influ\u00eancia de par\u00e2mentros sobre a produtividade (Vicente et al., 2003), o uso dessas\ncurvas se torna imprescind\u00edvel.\n\n\n\nCap\u00edtulo 3\n\nMetodologia\n\nA aplica\u00e7\u00e3o da rede neural artificial no presente trabalho baseia-se no grande potencial da rede\ntipo MLP em mapear fun\u00e7\u00f5es a partir de um conjunto de padr\u00f5es de treinamento a ela apresen-\ntado. Desta forma, foi desenvolvida uma metodologia para utilizar esse potencial para mapear um\npar\u00e2metro de fluxo proveniente dos resultados do modelo tridimensional do acoplamento po\u00e7o-\nreservat\u00f3rio. \u00c9 gerado um conjunto num\u00e9rico relativo a esse par\u00e2metro e apresentado \u00e0 rede neural\ncomo dados de treinamento. A MLP, ap\u00f3s o processo de treinamento, fornece ao modelo simplifi-\ncado unidimensional os valores de fluxo do reservat\u00f3rio, de forma que esse modelo gere resultados\ncompar\u00e1veis com o modelo tridimensional. Essa etapa \u00e9 desenvolvida a partir de um arranjo entre\na MLP e o modelo simplificado. Para gera\u00e7\u00e3o da MLP, foi desenvolvida uma biblioteca em C++,\nde forma a facilitar a aplica\u00e7\u00e3o junto ao modelo unidimensional.\n\nDestacar-se-\u00e1 primeiramente uma descri\u00e7\u00e3o geral da metodologia e das ferramentas computaci-\nonais utilizadas no trabalho. Posteriomente, descrever-se-\u00e1 o modelo simplificado unidimensional\ndo acoplamento po\u00e7o-reservat\u00f3rio. Em seguida, ser\u00e3o descritos a metodologia para gerar o con-\njunto de dados e o treinamento da MLP, finalizando-se com a descri\u00e7\u00e3o do arranjo da MLP e o\nmodelo unidimensional.\n\n3.1 Descri\u00e7\u00e3o Geral\n\nToda a metodologia \u00e9 baseada no desenvolvimento de arranjar as redes neurais artificiais com\num modelo simplificado do acoplamento po\u00e7o-reservat\u00f3rio. Para calibrar o modelo simplificado,\nutilizar-se-\u00e1 um conjunto de dados contendo informa\u00e7\u00f5es sobre um par\u00e2metro de fluxo do modelo\ntridimensional como padr\u00f5es de treinamento das redes neurais. Conforme ser\u00e1 descrito em 3.3 e em\n3.4, esse par\u00e2metro de fluxo \u00e9 definido como resistividade K(x), vari\u00e1vel ao longo do eixo do po\u00e7o.\nA ideia fundamental \u00e9 que essa resistividade K(x) mapeada pelas redes neurais possa ser utilizada\n\n29\n\n\n\n30 CAP\u00cdTULO 3. METODOLOGIA\n\nna resolu\u00e7\u00e3o do modelo simplificado unidimensioanl e, assim, produzir resultados semelhantes aos\nobtidos atrav\u00e9s do modelo de elementos finitos. A Figura 3.1 ilustra o processo de arranjo entre\nas redes neurais e o modelo unidimensional, juntamente com a compara\u00e7\u00e3o entre os resultados do\nmodelo de elementos finitos:\n\nFigura 3.1: Arranjo das redes neurais e modelo 1D do acomplamento po\u00e7o-resevat\u00f3rio\n\nConforme a Figura 3.1 apresenta, o modelo de elementos finitos fornece um conjunto de dados\nreferentes \u00e0 resistividade K(x) para treinamento das redes neurais; essas, ap\u00f3s treinamento, s\u00e3o\nrespons\u00e1veis por fornecer os valores de K(x) para o modelo simplificado unidimensional. Ao final,\nos resultados do modelo simplificado e do m\u00e9todo dos elementos finitos deveriam ser, ao menos,\npr\u00f3ximos ou semelhantes. Em 3.6.2, s\u00e3o descritas as medidas de erros utilizadas para avaliar os\nresultados entre os dois modelos.\n\n3.2 Ferramentas Computacionais Utilizadas\n\nO trabalho foi desenvolvido e implementado num ambiente de programa\u00e7\u00e3o em linguagem C++,\no Xcode. Foi utilizado, al\u00e9m disso, um software de matem\u00e1tica simb\u00f3lica como aux\u00edlio durante as\nimplementa\u00e7\u00f5es, valida\u00e7\u00f5es e visualiza\u00e7\u00f5es de gr\u00e1ficos bidimensionais e tridimensionais.\n\nO modelo tridimensional do acoplamento po\u00e7o-reservat\u00f3rio tem um kernel escrito em C++,\npossuindo entrada e sa\u00edda de dados a partir de uma interface ou estrutura de dados ligada ao kernel.\n\n\n\n3.3. ACOPLAMENTO PO\u00c7O-RESERVAT\u00d3RIO: UNIDIMENSIONAL 31\n\nDessa forma, a gera\u00e7\u00e3o e execu\u00e7\u00e3o dos casos para composi\u00e7\u00e3o do conjunto de treinamento, assim\ncomo a obten\u00e7\u00e3o dos resultados, foram realizadas a partir do acesso e manipula\u00e7\u00e3o dessa estrutura\nde dados.\n\nAs MLPs foram definidas a partir de uma biblioteca desenvolvida para gera\u00e7\u00e3o de redes neurais\nartificiais. Tal biblioteca, denominada NeuralLib, foi implementada na linguagem de programa\u00e7\u00e3o\nC++. Possui estrutura t\u00edpica de programa\u00e7\u00e3o orientada a objetos. A NeuralLib foi concebida para\npermitir a cria\u00e7\u00e3o e uso de redes neurais artificiais considerando v\u00e1rios tipos de neur\u00f4nios, fun\u00e7\u00f5es\nde ativa\u00e7\u00e3o, arquitetura de redes e algoritmos de treinamento. Essa versatilidade deve-se \u00e0 filosofia\nde programa\u00e7\u00e3o orientada a objetos, a qual permite reutiliza\u00e7\u00e3o de c\u00f3digos e facilidade de imple-\nmenta\u00e7\u00f5es em c\u00f3digos existentes. Por ser fruto do presente trabalho, essa biblioteca n\u00e3o contempla\nainda todas as tipologias pertinentes \u00e0s redes neurais de forma geral. Assim, no momento, somente\na arquitetura de rede do tipo perceptron de m\u00faltiplas camadas e o algoritmo backpropagation pos-\nsuem implementa\u00e7\u00f5es validadas. Implementa\u00e7\u00f5es futuras, no entanto, poder\u00e3o ser realizadas para\ndesenvolvimento de tal biblioteca.\n\nO trabalho utilizou o kernel do modelo tridimensional e a NeuralLib no mesmo ambiente de\nprograma\u00e7\u00e3o (Xcode), de forma a facilitar a intera\u00e7\u00e3o entre as estruturas de dados e rotinas de\nc\u00e1lculo. Detalhes da biblioteca desenvolvida est\u00e3o em 4.1 e no Ap\u00eandice A.\n\n3.3 Acoplamento Po\u00e7o-Reservat\u00f3rio: Unidimensional\n\nO modelo unidimensional do acoplamento po\u00e7o-reservat\u00f3rio baseia-se nas formula\u00e7\u00f5es de fluxo\ndescritas em 2.2.2 e 2.2.3. O modelo unidimensional modela, por\u00e9m, o fluxo no reservat\u00f3rio de\nforma simplificada: representa-se o reservat\u00f3rio sobre o po\u00e7o horizontal como uma esp\u00e9cie de\nresistividade K(x), a qual gera uma resist\u00eancia para o fluxo do reservat\u00f3rio em dire\u00e7\u00e3o ao po\u00e7o. A\nFigura 3.2 ilustra o modelo simplificado unidimensional.\n\nFigura 3.2: Modelo simplificado unidimensional do acoplamento po\u00e7o-reservat\u00f3rio.\n\n\n\n32 CAP\u00cdTULO 3. METODOLOGIA\n\nO equacionamento para este modelo pode ser desenvolvido analisando o fluxo no po\u00e7o. Pela\nlei de conserva\u00e7\u00e3o de massa, da Equa\u00e7\u00e3o 2.26, tem-se:\n\ndQhw(x)\ndx\n\n+ ql(x) = 0. (3.1)\n\nO fluxo de \u00f3leo entrando no po\u00e7o (ql(x)) \u00e9 proporcional \u00e0 diferen\u00e7a de press\u00e3o entre po\u00e7o (p(x))\ne press\u00e3o no reservat\u00f3rio pres. O fluxo pode ser equacionado como:\n\nql(x) = K(x)\u00b7(p(x)? pres) (3.2)\n\nem que K(x) \u00e9 a pr\u00f3pria resistividade que representa o reservat\u00f3rio. Substituindo a Equa\u00e7\u00e3o 3.2 em\n3.1, tem-se:\n\ndQhw(x)\ndx\n\n+ K(x)\u00b7(p(x)? pres) = 0 (3.3)\n\nou\ndQhw(x)\n\ndx\n=?K(x)\u00b7(p(x)? pres). (3.4)\n\nIsolando K(x), tem-se:\n\nK(x) =?\ndQhw(x)\n\ndx\n\u00b7\n\n1\n(p(x)? pres)\n\n. (3.5)\n\nO equacionamento para o escoamento no po\u00e7o \u00e9 o mesmo como apresentado em 2.2.3, ou seja:\n\nd p(x)\ndx\n\n=\n32 f ? Qhw(x)2\n\n? 2D5\n. (3.6)\n\nDessa forma, as equa\u00e7\u00f5es diferenciais a serem resolvidas para o modelo unidimensional s\u00e3o\ncompostas pelas Equa\u00e7\u00f5es 3.4 e 3.6:\n\ndQhw(x)\ndx\n\n=?K(x)\u00b7(p(x)? pres)\n\nd p(x)\ndx\n\n=\n32 f ? Qhw(x)2\n\n? 2D5\n\nem que f \u00e9 o mesmo apresentado em 2.2.3.\n\n\n\n3.4. AN\u00c1LISE DA RESISTIVIDADE K(X) 33\n\nAs condi\u00e7\u00f5es de contorno seguem as do modelo tridimensional:\n\n\u2022 Press\u00e3o no calcanhar po\u00e7o (heel) conhecida: P(xheel) = Pheel , em que xheel \u00e9 a posi\u00e7\u00e3o relativa\nao calcanhar do po\u00e7o;\n\n\u2022 Vaz\u00e3o no ded\u00e3o do po\u00e7o (toe) igual a zero: Qhw(xtoe) = Qtoe = 0, em que xtoe \u00e9 a posi\u00e7\u00e3o\nrelativa ao ded\u00e3o do po\u00e7o;\n\n\u2022 Press\u00e3o no reservat\u00f3rio pres (far field) constante: pres = constante.\n\nO perfil da resistividade K(x) denota o comportamento de fluxo resultante da intera\u00e7\u00e3o entre o\nreservat\u00f3rio e o po\u00e7o produtor. A Figura 3.3 apresenta um exemplo da curva K(x) proveniente de\num caso simulado a partir do modelo tridimensional. Ressalta-se que o eixo x representa o eixo do\npo\u00e7o.\n\nFigura 3.3: Exemplo da curva K(x).\n\nA Se\u00e7\u00e3o 3.4 descrever\u00e1 uma interpreta\u00e7\u00e3o e uma representa\u00e7\u00e3o mais detalhada sobre a curva\nK(x).\n\n3.4 An\u00e1lise da Resistividade K(x)\n\nA Equa\u00e7\u00e3o 3.5 indica que \u00e9 poss\u00edvel obter o valor para resistividade K(x) a partir do valor de dQhw(x)dx\ne p(x)? pres. A resistividade K(x) pode ser interpretada como uma corre\u00e7\u00e3o do fluxo no interior\ndo po\u00e7o devido \u00e0 presen\u00e7a de fluxos esf\u00e9ricos nas extremidades do mesmo quando considerada a\nequa\u00e7\u00e3o anal\u00edtica para fluxo radial, aplicada em po\u00e7os verticais. Ser\u00e1 descrita a adimensionaliza\u00e7\u00e3o\nda resistividade partindo dessa interpreta\u00e7\u00e3o. Na sequ\u00eancia, descrever-se-\u00e1 a representa\u00e7\u00e3o de K(x)\npor meio de polin\u00f4mios.\n\n\n\n34 CAP\u00cdTULO 3. METODOLOGIA\n\n3.4.1 Adimensionaliza\u00e7\u00e3o da Resistividade K(x)\n\nConforme apresentado em Rosa et al. (2006), a equa\u00e7\u00e3o de fluxo radial permanente considerando\nfluido incompress\u00edvel e perda de carga no interior do po\u00e7o como desprez\u00edvel \u00e9:\n\nQw =\n2? kh(pres ? pw)\n\n\u00b5 ln(rres/rw)\n(3.7)\n\nem que Qw \u00e9 a vaz\u00e3o total do po\u00e7o, k \u00e9 a permeabilidade do meio poroso, tido como isotr\u00f3pico,\nh \u00e9 a altura do reservat\u00f3rio (igual ao comprimento do po\u00e7o, considerando po\u00e7o verticais), \u00b5 \u00e9 a\nviscosidade do fluido, rres \u00e9 o raio externo do reservat\u00f3rio, rw \u00e9 o raio do po\u00e7o. Pode-se definir uma\nvaz\u00e3o por unidade de comprimento de po\u00e7o que o reservat\u00f3rio pode fornecer, como segue:\n\nqlw =\nQw\nh\n\n(3.8)\n\na qual possui valor constante, j\u00e1 que \u00e9 desprezada a perda de carga no interior do po\u00e7o vertical.\nConsiderando a lei de conserva\u00e7\u00e3o de massa a partir de 3.1, tem-se:\n\ndQw(x)\ndx\n\n+ qlw = 0. (3.9)\n\nUtilizando-se de 3.8 e de 3.9, tem-se:\n\ndQw(x)\ndx\n\n=?\nQw\nh\n\n= cte (3.10)\n\no que leva \u00e0 conclus\u00e3o de que o valor de dQw(x)dx =\ndQw\ndx \u00e9 constante para todo x, para um po\u00e7o\n\nvertical. De 3.7, pode-se escrever:\n\nQw\nh\n\n=\n2? k(pres ? pw)\n\u00b5 ln(rres/rw)\n\n. (3.11)\n\nSubstituindo Qwh em 3.11 a partir de 3.10, tem-se:\n\ndQw\ndx\n\n=?\n2? k(pres ? pw)\n\u00b5 ln(rres/rw)\n\n=\n2? k(pw ? pres)\n\u00b5 ln(rres/rw)\n\n. (3.12)\n\nRearranjando 3.12, tem-se:\n\ndQw\ndx\n\n1\n(pw ? pres)\n\n=\n2? k\n\n\u00b5 ln(rres/rw)\n. (3.13)\n\nUsando o conceito da resistividade pela 3.5, tem-se:\n\nKvw =?\n2? k\n\n\u00b5 ln(rres/rw)\n(3.14)\n\n\n\n3.4. AN\u00c1LISE DA RESISTIVIDADE K(X) 35\n\nem que Kvw \u00e9 a resistividade para po\u00e7o vertical.\n\nEmbora a resistividade Kvw seja constante, ela n\u00e3o \u00e9 adimensional. Pode-se definir uma resisti-\nvidade adimensional, Kvw, como se segue:\n\nKvw =?\n\u00b5 ln(rres/rw)\n\n2? k\nKvw = 1. (3.15)\n\nO racioc\u00ednio de desenvolvimento de K?vw pode ser utilizado para encontrar uma adimensionali-\nza\u00e7\u00e3o de K(x).\n\nEm po\u00e7os horizontais, a geometria do po\u00e7o e do reservat\u00f3rio fazem aparecer os fluxos esf\u00e9ricos\nnas extremidades do po\u00e7o. Desta forma, o uso da Equa\u00e7\u00e3o 3.7 em po\u00e7os horizontais perde sentido\ne aplicabilidade. Por\u00e9m, \u00e9 poss\u00edvel ainda escrever uma equa\u00e7\u00e3o com os mesmos princ\u00edpios, por\u00e9m\ncom um fator de corre\u00e7\u00e3o que contabilize os fluxos esf\u00e9ricos. Apenas com a motiva\u00e7\u00e3o de se obter\numa adimensionaliza\u00e7\u00e3o plaus\u00edvel da resisitividade K(x) no caso de po\u00e7os horizontais, pode-se\nescrever uma equa\u00e7\u00e3o \u00e0 semelhan\u00e7a da Equa\u00e7\u00e3o 3.7:\n\nQhw =\n2? klhw(pres ? pheel)\n\n\u00b5(Bres/Hres)\n(3.16)\n\nem que lhw \u00e9 o comprimento do po\u00e7o, Qhw \u00e9 vaz\u00e3o total produzida pelo po\u00e7o, Bres e Hres s\u00e3o largura\ne altura do reservat\u00f3rio, respectivamente. Ressalta-se que a Equa\u00e7\u00e3o 3.16 apenas foi desenvolvida\ncomo motiva\u00e7\u00e3o da adimensionaliza\u00e7\u00e3o que ser\u00e1 descrita na sequ\u00eancia, n\u00e3o possuindo uma descri-\n\u00e7\u00e3o fiel do problema f\u00edsico.\n\nUsando a Equa\u00e7\u00e3o 3.16, pode ser obtida uma vaz\u00e3o por comprimento de po\u00e7o, como se segue:\n\nQhw\nlhw\n\n=\n2? k(pres ? pheel)\n\n\u00b5(Bres/Hres)\n=?\n\n2? k(pheel ? pres)\n\u00b5(Bres/Hres)\n\n(3.17)\n\nque \u00e9 constante para todo o trecho. Pode-se relacionar essa vaz\u00e3o por comprimento com o dQhw(x)dx\nreal do po\u00e7o horizontal:\n\ndQhw(x)\ndx\n\n=\nQhw\nlhw\n\n?(x) =?\n2? k(pheel ? pres)\n\n\u00b5(Bres/Hres)\n?(x) (3.18)\n\nem que ?(x) \u00e9 um fator de corre\u00e7\u00e3o requerido para contabilizar os fluxos esf\u00e9ricos. Definindo uma\nresistividade de forma an\u00e1loga \u00e0 Equa\u00e7\u00e3o 3.14, tem-se:\n\nKhw =?\n2? k\n\n\u00b5(Bres/Hres)\n. (3.19)\n\nDa Equa\u00e7\u00e3o 3.4, e utilizando essa resistividade na Equa\u00e7\u00e3o 3.18, tem-se:\n\n\n\n36 CAP\u00cdTULO 3. METODOLOGIA\n\n?(x) =?\nK(x)(p(x)? pres)\nKhw(pheel ? pres)\n\n. (3.20)\n\nA Equa\u00e7\u00e3o 3.18 pode ser escrita ent\u00e3o como:\n\ndQhw(x)\ndx\n\n=?\nQw\nlhw\n\nK(x)(p(x)? pres)\nKhw(pheel ? pres)\n\n. (3.21)\n\nA partir da Equa\u00e7\u00e3o 3.21, pode-se definir uma nova resistividade, como se segue:\n\nK(x) =\nK(x)\nKhw\n\n. (3.22)\n\nEssa nova resistividade \u00e9 adimensional e pode ser interpretada como um fator de corre\u00e7\u00e3o para\ncontabilizar os fluxos esf\u00e9ricos sobre o Qhw/lhw proveniente da equa\u00e7\u00e3o de escoamento radial. Para\nos objetivos do trabalho, optou-se por trabalhar com essa resistividade adimesional, principalmente\npara o treinamento das redes neurais. Comparando com a Equa\u00e7\u00e3o 3.15, a resistividade K(x), al\u00e9m\nde n\u00e3o ser constante, n\u00e3o possui valor unit\u00e1rio como a Kvh; por\u00e9m, ela tende a variar em torno do\nvalor unit\u00e1rio por conta dos fluxos esf\u00e9ricos.\n\n3.4.2 Representa\u00e7\u00e3o de K(x)\n\nAl\u00e9m do processo de adimensionalizar, foi necess\u00e1rio representar a resistividade com poucos pa-\nr\u00e2metros, j\u00e1 que n\u00e3o h\u00e1 uma express\u00e3o anal\u00edtica que a defina, principalmente por se tratar de um\np\u00f3s-processamento dos resultados do m\u00e9todo dos elementos finitos. Desta forma, optou-se em\nrepresent\u00e1-la atrav\u00e9s de uma forma polinomial. Devido \u00e0s caracter\u00edsticas inerentes aos polin\u00f4mios\nortogonais, tais como ortogonalidade em um intervalo, ra\u00edzes distintas nesse intervalo etc. (Cunha,\n2009; Burden e Faires, 2008), optou-se por utilizar esse tipo de fun\u00e7\u00e3o. Dentre outros, escolheu-se\nos polin\u00f4mios de Legendre, que s\u00e3o ortogonais em [?1, 1] com rela\u00e7\u00e3o \u00e0 fun\u00e7\u00e3o peso ?(x) ? 1\n(Burden e Faires (2008)).\n\nA representa\u00e7\u00e3o da resistividade K(x) dada pela Equa\u00e7\u00e3o 3.5, isto \u00e9, dimensional, pode gerar\ncertos problemas num\u00e9ricos, devido \u00e0 gera\u00e7\u00e3o de coeficientes polinomiais com valores relativa-\nmente pequenos devido a alta ordem dos valores de press\u00e3o. Isso justifica a adimensionaliza\u00e7\u00e3o\ndescrita em 3.4.1. Al\u00e9m do mais, por ser uma fun\u00e7\u00e3o descrita ao longo do eixo do po\u00e7o, a adimen-\nsionaliza\u00e7\u00e3o do eixo x tamb\u00e9m torna-se justific\u00e1vel. Assim, para estar coerente com o intervalo\nonde os polin\u00f4mios de Legendre s\u00e3o ortogonais, optou-se escalonar o eixo x, definido original-\nmente entre [?lhw2 ,\n\nlhw\n2 ], para x? ? [?1, 1]. Dessa forma, a resistividade adimensional K?(x?) pode ser\n\naproximada por uma fun\u00e7\u00e3o polinomial como:\n\nK?(x?)?= K?p(x?) = anLn(x?)+ an?1Ln?1(x?)+...+ a1L1(x?)+ a0 (3.23)\n\n\n\n3.4. AN\u00c1LISE DA RESISTIVIDADE K(X) 37\n\nem que Ln(x?) \u00e9 o n-\u00e9simo polin\u00f4mio de Legendre, an s\u00e3o coeficientes multiplicadores e o sub\u00edndice\np indica que \u00e9 fun\u00e7\u00e3o polinomial.\n\nOs resultados provenientes do modelo de elementos finitos s\u00e3o expressos por pares de pontos,\nos quais relacionam coordenadas geom\u00e9tricas e valores de uma dada solu\u00e7\u00e3o, conforme descrito na\nSubse\u00e7\u00e3o 2.2.4. Dessa forma, para representar de forma polinomial, foi necess\u00e1rio ajustar os pares\nde pontos referentes \u00e0 resistividade K(x) atrav\u00e9s de regress\u00e3o linear. Foi utilizado o m\u00e9todo dos\nquadrados m\u00ednimos sobre o conjunto de resultados para calcular os coeficientes an. Testes conside-\nrando o valor num\u00e9rico de res\u00edduo proveniente da regress\u00e3o linear revelaram que um polin\u00f4mio de\ngrau 6 adequa-se suficientemente a tal ajuste. A metodologia para realizar a regress\u00e3o linear est\u00e1\ndescrita em Cunha (2009). Abaixo segue a sequ\u00eancia de passos para obten\u00e7\u00e3o de K?p(x?), para cada\ncaso do conjunto de treinamento (vide Subse\u00e7\u00e3o 3.5.1):\n\n1. C\u00e1lculo de K(x), a partir dos pares de pontos:\n\nK(xi) =\n{\n?\n\ndQhw(xi)\ndx\n\n\u00b7\n1\n\n(p(xi)? pres)\n\n}\nF EM?3D\n\nem que xi s\u00e3o os pontos no eixo do po\u00e7o e o sub\u00edndice F EM?3D indica que os argumentos\ns\u00e3o provenientes dos resultados do modelo de elementos finitos.\n\n2. Adimensionaliza\u00e7\u00e3o de K(xi) e c\u00e1lculo de K?(xi):\n\nK?(xi) =\nK(xi)\nKhw\n\nem que Khw \u00e9 dado pela Equa\u00e7\u00e3o 3.19.\n\n3. Reescalonamento dos xi:\n\nx?i =\n2\u00b7xi\nlwh\n\nsendo lwh o comprimento total do po\u00e7o horizontal.\n\n4. Composi\u00e7\u00e3o dos novos pares de pontos:\n\n(x?i, K?(x?i)), i = 1,..,np\n\nem que np \u00e9 o n\u00famero de pontos os quais o modelo de elementos finitos gera durante o\np\u00f3s-processamento.\n\n\n\n38 CAP\u00cdTULO 3. METODOLOGIA\n\n5. Regress\u00e3o linear - c\u00e1lculo de K?p(x?):\n\nEncontrar os coeficientes an, n = 0,..6, tal que minimize a fun\u00e7\u00e3o objetivo:\n\nE7 (a0,.., a6) =\nnp\n\n?\ni=1\n\n[K?(x?i)?K?p(x?i)]\n2 (3.24)\n\nem que K?p(x?i) = a6L6(x?i)+...+ a1L1(x?i)+ a0.\n\nO uso da regress\u00e3o linear permite utilizar a resistividade K?P(x?) no lugar de K(x) sem maiores perdas\nde precis\u00e3o, trazendo a vantagem de ser descrita atrav\u00e9s de um n\u00famero pequeno de par\u00e2metros,\nan, n = 0,.., 6. Esses par\u00e2metros, como ser\u00e1 descrito na Subse\u00e7\u00e3o 3.5.1, s\u00e3o os valores esperados\ndo conjunto de treinamento da MLP.\n\n3.5 Aprendizagem da Rede Neural Artificial\n\nA representa\u00e7\u00e3o da resistividade K(x) na forma polinomial (K?p(x?)), conforme descrito em 3.4.2,\npossibilita gerar conjunto de dados pass\u00edveis de serem usados no treinamento de uma rede neural\ntipo MLP. A gera\u00e7\u00e3o deste conjunto de dados e o processo de aprendizagem da rede neural s\u00e3o\ndescritos a seguir.\n\n3.5.1 Conjunto de Treinamento\n\nA cada c\u00e1lculo que se fa\u00e7a usando o modelo de elementos finitos tridimensional, um polin\u00f4mio de\ngrau 6 \u00e9 encontrado de forma a descrever a resistividade K(x) para tal simula\u00e7\u00e3o. Ou seja, em cada\nsimula\u00e7\u00e3o, existem 7 par\u00e2metros an, n = 0,.., 6, que descrevem a resistividade adimensionalizada\nK?p(x?). Esse conjunto de par\u00e2mentros para uma dada simula\u00e7\u00e3o (ou caso) k ser\u00e1 denominado como\n~a(k), isto \u00e9, vetor de sa\u00edda esperado do k-\u00e9simo caso do conjunto de treinamento, \u00e0 semelhan\u00e7a do\nvetor ~d(k) descrito em 2.1.4; portanto: ~a(k)? ~d(k).\n\nConforme descrito em 2.2.1, o modelo tridimensional do acomplamento po\u00e7o-reservat\u00f3rio apre-\nsenta uma gama de dados de entrada ou vari\u00e1veis inputs necess\u00e1rias para defini\u00e7\u00e3o do problema\nf\u00edsico a ser simulado. Al\u00e9m do mais, cada input ou vari\u00e1vel pode assumir qualquer valor dentro\nde um intervalo relativamente grande, de acordo com a natureza f\u00edsica da vari\u00e1vel. Por exemplo, a\nvari\u00e1vel \u201cpermeabilidade\u201d (horizontal e vertical) ter\u00e1 seu valor extremamente dependente do tipo\nde rocha pela qual o reservat\u00f3rio \u00e9 constitu\u00eddo. Assim, selecionou-se algumas vari\u00e1veis de input\npara formar os sinais de entrada da rede neural, com intuito de observar a capacidade e limita\u00e7\u00e3o\nda MLP quando do treinamento e generaliza\u00e7\u00e3o de valores. Para cada vari\u00e1vel selecionada, foi esti-\npulado um intervalo de varia\u00e7\u00e3o a partir de valores usuais de campo, de forma a respeitar o modelo\n\n\n\n3.5. APRENDIZAGEM DA REDE NEURAL ARTIFICIAL 39\n\nf\u00edsico. Algumas considera\u00e7\u00f5es adicionais com rela\u00e7\u00e3o ao modelo do reservat\u00f3rio e do po\u00e7o foram\nassumidas para o presente trabalho:\n\n\u2022 \u00c1rea de drenagem retangular;\n\n\u2022 Completa\u00e7\u00e3o de po\u00e7o tipo aberto;\n\n\u2022 Fator de dano (skin) nulo;\n\n\u2022 Comprimento do po\u00e7o (m): 400,0;\n\n\u2022 Press\u00e3o no reservat\u00f3rio - far field (kg f /cm2): 225,0;\n\nAs vari\u00e1veis de input selecionadas e os intervalos de varia\u00e7\u00e3o utilizados para gera\u00e7\u00e3o dos casos\ns\u00e3o:\n\n\u2022 Raio de drenagem (m), intervalo: [400,0; 1600,0];\n\n\u2022 Altura do reservat\u00f3rio (m), intervalo: [15,0; 60,0];\n\n\u2022 Afastamento vertical (m), intervalo: [5,0; 55,0];\n\n\u2022 Press\u00e3o no po\u00e7o - heel (kg f /cm2), intervalo: [117,0; 225,0);\n\n\u2022 Raio do po\u00e7o (m), intervalo: [0,05; 0,10];\n\n\u2022 Viscosidade do \u00f3leo (Pa\u00b7s), intervalo: [0,001; 0,007];\n\n\u2022 Massa espec\u00edfica do \u00f3leo (kg/m3), intervalo: [750,0; 950,0].\n\nAs permeabilidades horizontal e vertical do reservat\u00f3rio foram consideradas como constantes,\nsendo adotado um valor t\u00edpico. Assim, essas 7 vari\u00e1veis formam os sinais de entrada da rede\nneural, os quais ser\u00e3o definidos como ~?(k), isto \u00e9, vetor de sinal de entrada para o k-\u00e9simo caso do\nconjunto de treinamento.\n\nAo todo, foram gerados 100 casos a partir do modelo tridimensional variando de forma aleat\u00f3ria\nas vari\u00e1veis de input. Esses 100 casos formam o conjunto de dados para os quais as redes neurais\nforam treinadas, sendo definido como:\n\nA ={(~?(k),~a(k)) : ~a(k) = fF EM?3D(~?(k)), k = 1,..., nt, nt ? N} (3.25)\n\nem que fF EM?3D(~?) representa o modelo tridimensional, e nt = 100. Os casos foram numerados\nde 1 a 100 para facilitar a an\u00e1lise individual.\n\nEsse conjunto de dados foi dividido em tr\u00eas subconjuntos, a saber:\n\n\n\n40 CAP\u00cdTULO 3. METODOLOGIA\n\n\u2022 Subconjunto de treinamento. Esse \u00e9 o conjunto de casos utilizados para ajustar os pesos\nsin\u00e1pticos da rede neural atrav\u00e9s do algoritmo de treinamento. O subconjunto \u00e9 formado por\n50 casos escolhidos de forma aleat\u00f3ria, representando 50,0% do conjunto A. Define-se esse\nsubconjunto como:\n\nAT ? A|AT ={(~?(k),~a(k)) : ~a(k) = fF EM?3D(~?(k)), k = 1,..., 50}. (3.26)\n\n\u2022 Subconjunto de valida\u00e7\u00e3o. Esse \u00e9 o conjunto de casos utilizados para analisar o desempenho\ndo processo de treinamento da rede neural. O subconjunto \u00e9 utilizado de forma a evitar o\nchamado \"supertreinamento\" ou overfitting, isto \u00e9, quando a rede neural perde a capacidade\nde extrair as caracter\u00edsitcas gerais do subconjunto de treinamento e come\u00e7a a produzir gene-\nraliza\u00e7\u00f5es ruins. Assim, o uso desse subconjunto permite determinar o n\u00famero de itera\u00e7\u00f5es\nm\u00e1ximo no processo de treinamento. O termo overfitting ser\u00e1 empregado no decorrer do\ntexto. O subconjunto \u00e9 composto por 25 casos, compondo 25,0% do conjunto A. \u00c9 definido\ncomo:\n\nAv ? A|Av ={(~?(k),~a(k)) : ~a(k) = fF EM?3D(~?(k)), k = 51,..., 75}. (3.27)\n\n\u2022 Subconjunto de teste. Esse subconjunto n\u00e3o participa diretamente do processo de treina-\nmento, por\u00e9m tem a finalidade de avaliar o estado da rede neural ap\u00f3s treinamento com rela-\n\u00e7\u00e3o \u00e0s generaliza\u00e7\u00f5es. O subconjunto \u00e9 composto por 25 casos, ou seja, 25,0% do conjunto\nA. Define-se esse subconjunto como:\n\nAte ? A|Ate ={(~?(k),~a(k)) : ~a(k) = fF EM?3D(~?(k)), k = 76,..., 100}. (3.28)\n\nAssim, AT ?Av ?Ate = A.\n\nCabe ressaltar que, ap\u00f3s a gera\u00e7\u00e3o dos 100 casos, foi realizada uma an\u00e1lise entre os casos dos\nsubconjuntos Av e Ate e os do subconjunto de treinamento AT . O objetivo dessa an\u00e1lise foi de verifi-\ncar se os casos dos subconjuntos Av e Ate estavam no interior do hipercubo gerado pelos elementos\ndo subconjunto AT . Essa ressalva foi feita para garantir que todos os casos que n\u00e3o estivessem no\nsubconjunto de treinamento (AT ) fossem casos de interpola\u00e7\u00e3o e n\u00e3o de extrapola\u00e7\u00e3o. A an\u00e1lise\nadotada baseou-se em verificar individualmente se cada vari\u00e1vel de input e output dos casos de Av\ne Ate estavam entre os valores extremos das mesmas vari\u00e1veis nos casos de AT . Um procedimento\nmais aprimorado pode ser encontrado em Bazaraa et al. (1993), no qual s\u00e3o descritos algoritmos\npara an\u00e1lise de conjuntos (convex sets e convex hulls). Embora se tenha procurado evitar que a\nMLP extrapolasse algum caso gerado, sabe-se que as redes neurais s\u00e3o, idealmente, capazes de\nmapear valores de fun\u00e7\u00f5es mesmo estando al\u00e9m do intervalo de treinamento. No presente trabalho,\ncontudo, preferiu-se utilizar as redes neurais apenas como fun\u00e7\u00f5es interpoladoras.\n\nA an\u00e1lise e crit\u00e9rio de erros do processo de treinamento ser\u00e3o discutidos na Subse\u00e7\u00e3o 3.5.3. A\nestrutura de rede adotada \u00e9 descrita na sequ\u00eancia.\n\n\n\n3.5. APRENDIZAGEM DA REDE NEURAL ARTIFICIAL 41\n\n3.5.2 Arquitetura de Rede Neural Utilizada\n\nA arquitetura de rede utilizada no presente trabalho \u00e9 o perceptron de m\u00faltiplas camadas, ou MLP,\ndescrito com detalhes em 2.1.3. A estrutura adotada possui duas camadas de processamento, sendo\numa delas oculta. Embora o objetivo do trabalho n\u00e3o seja encontrar a arquitetura ideal para o pro-\nblema, optou-se por gerar tr\u00eas MLPs, variando-se a quantidade de neur\u00f4nios na camada oculta,\ncom intuito de comparar os resultados de cada uma. De acordo com o teorema da aproxima\u00e7\u00e3o\nuniversal, descrito em Haykin (2001), uma \u00fanica camada oculta \u00e9 suficiente para um perceptron\nde m\u00faltiplas camadas computar uma aproxima\u00e7\u00e3o ? uniforme para um dado conjunto de treina-\nmento representado pelo conjunto de entradas e a sa\u00eddas-alvo. Por\u00e9m, o teorema n\u00e3o diz que a\n\u00fanica camada oculta \u00e9 \u00f3tima no sentido do tempo de aprendizagem, facilidade de implementa\u00e7\u00e3o\nou generaliza\u00e7\u00e3o (Haykin, 2001), al\u00e9m de n\u00e3o prever a quantidade de neur\u00f4nios necess\u00e1rios para a\naproxima\u00e7\u00e3o ? adotada. Ou seja, trata-se de um teorema existencial, o qual justifica matematica-\nmente a aproxima\u00e7\u00e3o de uma fun\u00e7\u00e3o cont\u00ednua arbitr\u00e1ria por uma rede perceptron de uma camada\noculta. Dessa forma, a compara\u00e7\u00e3o entre as tr\u00eas arquiteturas adotadas ilustrar\u00e1 as capacidades e li-\nmita\u00e7\u00f5es de cada uma. Optou-se por gerar as tr\u00eas redes com 5, 10 e 15 neur\u00f4nios na camada oculta,\nsendo denominadas como MLP-5, MLP-10 e MLP-15, respectivamente. A camada de entrada pos-\nsui 7 neur\u00f4nios e a de sa\u00edda, 7, de acordo com as vari\u00e1veis de input e output, respectivamente.\n\nEscolheu-se como fun\u00e7\u00e3o de ativa\u00e7\u00e3o dos neur\u00f4nios a fun\u00e7\u00e3o sigmoidal (tangente hiperb\u00f3lica),\nque \u00e9 uma fun\u00e7\u00e3o anti-sim\u00e9trica. Para os par\u00e2metros a e b da fun\u00e7\u00e3o, conforme apresentado em\n2.8, adotou-se os seguintes valores:\n\na = 1,7159\n\ne\n\nb =\n2\n3\n.\n\nDe acordo com Haykin (2001), esses valores em especial produzem algumas propriedades \u00fateis\nno que se diz ao comportamento da fun\u00e7\u00e3o tangente hiperb\u00f3lica. Pode-se listar essas propriedades\ncomo:\n\n\u2022 ? (1) = 1 e ? (?1) =?1;\n\n\u2022 A inclina\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o fica pr\u00f3xima da unidade na origem, ou seja,\n\n?\n?(0) = ab = 1,7159 \u00b72/3 = 1,1424.\n\n\u2022 A derivada segunda de ?(v) atinge seu valor m\u00e1ximo em v = 1.\n\n\n\n42 CAP\u00cdTULO 3. METODOLOGIA\n\n3.5.3 Processo de Aprendizagem\n\nO tipo de treinamento utilizado \u00e9 o bem conhecido algoritmo retropropaga\u00e7\u00e3o ou backpropagation,\nconforme descrito em 2.1.4. Para sua utiliza\u00e7\u00e3o, alguns par\u00e2metros inerentes do algoritmo pre-\ncisaram ser definidos. Algumas heur\u00edsticas tamb\u00e9m foram adotadas com o intuito de aumentar o\ndesempenho do processo de aprendizagem. As heur\u00edsticas s\u00e3o descritas na sequ\u00eancia:\n\n\u2022 Taxa de treinamento: ? = 0,05. Foi desenvolvida uma fun\u00e7\u00e3o que altera esse valor com o\nn\u00famero de itera\u00e7\u00f5es de treinamento. A ideia \u00e9 que, \u00e0 medida que o n\u00famero de altera\u00e7\u00f5es\navan\u00e7a, a taxa de crescimento assume valores cada vez menores, implementando assim uma\nheur\u00edstica apresentada em Haykin (2001);\n\n\u2022 N\u00famero m\u00e1ximo de itera\u00e7\u00f5es: 10.000. O n\u00famero m\u00e1ximo de itera\u00e7\u00f5es foi determinado\npara evitar o overfitting da rede. Por\u00e9m, o n\u00famero de itera\u00e7\u00f5es para cada arquiteura de\nrede foi determinado atrav\u00e9s do processo de valida\u00e7\u00e3o juntamente com os ajustes dos pesos\nsin\u00e1pticos;\n\n\u2022 Crit\u00e9rio de parada: erro do subconjunto de treinamento, erro subconjunto de valida\u00e7\u00e3o ou\nn\u00famero m\u00e1ximo de itera\u00e7\u00f5es. O erro adotado para o subconjunto de treinamento foi a energia\nm\u00e9dia do erro ?med(n), conforme definida por 2.12;\n\n\u2022 Constante de momento: ? = 10?7. Conforme apresentado na Se\u00e7\u00e3o 2.1.4, optou-se em\nutilizar uma pondera\u00e7\u00e3o do valor do peso sin\u00e1ptico da itera\u00e7\u00e3o anterior, compondo assim o\najuste dado pela regra delta generalizada;\n\n\u2022 Modo sequencial de aprendizagem. Os pesos sin\u00e1pticos s\u00e3o atualizados a cada passo de\npropaga\u00e7\u00e3o do algoritmo backpropagation, e n\u00e3o no final de cada \u00e9poca;\n\n\u2022 Normaliza\u00e7\u00e3o dos padr\u00f5es de entrada e sa\u00edda. Todos os valores de entrada e sa\u00edda de todos\nos casos foram escalonados entre [?1, 1]. Al\u00e9m disso, antes do escalonamento, foi feita uma\nremo\u00e7\u00e3o da m\u00e9dia de cada par\u00e2metro, tanto do vetor de entrada quanto do vetor de sa\u00edda;\n\n\u2022 Inicializa\u00e7\u00e3o dos pesos e bias (polarizador). Todos os pesos sin\u00e1pticos foram inicializados\ncom n\u00fameros rand\u00f4micos, mantendo o cuidado de n\u00e3o serem grandes suficientes para satu-\nrarem os neur\u00f4nios, nem pequenos a ponto de atrasarem o aprendizado. Os biases foram\ninicializados com valores nulos.\n\nO treinamento das tr\u00eas arquiteturas de rede foi feito utilizando os dois subconjuntos de dados: o\nde treinamento e o de valida\u00e7\u00e3o. O primeiro subconjunto \u00e9 o que atualiza os pesos sin\u00e1pticos a\npartir do algoritmo de retropropaga\u00e7\u00e3o ou backpropagation. O segundo \u00e9 utilizado para avaliar a\ngeneraliza\u00e7\u00e3o da rede, de forma a evitar o overfitting. Embora o objetivo do trabalho seja avaliar\n\n\n\n3.6. MLP E O MODELO UNIDIMENSIONAL 43\n\nos resultados gerados pelo arranjo das redes neurais com o modelo unidimensional, o processo de\ntreinamento das redes somente se deu pelos dados referentes \u00e0 curva K?p(x?). No entanto, o bom trei-\nnamento da rede \u00e9 imprescind\u00edvel para que os erros nos resultados finais possam ser minimizados.\nIsso ser\u00e1 explicado na pr\u00f3xima se\u00e7\u00e3o e observado no Cap\u00edtulo 4.\n\n3.6 MLP e o Modelo Unidimensional\n\nO processo de arranjo da rede neural com o modelo unidimensional baseia-se em utilizar a MLP\ntreinada como a fun\u00e7\u00e3o que fornece o valor da resistividade K?p(x?) no processo de resolu\u00e7\u00e3o das\nEqua\u00e7\u00f5es 3.4 e 3.6.\n\nProcurou-se resolver as equa\u00e7\u00f5es do modelo unidimensional utilizando m\u00e9todo num\u00e9rico de\nsolu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais. Segundo Cunha (2009), os m\u00e9todos de Runge-Kutta s\u00e3o os mais\nusados dentre aqueles apropriados para os problemas de valor inicial. Existem varia\u00e7\u00f5es do m\u00e9todo,\nmas de acordo com Cunha (2009), o m\u00e9todo de Runge-Kutta de quarta ordem \u00e9 mais usado por\nser uma combina\u00e7\u00e3o de simplicidade, alta precis\u00e3o e economia. Desta forma optou-se em usar o\nm\u00e9todo Runge-Kutta de quarta ordem.\n\n3.6.1 Resolu\u00e7\u00e3o do Modelo Unidimensional\n\nAs Equa\u00e7\u00f5es 3.4 e 3.6, juntamente com as condi\u00e7\u00f5es de contorno, n\u00e3o podem ser resolvidas usando\no m\u00e9todo de Runge-Kutta de forma direta, j\u00e1 que este m\u00e9todo aplica-se a problemas de valor inicial.\nSendo assim, foi preciso \"transformar\" esse sistema de equa\u00e7\u00f5es com condi\u00e7\u00f5es de contorno num\nsistema equivalente com condi\u00e7\u00f5es iniciais. O ded\u00e3o (toe) do po\u00e7o foi escolhido para ser o ponto\nsobre o qual as condi\u00e7\u00f5es iniciais s\u00e3o definidas. De acordo com as condi\u00e7\u00f5es definidas em 3.3, a\nvaz\u00e3o no toe (Qhw(xtoe) = Qtoe = 0) ser\u00e1 aplicada sem qualquer altera\u00e7\u00e3o. Para que a vari\u00e1vel p(x)\ntivesse uma condi\u00e7\u00e3o inicial, optou-se em usar um diferencial ?p para calcular o valor inicial da\npress\u00e3o no toe. Isto \u00e9:\n\np(xtoe) = ptoe = pres ??p. (3.29)\n\nInicialmente o valor ?p n\u00e3o \u00e9 conhecido; assim, \u00e9 adotado um valor tal que ptoe seja maior\ndo que pheel . Ap\u00f3s a divis\u00e3o do dom\u00ednio do po\u00e7o em intervalos iguais, aplica-se o m\u00e9todo de\nRunge-Kutta de quarta ordem, conforme descrito em Cunha (2009) e em Press et al. (2007). Por\nserem equa\u00e7\u00f5es acopladas, o m\u00e9todo foi aplicado para as duas equa\u00e7\u00f5es num esquema iterativo:\ncalculou-se primeiramente o fator m0 para a vaz\u00e3o Qhw e, em seguida, o fator m0 para a press\u00e3o\np; repetiu-se esse procedimento para c\u00e1lculo dos mi, i = 1, 2, 3 at\u00e9 o c\u00e1lculo de Q\n\nk+1\nhw e de p\n\nk+1,\nsendo k o ponto do intervalo no dom\u00ednio do po\u00e7o. Ao final do procedimento, existir\u00e3o k + 1 pontos\nde valores para Qhw e para p. O procedimento num\u00e9rico somente \u00e9 finalizado quando o valor pk+1,\n\n\n\n44 CAP\u00cdTULO 3. METODOLOGIA\n\ncorrespondente ao valor da press\u00e3o no heel, se aproximar, a menos de uma toler\u00e2ncia, do valor\npheel . Essa toler\u00e2ncia foi definida em 10?5. Caso a diferen\u00e7a entre tais valores esteja al\u00e9m da\ntoler\u00e2ncia, o ?p \u00e9 corrigido e aplica-se novamente o m\u00e9todo de Runge-Kutta, compondo assim um\nprocedimento iterativo.\n\nRessalta-se que o valor da resistividade que aparece na Equa\u00e7\u00e3o 3.4 \u00e9 fornecido pela MLP trei-\nnada. Embora a rede neural forne\u00e7a o valor adimensionalizado da resistividade (K?p), esse \u00e9 conver-\ntido facilmente no valor usual (Kp) a partir da Equa\u00e7\u00e3o 3.22, utilizando-se para isso os par\u00e2metros\nque comp\u00f4e a resistividade Khw. Esse procedimento \u00e9 realizado em cada passo do Runge-Kutta.\n\nA finaliza\u00e7\u00e3o deste processo iterativo resultar\u00e1 num conjunto de k + 1 pontos correspondentes\naos valores de Qhw(x), p(x),\n\ndQhw(x)\ndx e\n\nd p(x)\ndx , al\u00e9m do valor da vaz\u00e3o total Qhw. Esses conjuntos de\n\npontos poder\u00e3o ser esbo\u00e7ados em gr\u00e1ficos e comparados com as curvas originais provenientes do\nmodelo de elementos finitos, assim como o valor da vaz\u00e3o do po\u00e7o. A an\u00e1lise desses resultados a\npartir de uma norma de erro ser\u00e1 descrito na Subse\u00e7\u00e3o 3.6.2.\n\n3.6.2 An\u00e1lise dos Resultados: Medida de Erros\n\nOs resultados obtidos com o arranjo MLP-Modelo1D devem ser comparados com os resultados\noriginais do modelo de elementos finitos perante algum crit\u00e9rio ou medida de erro. Isso permitir\u00e1\navaliar a utiliza\u00e7\u00e3o do arranjo em substitui\u00e7\u00e3o ao modelo tridimensional, motiva\u00e7\u00e3o e objetivo do\ntrabalho. Assim, adotou-se como medida de erro a norma L2 para avaliar as curvas Qhw(x), p(x),\ndQhw(x)\n\ndx e\nd p(x)\n\ndx , que \u00e9 uma norma muito comum em simula\u00e7\u00f5es de elementos finitos (Becker et al.,\n1981). Para a vaz\u00e3o total do po\u00e7o, Qhw, optou-se em usar a noma L1, por\u00e9m de forma alterada, a\nfim de que possa ser expressa a partir de um percentual.\n\nA norma L2 para uma fun\u00e7\u00e3o f (x) cont\u00ednua para um intervalo [a, b] \u00e9 definida, de acordo com\nKreyszig (1978):\n\n|| f (x)||L2 =\n\n????? b?\na\n\n[ f (x)]2 dx. (3.30)\n\nAssim, para avaliar uma medida de erro entre as curvas, usou-se:\n\nE f =\n\n????? lhw?\n0\n\n[\nf (x)? f? (x)\n\n]2\ndx (3.31)\n\nem que f (x) representa as curvas do modelo de elementos finitos, f? (x) as curvas do arranjo MLP-\nmodelo1D e lhw \u00e9 o comprimento do po\u00e7o horizontal. Dessa forma, contabiliza-se os erros para as\n\n\n\n3.6. MLP E O MODELO UNIDIMENSIONAL 45\n\nquatro curvas em quest\u00e3o: EQ, EdQdx, Ep e Ed pdx.\n\nPara a vaz\u00e3o total do po\u00e7o, usou-se a norma L1, escrita como:\n\nE L\n1\n\nQhw = |Qhw ?Q?hw| (3.32)\n\nem que Qhw \u00e9 a vaz\u00e3o original do modelo de elementos finitos e Q?hw do arranjo MLP-modelo1D.\nPor\u00e9m, por se tratar de um n\u00famero representativo em termos da an\u00e1lise de produtividade do po\u00e7o,\noptou-se em utilizar uma norma L1 alterada. Dessa forma, tem-se uma medida de erro relativo,\ndado como:\n\nEQhw =\n|Qhw ?Q?hw|\n\nQhw\n. (3.33)\n\nA Equa\u00e7\u00e3o 3.33 tamb\u00e9m pode ser expressa de forma percentual.\n\nAs normas de erros representadas pelas Equa\u00e7\u00f5es 3.31 e 3.33 foram aplicadas para todos os\ncasos dos tr\u00eas subconjuntos (treinamento, valida\u00e7\u00e3o e teste), para cada uma das tr\u00eas arquiteturas.\nPara analisar o comportamento de cada subconjunto, optou-se em fazer a m\u00e9dia aritm\u00e9tica das\nnormas dos erros para cada um. Assim, \u00e9 poss\u00edvel verificar qual arquitetura produz melhores\nresultados (menores erros), principalmente para os casos do subconjunto de teste Ate, pelo qual\nestaria avaliando a qualidade de generaliza\u00e7\u00e3o das MLPs. Optou-se em utilizar, juntamente com a\nm\u00e9dia, o desvio padr\u00e3o amostral ? (Bussab e Morettin, 1997) como forma de avaliar a variabilidade\ndos valores dos erros de cada subconjunto, em cada uma das arquiteturas.\n\n\n\n46 CAP\u00cdTULO 3. METODOLOGIA\n\n\n\nCap\u00edtulo 4\n\nResultados\n\nOs resultados foram obtidos de acordo com a metodologia apresentada no Cap\u00edtulo 3. Esses se\ndividem em tr\u00eas partes: a primeira apresenta a biblioteca escrita em C++ para desenvolvimento\nde redes neurais artificiais, NeuralLib; a segunda parte apresenta os resultados de treinamento da\nrede neural, juntamente com algumas an\u00e1lises acerca do processo de aprendizagem; a terceira e\n\u00faltima parte apresenta a compara\u00e7\u00e3o dos resultados obtidos com o modelo unidimensional frente\naos padr\u00f5es do modelo tridimensional.\n\n4.1 NeuralLib\n\nUm dos objetivos do trabalho era o desenvolvimento de uma biblioteca que permitisse compor re-\ndes neurais artificiais a fim de serem facilmente aplicadas na mec\u00e2nica computacional e simula\u00e7\u00e3o\nn\u00famerica. O desenvolvimento em C++ foi escolhido devido \u00e0s caracter\u00edsticas desej\u00e1veis de tal\nlinguagem em simula\u00e7\u00e3o e an\u00e1lise num\u00e9rica, tais como orienta\u00e7\u00e3o a objetos, heran\u00e7a e polimor-\nfismo, concep\u00e7\u00e3o em templates, entre outros (Lippman e Lajoie, 1998). O uso de templates nesse\ncaso foi necess\u00e1rio pois, assim, \u00e9 poss\u00edvel estender a biblioteca para diversos tipos de neur\u00f4nios,\nfun\u00e7\u00f5es de ativa\u00e7\u00e3o, arquiteturas de rede e mesmo padr\u00f5es de aprendizagem, compondo assim de\nfato a caracteriza\u00e7\u00e3o de uma biblioteca. Al\u00e9m do mais, a concep\u00e7\u00e3o do c\u00f3digo permite facilidade\npara suporte e implementa\u00e7\u00e3o de funcionalidades diversas, tais como an\u00e1lise da evolu\u00e7\u00e3o do erro\nde treinamento, an\u00e1lise de problemas com satura\u00e7\u00e3o dos neur\u00f4nios etc. Os detalhes de como as\nclasses template foram desenvolvidas e arranjadas est\u00e3o no Ap\u00eandice A.\n\nAlguns testes foram realizados para analisar e validar o c\u00f3digo implementado. Al\u00e9m do mais,\nalguns testes serviram como base para entender o funcionamento e comportamento das redes neu-\nrais artificiais. Abaixo, seguem algumas fun\u00e7\u00f5es utilizadas como forma de verificar a potencia-\nlidade de generaliza\u00e7\u00e3o das MLPs. As equa\u00e7\u00f5es originais e os gr\u00e1ficos por estas produzidas s\u00e3o\n\n47\n\n\n\n48 CAP\u00cdTULO 4. RESULTADOS\n\napresentados. Para treinamento das MLPs, foram gerados somente conjuntos de treinamento.\n\n\u2022 Fun\u00e7\u00e3o Peaks:\n\nEssa fun\u00e7\u00e3o \u00e9 definida pela fun\u00e7\u00e3o 4.1:\n\nf (x, y) = 3(1?x)2 e?x\n2?(y+1)2 ?10\n\n(x\n5\n?x3 ?y5\n\n)\ne?x\n\n2?y2 ?\n1\n3\n\ne?(x+1)\n2?y2. (4.1)\n\nO gr\u00e1fico dessa fun\u00e7\u00e3o \u00e9 apresentado pela Figura 4.1 :\n\nFigura 4.1: Fun\u00e7\u00e3o Peaks.\n\nA Figura 4.2 abaixo apresenta o gr\u00e1fico da fun\u00e7\u00e3o original (em vermelho) e o gr\u00e1fico produzido\npor uma MLP gerada pela NeuralLib (em verde):\n\nFigura 4.2: Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP.\n\n\n\n4.1. NEURALLIB 49\n\nObserva-se que os dois gr\u00e1ficos confundem-se. Para ter uma medida da aproxima\u00e7\u00e3o, usou-se\na norma L2 da diferen\u00e7a Epeak entre as duas superf\u00edcies dividida pela norma L2 da fun\u00e7\u00e3o original,\n|| f peak||L2 , sendo que Epeak \u00e9 obtida de forma semelhante \u00e0 3.31. Assim, obteve-se:\n\nE?peak =\nEpeak\n|| f peak||L2\n\n=\n0,2605\n11,6448\n\n= 0,0224.\n\nO valor da norma E?peak indica boa aproxima\u00e7\u00e3o da fun\u00e7\u00e3o original.\n\n\u2022 Fun\u00e7\u00e3o descont\u00ednua:\n\nA fun\u00e7\u00e3o descont\u00ednua utilizada apresenta a seguinte express\u00e3o, definida pela fun\u00e7\u00e3o 4.2:\n\nf (x, y) =\n\n?????????\n????????\n\nx2 + y2 ?25 se x&lt;?7\n\n?2 sin x? xy\n2\n\n10 + 15 se x&lt;?3\n\n0,5x2 + 20+ | y | se x &lt;0\n\n0,3\n?\n\nx + 25+ | y | se x ? 0\n\n(4.2)\n\ncom ?7,5 ? x ? 3,0 e ?3,0 ? y ? 3,0.\nO gr\u00e1fico dessa fun\u00e7\u00e3o \u00e9 apresentado pela Figura 4.3:\n\nFigura 4.3: Fun\u00e7\u00e3o descont\u00ednua.\n\n\n\n50 CAP\u00cdTULO 4. RESULTADOS\n\nA Figura 4.4 abaixo apresenta o gr\u00e1fico da fun\u00e7\u00e3o original (em vermelho) e o gr\u00e1fico produzido\npor uma MLP gerada pela NeuralLib (em verde):\n\nFigura 4.4: Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP.\n\nObserva-se que, embora a fun\u00e7\u00e3o original seja descont\u00ednua, a MLP conseguiu reproduzir os\ntrechos cont\u00ednuos com boa acur\u00e1cia. As descontinuidades foram mapeadas atrav\u00e9s de trechos com\ngradiente alto, o que \u00e9 plaus\u00edvel, j\u00e1 que as fun\u00e7\u00f5es de ativa\u00e7\u00e3o dos neur\u00f4nios s\u00e3o todas cont\u00ednuas.\nPara analisar a aproxima\u00e7\u00e3o, calculou-se a norma L2 da diferen\u00e7a EnSmooth entre as duas superf\u00edcies\ndividida pela norma L2 da fun\u00e7\u00e3o original, || fnSmooth||L2 , calculada em cada trecho onde ela \u00e9\ncont\u00ednua. Assim, obteve-se:\n\nE?nSmooth =\nEnSmooth\n|| fnSmooth||L2\n\n=\n8,1925\n\n189,6232\n= 0,0432.\n\nO valor da norma E?nSmooth indica que, mesmo sendo uma aproxima\u00e7\u00e3o cont\u00ednua de uma fun\u00e7\u00e3o\ndescont\u00ednua, o valor do erro em L2 para esse caso n\u00e3o ultrapassa 5% da norma da fun\u00e7\u00e3o original.\n\n\u2022 Fun\u00e7\u00e3o Rastringin:\n\nEssa fun\u00e7\u00e3o \u00e9 definida pela fun\u00e7\u00e3o 4.3:\n\nf (x, y) = 20 +\n(\nx2 + y2\n\n)\n?10(cos 2? x + cos 2? y) (4.3)\n\ncom ?5,0 ? x, y ? 5,0.\n\n\n\n4.1. NEURALLIB 51\n\nO gr\u00e1fico dessa fun\u00e7\u00e3o \u00e9 apresentado pela Figura 4.5:\n\nFigura 4.5: Fun\u00e7\u00e3o Rastringin.\n\nA Figura 4.6 abaixo apresenta o gr\u00e1fico da fun\u00e7\u00e3o original (em vermelho) e o gr\u00e1fico produzido\npor uma MLP gerada pela NeuralLib (em verde):\n\nFigura 4.6: Compara\u00e7\u00e3o entre os gr\u00e1fico original e o gerado pela MLP.\n\n\n\n52 CAP\u00cdTULO 4. RESULTADOS\n\nObserva-se a MLP consegue mapear globalmente a fun\u00e7\u00e3o original, isto \u00e9, preserva os limites\nm\u00e1ximos e m\u00ednimos da fun\u00e7\u00e3o Rastringin. Nem todas as oscila\u00e7\u00f5es s\u00e3o captadas pela MLP, por\u00e9m\na generaliza\u00e7\u00e3o global apresenta bom comportamento. Ressalta-se que nesse caso as fun\u00e7\u00f5es de\nativa\u00e7\u00e3o dos neur\u00f4nios foram definidas atrav\u00e9s de fun\u00e7\u00f5es senoidais. Isso foi feito com objetivo\nde aumentar a percep\u00e7\u00e3o da MLP frente \u00e0s oscila\u00e7\u00f5es da fun\u00e7\u00e3o original. O valor da norma L2 da\ndiferen\u00e7a entre as superf\u00edcies normalizada pela norma da superf\u00edcie original \u00e9:\n\nE?Rast =\nERast\n|| fRast||L2\n\n=\n131,0932\n396,4206\n\n= 0,3307.\n\nPercebe-se que o valor da norma da diferen\u00e7a representa cerca de 30% da norma da fun\u00e7\u00e3o\noriginal. Embora o erro seja razoavelmente grande, o mapeamento realizado pelas redes neurais\ncapta globalmente a topologia da fun\u00e7\u00e3o Rastringin.\n\n4.2 Aprendizagem da Rede Neural\n\nEssa se\u00e7\u00e3o descreve os resultados do treinamento das MLPs. Para cada uma das arquiteturas defi-\nnidas foram esbo\u00e7ados gr\u00e1ficos das curvas de evolu\u00e7\u00e3o das energias m\u00e9dias do erro, tanto para o\nsubconjunto AT quanto para o Av. Foram tamb\u00e9m compostas tabelas contendo as energias m\u00e9dias\ndo erro ao final do treinamento para os tr\u00eas subconjuntos, AT , Av e Ate. Juntamente com as ener-\ngias m\u00e9dias dos subconjuntos, foram acrescentados valores dos desvios padr\u00e3o. O uso do desvio\npadr\u00e3o amostral ? tem o intuito de avaliar a dispers\u00e3o do valor da energia total do erro ? (k) entre\nos k-\u00e9simos casos presentes nos tr\u00eas subconjuntos. Al\u00e9m do mais, servir\u00e1 como um indicativo na\ncompara\u00e7\u00e3o entre as tr\u00eas arquiteturas utilizadas. Para comparar os resultados de treinamento entre\nas arquiteturas foi composto um gr\u00e1fico com as energias totais de cada caso do conjunto A.\n\n\n\n4.2. APRENDIZAGEM DA REDE NEURAL 53\n\n4.2.1 Arquitetura 1: MLP-5\n\nAs Figuras 4.7 e 4.8 apresentam os gr\u00e1ficos de evolu\u00e7\u00e3o da energia m\u00e9dia do erro ?med(n) para os\nsubconjuntos AT e Av em fun\u00e7\u00e3o das n-\u00e9simas itera\u00e7\u00f5es ou \u00e9pocas. Ressalta-se que os valores das\nenergias foram esbo\u00e7adas a cada 50 \u00e9pocas, a fim de facilitar a visualiza\u00e7\u00e3o e an\u00e1lise.\n\n0 2000 4000 6000 8000 10 000\n0.00\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Treinamento\n\nM\nL\n\nP\n-\n\n5\n\nFigura 4.7: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas.\n\n0 2000 4000 6000 8000 10 000\n0\n\n2\n\n4\n\n6\n\n8\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Valida\u00e7\u00e3o\nM\n\nL\nP\n\n-\n5\n\nFigura 4.8: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas.\n\n\n\n54 CAP\u00cdTULO 4. RESULTADOS\n\nObserva-se que o valor m\u00e9dio da energia do erro ?med(n) do subconjunto AT assumiu valores\ncada vez menores ao longo das \u00e9pocas. Por\u00e9m, \u00e9 not\u00e1vel que a taxa de decaimento foi relativa-\nmente alta no \u00ednicio do treinamento, em oposi\u00e7\u00e3o ao final, no qual observa-se um decaimento bem\nmais lento. O mesmo pode-se dizer do valor m\u00e9dio da energia do erro do subconjunto Av, o qual\napresenta ligeira oscila\u00e7\u00e3o nas primeiras \u00e9pocas, estabilizando-se em decaimento nas \u00faltimas. Con-\ntudo, n\u00e3o h\u00e1 aumento do valor de ?med(n) em Av, o que poderia indicar overfitting. Assim sendo,\nconclui-se que o treinamento pode estar adequado para os casos de AT e Av. Por\u00e9m, para analisar o\ncomportamento real da MLP quanto \u00e0 generaliza\u00e7\u00e3o, os casos de Ate foram testados ap\u00f3s o treina-\nmento. A Tabela 4.1 apresenta os valores das energias m\u00e9dias do erro ?med e os desvios padr\u00e3o ?\nao final do treinamento para os tr\u00eas subconjuntos AT , Av e Ate:\n\nSubconjunto AT Av Ate\n?med 0,00064 0,01325 0,00970\n\n? 0,00059 0,03185 0,01364\n\nTabela 4.1: Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate.\n\nNa Tabela 4.1, \u00e9 poss\u00edvel observar que o subconjunto de teste Ate possui valor da energia m\u00e9dia\ndo erro menor em rela\u00e7\u00e3o ao subconjunto de valida\u00e7\u00e3o Av. O valor do desvio padr\u00e3o ? de Ate\ntamb\u00e9m \u00e9 menor em rela\u00e7\u00e3o a Av. Isso indica que a MLP apresenta um bom comportamento com\nrela\u00e7\u00e3o a generaliza\u00e7\u00e3o dos casos.\n\n\n\n4.2. APRENDIZAGEM DA REDE NEURAL 55\n\n4.2.2 Arquitetura 2: MLP-10\n\nAs Figuras 4.9 e 4.10 apresentam os gr\u00e1ficos de evolu\u00e7\u00e3o da energia m\u00e9dia do erro ?med(n) para os\nsubconjuntos AT e Av em fun\u00e7\u00e3o das n-\u00e9simas itera\u00e7\u00f5es ou \u00e9pocas.\n\n0 2000 4000 6000 8000 10 000\n0.00\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Treinamento\n\nM\nL\n\nP\n-\n\n10\n\nFigura 4.9: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas.\n\n0 2000 4000 6000 8000 10 000\n0\n\n2\n\n4\n\n6\n\n8\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Valida\u00e7\u00e3o\nM\n\nL\nP\n\n-\n10\n\nFigura 4.10: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas.\n\n\n\n56 CAP\u00cdTULO 4. RESULTADOS\n\nNota-se que as energias m\u00e9dias ?med(n) dos subconjuntos AT e Av mantiveram um comporta-\nmento semelhante \u00e0 MLP-5: ambas curvas deca\u00edram com n\u00famero de \u00e9pocas e o subconjunto Av\ntamb\u00e9m apresentou oscila\u00e7\u00e3o nas primeiras itera\u00e7\u00f5es. Da mesma forma que observado em 4.2.1,\nn\u00e3o h\u00e1 evid\u00eancias da ocorr\u00eancia de overfitting no processo de treinamento, j\u00e1 que a energia m\u00e9dia\nde Av decaiu ao longo das \u00e9pocas. Desta forma, conclui-se que o treinamento pode estar adequado\npara os casos de AT e Av, sendo necess\u00e1rio por\u00e9m testar a MLP para os casos de Ate. A Tabela 4.2\napresenta os valores das energias m\u00e9dias do erro ?med e os desvios padr\u00e3o ? ao final do treinamento\npara os tr\u00eas subconjuntos AT , Av e Ate.\n\nSubconjunto AT Av Ate\n?med 0,00052 0,01521 0,01209\n\n? 0,00037 0,03630 0,02010\n\nTabela 4.2: Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate.\n\nNa Tabela 4.2, observa-se que o subconjunto de teste Ate possui valor da energia m\u00e9dia do\nerro menor em rela\u00e7\u00e3o ao subconjunto de valida\u00e7\u00e3o Av. O valor do desvio padr\u00e3o ? de Ate tam-\nb\u00e9m \u00e9 menor em rela\u00e7\u00e3o a Av, \u00e0 semelhan\u00e7a da MLP-5, indicando que a MLP apresenta um bom\ncomportamento com rela\u00e7\u00e3o \u00e0 generaliza\u00e7\u00e3o.\n\n\n\n4.2. APRENDIZAGEM DA REDE NEURAL 57\n\n4.2.3 Arquitetura 3: MLP-15\n\nAs Figuras 4.11 e 4.12 apresentam os gr\u00e1ficos de evolu\u00e7\u00e3o da energia m\u00e9dia do erro ?med(n) para\nos subconjuntos AT e Av em fun\u00e7\u00e3o das n-\u00e9simas itera\u00e7\u00f5es ou \u00e9pocas.\n\n0 2000 4000 6000 8000 10 000\n0.00\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Treinamento\n\nM\nL\n\nP\n-\n\n15\n\nFigura 4.11: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto AT ao longo das \u00e9pocas.\n\n0 2000 4000 6000 8000 10 000\n0\n\n2\n\n4\n\n6\n\n8\n\n\u00c9pocas\n\nE\nne\n\nrg\nia\n\nM\n\u00e9d\n\nia\ndo\n\nE\nrr\n\no\n\nEvolu\u00e7\u00e3o da Energia M\u00e9dia do Erro - Conjunto de Valida\u00e7\u00e3o\nM\n\nL\nP\n\n-\n15\n\nFigura 4.12: Evolu\u00e7\u00e3o da energia m\u00e9dia do erro do subconjunto Av ao longo das \u00e9pocas.\n\n\n\n58 CAP\u00cdTULO 4. RESULTADOS\n\nAs energias m\u00e9dias ?med(n) dos subconjuntos AT e Av, como as Figuras 4.11 e 4.12 apresentam,\nmantiveram um comportamento semelhante ao obtido nos treinamentos da MLP-5 e da MLP-10:\nambas curvas deca\u00edram com n\u00famero de \u00e9pocas e ainda ocorreu oscila\u00e7\u00e3o (nas primeiras \u00e9pocas) na\ncurva do subconjunto Av . Da mesma forma que observado em 4.2.1 e em 4.2.2, n\u00e3o h\u00e1 evid\u00eancias\nda ocorr\u00eancia de overfitting no processo de treinamento, j\u00e1 que a energia m\u00e9dia de Av decaiu ao\nlongo do processo de aprendizagem. Conclui-se que o treinamento est\u00e1 adequado para os casos de\nAT e Av, sendo necess\u00e1rio contudo testar a MLP para os casos de Ate a fim de verificar a genera-\nliza\u00e7\u00e3o da rede. A Tabela 4.3 apresenta os valores das energias m\u00e9dias do erro ?med e os desvios\npadr\u00e3o ? ao final do processo de treinamento para os tr\u00eas subconjuntos AT , Av e Ate.\n\nSubconjunto AT Av Ate\n?med 0,00056 0,01789 0,01175\n\n? 0,00040 0,04643 0,01534\n\nTabela 4.3: Energia m\u00e9dia do erro para os subconjuntos AT , Av e Ate.\n\nObserva-se na Tabela 4.3 que o subconjunto de teste Ate possui valor da energia m\u00e9dia do erro\nmenor em rela\u00e7\u00e3o ao subconjunto de valida\u00e7\u00e3o Av. O valor do desvio padr\u00e3o ? de Ate tamb\u00e9m\n\u00e9 menor em rela\u00e7\u00e3o a Av, \u00e0 semelhan\u00e7a do observado nos treinamentos da MLP-5 e da MLP-10.\nAssim, esses resultados indicam que a MLP apresenta comportamento adequado com rela\u00e7\u00e3o \u00e0\ngeneraliza\u00e7\u00e3o dos casos. Na Se\u00e7\u00e3o 4.2.4, s\u00e3o apresentados os valores da energia total ? (k) para os\nk-\u00e9simos casos de A, para as tr\u00eas MLPs, permitindo a visualiza\u00e7\u00e3o e compara\u00e7\u00e3o dos resultados de\ntreinamento de cada rede neural.\n\n\n\n4.2. APRENDIZAGEM DA REDE NEURAL 59\n\n4.2.4 Compara\u00e7\u00e3o\n\nA fim de observar e comparar os resultados obtidos no processo de treinamento de cada arquitetura,\ngerou-se um gr\u00e1fico contendo todos os valores da energia total do erro ? (k) para cada k-\u00e9simo caso\ndo conjunto de treinamento A, para as tr\u00eas MLPs. Como os casos de A s\u00e3o numerados, facilmente\nse distinguem no gr\u00e1fico quais casos pertencem a cada um dos subconjuntos AT , Av e Ate, conforme\ndefinidos em 3.5.1.\n\n0?\n\n0.05?\n\n0.1?\n\n0.15?\n\n0.2?\n\n0.25?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nE\nne\n\nrg\nia\n\n T\not\n\nal\n  d\n\no \nE\n\nrr\no \n\nCasos \n\nCompara\u00e7\u00e3o das Energias Totais do Erro \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.13: Compara\u00e7\u00e3o das energias totais dos erros do conjunto A para as tr\u00eas MLPs.\n\nPode-se observar que, para os 50 primeiros casos, correspondentes ao subconjunto AT , as ener-\ngias totais para as tr\u00eas MLPs mantiveram-se relativamente pr\u00f3ximas e em valores razoavelmente\nbaixos, conforme observado nas Tabelas 4.1, 4.2 e 4.3. Para os 25 casos seguintes, correspondentes\nao subconjunto Av, observa-se maior oscila\u00e7\u00e3o entre os valores da energia, sendo a MLP-5 a que\napresentou menor oscila\u00e7\u00e3o, em oposi\u00e7\u00e3o a MLP-15, que oscilou com maior amplitude. Para os\n\u00faltimos 25 casos, referentes ao subconjunto Ate, a MLP-10 apresentou maiores amplitudes do erro\nem alguns casos, enquanto que a MLP-15 gerou maiores erros em outros. A MLP-5, para esse con-\njunto, manteve-se com menor valor da energia total do erro em rela\u00e7\u00e3o \u00e0s outras duas. Ressalta-se\ntamb\u00e9m que as distribui\u00e7\u00f5es dos erros totais das tr\u00eas MLPs tenderam \u00e0 mesma varia\u00e7\u00e3o, por\u00e9m\ncom amplitudes diferentes. Para quantificar essas diferen\u00e7as observadas, elencou-se em tabelas os\n\n\n\n60 CAP\u00cdTULO 4. RESULTADOS\n\nvalores da energia m\u00e9dia e do desvio padr\u00e3o para cada um dos conjuntos, para as tr\u00eas redes neurais.\nAs Tabelas 4.4, 4.5 e 4.6 apresentam os resultados para os tr\u00eas subconjuntos AT , Av e Ate.\n\nAT\nArquiteturas MLP-5 MLP-10 MLP-15\n\n?med 0,00064 0,00052 0,00056\n? 0,00059 0,00037 0,00040\n\nTabela 4.4: Energia m\u00e9dia do erro para o subconjunto AT .\n\nAv\nArquiteturas MLP-5 MLP-10 MLP-15\n\n?med 0,01325 0,01521 0,01789\n? 0,03185 0,03630 0,04643\n\nTabela 4.5: Energia m\u00e9dia do erro para o subconjunto Av.\n\nAte\nArquiteturas MLP-5 MLP-10 MLP-15\n\n?med 0,00970 0,01209 0,01175\n? 0,01364 0,02010 0,01534\n\nTabela 4.6: Energia m\u00e9dia do erro para o subconjunto Ate.\n\nObserva-se na Tabela 4.4 que as arquiteturas MLP-10 e MLP-15 apresentam menores valores de\nenergia m\u00e9dia do erro. Por essa observa\u00e7\u00e3o, pode-se afirmar que elas possuem melhor aproxima\u00e7\u00e3o\nda fun\u00e7\u00e3o original. Por\u00e9m, observa-se atrav\u00e9s das Tabelas 4.5 e 4.6, que a MLP-5 \u00e9 a que apresenta\nmenor valor tanto para a energia m\u00e9dia quanto para o desvio padr\u00e3o nos casos de Av e Ate, refutando\na afirma\u00e7\u00e3o anterior. Isso refor\u00e7a a conclus\u00e3o obtida com a observa\u00e7\u00e3o do gr\u00e1fico da Figura 4.13.\nAssim, a rede neural que aprensenta apresenta melhor desempenho com rela\u00e7\u00e3o \u00e0 generaliza\u00e7\u00e3o \u00e9\na MLP-5. Essa conclus\u00e3o pode estar relacionada aos efeitos observados do superdimensionamento,\ndescritos em Braga et al. (2007). Com o aumento do n\u00famero de neur\u00f4nios, o percentual de solu\u00e7\u00f5es\nboas \u00e9 cada vez menor em rela\u00e7\u00e3o ao n\u00famero de todas as outras solu\u00e7\u00f5es poss\u00edveis. Isso pode fazer\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 61\n\ncom que o algoritmo de aprendizado, em sua busca estoc\u00e1stica pela solu\u00e7\u00e3o, escolha qualquer\numa delas como satisfat\u00f3rias segundo o crit\u00e9rio de minimiza\u00e7\u00e3o de erro (Braga et al., 2007), sem,\ncontudo, satisfazer necessariamente uma melhor generaliza\u00e7\u00e3o para os casos de valida\u00e7\u00e3o e de\nteste. Assim, a MLP-5, pelos dados do conjunto A, parece fornecer uma solu\u00e7\u00e3o melhor do que as\noutras duas arquiteturas, sob essa \u00f3tica do superdimensionamento.\n\n4.3 Redes Neurais e o Modelo Unidimensional\n\nEsta se\u00e7\u00e3o tem o objetivo de apresentar os resultados do arranjo das redes neurais com o modelo\nunidimensional do acoplamento po\u00e7o-reservat\u00f3rio. S\u00e3o apresentados os resultados obtidos com o\narranjo das MLPs treinadas com o modelo e os erros em norma L2 para as curvas de fluxo Qhw(x),\np(x), dQhw(x)dx e\n\nd p(x)\ndx . Tamb\u00e9m s\u00e3o apresentados os erros das vaz\u00f5es finais obtidas em cada arranjo.\n\nAo final, uma compara\u00e7\u00e3o entre as tr\u00eas arquiteturas \u00e9 realizada a fim de avaliar o comportamento\ndelas quando aplicadas na resolu\u00e7\u00e3o das equa\u00e7\u00f5es do modelo simplificado.\n\n4.3.1 MLP-5 e Modelo Unidimensional\n\nAs Tabelas 4.7, 4.8, 4.9 e 4.10 apresentam os valores m\u00e9dios e os desvios padr\u00e3o dos erros em\nL2 para os tr\u00eas subconjuntos AT , Av e Ate, para cada uma das curvas correspondentes ao fluxo\nno interior do po\u00e7o. Ressalta-se que os valores dos erros e desvios padr\u00e3o foram normalizados a\npartir de um valor m\u00e9dio da norma L2 das curvas considerando todos os casos do conjunto A. Essa\nnormalia\u00e7\u00e3o foi realizada com objetivo de adimensionalizar os valores dos erros e desvios padr\u00e3o.\n\nL2 ?Qhw(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00729 0,01684 0,01459\n? 0,00824 0,02432 0,01733\n\nTabela 4.7: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x).\n\nL2 ? p(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 7,24E ?6 8,19E ?6 7,68E ?6\n? 3,77E ?6 3,63E ?6 4,17E ?6\n\nTabela 4.8: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x).\n\n\n\n62 CAP\u00cdTULO 4. RESULTADOS\n\nL2 ?dQhw(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,02222 0,06796 0,02353\n? 0,02745 0,12939 0,02745\n\nTabela 4.9: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx.\n\nL2 ?d p(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00181 0,01595 0,01878\n? 0,00393 0,01859 0,02200\n\nTabela 4.10: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx.\n\nPode-se observar, pelas Tabelas 4.7 e 4.9, que os valores m\u00e9dios do erro em norma L2 e do\ndesvio padr\u00e3o foram menores para o subconjunto Ate em rela\u00e7\u00e3o ao subconjunto Av. Na Tabela 4.8,\nobserva-se que o valor m\u00e9dio do erro em L2 \u00e9 menor para Ate, por\u00e9m o desvio padr\u00e3o \u00e9 maior em\nrela\u00e7\u00e3o ao subconjunto Av. Pela Tabela 4.10, \u00e9 poss\u00edvel notar que o valor m\u00e9dio do erro e do desvio\npadr\u00e3o s\u00e3o menores para o conjunto de valida\u00e7\u00e3o, Av. Esses resultados s\u00e3o semelhantes aos obtidos\nno treinamento da MLP-5 (Subse\u00e7\u00e3o 4.2.1), a menos da distribui\u00e7\u00e3o do erro da curva d p(x)dx . Uma\nexplica\u00e7\u00e3o para essa diferen\u00e7a \u00e9 o fato de que as medidas dos erros s\u00e3o diferentes entre as usadas\npara an\u00e1lise do desempenho das MLPs e as usadas nas curvas de fluxo.\n\nA Tabela 4.11 apresenta a m\u00e9dia do erro em norma L1 alterada da vaz\u00e3o total Qhw e o desvio\npadr\u00e3o para os subconjuntos AT , Av e Ate.\n\nL1 ?Qhw\nSubconjunto AT Av Ate\n\nL1 me?dia 0,615 1,362 1,560\n? 0,338 0,828 1,581\n\nTabela 4.11: Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw.\n\nObserva-se que os maiores valores do erro e do desvio padr\u00e3o ocorrem para o subconjunto Ate.\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 63\n\n4.3.2 MLP-10 e Modelo Unidimensional\n\nAs Tabelas 4.12, 4.13, 4.14 e 4.15 apresentam os valores m\u00e9dios e os desvios padr\u00e3o dos erros em\nL2 para os tr\u00eas subconjuntos AT , Av e Ate, para todas as curvas de fluxo do po\u00e7o. Destaca-se que os\nvalores foram normalizados, assim como realizado na Subse\u00e7\u00e3o 4.3.1.\n\nL2 ?Qhw(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00729 0,01449 0,01373\n? 0,00824 0,01980 0,01343\n\nTabela 4.12: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x).\n\nL2 ? p(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 7,14E ?6 8,19E ?6 7,43E ?6\n? 3,77E ?6 3,67E ?6 4,25E ?6\n\nTabela 4.13: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x).\n\nL2 ?dQhw(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,02222 0,07058 0,03137\n? 0,02745 0,12286 0,03398\n\nTabela 4.14: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx.\n\nL2 ?d p(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00181 0,01428 0,01949\n? 0,00393 0,01508 0,02270\n\nTabela 4.15: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx.\n\n\n\n64 CAP\u00cdTULO 4. RESULTADOS\n\nObserva-se nas Tabelas 4.12 e 4.14 que os valores m\u00e9dios do erro em L2 e do desvio padr\u00e3o\nforam menores para o subconjunto Ate em rela\u00e7\u00e3o ao subconjunto Av, \u00e0 semelhan\u00e7a do que fora\nobservado na Subse\u00e7\u00e3o 4.3.1. O valor m\u00e9dio do erro em L2 para curva p(x) \u00e9 menor para o sub-\nconjunto Ate, por\u00e9m o desvio padr\u00e3o \u00e9 maior em rela\u00e7\u00e3o ao subconjunto Av, conforme observado\nna Tabela 4.13. Na Tabela 4.15, observa-se que os valores m\u00e9dios de erro em L2 e do desvio\npadr\u00e3o para a curva d p(x)dx s\u00e3o menores para o conjunto de valida\u00e7\u00e3o, Av. Essas observa\u00e7\u00f5es s\u00e3o\nsemelhantes aos resultados apresentados na Subse\u00e7\u00e3o 4.3.1.\n\nA Tabela 4.16 apresenta a m\u00e9dia do erro em norma L1 alterada da vaz\u00e3o total Qhw e o desvio\npadr\u00e3o para os tr\u00eas subconjuntos, AT , Av e Ate.\n\nL1 ?Qhw\nSubconjunto AT Av Ate\n\nL1 me?dia 0,615 1,191 1,689\n? 0,338 0,980 1,481\n\nTabela 4.16: Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw.\n\nNota-se, pela Tabela 4.16, que os maiores valores m\u00e9dios do erro e do desvio padr\u00e3o ocorrem\npara o subconjunto Ate.\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 65\n\n4.3.3 MLP-15 e Modelo Unidimensional\n\nAs Tabelas 4.17, 4.18, 4.19 e 4.20 apresentam os valores m\u00e9dios e os desvios padr\u00e3o dos erros em\nL2 para os tr\u00eas subconjuntos AT , Av e Ate, para as curvas correspondentes ao fluxo no interior do\npo\u00e7o. Os valores foram normalizados, conforme realizado nas subse\u00e7\u00f5es anteriores.\n\nL2 ?Qhw(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00729 0,01624 0,01670\n? 0,00824 0,01903 0,01728\n\nTabela 4.17: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva Qhw(x).\n\nL2 ? p(x)\nSubconjunto AT Av Ate\n\nL2 me?dia 7,14E ?6 8,10E ?6 7,59E ?6\n? 3,77E ?6 3,47E ?6 4,17E ?6\n\nTabela 4.18: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva p(x).\n\nL2 ?dQhw(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,02222 0,07058 0,03398\n? 0,02745 0,12024 0,02875\n\nTabela 4.19: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva dQhw(x)/dx.\n\nL2 ?d p(x)/dx\nSubconjunto AT Av Ate\n\nL2 me?dia 0,00181 0,01437 0,02490\n? 0,00393 0,01651 0,02870\n\nTabela 4.20: Norma L2 m\u00e9dia do erro e desvio padr\u00e3o - curva d p(x)/dx.\n\n\n\n66 CAP\u00cdTULO 4. RESULTADOS\n\nObserva-se, na Tabela 4.17, que o erro m\u00e9dio em L2 da curva Qhw(x) para o subconjunto Ate\nfoi ligeiramente maior em rela\u00e7\u00e3o ao subconjunto Av, ocorrendo o oposto com o valor do desvio\npadr\u00e3o. A Tabela 4.19 indica que os valores m\u00e9dios do erro e do desvio padr\u00e3o foram menores para\no subconjunto Ate. Pela Tabela 4.18, observa-se que o valor m\u00e9dio do erro da curva p(x) \u00e9 menor\npara Ate, por\u00e9m o desvio padr\u00e3o \u00e9 maior em rela\u00e7\u00e3o ao subconjunto Av. Os valores m\u00e9dios de erro\ne do desvio padr\u00e3o da curva d p(x)dx s\u00e3o menores para o conjunto de valida\u00e7\u00e3o, Av. Esses resultados\nassemelham-se aos apresentados nas Subse\u00e7\u00f5es 4.3.1 e 4.3.2, a menos do erro da curva Qhw(x).\n\nA Tabela 4.21 apresenta a m\u00e9dia do erro em norma L1 alterada da vaz\u00e3o total Qhw e o desvio\npadr\u00e3o para os subconjuntos AT , Av e Ate.\n\nL1 ?Qhw\nSubconjunto AT Av Ate\n\nL1 me?dia 0,615 1,637 1,905\n? 0,338 1,314 1,746\n\nTabela 4.21: Norma L1 (alterada) m\u00e9dia do erro e desvio padr\u00e3o - Qhw.\n\nObserva-se que os maiores valores m\u00e9dios do erro e do desvio padr\u00e3o ocorrem para o subcon-\njunto Ate, \u00e0 semelhan\u00e7a do observado em 4.3.1 e em 4.3.2.\n\n4.3.4 Compara\u00e7\u00e3o\n\nA fim de comparar o desempenho de cada MLP com rela\u00e7\u00e3o \u00e0 aplica\u00e7\u00e3o no modelo do acopla-\nmento po\u00e7o-reservat\u00f3rio, foram gerados gr\u00e1ficos contendo os erros em L2 para cada uma das cur-\nvas Qhw(x), p(x),\n\ndQhw(x)\ndx e\n\nd p(x)\ndx , para cada caso do conjunto A. Como os casos s\u00e3o numerados,\n\n\u00e9 poss\u00edvel verificar quais pertencem a cada subconjunto AT , Av e Ate, de acordo com a defini\u00e7\u00e3o\napresentada em 3.5.1. As figuras na sequ\u00eancia apresentam os gr\u00e1ficos comparativos:\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 67\n\n0?\n\n0.01?\n\n0.02?\n\n0.03?\n\n0.04?\n\n0.05?\n\n0.06?\n\n0.07?\n\n0.08?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nN\nor\n\nm\na \n\nL2\n d\n\no \nE\n\nrr\no \n\nCasos \n\nCompara\u00e7\u00e3o Norma L2 do Erro da Curva Qhw(x) \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.14: Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva Qhw(x).\n\n0?\n\n50?\n\n100?\n\n150?\n\n200?\n\n250?\n\n300?\n\n350?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nN\nor\n\nm\na \nL2\n\n d\no \n\nE\nrr\n\no \n\nCasos \n\n Compara\u00e7\u00e3o Norma L2 do Erro da Curva p(x) \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.15: Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva p(x).\n\n\n\n68 CAP\u00cdTULO 4. RESULTADOS\n\n0.00E+00?\n\n5.00E?04?\n\n1.00E?03?\n\n1.50E?03?\n\n2.00E?03?\n\n2.50E?03?\n\n3.00E?03?\n\n3.50E?03?\n\n4.00E?03?\n\n4.50E?03?\n\n5.00E?03?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nN\nor\n\nm\na \n\nL2\n d\n\no \nE\n\nrr\no \n\nCasos \n\nCompara\u00e7\u00e3o Norma L2 do Erro da Curva dQhw/dx \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.16: Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva dQhw(x)/dx.\n\n0?\n\n500?\n\n1000?\n\n1500?\n\n2000?\n\n2500?\n\n3000?\n\n3500?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nN\nor\n\nm\na \n\nL2\n d\n\no \nE\n\nrr\no \n\nCasos \n\nCompara\u00e7\u00e3o Norma L2 do Erro da Curva dp/dx \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.17: Compara\u00e7\u00e3o norma L2 do conjunto A para as tr\u00eas MLPs - curva d p(x)/dx.\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 69\n\nObserva-se, na Figura 4.14, que os valores do erro s\u00e3o id\u00eanticos entre as tr\u00eas MLPs para o\nsubconjunto AT . Observa-se tamb\u00e9m que os erros entre as MLPs parecem variar nos mesmos casos\ndos conjuntos Av e Ate, por\u00e9m em amplitudes diferentes. O mesmo \u00e9 observado nas outras curvas,\nconforme esbo\u00e7am as Figuras 4.15, 4.16 e 4.17.\n\nPara comparar os erros em L1 da vaz\u00e3o total Qhw, a Figura 4.18 apresenta as curvas dos erros\npara cada caso do conjunto A, para as tr\u00eas MLPs:\n\n0?\n\n1?\n\n2?\n\n3?\n\n4?\n\n5?\n\n6?\n\n7?\n\n0? 5? 10? 15? 20? 25? 30? 35? 40? 45? 50? 55? 60? 65? 70? 75? 80? 85? 90? 95? 100?\n\nE\nrr\n\nos\n P\n\ner\nce\n\nnt\nua\n\nis\n \n\nCasos?\n\nCompara\u00e7\u00e3o dos Erros Percentuais da Vaz\u00e3o Total \n\nMLP - 5 \nMLP - 10 \nMLP - 15 \n\nFigura 4.18: Compara\u00e7\u00e3o norma L1 dos erros do conjunto A para as tr\u00eas MLPs - Qhw.\n\nNota-se, pela Figura 4.18, que as tr\u00eas redes neurais apresentam valores de erros semelhantes\npara os casos do subconjunto AT . Por\u00e9m, para os casos dos subconjuntos Av e Ate, as varia\u00e7\u00f5es\ndos erros das tr\u00eas MLPs n\u00e3o coincidem. Isso \u00e9 explicado pelo fato de que os valores das vaz\u00f5es\ntotais s\u00e3o resultados das intera\u00e7\u00f5es das quatro curvas de fluxo, Qhw(x), p(x),\n\ndQhw(x)\ndx e\n\nd p(x)\ndx . Assim,\n\npresume-se que pequenas diferen\u00e7as observadas nas curvas entre as tr\u00eas redes s\u00e3o suficientes para\nocasionar maiores diferen\u00e7as nos valores das vaz\u00f5es totais.\n\nA fim de se estabelecer uma compara\u00e7\u00e3o, em termos dos valores dos erros e de desvio padr\u00e3o,\nresumiu-se nas Tabelas 4.22, 4.23 e 4.24, os resultados de cada arranjo MLP-modelo1D, para cada\nsubconjunto de A:\n\n\n\n70 CAP\u00cdTULO 4. RESULTADOS\n\nAT\nCurvas Arquiteturas MLP-5 MLP-10 MLP-15\n\nQhw(x)\nL2 me?dia 0,00729 0,00729 0,00729\n\n? 0,00824 0,00824 0,00824\n\np(x)\nL2 me?dia 7,24E ?6 7,24E ?6 7,24E ?6\n\n? 3,77E ?6 3,77E ?6 3,77E ?6\n\ndQhw(x)/dx\nL2 me?dia 0,02222 0,02222 0,02222\n\n? 0,02745 0,02745 0,02745\n\nd p(x)/dx\nL2 me?dia 0,00181 0,00181 0,00181\n\n? 0,00393 0,00393 0,00393\n\nQhw\nL1 me?dia 0,615 0,615 0,615\n\n? 0,338 0,338 0,338\n\nTabela 4.22: Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto AT .\n\nAv\nCurvas Arquiteturas MLP-5 MLP-10 MLP-15\n\nQhw(x)\nL2 me?dia 0,01684 0,01449 0,01624\n\n? 0,02432 0,01980 0,01903\n\np(x)\nL2 me?dia 8,19E ?6 8,19E ?6 8,10E ?6\n\n? 3,63E ?6 3,67E ?6 3,47E ?6\n\ndQhw(x)/dx\nL2 me?dia 0,06796 0,07058 0,07058\n\n? 0,12939 0,12286 0,12024\n\nd p(x)/dx\nL2 me?dia 0,01595 0,01428 0,01437\n\n? 0,01859 0,01508 0,01651\n\nQhw\nL1 me?dia 1,363 1,191 1,637\n\n? 0,828 0,980 1,314\n\nTabela 4.23: Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto Av.\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 71\n\nAte\nCurvas Arquiteturas MLP-5 MLP-10 MLP-15\n\nQhw(x)\nL2 me?dia 0,01459 0,01373 0,01670\n\n? 0,01733 0,01343 0,01728\n\np(x)\nL2 me?dia 7,68E ?6 7,43E ?6 7,59E ?6\n\n? 4,17E ?6 4,25E ?6 4,17E ?6\n\ndQhw(x)/dx\nL2 me?dia 0,02353 0,03137 0,03398\n\n? 0,02745 0,03398 0,02875\n\nd p(x)/dx\nL2 me?dia 0,01878 0,01949 0,02490\n\n? 0,02200 0,02270 0,02870\n\nQhw\nL1 me?dia 1,560 1,689 1,905\n\n? 1,581 1,481 1,746\n\nTabela 4.24: Comparativo entre os erros das curvas e da vaz\u00e3o total, subconjunto Ate.\n\nNota-se, pela Tabela 4.22, que os valores dos erros e dos desvios padr\u00e3o s\u00e3o interessantemente\nid\u00eanticos para todas as MLPs, em cada curva, comprovando o que foi observado nos gr\u00e1ficos an-\nteriores. Isso pode levar \u00e0 conclus\u00e3o de que as MLPs atingiram uma configura\u00e7\u00e3o semelhante\nreferente aos casos utilizados para os ajustes dos pesos sin\u00e1pticos (subconjunto de treinamento).\nConsiderando a Tabela 4.23, nota-se que os menores valores de erros e desvios padr\u00e3o aparecem\nora na MLP-10, ora na MLP-15. Pela Tabela 4.24, os menores valores s\u00e3o notados para as MLP-5 e\nMLP-10. Essas observa\u00e7\u00f5es podem levar \u00e0 conclus\u00e3o de que o projeto de uma arquitetura adequada\npara um problema como tal exige ponderar o comportamento das redes em todos os subconjuntos\ndispon\u00edveis, e n\u00e3o somente no subconjunto de treinamento. Para a presente an\u00e1lise, parece que a\nMLP-10 seria a mais adequada, por apresentar em maior parte das curvas dos casos de A os menores\nerros e desvios padr\u00e3o.\n\nA fim de ilustrar os resultados em termos das topologias das curvas de fluxo entre o modelo\nunidimensional e o tridimensional, seguem gr\u00e1ficos referentes a dois casos. Optou-se em escolher\ndois casos que n\u00e3o pertencem ao subconjunto AT e que apresentam energia de erro (? ) baixa e alta,\nrepresentando assim casos para os quais a generaliza\u00e7\u00e3o da MLP foi boa e ruim, respectivamente.\nDessa forma, a partir do gr\u00e1fico da Figura 4.13, observa-se que o caso 52, para MLP-15, apresenta\num salto relativamente grande no valor da energia total. Optou-se assim em escolh\u00ea-lo como um\nrepresentante para o qual a generaliza\u00e7\u00e3o apresentou maior distor\u00e7\u00e3o. Em oposi\u00e7\u00e3o, optou-se no\ncaso 100, MLP-10, como um caso em que a generaliza\u00e7\u00e3o apresentou-se boa. Seguem as figuras re-\nferentes \u00e0 compara\u00e7\u00e3o topol\u00f3gica das curvas de fluxo provenientes do modelo de elementos finitos\ne do modelo unidimensional:\n\n\n\n72 CAP\u00cdTULO 4. RESULTADOS\n\n\u2022 Exemplo de boa generaliza\u00e7\u00e3o, caso 100, MLP-10.\n\n0?\n\n0.1?\n\n0.2?\n\n0.3?\n\n0.4?\n\n0.5?\n\n0.6?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nm\n3 /\n\ns \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva Qhw(x) - Caso 100 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.19: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva Qhw(x).\n\n1.20E+07?\n\n1.25E+07?\n\n1.30E+07?\n\n1.35E+07?\n\n1.40E+07?\n\n1.45E+07?\n\n1.50E+07?\n\n1.55E+07?\n\n1.60E+07?\n\n1.65E+07?\n\n1.70E+07?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nPa\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva p(x) - Caso 100 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.20: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva p(x).\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 73\n\n0?\n\n0.0005?\n\n0.001?\n\n0.0015?\n\n0.002?\n\n0.0025?\n\n0.003?\n\n0.0035?\n\n0.004?\n\n0.0045?\n\n0.005?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nm\n3 /\n\ns/\nm\n\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva dQhw/dx - Caso 100 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.21: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva dQhw(x)/dx.\n\n0?\n\n5000?\n\n10000?\n\n15000?\n\n20000?\n\n25000?\n\n30000?\n\n35000?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nPa\n/m\n\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva dp/dx - Caso 100 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.22: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 100 - curva d p(x)/dx(x).\n\n\n\n74 CAP\u00cdTULO 4. RESULTADOS\n\n\u2022 Exemplo de generaliza\u00e7\u00e3o ruim, caso 52, MLP-15.\n\n0?\n\n0.2?\n\n0.4?\n\n0.6?\n\n0.8?\n\n1?\n\n1.2?\n\n1.4?\n\n1.6?\n\n1.8?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nm\n3 /\n\ns \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva Qhw(x) - Caso 52 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.23: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva Qhw(x).\n\n1.20E+07?\n\n1.30E+07?\n\n1.40E+07?\n\n1.50E+07?\n\n1.60E+07?\n\n1.70E+07?\n\n1.80E+07?\n\n1.90E+07?\n\n2.00E+07?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nPa\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva p(x) - Caso 52 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.24: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva p(x).\n\n\n\n4.3. REDES NEURAIS E O MODELO UNIDIMENSIONAL 75\n\n0?\n\n0.005?\n\n0.01?\n\n0.015?\n\n0.02?\n\n0.025?\n\n0.03?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nm\n3 /\n\ns/\nm\n\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva dQhw/dx - Caso 52 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.25: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva dQhw(x)/dx.\n\n0?\n\n10000?\n\n20000?\n\n30000?\n\n40000?\n\n50000?\n\n60000?\n\n70000?\n\n?200? ?150? ?100? ?50? 0? 50? 100? 150? 200?\n\nPa\n/m\n\n \n\nPosi\u00e7\u00e3o Po\u00e7o (m) \n\nCompara\u00e7\u00e3o Curva dp/dx - Caso 52 \n\nModelo 3D \nMLP+Modelo 1D \n\nFigura 4.26: Compara\u00e7\u00e3o modelo 3D e MLP+modelo 1D, caso 52 - curva d p(x)/dx(x).\n\n\n\n76 CAP\u00cdTULO 4. RESULTADOS\n\nPode-se observar que, para o caso 100, as topologias das curvas dos dois modelos parecem\nestar bem pr\u00f3ximas, o que era esperado pelos resultados do treinamento da arquitetura. Para o caso\n52, por\u00e9m, pode-se notar diferen\u00e7as topol\u00f3gicas razo\u00e1veis entre as curvas, notavelmente peculiar\nnas curvas p(x), entre as quais existe um ?p relativamente alto na regi\u00e3o do ded\u00e3o do po\u00e7o. Para\nas curvas Qhw(x) e d p(x)/dx, as diferen\u00e7as ocorrem na regi\u00e3o interna do po\u00e7o. J\u00e1 para a curva\ndQhw(x)/dx, existe ligeira diferen\u00e7a tanto no interior quanto nos extremos, embora a curvatura\ngeral do modelo unidimensional esteja em conformidade com a do modelo tridimensional. Embora\nseja um caso mais cr\u00edtico em termos do treinamento das MPLs, o caso 52 apresentou um erro\npercentual (norma L1alterada) para vaz\u00e3o total Qhw em torno de 2%, o que \u00e9 considerado um bom\nresultado.\n\nAlgumas discuss\u00f5es e conclus\u00f5es sobre os resultados do processo de treinamento e da resolu\u00e7\u00e3o\ndo modelo unidimensional s\u00e3o descritas no pr\u00f3ximo cap\u00edtulo.\n\n\n\nCap\u00edtulo 5\n\nConclus\u00e3o\n\nO presente trabalho concentrou no estudo e desenvolvimento de redes neurais artificiais com intuito\nn\u00e3o somente da capacit\u00e7\u00e3o pessoal, mas tamb\u00e9m em aplic\u00e1-las num problema num\u00e9rico de enge-\nnharia. Assim sendo, n\u00e3o somente uma biblioteca para cria\u00e7\u00e3o de redes neurais foi desenvolvida,\nmas tamb\u00e9m uma metodologia para utiliza\u00e7\u00e3o de suas potencialidades como fun\u00e7\u00e3o aproximadora\nfoi elaborada a fim de analisar poss\u00edvel substitui\u00e7\u00e3o (em certas ocasi\u00f5es) de um modelo tridimen-\nsional por um modelo unidimensional simplificado.\n\nO desenvolvimento da biblioteca de redes neurais artificiais, NeuralLib, teve o intuito de per-\nmitir ao grupo de pesquisa, no qual o trabalho foi realizado, uma ferramenta que pudesse atender\n\u00e0s necessidades com rela\u00e7\u00e3o \u00e0 simula\u00e7\u00e3o num\u00e9rica e tamb\u00e9m servir como base de ensino dessa\ntecnologia. Al\u00e9m do mais, futuras implementa\u00e7\u00f5es de forma a ampliar as potencialidades de uso e\naplica\u00e7\u00e3o poder\u00e3o ser realizadas, j\u00e1 que a estrutura do c\u00f3digo como todo foi projetada para isso. A\nsua aplica\u00e7\u00e3o no presente trabalho denota uma valida\u00e7\u00e3o um tanto quanto plaus\u00edvel, por\u00e9m, vali-\nda\u00e7\u00f5es futuras ainda poder\u00e3o ocorrer a fim de deix\u00e1-la mais confi\u00e1vel e robusta. Um outro recurso\nadicional proveniente do fato de se desenvolver uma biblioteca \u00e9 o de abrir um outro ramo de pes-\nquisa e desenvolvimento, j\u00e1 que se trata de um assunto relativamente novo e promete aplica\u00e7\u00f5es\ndas mais variadas.\n\nA metodologia descrita e articulada com rela\u00e7\u00e3o \u00e0 simula\u00e7\u00e3o num\u00e9rica do acoplamento po\u00e7o-\nreservat\u00f3rio procurou aplicar as MLPs num problema num\u00e9rico espec\u00edfico, utilizando-se de suas\ncapacidades de mapear e generalizar fun\u00e7\u00f5es a partir de conjunto de padr\u00f5es de treinamento. Nesse\ncaso, os resultados mostraram-se adequados aos objetivos do trabalho. As MLPs utilizadas mos-\ntraram um bom treinamento quanto aos conjuntos de dados gerados pelo modelo tridimensional,\ne os resultados de treinamento e generaliza\u00e7\u00e3o poderiam indicar qual das tr\u00eas arquiteturas seria\nmais adequada (no caso, MLP-5). No entanto, a an\u00e1lise nos erros das curvas geradas pelo modelo\nunidimensional arranjada com as MLPs parece indicar outra arquitetura como a mais adequada\n\n77\n\n\n\n78 CAP\u00cdTULO 5. CONCLUS\u00c3O\n\n(MLP-10, nesse caso). Essas diferen\u00e7as prov\u00eam do fato de que os valores de sa\u00edda das MLPs s\u00e3o\ncoeficientes de uma fun\u00e7\u00e3o polinomial, a qual \u00e9 utilizada na resolu\u00e7\u00e3o de duas equa\u00e7\u00f5es diferen-\nciais acopladas. Em outros termos, cada sa\u00edda da rede possui influ\u00eancia sobre o comportamento\nda fun\u00e7\u00e3o polinomial diferente uma da outra: uns par\u00e2metros geram mais sensibilidade, outros\nn\u00e3o. Assim, um erro maior numa sa\u00edda (ou par\u00e2metro), mesmo que isso n\u00e3o gerasse uma energia\ntotal do erro grande para dado caso de generaliza\u00e7\u00e3o (o que indicaria um bom resultado da rede),\npoderia gerar um comportamento na fun\u00e7\u00e3o polinomial de forma a produzir um erro (em norma\nL2, por exemplo) relativamente grande em rela\u00e7\u00e3o a outros casos para os quais a energia total do\nerro foram maiores. Esses resultados s\u00e3o inerentes da metodologia desenvolvida, j\u00e1 que as sa\u00eddas\nda rede neural n\u00e3o foram utilizadas como valores objetivos em si, mas sim como par\u00e2metros de\numa outra fun\u00e7\u00e3o, a qual \u00e9 utilizada na resolu\u00e7\u00e3o de equa\u00e7\u00f5es diferenciais. Assim, a propaga\u00e7\u00e3o e\npotencializa\u00e7\u00e3o do erro ao longo desse processo todo acaba sendo algo inevit\u00e1vel e at\u00e9, por assim\ndizer, curiosamente interessante.\n\nUma poss\u00edvel forma de se contornar esses pormenores seria procurar avaliar os resultados das\nredes neurais por uma outra medida de erro, de forma a contabilizar as varia\u00e7\u00f5es de cada sa\u00edda indi-\nvidualmente e n\u00e3o somente pela energia total. Isso poderia auxiliar a avalia\u00e7\u00e3o do treinamento das\nredes, buscando uma minimiza\u00e7\u00e3o mais equalizada nos erros dos valores de sa\u00edda. Tal metodologia\nteria muita utilidade no presente trabalho e poder\u00e1, futuramente, ser facilmente implementada na\nbiblioteca NeuralLib. Contudo, \u00e9 algo que ser\u00e1 elencado para trabalhos futuros.\n\nConclui-se, por fim, que os resultados, tanto do treinamento das redes neurais quanto do arranjo\ndessas com o modelo unidimensional do acoplamento po\u00e7o-reservat\u00f3rio, atendem aos objetivos\ndo trabalho. Tais resultados poder\u00e3o produzir futuras aplica\u00e7\u00f5es n\u00e3o somente para ind\u00fastria do\npetr\u00f3leo, mas tamb\u00e9m para simula\u00e7\u00e3o num\u00e9rica em problemas comuns na engenharia.\n\n\n\nRefer\u00eancias Bibliogr\u00e1ficas\n\nALRUMAH, Muhammad. Neural Networks Predict Well Inflow Performance. 2003. Disserta-\n\u00e7\u00e3o de Mestrado. Texas A&amp;M University, Texas, USA.\n\nAMINZADEH, F.; BARHEN, J.; GLOVER, C.W. e TOOMARIAN, N.B. Reservoir parameter\nestimation using a hybrid neural network. Computers and Geosciences, vol. 26, 2000.\n\nARRIETA, Juan Carlos Galvis. Finite Elements For Well-Reservoir Coupling. 2004. Disserta\u00e7\u00e3o\nde Mestrado. Instituto Nacional de Matem\u00e1tica Pura e Aplicada, Rio de Janeiro, RJ.\n\nARTURO, N.V.C.; SANTOS, D.V.; MENDES, J.R.P.; MIURA, K. e MOROOKA, C.K. Estudo do\nacoplamento po\u00e7o-reservat\u00f3rio para po\u00e7os horizontais. In Anais do 4o. Congresso Brasileiro\nde P&amp;D em Petr\u00f3leo e G\u00e1s. Campinas, SP, 2007.\n\nAZIZ, K. e SETTARI, A. Petroleum Reservoir Simulation. Appied Science Publishers, England,\n1983.\n\nBAZARAA, M.S.; SHERALI, H.D. e SHETTY, C.M. Nonlinear Programming - Theory and\nAlgorithms. John Wiley and Sons, Inc., U.S.A., 2 ed., 1993.\n\nBECKER, E.B.; CAREY, G.F. e ODEN, J.T. Finite Elements: An Introduction, vol. I. Prentice-\nHall, Inc., Englewood Cliffs, New Jersey, USA, 1981.\n\nBRAGA, A.; DE LEON FERREIRA DE CARVALHO, A.P. e LUDERMIR, T.B. Redes Neurais\nArtificiais: Teoria e Aplica\u00e7\u00f5es. LTC, Rio de Janeiro, RJ, Brasil, 2 ed., 2007.\n\nBURDEN, R.L. e FAIRES, J.D. An\u00e1lise Num\u00e9rica. Cengage Learning, S\u00e3o Paulo, SP, 8 ed., 2008.\n\nBUSSAB, W.O. e MORETTIN, P.A. Estat\u00edstica B\u00e1sica. Atual Editora, S\u00e3o Paulo, SP, 4 ed., 1997.\n\nCRICHLOW, H.B. Modern Reservoir Engineering - A Simulation Approach. Prentice-Hall,\nEnglewood Cliffs, New Jersey, USA, 1977.\n\nCUNHA, M.C.C. M\u00e9todos Num\u00e9ricos. Editora da Unicamp, Campinas, SP, 2 ed., 2009.\n\n79\n\n\n\n80 REFER\u00caNCIAS BIBLIOGR\u00c1FICAS\n\nDEVLOO, P.R.B.; RYLO, E.C.; LONGHIN, G.C.; FORTI, T.L.; FARIAS, A.M.; LUCCI, P.C.A.\ne FERNANDES, P.D. Desenvolvimento de ferramenta computacional para c\u00e1lculo de \u00edndice de\nprodutividade de po\u00e7os verticais e horizontais. In Anais do ENAHPE. Campos do Jord\u00e3o, SP,\n2009.\n\nDICKSTEIN, F.; LARA, A.Q.; NERI, C. e PERES, A.M. Modeling and simulation of horizontal\nwellbore-reservoir flow equations. In Latin American and Caribbean Petroleum Engineering\nConference, Paper SPE 39064-MS. Society of Petroleum Engineers, Rio de Janeiro, RJ, Brasil,\n1997.\n\nESCOBAR, F.H. e MONTEALEGRE, M. A more accurate correlation for the productivity index\nfo horizontal wells. Journal of Engineering and Applied Sciences, vol. 3, 70\u201378, 2008.\n\nFAUSETT, L.V. Fundamentals of Neural Networks: Architectures, Algorithms, and Applica-\ntions. Prentice-Hall, Upper Saddle River, New Jersey, USA, 1 ed., 1994.\n\nFERNANDES, P.D.; DA SILVA, M.G.F. e BEDRIKOVETSKY, P. Uniformiza\u00e7\u00e3o de fluxo em\npo\u00e7os horizontais. In Anais do ENAHPE. Pedra Azul - Domingos Martins, ES, 2006.\n\nGALUSHKIN, A.I. Neural Networks Theory. Springer, Moskva Region, R\u00fassia, 2007.\n\nGOMES, Jos\u00e9 Adilson Ten\u00f3rio. Simula\u00e7\u00e3o Num\u00e9rica de Po\u00e7os Horizontais em Reservat\u00f3rios\ncom Fluxo Multif\u00e1sico, Usando Refinamento Local. 1990. Disserta\u00e7\u00e3o de mestrado. Facul-\ndade de Engenharia Mec\u00e2nica, Universidade Estadual de Campinas, Campinas, SP.\n\nHASSOUN, M.H. Fundamentals of Artificial Neural Networks. A Bradford book, The MIT\nPress, Massachusetts Institute of Technology, USA, 1 ed., 1995.\n\nHAYKIN, S. Redes Neurais: Princ\u00edpios e Pr\u00e1tica. Bookman, Porto Alegre, RS, Brasil, 2 ed.,\n2001.\n\nJOSHI, S.D. Horizontal Well Technology. PennWell Books, Tulsa, Oklahoma, USA, 1991.\n\nJUNIOR, U.S.; FERNANDES, P.D.; DE ASSIS RESSEL PEREIRA, F. e REIS, M.V.F. Estudo\ndo acoplamento po\u00e7o-reservat\u00f3rio: Uso de ferramentas de cfd para an\u00e1lise do escoamento no\nentorno do po\u00e7o. In C. Petrobras, editor, Boletim T\u00e9cnico da Produ\u00e7\u00e3o de Petr\u00f3leo, vol. 2,\np\u00e1ginas 255\u2013272. Petrobras/CENPES, Rio de Janeiro, Dezembro 2007.\n\nKOV\u00c1CS, Z.L. O C\u00e9rebro e a Sua Mente: Uma Introdu\u00e7\u00e3o \u00e0 Neuroci\u00eancia Computacional.\nEdi\u00e7\u00e3o Acad\u00eamica, S\u00e3o Paulo, SP, Brasil, 1997.\n\n\n\nREFER\u00caNCIAS BIBLIOGR\u00c1FICAS 81\n\nKOV\u00c1CS, Z.L. Redes Neurais Artificiais: Fundamentos e Aplica\u00e7\u00f5es: um texto b\u00e1sico. Livra-\nria da F\u00edsica, S\u00e3o Paulo, SP, Brasil, 4a. ed., 2006.\n\nKREYSZIG, E. Introductory Fuctional Analysis with Applicantions. John Wiley and Sons,\nInc., U.S.A., 1978.\n\nLEMOS, Walter Petrone. Acoplamento Po\u00e7o-Reservat\u00f3rio para An\u00e1lise de Testes em Po\u00e7os n\u00e3o\nSurgentes. 1993. Disserta\u00e7\u00e3o de Mestrado. Faculdade de Engenharia Mec\u00e2nica, Universidade\nEstadual de Campinas, Campinas, SP.\n\nLIPPMAN, S.B. e LAJOIE, J. C++ Primer. Addson-Wesley, Reading, Massachusetts, USA, 3\ned., 1998.\n\nLUCCI, Paulo Cesar A. Descri\u00e7\u00e3o Matem\u00e1tica de Geometrias Curvas por Interpola\u00e7\u00e3o Trans-\nfinita. 2009. Disserta\u00e7\u00e3o de Mestrado. Faculdade de Engenharia Civil, Arquitetura e Urbanismo,\nUniversidade Estadual de Campinas, Campinas, SP.\n\nMCCULLOCH, W.S. e PITTS, W.H. A logical calculus of the ideas immanent in nervous activity.\nBulletin of Mathematical Biophysics, vol. 5, 115\u2013133, 1943.\n\nNGUYEN, H.H.; CHAN, C.W. e WILSON, M. Prediction of oil well production using multi-neural\nnetworks. In IEEE 2002 Canadian Conference on Electrical and Computer Engineering,\nCCECE\u201902. Winnipeg, Manitoba, Canada, 2002.\n\nPIMENTEL, W.R.O.; LISB\u00d4A, A.C.L. e MOREIRA, D.R.R. Modelagem via redes neurais artifi-\nciais de uma unidade industrial de craqueamento catal\u00edtico fluido. In 3o Congresso Brasileiro\nde P&amp;D em Petr\u00f3leo e G\u00e1s. Salvador, 2005.\n\nPRESS, W.H.; TEUKOLSKY, S.A.; VETTERLING, W.T. e FLANNERY, B.P. Numerical Reci-\npes: The Arte of Scientific Computing. Cambridge University Press, New York, NY, 3 ed.,\n2007.\n\nROSA, A.J.; DE SOUZA CARVALHO, R. e XAVIER, J.A.D. Engenharia de Reservat\u00f3rios de\nPetr\u00f3leo. Editora Interci\u00eancia, PETROBRAS, Rio de Janeiro, 2006.\n\nSHIRDEL, M. e SEPEHRNOORI, K. Development of a coupled compositional wellbore/reservoir\nsimulator for modeling pressure and temperature distribution in horizontal wells. In SPE Annual\nTechnical Conference and Exhibition, Paper SPE 124806-MS. Society of Petroleum Engine-\ners, New Orleans, Louisiana, USA, 2009.\n\nTHOMAS, J.E. Fundamentos de Engenharia de Petr\u00f3leo. Editora Interci\u00eancia, PETROBRAS,\nRio de Janeiro, 2001.\n\n\n\n82 REFER\u00caNCIAS BIBLIOGR\u00c1FICAS\n\nVICENTE, Ronaldo. A Numerical Model Coupling Reservoir and Horizontal Well Flow Dy-\nnamics. 2000. Phd thesis. Pennsylvania State University, Pennsylvania.\n\nVICENTE, R.; SARICA, C. e ERTEKIN, T. Horizontal well design optimization: A study of the\nparameters affecting the productivity and flux distribution of a horizontal well. In SPE Annual\nTechnical Conference and Exhibition, Paper SPE 84194-MS. Society of Petroleum Engineers,\nDenver, Colorado, USA, 2003.\n\nVILLANUEVA, J.M.M. Modelo de aproxima\u00e7\u00e3o de um simulador de produ\u00e7\u00e3o de petr\u00f3leo uti-\nlizando redes neurais artificiais. Revista de Intelig\u00eancia Computacional Aplicada (RICA),\nPUC-RJ, , no 1, abril 2008.\n\nZUBEN, Fernando Jos\u00e9 Von. Modelos Param\u00e9tricos e N\u00e3o-Param\u00e9tricos de Redes Neurais\nArtificiais e Aplica\u00e7\u00f5es. 1996. Tese de Doutorado. Faculdade de Engenharia El\u00e9trica e de\nComputa\u00e7\u00e3o, UNICAMP.\n\n\n\nAp\u00eandice A\n\nNeuralLib\n\nA NeuralLib foi desenvolvida em linguagem C++. Essa linguagem foi escolhida por possuir di-\nversas caracter\u00edsticas desej\u00e1veis em simula\u00e7\u00e3o e an\u00e1lsie num\u00e9rica, tais como orienta\u00e7\u00e3o a objetos,\nheran\u00e7a e polimorfismo, concep\u00e7\u00e3o em templates, entre outros tantos. Alem do mais, essa \u00e9 a\nlinguagem que o grupo de pesquisa do Laborat\u00f3rio de Mec\u00e2nica Computacional da Faculdade de\nEngenharia Civil, Arquitetura e Urbanismo da UNICAMP utiliza no desenvolvimento de simula-\n\u00e7\u00f5es e manuten\u00e7\u00e3o do ambiente de programa\u00e7\u00e3o em elementos finitos. Dessa forma, a NeuralLib\npode ser facilmente utilizada nos programas desenvolvidos no laborat\u00f3rio.\n\nNa sequ\u00eancia, ser\u00e3o descritas as classes desenvolvidas e ilustrado um caso de uso. Ser\u00e1 exem-\nplificado tamb\u00e9m detalhes sobre um arquivo de treinamento.\n\nA.1 Descri\u00e7\u00e3o das Classes\n\nO uso de templates para cria\u00e7\u00e3o de algumas classes foi necess\u00e1rio para possibilitar a extens\u00e3o da\nbiblioteca para diversos tipos de neur\u00f4nios, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, arquiteturas de rede e mesmo\npadr\u00f5es de aprendizagem, compondo assim de fato a caracteriza\u00e7\u00e3o de uma biblioteca. Al\u00e9m do\nmais, a concep\u00e7\u00e3o do c\u00f3digo permite facilidade para suportes e implementa\u00e7\u00e3o de funcionalidades\ndiversas, tais como an\u00e1lise da evolu\u00e7\u00e3o do erro de treinamento, an\u00e1lise de problemas com satura\u00e7\u00e3o\ndos neur\u00f4nios etc. Na sequ\u00eancia, descrever-se-\u00e1 a lista das classes desenvolvidas e uma breve\ndescri\u00e7\u00e3o de cada uma:\n\n1. abstLayer. Uma classe abstrata, a qual possui todos os m\u00e9todos virtuais. A ideia \u00e9 conceber\ne formalizar uma layer ou camada de uma rede neural. Os m\u00e9todos virtuais prescrevem a\npassagem de sinal para frente e para tr\u00e1s, acerta os valores dos pesos sin\u00e1pticos e biases\ndos neuronios, os valores de sa\u00edda da rede, entre outros. Por serem m\u00e9todos virtuais, a\nimplementa\u00e7\u00e3o fica a cargo das classes derivadas.\n\n83\n\n\n\n84 AP\u00caNDICE A. NEURALLIB\n\n2. layer<GActFun>. Classe derivada da abstLayer. Essa classe template implementa os m\u00e9-\ntodos definidos na classe m\u00e3e, de acordo com o tipo de fun\u00e7\u00e3o de ativa\u00e7\u00e3o GActFun. Al\u00e9m\ndos m\u00e9todos, a classe possui dois atributos importantes: um vetor de neuron<GActFun> e\num vetor de valores do tipo double. O primeiro vetor, de fato, s\u00e3o os neur\u00f4nios da camada\nou layer, definidos tamb\u00e9m de acordo com o tipo de fun\u00e7\u00e3o de ativa\u00e7\u00e3o. O segundo vetor\ncorresponde aos valores de sa\u00edda de cada neur\u00f4nio, ou seja, sa\u00edda da pr\u00f3pria layer.\n\n3. HeavysideFunction. Classe que implementa a fun\u00e7\u00e3o de ativa\u00e7\u00e3o do tipo Heavyside. Apre-\nsenta somente dois m\u00e9todos: um para processar a fun\u00e7\u00e3o e outra para processar a sua deri-\nvada. Todos os par\u00e2metros necess\u00e1rios para processar s\u00e3o passados via m\u00e9todo.\n\n4. LinearFunction. Classe que implementa a fun\u00e7\u00e3o de ativa\u00e7\u00e3o do tipo Linear. Apresenta so-\nmente dois m\u00e9todos: um para processar a fun\u00e7\u00e3o e outra para processar a sua derivada. Todos\nos par\u00e2metros necess\u00e1rios para processar s\u00e3o passados via m\u00e9todo. Possui dois atributos: o\nvalor de sa\u00edda da fun\u00e7\u00e3o de ativa\u00e7\u00e3o e o sinal de entrada. Esses atributos s\u00e3o usados para os\nc\u00e1lculos internos.\n\n5. LogistFunction. Classe que implementa a fun\u00e7\u00e3o de ativa\u00e7\u00e3o do tipo Logistic. Apresenta\nsomente dois m\u00e9todos: um para processar a fun\u00e7\u00e3o e outra para processar a sua derivada.\nTodos os par\u00e2metros necess\u00e1rios para processar s\u00e3o passados via m\u00e9todo. Possui um atributo:\no valor de sa\u00edda da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, o qual \u00e9 usado para os c\u00e1lculos internos.\n\n6. TanhFunction. Classe que implementa a fun\u00e7\u00e3o de ativa\u00e7\u00e3o do tipo Hyperbolic Tangent.\nApresenta somente dois m\u00e9todos: um para processar a fun\u00e7\u00e3o e outra para processar a sua\nderivada. Todos os par\u00e2metros necess\u00e1rios para processar s\u00e3o passados via m\u00e9todo. Possui\num atributo: o valor de sa\u00edda da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, o qual \u00e9 usado para os c\u00e1lculos internos.\n\n7. SineFunction. Classe que implementa a fun\u00e7\u00e3o de ativa\u00e7\u00e3o do tipo Sine. Apresenta somente\ndois m\u00e9todos: um para processar a fun\u00e7\u00e3o e outra para processar a sua derivada. Todos os\npar\u00e2metros necess\u00e1rios para processar s\u00e3o passados via m\u00e9todo. Possui um atributo: o valor\nde entrada da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, o qual \u00e9 usado para os c\u00e1lculos internos.\n\n8. network. Classe que armazena os componentes de uma rede neural. Possui diversos atributos\nnecess\u00e1rios para defini\u00e7\u00e3o de uma rede, armazenando n\u00fameros de neur\u00f4nios e os tipos de\nfun\u00e7\u00f5es de ativa\u00e7\u00e3o para cada layer, assim como os par\u00e2metros necess\u00e1rios a cada fun\u00e7\u00e3o;\npossui um vetor que armazena todas as layers, um vetor dos valores de sa\u00edda da pr\u00f3pria\nrede, entre outros. Os m\u00e9todos operam sobre cada layer e, consequentemente, sobre cada\nneur\u00f4nio, acessando assim qualquer unidade ou neur\u00f4nio da rede de acordo com o tipo de\nestrutura utilizada.\n\n\n\nA.1. DESCRI\u00c7\u00c3O DAS CLASSES 85\n\n9. neuron<GActFun>. Classe template que implementa as funcionalidades de um neur\u00f4nio\nartificial, de acordo com a defini\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o GActFun. Possui diversos m\u00e9todos\nque operam sobre as fun\u00e7\u00f5es de ativa\u00e7\u00e3o e sobre a inicializa\u00e7\u00e3o dos pesos sin\u00e1pticos e bias.\nPossui diversos atributos, dentre os quais destacam-se o vetor dos pesos sin\u00e1pticos e o valor\ndo bias, al\u00e9m do valor de sa\u00edda do neur\u00f4nio.\n\n10. MLPerceptron<GTypeTraining>. Classe template que implemanta a estrutura de uma rede\nneural do tipo Perceptron de M\u00faltiplas Camadas (MLP). O \u00fanico atributo \u00e9 um objeto do tipo\nnetwork, que armazena todos os dados necess\u00e1rios da rede. Possui alguns m\u00e9todos que ope-\nram sobre esse objeto, de forma a compor a estrutura de uma MLP, a partir do gerenciamento\nde fluxos de dados de uma layer para outra. O treinamento da rede \u00e9 realizado de acordo com\no tipo definido, GTypeTraining, passado como argumento na defini\u00e7\u00e3o da classe.\n\n11. backpropagation. Classe que implementa o algoritmo de retropropaga\u00e7\u00e3o, utilizado no trei-\nnamento das MLPs. A classe possui m\u00e9todos necess\u00e1rios para execu\u00e7\u00e3o do algoritmo, al\u00e9m\ndo pr\u00f3prio algoritmo. Possui tamb\u00e9m diversos atributos que armazenam dados necess\u00e1rios\npara execu\u00e7\u00e3o do treinamento, tais como vetores das energias de erro, valores limites para\nn\u00famero m\u00e1ximo de itera\u00e7\u00f5es e valor da energia m\u00e9dia m\u00ednima, taxa de treinamento, entre\noutros. O crit\u00e9rio de parada do algoritmo \u00e9 definido nessa classe e sua estrutura permite a\nimplementa\u00e7\u00e3o de logs para verifica\u00e7\u00e3o do desempenho do treinamento ao longo das itera-\n\u00e7\u00f5es.\n\n12. networkdata. Classe que armazena, estrutura e gerencia os dados que entram e saem numa\nrede neural. \u00c9 a classe cujo objeto faz a interface entre o usu\u00e1rio e a rede. Possui diversos\nm\u00e9todos, todos dedicados ao gereciamento dos dados de treinamento, escalonamento dos da-\ndos, leitura e salvamento dos arquivos correpondentes aos casos de treinamento etc. Possui\nv\u00e1rios atributos correspondes aos diversos conjuntos de treinamento (valida\u00e7\u00e3o, teste etc),\nassim como valores escalonados, entre outros. O objeto dessa classe \u00e9 passado como argu-\nmento para os m\u00e9todos de treinamento e generaliza\u00e7\u00e3o da classe de arquitetura, por exemplo,\nMLPerceptron<GTypeTraining>. A classe networkdata pode receber diversas novas imple-\nmenta\u00e7\u00f5es referentes ao tratamento e salvamento dos dados, de acordo com a necessidade de\nacoplar as redes neurais em outros ambientes ou programas.\n\n\n\n86 AP\u00caNDICE A. NEURALLIB\n\nA.2 Caso de Uso\n\nPara ilustrar a utiliza\u00e7\u00e3o da NeuralLib na gera\u00e7\u00e3o de redes do tipo Multilayer Perceptron no mape-\namento de fun\u00e7\u00f5es, \u00e9 apresentado uma sequ\u00eancia de defini\u00e7\u00f5es de objetos e par\u00e2metros espec\u00edficos\npara um caso de uso da MLP. As etapas em destaques apresentam e definem as vari\u00e1veis necess\u00e1rias\ne est\u00e3o escritas em linguagem C++.\n\nDefini\u00e7\u00e3o dos Arquivos\n\nOs arquivos a serem definidos s\u00e3o: arquivo de dados de treinamento, no qual est\u00e3o os valores de\ninput e output dos padr\u00f5es a serem mapeados; arquivo de resultado, no qual s\u00e3o escritos os valores\nde input originais e os de output produzidos pela rede; arquivo da rede treinada, no qual s\u00e3o escritos\na configura\u00e7\u00e3o da rede e os valores dos pesos sin\u00e1pticos e biases ap\u00f3s o treinamento.\n\nstring TrainFile = \"C:\\RNA.txt\";\n\nstring TrainFileOut = \"C:\\RNA_Out.txt\";\n\nstring TrainedNetworkFile = \"C:\\TrainedNetwork.txt\";\n\nifstream TrainData( TrainFile.c_str() );\n\nofstream TrainDataOut( TrainFileOut .c_str() );\n\nofstream TrainedNetworkData( TrainedNetworkFile.c_str() );\n\nPar\u00e2metros Gerais\n\nOs par\u00e2metros gerais s\u00e3o: tratamento dos dados de input e output, o qual escalona todos os valores\npara intervalos espec\u00edficos; n\u00famero de camadas e de neur\u00f4nios em cada uma; tipos de neur\u00f4nios\nem cada camada; par\u00e2metros do processo de treinamento.\n\nTratamento Dados de Input e Output\ndouble MaxLimitIn = 1.0;\n\ndouble MinLimitIn = -1.0;\n\ndouble MaxLimitOut = 1.0;\n\ndouble MinLimitOut = -1.0;\n\nN\u00famero de Camadas e Neur\u00f4nios\nint InputNum = 8;\n\nint OutputNum = 7;\n\nint LayerNum = 3;\n\nstd::vector<int> NeuronForLayer( LayerNum );\n\n\n\nA.2. CASO DE USO 87\n\nNeuronForLayer[0] = InputNum;\n\nNeuronForLayer[1] = 10;\n\nNeuronForLayer[2] = OutputNum;\n\nint TotalNeuronNum = InputNum + OutputNum + NeuronForLayer[1];\n\nTipos de Neur\u00f4nios nas Camadas\nstd::vector<int> ActFuncType( LayerNum );\n\nstd::vector<double> a( LayerNum );\n\nstd::vector<double> b( LayerNum );\n\nActFuncType[0] = 2;\n\nActFuncType[1] = 2;\n\nActFuncType[2] = 2;\n\na[0] = 1.7159;\n\na[1] = 1.7159;\n\na[2] = 1.7159;\n\nb[0] = 2.0/3.0;\n\nb[1] = 2.0/3.0;\n\nb[2] = 2.0/3.0;\n\nProcesso de Treinamento - backpropagation\ndouble n = 0.05;\n\ndouble alpha = 0.0000001;\n\ndouble MaxValueError = 0.001;\n\nint ErrorCriteriaType = 2;\n\nint MaxInteratorNumber = 10000;\n\nint K0 = 500;\n\nDefini\u00e7\u00e3o da Rede\n\nOs par\u00e2metros da rede neural s\u00e3o armazenados em uma estrutura de dados, a qual os concentra e\nos disponibiliza para outros objetos.\n\nnetwork MyNetwork( LayerNum, NeuronNum, NeuronForLayer );\n\nMyNetwork.SetTypeFunctionParam( ActFuncType, a, b );\n\n\n\n88 AP\u00caNDICE A. NEURALLIB\n\nDefini\u00e7\u00e3o da Arquitetura\n\nA arquitetura da rede a ser utilizada \u00e9 definida com base na estrutura de dados network. Define-se\ntamb\u00e9m o tipo de treinamento da arquitetura.\n\nMLPerceptron<backpropagation> MyMLP;\n\nMyMLP.DefineNetwork( MyNetwork );\n\nDefini\u00e7\u00e3o do Treinamento\n\nOs par\u00e2metros de treinamento da arquitetura s\u00e3o definidos. Cada tipo de treinamento possui par\u00e2-\nmetros espec\u00edficos. No exemplo, s\u00e3o definidos os que s\u00e3o necess\u00e1rios para o algoritmo backpropa-\ngation.\n\nbackpropagation MyTraining;\n\nMyTraining.SetTypeCritStop( ErrorCriteriaType, MaxValueError );\n\nMyTraining.SetN( n );\n\nMyTraining.SetAlpha( alpha );\n\nMyTraining.SetK0( K0 );\n\nMyTraining.SetNumberMaxIteration( MaxInteratorNumber );\n\nDefini\u00e7\u00e3o do Network Data\n\nA estrutura de dados que armazena todos os dados de entrada e sa\u00edda da rede neural \u00e9 definida. Essa\nestrutura faz um tratamento dos padr\u00f5es antes do processo de treinamento e gerencia todo fluxo de\ndados que entra e sai da rede neural.\n\nnetworkdata MyData;\n\nMyData.SetSchedullingParam( MaxLimitIn, MinLimitIn, MaxLimitOut, MinLimitOut );\n\nMyData.LoadData( TrainData ) ;\n\nTreinamento e Generaliza\u00e7\u00e3o\n\nEssa etapa \u00e9 o acionamento do processo de treinamento utilizando os conjuntos de treinamento e\nde valida\u00e7\u00e3o. Pode-se tamb\u00e9m realizar o processo de generaliza\u00e7\u00e3o, o qual executa todo os casos\npresentes no arquivo de treinamento, ou seja, os de treinamento, os de valida\u00e7\u00e3o e os de teste.\n\nMyMLP.TrainNetwork( MyTraining, MyData );\n\nMyMLP.RunNetwork( MyData );\n\n\n\nA.2. CASO DE USO 89\n\nSalvamento dos Resultados\n\nAp\u00f3s o processo de treinamento e generaliza\u00e7\u00e3o, faz-se o salvamento dos resultados e da rede\ntreinada. O arquivo de resultados \u00e9 estruturado semelhantemente ao de treinamento, por\u00e9m os\nvalores de output s\u00e3o os produzidos pela rede neural. O arquivo da rede treinada apresenta todos\nos par\u00e2metros necess\u00e1rios para reconstru\u00ed-la posteriormente.\n\nMyData.SaveAllResults( TrainDataOut );\n\nMyMLP.SaveTrainedNetwork( TrainedNetworkData );\n\n\n\n90 AP\u00caNDICE A. NEURALLIB\n\nA.3 Arquivo Treinamento\n\nO arquivo de treinamento contem todos os padr\u00f5es que s\u00e3o utilizados no processo de treinamento\nda rede. Os padr\u00f5es s\u00e3o agrupados em subconjuntos espec\u00edficos: treinamento, valida\u00e7\u00e3o e teste. O\npreenchimento do arquivo deve seguir essa sequ\u00eancia. Apenas para ilustrar, a Tabela A.1 apresenta\num trecho de arquivo de treinamento, no qual s\u00e3o apresentados os conjuntos de treinamento, de\nvalida\u00e7\u00e3o e de teste.\n\n9\n7\n\n50\n25\n25\n0 1233 833 7 (\u00b7\u00b7\u00b7) -1.03684 0.284343 (\u00b7\u00b7\u00b7)\n1 3314 2914 7 (\u00b7\u00b7\u00b7) -2.29589 0.409811 (\u00b7\u00b7\u00b7)\n2 2223 1823 11 (\u00b7\u00b7\u00b7) -1.21972 0.774174 (\u00b7\u00b7\u00b7)\n\n...\n0 1853 1453 8 (\u00b7\u00b7\u00b7) -1.21977 0.466505 (\u00b7\u00b7\u00b7)\n1 3032 2632 5 (\u00b7\u00b7\u00b7) -1.94562 1.26945 (\u00b7\u00b7\u00b7)\n2 3353 2953 17 (\u00b7\u00b7\u00b7) -1.81301 1.14936 (\u00b7\u00b7\u00b7)\n\n...\n0 3123 2723 8 (\u00b7\u00b7\u00b7) -2.16192 0.367926 (\u00b7\u00b7\u00b7)\n1 2300 1900 11 (\u00b7\u00b7\u00b7) -1.24067 0.986081 (\u00b7\u00b7\u00b7)\n2 1892 1492 6 (\u00b7\u00b7\u00b7) -1.57812 0.104688 (\u00b7\u00b7\u00b7)\n\n...\n\nTabela A.1: Trecho de um arquivo de treinamento de uma rede neural.\n\nOs primeiros dois n\u00fameros do arquivo referem-se ao n\u00famero de input e output, respectiva-\nmente. Os tr\u00eas n\u00fameros seguintes s\u00e3o as quantidades de padr\u00f5es existentes nos subconjuntos de\ntreinamento, valida\u00e7\u00e3o e de teste, respectivamente. Os n\u00fameros na sequ\u00eancia s\u00e3o os padr\u00f5es dos\nsubconjuntos. A primeira coluna \u00e9 o n\u00famero ou ID do padr\u00e3o. A sequ\u00eancia de n\u00fameros ap\u00f3s o ID\ns\u00e3o os valores de input, seguidos pelos valores de output. Ressalta-se que os IDs s\u00e3o numera\u00e7\u00f5es\nespec\u00edficas de cada subconjunto.\n\nEmbora seja denominado arquivo de treinamento, esse arquivo pode ser usado apenas para\ntestar casos em uma rede j\u00e1 treinada. Nessa situa\u00e7\u00e3o, define-se como 0 (zero) as quantidades de\ncasos para os subconjuntos de treinamento e valida\u00e7\u00e3o, preenchendo apenas o de teste.\n\n\n\tSum\u00e1rio\n\tLista de Figuras\n\tLista de Tabelas\n\tLista de S\u00edmbolos\n\tIntrodu\u00e7\u00e3o\n\tApresenta\u00e7\u00e3o: Redes Neurais Artificiais\n\tMotiva\u00e7\u00e3o\n\tObjetivos\n\tOrganiza\u00e7\u00e3o do Trabalho\n\n\tRevis\u00e3o Bibliogr\u00e1fica\n\tRedes Neurais Artificiais\n\tDefini\u00e7\u00e3o\n\tNeur\u00f4nio Artificial\n\tArquiteturas de Redes\n\tProcessos de Aprendizagem\n\n\tAcoplamento Po\u00e7o-Reservat\u00f3rio\n\tModelo Tridimensional do Acoplamento Po\u00e7o-Reservat\u00f3rio\n\tEscoamento no Reservat\u00f3rio\n\tEscoamento no Po\u00e7o Horizontal\n\tCurvas de Fluxo do Modelo Tridimensional\n\n\n\tMetodologia\n\tDescri\u00e7\u00e3o Geral\n\tFerramentas Computacionais Utilizadas\n\tAcoplamento Po\u00e7o-Reservat\u00f3rio: Unidimensional\n\tAn\u00e1lise da Resistividade K(x)\n\tAdimensionaliza\u00e7\u00e3o da Resistividade K(x)\n\tRepresenta\u00e7\u00e3o de K(x)\n\n\tAprendizagem da Rede Neural Artificial\n\tConjunto de Treinamento\n\tArquitetura de Rede Neural Utilizada\n\tProcesso de Aprendizagem\n\n\tMLP e o Modelo Unidimensional\n\tResolu\u00e7\u00e3o do Modelo Unidimensional\n\tAn\u00e1lise dos Resultados: Medida de Erros\n\n\n\tResultados\n\tNeuralLib\n\tAprendizagem da Rede Neural\n\tArquitetura 1: MLP-5\n\tArquitetura 2: MLP-10\n\tArquitetura 3: MLP-15\n\tCompara\u00e7\u00e3o\n\n\tRedes Neurais e o Modelo Unidimensional\n\tMLP-5 e Modelo Unidimensional\n\tMLP-10 e Modelo Unidimensional\n\tMLP-15 e Modelo Unidimensional\n\tCompara\u00e7\u00e3o\n\n\n\tConclus\u00e3o\n\tRefer\u00eancias Bibliogr\u00e1ficas\n\tNeuralLib\n\tDescri\u00e7\u00e3o das Classes\n\tCaso de Uso\n\tArquivo Treinamento"}]}}}