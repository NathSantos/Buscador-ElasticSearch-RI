{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.18484"}, {"@name": "filename", "#text": "2545_Redes%20Neurais%20Convolucionais%20Profundas%20na%20Detec%c3%a7%c3%a3o%20de%20Plantas%20Daninhas%20em%20Lavoura%20de%20Soja.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "Redes Neurais Convolucionais\nProfundas na Detec\u00e7\u00e3o de Plantas\n\nDaninhas em Lavoura de Soja\n\nAlessandro dos Santos Ferreira\n\n\n\nUNIVERSIDADE FEDERAL DE MATO GROSSO DO SUL\nFACULDADE DE COMPUTA\u00c7\u00c3O - FACOM\n\nRedes Neurais Convolucionais\nProfundas na Detec\u00e7\u00e3o de Plantas\n\nDaninhas em Lavoura de Soja\n\nAlessandro dos Santos Ferreira\n\nOrientador: Prof. Dr. Hemerson Pistori\n\nDisserta\u00e7\u00e3o apresentada ao curso de P\u00f3s\nGradua\u00e7\u00e3o em Ci\u00eancias da Computa\u00e7\u00e3o,\nda Universidade Federal de Mato Grosso\ndo Sul, como requisito parcial \u00e0 obten\u00e7\u00e3o\ndo t\u00edtulo de Mestre em Ci\u00eancias da\nComputa\u00e7\u00e3o.\n\nUFMS - Campo Grande\nmar\u00e7o/2017\n\nii\n\n\n\nAgradecimentos\n\nA todos os amigos do grupo INOVIS\u00c3O, em especial ao meu Orientador\n\nHemerson Pistori e aos colegas do grupo VANTAGRO, Gercina Gon\u00e7alves e\n\nDiogo Soares.\n\nAos amigos da Tend\u00eancia Informa\u00e7\u00f5es e Sistemas, empresa que\n\npossibilitou que eu realizasse o mestrado concorrentemente \u00e0s minhas\n\natividades na mesma.\n\nA toda minha fam\u00edlia, em especial ao meu pai, Jos\u00e9 de Oliveira Ferreira.\n\nEste trabalho recebeu apoio financeiro da Universidade Cat\u00f3lica Dom\n\nBosco, UCDB e da Funda\u00e7\u00e3o de Apoio ao Desenvolvimento do Ensino,\n\nCi\u00eancia e Tecnologia do Estado de Mato Grosso do Sul, FUNDECT. Tamb\u00e9m\n\ncontou com o apoio do Conselho Nacional de Desenvolvimento Cient\u00edfico e\n\nTecnol\u00f3gico, CNPQ e da Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel\n\nSuperior, CAPES.\n\niii\n\n\n\nAbstract\n\nWeeds are undesirable plants that grow in agricultural crops, such as\n\nsoybean crops, competing for elements such as sunlight and water, causing\n\nlosses to crop yields. The objective of this work was to use Convolutional\n\nNeural Networks (ConvNets or CNNs) to perform weed detection in soybean\n\ncrop images and classify these weeds among grass and broadleaf, aiming to\n\napply the specific herbicide to weed detected. For this purpose, a soybean\n\nplantation was carried out in Campo Grande, Mato Grosso do Sul, Brazil,\n\nand the Phantom DJI 3 Professional drone was used to capture a large\n\nnumber of crop images. With these photographs, an image database was\n\ncreated containing over fifteen thousand images of the soil, soybean,\n\nbroadleaf and grass weeds. The Convolutional Neural Networks used in this\n\nwork represent a Deep Learning architecture that has achieved remarkable\n\nsuccess in image recognition. For the training of Neural Network the CaffeNet\n\narchitecture was used. Available in Caffe software, it consists of a replication\n\nof the well known AlexNet, network which won the ImageNet Large Scale\n\nVisual Recognition Challenge 2012 (ILSVRC2012). A software was also\n\ndeveloped, Pynovis\u00e3o, which through the use of the superpixel segmentation\n\nalgorithm SLIC, was used to build a robust image dataset and classify images\n\nusing the model trained by Caffe software. In order to compare the results of\n\nConvNets, Support Vector Machines, AdaBoost and Random Forests were\n\nused in conjunction with a collection of shape, color and texture feature\n\nextraction techniques. As a result, this work achieved above 98% accuracy\n\nusing ConvNets in the detection of broadleaf and grass weeds in relation to\n\nsoil and soybean, with an accuracy average between all images above 99%.\n\niv\n\n\n\nResumo\n\nErvas daninhas s\u00e3o plantas indesejadas que crescem em culturas\n\nagr\u00edcolas, como as de soja, competindo por diversos fatores como luz e \u00e1gua e\n\ncausando preju\u00edzos \u00e0s lavouras. O objetivo deste trabalho foi utilizar Redes\n\nNeurais Convolucionais para realizar a detec\u00e7\u00e3o de ervas daninhas em\n\nimagens de lavouras de soja e classificar essas ervas daninhas entre\n\ngram\u00edneas e folhas largas, visando direcionar o herbicida espec\u00edfico ao tipo de\n\nerva daninha detectado. Para esse objetivo foi realizada uma planta\u00e7\u00e3o de\n\nsoja em Campo Grande, Mato Grosso do Sul, Brasil e com o uso do drone\n\nPhantom DJI 3 Professional foi capturado um grande n\u00famero de imagens da\n\ncultura. Com essas fotografias foi constru\u00eddo um banco de imagens contendo\n\nmais de quinze mil imagens do solo, soja e ervas daninhas de folhas largas e\n\ngram\u00edneas. As Redes Neurais Convolucionais utilizadas representam uma\n\narquitetura de Aprendizado Profundo que v\u00eam alcan\u00e7ando not\u00e1vel destaque\n\nno reconhecimento de imagens. Para o treinamento da Rede Neural foi\n\nutilizada a arquitetura CaffeNet, dispon\u00edvel no software Caffe, que consiste de\n\numa replica\u00e7\u00e3o da conhecida rede AlexNet, que venceu a competi\u00e7\u00e3o\n\nImageNet LSRVC de 2012. Foi implementado tamb\u00e9m um software,\n\nPynovis\u00e3o, que atrav\u00e9s do uso do segmentador SLIC Superpixel, ajudou na\n\nconstru\u00e7\u00e3o de um banco de imagens robusto e na classifica\u00e7\u00e3o das imagens\n\nutilizando o modelo treinado pelo software Caffe. Para comparar os\n\nresultados da Rede Neural Convolucional, foram utilizados os algoritmos\n\nM\u00e1quina de Vetores de Suporte, AdaBoost e Florestas Aleat\u00f3rias em conjunto\n\ncom uma cole\u00e7\u00e3o de extratores de atributos de forma, cor e textura. Como\n\nresultado, utilizando as Redes Neurais Convolucionais, este trabalho obteve\n\nprecis\u00e3o acima de 98% na detec\u00e7\u00e3o de ervas daninhas de folhas largas e\n\ngram\u00edneas em rela\u00e7\u00e3o ao solo e a soja, com m\u00e9dia de precis\u00e3o entre todas as\n\nimagens superior a 99%.\n\nv\n\n\n\nSum\u00e1rio\n\nSum\u00e1rio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\n\nLista de Figuras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix\n\nLista de Tabelas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . x\n\n1 Introdu\u00e7\u00e3o 1\n\n2 Fundamenta\u00e7\u00e3o Te\u00f3rica 4\n2.1 Vis\u00e3o Computacional . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n2.2 Ervas Daninhas e a Soja . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n2.2.1 Est\u00e1dios Fenol\u00f3gicos da Soja . . . . . . . . . . . . . . . . . . 6\n\n2.3 VANTs \u2013 Ve\u00edculos A\u00e9reos N\u00e3o Tripulados . . . . . . . . . . . . . . . 8\n\n2.4 Segmenta\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n\n2.4.1 SLIC Superpixels . . . . . . . . . . . . . . . . . . . . . . . . . 11\n\n2.5 Extra\u00e7\u00e3o de Atributos . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n2.5.1 Matrizes de Coocorr\u00eancia - GLCM . . . . . . . . . . . . . . . 13\n\n2.5.2 Histograma de Gradientes Orientados . . . . . . . . . . . . 13\n\n2.5.3 Padr\u00f5es Bin\u00e1rios Locais . . . . . . . . . . . . . . . . . . . . . 14\n\n2.5.4 Espa\u00e7os de cores RGB, HSV e CIELab . . . . . . . . . . . . 14\n\n2.6 Classificadores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n2.6.1 C4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n2.6.2 AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n2.6.3 Florestas Aleat\u00f3rias . . . . . . . . . . . . . . . . . . . . . . . 18\n\n2.6.4 M\u00e1quinas de Vetores de Suporte . . . . . . . . . . . . . . . . 18\n\n3 Trabalhos Correlatos 20\n\n4 Aprendizado Profundo 24\n4.1 Redes Neurais Artificiais . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n4.2 Convolu\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n\n4.3 Redes Neurais Convolucionais . . . . . . . . . . . . . . . . . . . . . 29\n\nvi\n\n\n\n5 Metodologia 33\n5.1 Vis\u00e3o Geral . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n5.2 Plantio da Soja . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n\n5.3 Captura de imagens . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.3.1 Materiais . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.3.2 Planejamento de voo . . . . . . . . . . . . . . . . . . . . . . . 38\n\n5.3.3 Banco de imagens . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n5.4 Segmenta\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n\n5.5 Extra\u00e7\u00e3o de Atributos . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n5.6 Classifica\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n5.7 M\u00e9tricas de Avalia\u00e7\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n\n6 Resultados e discuss\u00f5es 49\n6.1 Avalia\u00e7\u00e3o com Classes Balanceadas . . . . . . . . . . . . . . . . . . 49\n\n6.2 Avalia\u00e7\u00e3o com Classes Desbalanceadas . . . . . . . . . . . . . . . 53\n\n6.3 Avalia\u00e7\u00e3o por Extratores . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n6.4 Software Pynovis\u00e3o . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\n6.5 Classifica\u00e7\u00e3o em Imagens . . . . . . . . . . . . . . . . . . . . . . . . 59\n\n7 Conclus\u00e3o 63\n\nRefer\u00eancias 70\n\nvii\n\n\n\nLista de Figuras\n\n1.1 Ervas daninhas na lavoura de soja. . . . . . . . . . . . . . . . . . . 2\n\n2.1 Soja nos est\u00e1dios VE e VC. . . . . . . . . . . . . . . . . . . . . . . . 7\n\n2.2 Soja nos est\u00e1dios V1, V6 e R1. . . . . . . . . . . . . . . . . . . . . . 8\n\n2.3 VANT DJI Phantom 3 Professional. . . . . . . . . . . . . . . . . . . 9\n\n2.4 VANTs multirotores e de asa fixa. . . . . . . . . . . . . . . . . . . . 10\n\n2.5 Imagem de soja ap\u00f3s a aplica\u00e7\u00e3o do algoritmo SLIC. . . . . . . . . 11\n\n2.6 Padr\u00f5es Bin\u00e1rios Locais. . . . . . . . . . . . . . . . . . . . . . . . . 14\n\n2.7 Espa\u00e7os de cores. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n\n4.1 Convolu\u00e7\u00e3o em imagens. . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n4.2 Filtro de Convolu\u00e7\u00e3o. . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n4.3 Exemplo de max pooling. . . . . . . . . . . . . . . . . . . . . . . . . 30\n\n4.4 Arquitetura tradicional das Redes Neurais Convolucionais. . . . . 31\n\n5.1 Fluxograma da metodologia proposta . . . . . . . . . . . . . . . . . 33\n\n5.2 Diagrama ilustrando o plantio na fazenda S\u00e3o Jos\u00e9. . . . . . . . . 34\n\n5.3 Imagem de lavoura de soja capturada por VANT. . . . . . . . . . . 38\n\n5.4 Software Pynovis\u00e3o. . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n5.5 Par\u00e2metro K do SLIC Superpixels . . . . . . . . . . . . . . . . . . . 41\n\n5.6 Par\u00e2metro compacidade do SLIC Superpixels . . . . . . . . . . . . 42\n\n5.7 Pr\u00e9-processamento da imagem. . . . . . . . . . . . . . . . . . . . . 45\n\n5.8 Recortes da imagem. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\n6.1 Exemplos das classes do banco de imagens. . . . . . . . . . . . . . 49\n\n6.2 Erros na classifica\u00e7\u00e3o da avalia\u00e7\u00e3o com classes balanceadas. . . 51\n\n6.3 Gr\u00e1fico de confian\u00e7a na classifica\u00e7\u00e3o para o grupo de 4500\n\nimagens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n\n6.4 Erros na classifica\u00e7\u00e3o na avalia\u00e7\u00e3o com classes desbalanceadas. 55\n\n6.5 Software Pynovis\u00e3o - segmenta\u00e7\u00e3o. . . . . . . . . . . . . . . . . . . 57\n\n6.6 Software Pynovis\u00e3o - extra\u00e7\u00e3o de atributos e classifica\u00e7\u00e3o.. . . . . 58\n\nviii\n\n\n\n6.7 Classifica\u00e7\u00e3o da imagem utilizando o software Pynovis\u00e3o. . . . . . 60\n\n6.8 Classifica\u00e7\u00e3o e segmenta\u00e7\u00e3o da imagem utilizando o software\n\nPynovis\u00e3o. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n\nix\n\n\n\nLista de Tabelas\n\n5.1 Calend\u00e1rio do Plantio \u2013 Est\u00e1dios Vegetativos. . . . . . . . . . . . . 35\n\n5.2 Calend\u00e1rio do Plantio \u2013 Est\u00e1dios Reprodutivos. . . . . . . . . . . . 36\n\n5.3 Aplica\u00e7\u00e3o de Herbicida. . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.4 Dose das aplica\u00e7\u00f5es. . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.5 Coleta de imagens por dia. . . . . . . . . . . . . . . . . . . . . . . . 39\n\n5.6 Imagens e segmentos do banco de imagem. . . . . . . . . . . . . . 40\n\n5.7 Extratores de atributos. . . . . . . . . . . . . . . . . . . . . . . . . . 43\n\n6.1 Matriz de confus\u00e3o da avalia\u00e7\u00e3o com classes balanceadas. . . . . 50\n\n6.2 Tabela de confian\u00e7a na avalia\u00e7\u00e3o com classes balanceadas. . . . . 52\n\n6.3 Matriz de confus\u00e3o da avalia\u00e7\u00e3o com classes desbalanceadas. . . 53\n\n6.4 Matriz de confus\u00e3o por extrator de atributo. . . . . . . . . . . . . . 56\n\n6.5 Classifica\u00e7\u00e3o da imagens pelo software Pynovis\u00e3o. . . . . . . . . . 59\n\nx\n\n\n\nCAP\u00cdTULO\n\n1\nIntrodu\u00e7\u00e3o\n\nA soja representa o papel de principal oleaginosa consumida\n\nmundialmente, tanto para o consumo animal atrav\u00e9s do farelo de soja quanto\n\npara consumo humano atrav\u00e9s do \u00f3leo [1]. No Brasil a sua relev\u00e2ncia para o\n\nagroneg\u00f3cio come\u00e7ou a partir dos anos 70 atrav\u00e9s do aumento da \u00e1rea\n\ncultivada e principalmente pelo aumento da produtividade obtido atrav\u00e9s do\n\nuso de novas tecnologias. A partir dos anos 90, a agricultura brasileira\n\npassou por um processo de moderniza\u00e7\u00e3o fazendo com que o setor de soja\n\nalcan\u00e7asse maior crescimento e dinamismo. A import\u00e2ncia do complexo de\n\nsoja para o Brasil pode ser observada tanto pelo grande crescimento de\n\nprodu\u00e7\u00e3o quanto pela arrecada\u00e7\u00e3o com a sua exporta\u00e7\u00e3o e de seus derivados\n\ncomo \u00f3leo e farelo de soja [1].\n\nDe acordo com o portal do Minist\u00e9rio da Agricultura, a soja \u00e9 a cultura\n\nagr\u00edcola que mais cresceu nas \u00faltimas tr\u00eas d\u00e9cadas e corresponde a 49% da\n\n\u00e1rea plantada com gr\u00e3os no pa\u00eds. Cultivada principalmente nas regi\u00f5es\n\nCentro-Oeste e Sul do pa\u00eds, a soja \u00e9 um dos produtos mais destacados da\n\nagricultura nacional e na balan\u00e7a comercial, sendo o principal gerador de\n\ndivisas cambiais do Brasil, com valores anuais que ultrapassam os 20\n\nbilh\u00f5es de d\u00f3lares. A n\u00edvel mundial, segundo dados de 2015, foram\n\nproduzidas mais de 320 milh\u00f5es de toneladas de soja. O Brasil se encontra\n\nna segunda posi\u00e7\u00e3o, atr\u00e1s apenas dos Estados Unidos, sendo respons\u00e1vel\n\npor aproximadamente 31% da produ\u00e7\u00e3o mundial, o que correspondeu a 100\n\nmilh\u00f5es de toneladas. Dada a import\u00e2ncia da soja no contexto econ\u00f4mico \u00e9\n\nimprescind\u00edvel o uso de t\u00e9cnicas visando maximizar a produtividade e\n\nqualidade do produto.\n\nErvas daninhas s\u00e3o plantas que nascem espontaneamente em lavouras,\n\n1\n\n\n\ncomo as de soja, competindo por luz, \u00e1gua e nutrientes, causandos preju\u00edzos\n\n\u00e0 colheita. Rizzardi e Fleck [2] citam que o conhecimento da infesta\u00e7\u00e3o \u00e9 um\n\nprocedimento fundamental para a utiliza\u00e7\u00e3o de medidas preventivas no\n\ncontrole das ervas daninhas. Eles focam a import\u00e2ncia de disp\u00f4r-se de\n\nm\u00e9todos que realizem a quantifica\u00e7\u00e3o e an\u00e1lise da distribui\u00e7\u00e3o da infesta\u00e7\u00e3o\n\nde ervas daninhas de forma r\u00e1pida e econ\u00f4mica, evidenciando a necessidade\n\nde um m\u00e9todo mais pr\u00e1tico que a realiza\u00e7\u00e3o de observa\u00e7\u00f5es sistem\u00e1ticas das\n\nlavouras.\n\nFigura 1.1: Ervas daninhas na lavoura de soja. \u00c0 esquerda, ervas daninhas de folhas\nlargas e \u00e0 direita, gram\u00edneas.\n\nAs ervas daninhas podem ser classificadas com base no formato das suas\n\nfolhas. As suas folhas t\u00eam uma infinidade de formatos mas as gram\u00edneas, de\n\nfolhas estreitas e longas, se distinguem claramente, permitindo assim\n\nclassificar todas as outras como ervas daninhas de folhas largas, como pode\n\nser visto na Figura 1.1. A separa\u00e7\u00e3o nessas duas classes \u00e9 adequada porque\n\ngram\u00edneas e ervas daninhas de folhas largas s\u00e3o diferenciadas no tratamento\n\ndevido \u00e0 seletividade de alguns herbicidas ao grupo espec\u00edfico [3]. A aplica\u00e7\u00e3o\n\nde herbicidas consegue melhores resultados se o tratamento for direcionado\n\nao tipo espec\u00edfico de ervas daninhas.\n\nNeste trabalho nosso objetivo foi utilizar Redes Neurais Convolucionais\n\nProfundas ou ConvNets para realizar a identifica\u00e7\u00e3o das ervas daninhas em\nrela\u00e7\u00e3o \u00e0 soja e ao solo e classifica\u00e7\u00e3o delas em gram\u00edneas e folhas largas. O\n\nAprendizado Profundo (Deep Learning), em especial a arquitetura de redes\nconvolucionais, representa atualmente o estado da arte no reconhecimento\n\nde imagens e objetos [4]. Classificadores utilizados para esse fim como\n\nm\u00e1quinas de vetores de suporte e redes neurais tradicionais s\u00e3o dependentes\n\nde bons algoritmos extratores de atributos como FFT, SIFT e Gabor wavelet\n[5]. O fator chave do aprendizado profundo \u00e9 que sua extra\u00e7\u00e3o de\n\ncaracter\u00edsticas \u00e9 aprendida automaticamente a partir do dado bruto [4]. Suas\n\ncamadas de atributos n\u00e3o s\u00e3o modeladas manualmente, n\u00e3o sendo\n\nnecess\u00e1ria a utiliza\u00e7\u00e3o de algoritmos extratores para a realiza\u00e7\u00e3o do\n\n2\n\n\n\ntreinamento e classifica\u00e7\u00e3o da rede convolucional. Assim, \u00e9 esperado que\n\nesta t\u00e9cnica tenha muito mais sucesso num futuro pr\u00f3ximo pois necessita de\n\npouca engenharia manual, podendo se beneficiar do aumento de capacidade\n\nde computa\u00e7\u00e3o e dados [4].\n\nPara realizar a detec\u00e7\u00e3o autom\u00e1tica de ervas daninhas em lavouras de\n\nsoja, foi realizada uma planta\u00e7\u00e3o experimental de soja em Campo Grande,\n\nMato Grosso do Sul. Imagens a\u00e9reas dessa lavoura foram coletadas por\n\nVe\u00edculos A\u00e9reos n\u00e3o Tripulados, VANTs, tamb\u00e9m conhecidos como Aeronaves\n\nRemotamente Pilotadas (RPA), que t\u00eam se mostrado uma alternativa bastante\n\natraente n\u00e3o apenas economicamente mas tamb\u00e9m superando outras\n\nlimita\u00e7\u00f5es comuns das imagens obtidas por sat\u00e9lites e aeronaves [6]. Essas\n\nimagens foram segmentadas utilizando o algoritmo SLIC [7] possibilitando a\n\ncria\u00e7\u00e3o de um banco com mais de quinze mil segmentos identificando soja,\n\nsolo e ervas daninhas, separadas por folhas largas e gram\u00edneas. Para efeitos\n\nde compara\u00e7\u00e3o, al\u00e9m dos testes utilizando redes neurais convolucionais,\n\nrealizamos testes de detec\u00e7\u00e3o com outros classificadores tradicionais como\n\nm\u00e1quinas de vetores de suporte e florestas aleat\u00f3rias.\n\nEste trabalho encontra-se organizado em sete cap\u00edtulos. O segundo\n\ncap\u00edtulo \u00e9 composto pela fundamenta\u00e7\u00e3o te\u00f3rica, abordando vis\u00e3o\n\ncomputacional, ervas daninhas e VANTs. Tamb\u00e9m ser\u00e3o descritas as t\u00e9cnicas\n\nde segmenta\u00e7\u00e3o, extra\u00e7\u00e3o de atributos e classifica\u00e7\u00e3o utilizadas neste\n\ntrabalho para a detec\u00e7\u00e3o de ervas daninhas. No cap\u00edtulo tr\u00eas s\u00e3o listados\n\ntrabalhos correlatos que abordam problemas similares ao desta pesquisa. O\n\ncap\u00edtulo quatro possui a fundamenta\u00e7\u00e3o te\u00f3rica relativa ao Aprendizado\n\nProfundo e Redes Neurais Convolucionais. O quinto cap\u00edtulo \u00e9 composto pela\n\nmetodologia do trabalho e o sexto descreve os resultados e discuss\u00f5es. Por\n\nfim este trabalho traz a conclus\u00e3o como \u00faltimo cap\u00edtulo.\n\n3\n\n\n\nCAP\u00cdTULO\n\n2\nFundamenta\u00e7\u00e3o Te\u00f3rica\n\n2.1 Vis\u00e3o Computacional\n\nPrince [8] define que o objetivo da vis\u00e3o computacional \u00e9 extrair\n\ninforma\u00e7\u00e3o \u00fatil das imagens. Esta tarefa tem se demonstrado\n\nsurpreendentemente desafiante, despertando grande interesse dos\n\npesquisadores nas \u00faltimas d\u00e9cadas. De acordo com Shapiro e Stockman [9],\n\no seu prop\u00f3sito \u00e9 tomar decis\u00f5es \u00fateis sobre objetos f\u00edsicos e cenas atrav\u00e9s de\n\nimagens. Para tomar decis\u00f5es sobre objetos reais \u00e9 quase sempre necess\u00e1rio\n\nconstruir alguma descri\u00e7\u00e3o ou modelo deles a partir das imagens. Devido a\n\nesse fato, muitos pesquisadores definem o objetivo da vis\u00e3o computacional\n\ncomo a constru\u00e7\u00e3o da descri\u00e7\u00e3o de cenas obtidas das imagens.\n\nNa vis\u00e3o computacional tenta-se descrever o mundo que vemos em\n\nimagens e reconstruir suas propriedades como forma, ilumina\u00e7\u00e3o e\n\ndistribui\u00e7\u00e3o de cor. Como humanos e animais tendem a realizar o\n\nreconhecimento de imagens e objetos com facilidade, a dificuldade da\n\nresolu\u00e7\u00e3o do problema computacionalmente pode ser subestimada [10]. De\n\nqualquer forma, tem sido alcan\u00e7ado um not\u00e1vel progresso recente em nosso\n\nentendimento da vis\u00e3o computacional e na \u00faltima d\u00e9cada se viu uma grande\n\nescalada de desenvolvimento de tecnologias relacionadas \u00e0 \u00e1rea para o\n\nconsumidor. Um exemplo \u00e9 que atualmente a maioria das c\u00e2meras digitais\n\npossuem algoritmos embutidos para reconhecimento de face [8].\n\nA vis\u00e3o computacional \u00e9 relacionada a in\u00fameras outras \u00e1reas. Muitas\n\nt\u00e9cnicas desenvolvidas em outras \u00e1reas podem ser utilizadas para recuperar\n\ninforma\u00e7\u00f5es das imagens. Jain et al. [11] citam processamento de imagens,\n\n4\n\n\n\ncomputa\u00e7\u00e3o gr\u00e1fica, reconhecimento de padr\u00f5es e intelig\u00eancia artificial como\n\n\u00e1reas que contribuem com t\u00e9cnicas \u00fateis \u00e0 vis\u00e3o computacional.\n\nDado o grande crescimento da popula\u00e7\u00e3o mundial faz-se necess\u00e1rio o\n\naumento da produtividade agr\u00edcola. Por\u00e9m uma variedade de malef\u00edcios como\n\npragas, doen\u00e7as e defici\u00eancias minerais impactam na produtividade e\n\nqualidade das culturas. O monitoramento convencional das planta\u00e7\u00f5es \u00e9\n\ncustoso e de baixa efici\u00eancia [12]. Para reduzir o trabalho manual e aumentar\n\na efic\u00e1cia dos tratamentos utilizados, t\u00e9cnicas de processamento de imagem\n\ne vis\u00e3o computacional est\u00e3o sendo utilizadas na agricultura.\n\nNas se\u00e7\u00f5es 2.4, 2.5 e 2.6 ser\u00e3o descritas as t\u00e9cnicas de vis\u00e3o computacional\n\nassim como os algoritmos utilizados neste trabalho para realizar a detec\u00e7\u00e3o de\n\nervas daninhas.\n\n2.2 Ervas Daninhas e a Soja\n\nErvas daninhas, tamb\u00e9m conhecidas como plantas daninhas, invasoras,\n\nin\u00e7os e tingueras, s\u00e3o plantas que crescem espontaneamente em solos\n\nagr\u00edcolas onde n\u00e3o s\u00e3o desejadas. O crescimento dessas plantas competindo\n\ncom culturas econ\u00f4micas, como a soja, causa preju\u00edzos dificultando a\n\nopera\u00e7\u00e3o de m\u00e1quinas colhedoras e aumentando a impureza e umidade dos\n\ngr\u00e3os [13].\n\nOs efeitos negativos das plantas daninhas nas lavouras incluem a\n\ncompeti\u00e7\u00e3o de \u00e1gua, luz, nutrientes e espa\u00e7o, aumento de custos de\n\nprodu\u00e7\u00e3o, dificuldade de colheita, deprecia\u00e7\u00e3o da qualidade do produto,\n\nhospedagem de pragas e doen\u00e7as e diminui\u00e7\u00e3o do valor comercial das \u00e1reas\n\ncultivadas [2]. Para controlar a competi\u00e7\u00e3o das ervas daninhas s\u00e3o utilizados\n\nquatro tipos de manejos: exclus\u00e3o, preven\u00e7\u00e3o, supress\u00e3o e erradica\u00e7\u00e3o [13].\n\nAtrav\u00e9s de observa\u00e7\u00f5es e levantamentos nas regi\u00f5es produtoras de soja no\n\nBrasil, podemos indicar como mais frequentes as seguintes invasoras,\n\nclassificadas pelo formato das suas folhas [13]:\n\n\u2022 gram\u00edneas: capim-cust\u00f3dio (Pennisetum setosum), c.-marmelada\n(Brachiaria plantaginea), braqui\u00e1ria (B. decumbens), c.-carrapicho\n(Cenchrus echinatus), c.-colch\u00e3o (Digitaria spp.) e trapoeraba\n(Commelina benghalensis).\n\n\u2022 folhas largas: carrapicho-rasteiro (Acanthospermum australe),\npic\u00e3o-preto (Bidens pilosa), corda-de-viola (Ipomoea spp.),\namendoim-bravo (Euphorbia heterophylla), caruru (Amaranthus spp.),\nerva-quente (Spermacoce latifolia), jo\u00e1 (Solanum spp.), falsa-serralha\n(Emilia sonchifolia), guanxuma (Sida rhombifolia), poaia-branca\n\n5\n\n\n\n(Richardia brasilienses), cheirosa (Hyptis suaveolens), mentrasto\n(Agetarum conizoides) e o desmodio (Desmodium tortuosum).\n\nNo estado do Mato Grosso do Sul, de acordo com dados do projeto SIGA MS,\n\nas plantas daninhas de maior incid\u00eancia na safra 2014/2015 foram a buva\n\n(Conyza bonariensis), o capim amargoso (Elionurus candidus), o carrapicho\n(Acanthospermum australe) e o pic\u00e3o preto (Bidens pilosa). Destas, a buva e o\ncapim amargoso tiveram maior incid\u00eancia nas lavouras da regi\u00e3o.\n\n2.2.1 Est\u00e1dios Fenol\u00f3gicos da Soja\n\nFarias et al. [14] ressaltam a necessidade de uma linguagem unificada na\n\ndescri\u00e7\u00e3o dos est\u00e1dios fenol\u00f3gicos da soja, uma metodologia que necessita\n\nser objetiva, precisa e universal. A metodologia de descri\u00e7\u00e3o mais utilizada\n\nno mundo \u00e9 a proposta por Fehr e Caviness [15] que identifica precisamente\n\no est\u00e1dio de desenvolvimento de uma planta ou lavoura de soja. Este sistema\n\ndivide os est\u00e1dios de desenvolvimento da soja em est\u00e1dios vegetativos,\n\nrepresentados pela letra V, e reprodutivos, representados pela letra R. Com\n\nexce\u00e7\u00e3o dos est\u00e1dios VE (emerg\u00eancia) e VC (cotil\u00e9done), as letras V e R, s\u00e3o\n\nseguidas de \u00edndices num\u00e9ricos que identificam sequencialmente os est\u00e1dios\n\nespec\u00edficos de desenvolvimento nessas duas fases da planta.\n\nOs dois primeiros est\u00e1dios de desenvolvimento da planta s\u00e3o,\n\nrespectivamente, os est\u00e1dios VE e VC. A partir destes est\u00e1dios, as\n\nsubdivis\u00f5es dos est\u00e1dios vegetativos s\u00e3o numerados sequencialmente (V1,\n\nV2, V3, V4, ..., Vn), onde n \u00e9 o n\u00famero de n\u00f3s acima do n\u00f3 cotiledonar [14].\n\nAp\u00f3s esses est\u00e1dios temos os est\u00e1dios reprodutivos que s\u00e3o denominados\n\npela letra R seguida dos n\u00famero de um at\u00e9 oito e descrevem detalhadamente\n\no per\u00edodo de florescimento-matura\u00e7\u00e3o da soja. Eles abrangem quatro fases\n\ndistintas do desenvolvimento reprodutivo da planta: florescimento (R1 e R2),\n\ndesenvolvimento da vagem (R3 e R4), desenvolvimento do gr\u00e3o (R5 e R6) e\n\nmatura\u00e7\u00e3o da planta (R7 e R8).\n\nO per\u00edodo cr\u00edtico de controle de ervas daninhas \u00e9 um importante fator em\n\num sistema de combate \u00e0s ervas daninhas. Ele representa o intervalo\n\ndurante o ciclo de crescimento da cultura onde as ervas daninhas devem ser\n\ncontroladas para prevenir perdas. Mohammadi e Amiri [16] citam trabalhos\n\nque sugerem que as ervas daninhas precisam ser removidas entre os est\u00e1dios\n\nV1 e V3 e entre V3 e V4. Infelizmente n\u00e3o h\u00e1 uma regra trivial para se\n\ndeterminar esse per\u00edodo [17]. H\u00e1 uma gama de fatores que influenciam esse\n\nintervalo como densidade da cultura, condi\u00e7\u00f5es ambientais, t\u00e9cnicas de\n\nprodu\u00e7\u00e3o, caracter\u00edstica da cultura e das ervas daninhas.\n\nVan Acker et al. [18] conduziram um estudo para definir o per\u00edodo cr\u00edtico\n\nde controle na cultura de soja em Ontario, Canad\u00e1. Eles verificaram que o\n\n6\n\n\n\nFigura 2.1: \u00c0 esquerda a soja no est\u00e1dio VE (emerg\u00eancia). \u00c0 direita, a soja no\nest\u00e1dio VC (cotil\u00e9done). Ao contr\u00e1rio do est\u00e1dio VE as bordas das folhas unifolioladas\nn\u00e3o mais se tocam.\n\nper\u00edodo cr\u00edtico para remo\u00e7\u00e3o das ervas daninhas coincidiram com os est\u00e1dios\n\nV3, V2 e V1 para culturas espa\u00e7adas com 19, 38 e 76 cent\u00edmetros,\n\nrespectivamente. Todavia eles tamb\u00e9m conclu\u00edram que esse per\u00edodo \u00e9\n\nextremamente vari\u00e1vel por regi\u00f5es e ao longo dos anos. Como resultado\n\nrealmente conclusivo eles apontam que caso as ervas daninhas n\u00e3o sejam\n\ncontroladas nos est\u00e1dios iniciais de desenvolvimento, elas obrigatoriamente\n\nprecisam ser removidas antes do est\u00e1gio R1 para evitar perdas na lavoura.\n\nDo ponto de vista da Vis\u00e3o Computacional, \u00e9 interessante avaliar as\n\nmudan\u00e7as visuais nos est\u00e1dios vegetativos da soja, para estudarmos a\n\nnecessidade de classifica\u00e7\u00e3o por est\u00e1dios. Para este trabalho, de detec\u00e7\u00e3o das\n\nervas daninhas, \u00e9 relevante avaliar as mudan\u00e7as durante o est\u00e1dio\n\nvegetativo, est\u00e1dio onde as ervas daninhas devem obrigatoriamente ser\n\nremovidas. Cotil\u00e9dones s\u00e3o as primeiras folhas que surgem quando a\n\nsemente germina, e cuja fun\u00e7\u00e3o \u00e9 nutrir a planta no in\u00edcio do crescimento.\n\nDe acordo com Farias et al. [14], o est\u00e1dio VE representa a emerg\u00eancia dos\n\ncotil\u00e9dones. Em outras palavras uma planta rec\u00e9m emergida \u00e9 considerada\n\nno est\u00e1dio VE. Uma planta pode ser considerada emergida quando\n\nencontra-se com os cotil\u00e9dones acima da superf\u00edcie do solo e os mesmos\n\nformam um \u00e2ngulo igual ou superior a 90 graus com seus respectivos\n\nhipoc\u00f3tilos, a parte do caule logo abaixo dos cotil\u00e9dones.\n\nNo est\u00e1dio VC os cotil\u00e9dones se encontram completamente abertos e\n\nexpandidos e as bordas das folhas unifolioladas n\u00e3o mais se tocam. Nesse\n\nest\u00e1dio a soja se apresenta visualmente bastante diferente dos outros\n\nest\u00e1dios pois ainda n\u00e3o possui folhas completamente desenvolvidas e os\n\ncotil\u00e9dones est\u00e3o completamente abertos, ao contr\u00e1rio do est\u00e1dio VE, onde os\n\ncotil\u00e9dones est\u00e3o fechados.\n\n7\n\n\n\nFigura 2.2: Da esquerda para a direita temos imagens de soja nos est\u00e1dios V1,\nV6 e R1. Os est\u00e1dios V1 e V6 se diferenciam pela quantidade de folhas trifolioladas\ncompletamente desenvolvidas. No est\u00e1dio R1 j\u00e1 temos flores abertas.\n\nConsidera-se que a soja est\u00e1 no est\u00e1dio V1 quando todas as suas folhas\n\nunifolioladas est\u00e3o completamente desenvolvidas. Deste est\u00e1dio em diante a\n\nclassifica\u00e7\u00e3o \u00e9 baseada na quantidade de folhas trifolioladas completamente\n\ndesenvolvidas. No est\u00e1dio V2, temos a primeira folha trifoliolada\n\ncompletamente desenvolvida, no est\u00e1dio V3 a segunda folha trifoliolada e\n\nassim sucessivamente. Nos est\u00e1dios Vn, a soja j\u00e1 se apresenta visualmente\n\nde maneira mais uniforme, se distinguindo basicamente na quantidade de\n\nfolhas completamente desenvolvidas. Al\u00e9m disso nesses est\u00e1dios se torna\n\nmais f\u00e1cil a identifica\u00e7\u00e3o da soja em imagens a\u00e9reas que nos est\u00e1dios VE e\n\nVC.\n\n2.3 VANTs \u2013 Ve\u00edculos A\u00e9reos N\u00e3o Tripulados\n\nOs VANTs (Ve\u00edculos A\u00e9reos N\u00e3o Tripulados) ou Aeronaves Remotamente\n\nPilotadas (RPA) s\u00e3o aeronaves capazes de serem operadas por controle\n\nremoto ou autonomamente. Tamb\u00e9m s\u00e3o conhecidos como Unmanned Aerial\nVehicles (UAVs), Uninhabited Aerial Vehicles e Unmanned Aircraft Systems\n(UASs). O conceito de construir aeronaves n\u00e3o tripuladas para aplica\u00e7\u00f5es\n\ndiversas surgiu, inicialmente, em virtude de necessidades militares. Ap\u00f3s as\n\nopera\u00e7\u00f5es de 1991, com o VANT Pioneer sendo utilizado em mais de 300\n\nmiss\u00f5es durante a opera\u00e7\u00e3o Tempestade no Deserto \u00e9 que a utiliza\u00e7\u00e3o de\n\nVANTs foi impulsionada [19]. Com o avan\u00e7o tecnol\u00f3gico nos setores de\n\nprocessamento de dados e miniaturiza\u00e7\u00e3o de componentes eletr\u00f4nicos\n\nocorridos nas \u00faltimas duas d\u00e9cadas, diversas aplica\u00e7\u00f5es militares de VANTs\n\nforam desenvolvidas ao redor do mundo.\n\nO desenvolvimento de pequenos VANTs tem se tornado poss\u00edvel gra\u00e7as \u00e0\n\n8\n\n\n\nFigura 2.3: Imagem do VANT DJI Phantom 3 Professional utilizado nas capturas de\nimagem da safra 2015/2016.\n\nminituariza\u00e7\u00e3o e redu\u00e7\u00e3o de custos de componentes eletr\u00f4nicos como\n\nmicroprocessadores, sensores, baterias e unidades de comunica\u00e7\u00e3o wireless.\n\nMais recentemente, usos cient\u00edficos e civis t\u00eam sido desenvolvidos devido ao\n\nfato que esses ve\u00edculos a\u00e9reos desprovidos de tripula\u00e7\u00e3o podem apresentar\n\nvantagens t\u00e9cnicas e econ\u00f4micas nas mais diversas \u00e1reas. Floreano e Wood\n\n[20] citam v\u00e1rias atividades civis onde VANTs podem ser potencialmente\n\nutilizados como envio de ajuda de organiza\u00e7\u00f5es humanit\u00e1rias a campos de\n\nrefugiados, entrega de produtos em regi\u00f5es sem uma rede de transporte\n\nadequada, inspe\u00e7\u00f5es em \u00e1reas de riscos, monitoramento de \u00e1reas fora do\n\nalcance das c\u00e2meras de vigil\u00e2ncia por companhias de seguran\u00e7a, entre\n\noutras. Entre as vantagens obtidas no sensoriamente remoto utilizando\n\nVANTs podemos citar redu\u00e7\u00e3o dos custos para obten\u00e7\u00e3o de imagens a\u00e9reas,\n\nmaior flexibilidade para a aquisi\u00e7\u00e3o de imagens em alta resolu\u00e7\u00e3o,\n\npossibilidade de execu\u00e7\u00e3o dos mais variados tipos de miss\u00e3o sem colocar em\n\nrisco a vida do piloto ou operador de c\u00e2mera, economia no gasto do\n\ntreinamento de pilotos e maior facilidade e velocidade de incorpora\u00e7\u00e3o de\n\nnovas tecnologias [21].\n\nEntre as atividades civis onde o uso de VANTs vem sendo cada vez mais\n\nutilizado nos \u00faltimos anos, podemos citar a agricultura. Devido \u00e0s vantagens\n\ncitadas anteriormente, a utiliza\u00e7\u00e3o de VANTs vem sendo aproveitada com os\n\nmais variados objetivos na agricultura tendo como foco reduzir os custos e\n\naumentar a produtividade no campo. Imagens capturadas por VANTs\n\ncapazes de voar alguns metros acima do solo representam uma alternativa\n\nentre as imagens fornecidas por sat\u00e9lites e as imagens obtidas por ve\u00edculos\n\nlimitadas pela perspectiva humana e acessibilidade das estradas. Atrav\u00e9s de\n\ncont\u00ednuo monitoramento da qualidade das planta\u00e7\u00f5es, os VANTs permitem\n\naos agricultores medir o progresso do trabalho em tempo real [20].\n\nA expans\u00e3o do uso de VANTs em larga escala para aplica\u00e7\u00f5es civis\n\ndepende de dois pr\u00e9-requisitos relacionados: a capacidade dos mesmos de\n\n9\n\n\n\nFigura 2.4: Imagem de VANTs multirotores e de asa fixa.\n\nrealizarem de forma aut\u00f4noma manobras seguras em ambientes confinados e\n\na remo\u00e7\u00e3o da exig\u00eancia legal de opera\u00e7\u00e3o supervisionada. Todavia, a\n\nexig\u00eancia legal de um operador certificado dentro do campo de vis\u00e3o de cada\n\nVANT \u00e9 uma barreira que deve perdurar nos Estados Unidos e Europa at\u00e9 o\n\nfinal desta d\u00e9cada e remover esta barreira depende da confiabilidade e\n\nseguran\u00e7a dos pequenos VANTs [20]. No Brasil, ainda n\u00e3o foi aprovada\n\nregulamenta\u00e7\u00e3o para a utiliza\u00e7\u00e3o de VANTs de maneira segura e legal, mas\n\nh\u00e1 previs\u00e3o que seja lan\u00e7ada a primeira regulamenta\u00e7\u00e3o em breve.\n\nA \u00e1rea cient\u00edfica tamb\u00e9m tem se beneficiado da populariza\u00e7\u00e3o do uso dos\n\nVANTs. Eles j\u00e1 s\u00e3o uma eficiente ferramenta na coleta de dados permitindo\n\navan\u00e7os importantes em campos como pesquisa polar, estudo de vulc\u00f5es e\n\nbiodiversidade selvagem [22]. Mas quest\u00f5es legais ainda s\u00e3o um empecilho na\n\nexpans\u00e3o do uso. De qualquer modo, pesquisadores est\u00e3o empenhados em\n\nmelhorar a autonomia, confiabilidade das manobras e dura\u00e7\u00e3o dos v\u00f4os.\n\n2.4 Segmenta\u00e7\u00e3o\n\nSegmenta\u00e7\u00e3o da imagem \u00e9 o processo de dividir uma imagem em um\n\nconjunto de regi\u00f5es que a comp\u00f5em. Estas regi\u00f5es devem ser obtidas de\n\nforma a representar \u00e1reas significativas em imagens como planta\u00e7\u00f5es, \u00e1reas\n\nurbanas, florestas ou outras imagens obtidas por sat\u00e9lite [9]. A segmenta\u00e7\u00e3o\n\ntamb\u00e9m desempenha um papel vital em in\u00fameros casos da imagiologia\n\nm\u00e9dica, como diagn\u00f3sticos, localiza\u00e7\u00e3o de patologias, estudo de estruturas\n\nanat\u00f4micas e no planejamento de tratamentos [23].\n\nUm dos objetivos da segmenta\u00e7\u00e3o \u00e9 decomp\u00f4r a imagem em partes\n\n10\n\n\n\nmenores para facilitar a an\u00e1lise posterior. Outro objetivo da segmenta\u00e7\u00e3o \u00e9\n\nmudar a forma de representa\u00e7\u00e3o de uma imagem. Ela permite organizar os\n\npixels da imagem em agrupamentos que representam um maior n\u00edvel de\n\ninforma\u00e7\u00e3o que os pixels brutos [9]. Entre as t\u00e9cnicas de segmenta\u00e7\u00e3o, a\n\nutiliza\u00e7\u00e3o de superpixels vem se destacando pelo baixo custo computacional\n\ne alta qualidade da segmenta\u00e7\u00e3o [24].\n\n2.4.1 SLIC Superpixels\n\nAlgoritmos de superpixel agrupam os pixels em regi\u00f5es at\u00f4micas que\n\npodem ser utilizadas como substitutas da grade de pixels. Eles capturam a\n\nredund\u00e2ncia na imagem gerando uma estrutura que diminui\n\nsignificativamente a complexidade das tarefas de processamento de imagens.\n\nEsse algoritmos t\u00eam demonstrado grande utilidade em aplica\u00e7\u00f5es como\n\nlocaliza\u00e7\u00e3o de objetos e segmenta\u00e7\u00e3o de imagens. Entre os algoritmos para\n\ngera\u00e7\u00e3o de superpixels, o algoritmo Simple Linear Iterative Clustering (SLIC)\nse destaca pela simplicidade de uso al\u00e9m de baixa utiliza\u00e7\u00e3o de mem\u00f3ria e\n\nprocessamento [7].\n\nFigura 2.5: Imagem de soja ap\u00f3s a aplica\u00e7\u00e3o do algoritmo Simple Linear Iterative\nClustering (SLIC Superpixels). \u00c9 poss\u00edvel realizar parametriza\u00e7\u00f5es para definir o\nformato do superpixel.\n\nA extrat\u00e9gia deste algoritmo consiste em agrupar pixels baseado na\n\nsimilaridade da cor e proximidade espacial na imagem. Para isso \u00e9 utilizado o\n\nespa\u00e7o de cinco dimens\u00f5es [labxy], onde [lab] \u00e9 a cor do pixel no espa\u00e7o de\n\ncores CIELab e [xy] representa a posi\u00e7\u00e3o do pixel na imagem. O algoritmo\n\nrecebe como entrada o n\u00famero de superpixels, de aproximadamente mesmo\n\ntamanho, K. Sendo assim, para uma imagem com N pixels o tamanho\n\naproximado de cada superpixel \u00e9 N/K pixels. No caso de superpixels de\n\nmesmo tamanho, haveria um centro de superpixel em cada grade no\n\nintervalo S =\n?\n\nN/K.\n\n11\n\n\n\nO algoritmo SLIC constroi agrupamentos de pixels utilizando uma varia\u00e7\u00e3o\n\ndo algoritmo k-means que realiza a busca num espa\u00e7o reduzido, proporcional\n\na regi\u00e3o do superpixel, ao contr\u00e1rio do algoritmo tradicional que realiza a\n\ncompara\u00e7\u00e3o com todos os centros dos agrupamentos. Esse comportamento\n\naumenta de maneira significante a efici\u00eancia do algoritmo.\n\nPara isso o algoritmo define K centros de superpixels Ck = [lk,ak,bk,xk,yk], com\n\nk = [1,K] em cada grade no intervalo S. Ent\u00e3o, para cada pixel Pi = [li,ai,bi,xi,yi]\nda imagem, \u00e9 calculada a proximidade aos centros Ck atrav\u00e9s da dist\u00e2ncia\n\ndefinida em 2.1:\n\ndlab =\n?\n\n(lk ?li)2 +(ak ?ai)2 +(bk ?bi)2\n\ndxy =\n?\n\n(xk ?xi)2 +(yk ?yi)2\n\nDs = dlab +\nm\nS\n\ndxy\n\n(2.1)\n\nonde Ds representa a soma da dist\u00e2ncia no espa\u00e7o CIELab, representada pelas\n\nvari\u00e1veis lab, com a dist\u00e2ncia xy normalizada no plano pelo intervalo S. A\n\nvari\u00e1vel m representa a compacidade do superpixel e tem o valor padr\u00e3o 10,\ndefinido no trabalho original.\n\n2.5 Extra\u00e7\u00e3o de Atributos\n\nA pesquisa em aprendizado de m\u00e1quina tem como foco encontrar\n\nrelacionamentos nos dados e analisar os processos para extrair tais rela\u00e7\u00f5es\n\ndos mesmos. Um problema de aprendizado de m\u00e1quina \u00e9 uma tarefa\n\nexecutada utilizando uma aprendizagem sobre um conjunto de casos ou\n\nexemplos em substitui\u00e7\u00e3o a tradicional execu\u00e7\u00e3o a partir de um conjunto de\n\nregras predefinidas. Tais problemas s\u00e3o encontrados em uma grande\n\nvariedade de aplica\u00e7\u00f5es como reconhecimento de padr\u00f5es ou aplica\u00e7\u00f5es\n\nm\u00e9dicas [25].\n\nOs dados a serem analisados s\u00e3o representados por um conjunto de\n\ncaracter\u00edsticas ou atributos. Encontrar uma boa representa\u00e7\u00e3o dos dados \u00e9\n\nespec\u00edfico por dom\u00ednio e geralmente dependentes de especialistas, embora\n\npossa ser complementada por t\u00e9cnicas de extra\u00e7\u00e3o autom\u00e1tica [25]. No\n\nreconhecimento de imagem s\u00e3o extra\u00eddas informa\u00e7\u00f5es dos pixels brutos da\n\nimagem de forma a representar caracter\u00edsticas como cor, forma e textura.\n\nNeste trabalho foi utilizado um conjunto composto por extratores que ser\u00e3o\n\nbrevemente descritos nas pr\u00f3ximas subse\u00e7\u00f5es.\n\n12\n\n\n\n2.5.1 Matrizes de Coocorr\u00eancia - GLCM\n\nUma Matriz de Coocorr\u00eancia GLCM (Gray-Level Co-occurrence Matrix)\narmazena informa\u00e7\u00f5es sobre a textura de uma imagem. Esta informa\u00e7\u00e3o \u00e9\n\narmazenada em uma matriz de frequ\u00eancias P com dois pixels vizinhos\n\nseparados por uma dist\u00e2ncia d na imagem, sendo um deles com tom de cinza\n\ni e o outro com tom de cinza j. Estas frequ\u00eancias representam uma fun\u00e7\u00e3o\n\ndo relacionamento angular e dist\u00e2ncia entre os pixels vizinhos [26].\n\nSeja p(i, j) a (i, j)-\u00e9sima posi\u00e7\u00e3o da matriz P. Os desvios m\u00e9dio e padr\u00e3o das\n\nlinhas e colunas da matriz s\u00e3o definidos nas Equa\u00e7\u00f5es 2.2:\n\n\u00b5x = ?\ni\n\n?\nj\n\ni\u00b7 p(i, j), \u00b5y = ?\ni\n\n?\nj\n\nj \u00b7 p(i, j)\n\n?x = ?\ni\n\n?\nj\n(i?\u00b5x)2 \u00b7 p(i, j), ?y = ?\n\ni\n?\n\nj\n( j?\u00b5y)2 \u00b7 p(i, j)\n\n(2.2)\n\nA partir dessas defini\u00e7\u00f5es, temos as seguintes propriedades de texturas:\n\n1) Energia: f1 = ?\ni\n\n?\nj\n\np(i, j)2,\n\n2) Contraste: f2 =\nNg?1\n\n?\nn=0\n\nn2\n{\n\nNg\n\n?\ni=1\n\nNg\n\n?\nj=1\n\np(i, j)\n???? | i? j |= n\n\n}\n,\n\n3) Correla\u00e7\u00e3o: f3 =\n?i ? j(i j)p(i, j)?\u00b5x\u00b5y\n\n?x?y\n,\n\n4) Homogeneidade: f4 = ?\ni\n\n?\nj\n\n1\n1 +(i? j)2\n\np(i, j),\n\n5) Dissimilaridade: f5 = ?\ni\n\n?\nj\n| i? j | \u00b7p(i, j).\n\n(2.3)\n\n2.5.2 Histograma de Gradientes Orientados\n\nHistograma de Gradientes Orientados (Histogram of Oriented Gradients) \u00e9\numa t\u00e9cnica de extra\u00e7\u00e3o de atributos apresentada por Dalai e Triggs em 2005\n\n[27], com o objetivo inicial de auxiliar na detec\u00e7\u00e3o de pessoas em imagens. A\n\nideia b\u00e1sica do algoritmo \u00e9 que a forma e apar\u00eancia do objeto pode ser\n\ndefinida a partir da distribui\u00e7\u00e3o local de gradientes ou dire\u00e7\u00f5es das bordas.\n\nPara isso divide-se a imagem em c\u00e9lulas e para cada c\u00e9lula acumula-se um\n\nhistograma de gradientes de cada pixel contido naquela c\u00e9lula. Para\n\nmelhorar a invari\u00e2ncia \u00e0 ilumina\u00e7\u00e3o \u00e9 aplicada uma normaliza\u00e7\u00e3o de\n\ncontraste em blocos de c\u00e9lulas sobrepostos na imagem. Os blocos\n\nnormalizados s\u00e3o definidos como descritores HOG.\n\n13\n\n\n\nO algoritmo apresentado pode ser resumido nos seguintes passos descritos\n\nem 1:\nAlgoritmo 1: HOG Histograma de Gradientes Orientados\n\nNormaliza\u00e7\u00e3o da imagem1\n\nC\u00e1lculo dos gradientes2\n\nC\u00e1lculo do histograma de gradientes3\n\nNormaliza\u00e7\u00e3o de contraste dos blocos sobrepostos4\n\nColeta dos descritores HOG gerando um vetor de atributos5\n\n2.5.3 Padr\u00f5es Bin\u00e1rios Locais\n\nPadr\u00f5es Bin\u00e1rios Locais (Local Binary Patterns) s\u00e3o considerados um dos\n\nmelhores extratores de textura, sendo amplamente utilizados em diversas\n\naplica\u00e7\u00f5es [28]. Tem como vantagens sua invari\u00e2ncia a mudan\u00e7as em tons de\n\ncinza e efici\u00eancia computacional. Sua estrat\u00e9gia para detec\u00e7\u00e3o de textura \u00e9\n\nobservar para um ponto central a varia\u00e7\u00e3o da sua cor em rela\u00e7\u00e3o aos seus\n\nvizinhos. Esse procedimento \u00e9 realizado para todos os pixels da imagem,\n\nsendo cada pixel definido como o ponto central e tendo seu r\u00f3tulo atribu\u00eddo a\n\npartir do c\u00e1lculo em rela\u00e7\u00e3o aos seus vizinhos, conforme ilustrado em 2.6.\n\nPercorrendo a imagem no sentido anti-hor\u00e1rio, a partir da c\u00e9lula central\n\nesquerda, obt\u00e9m-se o valor bin\u00e1rio 011001100, correspondente ao valor\n\ndecimal 204. Ap\u00f3s realizar esse c\u00e1lculo para todos os pixels presentes na\n\nimagem, o histograma dos r\u00f3tulos dos pixels \u00e9 ent\u00e3o utilizado como um\n\nextrator de textura.\n\nFigura 2.6: Esquema ilustrando o funcionamento dos Padr\u00f5es Bin\u00e1rios Locais.\n\n2.5.4 Espa\u00e7os de cores RGB, HSV e CIELab\n\nForam utilizados como atributos de cores, informa\u00e7\u00f5es dos espa\u00e7os de\n\ncores RGB, HSV e CIELab. RGB \u00e9 o espa\u00e7o de cores formado a partir das\n\ncores vermelho, verde e azul, cujo nome vem do ingl\u00eas Red, Green e Blue. No\nmodelo RGB a cor \u00e9 resultante da adi\u00e7\u00e3o dos tr\u00eas componentes: vermelho,\n\nverde e azul, em diferentes intensidades.\n\n14\n\n\n\nHSV \u00e9 um sistema de coordenadas cil\u00edndricas correspondente ao modelo\n\nRGB. O nome HSV vem do ingl\u00eas, Hue, Saturation e Value que significam\nMatiz, Satura\u00e7\u00e3o e Valor. Seu objetivo \u00e9 fornecer uma representa\u00e7\u00e3o\n\ngeom\u00e9trica mais intuitiva e visualmente significativa que a representa\u00e7\u00e3o\n\ncartesiana do sistema RGB.\n\nCIELab \u00e9 um espa\u00e7o de cores que descreve todas as cores vis\u00edveis ao olho\n\nhumano e foi concebido para servir como um modelo independente de\n\ndispositivo. As tr\u00eas coordenadas do CIELab representam a luminosidade da\n\ncor, sua posi\u00e7\u00e3o entre as cores vermelho e verde e sua posi\u00e7\u00e3o entre as cores\n\namarelo e azul. Na Imagem 2.7, temos as representa\u00e7\u00f5es gr\u00e1ficas que\n\najudam a descrever esses espa\u00e7os.\n\nFigura 2.7: Representa\u00e7\u00f5es gr\u00e1ficas tradicionais dos espa\u00e7os de cores RGB, HSV e\nCIELab.\n\n2.6 Classificadores\n\nO aprendizado de m\u00e1quina supervisionado \u00e9 o processo de aprender um\n\nconjunto de regras a partir de inst\u00e2ncias ou exemplos de um conjunto de\n\ntreinamento. Tamb\u00e9m pode ser definido como a cria\u00e7\u00e3o de um classificador\n\nque pode ser generalizado para novas inst\u00e2ncias, n\u00e3o presentes no conjunto\n\nde treinamento. Um classificador \u00e9 um modelo que ap\u00f3s treinado pode ser\n\nutilizado para definir classes a inst\u00e2ncias de testes, cujas classes s\u00e3o\n\ndesconhecidas, utilizando a informa\u00e7\u00e3o dos seus atributos [29]. \u00c1rvores de\n\ndecis\u00e3o, redes neurais artificiais, redes bayesianas e m\u00e1quinas de vetores de\n\nsuporte s\u00e3o exemplos de classificadores. Nas pr\u00f3ximas subse\u00e7\u00f5es ser\u00e3o\n\ndescritos os classificadores utilizados neste trabalho para realizar a\n\ncompara\u00e7\u00e3o com as Redes Neurais Convolucionais.\n\n15\n\n\n\n2.6.1 C4.5\n\nC4.5 \u00e9 um algoritmo que utiliza \u00e1rvore de decis\u00e3o para realizar a\n\nclassifica\u00e7\u00e3o de um conjunto de casos de testes. Uma \u00e1rvore de decis\u00e3o \u00e9\n\numa estrutura que consiste de uma folha, que indica uma classe contida em\n\num conjunto de teste, ou um n\u00f3 de decis\u00e3o, que especifica um teste para ser\n\nrealizado, gerando uma ramifica\u00e7\u00e3o para cada poss\u00edvel resultado do teste.\n\nEsta estrutura \u00e9 utilizada para realizar a classifica\u00e7\u00e3o de uma entrada\n\npartindo da raiz da \u00e1rvore e movendo-se atrav\u00e9s dela, realizando os testes\n\npresentes nos n\u00f3s de decis\u00e3o, at\u00e9 que uma folha seja encontrada.\n\nO processo fundamental desse algoritmo \u00e9 a gera\u00e7\u00e3o inicial de uma \u00e1rvore\n\na partir de um conjunto de casos de testes. O m\u00e9todo para a constru\u00e7\u00e3o da\n\n\u00e1rvore de decis\u00e3o utiliza a estrat\u00e9gia de divis\u00e3o e conquista, baseando-se no\n\ntrabalho de Hoveland e Hunt [30]. O esqueleto do m\u00e9todo de Hunt para a\n\nconstru\u00e7\u00e3o da \u00e1rvore de decis\u00e3o para um conjunto de treinamento T , pode ser\n\ndefinido da seguinte maneira: sejam as classes do conjunto de treinamento\n\ndenotadas por C1,C2,...,Ck, temos tr\u00eas possibilidades:\n\n\u2022 T possui uma ou mais entradas, todas pertencentes \u00e0 classe C j: a \u00e1rvore\nde decis\u00e3o para T \u00e9 representada por uma \u00fanica folha identificando a\n\nclasse C j.\n\n\u2022 T n\u00e3o cont\u00e9m nenhuma entrada: a \u00e1rvore de decis\u00e3o novamente \u00e9\nrepresentada por uma \u00fanica folha, por\u00e9m nesse caso a classe dessa\n\nfolha \u00e9 determinada de alguma informa\u00e7\u00e3o que n\u00e3o esteja presente T . O\n\nalgoritmo C4.5 utiliza a classe mais frequente no pai desse n\u00f3.\n\n\u2022 T cont\u00e9m entradas que pertencem a classes variadas: neste cen\u00e1rio\ndivide-se T em subconjuntos que tendam a possuir uma \u00fanica classe.\n\nIsso \u00e9 feito a partir de um n\u00f3 de decis\u00e3o contendo um teste de um\n\ndeterminado atributo do conjunto de entrada, que possui um ou mais\n\nresultados mutualmente exclusivos O1,O2,...,On. A sa\u00edda desse n\u00f3 \u00e9\n\ncomposta por ramifica\u00e7\u00f5es Ti, que representam subconjuntos de T ,\n\nconstru\u00eddas a partir de todos os poss\u00edveis resultados do atributo\n\ntestado. Esta abordagem \u00e9 aplicada recursivamente em cada\n\nramifica\u00e7\u00e3o Ti, at\u00e9 que todos os subconjuntos possuam entradas\n\npertencentes a uma mesma classe.\n\nA escolha do atributo utilizado para a gera\u00e7\u00e3o dos subconjuntos Ti \u00e9 feita de\n\nmodo que cada subconjunto tenha o menor n\u00famero de ramifica\u00e7\u00f5es poss\u00edveis,\n\nou seja, deve ser escolhido um teste em cada n\u00edvel de modo que o tamanho\n\nfinal da \u00e1rvore seja o menor poss\u00edvel. Dada a inviabilidade de se testar todas\n\nas poss\u00edveis combina\u00e7\u00f5es de testes, o algoritmo usa uma estrat\u00e9gia gulosa\n\n16\n\n\n\nnesta escolha, baseada numa informa\u00e7\u00e3o definida como a Entropia de cada\nsubconjunto Ti [30].\n\nNeste trabalho utilizamos o algoritmo J48 dispon\u00edvel no software Weka,\n\nque consiste da re-implementa\u00e7\u00e3o em Java do software C4.5 Release 8, que\n\ncont\u00e9m caracter\u00edsticas adicionais \u00e0 vers\u00e3o descrita em [30].\n\n2.6.2 AdaBoost\n\nBoosting \u00e9 uma t\u00e9cnica utilizada para melhorar a performance de um\nalgoritmo de aprendizado. Na teoria, o boosting pode ser usado para\nmelhorar significativamente qualquer algoritmo que gere classificadores cujos\n\nresultados s\u00e3o pouco melhores que um palpite aleat\u00f3rio. AdaBoost \u00e9 um\n\nalgoritmo de boosting proposto por Freund e Schapire em 1996 [31]. Em seu\ntrabalho eles descreveram duas vers\u00f5es do algoritmo, AdaBoost.M1 e\n\nAdaBoost.M2. As duas vers\u00f5es s\u00e3o equivalentes para classifica\u00e7\u00e3o bin\u00e1ria, se\n\ndiferenciando na classifica\u00e7\u00e3o de problemas com mais de duas classes.\n\nA t\u00e9cnica de boosting funciona executando um algoritmo de aprendizado\nfraco repetidas vezes utilizando v\u00e1rias distribui\u00e7\u00f5es do conjunto de\n\ntreinamento e ent\u00e3o combinando os classificadores obtidos em um \u00fanico\n\nclassificador agregado. Embora a t\u00e9cnica tenha como foco melhorar a\n\nperformance de classificadores fracos, ela tamb\u00e9m pode ser utilizada com\n\nbons classificadores como o C4.5 [31]. Neste caso ainda \u00e9 percept\u00edvel o boost\nmas os resultados s\u00e3o menos significativos.\n\nO algoritmo AdaBoost recebe como entrada um conjunto S contendo m\n\ncasos de treinamento e um algoritmo de aprendizado denotado originalmente\n\ncomo WeakLearn. Ele ent\u00e3o executa o WeakLearn repetidamente por T\nitera\u00e7\u00f5es. A cada itera\u00e7\u00e3o a entrada do algoritmo WeakLearn \u00e9 uma\ndistribui\u00e7\u00e3o Dt sobre o conjunto S. Essa distribui\u00e7\u00e3o inicialmente \u00e9 uniforme,\n\nonde D1(i) = 1/m, para todo i. Nas itera\u00e7\u00f5es subsequentes a distribui\u00e7\u00e3o Dt+1\n\u00e9 atualizada com base na classifica\u00e7\u00e3o retornada do algoritmo WeakLearn na\ndistribui\u00e7\u00e3o Dt .\n\nNo final de T rodadas s\u00e3o obtidas T inst\u00e2ncias de WeakLearn que ser\u00e3o\nutilizadas para realizar a classifica\u00e7\u00e3o de um conjunto de caso de testes. Essa\n\nclassifica\u00e7\u00e3o \u00e9 feita submetendo o caso de teste a cada um dos classificadores\n\ngerados e calculando a soma ponderada das classes informadas por eles, onde\n\no peso da resposta de cada classificador \u00e9 inversamente proporcional \u00e0 sua\n\ntaxa de erro. A classe com maior peso nesta soma \u00e9 considerada como a\n\ncorreta pelo algoritmo AdaBoost [31].\n\n17\n\n\n\n2.6.3 Florestas Aleat\u00f3rias\n\nFlorestas Aleat\u00f3rias consistem de uma combina\u00e7\u00e3o de \u00e1rvores de decis\u00e3o\n\nformadas a partir de v\u00e1rias amostragens aleat\u00f3rias e independentes de um\n\nconjunto de entrada e com a mesma distribui\u00e7\u00e3o para todas as \u00e1rvores [32].\n\nEssa t\u00e9cnica \u00e9 conhecida como bagging, onde v\u00e1rias \u00e1rvores geradas de\nmaneira independente s\u00e3o agrupadas para fornecer uma classifica\u00e7\u00e3o [33]. A\n\nconstru\u00e7\u00e3o dessas \u00e1rvores \u00e9 realizada atrav\u00e9s da sele\u00e7\u00e3o de uma amostragem\n\naleat\u00f3ria, com reposi\u00e7\u00e3o, do conjunto de treinamento S. Para cada\n\namostragem selecionada \u00e9 constru\u00edda uma \u00e1rvore correspondente \u00e0quele\n\nconjunto.\n\nO algoritmo Florestas Aleat\u00f3rias tamb\u00e9m prop\u00f5e uma camada adicional de\n\naleatoriedade ao funcionamento tradicional do bagging. Al\u00e9m de construir\ncada \u00e1rvore utilizando uma amostragem diferente, tamb\u00e9m \u00e9 modificado o\n\nmodo como cada \u00e1rvore \u00e9 constru\u00edda. No modelo tradicional cada n\u00f3 \u00e9\n\nparticionado usando a melhor parti\u00e7\u00e3o entre todos os M atributos das\n\nentradas. Nesse algoritmo a escolha dos atributos \u00e9 baseada em um\n\nsubconjunto de atributos de tamanho m, onde m \u00e9 um valor fixo para todas as\n\n\u00e1rvores constru\u00eddas e m &lt;M. Esses atributos s\u00e3o escolhidos aleatoriamente e\n\na constru\u00e7\u00e3o de cada \u00e1rvore \u00e9 realizada utilizando apenas o subconjunto de\n\natributos escolhidos [33]. A sele\u00e7\u00e3o do subconjunto tamb\u00e9m \u00e9 independente\n\nentre todas as \u00e1rvores geradas.\n\nPara se obter a classifica\u00e7\u00e3o de um caso de teste, \u00e9 realizado o agrupamento\n\ndos resultados fornecidos por todas as \u00e1rvores para aquela entrada. A classe\n\ninformada pelas Florestas Aleat\u00f3rias \u00e9 definida a partir da maioria de votos\n\nfornecidos por todas as \u00e1rvores.\n\nA taxa de erro do classificador depende de dois fatores: a correla\u00e7\u00e3o entre\n\ncada par de \u00e1rvores e a for\u00e7a individual de cada \u00e1rvore na floresta. Uma maior\n\ncorrela\u00e7\u00e3o aumenta a taxa de erro enquanto a maior for\u00e7a diminui a taxa\n\n[32]. Esses dois fatores s\u00e3o diretamente proporcionais ao valor m escolhido\n\ncomo o n\u00famero de atributos a ser utilizado na constru\u00e7\u00e3o de cada \u00e1rvore.\n\nO objetivo do algoritmo \u00e9 gerar um classificador que possua simultaneamente\n\nbaixa taxa de erros sem a ocorr\u00eancia de overfitting no conjunto de treinamento,\num problema comum nas \u00e1rvores de decis\u00e3o.\n\n2.6.4 M\u00e1quinas de Vetores de Suporte\n\nM\u00e1quinas de vetores de suporte (Support Vector Machines - SVM) \u00e9 uma\nt\u00e9cnica de aprendizagem de m\u00e1quina definida originalmente por Vladmir\n\nVapnik [34]. Em sua forma mais simples, a forma linear, m\u00e1quinas de\n\nvetores de suporte constituem um hiperplano de tal forma que haja uma\n\n18\n\n\n\nmargem separando um conjunto de exemplos positivos e negativos em um\n\nespa\u00e7o com um alto n\u00famero de dimens\u00f5es [34]. Dado o fato que podem haver\n\ninfinitas escolhas para a margem que fa\u00e7a a separa\u00e7\u00e3o desses exemplos, o\n\nobjetivo \u00e9 maximizar a dist\u00e2ncia dessa margem aos exemplos negativo e\n\npositivo mais pr\u00f3ximos.\n\nEm alguns casos n\u00e3o \u00e9 poss\u00edvel separar linearmente todos os casos de um\n\nconjunto de dados, logo n\u00e3o h\u00e1 um hiperplano que divida todos os pontos\n\nnegativos e positivos. Nesse caso \u00e9 aplicada uma penalidade a um exemplo\n\nque falhe em se posicionar na sua margem correta. Al\u00e9m disso, m\u00e1quinas de\n\nvetores de suporte podem ser generalizadas para classificadores n\u00e3o lineares.\n\nPara isso s\u00e3o utilizadas fun\u00e7\u00f5es Kernel que incluem, por exemplo,\nn\u00e3o-linearidades gaussianas e polinomiais [34].\n\nApesar do algoritmo fornecer uma solu\u00e7\u00e3o para problemas bin\u00e1rios, essa\n\nsolu\u00e7\u00e3o pode ser generalizada para problemas com m\u00faltiplas classes. Isso\n\npode ser alcan\u00e7ado atrav\u00e9s de duas estrat\u00e9gias: realizando o teste de uma\n\nclasse contra todas as outras ou uma abordagem com v\u00e1rios testes entre duas\n\nclasses [37].\n\nO treinamento de uma m\u00e1quina de vetores de suporte requer a solu\u00e7\u00e3o\n\ndo problema de otimiza\u00e7\u00e3o de programa\u00e7\u00e3o quadr\u00e1tica. Platt [34], em 1998,\n\nprop\u00f4s o algoritmo Sequential Minimal Optimization - SMO que apresenta uma\nsolu\u00e7\u00e3o onde quebra o problema de programa\u00e7\u00e3o quadr\u00e1tica original em uma\n\ns\u00e9rie de problemas menores. Esta abordagem resultou em uma queda no\n\ntempo de computa\u00e7\u00e3o do treinamento e deixou a utiliza\u00e7\u00e3o de mem\u00f3ria linear\n\nno tamanho do conjunto de treinamento, tornando poss\u00edvel o treinamento de\n\nenormes conjuntos de testes de forma eficiente. Utilizamos nesse trabalho o\n\nalgoritmo Sequential Minimal Optimization para o treinamento das m\u00e1quinas\nde vetores de suporte.\n\n19\n\n\n\nCAP\u00cdTULO\n\n3\nTrabalhos Correlatos\n\nV\u00e1rios projetos que utilizam t\u00e9cnicas de vis\u00e3o computacional direcionados\n\n\u00e0 agricultura v\u00eam sendo implementados. Pesquisas t\u00eam destacado as\n\npossibilidades da utiliza\u00e7\u00e3o de sistemas de vis\u00e3o em \u00e1reas da agricultura\n\ncomo an\u00e1lise do comportamento animal, agricultura de precis\u00e3o e orienta\u00e7\u00e3o\n\ndas m\u00e1quinas, silvicultura, an\u00e1lise de medida e crescimento das planta\u00e7\u00f5es\n\n[35]. Diversos trabalhos v\u00eam sendo realizados recentemente na identifica\u00e7\u00e3o\n\ne classifica\u00e7\u00e3o das ervas daninhas.\n\nNo trabalho de Herrera et al. [3] foram usados descritores de forma e\n\nFuzzy Decision-Making para reconhecimento e classifica\u00e7\u00e3o de ervas\ndaninhas entre gram\u00edneas e folhas largas. O conjunto de descritores de\n\nforma foi composto pelos sete momentos de Hu e seis descritores\n\ngeom\u00e9tricos: per\u00edmetro, di\u00e2metro, comprimento do menor eixo, comprimento\n\ndo maior eixo, excentricidade e \u00e1rea. M\u00e1quinas de vetores de suporte foram\n\nutilizadas para compara\u00e7\u00e3o dos resultados, dado seu sucesso no\n\nreconhecimento de ervas daninhas em conjunto com extratores de cor, forma\n\ne textura. Das 66 imagens avaliadas, 28 apresentavam ambos tipos de ervas\n\ndaninhas, 19 apenas gram\u00edneas e 19 apenas folhas largas. O trabalho obteve\n\ncomo melhores resultados 85.8% de classifica\u00e7\u00e3o utilizando todos os\n\nextratores e 92.9% utilizando o melhor conjunto de extratores avaliado,\n\ncompostos por tr\u00eas momentos de Hu e o comprimento do maior eixo.\n\nA utiliza\u00e7\u00e3o de m\u00e1quina de vetores de suporte vem sendo adotada e\n\nconseguindo bons resultados para classifica\u00e7\u00e3o e discrimina\u00e7\u00e3o de ervas\n\ndaninhas. Ahmed et al. [37] utilizou m\u00e1quina de vetores de suporte em\n\nconjunto com extratores de cor, forma e momentos invariantes da imagem,\n\nem um total de catorze atributos. Atrav\u00e9s de valida\u00e7\u00e3o cruzada foi verificada\n\n20\n\n\n\na classifica\u00e7\u00e3o de seis esp\u00e9cies de ervas daninhas. Esse trabalho alcan\u00e7ou\n\n97.3% de precis\u00e3o com a melhor combina\u00e7\u00e3o de extratores avaliados em um\n\nconjunto de 224 imagens, onde cada uma das seis esp\u00e9cies analisadas\n\npossu\u00eda entre 31 e 45 imagens.\n\nSiddiqi et al [5] realizaram a classifica\u00e7\u00e3o de ervas daninhas em tr\u00eas\n\nclasses: gram\u00edneas, folhas largas e desconhecido, classe constitu\u00edda por\n\nimagens que n\u00e3o representavam ervas daninhas. Foi avaliado um conjunto\n\ncomposto por 1200 imagens, sendo 500 imagens de gram\u00edneas, 500 imagens\n\nde folhas largas e 200 imagens definidas como desconhecido. Dessas\n\nimagens, 600 foram reservadas para o treinamento e as 600 restantes para\n\nos testes. Utilizando m\u00e1quinas de vetores de suporte, o trabalho atingiu\n\n98.1% de precis\u00e3o na classifica\u00e7\u00e3o m\u00e9dia destas tr\u00eas classes.\n\nTellaeche et al. [38] tamb\u00e9m adotaram m\u00e1quinas de vetores de suporte na\n\nidentifica\u00e7\u00e3o de ervas daninhas. O foco deste trabalho consistiu em\n\nsegmentar a imagem em c\u00e9lulas e, baseado na quantidade de ervas daninhas\n\npresentes na c\u00e9lula, determinar a necessidade da aplica\u00e7\u00e3o de herbicida. O\n\nprocesso foi composto por uma fase de segmenta\u00e7\u00e3o, baseada no\n\nconhecimento pr\u00e9vio das faixas de cultura da imagem a ser analisada, e pela\n\nfase de classifica\u00e7\u00e3o utilizando m\u00e1quinas de vetores de suporte. Foram\n\nanalisadas um total de 86 imagens e 3096 c\u00e9lulas, com o objetivo de verificar\n\nse a c\u00e9lula em quest\u00e3o deveria receber a aplica\u00e7\u00e3o do herbicida. Os melhores\n\nresultados obtidos foram 85% de porcentagem de classifica\u00e7\u00e3o correta.\n\nNo trabalho de Saha et al. [39] foi utilizado m\u00e1quina de vetores de suporte\n\nno desenvolvimento de um sistema de detec\u00e7\u00e3o de ervas daninhas. O sistema\n\nproposto utiliza os passos de segmenta\u00e7\u00e3o, extra\u00e7\u00e3o de atributos e\n\nclassifica\u00e7\u00e3o. Para o treinamento e testes foi utilizado um conjunto de\n\nimagens composto por 60 imagens de lavouras de cenouras. Esse conjunto\n\nfoi dividido em 40 imagens para treinamento e 20 imagens para os testes. As\n\n20 imagens de testes foram divididas em quatro grupos e avaliadas\n\nseparadamente. No total foram localizadas 1780 regi\u00f5es dentro das 20\n\nimagens. O sistema proposto conseguiu identificar as regi\u00f5es de plantas e\n\nervas daninhas nas imagens com sucesso de 96.37% e teve uma precis\u00e3o de\n\n88.99% na detec\u00e7\u00e3o de ervas daninhas.\n\nIshak et al. [40] utilizaram uma combina\u00e7\u00e3o de Gabor wavelet e gradient\nfield distribution para obter um conjunto de vetores de atributos que\npermitisse a classifica\u00e7\u00e3o das ervas daninhas como gram\u00edneas e folhas\n\nlargas. O classificador aplicado foi uma rede neural artificial SLP (single layer\nperceptron). Um conjunto com 100 imagens de gram\u00edneas e 100 imagens de\nervas daninhas de folhas largas foi utilizado para o treinamento. Um total de\n\n400 imagens, sendo 200 imagens de gram\u00edneas e 200 de folhas largas, com\n\n21\n\n\n\ndiferentes condi\u00e7\u00f5es de ilumina\u00e7\u00e3o, foram utilizadas para testar a\n\nperformance da rede. A rede conseguiu 93.75% de precis\u00e3o m\u00e9dia na\n\nclassifica\u00e7\u00e3o dos dois tipos de ervas daninhas.\n\nHung et al. [41] utilizou aprendizagem autom\u00e1tica de atributos com\n\nsparse autoencoders para classificar ervas daninhas em imagens capturadas\npor VANTs. Essas imagens foram utilizadas na cria\u00e7\u00e3o de mosaicos que\n\nforam analisados para a detec\u00e7\u00e3o das ervas daninhas. Foram realizados\n\ntestes de detec\u00e7\u00e3o com tr\u00eas esp\u00e9cies diferentes de ervas daninhas. Embora\n\ntenham sido usadas as mesmas configura\u00e7\u00f5es para os algoritmos utilizados\n\nnas tr\u00eas esp\u00e9cies de ervas daninhas, a performance da rede teve percept\u00edvel\n\ndiferen\u00e7a de desempenho nos testes. Como melhor resultado, foi alcan\u00e7ada\n\nprecis\u00e3o de 72,2%, 92,9% e 94.3% para as tr\u00eas diferentes esp\u00e9cies de ervas\n\ndaninhas testadas.\n\nEm rela\u00e7\u00e3o aos trabalhos utilizando VANTs, um novo mercado tem sido\n\ncriado com um grande potencial de expans\u00e3o nos pr\u00f3ximos anos [42], fazendo\n\ncom que in\u00fameras pesquisas relacionadas ao uso de VANTs na agricultura\n\nvenham sendo realizadas. Pe\u00f1a et al. [43] destacam o grande potencial do uso\n\nde imagens capturadas por VANTs no tratamento de ervas daninhas.\n\nEntre os trabalhos envolvendo a utiliza\u00e7\u00e3o de VANTs na agricultura\n\npodemos citar Costa et al. [44], que utilizaram modelos equipados com\n\nsensores sem fio para realizar a aplica\u00e7\u00e3o de pesticida e fertilizantes. O\n\nobjetivo dessa abordagem foi lidar com poss\u00edveis problemas da aplica\u00e7\u00e3o\n\nutilizando aeronaves, causados por fatores como condi\u00e7\u00f5es clim\u00e1ticas. A\n\ndire\u00e7\u00e3o e intensidade do vento, por exemplo, pode ser um fator de dif\u00edcil\n\ncontrole na precis\u00e3o da aplica\u00e7\u00e3o desses produtos por aeronaves, tornando o\n\nuso de VANTs uma op\u00e7\u00e3o atraente para esse trabalho.\n\nO trabalho de Primicerio et al. [45] utilizou um VANT modelo VIPtero no\n\naux\u00edlio de uma aplica\u00e7\u00e3o direcionada a agricultura de precis\u00e3o em um\n\nvinhedo na It\u00e1lia Central. Como conclus\u00e3o do seu trabalho, eles apontaram\n\nque a aplica\u00e7\u00e3o da tecnologia no setor da agricultura pode melhorar\n\nsignificativamente a efici\u00eancia, sustentabilidade ambiental e os lucros do\n\nagricultor. Tamb\u00e9m afirmam que embora melhorias sejam necess\u00e1rias, os\n\nresultados preliminares foram animadores.\n\nTorres-S\u00e1nchez et al. [46] realizaram um estudo fornecendo o\n\ndetalhamento das especifica\u00e7\u00f5es e configura\u00e7\u00f5es t\u00e9cnicas de um drone\n\nutilizado com o objetivo de capturar imagens de ervas daninhas, focando no\n\nplano de miss\u00e3o, v\u00f4o, captura e pr\u00e9-processamento das imagens. Foram\n\nrealizados v\u00e1rios testes com diferentes c\u00e2meras RGB e multiespectrais em\n\ndiferentes alturas de v\u00f4o. A conclus\u00e3o \u00e9 que essas configura\u00e7\u00f5es s\u00e3o\n\ndependentes de objetivos espec\u00edficos. Para discrimina\u00e7\u00e3o individual de ervas\n\n22\n\n\n\ndaninhas \u00e9 necess\u00e1rio que os v\u00f4os sejam realizados a uma altura inferior a\n\n100 metros. Tamb\u00e9m foi verificado que em v\u00f4os com altura superior a 30\n\nmetros, pixels da cultura e ervas daninhas tem valores espectrais similares, o\n\nque aumenta a chance de erros na detec\u00e7\u00e3o.\n\nGomez-Candon et al. [47] avaliaram a precis\u00e3o dos mosaicos gerados\n\natrav\u00e9s das imagens capturadas por VANTs. Esses mosaicos s\u00e3o de grande\n\nimport\u00e2ncia para representar com precis\u00e3o toda a planta\u00e7\u00e3o monitorada,\n\npossibilitando a discrimina\u00e7\u00e3o da cultura e ervas daninhas. Eles conclu\u00edram\n\nque a altura de v\u00f4o \u00e9 um importante par\u00e2metro para se avaliar na utiliza\u00e7\u00e3o\n\nde VANTs na aquisi\u00e7\u00e3o de imagens para detec\u00e7\u00e3o de ervas daninhas. Dois\n\nfatores s\u00e3o cruciais para se determinar essa altura: a resolu\u00e7\u00e3o necess\u00e1ria\n\npara se obter uma imagem com qualidade suficiente para se discriminar a\n\ncultura das ervas daninhas e o n\u00famero de imagens necess\u00e1rias para se\n\nrepresentar o mosaico. Um grande n\u00famero de imagens pode tornar o\n\nprocesso de cria\u00e7\u00e3o do mosaico mais dif\u00edcil de ser realizado. A autonomia da\n\nbateria tamb\u00e9m \u00e9 um fator limitante a ser considerado.\n\nEste trabalho apresenta uma abordagem mais ampla em rela\u00e7\u00e3o aos\n\ntrabalhos citados. Al\u00e9m de classificar as imagens separadas por classes,\n\ncomo alguns dos trabalhos mencionados, foi constru\u00eddo o software\n\nPynovis\u00e3o. Este software, atrav\u00e9s da utiliza\u00e7\u00e3o da segmenta\u00e7\u00e3o do algoritmo\n\nSLIC Superpixels, permitiu realizar a detec\u00e7\u00e3o das ervas daninhas em\n\nimagens de planta\u00e7\u00e3o capturadas por VANTs, retornando uma classifica\u00e7\u00e3o\n\nvisual e dados quantitativos sobre a presen\u00e7a de ervas daninhas na \u00e1rea\n\nfotografada. O Pynovis\u00e3o tamb\u00e9m permitiu a constru\u00e7\u00e3o de um banco de\n\nimagens robusto, contendo mais de quinze mil imagens, um valor\n\nsignificativamente superior aos dos trabalhos citados. Por fim, este trabalho\n\nrealiza a classifica\u00e7\u00e3o utilizando redes neurais convolucionais, uma\n\narquitetura que v\u00eam conseguindo excelentes resultados no reconhecimento\n\nde imagens, e faz a compara\u00e7\u00e3o dos resultados com classificadores que v\u00eam\n\nsendo tradicionalmente adotados com sucesso no problema de detec\u00e7\u00e3o e\n\ndiscrimina\u00e7\u00e3o de ervas daninhas, como m\u00e1quinas de vetores de suporte.\n\n23\n\n\n\nCAP\u00cdTULO\n\n4\nAprendizado Profundo\n\nAprendizado Profundo (Deep Learning) \u00e9 uma nova \u00e1rea de pesquisa de\naprendizado de m\u00e1quina que foi apresentada com a inten\u00e7\u00e3o de aproxim\u00e1-lo\n\nde um dos seus objetivos originais: a intelig\u00eancia artificial [48]. Deng e Yu\n\n[49], entre v\u00e1rias defini\u00e7\u00f5es, definem aprendizado profundo como uma classe\n\nde t\u00e9cnicas de aprendizado de m\u00e1quina que exploram muitas camadas de\n\nprocessamento de informa\u00e7\u00e3o n\u00e3o linear para extra\u00e7\u00e3o e transforma\u00e7\u00e3o\n\nsupervisionada ou n\u00e3o-supervisionada e para an\u00e1lise de padr\u00f5es e\n\nclassifica\u00e7\u00e3o. V\u00e1rios estudos v\u00eam demonstrando a efici\u00eancia do aprendizado\n\nprofundo em uma grande variedade de aplica\u00e7\u00f5es. Podemos citar o seu uso\n\nem aplica\u00e7\u00f5es de reconhecimento facial, reconhecimento e detec\u00e7\u00e3o de fala,\n\nreconhecimento de objetos em geral, processamento de linguagem e rob\u00f3tica.\n\nO interesse em aprendizado profundo n\u00e3o tem se limitado \u00e0 pesquisa na \u00e1rea\n\nacad\u00eamica, sendo tamb\u00e9m objeto de interesse do DARPA (Defense Advanced\nResearch Projects Agency) que anunciou um projeto de pesquisa focado\nexclusivamente na \u00e1rea [50].\n\nA sua aplica\u00e7\u00e3o na \u00e1rea de vis\u00e3o computacional tem alcan\u00e7ado um not\u00e1vel\n\nprogresso nos \u00faltimos anos, em especial no campo do reconhecimento de\n\nobjetos. Vis\u00e3o computacional pode ser considerada a segunda \u00e1rea onde a\n\naplica\u00e7\u00e3o das t\u00e9cnicas de aprendizado profundo foi utilizada com sucesso,\n\nseguindo o reconhecimento de fala. Durante muitos anos o reconhecimento\n\nde imagem em vis\u00e3o computacional ficou dependente de t\u00e9cnicas como SIFT\n\n(Scale Invariant Feature Transform) e HOG (Histogram of Oriented Gradients).\nEntretanto essas t\u00e9cnicas tem maior facilidade em capturar baixo n\u00edvel de\n\ninforma\u00e7\u00e3o, apresentando dificuldades para capturar maior n\u00edvel de\n\ninforma\u00e7\u00e3o como intersec\u00e7\u00e3o de bordas ou identificar partes de objetos. O\n\n24\n\n\n\naprendizado profundo visa superar essas dificuldades atrav\u00e9s de aprendizado\n\nsupervisionado e n\u00e3o-supervisionado dos dados da imagem [49].\n\nV\u00e1rios trabalhos v\u00eam sendo realizados e gradativamente provando a\n\nefic\u00e1cia do uso de aprendizado profundo no reconhecimento de imagens.\n\nCiresan et al. [51] utilizando uma arquitetura de redes neurais artificiais\n\nprofundas conseguiram bater a performance humana no reconhecimento de\n\nd\u00edgitos escritos a m\u00e3o e sinais de tr\u00e2nsito nos bancos de dados MNIST,\n\nNORB, entre outros.\n\nPara alcan\u00e7ar esse objetivo foram utilizadas redes neurais convolucionais\n\ncom v\u00e1rias (6-10) camadas, cada uma delas contendo centenas de mapas.\n\nEsse n\u00famero de camadas \u00e9 compar\u00e1vel ao n\u00famero de camadas encontradas\n\nentre a retina e o c\u00f3rtex visual dos macacos [51]. A implementa\u00e7\u00e3o de c\u00f3digo\n\ncuidadosamente modelado para GPUs permitiu um ganho de velocidade de\n\n50-100 vezes em rela\u00e7\u00e3o a computadores tradicionais. Como resultado, esta\n\nimplementa\u00e7\u00e3o pela primeira vez conseguiu um resultado competitivo com\n\no reconhecimento humano em um grande conjunto de dados. Em muitos\n\nconjuntos de imagens o algoritmo melhorou o estado da arte em 30-80% [51].\n\nOutro avan\u00e7o not\u00e1vel do uso de aprendizado profundo na \u00e1rea de\n\nreconhecimento de imagens foi obtido na competi\u00e7\u00e3o ImageNet LSVRC de\n\n2012. A competi\u00e7\u00e3o consiste de um treinamento baseado em 1.2 milh\u00e3o de\n\nimagens em alta resolu\u00e7\u00e3o, para ent\u00e3o classificar 1000 diferentes classes de\n\nimagens desconhecidas. Logo ap\u00f3s a divulga\u00e7\u00e3o dos resultados obtidos nessa\n\ncompeti\u00e7\u00e3o, houve intenso estudo dessas arquiteturas em vis\u00e3o\n\ncomputacional [49].\n\nO feito em quest\u00e3o, alcan\u00e7ado por Krizhevsky et al. [52], utilizou\n\nabordagens similares ao trabalho de Ciresan et al. [51]. descrito\n\nanteriormente, com o uso de redes neurais convolucionais e GPUs para\n\notimizar o tempo de treinamento dos conjuntos. Entretanto o\n\nreconhecimento em imagens real\u00edsticas, utilizadas nesse caso, exigem\n\nconjuntos de treinamento muito superiores que os d\u00edgitos escritos a m\u00e3o ou\n\nsinais de tr\u00e2nsito. Os resultados alcan\u00e7ados mostraram que uma rede neural\n\nconvolucional profunda \u00e9 capaz de obter excelentes resultados em conjuntos\n\nde dados utilizando puramente aprendizado supervisionado. Tamb\u00e9m foi\n\nobservado que a retirada de uma \u00fanica camada reduz a performance da rede,\n\nmostrando que a profundidade da rede \u00e9 determinante para o alcance dos\n\nresultados. Avan\u00e7os nos anos seguintes, utilizando melhorias em abordagem\n\nsimilares [49] v\u00eam comprovando qu\u00e3o promissora \u00e9 a utiliza\u00e7\u00e3o de\n\naprendizado profundo no reconhecimento de imagens.\n\nO aprendizado profundo est\u00e1 produzindo avan\u00e7os em resolver problemas\n\nque h\u00e1 muito anos resistiam as melhores solu\u00e7\u00f5es da intelig\u00eancia artificial.\n\n25\n\n\n\nSua utiliza\u00e7\u00e3o est\u00e1 quebrando recordes em reconhecimento de imagem e fala\n\ne tem superado outras t\u00e9cnicas de aprendizado de m\u00e1quina em prever a\n\natividade de potenciais drogas moleculares, an\u00e1lise de dados de acelerador de\n\npart\u00edculas e reconstru\u00e7\u00e3o de circuitos cerebrais. Al\u00e9m disso est\u00e1 gerando\n\nresultados extremamente promissores para tarefas como processamento de\n\nlinguagem natural, an\u00e1lise de sentimentos, respostas a quest\u00f5es e tradu\u00e7\u00e3o\n\nde linguagens [4].\n\nEntre as v\u00e1rias arquiteturas de aprendizagem profunda, dois tipos de\n\nredes neurais profundas v\u00eam alcan\u00e7ando um not\u00e1vel destaque em suas\n\nprincipais \u00e1reas. Redes neurais recorrentes tem obtido grande sucesso na\n\nmanipula\u00e7\u00e3o de dados sequenciais como texto e processamento de linguagem\n\nnatural. As redes neurais convolucionais, principal foco deste trabalho, v\u00eam\n\ngerando grandes avan\u00e7os no reconhecimento de fala, processamento de\n\n\u00e1udio, v\u00eddeo e imagens [4].\n\n4.1 Redes Neurais Artificiais\n\nA motiva\u00e7\u00e3o no estudo de redes neurais tem origem na an\u00e1lise da maneira\n\nque o c\u00e9rebro humano realiza com velocidade uma infinidade de tarefas\n\ncomplexas como reconhecimento visual. O c\u00e9rebro \u00e9 estruturado como um\n\ncomputador paralelo, complexo e n\u00e3o-linear, com capacidade de organizar\n\nsuas estruturas b\u00e1sicas, os neur\u00f4nios, para realizar os c\u00e1lculos de forma\n\nr\u00e1pida e paralela [53]. Cada um desses neur\u00f4nios biol\u00f3gicos possuem\n\ncomplexidade e at\u00e9 mesmo velocidade similares a um microprocessador [54].\n\nOs cientistas est\u00e3o apenas come\u00e7ando a entender o funcionamento destas\n\nredes neurais biol\u00f3gicas [54]. \u00c9 comumente aceito que todas as fun\u00e7\u00f5es\n\nneurais biol\u00f3gicas s\u00e3o armazenadas nos neur\u00f4nios e nas conex\u00f5es entre eles.\n\nO processo de aprendizado \u00e9 definido como a cria\u00e7\u00e3o de novas conex\u00f5es entre\n\nos neur\u00f4nios e a modifica\u00e7\u00e3o das conex\u00f5es existente entre eles [54]. Baseado\n\nneste conhecimento foi modelado um conjunto de neur\u00f4nios artificiais que\n\nagrupados constituem as chamadas redes neurais artificiais.\n\nO entendimento moderno das redes neurais artificiais tem in\u00edcio em\n\nmeados de 1940 com o trabalho de Warren McCulloch e Walter Pitts [55], que\n\ndemonstraram que redes neurais artificiais poderiam calcular fun\u00e7\u00f5es\n\naritm\u00e9ticas ou l\u00f3gicas. Este trabalho \u00e9 frequentemente reconhecido como a\n\norigem do campo de pesquisa das redes neurais artificiais [54]. A primeira\n\naplica\u00e7\u00e3o pr\u00e1tica para redes neurais artificiais surgiu no final dos anos 50,\n\ncom a inven\u00e7\u00e3o da arquitetura perceptron e seu algoritmo de aprendizado por\nFrank Rosenblatt [56].\n\nInfelizmente as primeiras redes neurais sofreram de limita\u00e7\u00f5es que n\u00e3o\n\n26\n\n\n\npuderam ser superadas na \u00e9poca e, devido a problemas como o baixo poder\n\nde processamento dos computadores, v\u00e1rios pesquisadores abandonaram a\n\n\u00e1rea [54]. Por\u00e9m na d\u00e9cada de 1980, avan\u00e7os como o desenvolvimento do\n\nalgoritmo de backpropagation revigoraram o estudo de redes neurais artificiais\nque se mant\u00e9m em constante evolu\u00e7\u00e3o nas \u00faltimas d\u00e9cadas.\n\nUm neur\u00f4nio artificial \u00e9 a unidade fundamental b\u00e1sica de uma rede\n\nneural artificial. Ele \u00e9 definido por tr\u00eas elementos b\u00e1sicos: um conjunto de\n\nsinapses definidas como um peso, mais especificamente, um sinal de entrada\n\nx j conectado ao neur\u00f4nio k \u00e9 multiplicado pelo peso da sinapse wk j. O\n\nsegundo elemento \u00e9 um somador repons\u00e1vel pela adi\u00e7\u00e3o do resultado da\n\nmultiplica\u00e7\u00e3o dos sinais de entrada pelas sinapses do neur\u00f4nio. Por fim, o\n\nneur\u00f4nio possui uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o, que define a amplitude do sinal de\n\nsa\u00edda a um valor finito [53].\n\nEm termos matem\u00e1ticos o neur\u00f4nio pode ser descrito nas equa\u00e7\u00f5es abaixo:\n\nuk =\nm\n\n?\nj=1\n\nwk jx j (4.1)\n\nyk = ?(uk + bk) (4.2)\n\nonde x1, x2, ..., xm s\u00e3o os sinais de entrada, wk1, wk2, ..., wkm s\u00e3o os pesos\n\nsin\u00e1pticos do neur\u00f4nio k e bK corresponde ao bias, respons\u00e1vel por realizar o\n\ndeslocamento da fun\u00e7\u00e3o de ativa\u00e7\u00e3o, definida por ?(\u00b7).\nPara constituir uma rede neural, os neur\u00f4nios s\u00e3o agrupados em\n\nestruturas denominadas camadas. Essas camadas s\u00e3o geralmente\n\nagrupadas definindo a profundidade da rede neural. O modo como essas\n\ncamadas s\u00e3o agrupadas e a forma de aprendizado definem a arquitetura de\n\numa rede neural artificial. Entre as v\u00e1rias arquiteturas de redes neurais, as\n\nRedes Neurais Convolucionais t\u00eam como uma das caracter\u00edsticas principais o\n\nuso de mapas de convolu\u00e7\u00e3o como conjunto de pesos compartilhados entre\n\nos v\u00e1rios neur\u00f4nios das camadas de convolu\u00e7\u00e3o.\n\n4.2 Convolu\u00e7\u00e3o\n\nConvolu\u00e7\u00e3o \u00e9 uma opera\u00e7\u00e3o matem\u00e1tica entre duas fun\u00e7\u00f5es f e g,\n\nproduzindo uma terceira fun\u00e7\u00e3o, que pode ser interpretada como uma fun\u00e7\u00e3o\n\nmodificada de f. No processamento de imagens, onde a imagem \u00e9 definida\n\ncomo uma fun\u00e7\u00e3o bidimensional, a convolu\u00e7\u00e3o \u00e9 \u00fatil para detec\u00e7\u00e3o de\n\nbordas, suaviza\u00e7\u00e3o de imagem, extra\u00e7\u00e3o de atributos, entre outras aplica\u00e7\u00f5es\n\n[57].\n\nSejam as fun\u00e7\u00f5es f e g, para uma vari\u00e1vel cont\u00ednua x, a convolu\u00e7\u00e3o \u00e9\n\n27\n\n\n\ndefinida como:\n\nf (x)?g(x) =\n?\n\n?\n\n??\nf (?)\u00b7g(x??)d? (4.3)\n\nonde ? representa o operador de convolu\u00e7\u00e3o. Para as fun\u00e7\u00f5es f e g, quando\nx est\u00e1 definido no conjunto Z de inteiros, a equa\u00e7\u00e3o da convolu\u00e7\u00e3o discreta \u00e9\n\ndefinida como:\n\nf [x]?g[x] =\n?\n\n?\nn=??\n\nf [n]\u00b7g[x?n] (4.4)\n\nEstendendo esta defini\u00e7\u00e3o para fun\u00e7\u00f5es com duas vari\u00e1veis x e y, obt\u00e9m-se\n\nas seguintes equa\u00e7\u00f5es:\n\nf (x,y)?g(x,y) =\n?\n\n?\n\n?1=??\n\n?\n?\n\n?2=??\nf (?1,?2)\u00b7g(x??1,y??2)d?1d?2 (4.5)\n\nf [x,y]?g[x,y] =\n?\n\n?\nn1=??\n\n?\n\n?\nn2=??\n\nf [n1,n2]\u00b7g[x?n1,y?n2] (4.6)\n\nA convolu\u00e7\u00e3o de uma imagem pode ser interpretada como o somat\u00f3rio da\n\nmultiplica\u00e7\u00e3o de cada elemento da imagem, junto com seus vizinhos locais,\n\npelos elementos da matriz que representa o filtro de convolu\u00e7\u00e3o. Esse c\u00e1lculo\n\n\u00e9 ilustrado na imagem 4.1.\n\nFigura 4.1: Exemplo de aplica\u00e7\u00e3o da convolu\u00e7\u00e3o na imagem. \u00c0 esquerda, uma\nimagem hipot\u00e9tica representada por um \u00fanico canal com dimens\u00f5es 5x5 que recebe a\naplica\u00e7\u00e3o de um filtro 3x3. \u00c1 direita, uma matriz ilustrando o somat\u00f3rio que fornece\no resultado da convolu\u00e7\u00e3o.\n\nNa abordagem utilizada nesse exemplo, ap\u00f3s a aplica\u00e7\u00e3o do filtro de\n\n28\n\n\n\nconvolu\u00e7\u00e3o a imagem original \u00e9 reduzida em tamanho proporcional \u00e0s\n\ndimens\u00f5es do filtro utilizado. Todavia existem outras abordagens de\n\naplica\u00e7\u00e3o utilizando, por exemplo, cria\u00e7\u00e3o de novas c\u00e9lulas adjacentes \u00e0s\n\nbordas da imagem, fazendo com que a imagem resultante mantenha as\n\nmesmas dimens\u00f5es da imagem original ap\u00f3s a aplica\u00e7\u00e3o do filtro de\n\nconvolu\u00e7\u00e3o.\n\n4.3 Redes Neurais Convolucionais\n\nEm meados de 2006, um grupo de pesquisadores do Canadian Institute for\nAdvanced Research (CIFAR), utilizando redes neurais artificiais, introduziram\nprocedimentos de aprendizagem n\u00e3o supervisionada que poderiam criar\n\ncamadas de detec\u00e7\u00e3o de atributos sem necessitarem de informa\u00e7\u00e3o\n\npr\u00e9-rotulada [4]. A primeira grande aplica\u00e7\u00e3o desta abordagem de\n\ntreinamento foi o reconhecimento de fala, sendo poss\u00edvel gra\u00e7as \u00e0s novas\n\nGPUs que permitiram os pesquisadores realizarem o treinamento 10 a 20\n\nvezes mais r\u00e1pido. Em 2009, esse novo modelo de treinamento quebrou\n\nrecordes em um benchmark de reconhecimento de fala que usava um\n\npequeno vocabul\u00e1rio e foi rapidamente modificado para alcan\u00e7ar excelentes\n\nresultados com um vocabul\u00e1rio extenso.\n\nTodavia, embora redes neurais geralmente tenham sido consideradas\n\ndif\u00edceis de serem bem treinadas [58], um tipo particular de rede neural\n\nprofunda se mostrou muito mais f\u00e1cil de treinar e generalizar do que redes\n\ncompletamente conectadas: as Redes Neurais Convolucionais, que\n\nalcan\u00e7aram um not\u00e1vel sucesso pr\u00e1tico e t\u00eam sido largamente adotadas\n\nrecentemente pela comunidade de Vis\u00e3o Computacional [4].\n\nFigura 4.2: Exemplo de aplica\u00e7\u00e3o de um filtro de convolu\u00e7\u00e3o sobre uma imagem de\nsoja, em tons de cinza. O mapa de convolu\u00e7\u00e3o utilizado visa identificar as bordas da\nimagem.\n\nRedes Neurais Convolucionais ou ConvNets foram inspiradas na estrutura\ndo sistema visual. Os primeiros modelos computacionais baseados nestas\n\nconectividades locais entre neur\u00f4nios e em transforma\u00e7\u00f5es da imagens\n\n29\n\n\n\norganizadas hierarquicamente s\u00e3o encontradas no Neocognitron de\n\nFukushima [59]. Posteriormente LeCun, usando a arquitetura de Redes\n\nConvolucionais, alcan\u00e7ou o estado da arte em v\u00e1rias tarefas de\n\nreconhecimento de imagens [60]. O entendimento moderno da fisiologia do\n\nsistema visual \u00e9 consistente com o estilo de processamento encontrado nas\n\nredes convolucionais [58]. Em alguns casos, filtros de Gabor t\u00eam sido\n\nutilizados como um pr\u00e9-processamento inicial para emular a resposta visual\n\nhumana \u00e0s percep\u00e7\u00f5es visuais [50].\n\nRedes Neurais Convolucionais Profundas foram o primeiro sucesso\n\nconfi\u00e1vel de treinamento onde m\u00faltiplas camadas de uma hierarquia foram\n\ntreinadas de maneira robusta. Elas constituem uma escolha de topologia ou\n\narquitetura projetadas para reduzir o n\u00famero de par\u00e2metros a serem\n\naprendidos otimizando o tempo de treinamento atrav\u00e9s de backpropagation.\nConvNets s\u00e3o projetadas para processar dados armazenados na forma de\nm\u00faltiplas matrizes de uma dimens\u00e3o para sinais e sequ\u00eancias, inclu\u00edndo\n\nlinguagens, duas dimens\u00f5es para imagens e espectogramas e tr\u00eas dimens\u00f5es\n\npara v\u00eddeos e imagens volum\u00e9tricas.\n\nA Rede Convolucional proposta por LeCun, em 1989, era organizada em\n\ndois tipo de camadas, camadas convolucionais e camada de subsampling.\nCada camada possui uma estrutura topogr\u00e1fica, ou seja, cada neur\u00f4nio \u00e9\n\nassociado com um posi\u00e7\u00e3o bidimensional da imagem de entrada junto com\n\num campo receptivo. Em cada localiza\u00e7\u00e3o de cada camada h\u00e1 um n\u00famero\n\ndiferente de neur\u00f4nios, cada um com seu pr\u00f3prio conjunto de par\u00e2metros,\n\nassociados com os neur\u00f4nios de uma mapa retangular da camada anterior. O\n\nmesmo conjunto de par\u00e2metros, mas com uma diferente regi\u00e3o, \u00e9 associado\n\ncom neur\u00f4nios de diferentes localiza\u00e7\u00f5es.\n\nFigura 4.3: Aplica\u00e7\u00e3o de max pooling em uma imagem 4x4 utilizando um\nfiltro 2x2. Al\u00e9m de reduzir o tamanho da imagem, consequentemente reduzindo\no processamento para as pr\u00f3ximas camadas, essa t\u00e9cnica tamb\u00e9m auxilia no\ntratamento de invari\u00e2ncias locais.\n\nAtualmente uma t\u00edpica arquitetura de uma Rede Convolucional \u00e9 dividida\n\n30\n\n\n\nem uma s\u00e9rie de est\u00e1gios. Os primeiros est\u00e1gios s\u00e3o compostos de dois tipos\n\nde camadas, as camadas de convolu\u00e7\u00e3o e as camadas de pooling [4]. A\ncamada de convolu\u00e7\u00e3o consiste em mapas de atributos, similares ao\n\ndemonstrado na Figura 4.2, conectados a cada unidade da camada anterior\n\natrav\u00e9s de um conjunto de par\u00e2metros compartilhados entre todas as\n\nunidades e possui ReLUs (Rectified Linear Units), neur\u00f4nios com fun\u00e7\u00e3o de\nativa\u00e7\u00e3o definida como a n\u00e3o-linearidade na forma descrita na Equa\u00e7\u00e3o 4.7:\n\nf (x) = max(0,x) (4.7)\n\naplicados na sa\u00edda de cada camada convolucional [52].\n\nAs camadas de pooling s\u00e3o uma forma de down-sampling. Uma t\u00edpica\ncamada de pooling computa o m\u00e1ximo local de uma determinada regi\u00e3o do\nmapa de atributos, como pode ser visto na Figura 4.3. Elas s\u00e3o \u00fateis por\n\neliminar valores n\u00e3o m\u00e1ximos, reduzindo a dimens\u00e3o da representa\u00e7\u00e3o dos\n\ndados e consequentemente a computa\u00e7\u00e3o necess\u00e1ria para as pr\u00f3ximas\n\ncamadas, al\u00e9m de criar uma invari\u00e2ncia a pequenas mudan\u00e7as e distor\u00e7\u00f5es\n\nlocais. Dois ou tr\u00eas est\u00e1gios de convolu\u00e7\u00e3o, n\u00e3o-linearidade e pooling s\u00e3o\nempilhados, seguidos por mais camadas de convolu\u00e7\u00e3o e camadas\n\ncompletamente conectadas. As camadas convolucionais e de pooling s\u00e3o\ndiretamente inspiradas por no\u00e7\u00f5es cl\u00e1ssicas de c\u00e9lulas simples e c\u00e9lulas\n\ncomplexas na neuroci\u00eancia visual [4].\n\nFigura 4.4: Arquitetura tradicional das Redes Neurais Convolucionais. Os primeiros\nest\u00e1gios s\u00e3o compostos de camadas de convolu\u00e7\u00e3o e pooling. As \u00faltimas camadas s\u00e3o\ncompletamente conexas.\n\nA arquitetura utilizada por Krizhevsky no ImageNet LSVRC de 2012,\n\nconsiderada o marco na utiliza\u00e7\u00e3o de Redes Neurais Convolucionais para o\n\nreconhecimento de imagens, consistia de 8 camadas. As 5 primeiras\n\ncamadas s\u00e3o convolucionais e as 3 \u00faltimas s\u00e3o camadas completamente\n\nconexas. A sa\u00edda da \u00faltima camada alimenta uma 1000-way softmax, que\nproduz a distribui\u00e7\u00e3o probabil\u00edstica sobre as 1000 classes utilizadas na\n\ncompeti\u00e7\u00e3o. As n\u00e3o-linearidades ReLUs foram utilizadas na sa\u00edda de todas as\n\ncamadas convolucionais e completamente conexas [52]. As imagens originais\n\n31\n\n\n\nfornecidas para o treinamento foram acrescidas das reflex\u00f5es horizontais das\n\nmesmas e recortes das imagens originais e suas reflex\u00f5es, para ampliar o\n\nconjunto de treinamento. Al\u00e9m disso foram alteradas as intensidades dos\n\ncanais RGB das imagens. Para reduzir o tempo de treinamento deste\n\nconjunto de imagens e evitar o overfitting, foi utilizada a t\u00e9cnica de dropout,\nque consiste de remover aleatoriamente metade dos neur\u00f4nios das camadas\n\nocultas a cada itera\u00e7\u00e3o de treinamento, readicionando-os na itera\u00e7\u00e3o\n\nseguinte. Essa t\u00e9cnica tamb\u00e9m d\u00e1 \u00e0 rede habilidade de aprender atributos\n\nmais robustos, j\u00e1 que um neur\u00f4nio n\u00e3o pode depender da presen\u00e7a espec\u00edfica\n\nde outros neur\u00f4nios.\n\nA rede proposta por Krizhevsky inspirou dezenas de outras Redes\n\nConvolucionais para reconhecimento de imagens, algumas redes bem mais\n\nprofundas, possu\u00edndo um n\u00famero significativamente superior de camadas,\n\ncomo a GoogLeNet, uma arquitetura com 22 camadas, que venceu o\nImageNet LSVRC de 2014 [61]. Recentes arquiteturas de Redes Neurais\n\nConvolucionais possuem de 10 a 20 camadas, centenas de milh\u00f5es de pesos\n\ne bilh\u00f5es de conex\u00f5es. Gra\u00e7as ao progresso em hardware, software e\n\nalgoritmos paralelos, redes como estas, que levariam semanas para serem\n\ntreinadas, hoje podem ser treinadas em quest\u00e3o de horas [4].\n\nAl\u00e9m da utiliza\u00e7\u00e3o no reconhecimento de imagens, varia\u00e7\u00f5es de Redes\n\nNeurais Convolucionais tamb\u00e9m v\u00eam obtendo excelentes resultados em\n\noutras \u00e1reas. Kalchbrenner et al. [62] utilizaram Redes Neurais\n\nConvolucionais Din\u00e2micas para uma s\u00e9rie de tarefas de Processamento\n\nNatural de Linguagens como an\u00e1lise de sentimentos e classifica\u00e7\u00f5es de tipos\n\nde perguntas. Dos Santos e Gatti [63] alcan\u00e7aram o estado de arte no\n\nStanford Sentiment Treebank, que cont\u00e9m avalia\u00e7\u00f5es de filmes, em\nclassifica\u00e7\u00e3o bin\u00e1rias de frases como positivo ou negativo, utilizando Redes\n\nNeurais Convolucionais Profundas. AtomNet \u00e9 uma Rede Convolucional\n\nprofunda projetada para predi\u00e7\u00e3o da bioatividade de pequenas mol\u00e9culas\n\nvisando a descoberta de drogas, conseguindo excelentes resultados [64].\n\nAl\u00e9m disso, combina\u00e7\u00f5es de Redes Convolucionais Neurais e Rede Neurais\n\nRecorrentes est\u00e3o possibilitando novos mecanismos para a gera\u00e7\u00e3o de\n\nlegendas a partir de imagens [4].\n\n32\n\n\n\nCAP\u00cdTULO\n\n5\nMetodologia\n\n5.1 Vis\u00e3o Geral\n\nA abordagem proposta neste trabalho para a detec\u00e7\u00e3o de ervas daninhas \u00e9\n\ncomposta por cinco fases. A primeira fase consiste na captura de imagens de\n\nplanta\u00e7\u00e3o de soja, para a qual utilizamos VANTs. A segunda fase \u00e9 a\n\nsegmenta\u00e7\u00e3o destas imagens utilizando o algoritmo de superpixels, cujos\n\nsegmentos extra\u00eddos foram anotados manualmente e utilizados na\n\nconstru\u00e7\u00e3o de um banco de imagens de soja e ervas daninhas. Na terceira\n\nfase, exclusiva para os classificadores que ser\u00e3o utilizados na compara\u00e7\u00e3o \u00e0\n\nperformance das ConvNets, realizamos a extra\u00e7\u00e3o do vetor de atributos dos\nsegmentos do banco de imagem com uma cole\u00e7\u00e3o de extratores de cor, forma\n\ne textura.\n\nFigura 5.1: Fluxograma da metodologia: (1) Captura de imagens. (2) Segmenta\u00e7\u00e3o.\n(3) Extra\u00e7\u00e3o de atributos. (4) Treinamento. (5) Classifica\u00e7\u00e3o.\n\nA quarta fase consiste do treinamento dos classificadores. As redes\n\nneurais realizam este treinamento utilizando os segmentos do banco de\n\nimagens \u00e0 medida que os outros classificadores utilizam o vetor de atributos\n\n33\n\n\n\nobtido na terceira fase deste processo. A \u00faltima fase consiste da segmenta\u00e7\u00e3o\n\ne classifica\u00e7\u00e3o da imagem de uma planta\u00e7\u00e3o de soja, retornando dados\n\nquantitativos relativos \u00e0 presen\u00e7a de ervas daninha na imagem.\n\n5.2 Plantio da Soja\n\nFoi conduzida uma planta\u00e7\u00e3o experimental na fazenda S\u00e3o Jos\u00e9,\n\nlocalizada sob as coordenadas geogr\u00e1ficas de latitude 20?24\u20199.88\"S e\n\nlongitude 54?36\u201931.49\"O, em uma \u00e1rea de uma hectare. Este experimento foi\n\nfeito para ser utilizado por v\u00e1rios projetos do grupo VANTAGRO, que\n\nenvolvem pesquisas visando a solu\u00e7\u00e3o de diversos problemas que ocorrem na\n\nsoja como ataque de doen\u00e7as, infesta\u00e7\u00e3o de insetos e infesta\u00e7\u00e3o de plantas\n\ndaninhas. Deste modo a planta\u00e7\u00e3o foi feita com quatro quadrantes distintos,\n\nconforme Figura 5.2, com objetivo de contemplar cada um desses objetivos.\n\nFigura 5.2: Diagrama ilustrando o plantio na fazenda S\u00e3o Jos\u00e9. As letras L, E, P e\nD representam, respectivamente, os blocos reservados \u00e0 pesquisa de lagartas, ervas\ndaninhas, percevejos e doen\u00e7as.\n\nO plantio foi realizado em quadrantes com 4 tratamentos e 5 repeti\u00e7\u00f5es\n\n(parcelas 6x10m). Os quadrantes foram reservados \u00e0 pesquisa de lagartas,\n\nervas daninhas, percevejos e doen\u00e7as. A planta\u00e7\u00e3o experimental de soja\n\ntransg\u00eanica foi controlada no que tange as defici\u00eancias nutricionais e\n\ninfesta\u00e7\u00f5es de plantas daninhas para que fossem obtidas imagens que\n\npermitam a busca por padr\u00f5es visuais causados por defici\u00eancias espec\u00edficas e\n\ninfesta\u00e7\u00f5es.\n\n34\n\n\n\nO calend\u00e1rio do plantio pode ser visualizado nas Tabelas 5.1 e 5.2.\n\nEst\u00e1dio Denomina\u00e7\u00e3o Data Caracter\u00edsticas\n\nVE\nEmerg\u00eancia (1? ao\n\n7? dia)\n10/12 a\n16/12\n\nCotil\u00e9dones acima da\nsuperf\u00edcie do solo formando\num \u00e2ngulo de 90? com seus\nrespectivos hipoc\u00f3tilos.\n\nVC\nCotil\u00e9done (8? ao\n\n14? dia)\n17/12 a\n23/12\n\nCotil\u00e9dones completamente\nabertos e expandidos. As\nbordas de suas folhas\nunifolioladas n\u00e3o mais se\ntocam.\n\nV1\nPrimeiro n\u00f3 (15? ao\n\n21? dia)\n24/12 a\n30/12\n\nFolhas unifolioladas\ncompletamente abertas.\n\nV2\nSegundo n\u00f3 (a\n\npartir do 22? dia)\n31/12 a\n05/01\n\nPrimeira folha trifoliolada\naberta.\n\nV3 e V4\nTerceiro e quarto\n\nn\u00f3 (a partir do 29?\n\ndia)\n\n06/01 a\n12/01\n\nSegunda e terceira folha\ntrifoliolada aberta.\n\nV5\nQuinto n\u00f3 (a partir\n\ndo 36? dia)\n13/01 a\n19/01\n\nQuarta folha trifoliolada\naberta.\n\nV6 a V(n)\nEn\u00e9simo n\u00f3 (At\u00e9 o\n\n49? dia)\n20/01 a\n26/01\n\nDa quinta folha trifoliolada\naberta ao en\u00e9simo n\u00f3 ao\nlongo da haste principal com\ntrif\u00f3lio aberto.\n\nTabela 5.1: Calend\u00e1rio do plantio com as datas para os est\u00e1dios vegetativos\nda soja.\n\n35\n\n\n\nEst\u00e1dio Denomina\u00e7\u00e3o Data Caracter\u00edsticas\n\nR1\nFlorescimento (50?\n\nao 58? dia)\n27/01 a\n04/02\n\nIn\u00edcio da flora\u00e7\u00e3o: at\u00e9 50%\ndas plantas com flor.\n\nR2\nPleno\n\nFlorescimento (59?\n\nao 65? dia)\n\n05/02 a\n11/02\n\nFlora\u00e7\u00e3o plena: maioria dos\nracemos com flores abertas.\n\nR3\nIn\u00edcio da Forma\u00e7\u00e3o\nde Vagens (66? ao\n\n75? dia)\n\n13/02 a\n22/02\n\nFinal da flora\u00e7\u00e3o: flores e\nvagens com at\u00e9 1,5cm.\n\nR4\nPlena Forma\u00e7\u00e3o\n\ndas vagens (76? ao\n87? dia)\n\n23/02 a\n05/03\n\nMaioria das vagens no ter\u00e7o\nsuperior com 2-4cm.\n\nR5\n\nIn\u00edcio do\nenchimento das\nsementes (88? ao\n\n100? dia)\n\n06/03 a\n18/03\n\nR5.1. Gr\u00e3os percept\u00edveis ao\ntato a 10% da grana\u00e7\u00e3o;\nR5.2.Maioria das vagens\ncom grana\u00e7\u00e3o de 10%-25%;\nR5.3. Maioria das vagens\nentre 25-50% de grana\u00e7\u00e3o;\nR5.4. Maioria das vagens\nentre 50-75% de grana\u00e7\u00e3o, e\nR5.5. Maioria das vagens\nentre 75-100% de grana\u00e7\u00e3o.\n\nR6\n\nPleno do\nenchimento das\nvagens (101? ao\n\n111? dia)\n\n19/03 a\n29/03\n\nVagens com grana\u00e7\u00e3o de\n100% e folhas verdes.\n\nR7\nIn\u00edcio da\n\nmatura\u00e7\u00e3o (112?\n\nao 118? dia)\n\n30/03 a\n05/04\n\nR7.1. In\u00edcio: 50% de\namarelecimento de folhas e\nvagens\nR7.2. Entre 51-75% de\nfolhas e vagens amarelas, e\nR7.3. Mais de 76% de folhas\ne vagens amarelas.\n\nR8\nMatura\u00e7\u00e3o plena\n(119? ao 125? dia)\n\n06/04 a\n12/04\n\nR8.1. In\u00edcio a 50% de\ndesfolha, e\nR8.2. Mais de 50% de\ndesfolha \u00e0 pr\u00e9-colheita.\n\nR9 126? dia. 13/04\nR9. Ponto de matura\u00e7\u00e3o de\ncolheita.\n\nTabela 5.2: Calend\u00e1rio do plantio com as datas para os est\u00e1dios reprodutivos\nda soja.\n\n36\n\n\n\nAs aplica\u00e7\u00f5es foram realizadas conforme mostrado na Tabela 5.3, sendo\n\nque o \u2019X\u2019 indica quando as aplica\u00e7\u00f5es foram feitas e o produto utilizado.\n\nHerbicida\n\nGlifosato\n(3,5L/ha) +\n\nNimbus\n(600mL/ha)\n\nGramoxone\n(2L/ha) +\nNimbus\n\n(600mL/ha)\n\nGlifosato\n(2L/ha) +\nVerdict\n\n(0,5L/ha) +\nNimbus\n\n(600mL/ha)\n\nGlifosato\n(2L/ha) +\nVerdict\n\n(0,5L/ha) +\nNimbus\n\n(600mL/ha)\nTratamento /\n\n\u00c9poca de\nAplica\u00e7\u00e3o\n\n15-20 dias\nantes do\nplantio\n\n1-2 dias\nantes do\nplantio\n\nV2 / V4 Se precisar\n\n100% X X X X\n60% X X\n30% X\n0%\n\nTabela 5.3: Est\u00e1dios fenol\u00f3gicos em que foram realizadas as aplica\u00e7\u00f5es para o\nmanejo de plantas daninhas.\n\nNa Tabela 5.4 seguem as quantidades necess\u00e1rias para a condu\u00e7\u00e3o das\n\naplica\u00e7\u00f5es:\n\nItem\nDose\n\n(mL/ha)\nTamanho\nParcela\n\n(m2)\n\nVolume\npor\n\nParcela\n(mL)\n\nRepe-\nti\u00e7\u00f5es\n\nAplica-\n\u00e7\u00f5es\n\nVolume\nTotal\n(mL)\n\nVolume\ncom\n\nMargem\nde\n\nSeguran\u00e7a\n(mL)\n\nNimbus 600 100 6 5 27 810 972\nGlifosato 3500 100 35 5 3 525 630\nGlifosato 2000 100 20 5 2 200 240\n\nGramoxone\n2000 100 20 5 1 100 120\n\nVerdict 500 100 5 5 2 50 60\n\nTabela 5.4: Dose de cada item aplicado na planta\u00e7\u00e3o.\n\n5.3 Captura de imagens\n\n5.3.1 Materiais\n\nOs registros foram realizados em forma de imagem entre 3 e 6 metros de\n\naltura. Elas foram coletadas utilizando-se do equipamento DJI Phantom 3\n\nProfessional, com peso de 1.280 gramas e velocidade m\u00e1xima de 16 m/s. A\n\nbateria LiPo 4S 15.2 V tem autonomia de v\u00f4o de aproximadamente 23\n\n37\n\n\n\nminutos. O equipamento \u00e9 equipado com uma c\u00e2mera Sony EXMOR 1/2.3\",\n\n12.4 M (total de pixels: 12.76 M), lente FOV 94? 20 mm, suportando os\n\nformatos de arquivo FAT32/exFAT, JPEG, DNG, MP4 e MOV (MPEG-4\n\nAVC/H.264). Possui tamb\u00e9m um gimbal com estabiliza\u00e7\u00e3o nos 3 eixos e\n\nsuporte a Micro SD com capacidade m\u00e1xima de 64 GB.\n\nFigura 5.3: Imagem da lavoura de soja da Fazenda S\u00e3o Jos\u00e9, capturada pela c\u00e2mera\nSony EXMOR 1/2.3\"do VANT DJI Phantom 3 Professional, em dezembro de 2015. \u00c9\nnot\u00e1vel a presen\u00e7a de ervas daninhas na planta\u00e7\u00e3o.\n\nO delineamento foi realizado em bloco com 4 tratamentos e 5 repeti\u00e7\u00f5es.\n\nAntes do in\u00edcio do per\u00edodo de captura, cada bloco foi sinalizado com uma\n\nestaca de bambu de 1,3 metros de altura, conforme Figura 5.2. Os v\u00f4os\n\nforam realizados entre os meses de dezembro de 2015 e mar\u00e7o de 2016 pelo\n\nmenos uma vez por semana, geralmente no per\u00edodo das oito \u00e0s dez horas da\n\nmanh\u00e3. Foram realizadas coleta de imagens da planta\u00e7\u00e3o, sendo a propor\u00e7\u00e3o\n\nda imagem utilizada 4x3, com resolu\u00e7\u00e3o de 4000x3000px com todos os\n\npar\u00e2metros na configura\u00e7\u00e3o original de f\u00e1brica.\n\n5.3.2 Planejamento de voo\n\nAtrav\u00e9s do DJI GO APP \u00e9 poss\u00edvel utilizar no Phantom 3 cinco categorias\n\nde v\u00f4os inteligentes: Follow Me, Course Lock, Waypoints, Home Lock e Point of\nInterest. O plano original do v\u00f4o previa usar a categoria Waypoints, que\nconsiste em setar m\u00faltiplos pontos atrav\u00e9s do GPS, com o m\u00ednimo de 5\n\nmetros de dist\u00e2ncia entre pontos adjacentes, fazendo com que o Phantom\n\nsobrevoe o percurso pr\u00e9-definido enquanto efetuamos o controle manual do\n\ngimbal e da c\u00e2mera. Com isso poder\u00edamos certificar que todos os trajetos\n\nefetuados em cada trajet\u00f3ria respeitasse um padr\u00e3o pr\u00e9-definido. Todavia\n\nessa trajet\u00f3ria planejada para o v\u00f4o e o uso de Waypoints n\u00e3o foi utilizado\n\n38\n\n\n\ndurante as visitas devido \u00e0s limita\u00e7\u00f5es de tempo de v\u00f4o causadas por\n\nrestri\u00e7\u00f5es em rela\u00e7\u00e3o \u00e0 quantidade e autonomia das baterias do VANT. Sendo\n\nassim a captura de imagens foi realizada atrav\u00e9s de controle manual da\n\ntrajet\u00f3ria de v\u00f4o do Phantom.\n\n5.3.3 Banco de imagens\n\nFigura 5.4: Software Pynovis\u00e3o. Este software foi desenvolvido nesse trabalho e\nrealiza segmenta\u00e7\u00e3o, extra\u00e7\u00e3o de atributos e classifica\u00e7\u00e3o da imagem.\n\nAs imagens capturadas pelo VANT DJI Phantom 3 Professional foram\n\nseparadas por datas, como pode ser visto na Tabela 5.5.\n\nData 29/12 07/01 15/01 25/01 26/01 27/01 30/01\nImagens 248 77 157 126 19 108 525\n\nData 02/02 03/02 05/02 08/02 11/02 16/02 18/02\nImagens 61 106 265 173 189 202 79\n\nData 19/02 25/02 26/02 01/03 04/03 08/03 11/03 15/03\nImagens 257 226 247 268 314 335 250 402\n\nTabela 5.5: Coleta de imagens por dia.\n\nA partir desse conjunto foram selecionadas todas as imagens com\n\nocorr\u00eancia de ervas daninhas, resultando um total de 400 imagens. Atrav\u00e9s\n\ndo software Pynovis\u00e3o, mostrado na imagem 5.4, essas imagens foram\n\n39\n\n\n\nsegmentadas e os segmentos anotados manualmente com sua respectiva\n\nclasse. Esses segmentos foram utilizados na constru\u00e7\u00e3o do banco de imagens\n\ndos testes finais,\n\nForam anotados segmentos de cada imagem que identificassem de maneira\n\nbem definida uma das quatro classes utilizadas neste experimento: solo, soja,\n\nervas daninhas de folhas largas e gram\u00edneas. Dada a grande presen\u00e7a de soja\n\nnas imagens, v\u00e1rios segmentos de soja foram ignorados arbitrariamente, para\n\nevitar um desbalanceamento no banco1. As imagens do m\u00eas de janeiro de\n\n2016 n\u00e3o puderam ser aproveitadas devido \u00e0 baixa qualidade causada por\n\nproblemas configura\u00e7\u00e3o da c\u00e2mera do VANT DJI Phantom 3.\n\nO banco foi finalizado com 15336 segmentos, sendo 3249 de solo, 7376 de\n\nsoja, 3520 gram\u00edneas e 1191 de ervas daninhas de folhas largas. Na Tabela\n\n5.6 pode ser vista a divis\u00e3o de imagens selecionadas por data e o n\u00famero de\n\nsegmentos por classes.\n\nData Imagens Solo Soja Gram\u00edneas Folhas Largas Segmentos\n29/12/2015 22 541 53 152 509 1255\n05/02/2016 69 875 1997 682 140 3694\n16/02/2016 40 127 305 490 88 1010\n18/02/2016 12 48 288 341 36 713\n19/02/2016 38 315 935 373 4 1627\n25/02/2016 40 11 82 267 36 396\n26/02/2016 50 411 1791 128 158 2488\n01/03/2016 29 3 6 221 8 238\n04/03/2016 100 918 1919 866 212 3915\n\nTotal 400 3249 7376 3520 1191 15336\n\nTabela 5.6: Lista de imagens contendo ervas daninhas e a quantidade de\nsegmentos selecionados, por classe. As imagens do m\u00eas de janeiro de 2016\nn\u00e3o puderam ser aproveitadas por baixa qualidade.\n\n5.4 Segmenta\u00e7\u00e3o\n\nO algoritmo SLIC Superpixels [7] foi utilizado para fazer a segmenta\u00e7\u00e3o\n\nnas imagens e auxiliar na constru\u00e7\u00e3o do banco. Por padr\u00e3o, o \u00fanico\n\npar\u00e2metro de entrada do algoritmo SLIC Superpixels \u00e9 o n\u00famero de\n\nsuperpixels, de aproximadamente mesmo tamanho, K. Todavia,\n\nopcionalmente \u00e9 poss\u00edvel ajustar o par\u00e2metro compacidade que permite\n\ncontrolar a forma do superpixel tornando-a mais quadrada. Na\n\nimplementa\u00e7\u00e3o utilizada neste trabalho, dispon\u00edvel na biblioteca scikit-image\n\n1\u00c9 importante destacar que esta decis\u00e3o introduz uma artificialidade extra no banco de\nimagens.\n\n40\n\n\n\n[65], tamb\u00e9m \u00e9 poss\u00edvel configurar o par\u00e2metro sigma, que permite aplicar\n\numa suaviza\u00e7\u00e3o na imagem, utilizando filtros gaussianos, antes da\n\nsegmenta\u00e7\u00e3o.\n\nFigura 5.5: Da esquerda para a direita, temos a imagem segmentada com\nK = 300, 600 e 1200. Para o valor 1200 os segmentos englobam as classes de maneira\nbem definida. Todavia a quantidade de informa\u00e7\u00e3o relevante no segmento \u00e9 bastante\ninferior \u00e0 segmenta\u00e7\u00e3o com o valor 300.\n\nForam realizados v\u00e1rios testes para definir o valor de K. No nosso\n\nproblema o objetivo n\u00e3o \u00e9 segmentar a imagem a n\u00edvel de folha, mas separar\n\na imagem em segmentos que contivessem v\u00e1rias folhas de soja ou ervas\n\ndaninhas. Segmentar a imagem com um alto valor para K resulta em\n\nsuperpixels com menores dimens\u00f5es, consequentemente, segmentando a\n\nimagem em classes bem definidas. Entretanto, superpixels com pequenas\n\ndimens\u00f5es tendem a armazenar poucas caracter\u00edsticas descritivas de cada\n\nclasse, impactando a qualidade da classifica\u00e7\u00e3o. Para definir o valor de K\n\nforam testados valores m\u00faltiplos de 100, no intervalo de 100 a 1200.\n\nO valor K = 300 foi escolhido por lidar satisfatoriamente com ambos os\nproblemas. As dimens\u00f5es de um superpixel com essas configura\u00e7\u00e3o, para as\n\nimagens de dimens\u00f5es 4000x3000px utilizadas nesse trabalho, s\u00e3o\naproximadamente 200x200px. Estas dimens\u00f5es permitiram, simultaneamente,\nque cada segmento contivesse uma quantidade relevante de informa\u00e7\u00f5es da\n\nclasse que representa al\u00e9m de uma segmenta\u00e7\u00e3o da imagem principal em\n\nclasses bem definidas.\n\nPara o par\u00e2metro compacidade, foram avaliados valores m\u00faltiplos de 10\n\nno intervalo de 10 a 50. O objetivo dessa avalia\u00e7\u00e3o foi encontrar o valor que\n\nmelhor segmentasse a imagem, mantendo no mesmo superpixel apenas\n\nelementos pertencentes a mesma classe. Em imagens capturadas em dias\n\nnublados, com pouca altern\u00e2ncia de luz e sombra na planta\u00e7\u00e3o, se mostrou\n\nvi\u00e1vel utilizar valores menores, deixando as bordas do superpixel mais\n\nr\u00edgidas \u00e0s bordas dos elementos da imagem. Todavia em dias ensolarados, o\n\n41\n\n\n\nFigura 5.6: \u00c0 esquerda temos a imagem segmentada com compacidade 20 e \u00e0 direita\ncom compacidade 40. Para o valor 40 os segmentos s\u00e3o mais sens\u00edveis \u00e0 varia\u00e7\u00e3o de\nsoja e ervas daninhas na imagem, agrupando no mesmo superpixel, folhas de soja\ncom diferentes taxas de ilumina\u00e7\u00e3o. Para o valor 20 os segmentos s\u00e3o mais sens\u00edveis\nas mudan\u00e7as de cores provocadas pelo efeito da luz e sombra.\n\nefeito da altern\u00e2ncia da luz e sombra na imagem tornou os superpixels mais\n\nsens\u00edveis \u00e0 ilumina\u00e7\u00e3o da imagem, inclu\u00edndo no mesmo superpixel\n\nsegmentos da imagem pertencentes a diferentes classes. Somado a isso, o\n\nalgoritmo agrupava, em superpixels distintos, elementos adjacentes de uma\n\nmesma classe, devido a varia\u00e7\u00e3o de luz e sombra.\n\nSendo assim fez-se necess\u00e1ria a utiliza\u00e7\u00e3o de valores mais altos para que\n\na informa\u00e7\u00e3o de proximidade espacial tivesse um maior peso em rela\u00e7\u00e3o \u00e0\n\nsimilaridade de cor e ilumina\u00e7\u00e3o. Para padronizar um valor pr\u00f3ximo ao ideal\n\na imagens com variados tipos de ilumina\u00e7\u00e3o, foi escolhido o valor 40 para a\n\ncompacidade.\n\n5.5 Extra\u00e7\u00e3o de Atributos\n\nPara os classificadores que comparamos \u00e0s redes neurais, ap\u00f3s o passo da\n\nsegmenta\u00e7\u00e3o, realizamos a extra\u00e7\u00e3o de atributos de cada segmento do banco\n\nutilizando uma cole\u00e7\u00e3o de extratores de forma, cor, textura e orienta\u00e7\u00e3o da\n\nimagem implementados nas bibliotecas OpenCV [66] e scikit-image [65].\n\nA cole\u00e7\u00e3o de extratores foi composta por um total de 218 atributos. Foram\n\nextra\u00eddos atributos da Matriz de Coocorr\u00eancia GLCM, para matrizes 4x4 nas\ndist\u00e2ncias 1 e 2 e com \u00e2ngulos 0?, 45? e 90?. A partir dessas configura\u00e7\u00e3o\nforam utilizadas as seguintes propriedades: energia, contraste, correla\u00e7\u00e3o,\n\nhomogeneidade e dissimilaridade [26], em um total de 36 caracter\u00edsticas.\n\nPara o algoritmo Histograma de Gradientes Orientados [27], os 128 atributos\n\nde forma e orienta\u00e7\u00e3o extra\u00eddos correspondem \u00e0s 128 posi\u00e7\u00f5es do vetor de\n\nHOGs calculados sobre a imagem original redimensionada para as dimens\u00f5es\n\n128x128px.\n\n42\n\n\n\nGLCM HOG LBP Cores\nglcm_cont_1_0 hog_0 lbp_0 cor_rmin\nglcm_cont_1_45 hog_1 lbp_1 cor_rmax\nglcm_cont_1_90 hog_2 lbp_2 cor_rmedia\nglcm_cont_2_0 hog_3 lbp_3 cor_rdesvio\nglcm_cont_2_45 hog_4 lbp_4 cor_gmin\nglcm_cont_2_90 hog_5 lbp_5 cor_gmax\nglcm_diss_1_0 hog_6 lbp_6 cor_gmedia\nglcm_diss_1_45 hog_7 lbp_7 cor_gdesvio\nglcm_diss_1_90 hog_8 lbp_8 cor_bmin\nglcm_diss_2_0 hog_9 lbp_9 cor_bmax\nglcm_diss_2_45 hog_10 lbp_10 cor_bmedia\nglcm_diss_2_90 hog_11 lbp_11 cor_bdesvio\nglcm_homo_1_0 hog_12 lbp_12 cor_hmin\nglcm_homo_1_45 hog_13 lbp_13 cor_hmax\nglcm_homo_1_90 hog_14 lbp_14 cor_hmedia\nglcm_homo_2_0 hog_15 lbp_15 cor_hdesvio\nglcm_homo_2_45 hog_16 lbp_16 cor_smin\nglcm_homo_2_90 hog_17 lbp_17 cor_smax\nglcm_asm_1_0 hog_18 cor_smedia\nglcm_asm_1_45 hog_19 cor_sdesvio\nglcm_asm_1_90 hog_20 cor_vmin\nglcm_asm_2_0 hog_21 cor_vmax\nglcm_asm_2_45 hog_22 cor_vmedia\nglcm_asm_2_90 hog_23 cor_vdesvio\nglcm_ener_1_0 hog_24 cor_cielmin\nglcm_ener_1_45 hog_25 cor_cielmax\nglcm_ener_1_90 hog_26 cor_cielmedia\nglcm_ener_2_0 hog_27 cor_cieldesvio\nglcm_ener_2_45 hog_28 cor_cieamin\nglcm_ener_2_90 hog_29 cor_cieamax\nglcm_corr_1_0 hog_30 cor_cieamedia\nglcm_corr_1_45 hog_31 cor_cieadesvio\nglcm_corr_1_90 hog_32 cor_ciebmin\nglcm_corr_2_0 hog_33 cor_ciebmax\nglcm_corr_2_45 ... cor_ciebmedia\nglcm_corr_2_90 hog_127 cor_ciebdesvio\n\nTabela 5.7: Nome dos atributos separados por extrator. Em rela\u00e7\u00e3o ao\nextrator HOG, existem ainda os atributos cont\u00edguos entre hog_33 e hog_127,\ncorrespondente \u00e0s 128 posi\u00e7\u00f5es do vetor de HOGs.\n\n43\n\n\n\nPara o algoritmo Padr\u00f5es Bin\u00e1rios Locais [28], foram calculados os valores\n\npara todos os pixels da imagem. Esses valores foram separados em um\n\nhistograma de 18 faixas de mesmo tamanho e utilizados como 18 atributos\n\nde textura da imagem. Como extratores de cores, foram utilizados os\n\natributos m\u00ednimo, m\u00e1ximo, m\u00e9dia e desvio padr\u00e3o da imagem representada\n\nnos espa\u00e7os de cores RGB, HSV e CIELab, resultando em 36 atributos de\n\ncores.\n\n5.6 Classifica\u00e7\u00e3o\n\nPara compara\u00e7\u00e3o com a performance das Redes Neurais Convolucionais\n\nforam realizados testes com outros classificadores. Os algoritmos utilizados\n\nforam AdaBoost M1 [31], Florestas Aleat\u00f3rias [32] e M\u00e1quina de Vetores de\n\nSuporte (SVM) [67], utilizando para o seu treinamento o algoritmo SMO, a\n\nimplementa\u00e7\u00e3o da Sequential Minimal Optimization [34]. Para o algoritmo de\nboosting AdaBoost M1, o classificador escolhido foi o algoritmo J48, que\nconsiste da implementa\u00e7\u00e3o de uma evolu\u00e7\u00e3o do algoritmo C4.5 [30].\n\nPara estes algoritmos foi utilizada a implementa\u00e7\u00e3o dispon\u00edvel no software\n\nWeka e atrav\u00e9s da biblioteca python-weka-wrapper. O software Weka consiste\n\nde uma cole\u00e7\u00e3o de algoritmos de aprendizado de m\u00e1quina que t\u00eam como\n\nentrada arquivos no formato ARFF (Attribute-Relation File Format), um\n\narquivo de texto ASCII que descreve uma lista de inst\u00e2ncias, representando a\n\nmatriz de atributos de entrada para o classificador. Os ARFFs utilizados\n\ncomo entrada para os testes no Weka foram gerados a partir dos extratores\n\nde atributos citados na Se\u00e7\u00e3o 5.5. Todos os algoritmos foram executados com\n\nas configura\u00e7\u00f5es definidas por padr\u00e3o no software Weka vers\u00e3o 3.6.6 [70].\n\nPara o teste da performance das Redes Neurais Convolucionais foi\n\nutilizado o software Caffe [68]. Caffe \u00e9 um framework para Aprendizado\n\nProfundo implementado em C++/CUDA para o treinamento e\n\ndesenvolvimento de redes neurais. Sua motiva\u00e7\u00e3o principal \u00e9 o\n\nreconhecimento de imagens, sendo amplamente utilizado diretamente ou\n\ncomo base para implementa\u00e7\u00f5es de redes neurais convolucionais. A topologia\n\nda rede neural utilizada foi a rede CaffeNet, uma replica\u00e7\u00e3o da topologia\n\nAlexNet [52], com 8 camadas, sendo as 5 primeiras camadas convolucionais e\n\nas 3 \u00faltimas camadas completamente conexas. Na sa\u00edda da \u00faltima camada\n\nfoi utilizada uma 4-way softmax, que produziu a distribui\u00e7\u00e3o probabil\u00edstica\nsobre as 4 classes utilizadas neste problema.\n\nAs imagens do banco final geradas no passo da segmenta\u00e7\u00e3o foram salvas\n\nno formato .TIF e com tamanho proporcional ao menor ret\u00e2ngulo que\n\nenglobasse todo o superpixel marcado. A rede CaffeNet utiliza como entrada\n\n44\n\n\n\numa imagem no formato JPEG e com dimens\u00f5es 256x256px. Para lidar com\n\nessa restri\u00e7\u00e3o, cada imagem .TIF foi colada no quadrante superior esquerdo\n\nde uma imagem composta por um fundo preto, no formato JPEG, com\n\ndimens\u00f5es 512x512px. Ap\u00f3s esse passo, a imagem foi recortada, sem\n\nredimensionamento, em rela\u00e7\u00e3o ao quadrante superior esquerdo com\n\ndimens\u00f5es 256x256px, para ent\u00e3o ser submetida ao treinamento. Esse\n\nprocesso pode ser visto na imagem 5.7. Como a maior parte das imagens do\n\nbanco possu\u00eda altura e largura inferiores 256 pixels, em menos de 5% casos o\n\nrecorte causou perda de informa\u00e7\u00e3o.\n\nFigura 5.7: A imagem original .TIF com dimens\u00f5es 324x191px \u00e9 copiada para uma\nimagem .JPEG de dimens\u00f5es 512x512px. No passo seguinte, a imagem \u00e9 recortada\ncom tamanho 256x256px.\n\nPara ampliar o conjunto de treinamento foram aplicados os mesmos\n\npassos de pr\u00e9-processamento descritos no trabalho original da AlexNet [52].\n\nEm cada imagem do conjunto de treinamento foram aplicados 5 recortes de\n\ntamanho 227x227px, sendo um recorte partindo de cada diagonal da imagem\n\nacrescido de um recorte central. Todos esses recortes foram espelhados\n\nhorizontalmente, resultando um total de 10 recortes para cada imagem. que\n\nforam utilizados no treinamento da rede neural convolucional. O\n\nprocedimento \u00e9 ilustrado na imagem 5.8.\n\nPara realiza\u00e7\u00e3o dos testes de avalia\u00e7\u00e3o foram criados dois conjuntos a\n\npartir do banco de imagens. O primeiro conjunto foi composto com todas as\n\nclasses balanceadas, ou seja, as imagens foram divididas de modo que todas\n\nas classes contivessem o mesmo n\u00famero de imagens2. Como a classe com o\n\nmenor n\u00famero de imagens, folhas largas, era composta por 1191 imagens, o\n\nvalor escolhido foi 1125, resultando em 4500 imagens no total . Essas 4500\n\nimagens foram selecionadas automaticamente do banco de imagens principal\n\natrav\u00e9s de um script que fez a sele\u00e7\u00e3o de maneira aleat\u00f3ria. Dessas imagens,\n\n3000 foram utilizadas para o treinamento, 500 para valida\u00e7\u00e3o e 1000 para os\n\n2\u00c9 importante destacar que esta decis\u00e3o introduz uma artificialidade extra no banco de\nimagens.\n\n45\n\n\n\nFigura 5.8: Da esquerda da direita temos a imagem com 256x256px sendo recortada\npara as dimens\u00f5es 227x227px, a partir de cada diagonal e com um recorte central. Na\nfileira inferior temos o mesmo procedimento sendo aplicado com a imagem espelhada\nhorizontalmente.\n\ntestes. As imagens de valida\u00e7\u00e3o n\u00e3o foram utilizadas no treinamento dos\n\nalgoritmos comparado \u00e0s redes neurais, sendo ent\u00e3o descartadas.\n\nO segundo conjunto avaliado foi formado sem a restri\u00e7\u00e3o de que as\n\nclasses fossem balanceadas. Sem essa restri\u00e7\u00e3o foi poss\u00edvel utilizar 15000\n\nimagens das 15336 imagens totais do banco. As 336 imagens descartadas\n\nforam selecionadas de maneira an\u00e1loga ao m\u00e9todo utilizado no primeiro\n\ngrupo, com a premissa que todas pertencessem \u00e0 classe soja, por possuir um\n\nn\u00famero de imagens superior \u00e0s outras classes, reduzindo o impacto do\n\ndescarte. Para as outras classes todas as imagens do banco foram utilizadas.\n\nEsse banco foi dividido com 70% das imagens utilizadas para o treinamento,\n\n10% das imagens utilizadas para valida\u00e7\u00e3o e 20% das imagens utilizadas\n\npara os testes finais, seguindo essa propor\u00e7\u00e3o em todas as classes. Assim\n\ncomo no grupo das imagens balanceadas, as imagens de valida\u00e7\u00e3o foram\n\ndescartadas para os algoritmos comparados \u00e0s redes neurais.\n\nPara o conjunto com classes balanceadas, foram realizadas 7500 itera\u00e7\u00f5es\n\nno conjunto de treinamento, onde em cada itera\u00e7\u00e3o foi utilizado um mini-\n\nbatch de 50 imagens para o treinamento da rede. A taxa de aprendizado inicial\n\nda rede foi de 10?3, sendo a cada 3000 itera\u00e7\u00f5es multiplicada por 10?1. Ou seja,\nna itera\u00e7\u00e3o 3000 a taxa de aprendizado foi modificada para 10?4 e taxa final do\ntreinamento foi de 10?5. Para o conjunto com classes desbalanceadas, foram\nrealizadas 15000 itera\u00e7\u00f5es, com a taxa de aprendizado inicial de 10?3. Todavia\nnesse conjunto ela foi modificada apenas uma vez, com 10000 itera\u00e7\u00f5es, para\n\no valor de 10?4.\nOs valores finais dos par\u00e2metros utilizados neste treinamento foram\n\nobtidos ap\u00f3s tentativas emp\u00edricas de combina\u00e7\u00f5es. Esses c\u00e1lculos emp\u00edricos\n\nforam baseados na avalia\u00e7\u00e3o da precis\u00e3o da rede no conjunto de valida\u00e7\u00e3o e\n\n46\n\n\n\natrav\u00e9s da utiliza\u00e7\u00e3o de snapshots. Snapshots representam uma c\u00f3pia do\nmodelo da rede em uma determinada itera\u00e7\u00e3o do treinamento, que pode ser\n\nutilizada para modificar os par\u00e2metros de treinamento posteriores \u00e0quela\n\nitera\u00e7\u00e3o. Utilizando essa estrutura, foi poss\u00edvel testar v\u00e1rias ramifica\u00e7\u00f5es de\n\npar\u00e2metros at\u00e9 se chegar na combina\u00e7\u00e3o mais consistente para o conjunto\n\nde valida\u00e7\u00e3o.\n\nPara a defini\u00e7\u00e3o do valor da taxa de aprendizado inicial, foram avaliadas\n\nas pot\u00eancias negativas de 10 no intervalo entre 1 e 5, ou seja, 10?i, onde\n1 ? i ? 5. Classifica\u00e7\u00f5es no conjunto de testes finais s\u00f3 foram realizadas ap\u00f3s\na defini\u00e7\u00e3o dos par\u00e2metros finais, ou seja, os testes n\u00e3o tiveram influ\u00eancia na\n\nescolha dos par\u00e2metros. Dada a robustez do modelo, nenhuma modifica\u00e7\u00e3o\n\nna arquitetura original da rede CaffeNet se mostrou necess\u00e1ria.\n\nTamb\u00e9m foi realizada a an\u00e1lise dos resultados baseada no n\u00edvel de\n\nconfian\u00e7a fornecido pelas redes neurais, no conjunto com classes\n\nbalanceadas, e a an\u00e1lise de cada classificador comparado \u00e0s redes neurais\n\nusando os atributos extra\u00eddos por cada algoritmo individualmente. Esta\n\navalia\u00e7\u00e3o foi feita para se testar o poder individual de cada extrator e foi\n\nrealizada utilizando o conjunto de imagens com classes desbalanceadas.\n\nAl\u00e9m das avalia\u00e7\u00f5es na classifica\u00e7\u00e3o dos segmentos do banco de imagem, foi\n\nfeita a descri\u00e7\u00e3o do software Pynovis\u00e3o e avalia\u00e7\u00e3o da sua performance na\n\ndetec\u00e7\u00e3o e classifica\u00e7\u00e3o de ervas daninhas em imagens de lavouras de soja\n\ncapturadas por VANTs.\n\n5.7 M\u00e9tricas de Avalia\u00e7\u00e3o\n\nComo o problema consiste de m\u00faltiplas classes, \u00e9 preciso estender as\n\ndefini\u00e7\u00f5es de exemplos positivos e negativos testando cada classe contra\n\ntodas as outras. Sendo assim a classe de interesse \u00e9 definida como a classe\n\npositiva enquanto todas as outras s\u00e3o definidas como negativas [69]. O\n\nrelacionamento entre as classifica\u00e7\u00f5es como positiva e negativa pode ser\n\ndefinida como uma matriz de confus\u00e3o 2x2, onde obt\u00e9m-se as seguintes\n\ndefini\u00e7\u00f5es:\n\n\u2022 Verdadeiro Positivo (VP): Inst\u00e2ncias corretamente classificadas como a\nclasse de interesse.\n\n\u2022 Verdadeiro Negativo (VN): Inst\u00e2ncias corretamente classificadas como\nn\u00e3o sendo a classe de interesse.\n\n\u2022 Falso Positivo (FP): Inst\u00e2ncias incorretamente classificadas como a classe\nde interesse.\n\n47\n\n\n\n\u2022 Falso Negativo (FN): Inst\u00e2ncias incorretamente classificadas como n\u00e3o\nsendo a classe de interesse.\n\nAssumindo essas defini\u00e7\u00f5es, para avaliar a performance dos\n\nclassificadores, al\u00e9m da matriz de confus\u00e3o para cada classe, utilizamos as\n\nm\u00e9tricas precis\u00e3o ou valor preditivo positivo e sensibilidade.\n\nA precis\u00e3o \u00e9 definida na equa\u00e7\u00e3o 5.1:\n\nPrecis\u00e3o =\nV P\n\nV P + F P\n(5.1)\n\nonde V P representa a taxa de verdadeiros positivos e F P a taxa de falsos\n\npositivos, nos fornece a propor\u00e7\u00e3o de todos os segmentos identificados a uma\n\ndeterminada classe de fato pertencerem \u00e0quela classe. Essa an\u00e1lise se\n\nmostra relevante ao problema porque a falsa identifica\u00e7\u00e3o de ervas daninhas\n\nna lavoura poderia levar a um falso alerta de infesta\u00e7\u00e3o e custos com\n\ntratamento desnecess\u00e1rio.\n\nA sensibilidade \u00e9 definida na equa\u00e7\u00e3o 5.2:\n\nSensibilidade =\nV P\n\nV P + F N\n(5.2)\n\nonde F N representa a taxa de falsos negativos, nos fornece a capacidade do\n\nalgoritmo identificar cada classe nos segmentos. Baixa sensibilidade na\n\ndetec\u00e7\u00e3o das classes correspondentes \u00e0 ervas daninhas, poderia levar a uma\n\ndemora na identifica\u00e7\u00e3o da infesta\u00e7\u00e3o das ervas daninhas, causando\n\npreju\u00edzos \u00e0 lavoura.\n\n48\n\n\n\nCAP\u00cdTULO\n\n6\nResultados e discuss\u00f5es\n\n6.1 Avalia\u00e7\u00e3o com Classes Balanceadas\n\nNa avalia\u00e7\u00e3o com classes balanceadas, utilizando 4500 imagens similares\n\n\u00e0s demonstradas na Figura 6.1, os resultados alcan\u00e7aram alta precis\u00e3o e\n\nsensibilidade, como pode ser visto na Tabela 6.1. Estes resultados\n\ndemonstram a efici\u00eancia da rede convolucional em lidar com um problema\n\ncontendo poucas classes, quando fornecido um conjunto robusto de\n\ntreinamento. Ela apresentou resultados superiores a todos os outros\n\nclassificadores em todas as m\u00e9tricas avaliadas.\n\nFigura 6.1: Exemplares do banco de imagens final. Na fileira superior exemplos das\nclasses folhas largas e gramineas. Na fileira inferior exemplos das classes soja e solo.\n\nNo desempenho por classe, a classe solo teve o melhor desempenho em\n\ntodos os algoritmos analisados, por apresentar valores completamente\n\ndistintos de todas outras classes nos espa\u00e7os de cores RGB e CIELab. Na\n\nidentifica\u00e7\u00e3o das ervas daninhas, as redes neurais apresentaram uma\n\n49\n\n\n\nsuperioridade mais n\u00edtida no desempenho, em rela\u00e7\u00e3o aos outros algoritmos,\n\napresentando valores superiores a 0.98 na precis\u00e3o e sensibilidade nas\n\ngram\u00edneas e ervas daninhas de folhas largas. Todavia \u00e9 importante ressaltar\n\nque, com exce\u00e7\u00e3o das Florestas Aleat\u00f3rias na precis\u00e3o das folhas largas,\n\ntodos os algoritmos comparados apresentaram resultados acima de 90% na\n\nprecis\u00e3o e sensibilidade de todas as classes se apresentando como boas\n\nalternativas ao problema.\n\nSolo Soja\nGram\u00ed-\nneas\n\nFolhas\nLargas\n\nPrecis\u00e3o\nSensibi-\nlidade\n\nRedes Neurais Convolucionais\nSolo 250 0 0 0 1.000 1.000\nSoja 0 247 1 2 0.988 0.988\nGram\u00edneas 0 2 246 2 0.991 0.984\nFolhas Largas 0 1 1 248 0.984 0.992\nM\u00e9dia 0.991 0.991\nM\u00e1quina de Vetores de Suporte\nSolo 250 0 0 0 1.000 1.000\nSoja 0 241 3 6 0.953 0.964\nGram\u00edneas 0 9 237 4 0.967 0.948\nFolhas Largas 0 3 5 242 0.960 0.968\nM\u00e9dia 0.970 0.970\nAdaBoost M1 - C4.5\nSolo 249 0 1 0 0.996 0.996\nSoja 0 238 5 7 0.967 0.952\nGram\u00edneas 1 6 236 7 0.948 0.944\nFolhas Largas 0 2 7 241 0.945 0.964\nM\u00e9dia 0.964 0.964\nFlorestas Aleat\u00f3rias\nSolo 246 3 0 1 0.996 0.984\nSoja 1 229 10 10 0.954 0.916\nGram\u00edneas 0 6 228 16 0.908 0.912\nFolhas Largas 0 2 13 235 0.897 0.940\nM\u00e9dia 0.938 0.938\n\nTabela 6.1: Matriz de confus\u00e3o da avalia\u00e7\u00e3o com classes balanceadas para\ntodos os classificadores avaliados.\n\nNa Figura 6.4 podemos analisar as nove imagens classificadas\n\nerroneamente pelas Rede Neural Convolucional. Na primeira imagem de\n\nfolhas largas, vemos que tamb\u00e9m h\u00e1 na imagem a presen\u00e7a de gram\u00edneas, o\n\nque induziu a classifica\u00e7\u00e3o nessa classe. O mesmo comportamento se reflete\n\nna segunda imagem, onde h\u00e1 algumas folhas de soja pr\u00f3ximas \u00e0s ervas\n\ndaninhas, causando o erro na identifica\u00e7\u00e3o. As duas primeiras imagens de\n\nsoja correspondem \u00e0s fotografias capturadas no m\u00eas de dezembro. Essas\n\nimagens foram capturadas nos primeiros est\u00e1dios da soja e com uma altura\n\n50\n\n\n\nsuperior \u00e0s imagens obtidas em 2016, se assemelhando \u00e0s imagens de ervas\n\ndaninhas de folhas largas capturadas neste per\u00edodo.\n\nA terceira imagem de soja possui hastes que se assemelham ao formato das\n\ngram\u00edneas. No entanto \u00e9 necess\u00e1rio ressaltar que essa imagem foi identificada\n\ncomo gram\u00ednea com uma confian\u00e7a inferior a 70%. Em rela\u00e7\u00e3o \u00e0s imagens\n\nde gram\u00edneas, \u00e9 not\u00e1vel que a grande presen\u00e7a de luz e sombra dificulta a\n\nidentifica\u00e7\u00e3o da classe em duas imagens. Todavia novamente \u00e9 interessante\n\nperceber que oito das nove classifica\u00e7\u00f5es incorretas da rede foram realizadas\n\ncom uma confian\u00e7a inferior a 90%. Este fato nos leva a an\u00e1lise da confian\u00e7a\n\ndas classifica\u00e7\u00f5es da rede neural tamb\u00e9m nas imagens que foram classificadas\n\ncorretamente, como pode ser vista na tabela 6.2.\n\nFigura 6.2: Imagens identificadas com a classe incorreta pela Rede Neural\nConvolucional na avalia\u00e7\u00e3o com classes balanceadas. As cores vermelha, verde e\nazul no gr\u00e1fico representam, respectivamente, as ervas daninhas de folhas largas,\na soja e as gram\u00edneas. O gr\u00e1fico mostra a probabilidade na classifica\u00e7\u00e3o de cada\nimagem nas quatros classes do problema.\n\n51\n\n\n\nClassificado\nConfian\u00e7a Corretamente Incorretamente N\u00e3o classificado\n=1 480 0 520\n>= 0.999 860 0 140\n>= 0.99 947 0 53\n>= 0.98 963 0 37\n>= 0.96 973 1 26\n>= 0.94 978 2 20\n>= 0.90 981 2 17\n>= 0.75 985 4 11\n>= 0.50 991 9 0\n\nTabela 6.2: Tabela ilustrando a confian\u00e7a das classifica\u00e7\u00f5es da Rede Neural\nConvolucional na avalia\u00e7\u00e3o com classes balanceadas.\n\nPodemos ver na tabela que das 1000 imagens classificadas pela rede\n\nneural, 480 foram classificadas corretamente com 100% de probabilidade. Se\n\nestabelecermos um limiar de 0.98, temos que 96.3% das imagens foram\n\nclassificadas corretamente e nenhuma delas recebeu identifica\u00e7\u00e3o incorreta.\n\nA primeira imagem classificada incorretamente aparece apenas com o limiar\n\nde 0.96 e corresponde \u00e0s ervas daninhas de folhas largas que foram\n\nclassificadas como gram\u00edneas com confian\u00e7a de 0.9738.\n\n\u00c9 esperado que em algumas imagens mesmo especialistas treinados\n\npossam cometer erros. Esses erros podem ser causados por baixa qualidade\n\nda imagem, influ\u00eancia de ilumina\u00e7\u00e3o ou em casos da imagem englobar duas\n\nou mais classes em propor\u00e7\u00f5es similares. Utilizando a informa\u00e7\u00e3o de\n\nconfian\u00e7a \u00e9 poss\u00edvel adequar a aplica\u00e7\u00e3o para retornar identifica\u00e7\u00e3o positiva\n\napenas em casos que a confian\u00e7a seja superior a um limiar predefinido, que\n\npode ser escolhido por um especialista ou atrav\u00e9s de an\u00e1lise estat\u00edsticas dos\n\nresultados.\n\nFigura 6.3: Gr\u00e1fico ilustrando a confian\u00e7a das classifica\u00e7\u00f5es da Rede Neural.\n\n52\n\n\n\n6.2 Avalia\u00e7\u00e3o com Classes Desbalanceadas\n\nNesta segunda avalia\u00e7\u00e3o, as m\u00e9dias de desempenho tiveram um aumento\n\npara todos os algoritmos analisados, conforme a Tabela 6.3. No caso dos\n\nalgoritmos comparados \u00e0s redes neurais o aumento foi ainda mais\n\nsignificativo. \u00c9 um comportamento interessante, pois era esperado que a rede\n\nneural precisasse de uma grande massa de dados pra conseguir resultados\n\nsatisfat\u00f3rios. Todavia ela j\u00e1 obteve resultados pr\u00f3ximos ao ideal com uma\n\nquantidade significativamente menor de entradas, utilizadas na avalia\u00e7\u00e3o\n\nanterior, enquanto os outros classificadores precisaram de uma maior\n\nquantidade de dados para se aproximar do resultado das redes neurais.\n\nSolo Soja\nGram\u00ed-\nneas\n\nFolhas\nLargas\n\nPrecis\u00e3o\nSensibi-\nlidade\n\nRedes Neurais Convolucionais\nSolo 650 0 0 0 1.000 1.000\nSoja 0 1406 2 0 0.995 0.998\nGram\u00edneas 0 4 696 4 0.997 0.988\nFolhas Largas 0 3 0 235 0.983 0.987\nM\u00e9dia 0.995 0.995\nM\u00e1quina de Vetores de Suporte\nSolo 649 0 1 0 0.997 0.998\nSoja 0 1392 13 3 0.980 0.989\nGram\u00edneas 2 25 669 8 0.974 0.950\nFolhas Largas 0 3 4 231 0.955 0.971\nM\u00e9dia 0.980 0.980\nAdaBoost M1 - C4.5\nSolo 650 0 0 0 0.995 1.000\nSoja 0 1395 9 4 0.982 0.991\nGram\u00edneas 3 14 686 1 0.972 0.974\nFolhas Largas 0 12 11 215 0.977 0.903\nM\u00e9dia 0.982 0.982\nFlorestas Aleat\u00f3rias\nSolo 649 1 0 0 0.991 0.998\nSoja 0 1367 37 4 0.966 0.971\nGram\u00edneas 5 29 658 12 0.928 0.935\nFolhas Largas 1 18 14 205 0.928 0.861\nM\u00e9dia 0.960 0.960\n\nTabela 6.3: Matriz de confus\u00e3o da avalia\u00e7\u00e3o com classes desbalanceadas para\ntodos os classificadores avaliados.\n\nAl\u00e9m disso, apesar do aumento das m\u00e9dias de desempenho, a precis\u00e3o e\n\nsensibilidade \u00e0s ervas daninhas de folhas largas tiveram redu\u00e7\u00e3o de\n\ndesempenho em alguns algoritmos, mesmo sendo treinada uma amostragem\n\nde imagens de ervas daninhas de folhas largas de tamanho similar nas duas\n\n53\n\n\n\navalia\u00e7\u00f5es. Esse comportamento demonstra como a propor\u00e7\u00e3o desigual de\n\nelementos treinados por classe pode levar a um impacto no resultado da\n\nclassifica\u00e7\u00e3o. Classes com mais elementos treinados podem se tornar mais\n\nsens\u00edveis a identifica\u00e7\u00e3o em rela\u00e7\u00e3o a classes com menos elementos\n\ntreinados, como aparenta ter acontecido no caso das folhas largas. Mesmo as\n\nredes neurais sofreram com esse problema, embora de maneira menos\n\nconclusiva.\n\nPara analisar mais a fundo esse problema nas redes neurais, temos na\n\nFigura 6.4 as treze imagens classificadas incorretamente. Pode-se ver nas\n\ntr\u00eas imagens incorretas de folhas largas que embora semelhan\u00e7as dessas\n\ncom imagens de soja n\u00e3o sejam visualmente percept\u00edveis, elas foram\n\nclassificadas como soja com grande confian\u00e7a. Entretanto das 1408 imagens\n\nde soja avaliadas, as \u00fanicas duas classificadas incorretamente, foram\n\nclassificadas com confian\u00e7a inferior a 0.75. Nas gram\u00edneas tamb\u00e9m temos\n\ndois casos de imagens classificadas como soja com confian\u00e7a superior a 0.90.\n\nEsse comportamento n\u00e3o foi observado nos erros de classifica\u00e7\u00e3o na\n\navalia\u00e7\u00e3o com classes balanceadas.\n\nTamb\u00e9m \u00e9 importante notar que apesar de ser a classe com mais imagens\n\nincorretas, sendo oito no total, as gram\u00edneas apresentaram, na classifica\u00e7\u00e3o\n\npela rede neural, n\u00fameros superiores na precis\u00e3o e sensibilidade em rela\u00e7\u00e3o \u00e0\n\navalia\u00e7\u00e3o com classes balanceadas. Na precis\u00e3o o aumento foi de 0.991 para\n\n0.997 e na sensibilidade de 0.984 para 0.988.\n\n54\n\n\n\nFigura 6.4: Imagens identificadas com a classe incorreta pela Rede Neural\nConvolucional na avalia\u00e7\u00e3o com classes desbalanceadas. As cores vermelha, verde\ne azul no gr\u00e1fico representam, respectivamente, as ervas daninhas de folhas largas,\na soja e as gram\u00edneas. O gr\u00e1fico mostra a probabilidade na classifica\u00e7\u00e3o para cada\nimagem nas quatros classes do problema.\n\n55\n\n\n\n6.3 Avalia\u00e7\u00e3o por Extratores\n\nUm fator determinante na efic\u00e1cia da classifica\u00e7\u00e3o dos algoritmos\n\ncomparados \u00e0s redes neurais \u00e9 a qualidade dos atributos fornecidos pelos\n\nextratores escolhidos. Para avaliar esse impacto, foi feita a an\u00e1lise de cada\n\nclassificador usando os atributos extra\u00eddos por cada algoritmo\n\nindividualmente. Com essa avalia\u00e7\u00e3o visa-se analisar o poder de cada\n\nextrator e se o uso dos mesmos em conjunto n\u00e3o poderia ter de alguma forma\n\nprejudicado a performance dos classificadores.\n\nTodos GLCM HOG LBP Cores\nM\u00e1quina de Vetores de Suporte\nPrecis\u00e3o 0.980 0.663 0.562 0.899 0.885\nAdaBoost M1 - C4.5\nPrecis\u00e3o 0.982 0.702 0.552 0.877 0.941\nFlorestas Aleat\u00f3rias\nPrecis\u00e3o 0.960 0.697 0.550 0.865 0.930\n\nTabela 6.4: Matriz de confus\u00e3o dos classificadores comparados \u00e0s Redes\nNeurais, com performance avaliada individualmente por extrator.\n\nVemos na Tabela 6.4 que houve uma queda percept\u00edvel na performance\n\ndos classificadores quando utilizados os extratores individualmente. Os\n\nalgoritmos AdaBoost e Florestas Aleat\u00f3rias conseguiram seu melhores\n\nresultados usando os atributos de cores enquanto a m\u00e1quina de vetores de\n\nsuporte teve seu melhor resultado usando o extrator de textura LBP. Outra\n\nan\u00e1lise interessante mostra que todos os classificadores analisados n\u00e3o\n\ntiveram bons resultados quando usados os extratores GLCM e HOG, tendo o\n\nextrator de forma HOG um desempenho muito inferior aos outros. Esse\n\ncomportamento sugere que atributos de cor e textura s\u00e3o mais adequados\n\npara esse problema. Enquanto a cor \u00e9 determinante na discrimina\u00e7\u00e3o do solo\n\ne plantas, a textura \u00e9 um fator essencial na discrimina\u00e7\u00e3o entre a soja e as\n\nervas daninhas.\n\nDada a alta precis\u00e3o alcan\u00e7ada por todos classificadores comparados pode\n\nser questionada a necessidade da aplica\u00e7\u00e3o de redes convolucionais, uma\n\nestrutura que necessita substancialmente de mais tempo e mem\u00f3ria para o\n\ntreinamento do conjunto de dados. Todavia essa \u00faltima avalia\u00e7\u00e3o demonstra\n\ncomo a for\u00e7a da cole\u00e7\u00e3o de extratores de atributos utilizada foi determinante\n\npara que esses classificadores se apresentassem de maneira competitiva\n\nfrente \u00e0s redes neurais, desempenho que n\u00e3o foi alcan\u00e7ado usando os\n\nextratores de maneira individual.\n\nPortanto embora seja poss\u00edvel deduzir que utilizando outras combina\u00e7\u00f5es\n\nde extratores de atributos fosse poss\u00edvel alcan\u00e7ar resultados at\u00e9 superiores\n\n56\n\n\n\n\u00e0s redes convolucionais, temos que levar em considera\u00e7\u00e3o que seriam\n\nnecess\u00e1rios exaustivos testes ou pesquisas com especialistas de dom\u00ednio at\u00e9\n\nencontrar a melhor combina\u00e7\u00e3o de extratores e classificadores para esse\n\nproblema em espec\u00edfico. Com a rede neural temos a vantagem de abrir m\u00e3o\n\ndessa parte do trabalho, chegando \u00e0 solu\u00e7\u00e3o de maneira mais direta e\n\ntamb\u00e9m flex\u00edvel, afinal uma combina\u00e7\u00e3o de extratores que funcione bem para\n\num determinado problema ou mesmo banco de imagens, n\u00e3o\n\nnecessariamente pode ser utilizada em outro problema.\n\nAtrav\u00e9s do uso de redes neurais temos uma boa chance de aplicar a\n\nmesma t\u00e9cnica utilizada na identifica\u00e7\u00e3o de ervas daninhas em lavouras de\n\nsoja a outros tipos de cultura apenas modificando a composi\u00e7\u00e3o do banco de\n\nimagens de treinamento. Esse aspecto pode ser considerado um fator de\n\ncompensa\u00e7\u00e3o em rela\u00e7\u00e3o aos altos custos de tempo e mem\u00f3ria gastos no\n\ntreinamento das redes convolucionais, principalmente pelo fato desses custos\n\nj\u00e1 estarem sendo significativamente reduzidos pelos recentes avan\u00e7os de\n\nhardware.\n\n6.4 Software Pynovis\u00e3o\n\nO Pynovis\u00e3o foi um software desenvolvido neste trabalho com o objetivo de\n\nser um m\u00f3dulo integrado de t\u00e9cnicas de vis\u00e3o computacional. Ele realiza\n\ntarefas como segmenta\u00e7\u00e3o, extra\u00e7\u00e3o de atributos e classifica\u00e7\u00e3o da imagem.\n\nPara a segmenta\u00e7\u00e3o ele fornece tr\u00eas algoritmos de superpixels, sendo SLIC, o\n\nalgoritmo utilizado neste trabalho. Ap\u00f3s a divis\u00e3o das imagens em\n\nsuperpixels ele permite a cria\u00e7\u00e3o de um banco de imagens, clicando no\n\nsuperpixel correspondente \u00e0 sua classe espec\u00edfica.\n\nFigura 6.5: Telas de abertura de imagem e segmenta\u00e7\u00e3o do software Pynovis\u00e3o.\n\n57\n\n\n\nAp\u00f3s o clique no superpixel o software automaticamente seleciona o\n\nmenor ret\u00e2ngulo que engloba todo o segmento e salva a imagem desse\n\nret\u00e2ngulo na pasta correspondente \u00e0 classe selecionada. Para realizar a\n\nextra\u00e7\u00e3o \u00e9 realizado o rastreamento de todas as pastas relativas \u00e0s classes\n\ninformadas pelo usu\u00e1rio e feita a extra\u00e7\u00e3o de atributos de todas as imagens\n\ncontidas nestes diret\u00f3rios. O software tamb\u00e9m permite a configura\u00e7\u00e3o dos\n\nextratores de atributos e classificadores utilizados. Essa caracter\u00edstica\n\nauxiliou na avalia\u00e7\u00e3o de classes por extratores, mostrada na Se\u00e7\u00e3o 6.3.\n\nTodavia a funcionalidade mais importante do Pynovis\u00e3o, para este\n\ntrabalho, \u00e9 a detec\u00e7\u00e3o de ervas daninhas em uma imagem de planta\u00e7\u00e3o de\n\nsoja. Para atingir esse objetivo o software realiza a segmenta\u00e7\u00e3o utilizando o\n\nalgoritmo SLIC, atrav\u00e9s de par\u00e2metros definidos manualmente. Ap\u00f3s o passo\n\nde segmenta\u00e7\u00e3o, ele salva todos os segmentos da imagem e realiza a extra\u00e7\u00e3o\n\nde atributos nos mesmos. Por fim, ele realiza o treinamento do classificador,\n\nusando imagens do banco de imagens armazenadas previamente nos\n\ndiret\u00f3rios das classes. Com o classificador treinado e o vetor de atributos\n\ncorrespondentes aos segmentos da imagem, ele realiza a classifica\u00e7\u00e3o\n\nindividual de cada superpixel da imagem e mostra o resultado visual\n\npintando cada segmento com a cor caracter\u00edstica \u00e0 sua classe.\n\nFigura 6.6: Telas de sele\u00e7\u00e3o de extratores de atributos e classifica\u00e7\u00e3o da imagem do\nsoftware Pynovis\u00e3o.\n\nPara a rede convolucional n\u00e3o \u00e9 necess\u00e1ria a realiza\u00e7\u00e3o da extra\u00e7\u00e3o dos\n\natributos, pois a classifica\u00e7\u00e3o \u00e9 feita utilizando o dado bruto da imagem de\n\ncada segmento. Entretanto o treinamento da rede neural n\u00e3o \u00e9 realizado pelo\n\nPynovis\u00e3o, sendo necess\u00e1rio que o mesmo seja feito por outro software. Para\n\neste trabalho, o treinamento e classifica\u00e7\u00e3o, utilizando redes neurais, foram\n\nrealizados de maneira integrada com o software Caffe.\n\n58\n\n\n\n6.5 Classifica\u00e7\u00e3o em Imagens\n\nAl\u00e9m das avalia\u00e7\u00f5es de classifica\u00e7\u00e3o dos segmentos do nosso banco de\n\nimagens, atrav\u00e9s do software Pynovis\u00e3o, foi poss\u00edvel realizar a classifica\u00e7\u00e3o\n\ndas imagens capturadas pelo VANT na planta\u00e7\u00e3o de soja. Para isso o\n\nsoftware realiza a segmenta\u00e7\u00e3o da imagem utilizando o algoritmo SLIC e ap\u00f3s\n\nesse passo classifica cada segmento de maneira independente em uma das\n\nquatro classes definidas.\n\nPara realizar a classifica\u00e7\u00e3o utilizamos o modelo treinado para a avalia\u00e7\u00e3o\n\ncom classes desbalanceadas. Para garantir que os segmentos gerados na\n\nimagem de teste n\u00e3o fossem id\u00eanticos a algum segmento usado no\n\ntreinamento dos nossos algoritmos, foi utilizado, como par\u00e2metro do\n\nsegmentador SLIC, o valor de compacidade 30, enquanto na nossa gera\u00e7\u00e3o\n\ndo banco de imagens foi utilizado o valor 40. Isso tamb\u00e9m p\u00f5e a prova a\n\ncapacidade do nosso modelo de adequar a classifica\u00e7\u00e3o a varia\u00e7\u00f5es em\n\nrela\u00e7\u00e3o aos par\u00e2metros que foi treinado. De qualquer forma, \u00e9 percept\u00edvel\n\nque mesmo com essa varia\u00e7\u00e3o parte da informa\u00e7\u00e3o do treinamento continua\n\nsendo reaproveitada.\n\nSolo Soja Gram\u00edneas Folhas Largas\nImagem de Dezembro\nRedes Neurais\nConvolucionais\n\n51.49% 4.70% 24.57% 19.24%\n\nM\u00e1quinas de\nVetores de Suporte\n\n51.53% 1.10% 25.01% 22.36%\n\nImagem de Fevereiro\nRedes Neurais\nConvolucionais\n\n15.70% 45.11% 25.88% 13.31%\n\nM\u00e1quinas de\nVetores de Suporte\n\n19.25% 41.35% 25.89% 13.51%\n\nImagem de Mar\u00e7o\nRedes Neurais\nConvolucionais\n\n12.32% 76.50% 0.00% 11.18%\n\nM\u00e1quinas de\nVetores de Suporte\n\n11.73% 74.59% 3.19% 10.49%\n\nTabela 6.5: Tabela com a distribui\u00e7\u00e3o da classifica\u00e7\u00e3o realizada pelo software\nPynovis\u00e3o nas tr\u00eas imagens analisadas.\n\nRealizamos o teste com tr\u00eas imagens, correspondentes aos meses de\n\ndezembro, fevereiro e mar\u00e7o. Para comparar os resultados das redes\n\nconvolucionais realizamos o mesmo experimento com a m\u00e1quina de vetores\n\nde suporte. O resultado da classifica\u00e7\u00e3o em cada imagem pode ser visto na\n\nTabela 6.5. Como essas imagens n\u00e3o foram marcadas manualmente por um\n\n59\n\n\n\nconjunto de especialistas antes da classifica\u00e7\u00e3o autom\u00e1tica, para que fosse\n\nposs\u00edvel realizar uma an\u00e1lise quantitativa do resultado, a an\u00e1lise da\n\nclassifica\u00e7\u00e3o pode ser feita de maneira qualitativa baseada no resultado\n\nvisual fornecido pelo software.\n\nA primeira imagem, na Figura 6.7, cont\u00e9m segmentos de todas as classes\n\navaliadas. As ervas daninhas foram corretamente identificadas. Todavia\n\nhouve uma certa discrep\u00e2ncia entre os algoritmos analisados na discrima\u00e7\u00e3o\n\ndas mesmas. Uma das poss\u00edveis causas desse comportamento \u00e9 relacionada\n\nao fato que v\u00e1rios trechos da lavoura cont\u00eam a presen\u00e7a de ambos tipos de\n\nervas daninhas, fazendo com que v\u00e1rios dos superpixels englobassem mais\n\nde uma classe do problema. Sem a defini\u00e7\u00e3o de um limiar m\u00ednimo para que a\n\nclassifica\u00e7\u00e3o de um segmento seja considerada v\u00e1lida, o software \u00e9 obrigado\n\na fornecer classifica\u00e7\u00e3o a todos os segmentos, mesmo sem alto n\u00edvel de\n\nconfian\u00e7a. Nessa imagem ambos algoritmos atingiram uma identifica\u00e7\u00e3o\n\nprecisa dos segmentos de soja.\n\nFigura 6.7: Da esquerda para a direita, a imagem original, a imagem classificada\npelas Redes Neurais Convolucionais e classificada pela M\u00e1quina de Vetores de\nSuporte. Em vermelho as ervas daninhas de folha larga, em azul as gram\u00edneas e\nem roxo o solo. A soja foi mantida na sua cor original.\n\nNa segunda imagem analisada, correspondente \u00e0 planta\u00e7\u00e3o no m\u00eas de\n\nmar\u00e7o, havia apenas exemplares de ervas daninhas de folhas largas. A\n\nm\u00e1quina de vetores de suporte obteve 3.19% de falsos positivos em rela\u00e7\u00e3o\n\n\u00e0s gram\u00edneas, como pode ser visto nos segmentos em azul, no canto inferior\n\ndireito da imagem 6.7. As redes neurais, corretamente, n\u00e3o classificaram\n\nnenhum segmento da imagem como gram\u00edneas. Os dois algoritmos\n\n60\n\n\n\nconseguiram detectar os focos de ervas daninhas de folhas largas na imagem,\n\napesar de classificar alguns segmentos como soja.\n\nA \u00faltima imagem analisada, correspondente ao m\u00eas de dezembro, possui\n\nalgumas caracter\u00edsticas a serem analisadas. Ela foi fotografada em uma altura\n\nsuperior ao padr\u00e3o utilizado e com resolu\u00e7\u00e3o 16x9, ao contr\u00e1rio do padr\u00e3o 4x3\n\nadotado nas imagens utilizadas na gera\u00e7\u00e3o do banco. Al\u00e9m disso devido aos\n\nproblemas na aquisi\u00e7\u00e3o de imagens do m\u00eas de janeiro, a grande maioria das\n\nimagens utilizadas na cria\u00e7\u00e3o do banco foi de imagens correspondentes aos\n\nest\u00e1dios reprodutivos da soja. Essa imagem representa a soja no seu est\u00e1dio\n\nvegetativo.\n\nFigura 6.8: Imagem do m\u00eas de dezembro segmentada e classificada atrav\u00e9s\ndo software Pynovis\u00e3o. Na fileira superior temos a imagem original e a imagem\nclassificada pelas Redes Neurais Convolucionais. Na fileira inferior a imagem\nsegmentada pelo SLIC e classificada pela M\u00e1quina de Vetores de Suporte.\n\nMesmo com essas varia\u00e7\u00f5es, os algoritmos analisados conseguiram uma\n\nalta precis\u00e3o de detec\u00e7\u00e3o e discrimina\u00e7\u00e3o das ervas daninhas na foto. Em\n\nresultados quantitativos as redes convolucionais detectaram 24.57% de\n\ngram\u00edneas e 19.24% de folhas largas na \u00e1rea da planta\u00e7\u00e3o. A m\u00e1quina de\n\nvetores de suporte obteve um resultado similar, detectando 25.01% de\n\ngram\u00edneas e 22.36% de folhas largas. Todavia, analisando o resultado visual\n\nda imagem 6.8, \u00e9 poss\u00edvel identificar que a diferen\u00e7a de 3.12% entre os dois\n\nalgoritmos em rela\u00e7\u00e3o \u00e0s folhas largas correspondem a falsos positivos da\n\nm\u00e1quina de vetores de suporte em rela\u00e7\u00e3o a segmentos que representavam\n\nsoja.\n\nDe qualquer modo \u00e9 poss\u00edvel analisar a baixa taxa de falsos positivos, em\n\n61\n\n\n\nambos algoritmos, na detec\u00e7\u00e3o de ervas daninhas. As redes neurais\n\nconvolucionais classificaram com boa sensibilidade os segmentos de soja.\n\nIsso demonstra a capacidade do software reconhecer a soja independente do\n\nseu est\u00e1dio fenol\u00f3gico. Mesmo tendo sido utilizados majoritariamente\n\nexemplares de soja no est\u00e1dio reprodutivo, o software conseguiu uma alta\n\ntaxa de precis\u00e3o e sensibilidade na classifica\u00e7\u00e3o de segmentos contendo soja\n\nno est\u00e1dio vegetativo, que \u00e9 o est\u00e1gio onde as ervas daninhas necessitam ser\n\nidentificadas e controladas para impedir os preju\u00edzos na safra.\n\n62\n\n\n\nCAP\u00cdTULO\n\n7\nConclus\u00e3o\n\nNeste trabalho foi desenvolvido um software que realiza a detec\u00e7\u00e3o de\n\nervas daninhas em imagens de lavouras de soja, al\u00e9m de discriminar as\n\nmesmas entre ervas daninhas de folhas largas e gram\u00edneas. O algoritmo SLIC\n\nSuperpixel se mostrou uma eficiente ferramenta de segmenta\u00e7\u00e3o para\n\nimagens de planta\u00e7\u00f5es capturadas por VANTs, al\u00e9m de otimizar o tempo\n\ngasto na constru\u00e7\u00e3o do banco de imagens. O banco constru\u00eddo neste\n\ntrabalho \u00e9 composto por mais de quinze mil imagens de solo, soja e ervas\n\ndaninhas e ser\u00e1 disponibilizado.\n\nO uso de Redes Neurais Convolucionais alcan\u00e7ou excelentes resultados,\n\ncom precis\u00e3o superior a 98% na classifica\u00e7\u00e3o de todas as classes. No\n\nconjunto com 15 mil imagens, foi obtido 99.5% de precis\u00e3o m\u00e9dia entre todas\n\nas imagens analisadas. Os algoritmos comparados tamb\u00e9m obtiveram bons\n\nresultados na classifica\u00e7\u00e3o, mas as redes neurais apresentam a vantagem\n\ndos seus resultados n\u00e3o serem dependentes da escolha de bons extratores de\n\natributos. Al\u00e9m disso, o uso de redes convolucionais podem contar com os\n\nbenef\u00edcios recentes do r\u00e1pido aumento do poder de processamento e\n\nmem\u00f3ria, que viabilizam o treinamento de grandes conjuntos de imagens em\n\num tempo vi\u00e1vel.\n\nPara trabalhos futuros seria interessante abordar a avalia\u00e7\u00e3o com um\n\nbanco de imagens cobrindo uma maior gama de vari\u00e1veis, como locais de\n\nplantio e altura da captura das imagens. Sem a depend\u00eancia dos extratores,\n\n\u00e9 prov\u00e1vel que a metodologia utilizada neste trabalho possa ser estendida a\n\nproblemas similares, envolvendo outros tipos de planta\u00e7\u00f5es, com poucas\n\nadequa\u00e7\u00f5es. Dada a alta precis\u00e3o alcan\u00e7ada neste trabalho na classifica\u00e7\u00e3o\n\nde ervas daninhas por tipo de folhas, tamb\u00e9m seria interessante avaliar a\n\n63\n\n\n\nprecis\u00e3o atingida na classifica\u00e7\u00e3o das ervas daninhas por esp\u00e9cies.\n\nEste estudo tamb\u00e9m demonstrou que as redes neurais conseguiram alta\n\nprecis\u00e3o, sem a necessidade da utiliza\u00e7\u00e3o das quinze mil imagens do banco.\n\nEste fato sugere um estudo adicional para verificar o comportamento da rede,\n\npara este problema, sendo treinada com menos imagens, o que reduziria o\n\ntempo de treinamento e marca\u00e7\u00e3o manual das imagens. O uso de\n\naprendizagem semi-supervisionada da rede \u00e9 uma alternativa a ser avaliada\n\npara auxiliar a marca\u00e7\u00e3o manual das imagens.\n\nH\u00e1 tamb\u00e9m algumas quest\u00f5es que podem ser melhor exploradas como o\n\nefeito de desbalanceamento das classes na detec\u00e7\u00e3o de ervas daninhas e uma\n\nan\u00e1lise mais profunda na performance e erros dos extratores e\n\nclassificadores comparados \u00e0s redes neurais. Por fim, testes de avalia\u00e7\u00e3o do\n\nsoftware Pynovis\u00e3o utilizando imagens de refer\u00eancia pr\u00e9-rotuladas por um\n\nconjunto de especialistas podem dar uma maior dimens\u00e3o aos resultados\n\nobtidos nesse trabalho.\n\n64\n\n\n\nRefer\u00eancias Bibliogr\u00e1ficas\n\n[1] A. C. Silva, E. P. C. Lima and H.R. Batista. A Import\u00e2ncia da Soja\n\npara o Agroneg\u00f3cio Brasileiro: uma An\u00e1lise sob o Enfoque da Produ\u00e7\u00e3o,\n\nEmprego e Exporta\u00e7\u00e3o. 2011.\n\n[2] M. A. Rizzardi and N. G. Fleck. M\u00e9todos de quantifica\u00e7\u00e3o da cobertura\n\nfoliar da infesta\u00e7\u00e3o das plantas daninhas e da cultura de soja. Ci\u00eancia\nRural, v34(1), 2004.\n\n[3] P. J. Herrera, J. Dorado and A. Ribeiro. A Novel Approach for Weed Type\n\nClassification Based on Shape Descriptors and a Fuzzy Decision-Making\n\nMethod. Sensors, v14(8), p15304-15324, 2014.\n\n[4] Y. LeCun, Y. Bengio and G. Hinton. Deep Learning. Nature, v521, p436-\n444, 2015.\n\n[5] M. H. Siddiqi, S. Lee and A. M. Kwan. Weed Image Classification using\n\nWavelet Transform, Stepwise Linear Discriminant Analysis and Support\n\nVector Machines for Real-Time Selective Herbicide Applications. Journal\nof Information Science and Engineering, v30, p1253-1270, 2014.\n\n[6] J. M. Pe\u00f1a Barrag\u00e1n, M. Kelly, A. I. D. Castro and F. L\u00f3pez Granados.\n\nObject-base approach for crop row characterization in UAV images for\n\nsite-specific weed management. Proceedings of the 4th GEOBIA, p426,\n2012.\n\n[7] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua e S. Susstrunk. SLIC\n\nsuperpixels compared to state-of-the-art superpixel methods. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions, 34(11), p2274-\n2282, 2012.\n\n[8] S. J. D. Prince. Computer Vision: Models, Learning and Inference.\n\nCambridge University Press, 2012.\n\n[9] L. Shapiro and G. Stockman. Computer Vision. Prentice Hall, 2000.\n\n65\n\n\n\n[10] R. Szeliski. Computer Vision: Algorithms and Applications. Springer,\n2010.\n\n[11] R. Jain, R. Kasturi and B. Schunck. Machine Vision. McGraw-Hill, 1995.\n\n[12] V. Ugale and D. Gupta. A Comprehensive Survey on Agricultural Image\n\nProcessing. International Journal of Science and Research, v5(1), p133-\n135, 2016.\n\n[13] E. Voll, D. L. P. Gazziero, A. M. Brighenti, F. S. Adegas, C. A. Gaudencio\n\nand C. E. Voll. Din\u00e2mica das Plantas Daninhas e Pr\u00e1ticas de Manejo.\n\nEmbrapa, Documentos 260, ISSN 1516-781X, 2005.\n\n[14] J. R. B. Farias, A. L. Nepomuceno and N. Neumaler. Ecofisiologia da Soja.\n\nCircular T\u00e9cnica 48, 2007.\n\n[15] W. R. Fehr and C. E. Caviness. Stages of soybean development. Ames:\nIowa State University, Special Report, 80, p12, 1977.\n\n[16] G. R. Mohammadi and F. Amiri. Critical period of weed control in soybean\n\n(Glycine max) as influenced by starter fertilizer. Australian Journal of Crop\nScience, v5(11), p1350-1355, 2011.\n\n[17] B. Hartzler. Protecting soybean yields from early-season competition. IC-\n498, v3, p76, 2007.\n\n[18] R. C. Van Acker, C. J. Swanton and S. F. Weise. The Critical Period of\n\nWeed Control in Soybean. Weed Science, v41, p194-200, 1993.\n\n[19] A. N. Chaves, P. S. Cugnasca and J. J. Neto Busca Adaptativa\n\ncom M\u00faltiplos Ve\u00edculos A\u00e9reos N\u00e3o Tripulados. Revista de Sistemas e\nComputa\u00e7\u00e3o, Salvador, v2(1), p53-59, 2012.\n\n[20] D. Floreano and R. J. Wood. Science, technology and the future of small\n\nautonomous drones. Nature, v521, p460-466, 2015.\n\n[21] G. A. Longhitano. VANTs para sensoriamento remoto: aplicabilidade\n\nna avalia\u00e7\u00e3o e monitoramento de impactos ambientais causados por\n\nacidentes com cargas perigosas. Escola Polit\u00e9cnica da Universidade de\nS\u00e3o Paulo, 2010.\n\n[22] E. Marris. Drones in science: Fly, and bring me data. Nature, v498,\np156-158, 2013.\n\n[23] D. L. Pham, C. Xu and J. L. Prince. Current methods in medical image\n\nsegmentation 1. Annual review of biomedical engineering, v2(1), p315-\n337, 2000.\n\n66\n\n\n\n[24] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua e S. Susstrunk. SLIC\n\nSuperpixels. EPFL Technical, Report 149300, 2010.\n\n[25] I. Guyon and A. Elisseeff. An introduction to feature extraction. Feature\nextraction, p1-25, 2006.\n\n[26] L. K. Soh and C. Tsatsoulis. Texture analysis of SAR sea ice imagery\n\nusing gray level co-occurrence matrices. IEEE Transactions on geoscience\nand remote sensing, v37(2), p780-795, 1999.\n\n[27] N. Dalal and B. Triggs. Histograms of oriented gradients for human\n\ndetection. IEEE Computer Society Conference on Computer Vision and\nPattern Recognition (CVPR\u201905), v1, p886-893, 2005.\n\n[28] T. Ahonen, A. Hadid and M. Pietikainen. Face description with local\n\nbinary patterns: Application to face recognition. IEEE transactions on\npattern analysis and machine intelligence, v28(12), p2037-2041, 2006.\n\n[29] S .B .Kotsiantis, I. Zaharakis and P. Pintelas. Supervised machine\n\nlearning: A review of classification techniques. 2007.\n\n[30] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann,\n1993.\n\n[31] Y. Freund and R. E. Schapire. Experiments with a New Boosting\n\nAlgorithm. Machine Learning: Proceedings of the Thirteenth International\nConference, 1996.\n\n[32] L. Breiman. Random Forests. Machine Learning, v45, issue 1, p5-32,\n2001.\n\n[33] A. Liaw and M. Wiener. Classification and regression by randomForest. R\nnews, v2(3), p18-22, 2002.\n\n[34] J. C. Platt. Sequential Minimal Optimization: A Fast Algorithm for\n\nTraining Support Vector Machines. Microsoft Reasearch, 1998.\n\n[35] T. Brosnan and D. Sun. Inspection and grading of agricultural and\n\nfoodproducts by computervision systems \u2013 a review. Computers and\nElectronics in Agriculture, p193-213, 2002.\n\n[36] A. Arif and K. M. Butt. Computer vision based navigation module for\n\nsustainable broad-acre culture robots. ISSN 1013-5316, 2014.\n\n[37] F. Ahmed, H. A .Al-Mamun, A. S. M. H. Bari, E. Hossain and P. Kwan.\n\nClassification of crops and weeds from digital images: A support vector\n\nmachine approach. Crop Protection, v40, p98-104, 2012.\n\n67\n\n\n\n[38] A. Tellaeche, G. Pajares, X. P. Burgos-Artizzu and A. Ribeiro. A\n\ncomputer vision approach for weeds identification through Support\n\nVector Machines. Applied Soft Computing, v11(1), p908-915, 2011.\n\n[39] D. Saha, A. Hanson and S. Y. Shin. Development of Enhanced\n\nWeed Detection System with Adaptive Thresholding and Support Vector\n\nMachine. Proceedings of the International Conference on Research in\nAdaptive and Convergent Systems, ACM, p85-88, 2016.\n\n[40] A. J. Siddiqi, A. Hussain e M. Mustafa. Weed image classification using\n\nGabor wavelet and gradient field distribution. Computers and Electronics\nin Agriculture, v66, p53-61, 2009.\n\n[41] C. Hung, Z. Xu e S. Sukkarieh. Feature Learning Based Approach for\n\nWeed Classification Using High Resolution Aerial Images from a Digital\n\nCamera Mounted on a UAV. Remote Sensing, v6(12), p12037-12054.\n2014.\n\n[42] I. Colomina and P. Molina. Unmanned aerial systems for photogrammetry\n\nand remote sensing: A review. ISPRS Journal of Photogrammetry and\nRemote Sensing, v92, p79-97, 2014.\n\n[43] J. M. Pe\u00f1a, J. T. Torres-S\u00e1nchez, A. I. de Castro, M. Kelly and F. Lopez-\n\nGranados. Weed Mapping in Early-Season Maize Fields Using Object-\n\nBased Analysis of Unmanned Aerial Vehicle (UAV) Images. PLoS One,\nv8(10), e77151, 2013.\n\n[44] F. G. Costa, J. Ueyama, T. Braun, G. Pessin, F. S. Os\u00f3rio and P. A. Vargas.\n\nThe use of Unmanned and wireless sensor network in agricultural\n\napplications. Geoscience and Remote Sensing Symposium (IGARSS), 2012\nIEEE International, p5045-5048, 2012.\n\n[45] J. Primicerio, S. F. Gennaro and E. Fiorillo. A flexible unmanned aerial\n\nvehicle for precision agriculture. Springer, 2012.\n\n[46] JJ. T. Torres-S\u00e1nchez, F. Lopez-Granados, A. I. de Castro and J. M. Pe\u00f1a\n\nBarrag\u00e1n. Configuration and Specifications of an Unmanned Aerial\n\nVehicle (UAV) for Early Site Specific Weed Management. PLoS One, v8(3),\ne58210, 2013.\n\n[47] D. Gomez-Candon, A. I. De Castro and F. Lopez-Granados. Assessing\n\nthe accuracy of mosaics from unmanned aerial vehicle (UAV) imagery for\n\nprecision agriculture purposes in wheat. Precision Agric, v15, p44-56,\n2014.\n\n68\n\n\n\n[48] Lisa LAB. Deep Learning Tutorial. University of Montreal, 2015.\n\n[49] L. Deng and D. Yu. Deep Learning Methods and Applications.\n\nFoundations and Trends in Signal Processing, v7, 2013.\n\n[50] I. Arel, D. C. Rose and T. P. Karnowski. Deep Machine Learning \u2013 A New\n\nFrontier in Artificial Intelligence Research. The University of Tennessee,\nIEEE Computational Intelligence Magazine, 2010.\n\n[51] D. Ciresan, U. Meier and J. Schmidhuber. Multi-column Deep Neural\n\nNetworks for Image Classification. 2012.\n\n[52] A. Krizhevsky, I. Sutskever and G. E. Hinton. ImageNet Classification\n\nwith Deep Convolutional Neural Networks. Proceedings of Neural\nInformation Processing Systems, p1097-1105, 2012.\n\n[53] S. Haykin and N. Network. A comprehensive foundation. Neural\nNetworks, v2(2004), p41, 2004.\n\n[54] H. B. Demuth, M. H. Beale, O. De Jess and M. T. Hagan. Neural network\n\ndesign. Martin Hagan, 2014.\n\n[55] W. S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in\n\nnervous activity. The bulletin of mathematical biophysics, v5(4), p115-133,\n1943.\n\n[56] F. Rosenblatt. The perceptron: A probabilistic model for information\n\nstorage and organization in the brain. Psychological review, v65(6), p386,\n1958.\n\n[57] J. R. Parker. Algorithms for image processing and computer vision. John\nWiley &amp; Sons, 2010.\n\n[58] Y. Bengio. Learning deep architectures for AI. Foundations and trends in\nMachine Learning, v2(1), p1-127, 2009\n\n[59] K. Fukushima. Neocognitron: A self-organizing neural network model\n\nfor a mechanism of pattern recognition unaffected by shift in position.\n\nBiological cybernetics, v36(4), p193-202, 1980.\n\n[60] Y. LeCun, J. S. Denker, D. Hederson, R. E. Howard, W. Hubbard and\n\nL. D. Jackel. Handwritten digit recognition with a back-propagation\n\nnetwork. Advances in neural information processing systems, 1990.\n\n[61] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov and\n\nA. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv,\np1409.4842, 2014.\n\n69\n\n\n\n[62] N. Kalchbrenner, E. Grefenstette and P. A. Blunsom. A convolutional\n\nneural network for modelling sentences. arXiv preprint arXiv,\np1404.2188, 2014.\n\n[63] C. N. Dos Santos and M. Gatti. Deep convolutional neural networks for\n\nsentiment analysis of short texts. In Proceedings of the 25th International\nConference on Computational Linguistics (COLING), Dublin, Ireland, 2014.\n\n[64] I. Wallach, M. Dzamba and A. Heifets. AtomNet: A Deep Convolutional\n\nNeural Network for Bioactivity Prediction in Structure-based Drug\n\nDiscovery. arXiv preprint arXiv, p1510.02855, 2015.\n\n[65] Van der Walt, S. J. G. Sch\u00f6nberger, J. Nunez-Iglesias, F. Boulogne,\n\nJ. D. Warner, N. Yager, E. Gouillart and T. Yu scikit-image: image\n\nprocessing in Python. PeerJ, v2, p453, 2014.\n\n[66] G. Bradski. The OpenCV Library. Dr. Dobb\u2019s Journal of Software Tools,\n2000.\n\n[67] L. Zanni, T. Serafini and G. Zanghirati. Parallel Software for Training\n\nLarge Scale Support Vector Machines on Multiprocessor Systems. Journal\nof Machine Learning Research 7, p1467-1492, 2006.\n\n[68] Y. Jia. Caffe: An open source convolutional architecture for fast feature\n\nembedding. http://caffe.berkeleyvision.org/, 2013.\n\n[69] L. Brett.. Machine learning with R. Packt Publishing Ltd, 2013.\n\n[70] \"M. Hall, E. Frank, G. Holmes and Pfahringer The WEKA Data Mining\n\nSoftware: An Update. SIGKDD Explorations, v11(1), 2009.\n\n70\n\n\n\tSum\u00e1rio\n\tLista de Figuras\n\tLista de Tabelas\n\tIntrodu\u00e7\u00e3o\n\tFundamenta\u00e7\u00e3o Te\u00f3rica\n\tVis\u00e3o Computacional\n\tErvas Daninhas e a Soja\n\tEst\u00e1dios Fenol\u00f3gicos da Soja\n\n\tVANTs \u2013 Ve\u00edculos A\u00e9reos N\u00e3o Tripulados\n\tSegmenta\u00e7\u00e3o\n\tSLIC Superpixels\n\n\tExtra\u00e7\u00e3o de Atributos\n\tMatrizes de Coocorr\u00eancia - GLCM\n\tHistograma de Gradientes Orientados\n\tPadr\u00f5es Bin\u00e1rios Locais\n\tEspa\u00e7os de cores RGB, HSV e CIELab\n\n\tClassificadores\n\tC4.5\n\tAdaBoost\n\tFlorestas Aleat\u00f3rias\n\tM\u00e1quinas de Vetores de Suporte\n\n\n\tTrabalhos Correlatos\n\tAprendizado Profundo\n\tRedes Neurais Artificiais\n\tConvolu\u00e7\u00e3o\n\tRedes Neurais Convolucionais\n\n\tMetodologia\n\tVis\u00e3o Geral\n\tPlantio da Soja\n\tCaptura de imagens\n\tMateriais\n\tPlanejamento de voo\n\tBanco de imagens\n\n\tSegmenta\u00e7\u00e3o\n\tExtra\u00e7\u00e3o de Atributos\n\tClassifica\u00e7\u00e3o\n\tM\u00e9tricas de Avalia\u00e7\u00e3o\n\n\tResultados e discuss\u00f5es\n\tAvalia\u00e7\u00e3o com Classes Balanceadas\n\tAvalia\u00e7\u00e3o com Classes Desbalanceadas\n\tAvalia\u00e7\u00e3o por Extratores\n\tSoftware Pynovis\u00e3o\n\tClassifica\u00e7\u00e3o em Imagens\n\n\tConclus\u00e3o\n\tRefer\u00eancias"}]}}}