{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-BG.04043"}, {"@name": "filename", "#text": "BGP_1992_6_3_4_07_Aplicacao_de_inteligencia_artificial_na_identificacao.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "APLICA\u00c7\u00c3O DE INTELIG\u00caNCIA ARTIFICIAL NA IDENTIFICA\u00c7\u00c3O DE ELETROF\u00c1CIES REDES NEURONIAIS VERSUS AN\u00c1LISE\nDISCRIMINANTE\nAPPLICATION OFARTIFICIAL INTELUGENCE TO ELETROF\u00c1CIES IDENTIFICATION NEURAL NETWORKS VERSUS DISCRIMINANT ANALYSIS\nFernando da Silva Rodrigues1 1 e Izidro Avelino de Queiroz Neto2\nRESUMO \u2014 Utiliza-se uma Rede Neuronial (RN), treinada com dados de perfis el\u00e9tricos e de testemunho, para identificar eletrof\u00e1cies. Diferentemente dos sistemas especialistas tradicionais, as RN's n\u00e3o necessitam de regras e o aprendizado se d\u00e1 por meio da modifica\u00e7\u00e3o dos pesos das conex\u00f5es, ap\u00f3s a entrada de est\u00edmulos. Al\u00e9m disso, as RN's est\u00e3o preparadas para tratar dados incompletos e degradam suavemente quando partes da rede s\u00e3o destru\u00eddas. Perfis el\u00e9tricos de raios gama, porosidade neutr\u00f4nica e densidade da matriz, e ainda dados de testemunho, s\u00e3o utilizados como entrada para uma rede de retropropaga\u00e7\u00e3o. Nesta, o erro se propaga da camada de sa\u00edda para a de entrada, modificando os pesos nas conex\u00f5es proporcionalmente \u00e0 contribui\u00e7\u00e3o destas conex\u00f5es para o erro total. Ap\u00f3s treinada, a rede \u00e9 capaz de identificar eletrof\u00e1cies com um rendimento t\u00e3o bom quanto o da an\u00e1lise discr\u00edminante. Acredita-se que combinando a neurocomputa\u00e7\u00e3o com m\u00e9todos tradicionais, como a an\u00e1lise discr\u00edminante, pode-se obter a solu\u00e7\u00e3o de muitos problemas na defini\u00e7\u00e3o de eletrof\u00e1cies.\n(Originais recebidos em 10.06.92).\nABSTRACT \u2014 Electro-facies are identified by a neurai network (NN) trained with weii-iog and core data. Uniike traditionai expert systems, it is unnecessary to feed ruies to an NN, iearning being achieved instead through changes that occur in the network \"s connection weights foilowing stimuii input. Furthermore, NNs are we/f suited for anaiyzing missing or incomplete data, and they \"degrade gracefuiiy\" when a portion of the network is destroyed. in the proposed backpropagation network, gamma-ray, n\u00eautron porosity, and bulk density iogs, coupied with core data, are used as input. The error is propagated backwards from the output layer to the input iayer, and the connection weights are modified in proportion to the contribution of the connections to the overaii error. Once trained, th/s NN performs as weii as discriminant anaiysis in identifying eiectric-log facies. The authors beiieve that the combination of neurocomputing and traditional computing methods iike discriminant anaiysis can help in the soiution of many probiems in electro-facies identification.\n(Expanded abstract avaiiabie at the end o f the paper).\n1\t- INTRODU\u00c7\u00c3O\nA utiliza\u00e7\u00e3o de intelig\u00eancia artificial torna-se cada vez mais comum nos mais diversos setores. O uso da m\u00e1quina, em tarefas que envolvem decis\u00f5es e an\u00e1lise de dados com estrutura complexa, \u00e9 hoje uma realidade bastante palp\u00e1vel. Recentemente, assistiu-se a uma demonstra\u00e7\u00e3o impressionante do potencial dos m\u00e9todos de intelig\u00eancia artificial na Ind\u00fastria B\u00e9lica. Mas m\u00edsseis \"inteligentes\", e programas de an\u00e1lise de alvos militares, n\u00e3o s\u00e3o \u00f3s \u00fanicos frutos do intenso esfor\u00e7o dispendido nos \u00faltimos anos em busca de m\u00e1quinas com comportamento mais pr\u00f3ximo ao do\nhomem. S\u00e3o encontradas aplica\u00e7\u00f5es em todas as \u00e1reas do conhecimento.\nAs Redes Neuroniais s\u00e3o uma tentativa de simular o funcionamento do c\u00e9rebro, utilizando um modelo extremamente simplificado das intera\u00e7\u00f5es entre os neur\u00f4nios biol\u00f3gicos. A escolha de um modelo biol\u00f3gico se justifica pela dificuldade encontrada por sistemas tradicionais de computa\u00e7\u00e3o para resolver problemas que parecem triviais aos organismos vivos.\nDentre os problemas j\u00e1 mencionados, existe uma classe muito interessante para a ind\u00fastria do petr\u00f3leo: problemas cujos dados estejam sujeitos a falhas e com elevado n\u00edvel de ru\u00eddo.\n1\t- Setor de Tratamento de Dados Geol\u00f3gicos (SETRAG); Divis\u00e3o de Opera\u00e7\u00f5es Geol\u00f3gicas (DIGEO), Departamento de Explora\u00e7\u00e3o (DEPEX)\nAv. Rep\u00fablica do Chile, 65, Centro, CEP 20035, Rio da Janeiro, RJ, Brasil.\n2\t- Setor de Processamento de Dados (SEPROC), Superintend\u00eancia de Pesquisa de Explora\u00e7\u00e3o e Produ\u00e7\u00e3o (SUPEPI, Centro de Pesquisas\n(CENPES), Cidade Universit\u00e1ria, Quadra 7, Ilha do Fund\u00e3o, CEP 21910, Rio de Janeiro, RJ, Brasil.\n0 c\u00e9rebro dos organismos vivos, em particular o do homem, parece ter resolvido de forma satisfat\u00f3ria problemas que desafiam nossas m\u00e1quinas mais sofisticadas. Alguns exemplos s\u00e3o mencionados a seguir:\n\u2014\to reconhecimento da voz humana mesmo em ambientes intensamente contaminados por ru\u00eddo \u00e9 um fato not\u00e1vel. Um bom exemplo \u00e9 uma conversa com v\u00e1rios interlocutores falando ao mesmo tempo;\n\u2014\tpodem-se processar dados incompletos e com grande facilidade de erros. Um exemplo \u00e9 a capacidade de se reconhecer uma melodia mesmo que v\u00e1rias notas musicais estejam incorretas ou ausentes;\n\u2014\to c\u00e9rebro \u00e9 muito mais tolerante a falhas nas unidades de processamento do que as m\u00e1quinas convencionais. Imagine o que aconteceria com um computador digital se periodicamente fosse submetido a um tratamento semelhante ao dispensado aos lutadores de boxe.\nA classifica\u00e7\u00e3o de eletrof\u00e1cies por meio de perfis el\u00e9tricos \u00e9 um problema que j\u00e1 mereceu a aten\u00e7\u00e3o de muitos especialistas. Atualmente, uma das solu\u00e7\u00f5es mais frequentes \u00e9 a an\u00e1lise discriminante, cujos resultados ser\u00e3o comparados com os das RN's.\n0 termo \"an\u00e1lise discriminante\" \u00e9 a designa\u00e7\u00e3o gen\u00e9rica de v\u00e1rios m\u00e9todos utilizados para classificar amostras em dois ou mais grupos de caracter\u00edsticas conhecidas, utilizando uma ou mais vari\u00e1veis quantitativas.\n2\t- HISTORICO\nAs origens da neurocomputa\u00e7\u00e3o podem ser identificadas nos trabalhos de Jackson (1958), que criticou as doutrinas \"localizacionistas\", isto \u00e9, afirmou que a distribui\u00e7\u00e3o de tarefas no c\u00e9rebro n\u00e3o se d\u00e1 de forma completamente localizada, como se pensava no s\u00e9c. XIX. Este modelo, embora bastante rudimentar, do funcionamento do c\u00e9rebro, contribuiu para o nascimento das primeiras id\u00e9ias de processamento distribu\u00eddo em paralelo.\nNos anos 40, iniciou-se o estudo dos modelos de aprendizado do c\u00e9rebro (Hebb, 1949).\nEm 1956, realizou-se a primeira confer\u00eancia sobre intelig\u00eancia artificial, patrocinada pela funda\u00e7\u00e3o Rockfeller, na qual foram lan\u00e7ados por pesquisadores de todo o mundo os fundamentos da intelig\u00eancia artificial e da neurocomputa\u00e7\u00e3o.\nNos anos 50, a utiliza\u00e7\u00e3o do computador digital permitiu a programa\u00e7\u00e3o de modelos de RN's que simulavam a intera\u00e7\u00e3o entre as c\u00e9lulas do c\u00e9rebro. Como estes modelos apresentavam caracter\u00edsticas de aprendizado semelhantes \u00e0s dos organismos vivos, acreditava-se que, com uma m\u00e1quina suficientemente grande e r\u00e1pida, poder-se-ia reproduzir um c\u00e9rebro humano.\nO sonho acabou em 1969, quando Minsky e Pa-pert (Minsky et al. 1969), provaram matematicamente que os perceptrons (uma das mais simples RN's estudadas na \u00e9poca), eram incapazes de resolver\nv\u00e1rios problemas. Os financiamentos e, conseq\u00fcente-mente, as pesquisas sobre o assunto diminu\u00edram drasticamente nos anos 70.\nEm 1982, John Hopfield, do CalTech, apresentou na Academia Nacional de Ci\u00eancias dos EUA um trabalho sobre RN's. Este foi o primeiro trabalho sobre o tema relatado \u00e0 Academia desde os anos 60 (Hopfield, 1982). A posi\u00e7\u00e3o respeitada de Hopfield na comunidade cient\u00edfica contribuiu para que outros pesquisadores se interessassem pelas RN's. Este interesse foi acompanhado por um significativo aumento do suporte para as pesquisas.\nAinda na d\u00e9cada de 80, mostrou-se que os problemas levantados por Minsky e Papert poder\u00edam ser resolvidos por redes de arquitetura mais complexa que a dos perceptrons (Rumelhart etal. 1986).\nAtualmente, o estudo das RN's encontra-se em franca expans\u00e3o, tendo atingido recentemente a ind\u00fastria do petr\u00f3leo (Baldwin et at. 1989; Baldwin et al. 1990; McCormack, 1991).\n3\t- O MODELO BIOLOGICO\nO neur\u00f4nio \u00e9 a unidade fundamental do c\u00e9rebro (fig. 1). O n\u00facleo atua como uma esp\u00e9cie de processador, que tem como entradas sinais provenientes de v\u00e1rios outros neur\u00f4nios (frequentemente milhares). O caminho por onde se propagam os sinais de entrada s\u00e3o os dendritos. Se a combina\u00e7\u00e3o dos sinais de entrada (usualmente uma simples soma), ultrapassa um determinado n\u00edvel, o neur\u00f4nio produz um sinal de sa\u00edda. O caminho por onde se propaga o sinal de sa\u00edda \u00e9 chamado ax\u00f4nio.\nA transfer\u00eancia do sinal do ax\u00f4nio para outros neur\u00f4nios se d\u00e1 nas sinapses. Os ax\u00f4nios podem se ramificar, podendo desta forma enviar o mesmo sinal para muitos outros neur\u00f4nios.\nAcredita-se que \u00e9 nas sinapses onde reside o que se chama de mem\u00f3ria. A quantidade de informa\u00e7\u00e3o\nFig. 1 - Representa\u00e7\u00e3o simplificada de um neur\u00f4nio biol\u00f3gico.\nFig. 1 - SimpUfied representation of biological neuron.\nFig. 2 - Modelo do neur\u00f4nio artificial.\nFig. 2 ~ Model of artificiai neuron.\n(n\u00edvel de sinal), transmitida de um neur\u00f4nio para outro depende do qu\u00e3o forte \u00e9 a liga\u00e7\u00e3o sin\u00e1ptica. \u00c9 esta for\u00e7a de liga\u00e7\u00e3o que se modifica quando o c\u00e9rebro aprende.\nO modelo apresentado n\u00e3o \u00e9, e nem pretende ser completo. A complexidade do comportamento dos seres vivos pressup\u00f5e algo mais sofisticado. No entanto, \u00e9 not\u00e1vel a quantidade de aplica\u00e7\u00f5es bem sucedidas partindo-se, simplesmente, deste modelo.\nPara se ter uma no\u00e7\u00e3o do n\u00edvel de complexidade do c\u00e9rebro, basta multiplicar o n\u00famero estimado de neur\u00f4nios do homem, da ordem de 101 0, pelo n\u00famero m\u00e9dio de sinapses de um neur\u00f4nio, da ordem de 103.\n4\t- REDES NEURONIAIS ARTIFICIAIS\nUma RN artificial \u00e9 composta por elementos an\u00e1logos aos neur\u00f4nios biol\u00f3gicos (fig. 2). Associam-se \u00e0s conex\u00f5es entre estes elementos pesos escalares que desempenham o mesmo papel da for\u00e7a de liga\u00e7\u00e3o sin\u00e1ptica. S\u00e3o estes pesos que se modificam durante o processo de aprendizado (treinamento).\nAnteriormente, foi visto que nos neur\u00f4nios biol\u00f3gicos s\u00f3 existe sinal de sa\u00edda ap\u00f3s um certo n\u00edvel de soma dos sinais de entrada. As fun\u00e7\u00f5es que se mostram mais adequadas para reproduzir este comportamento nos neur\u00f4nios artificiais s\u00e3o as do tipo sigm\u00f3i-de, por exemplo :\nf(x) = (1+6? (\u20141\nA sa\u00edda de um neur\u00f4nio serve de entrada para outros neur\u00f4nios. A cada linha, conectando dois neur\u00f4nios artificiais, associa-se um \u00fanico peso escalar. S\u00e3o estes pesos que se modificam durante o treinamento das RN's.\nUsualmente, os neur\u00f4nios artificiais se organi zam em camadas:\n\u2014\tcamada de entrada \u2014 recebe os sinais de entrada;\n\u2014\tcamadas intermedi\u00e1rias (ou escondidas);\n\u2014\tcamada de sa\u00edda \u2014 fornece as respostas da rede.\nExistem duas fases principais de funcionamento de uma rede \u2014 aprendizado e verifica\u00e7\u00e3o (Recall).\nDurante o aprendizado, os pesos se modificam at\u00e9 que a rede encontre um ponto de equil\u00edbrio. Nesta fase, v\u00e1rios sinais s\u00e3o introduzidos na camada de entrada e, opcionalmente, na camada de sa\u00edda.\nO mais importante durante o processo de aprendizado \u00e9 a forma de como os pesos se modificam \u00e0 medida que s\u00e3o fornecidos dados \u00e0 rede. Existem diversas estrat\u00e9gias de modifica\u00e7\u00e3o destes pesos. Neste caso espec\u00edfico, utiliza-se a retropropaga\u00e7\u00e3o (Rume-Ihart et at. 1986).\n5\t- DIFEREN\u00c7AS ENTRE A COMPUTA\u00c7\u00c3O TRADICIONAL E AS REDES NEURONIAIS\nAs RN's possuem caracter\u00edsticas pouco ou nunca encontradas em programas \"inteligentes\" tradicionais, onde algumas podem ser vistas:\n\u2014\taprendizado por exemplos \u2014 as RN's modificam\nseus pesos procurando reproduzir o comportamento dos dados, sendo, portanto, diferentes dos sistemas especialistas, onde \u00e9 necess\u00e1ria a participa\u00e7\u00e3o de uma pessoa experiente no problema que se pretende resolver, para gerar as regras que definir\u00e3o o comportamento do sistema;\n\u2014\tmem\u00f3ria distribu\u00edda \u2014 as RN's est\u00e3o mais adapta-\ndas para tratarem entradas incompletas que os sistemas tradicionais, al\u00e9m de serem mais tolerantes a falhas em suas unidades. Isto se deve \u00e0 pr\u00f3pria ess\u00eancia de constru\u00e7\u00e3o das redes, elementos de processamento bastante simples e trabalhando em paralelo. Estas caracter\u00edsticas garantem que \u00e9 muito pequena a chance de qualquer elemento desempenhar um papel fundamental no processamento. Por outro lado, em sistemas tradicionais, \u00e9 normalmente muito f\u00e1cil identificar elementos fundamentais, isto \u00e9, elementos sem os quais o sistema entra em colapso;\tv\n\u2014\treconhecimento de padr\u00f5es \u2014 da mesma forma que o c\u00e9rebro, as redes utilizam processamento em paralelo de forma maci\u00e7a. Acredita-se que esta forma de processamento seja a principal respons\u00e1vel pela enorme capacidade do homem de reconhecimento de padr\u00f5es.\n6\t- NOSSO PROBLEMA\nOs perfis el\u00e9tricos s\u00e3o registros de profundidade das propriedades f\u00edsicas das diferentes litologias presentes em um po\u00e7o. Estas propriedades podem ser a resistividade, densidade, porosidade, radioatividade, etc.\nOs princ\u00edpios f\u00edsicos das medidas efetuadas pelas ferramentas de perfilagem, embora conhecidos, levam a equa\u00e7\u00f5es de dif\u00edcil solu\u00e7\u00e3o. Al\u00e9m disso, em muitos casos, alguns par\u00e2metros destas equa\u00e7\u00f5es s\u00e3o desconhecidos, devido \u00e2s condi\u00e7\u00f5es peculiares de um po\u00e7o de petr\u00f3leo (altas press\u00f5es, temperaturas superiores a 100 \u00b0C, interfer\u00eancias decorrentes do fluido de perfura\u00e7\u00e3o, etc.).\nPor outro lado, os po\u00e7os testemunhados oferecem uma \u00f3tima base de dados, com os resultados desejados do processamento e interpreta\u00e7\u00e3o dos perfis ef\u00e9tricos.\n7 - solu\u00e7\u00e3o proposta\n7.1\t\u2014 Descri\u00e7\u00e3o da Rede\nA identifica\u00e7\u00e3o de eletrof\u00e1cies utilizando RN's parece uma solu\u00e7\u00e3o vi\u00e1vel, considerando a boa performance deste m\u00e9todo em outros problemas de classifica\u00e7\u00e3o.\nO objetivo da rede proposta \u00e9 indicar, a partir dos perfis, qual a eletrof\u00e1cies predominante em cada profundidade. Durante o processo de treinamento, s\u00e3o apresentados \u00e0 rede os valores dos perfis el\u00e9tricos (raios gama, densidade e porosidade neutr\u00f4nica) e a eletrof\u00e1cies correspondente para cada profundidade. Estas eletrof\u00e1cies s\u00e3o obtidas a partir das descri\u00e7\u00f5es de testemunho. O po\u00e7o escolhido para treinamento foi o 1-RUC-1-AM.\nA rede tem a seguinte arquitetura b\u00e1sica (fig. 3):\n\u2014\tuma camada de entrada, onde cada neur\u00f4nio corresponde a um dos perfis. Um neur\u00f4nio para raios gama, um para densidade, e um para a porosidade neutr\u00f4nica (tr\u00eas neur\u00f4nios);\n\u2014\tuma camada intermedi\u00e1ria (dez neur\u00f4nios);\n\u2014\tuma camada de sa\u00edda (cinco neur\u00f4nios), onde cada neur\u00f4nio corresponde a uma das eletrof\u00e1cies poss\u00edveis. A classifica\u00e7\u00e3o da rede \u00e9 dada pelo neur\u00f4nio de sa\u00edda com maior ativa\u00e7\u00e3o.\nA escolha do algoritmo retropropaga\u00e7\u00e3o (Mc-Clelland e Rumelhart, 1988), para o aprendizado, deve-se \u00e0 sua generalidade (Simpson, 1990) e\u00e0 facilidade de implementa\u00e7\u00e3o.\nPara o treinamento foi montado um arquivo contendo os valores dos perfis, que servem de entrada para a rede, e o valor de sa\u00edda desejado. Este valor de sa\u00edda \u00e9 um c\u00f3digo bin\u00e1rio onde cada bit representa uma das eletrof\u00e1cies.\nFig. 3 - Arquitetura da rede proposta. Fig. 3 - Proposed network architecture.\nO aprendizado consiste na apresenta\u00e7\u00e3o do arquivo de treinamento diversas vezes \u00e2 rede. Em cada passo, o algoritmo procura minimizar o erro entre a classifica\u00e7\u00e3o calculada pela rede e a eletrof\u00e1cies correta.\nCada passagem completa pelo arquivo de treinamento \u00e9 chamada de \u00e9poca. Para medir o desempenho da rede, faz-se a verifica\u00e7\u00e3o treca!!} do arquivo de treinamento a cada \u00e9poca, calculando-se o percentual de acerto obtido. Considera-se que a rede est\u00e1 suficientemente treinada quando o percentual de acerto atinge um n\u00edvel previamente estabelecido.\nDurante o per\u00edodo de projeto da rede, constatou-se que a escolha da forma de codifica\u00e7\u00e3o dos dados \u00e9 de fundamental import\u00e2ncia. Uma escolha inadequada reduz enormemente a capacidade de classifica\u00e7\u00e3o. A melhor performance \u00e9 obtida quando se utiliza uma transforma\u00e7\u00e3o linear para colocar todos os dados entre \u2014 1 e 1.\n7.2\t\u2014 Implementa\u00e7\u00e3o e Resultados\nInicialmente, usa-se um simulador que estabelece um ambiente para o projeto de RN's (Klimasauskas e Guiver, 1988), instalado em um microcomputador Itautec 286. A demora do processo de treinamento (v\u00e1rias horas) levou \u00e0 implementa\u00e7\u00e3o do algoritmo em FORTRAN, em ambiente IBM/3090. Para maiores detalhes do algoritmo, veja McCIelland e Rumelhart (1988).\nAo findar cada \u00e9poca, \u00e9 feita a verifica\u00e7\u00e3o do arquivo de treinamento e \u00e9 calculado o percentual de acerto. Al\u00e9m disso, \u00e9 montada uma matriz contendo o n\u00famero de padr\u00f5es inclu\u00eddos em cada classe.\nNo ambiente do CENPES (IBM/3090-150-VF), o treinamento da rede consumiu aproximadamente 15 min de CPU. 0 arquivo de treinamento tem 2 000 pontos, e o \u00edndice de acertos estabelecido como satisfat\u00f3rio foi de 82%. Na figura 4 pode ser acompanhada a evolu\u00e7\u00e3o do aprendizado.\nSegundo a literatura (McCIelland e Rumelhart, 1988), a evolu\u00e7\u00e3o do aprendizado \u00e9 dependente do conjunto de pesos iniciais. Para testaresta depend\u00eancia, efetuou-se tr\u00eas seq\u00fc\u00eancias de treinamento, com pesos iniciais distintos. Como pode ser demonstrado na figura 4, n\u00e3o foram observadas diferen\u00e7as significativas no treinamento.\nPara medir a capacidade de generaliza\u00e7\u00e3o da rede, ou seja, de classificar dados desconhecidos, foram utilizados os perfis do po\u00e7o 4-RUC-2-AM (fig. 5), obtendo-se 90% de acerto quando comparados os resultados com a classifica\u00e7\u00e3o do testemunho. O tempo de CPU consumido na verifica\u00e7\u00e3o \u00e9 de aproximadamente 0.5 segundos.\nUma quest\u00e3o interessante \u00e9 a raz\u00e3o da melhoria da performance na verifica\u00e7\u00e3o {recall). Ap\u00f3s uma an\u00e1lise mais detalhada dos dados, concluiu-se que os dados do po\u00e7o de treinamento (1-RUC-1) t\u00eam uma estrutura mais complexa que o po\u00e7o de verifica\u00e7\u00e3o (4-RUC-2). Em outras palavras, a rede foi treinada pa-\nTreinamento para o po\u00e7o 1-RUC-1\n1 QC*dtl\nFig. 4 \u25a0 Evolu\u00e7\u00e3o do aprendizado com pesos iniciais distintos. fig. 4 - Evofution of \u00edearning with different initiai weights.\nFig. 5 - Resultado da classifica\u00e7\u00e3o do po\u00e7o 4-RUC-2 (AMV Apresentam-se, tambdm, para compara\u00e7\u00e3o, os dados de testemunho e o resultado da an\u00e1lise de discriminantes.\nFig. 5 - Classification results for Wall 4-RUC-2-AM. (A) discriminant analysis; (B) NN; (C) core sampling.\nA B C\nra situa\u00e7\u00f5es mais complexas do que as apresentadas durante a verifica\u00e7\u00e3o. Quando a rede foi treinada com o po\u00e7o 4-RUC-2 e feita a verifica\u00e7\u00e3o com o po\u00e7o 1-RUC-1,o \u00edndice de acertos cai para 78%.\nOs resultados da an\u00e1lise discriminante, nos casos discutidos anteriormente, s\u00e3o ligeiramente inferiores aos das redes. A queda de performance \u00e9 de aproxi madamente 5%. A an\u00e1lise discriminante foi realizada com o aux\u00edlio do pacote estat\u00edstico SAS.\nOutro ponto interessante \u00e9 o comportamento das redes e da an\u00e1lise discriminante quando os dados apresentam falhas, isto \u00e9, um dos perfis de entrada pode estar ausente. No quadro I, apresentam-se os resultados obtidos, com a mudan\u00e7a dos perfis de entrada, na classifica\u00e7\u00e3o de eletrof\u00e1cies do po\u00e7o 4-RUC-2. Os \u00edndices de acerto indicam que a an\u00e1lise discriminante parece ser mais tolerante a falhas que a rede proposta. Nota-se, por\u00e9m, que os m\u00e9todos podem ser vistos como complementares, isto \u00e9, o uso conjunto melhora o desempenho.\nQUADRO l/CHAf\u00edT /\nI M\u00e9todo\tGR, Nphi, Rhob\tNphi, Rhob\tGR, Rhob\tGR, Nphi\nRede iDiscrimin.\t90.5% 89%\t81.5% 70.2%\t68.6% 88%\t60% 65%\nFoi testada tamb\u00e9m a toler\u00e2ncia das RN's a danos em sua estrutura. 0 \u00edndice de acertos cai de 90% para 89% quando se eliminam dois neur\u00f4nios de rede. Esta \"degrada\u00e7\u00e3o suave\" de desempenho seria de grande utilidade para redes implementadas em hardware, pois poder\u00edam ser projetadas m\u00e1quinas com grande toler\u00e2ncia a falhas em seus componentes.\n8\t- CONCLUS\u00d5ES\nUtiliza-se uma RN para auxiliar no trabalho de classifica\u00e7\u00e3o de eletrof\u00e1cies. Os resultados obtidos indicam que esta nova metodologia \u00e9 promissora.\nPodem ser ressaltados como pontos fortes das RN's a capacidade de \"aprender\" a partir dos dados, permitindo construir redes treinadas para cada situa\u00e7\u00e3o espec\u00edfica, e a portabilidade, pois ap\u00f3s o treinamento a rede pode ser instalada em micros, ocupando pouco espa\u00e7o (aprox. 10k bytes}. As redes tamb\u00e9m podem ser empregadas em conjunto com outras ferramentas, como a an\u00e1lise discriminante, fornecendo informa\u00e7\u00f5es complementares.\nA performance da rede proposta na classifica\u00e7\u00e3o de eletrof\u00e1cies serve de est\u00edmulo para a busca de solu\u00e7\u00f5es de outros problemas, do cotidiano exploracio-nista, com o aux\u00edlio desta nova metodologia.\nAGRADECIMENTOS\n\u00c3 colabora\u00e7\u00e3o dos Ge\u00f3logos Elza Santa Oliveira e Jos\u00e9 Roberto Barbosa Corr\u00eaa (DENOC/DINTER/ SEGED), pela an\u00e1lise dos testemunhos dos po\u00e7os utilizados no treinamento e na verifica\u00e7\u00e3o da RN, ao APD Gustavo do Ros\u00e1rio Batista (DEPEX/DITREX/ SEPRAN), pela participa\u00e7\u00e3o na fase inicial do trabalho, aos Ge\u00f3logos Dirceu Abrah\u00e3o (DEPEX/DIGEO) e Paulo Roberto Avila (DEPEX/DIGEO/SETRAG) pela corre\u00e7\u00e3o de erros nos originais, ao Geof\u00edsico Andr\u00e9 White (DEPEX/DIGEO/SETRAG), pela ajuda na elabora\u00e7\u00e3o dos gr\u00e1ficos, ao Prof. Manoel Ten\u00f3rio, da Purdue University, pelas sugest\u00f5es apresentadas para a constru\u00e7\u00e3o da Rede e a The\u00f3gnis Castejon Rodrigues (DEPEX/DIGEO/SEDEX), pelo est\u00edmulo e apoio em todas as etapas do trabalho.\nREFER\u00caNCIAS BIBLIOGR\u00c1FICAS\nBALDWIN, J. L., OTTT, D. N. Computer emulation of human mental processes: application of neural network simulator to problem in well log pattern recognition. In: CONFE-RENCE ON ARTIFICIAL INTELIGENCE IN PETROLEUM EXPLORATION AND PRODUCTION, Texas, 1989. Proceedings. .. Texas: A &amp;M University, 1989. p. 145-175.\nBALDWIN. J. L\u201e BATEMAN, M. B., WH\u00caATLEY, C. L. Application of a neural network to the problem of mineral identificaiion from well logs. The log analysts, p. 279-293, sep./oct. 1990.\nHEBB, D. O. The organization of behavior. New York: Wiley, 1949.\nHOPFI\u00caLD, J. J. Neural networks and physical systems with emergent collective computational. Proceedings of the Academy of Sciences, v. 79, p. 2554-2558, 1982.\nJACKSON, J. On localization. In: SELECTED WRITINGS, v. 2. New York: Basic Book, 1958.\nKLIMASAUSKAS, C. C., GUIVER, J. P. Neural Works. Pittsburgh: Neuralware, 1988.\nMcCORMACK, M. D. Neural computing in geophysics. Geo-physics: the leading edge of exploration, v. 10, n. 1, p. 1115, 1991.\nMcCLELLAND, J. L., RUMELHART, D. E. Explorations in parallel distributed processing: a handbook of models, program and exercices. Cambridge: Mit Press, 1988. 344 p.\nMINSKY, M., PAPERT, S. Perceptrons. Massachusetts: Mit Press, 1969.\nRUMELHART, D. E. et al. Parallel distributed processing: explorations in the microstructure of cognition. Cambridge: Mit Press, 1986. 2 v.\nSIMPSON, K. S. Artificial neural system. [s.l.]: Pergamon Press, 1990.\nEXPANDED ABSTRACT\nNeura! networks (NNs) attempt to simu\u00edate the workings of the human brain using an extremely simpHfied mode! of interactions between biological neurons.\nThe study of learning models of the brain began in the 1940s (Hebb, 1949). in 1956,at the first conference on artificial intelligence, sponsored by the Rockefeller Foundation, researchers from around the world launched the foundations of artificia! intelligence and neurocomputing. During the 1950s, use of the digita! computer made it possible to programm NN models that simulated the Interaction of brain cells. Since these models disp\u00edayed tearning characteristics similar to those of living organisms, it was believed that a machine of sufficient size and speed coutd reproduce the human brain.\nThis dream ended in 1969 when Minsky and Papert (Minsky etal. 1969) proved mathematically thatperceptrons (one of the simplest NNs under study at that time) were incapable of solving various problems. Funding of research into the topic, and consequentiy research itself, fell sharply during the 1970s.\nDuring the 1980s, it was shown that the problems raised by Minsky and Papert coutd be sotved using architecturai networks more comptex than perceptrons (Rumelhart et al. 1986). At present, the study of NNs is expanding stead\u00fcy and has recently reached the oi! industry (Baldwin et al. 1989; Baldwin et al. 1990; and McCormack, 1991).\nThe neuron is the basic unit of the brain. Its nucieus acts as a kind of processor, whose input are the signals from various other neurons (often from thousands of them). The dendrites are the paths along which these input signals are propagated. tf the combination of input signals (usually a simpte summation) surpasses a certain levei, the neuron wi/l produce an output signa!, which traveis alonga path called the axon.\nThe signa! trave!ing along the axon is transmitted to other neurons at the synapses. Axons can brandi out, sending the same signa! to many other neurons. !t is believed that what is usually known as memory resides at the synapses. The quantity of information (signa! levei) transmitted from one neuron to another depends upon how strong the synaptic connection is. It is the strength of this connection that is changed when the brain learns.\nAn artificial NN consists of e/ements analogous to biological neurons. Associated to the connections between these elements are scaled weights that play the same role as the strength of the synaptic connection. These weights are what change during the tearning process (i.e., training).\nElectric logs are depth records of the physica! properties of the different lithologies found in a well. These properties indude resistivity, density, porosity, radi\u00f2activity, and others.\nAlthough the physica! principies behind the measurements taken by togging tools are known, they produce hard-to-solve equations. Furthermore, some\nparameters of these equations are often unknown because of the particular conditions disp\u00edayed by an oi! well (e.g., high pressures, temperatures above 100 deg.\nC, or interferences caused by drilling mud). On the other hand, cored wellsprovide an exceUent data base, with the desired results from the processing and interpretation of E-logs.\nUsing NNs in the identification of electric-log f\u00e1cies would appear to be a viabte proposal, considering how well this method has handled other classification problems. The purpose of the network proposed herein is to use well logs to indica te the predominant electro-facies at each depth. During the trainingprocess, the network is fed the E-!og values (gamma ray, density, and n\u00eautron porosity) and the corresponding electric-log f\u00e1cies obtained through core descriptions for each depth. Well 1-RUC-1-AM waschosen for training purposes.\nThe network's basic architecture (see fig. 3) consists of:\n\u2014\ta three-neuron input layer, where each neuron corresponds to one of the logs (one neuron for gamma-ray logs, one for density, and one for n\u00eautron porosity);\n\u2014\ta ten-neuron intermediary layer;\n\u2014\ta five-neuron output layer, where each neuron corresponds to one of the possible electric-log f\u00e1cies. Classification of the network is defined by the most-activated output neuron.\nAn NN has two phases of operation: learning and recall. During the learning process, the connection weights are modified until the network reaches apoint of equitibr\u00edum. In this phase, various signals are introduced at the input layer and, optionally, at the output layer. What is most important during learning is the way in which the weights are modified as data are fed into the network. There are a number of strategies for alter ing these weights. !n this specific case, backpropagation is used.\nFor training purposes, a file was compiied containing a/I log values (which serve as input) and the desired output value. The latter is a binary code where each bit represents one of the etectro-facies. Learning consists of feeding the training fite to the network severa! times. At each step, the a/gorithm endeavors to minimize the error between the classification calculated by the network and the correct electric-log f\u00e1cies.\nLogs from Well 4-RUC-2-AM were used to measure the network's capacity for genera\u00fczation that is, its ability to dassify nove! data. A comparison of NN results with the classification according to core samples yieldeda misdassification rate of 10%.\nToterance of damage to the NN structure was also tested. The misdassification rate encreases from 10% to 11% when two neurons are removed from the network. This \"gracefu! degrading\" in performance wouldbe most usefu! for networks implemented in hardware.\nThe performance achieved by the proposed network in dassifying electric-log f\u00e1cies should encourage the search to sol ve other oil-exploration problems through re\u00fcance on this new methodology."}]}}}