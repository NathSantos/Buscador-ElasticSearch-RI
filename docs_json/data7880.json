{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.11371"}, {"@name": "filename", "#text": "16647_001077535.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nINSTITUTO DE INFORM\u00c1TICA\n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM COMPUTA\u00c7\u00c3O\n\nMATHEUS DA SILVA SERPA\n\nSource Code Optimizations to Reduce\nMulti-core and Many-core\nPerformance Bottlenecks\n\nThesis presented in partial fulfillment\nof the requirements for the degree of\nMaster of Computer Science\n\nAdvisor: Prof. Dr. Philippe O. A. Navaux\n\nPorto Alegre\nJuly 2018\n\n\n\nCIP \u2014 CATALOGING-IN-PUBLICATION\n\nSerpa, Matheus da Silva\n\nSource Code Optimizations to Reduce Multi-core and Many-\ncore\nPerformance Bottlenecks / Matheus da Silva Serpa. \u2013 Porto Ale-\ngre: PPGC da UFRGS, 2018.\n\n74 f.: il.\n\nThesis (Master) \u2013 Universidade Federal do Rio Grande do Sul.\nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o, Porto Alegre, BR\u2013\nRS, 2018. Advisor: Philippe O. A. Navaux.\n\n1. Performance evaluation. 2. Source code optimizations.\n3. Many-core. 4. HPC. I. Navaux, Philippe O. A.. II. T\u00edtulo.\n\nUNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nReitor: Prof. Rui Vicente Oppermann\nVice-Reitora: Profa. Jane Fraga Tutikian\nPr\u00f3-Reitor de P\u00f3s-Gradua\u00e7\u00e3o: Prof. Celso Giannetti Loureiro Chaves\nDiretora do Instituto de Inform\u00e1tica: Profa. Carla Maria Dal Sasso Freitas\nCoordenador do PPGC: Prof. Jo\u00e3o Luiz Dihl Comba\nBibliotec\u00e1ria-chefe do Instituto de Inform\u00e1tica: Beatriz Regina Bastos Haro\n\n\n\n\u201cSuccess is the ability to move\n\nfrom one failure to another\n\nwithout loss of enthusiasm.\u201d\n\n\u2014 SIR WINSTON CHURCHILL\n\n\n\n\n\nAGRADECIMENTOS\n\nEm primeiro lugar, gostaria de agradecer ao meu orientador Prof. Philippe Olivier\n\nAlexandre Navaux pelo apoio durante os \u00faltimos dois anos. Sou muito grato por todos\n\nensinamentos e oportunidades que contribuiram para o meu crescimento profissional e\n\npessoal. Obrigado pela confian\u00e7a no Mestrado e agora no Doutorado.\n\nAo Prof. Claudio Schepke, meu orientador no curso de Bacharelado em Ci\u00eancia\n\nda Computa\u00e7\u00e3o. Obrigado por apostar em mim quando ainda estava no terceiro semestre\n\ndo curso. Tamb\u00e9m agrade\u00e7o pelos conselhos pessoais e profissionais, pelas v\u00e1rias viagens\n\njuntos \u00e0 confer\u00eancias e pela amizade mantida at\u00e9 hoje.\n\n\u00c0 Profa. M\u00e1rcia Cristina Cera (in memoriam), que foi muito importante para\n\nminha forma\u00e7\u00e3o. Obrigado por todos ensinamentos. Obrigado por me fazer um profis-\n\nsional melhor. Lembro, como se fosse hoje, a \u00faltima vez que tive o prazer de ouvir seus\n\nconselhos durante o WSCAD 2016 em Aracaju.\n\nAos Professores Marcelo Pasin e Pascal Felber, pela oportunidade e pelas con-\n\ntribui\u00e7\u00f5es durante os tr\u00eas meses em que fui pesquisador visitante na Universit\u00e9 de Neuch\u00e2-\n\ntel. Tamb\u00e9m \u00e0 senhora Pepita e ao senhor Georges que me receberam em sua casa em\n\nNeuch\u00e2tel. E, finalmentente, aos amigos que fiz na Su\u00ed\u00e7a: Rafael, S\u00e9bastien, Dorian,\n\nVeronica e Amanda.\n\nAo corpo docente e quadro de funcion\u00e1rios do Instituto de Inform\u00e1tica da UFRGS,\n\nem especial: Prof. Carissimi, Prof. Lucas, Prof. Antonio Beck, Prof. Luigi, Claudia\n\ne Rafael. Tamb\u00e9m gostaria de agradecer as empresas HPE, Intel, Petrobras e ag\u00eancias\n\nfinanciadoras CAPES e CNPq, pelo apoio financeiro.\n\nAos colegas do Grupo de Processamento Paralelo e Distribu\u00eddo (GPPD). Em espe-\n\ncial: Eduardo Cruz, Emmanuell, Francis, Eduardo Roloff, Matthias, Edson Padoin, Pablo,\n\nJ\u00e9ssica, V\u00edctor, Vinicius, Lucas, Jean e Ot\u00e1vio. Estendo aos amigos do LSE-UFRGS.\n\n\u00c0 minha namorada, Luana, por todo apoio, amor, carinho e dedica\u00e7\u00e3o. Aos ami-\n\ngos da \"Sala 303\", \"EAD Paizinho\", \"Guarita\", \"GAMA\", \"GESEP\", \"Engenharias\" e\n\nERADs. Agrade\u00e7o de forma especial as pessoas mais pr\u00f3ximas a mim em Porto Alegre:\n\nArthur, Gabriel, Tha\u00edse e Uillian. Tamb\u00e9m agrade\u00e7o ao Dalvan e ao Prof. Jo\u00e3o.\n\nPor fim, o mais importante, meu Pai Airton e minha M\u00e3e Maria. Obrigado por\n\ntodo apoio, ensinamentos e confian\u00e7a. Dedico este trabalho a voc\u00eas. Pe\u00e7o desculpas por\n\nn\u00e3o estar presente em v\u00e1rios momentos nos \u00faltimos seis anos.\n\nAos familiares e amigos aqui n\u00e3o mencionados, obrigado por tudo.\n\n\n\n\n\nABSTRACT\n\nNowadays, there are several different architectures available not only for the industry\n\nbut also for final consumers. Traditional multi-core processors, GPUs, accelerators such\n\nas the Xeon Phi, or even energy efficiency-driven processors such as the ARM family,\n\npresent very different architectural characteristics. This wide range of characteristics\n\npresents a challenge for the developers of applications. Developers must deal with differ-\n\nent instruction sets, memory hierarchies, or even different programming paradigms when\n\nprogramming for these architectures. To optimize an application, it is important to have a\n\ndeep understanding of how it behaves on different architectures. Related work proved to\n\nhave a wide variety of solutions. Most of then focused on improving only memory per-\n\nformance. Others focus on load balancing, vectorization, and thread and data mapping,\n\nbut perform them separately, losing optimization opportunities.\n\nIn this master thesis, we propose several optimization techniques to improve the perfor-\n\nmance of a real-world seismic exploration application provided by Petrobras, a multi-\n\nnational corporation in the petroleum industry. In our experiments, we show that loop\n\ninterchange is a useful technique to improve the performance of different cache memory\n\nlevels, improving the performance by up to 5.3\u00d7 and 3.9\u00d7 on the Intel Broadwell and\n\nIntel Knights Landing architectures, respectively. By changing the code to enable vector-\n\nization, performance was increased by up to 1.4\u00d7 and 6.5\u00d7. Load Balancing improved\n\nthe performance by up to 1.1\u00d7 on Knights Landing. Thread and data mapping techniques\n\nwere also evaluated, with a performance improvement of up to 1.6\u00d7 and 4.4\u00d7. We also\n\ncompared the best version of each architecture and showed that we were able to improve\n\nthe performance of Broadwell by 22.7\u00d7 and Knights Landing by 56.7\u00d7 compared to a\n\nnaive version, but, in the end, Broadwell was 1.2\u00d7 faster than Knights Landing.\n\nKeywords: Performance evaluation. source code optimizations. many-core. HPC.\n\n\n\n\n\nOtimiza\u00e7\u00f5es de C\u00f3digo Fonte para Reduzir Gargalos de Desempenho em\n\nMulti-core e Many-core\n\nRESUMO\n\nAtualmente, existe uma variedade de arquiteturas dispon\u00edveis n\u00e3o apenas para a ind\u00fas-\n\ntria, mas tamb\u00e9m para consumidores finais. Processadores multi-core tradicionais, GPUs,\n\naceleradores, como o Xeon Phi, ou at\u00e9 mesmo processadores orientados para efici\u00eancia\n\nenerg\u00e9tica, como a fam\u00edlia ARM, apresentam caracter\u00edsticas arquiteturais muito diferen-\n\ntes. Essa ampla gama de caracter\u00edsticas representa um desafio para os desenvolvedores de\n\naplica\u00e7\u00f5es. Os desenvolvedores devem lidar com diferentes conjuntos de instru\u00e7\u00f5es, hie-\n\nrarquias de mem\u00f3ria, ou at\u00e9 mesmo diferentes paradigmas de programa\u00e7\u00e3o ao programar\n\npara essas arquiteturas. Para otimizar uma aplica\u00e7\u00e3o, \u00e9 importante ter uma compreens\u00e3o\n\nprofunda de como ela se comporta em diferentes arquiteturas. Os trabalhos relacionados\n\nprovaram ter uma ampla variedade de solu\u00e7\u00f5es. A maioria deles se concentrou em melho-\n\nrar apenas o desempenho da mem\u00f3ria. Outros se concentram no balanceamento de carga,\n\nna vetoriza\u00e7\u00e3o e no mapeamento de threads e dados, mas os realizam separadamente,\n\nperdendo oportunidades de otimiza\u00e7\u00e3o.\n\nNesta disserta\u00e7\u00e3o de mestrado, foram propostas v\u00e1rias t\u00e9cnicas de otimiza\u00e7\u00e3o para melho-\n\nrar o desempenho de uma aplica\u00e7\u00e3o de explora\u00e7\u00e3o s\u00edsmica real fornecida pela Petrobras,\n\numa empresa multinacional do setor de petr\u00f3leo. Os experimentos mostram que loop in-\n\nterchange \u00e9 uma t\u00e9cnica \u00fatil para melhorar o desempenho de diferentes n\u00edveis de mem\u00f3ria\n\ncache, melhorando o desempenho em at\u00e9 5,3\u00d7 e 3,9\u00d7 nas arquiteturas Intel Broadwell\n\ne Intel Knights Landing, respectivamente. Ao alterar o c\u00f3digo para ativar a vetoriza\u00e7\u00e3o,\n\no desempenho foi aumentado em at\u00e9 1,4\u00d7 e 6,5\u00d7. O balanceamento de carga melhorou\n\no desempenho em at\u00e9 1,1\u00d7 no Knights Landing. T\u00e9cnicas de mapeamento de threads e\n\ndados tamb\u00e9m foram avaliadas, com uma melhora de desempenho de at\u00e9 1,6\u00d7 e 4,4\u00d7.\n\nO ganho de desempenho do Broadwell foi de 22,7\u00d7 e do Knights Landing de 56,7\u00d7 em\n\ncompara\u00e7\u00e3o com uma vers\u00e3o sem otimiza\u00e7\u00f5es, mas, no final, o Broadwell foi 1,2\u00d7 mais\n\nr\u00e1pido que o Knights Landing.\n\nPalavras-chave: avalia\u00e7\u00e3o de desempenho, otimiza\u00e7\u00f5es de c\u00f3digo fonte, many-core,\n\nHPC.\n\n\n\n\n\nLIST OF ABBREVIATIONS AND ACRONYMS\n\n2D Two-Dimensional\n\n3D Three-Dimensional\n\nALU Arithmetic Logic Unit\n\nAMD Advanced Micro Devices\n\nAPI Application Programming Interface\n\nAVI Audio Video Interleave\n\nAVX Advanced Vector Extensions\n\nB+ B+ tree\n\nBFS Breadth-First Search\n\nBP Back Propagation\n\nCFD CFD Solver\n\nCPU Central Processing Unit\n\nDDR Double Data Rate\n\nGB Gigabyte\n\nGPU Graphics Processing Unit\n\nHPC High-Performance Computing\n\nHS2D HotSpot 2D\n\nHS3D HotSpot 3D\n\nHW Heart Wall Tracking\n\nI/O Input/output\n\nILP Instruction-Level Parallelism\n\nIPC Instructions Per Cycle\n\nKM K-means\n\nKNC Knights Corner\n\n\n\nKNL Knights Landing\n\nKNN k-nearest neighbors\n\nLC Leukocyte Tracking\n\nLLC Last Level Cache\n\nLRU Least Recently Used\n\nLUD LU Decomposition\n\nMCDRAM Multi-Channel DRAM\n\nMD LavaMD\n\nMIC Many Integrated Core\n\nMIMD Multiple Instruction Multiple Data\n\nMPPA Massively Parallel Processor Array\n\nMS Myocyte Simulation\n\nNUMA Non-Uniform Memory Access\n\nNW Needleman-Wunsch\n\nODE Ordinary Differential Equation\n\nOpenMP Open Multi-Processing\n\nPATH Pathfinder\n\nPCM Performance Counter Monitor\n\nPDE Partial Differential Equation\n\nPF Particle Filter\n\nQPI QuickPath Interconnect\n\nRTM Reverse Time Migration\n\nSC Stream Cluster\n\nSIMD Single Instruction Multiple Data\n\nSMT Simultaneous Multithreading\n\nSRAD Speckle Reducing Anisotropic Diffusion\n\n\n\nSSE Streaming SIMD Extensions\n\nTLB Translation Lookaside Buffer)\n\nTLP Thread-Level Parallelism\n\n\n\n\n\nLIST OF SYMBOLS\n\np(x,y,z,t) Acoustic Pressure\n\n?2 Laplace Operator\n\n?(x,y,z) Media Density\n\n? Partial Derivative\n\nV (x,y,z) Propagation Speed\n\n\n\n\n\nLIST OF FIGURES\n\nFigure 2.1 Example of the memory subsystem in the Broadwell architecture. In\nthis figure there are 2 processors, each constituting a NUMA node. Each pro-\ncessor is composed of 22 physical processing cores, each executing 2 logical\nthreads. There are 3 levels of cache memory. ..........................................................26\n\nFigure 2.2 Example of the memory subsystem in the Knights Landing architecture.\nIn this figure there are 4 tiles, each with 2 cores. The processor is composed\nof 57 physical processing cores, each executing 4 logical threads. There are 2\nlevels of cache memory. ...........................................................................................27\n\nFigure 3.1 Performance of different architectures (higher IPC is better). ......................35\nFigure 3.2 Cache hit ratio in Broadwell architecture. .....................................................41\nFigure 3.3 Memory operations in the Broadwell architecture. The y-axis is in log-\n\narithmic scale in both figures. ..................................................................................42\nFigure 3.4 Cache hit ratio in the Knights Landing architecture. .....................................43\nFigure 3.5 Memory operations the Knights Landing architecture. The y-axis is in\n\nlogarithmic scale in both figures. .............................................................................44\n\nFigure 4.1 Speedup over the xyz sequence. ....................................................................48\nFigure 4.2 Cache hit ratio on different architectures.......................................................48\nFigure 4.3 Performance gain using vectorization. ..........................................................49\nFigure 4.4 Loop collapse and scheduling performance gain...........................................51\nFigure 4.5 Mapping results on Broadwell.......................................................................53\nFigure 4.6 Mapping results on Knights Landing. ...........................................................53\n\nFigure A.1 Desempenho de diferentes arquiteturas (maior IPC \u00e9 melhor).....................74\n\n\n\n\n\nLIST OF TABLES\n\nTable 2.1 Summary of related work. Each line represents a related work. Each\ncolumn represents a desired property........................................................................33\n\nTable 3.1 Execution Environment. ..................................................................................36\n\n\n\n\n\nCONTENTS\n\n1 INTRODUCTION.......................................................................................................23\n1.1 Contributions of this research................................................................................24\n1.2 Document Organization .........................................................................................24\n2 MULTI-CORE AND MANY-CORE: OVERVIEW AND RELATED WORK.....25\n2.1 Multi-core and Many-core Architectures .............................................................25\n2.1.1 The Broadwell Architecture...................................................................................25\n2.1.2 The Knights Landing Architecture ........................................................................26\n2.2 Related Work...........................................................................................................28\n2.2.1 Memory Optimization............................................................................................28\n2.2.2 Vectorization ..........................................................................................................29\n2.2.3 Load Balancing ......................................................................................................30\n2.2.4 Thread and Data Mapping .....................................................................................30\n2.2.5 Combined Different Properties ..............................................................................31\n2.3 Summary..................................................................................................................32\n3 ANALYSIS OF PERFORMANCE BOTTLENECKS.............................................35\n3.1 Methodology ............................................................................................................36\n3.1.1 Workloads ..............................................................................................................37\n3.2 Results on Broadwell ..............................................................................................40\n3.3 Results on Knights Landing ...................................................................................41\n3.4 Conclusions ..............................................................................................................43\n4 OPTIMIZATION STRATEGIES FOR MULTI-CORE AND MANY-CORE......45\n4.1 Seismic Exploration Application ...........................................................................45\n4.2 Results ......................................................................................................................46\n4.2.1 Memory Access Pattern to Improve Data Locality................................................46\n4.2.2 Exploiting SIMD for Floating-point Computations...............................................48\n4.2.3 Improving Load Balancing ....................................................................................49\n4.2.4 Optimizing Memory Affinity .................................................................................50\n4.3 Conclusions ..............................................................................................................53\n5 CONCLUSION AND FUTURE WORK ..................................................................55\n5.1 Future Work ............................................................................................................55\n5.2 Publications .............................................................................................................56\nREFERENCES...............................................................................................................61\nAPPENDIX A \u2014 RESUMO EM PORTUGU\u00caS ........................................................65\nA.1 Introdu\u00e7\u00e3o...............................................................................................................65\nA.1.1 Contribui\u00e7\u00f5es ........................................................................................................66\nA.2 Arquiteturas Multi-core e Many-core ...................................................................66\nA.2.1 Trabalhos Relacionados ........................................................................................67\nA.3 An\u00e1lise dos Gargalos de Desempenho ..................................................................68\nA.4 Estrat\u00e9gias de Otimiza\u00e7\u00e3o para Multi-core e Many-core....................................68\nA.4.1 Padr\u00e3o de Acesso \u00e0 Mem\u00f3ria para Melhorar a Localidade dos Dados.................69\nA.4.2 Explorando SIMD para Computa\u00e7\u00e3o de Ponto Flutuante.....................................70\nA.4.3 Melhorando o Balanceamento de Carga ...............................................................71\nA.4.4 Otimizando a Afinidade de Mem\u00f3ria....................................................................72\nA.5 Conclus\u00e3o e Trabalhos Futuros ............................................................................72\nA.5.1 Trabalhos Futuros..................................................................................................73\n\n\n\n\n\n23\n\n1 INTRODUCTION\n\nHigh-performance computing (HPC) has been responsible for a scientific revo-\n\nlution. The evolution of computer architectures improved the computational power, in-\n\ncreasing the range of problems and quality of solutions that could be solved in the required\n\ntime, e.g., weather forecast. The introduction of integrated circuits, pipelines, increased\n\nfrequency of operation, out-of-order execution and branch prediction are an important part\n\nof the technologies introduced up to the end of the 20th century. Recently, the concern\n\nabout energy consumption has grown, with the goal of achieving computation at the Ex-\n\nascale level in a sustainable way (HSU, 2016). The technologies so far developed do not\n\nenable Exascale computing, due to the high energy cost of increasing the frequency and\n\npipeline stages, as well as the fact that we are at the limits of exploration the instruction-\n\nlevel parallelism (ILP) (BORKAR; CHIEN, 2011; COTEUS et al., 2011).\n\nIn order to increase computational power, the industry has shifted its focus to\n\nparallel and heterogeneous architectures in recent years. The main feature of parallel ar-\n\nchitectures is the presence of several processing cores operating concurrently. To use such\n\nan architecture, an application must be programmed by separating it into several tasks that\n\ncommunicate with each other. Heterogeneous architectures, on the other hand, have dif-\n\nferent environments in the same system, each one with its specialized architecture for task\n\ntype. The usage of accelerators is one of the main forms of heterogeneous architectures,\n\nin which a generic processor is mostly responsible for managing the system, and several\n\naccelerators present in the system perform the computation of specific tasks to which they\n\nare tuned and expected to perform well.\n\nSeveral challenges must be addressed to support these architectures better and\n\nthereby achieve high performance (MITTAL; VETTER, 2015). Applications need to be\n\ncoded considering the particularities of each environment, as well as considering their\n\ndistinct architectural characteristics (GROPP; SNIR, 2013). For example, in the memory\n\nhierarchy, the presence of several cache memory levels, some shared and others private\n\nintroduces non-uniform access times, which impact applications\u2019 performance (CRUZ et\n\nal., 2016a). It is even more critical in heterogeneous architectures since each accelerator\n\ncan have its own, distinct, memory hierarchy. Also, in heterogeneous architectures, the\n\nnumber of functional units may vary between different accelerators, and the instruction\n\nset itself may not be the same. In this context, it is essential to analyze the behavior of\n\narchitectures, in order to provide better support for optimizing applications performance.\n\n\n\n24\n\n1.1 Contributions of this research\n\nThe main objective of our research is to evaluate multi-core and many-core ar-\n\nchitectures and reduce the performance bottlenecks using source code optimization tech-\n\nniques. Considering these goals, our main contributions are the following:\n\n\u2022 We analyze a set of performance metrics on several applications with distinct par-\n\nallel execution characteristics aiming to find a correlation between the metric and\n\nthe application performance (IPC).\n\n\u2022 We addressed a set of performance optimization strategies, aiming to increase the\n\nperformance of a real-world seismic exploration application. The techniques em-\n\nployed were: loop interchange to improve cache memory usage; vectorization to\n\nincrease the performance of floating point computations; loop scheduling and col-\n\nlapse to improve load balancing; and thread and data mapping to better use the\n\nmemory hierarchy.\n\n1.2 Document Organization\n\nThe document is organized as follows. Chapter 2 presents a background on the\n\ntopics of this dissertation and discusses related work in performance optimization. Chap-\n\nter 3 presents the results of our evaluation of the performance bottlenecks. In chapter 4,\n\nwe addressed some performance optimization strategies and optimized a real-world ap-\n\nplication. Finally, chapter 5 draws conclusions based on our findings and presents some\n\nfuture work insights.\n\n\n\n25\n\n2 MULTI-CORE AND MANY-CORE: OVERVIEW AND RELATED WORK\n\nThe following sections explain some concepts that serve as a base for this dis-\n\nsertation. Two state-of-art architectures for High-Performance Computing is presented.\n\nFurthermore, this chapter also details related work on performance optimization for these\n\narchitectures.\n\n2.1 Multi-core and Many-core Architectures\n\nTechnological innovations brought up powerful single-core processors with high\n\nclock frequencies. However, because of technological and power limitations, multicore\n\nand many-core processors emerged as new computer architectures (KIRK; WEN-MEI,\n\n2016). These architectures rely on both instruction-level parallelism (ILP) and thread-\n\nlevel parallelism (TLP) to achieve high performance. Today\u2019s multicore and many-core\n\nprocessors differ in the number of cores, the memory hierarchy, and their interconnection.\n\nThe design of multicore and many-core architectures is different to the point that\n\ndepending on the application, the performance can be high in one architecture and low in\n\nthe other (COOK, 2012). The multicore architecture uses sophisticated control logic to\n\nallow single-threaded statements to run in parallel. Large cache memories are provided\n\nto reduce access latencies to instructions and application data. Finally, the operations of\n\nthe arithmetic logic units (ALUs) are also designed to optimize latency.\n\nMany-core architecture takes advantage of a large number of execution threads.\n\nSmall cache memories are provided to prevent multiple threads accessing the same data\n\nfrom having to go to main memory. Besides, most of the chip is dedicated to floating-\n\npoint units. Such architectures are designed as floating-point computing mechanisms and\n\nnot for conventional operations, which are better executed by multicore architectures.\n\n2.1.1 The Broadwell Architecture\n\nThe Broadwell architecture (NALAMALPU et al., 2015) is a non-uniform mem-\n\nory access architecture. In such architectures, the latency in the access to the main mem-\n\nory will change, as it depends on which memory bank is being accessed (CRUZ et al.,\n\n2016a). Each processor in the system contains a memory controller, composing one\n\n\n\n26\n\nFigure 2.1: Example of the memory subsystem in the Broadwell architecture. In this fig-\nure there are 2 processors, each constituting a NUMA node. Each processor is composed\nof 22 physical processing cores, each executing 2 logical threads. There are 3 levels of\ncache memory.\n\nProcessor\n\nCore\n2-SMT\n\nL1/L2\n\nCore\n2-SMT\n\nL1/L2\n\n...\n\nL3\n\nDRAM\n\nInterconnection\n\nProcessor\n\nCore\n2-SMT\n\nL1/L2\n\nCore\n2-SMT\n\n...\n\nL1/L2\nL3\n\nDRAM\n\nSource: The Author.\n\nNUMA node. Each physical core executes 2 logical threads by employing simultaneous\n\nmulti-threading (SMT). The connection between different processors is made through In-\n\ntel\u2019s QuickPath Interconnect (QPI) (ZIAKAS et al., 2010). The memory hierarchy of this\n\nsystem is illustrated in Figure 2.1.\n\nTo exemplify how the memory hierarchy can impact a memory access latency,\n\nFigure 2.1 shows an example of a system where there are different possibilities for ac-\n\ncesses to memory. Threads might access memory by obtaining their data from the private\n\nL1 or L2 caches in each core, obtaining high-speed access. A significant change in the\n\nBroadwell architecture, when compared to the previous Intel processors, is the duplica-\n\ntion of available bandwidth in the private cache memories. The L1 cache memory has\n\n2 load ports and 1 store port, thus it supports 2 concurrent load requests given no bank\n\nconflict between the accesses. Threads might also access the L3 cache memory, which is\n\nshared between all cores, taking 3 to 4 times longer than accessing the L2 cache, and up\n\nto 10 times longer than accessing the L1 cache. If the requested data is not found in the\n\nlocal caches, the request is snooped by remote caches in the other NUMA nodes, which\n\nmight have the data. If the data is still not found, then the main memory of the NUMA\n\nnode responsible for the address is accessed.\n\n2.1.2 The Knights Landing Architecture\n\nThe Knights Landing (KNL) architecture (SODANI et al., 2016) is shown in Fig-\n\nure 2.2. The architecture is organized in tiles and has a distributed tag directory, and has\n\na mesh interconnection. Each tile contains two cores, with private L1 caches, a shared\n\n\n\n27\n\nFigure 2.2: Example of the memory subsystem in the Knights Landing architecture. In\nthis figure there are 4 tiles, each with 2 cores. The processor is composed of 57 physical\nprocessing cores, each executing 4 logical threads. There are 2 levels of cache memory.\n\nTile\n\nCore\n4-SMT\n\nL1\n\nCore\n4-SMT\n\nL1\nL2\n\nDRAMMCDRAM\n\nTile\n\nCore\n4-SMT\n\nL1\n\nCore\n4-SMT\n\nL1\nL2\n\nDRAMMCDRAM\n\nTile\n\nCore\n4-SMT\n\nL1\n\nCore\n4-SMT\n\nL1\nL2\n\nDRAMMCDRAM\n\nTile\n\nCore\n4-SMT\n\nL1\n\nCore\n4-SMT\n\nL1\nL2\n\nDRAMMCDRAM\n\nSource: The Author.\n\nL2 cache and a tag directory (omitted in the figure). The architecture, besides memory\n\ncontrollers to access external DDR4 memory, includes an MCDRAM memory, which\n\ncan work as a cache to the DDR4 memory (cache mode). We used this mode in the ex-\n\nperiments, or as a separate memory in the same address space (flat mode). The cores\n\nin KNL implement an out-of-order pipeline and can execute 4 threads in parallel using\n\nsimultaneous multithreading (SMT).\n\nIn KNL, in case of a cache miss, the corresponding tag directory is checked, and\n\nthe data is forwarded from another cache if present there. If no cache of any tile has\n\nthe data, two behaviors are possible. First, in cache mode, the MCDRAM works as an L3\n\ncache, such that the processor first checks if the data is cached in it, and if not, the memory\n\ncontroller is accessed to fetch the data from the DDR4 memory. Second, in flat mode, the\n\nmemory address determines if the data is stored in the MCDRAM or the DDR4 memory.\n\nThe distribution of memory addresses between the tag directories and memory controllers\n\ncan also be configured, which is called clustering mode. The two main configurations for\n\nclustering modes are: quadrant, where the tiles are split to 4 quadrants, and the addresses\n\nare divided between the quadrants by the hardware; and sub-NUMA clustering, where\n\neach memory controller and MCDRAM form a NUMA node, and the operating system is\n\nresponsible for selecting the node that stores each page.\n\n\n\n28\n\n2.2 Related Work\n\nIn this section, the most representative works that evaluate and optimize applica-\n\ntions performance are discussed. They are listed by property in chronological order.\n\n2.2.1 Memory Optimization\n\nFalch and Elster (FALCH; ELSTER, 2014) proposed the usage of a manually\n\nmanaged cache to combine the memory from multiple threads. Using their technique,\n\nthey achieved a speedup of up to 2.04 in a synthetic stencil. They concluded that manual\n\ncaching is an effective approach to improve memory access and that applications with\n\nregular access patterns are suitable to implement their technique.\n\nA mechanism is proposed by Jia, Shaw and Martonosi (JIA; SHAW; MARTONOSI,\n\n2014) to balance memory accesses. The authors\u2019 motivation is that the design of cache\n\nmemories used in GPU architectures are the same as those designed for CPU archi-\n\ntectures, which is unfit for their operation. The massive use of threads through block-\n\nthreading in warps means that normal caches provide few bytes per thread, and when all\n\nthreads need the cache, the data is thrown away without being reused (thrashing). When\n\nthreads from multiple warps share the cache, there is contention for the request queue it-\n\nself, which cannot timely serve requests from such a large number of threads. The authors\n\npropose two solutions: queue request reordering and cache bypassing. Through queues\n\nthat use block identifiers, it is possible to separate the requests and prioritize all requests\n\nof 1 block, to make use of the spatial and temporal localities of this block. To avoid\n\nstarving and contention, additional policies were developed to balance use cases, such as\n\nprioritizing a full queue that receives a new request. Because caches are also not designed\n\nfor so many threads, some warps have all their accesses redirected directly to main mem-\n\nory, effectively avoiding cache accesses. It improves the access of all the requests to the\n\ncache memory, since the waiting delays in the queue of the accesses that go to the cache\n\nare reduced, and the accesses that do not have access to the cache because they do not\n\nhave priority would probably be missed in the cache. By avoiding waiting and useless\n\naccesses, requests are serviced faster by being forwarded directly to main memory.\n\nMaruyama and Aoky (MARUYAMA; AOKI, 2014) present a method for stencil\n\ncomputations on the NVIDIA Kepler architecture that uses shared memory for better\n\ndata locality combined with warp specialization for higher instruction throughput. Their\n\n\n\n29\n\nmethod achieves approximately 80% of the value from roofline model estimation.\n\nIn Ausavarungnirun et al. (AUSAVARUNGNIRUN et al., 2015), the authors pro-\n\nposed a mechanism to balance the memory accesses. The main observation of the article\n\nis that a warp with several hits in the L2 cache, which also generates misses in the cache\n\nL2, has as bottleneck these same misses, despite all the hits, which become unutilized.\n\nHits on warps with lots of misses are also useless since the worst access time always\n\ndefines the execution of warp. Through changes in the memory subsystem, prioritizing\n\nwarps accesses with most hits and redirecting warps accesses with most misses directly\n\nto the main memory, the technique can improve application performance, on average, by\n\n21%. The work exploits an inherent architectural feature of the stream processor model,\n\nmitigating the problem of divergence of memory accesses in individual warps. The re-\n\nsearch considered in this article deals with issues related to the pressure of several threads\n\nof scalable benchmarks at a system level, although they are related problems when con-\n\nsidering the level of the stream processor.\n\nSao et al. (SAO et al., 2015) presented a sparse direct solver for distributed mem-\n\nory subsystems comprising hybrid multi-core CPU and Intel Xeon Phi coprocessors,\n\nwhich combines the use of asynchrony with the accelerated offload, lazy updates, and\n\ndata shadowing to hide and reduce communication costs.\n\nHeinecke et al. (HEINECKE et al., 2016) optimized seismic simulations in the\n\nKnights Landing architecture, exploiting its two-level memory subsystem and 2D mesh\n\ninterconnect.\n\nNasciutti and Panetta (NASCIUTTI; PANETTA, 2016) did a performance analy-\n\nsis of 3D stencils on GPUs focusing on the proper use of the memory hierarchy. They\n\nconclude that the preferred code is the combination of read-only cache reuse, inserting\n\nthe Z loop into the kernel and register reuse.\n\n2.2.2 Vectorization\n\nWang et al. (WANG et al., 2016) introduced a fast tridiagonal algorithm for the\n\nIntel MIC architecture, achieving the best utilization of vectorization and registers.\n\nHasib et al. (HASIB et al., 2017) investigate the effects on performance and energy\n\nfrom a data reuse methodology combined with parallelization and vectorization in multi-\n\ncore and many-core processors. They achieve a speedup of 17\u00d7 using AVX2 and 35\u00d7\n\nusing AVX512.\n\n\n\n30\n\n2.2.3 Load Balancing\n\nThe work of Lastovetsky, Szustak and Wyrzykowski (LASTOVETSKY; SZUS-\n\nTAK; WYRZYKOWSKI, 2017) have used load imbalance (different work partition sizes)\n\nas a way to improve performance in a KNC platform. As their application of inter-\n\nest, MPDATA, sometimes shows a decrease in execution time for larger domain (data)\n\npartitions, they use a self-adaptable implementation to benchmark some balanced (same\n\nsize of subdomain among groups of threads) and imbalanced (half of the threads have a\n\nslightly larger subdomain) partitions, and then choose the best partition among the mea-\n\nsurements, achieving performance improvements of 15% over the balanced distribution.\n\nNevertheless, this work distribution algorithm is limited to well-behaved, iterative, static\n\napplications.\n\n2.2.4 Thread and Data Mapping\n\nTousimojarad and Vanderbauwhede (TOUSIMOJARAD; VANDERBAUWHEDE,\n\n2014) show that the default thread mapping of Linux is inefficient when the number of\n\nthreads is as large as on a many-core processor and presents a new thread mapping policy\n\nthat uses the amount of time that each core does useful work to find the best target core\n\nfor each thread.\n\nLiu et al. (LIU et al., 2015) propose an approach based on profiling to determine\n\nthread-to-core mapping on the Knights Corner architecture that depends on the location of\n\nthe distributed tag directory, achieving significant reductions on communication latency.\n\nOn Cruz et al. (CRUZ et al., 2016b), a method that uses the time that an entry\n\nstays in the TLB and the threads that access that page as a metric to perform thread and\n\ndata mappings better is presented, achieving great performance improvements with low\n\noverhead.\n\nDiener et al. (DIENER et al., 2016b) proposes kmaf, a kernel-based framework\n\nthat uses page faults of parallel applications to profile their memory access pattern and\n\nimprove the thread and data mapping online (DIENER et al., 2016b).\n\nHe, Chen and Tang (HE; CHEN; TANG, 2016) introduces NestedMP, an extension\n\nto OpenMP that allows the programmer to give information about the structure of the tasks\n\ntree to the runtime, which then performs a locality-aware thread mapping.\n\nCruz et al. (CRUZ et al., 2018) improve state of the art by performing a very\n\n\n\n31\n\ndetailed analysis of the impact of thread mapping on communication and load balanc-\n\ning in two many-core systems from Intel, namely Knights Corner and Knights Landing.\n\nThey observed that the widely used metric of CPU time provides very inaccurate infor-\n\nmation for load balancing. They also evaluated the usage of thread mapping based on the\n\ncommunication and load information of the applications to improve the performance of\n\nmany-core systems.\n\nSerpa et al. (SERPA et al., 2018) focus on Intel\u2019s multi-core Xeon and many-core\n\naccelerator Xeon Phi Knights Landing, which can host several hundreds of threads on\n\nthe same CPU. They study the impact of mapping strategies, revealing that, with smart\n\nmapping policies, one can indeed significantly speed up machine learning applications on\n\nmany-core architectures. Execution time was reduced by up to 25.2% and 18.5% on Intel\n\nXeon and Xeon Phi Knights Landing, respectively.\n\n2.2.5 Combined Different Properties\n\nA memory model to analyze algorithms for many-core systems is presented in Ma,\n\nAgrawal and Chamberlain (MA; AGRAWAL; CHAMBERLAIN, 2014). The model con-\n\nsiders several architectural parameters, such as the latency for accessing the memory, the\n\nnumber of cores, the number of words that can be read from memory, the size of cache\n\nmemory and the number of threads per core. It also considers the total number of oper-\n\nations that the running application must perform, as well as the total number of memory\n\noperations, cache memory usage and the number of threads. The authors conclude that\n\napplications with similar characteristics can have different performance using different\n\narchitectural parameters.\n\nIn Andreolli et al. (ANDREOLLI et al., 2015), the authors focused on acoustic\n\nwave propagation equations, choosing the optimization techniques from systematically\n\ntuning the algorithm. The usage of collaborative thread blocking, cache blocking, register\n\nreuse, vectorization and loop redistribution resulted in significant performance improve-\n\nments.\n\nMei and Chu (MEI; CHU, 2015) analyzed the characteristics of the memory sub-\n\nsystem in 3 different GPU architectures: Fermi, Kepler, and Maxwell. They used a\n\npointer-chasing benchmark and observed the memory access latencies to define the char-\n\nacteristics of all memories within each GPU. In doing so, they were able to identify in-\n\nteresting features, such as a line replacement policy different from the expected Least\n\n\n\n32\n\nRecently Used (LRU) in the L2 cache memory. The authors conclude that the Kepler\n\narchitecture planning was aggressive in its memory bandwidth, which has often been un-\n\nderused, and that, in the Maxwell architecture, more resources were invested in shared\n\nmemory, generating a more efficient and balanced system.\n\nSlota, Rajamanickam and Madduri (SLOTA; RAJAMANICKAM; MADDURI,\n\n2015) presented a methodology for graph algorithm design on many-core architectures,\n\nsuch as NVIDIA and AMD GPUs and the Intel Xeon Phi MIC coprocessor, considering\n\nthread synchronization and access to global and shared memory, as well as load balancing.\n\nResearch efforts such as the presented in Castro et al. (CASTRO et al., 2016)\n\nimproved and evaluated the performance of the acoustic wave propagation equation on\n\nIntel Xeon Phi and compared it with MPPA-256, general-purpose processors and a GPU.\n\nThe optimizations include cache blocking, memory alignment with pointer shifting and\n\nthread affinity. They show that the best results are obtained from a combination of the\n\nfirst two and also that the performance with the Xeon Phi is close to the GPU.\n\nSerpa et al. (SERPA et al., 2017) propose several optimization strategies for a wave\n\npropagation model for six architectures: Intel Broadwell, Intel Haswell, Intel Knights\n\nLanding, Intel Knights Corner, NVIDIA Pascal and NVIDIA Kepler. The results show\n\nthat current GPU NVIDIA Pascal improves over Intel Broadwell, Intel Haswell, Intel\n\nKnights Landing, Intel Knights Corner, and NVIDIA Kepler performance by up to 8.5\u00d7.\n\nDeng et al. (DENG et al., 2018) analysis the performance difference between\n\nSandy Bridge, MIC, and Kepler. They also proposed some memory optimizations that\n\nimprove the performance of an ADI solver by up to 5.5 on a Kepler GPU in contrast to\n\ntwo Sandy Bridge CPUs.\n\n2.3 Summary\n\nIn this chapter, we analyzed the related work on memory optimization, vectoriza-\n\ntion, load balancing and mapping. The related work proved to have a wide variety of\n\nsolutions, with very different characteristics, in which we summarize in Table 2.1.\n\nWith the analysis of the related work, we can conclude that deep knowledge of\n\nthe application behavior at the architectural level allows developing techniques to gain\n\nperformance. Our work goes beyond analysis and looks for a greater understanding of\n\nthe performance of different applications on different multi-core and many-core systems.\n\nWe decide for using multi-core and many-core architectures because their programming\n\n\n\n33\n\nTable 2.1: Summary of related work. Each line represents a related work. Each column\nrepresents a desired property.\n\nM\nul\n\nti\n-c\n\nor\ne\n\nM\nan\n\ny-\nco\n\nre\n\nM\nem\n\nor\ny\n\nV\nec\n\nto\nri\n\nza\nti\n\non\n\nL\noa\n\nd\nB\n\nal\nan\n\nci\nng\n\nM\nap\n\npi\nng\n\nFalch e Elster (2014)\nJia, Shaw e Martonosi (2014)\nMa, Agrawal e Chamberlain (2014)\nMaruyama e Aoki (2014)\nTousimojarad e Vanderbauwhede (2014)\nAndreolli et al. (2015)\nAusavarungnirun et al. (2015)\nLiu et al. (2015)\nMei e Chu (2015)\nSao et al. (2015)\nSlota, Rajamanickam e Madduri (2015)\nCastro et al. (2016)\nCruz et al. (2016b)\nDiener et al. (2016b)\nHe, Chen e Tang (2016)\nHeinecke et al. (2016)\nNasciutti e Panetta (2016)\nWang et al. (2016)\nHasib et al. (2017)\nLastovetsky, Szustak e Wyrzykowski (2017)\nSerpa et al. (2017)\nCruz et al. (2018)\nDeng et al. (2018)\nSerpa et al. (2018)\nThis Thesis\n\nSource: The Author.\n\nstyle is similar, different from a GPU architecture that has a very different and unusual\n\nprogramming style.\n\nThe related work also shows that most of the work is focused on memory opti-\n\nmizations, but several works aimed at vectorization, load balancing and mapping. In this\n\nway, we decide to perform the four optimizations together on a real-world application.\n\nThe next chapters describe our proposals in detail.\n\n\n\n34\n\n\n\n35\n\n3 ANALYSIS OF PERFORMANCE BOTTLENECKS\n\nNowadays, there are several different architectures available not only for the in-\n\ndustry but also for final consumers. Traditional multi-core processors and many-core\n\naccelerators such as the Xeon Phi present very different architectural characteristics. This\n\nwide range of characteristics present a challenge for the developers of applications be-\n\ncause the same application can perform well when executing on one architecture, but\n\npoorly on another architecture.\n\nTo better explain our motivation, we show how parallel applications perform on\n\nthese architectures. Figure 3.1 shows the IPC (instructions per cycle) metric, which in-\n\ndicates the average number of instructions executed per cycle, for 18 benchmarks from\n\nthe Rodinia suite (CHE et al., 2010). As expected, the performance of each application\n\ndepends on the architecture. There are three groups of applications: better on Broadwell;\n\nbetter on Knights Landing; and almost the same performance in both architectures.\n\nIt motivates the study of applications and architectural characteristics, aimed to un-\n\nderstand what limit applications performance and how to improve that. We used hardware\n\nperformance counters to gather accurate measurements of the actual impact of different\n\nfactors that influence the performance. By doing so, we arrive the conclusion that some\n\nmetrics help in understand applications performance, but there are cases where a metric\n\nalone is not representative. Such study served as a basis for the next chapter where we\n\noptimize a real-world seismic exploration application.\n\nFigure 3.1: Performance of different architectures (higher IPC is better).\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\n\nC\nF\n\nD\n\nK\nN\n\nN\n\nPA\nT\n\nHLC\n\nH\nS\n\n3DH\nW\n\nB\nF\n\nS\n\nB\n+\n\nP\nF\n\n0\n\n0.5\n\n1\n\n1.5\n\nBenchmarks\n\nIn\nst\n\nru\nct\n\nio\nns\n\npe\nr\n\ncy\ncl\n\ne\n(I\n\nP\nC\n\n)\n\nBroadwell Knights Landing\n\nSource: The Author.\n\n\n\n36\n\nTable 3.1: Execution Environment.\nSystem Parameter Value\n\nBroadwell Processor 2 \u00d7 Intel Xeon E5-2699 v4, 2 x 22 cores, 2-SMT cores\nmulti-core Threads 88\n\nMicroarchitecture Broadwell-EP\nCaches/processor 22 \u00d7 32 KByte L1, 22 \u00d7 256 KByte L2, 55 MByte L3\nMemory 256 GByte DDR4-2400\nEnvironment Linux 4.4, Intel Compiler 18.0.1\n\nKnights Landing Processor Intel Xeon Phi 7250, 68 cores, 4-SMT cores\nmany-core Threads 272\n\nMicroarchitecture Knights Landing\nCaches 68 \u00d7 32 KByte L1, 68 \u00d7 512 KByte L2\nMemory 96 GByte DDR4, 16 GByte MCDRAM\nEnvironment Linux 4.4, Intel Compiler 17.0.4\n\nSource: The Author.\n\nThe remainder of this Chapter is organized as follows. First, we discuss the\n\nmethodology, introducing the architectures used to perform the experiments of this disser-\n\ntation and the benchmark suite applications. Then, the results on Broadwell and Knights\n\nLanding are presented. Finally, we discuss and summarize the conclusions of this study.\n\n3.1 Methodology\n\nThe experiments were performed in the Broadwell and Knights Landing system\n\nenvironments. The Broadwell system is composed of two Intel Xeon E5-2699 v4 pro-\n\ncessors, where each processor consists of 22 physical cores, allowing execution of 44\n\nthreads with 2-SMT Hyper-Threading. The Knights Landing system is an Intel Xeon Phi\n\n7250 processor with 68 physical cores and 272 threads with 4-SMT Hyper-Threading. It\n\nalso has a 16 GB MCDRAM memory which is almost four times faster than the DRAM.\n\nTable 3.1 exhibits the details of each system.\n\nThe experiments shown in the next sections present the average of 30 random ex-\n\necutions. The standard deviation presented is given by the t-Student distribution with a\n\n95% confidence interval. Moreover, we also investigate other metrics such as bandwidth\n\nand cache hit ratio in the memory subsystem. The Intel PCM (INTEL, 2012) and In-\n\ntel VTune (INTEL, 2016) tools were used to obtain data for the Broadwell and Knights\n\nLanding.\n\n\n\n37\n\n3.1.1 Workloads\n\nAs workloads, we used the OpenMP implementation of the Rodinia Benchmark,\n\nv3.1 (CHE et al., 2010). The benchmark suite Rodinia was chosen since it implements\n\na set of applications with distinct parallel execution characteristics. We configured the\n\nbenchmarks to run with the number of virtual cores of the architecture. The applications\n\nused were the following:\n\nBack Propagation (BP): BP is an iterative algorithm used to train the weights of\n\nthe connections between neurons in a multi-layer neural network. The training is per-\n\nformed in two phases: the Forward Phase, in which the activations are propagated from\n\nthe input to the output layer, and the Backward Phase, in which the error between the\n\nobserved and requested values in the output layer is propagated backward to adjust the\n\nweights and bias values. In each layer, the processing of all the nodes can be done in\n\nparallel.\n\nBreadth-First Search (BFS): BFS implements breadth-first search traversal of an\n\narbitrary graph. The application read a graph specification from a file. The result of the\n\ntraversal is an array of integers where each element is the distance from the source node\n\nto the node with the element\u2019s index. The traversal starting node is picked randomly by\n\nthe application. The host performs traversal in a loop until every node is visited.\n\nB+ Tree (B+): B+tree implements queries on large n-ary trees. The benchmark\n\nconstructs the tree as a dynamic data structure of heap allocated nodes. As the size of the\n\ntree grows, the cost of conversion increases substantially. If the tree is modified, the entire\n\ntree must be converted again. The application reads the query commands from a file, one\n\ncommand at a time, and processes them immediately.\n\nCFD Solver (CFD): CFD solves the 3D Euler equations for a zero density, com-\n\npressible fluid. The algorithm is implemented as an unstructured grid finite volume solver,\n\nwhere each thread operates on a block of the 3D space.\n\nHeart Wall Tracking (HW): HW implements the tracking stage of the Heart Wall\n\napplication. The application comes with a single input AVI file containing the sequence\n\nof ultrasound images. The kernel is invoked once for each frame in the video sequence.\n\nThus the application is a wrapper for the kernel implementing the tracking stage of the\n\nHeart Wall application.\n\nHotSpot 2D (HS2D): HS2D estimates the temperature of a logic circuit given the\n\ninitial temperature and the power dissipation of each logic cell. The temperature value\n\n\n\n38\n\nof all cells is calculated in parallel and stops after a predetermined number of timesteps.\n\nThis benchmark can be classified as Structured Grid.\n\nHotSpot 3D (HS3D): HS3D performs the same calculations as HS2D but for a\n\ntridimensional integrated circuit.\n\nK-Means (KM): A set of k initial centers is generated in the same multidimen-\n\nsional space as the data set one wishes to cluster. The algorithm assigns each point to the\n\ncenter that is closer to it. After that, the center of these clusters is calculated, and the pro-\n\ncess is repeated with the new centers for a predetermined number of iterations. The main\n\ncomputation performed is calculating the Euclidean distance between the points, and that\n\nis done in parallel. The problem is classified as Dense Linear Algebra.\n\nLavaMD (MD): MD calculates the movement of particles due to the forces from\n\nother particles in a large tridimensional space. The 3D space is first divided into boxes.\n\nThen, for every particle in a box, the loop processes the interactions with other particles\n\nin this box and then with particles in the neighbor boxes. The processing of each particle\n\nconsists of a single stage of calculation that is enclosed in the innermost loop. The nested\n\nloops in the application were parallelized in such a way that at any point of time wavefront\n\naccesses adjacent memory locations.\n\nLeukocyte Tracking (LC): Detect and track rolling leukocytes in video microscopy\n\nof blood vessels, useful for medical imaging. The cells are detected on the first frame by\n\ncomputing the Gradient Inverse Coefficient of Variation score for every pixel across a\n\nrange of possible ellipses that could be the contour of the cell. The application then com-\n\nputes the Motion Gradient Vector Flow matrix in the area around each cell to track the\n\nflow of the blood. The dwarf classification is Structured Grid.\n\nLU Decomposition (LUD): LUD performs the LU Decomposition of a matrix,\n\nthat means decomposing an input matrix into two triangular matrices and a diagonal ma-\n\ntrix. The factorization is a sum of products and is made in parallel. It is a Dense Linear\n\nAlgebra application.\n\nMyocyte Simulation (MS): MS is a simplified implementation of the simulation\n\nthat models the heart muscle cell. The simulation is based on ordinary differential equa-\n\ntions (ODE) describing the activity in the cell. It uses the Runge-Kutta-Fehlberg method\n\nto find approximate solutions for the ODE. The simulation is performed for a number of\n\nsteps in the time interval specified as a parameter on the command line. The number of\n\nODEs is 91, and the parameters and the initial data that specify the ODEs are read from\n\nthe input files.\n\n\n\n39\n\nK-Nearest Neighbors (KNN): The KNN is a simple clustering algorithm that\n\nattributes to each point the label of its nearest neighbor. The main computational task\n\nperformed is calculating the distance between the points, which is done in parallel and\n\nbehaves as a Dense Linear Algebra dwarf.\n\nNeedleman-Wunsch (NW): The NW algorithm is an algorithm for sequence align-\n\nments, which obtains the best alignment by using optimal alignments of smaller subse-\n\nquences. It is often used in bioinformatics to align protein or nucleotide sequences. It\n\nconsists of three steps: initialization of the score matrix, calculation of scores, and deduc-\n\ning the alignment from the score matrix. The second step is parallelized. Because NW\n\nhas diagonal stride memory access pattern, it is hard to exploit data locality to improve\n\nperformance.\n\nParticle Filter (PF): PF is a probabilistic model for tracking objects in a noisy\n\nenvironment using a given set of particle samples. The application has several parallel\n\nstages, and implicit synchronization between stages is required. It is a Structured Grid\n\ndwarf application.\n\nPathfinder (PATH): Pathfinder finds the shortest path in a 2D grid using dynamic\n\nprogramming. It computes row by row, achieving the smallest sum of weights between\n\nthe beginning and the end of the path. In each iteration, the shortest path calculation is\n\nparallelized.\n\nStream Cluster (SC): Given a stream of input points, SC calculates median points\n\nand assigns each point of the input stream to the cluster created from the median that\n\nis closer to it. The main computational work in the benchmark is calculating the cost\n\nof opening new centers and the distance of the points from those centers, and so these\n\ncalculations are made in parallel. In both, the distance between two points is the most\n\ntime-consuming task. It is a Dense Linear Algebra application.\n\nSpeckle Reducing Anisotropic Diffusion (SRAD): SRAD performs a Speckle\n\nReducing Anisotropic Diffusion on an image, frequently used on radar and ultrasonic\n\nimaging applications. In each iteration of the method, the whole image is updated. Each\n\nthread computes on a slice of the input matrix, providing high thread locality, and thus\n\ngood scalability. The dwarf classification of this benchmark is Structured Grid.\n\n\n\n40\n\n3.2 Results on Broadwell\n\nThe Broadwell architecture obtains data by accessing the private L1 and L2 caches\n\nof each core, the L3 cache shared by all cores of the same processor, or the main mem-\n\nory. Figures 3.2(a), 3.2(b) and 3.2(c) show the respective hit ratios of each cache level.\n\nFigure 3.2(d) shows the combined hit rate, which is the number of memory transactions\n\nserviced by any cache. The x-axis presents the name of each application ordered by the\n\nIPC (smaller shown first), and the y-axis indicates the hit ratio in percentage. The L1\n\ncache is the fastest and smallest memory and is private to each core. The average L1 hit\n\nratio was 96.5%. Even with an average close to 100%, small variations in the L1 hit ratio\n\nimplies in significant variations in performance, and therefore it is not a good predictor\n\nof IPC by itself. Applications such as MD, NW, KNN, PATH, HS3D, and BFS have L1\n\nhit rates up to 99.9% due to their data accesses pattern. On the other hand, applications\n\nsuch as BP, SRAD, LUD, CFD, and PF, have lower hit rates (86.4% in PF), affecting\n\ntheir performance because they access slower memories more frequently. The L2 cache\n\nis also private to each core. LUD, KNN, LC, HW, and PF are applications that take advan-\n\ntage of the L2 cache and thus have better performance results. The L2 hit ratios in these\n\napplications were up to 98.9%.\n\nThe L3 cache, the last-level cache in the Broadwell architecture, is shared between\n\nall cores of the same processor. It helps several applications by reducing the accesses\n\nin the main memory. The applications SC, MS, and LC, have an L3 hit ratio close to\n\n100%, which means that their entire data fit in the L3. On the other hand, HS2D has\n\nan L3 hit equals to 22.5%, which is low compared to the average (56.2%), and yet the\n\napplication performs above average when compared to the other applications. Most likely,\n\nthe majority of its accesses were already filtered by the higher level caches.\n\nWe can observe, in the combined hit ratio, that most of the applications with the\n\nhighest hit ratios are the ones with the highest IPCs, which indicates that the cache mem-\n\nories have a significant impact on the performance. For instance, PF has the best IPC\n\neven without the highest L1 hit ratio, but it has a very high combined hit ratio. To im-\n\nprove the performance, techniques such as loop interchange and loop tiling can be used.\n\nUsing these techniques, more data is fetched to the cache memories, the data reuse in the\n\ncaches increase, and cache line prefetchers can fetch data from the main memory more\n\naccurately.\n\n\n\n41\n\nFigure 3.2: Cache hit ratio in Broadwell architecture.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n0 %\n\n20 %\n\n40 %\n\n60 %\n\n80 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(a) Cache L1 hit ratio.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n0 %\n\n20 %\n\n40 %\n\n60 %\n\n80 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(b) Cache L2 hit ratio.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n0 %\n\n20 %\n\n40 %\n\n60 %\n\n80 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(c) Cache L3 hit ratio.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n90 %\n\n92 %\n\n94 %\n\n96 %\n\n98 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(d) Combined hit ratio.\n\nSource: The Author.\n\nFigures 3.3(a) and 3.3(b) show results of dynamic random-access memory (DRAM)\n\ntransactions per second and Quickpath Interconnect (QPI) transactions per second. Ap-\n\nplications with low cache hit ratios have a large number of DRAM and QPI transactions,\n\nwhich reduce their performance by accesses to local and remote memories. The DRAM\n\ntransactions per second were in average 13.1 \u00d7 109 which is 6.5 times less than the max-\n\nimum value that is 84.9 \u00d7 109 (in SRAD). The applications SRAD, LUD, and CFD, have\n\nthe highest values of transactions per second, which limit their performance. Transactions\n\nacross the QPI also reduce the performance of applications.\n\n3.3 Results on Knights Landing\n\nThis architecture has private L1 and L2 caches. Thus, the cores can allocate data\n\ninto its private L1 and L2, but on an L2 cache miss, the other L2 caches can be accessed\n\nremotely by the coherence protocol. Figures 3.4(a) and 3.4(b) show the L1 and L2 hit\n\n\n\n42\n\nFigure 3.3: Memory operations in the Broadwell architecture. The y-axis is in logarithmic\nscale in both figures.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n106\n107\n108\n109\n\n1010\n1011\n\nBenchmarks\n\nTr\nan\n\nsa\nct\n\nio\nns\n\n/s\n\n(a) DRAM transactions.\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\nC\n\nF\nD\n\nK\nN\n\nN\nPA\n\nT\nH LC\n\nH\nS\n\n3D H\nW\n\nB\nF\n\nS\nB\n\n+\nP\n\nF\n\n106\n107\n108\n109\n\n1010\n1011\n\nBenchmarks\n\nTr\nan\n\nsa\nct\n\nio\nns\n\n/s\n\n(b) Transactions across the QPI.\n\nSource: The Author.\n\nratio of the applications in the Knights Landing architecture. Figure 3.4(c) shows the\n\ncombined hit ratio. Most applications have L1 hit ratios greater than 80%. However,\n\nsome applications, including NW, KM, PATH, MS, and BFS, have L1 hit ratios from 56.6%\n\nto 66.4%. These applications performances are affected by the latency of L2 and main\n\nmemory, which are longer than L1 latency.\n\nThe L2 cache is the last level cache in the Knights Landing architecture and its\n\nhit ratio impacts directly in applications\u2019 performance. For instance, HW has one of the\n\nhighest performances and also high L2 hit ratio of 82.6%. This application has high\n\nhit ratios in L1 and L2, performing better than other applications. The opposite occurs\n\nwith NW, KM, and PATH, which have low L1 and L2 hit ratios and consequently the worst\n\nperformances in a Knights Landing. Analyzing the combined hit rate, we can observe that\n\nin most cases, applications with a higher combined hit ratio have a better performance.\n\nTherefore, the conclusion we arrive is similar to the one in Broadwell. It is more relevant\n\nto have a hit on any cache and avoid access to the main memory than to have a high hit\n\nrate on any specific level.\n\nThe number of memory accesses is also crucial to the performance of the Knights\n\nLanding architecture. Figures 3.5(a) and 3.5(b) show results of DRAM and MCDRAM\n\ntransactions per second, respectively. Applications with low cache hit ratios have a large\n\nnumber of DRAM and MCDRAM transactions, which reduce their performance by ac-\n\ncesses to local and remote slow memories. The DRAM transactions per second were in\n\naverage 0.29\u00d7109 which is 9.6 times less than the maximum value that is 2.79\u00d7109 (in\n\nSRAD). The applications SRAD, NW, and LUD, have the highest values of transactions per\n\nsecond, which limit their performance. MCDRAM transactions also reduce performance,\n\n\n\n43\n\nFigure 3.4: Cache hit ratio in the Knights Landing architecture.\n\nN\nW K\nM\n\nK\nN\n\nN\nH\n\nS\n3D\n\nC\nF\n\nD\nS\n\nC\nPA\n\nT\nH\n\nM\nS\n\nB\n+\n\nP\nF\n\nB\nP\n\nLC\nS\n\nR\nA\n\nD\nB\n\nF\nS\n\nH\nS\n\n2D H\nW\n\nLU\nD\n\nM\nD\n\n0 %\n\n20 %\n\n40 %\n\n60 %\n\n80 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(a) Cache L1 hit ratio.\n\nN\nW K\nM\n\nK\nN\n\nN\nH\n\nS\n3D\n\nC\nF\n\nD\nS\n\nC\nPA\n\nT\nH\n\nM\nS\n\nB\n+\n\nP\nF\n\nB\nP\n\nLC\nS\n\nR\nA\n\nD\nB\n\nF\nS\n\nH\nS\n\n2D H\nW\n\nLU\nD\n\nM\nD\n\n0 %\n\n20 %\n\n40 %\n\n60 %\n\n80 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(b) Cache L2 hit ratio.\n\nN\nW K\nM\n\nK\nN\n\nN\nH\n\nS\n3D\n\nC\nF\n\nD\nS\n\nC\nPA\n\nT\nH\n\nM\nS\n\nB\n+\n\nP\nF\n\nB\nP\n\nLC\nS\n\nR\nA\n\nD\nB\n\nF\nS\n\nH\nS\n\n2D H\nW\n\nLU\nD\n\nM\nD\n\n60 %\n\n70 %\n\n80 %\n\n90 %\n\n100 %\n\nBenchmarks\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\nio\n\n(c) Combined hit ratio.\n\nSource: The Author.\n\nbut it degrades the performance less than DRAM transactions. Applications with the\n\nhighest IPC such as MD, LUD, and HW have more MCDRAM transactions that DRAM.\n\nThe MCDRAM transactions per second were in average 15.25 \u00d7 109. The conclusion is\n\nthat we should reduce the DRAM and MCDRAM accesses, but is better to have access to\n\nMCDRAM than DRAM.\n\n3.4 Conclusions\n\nIn this chapter, we performed experiments in two architectures aiming to investi-\n\ngate the performance bottlenecks. We discussed the performance of 18 applications of the\n\nRodinia benchmark. The results show that some metrics help in understand applications\n\nperformance, but there are cases where a metric alone is not representative. In the next\n\nchapter, we use this knowledge to optimize a real-world seismic exploration application.\n\n\n\n44\n\nFigure 3.5: Memory operations the Knights Landing architecture. The y-axis is in loga-\nrithmic scale in both figures.\n\nN\nW K\nM\n\nK\nN\n\nN\nH\n\nS\n3D\n\nC\nF\n\nD\nS\n\nC\nPA\n\nT\nH\n\nM\nS\n\nB\n+\n\nP\nF\n\nB\nP\n\nLC\nS\n\nR\nA\n\nD\nB\n\nF\nS\n\nH\nS\n\n2D H\nW\n\nLU\nD\n\nM\nD\n\n106\n107\n108\n109\n\n1010\n1011\n\nBenchmarks\n\nTr\nan\n\nsa\nct\n\nio\nns\n\n/s\n\n(a) DRAM transactions.\n\nN\nW K\nM\n\nK\nN\n\nN\nH\n\nS\n3D\n\nC\nF\n\nD\nS\n\nC\nPA\n\nT\nH\n\nM\nS\n\nB\n+\n\nP\nF\n\nB\nP\n\nLC\nS\n\nR\nA\n\nD\nB\n\nF\nS\n\nH\nS\n\n2D H\nW\n\nLU\nD\n\nM\nD\n\n106\n107\n108\n109\n\n1010\n1011\n\nBenchmarks\n\nTr\nan\n\nsa\nct\n\nio\nns\n\n/s\n\n(b) MCDRAM transactions.\n\nSource: The Author.\n\n\n\n45\n\n4 OPTIMIZATION STRATEGIES FOR MULTI-CORE AND MANY-CORE\n\nChapter 3 investigates multi-core and many-core architectures performance bot-\n\ntlenecks. Based on that, we address some performance optimization strategies aiming\n\nto reduce the impact of performance bottlenecks. We first employ the loop interchange\n\ntechnique to improve cache memory usage. Afterward, we put vectorization into account\n\nintending to increase the performance of floating-point computations. Later, we apply\n\nloop scheduling and collapse to improve load balancing; Finally, we increase the memory\n\nhierarchy performance by use thread and data mapping.\n\nTherefore, in order to validate our assumptions, we improve the performance of a\n\nreal-world seismic exploration application provided by Petrobras. It implements an acous-\n\ntic wave propagation approximation which is the current backbone for seismic imaging\n\ntools. It has been extensively applied for imaging potential oil and gas reservoirs beneath\n\nsalt domes for the last five years. Such acoustic propagation engines should be continu-\n\nously ported to the newest HPC hardware available to maintain competitiveness.\n\nThe remainder of this Chapter is organized as follows. First, we introduced the\n\nseismic exploration application. Then, we present the optimization techniques and the\n\nresults for both architectures. Finally, we discuss and summarize the conclusions of this\n\nChapter.\n\n4.1 Seismic Exploration Application\n\nGeophysics exploration remains fundamental to the modern world to keep up with\n\nthe demand for energetic resources. This endeavor results in expensive drilling costs\n\n(100M$-200M$), with less than 50% of accuracy per drill. Thus, Oil &amp; Gas industries rely\n\non software focused on High-Performance Computing (HPC) as an economically viable\n\nway to reduce risks. The fundamentals of many software mechanisms for exploration\n\ngeophysics are based on wave propagation simulation engines. For instance, on seismic\n\nimaging tools, modeling, migration and inversion use wave propagators at the core. These\n\nsimulation engines are built as Partial Differential Equation (PDE) solvers, where the PDE\n\nsolved in each case defines the accuracy of the approximation to the real physics when a\n\nwave travels through the Earth\u2019s internals.\n\nIn this dissertation, we focus on improving the performance of a Reverse Time\n\nMigration (RTM) program provided by Petrobras, the leading Brazilian oil company.\n\n\n\n46\n\nThe program simulates the propagation of a single wavelet over time by solving the\n\nisotropic acoustic wave propagation (Equation 4.1), and the isotropic acoustic wave prop-\n\nagation with variable density (Equation 4.2) under Dirichlet boundary conditions over a\n\nfinite three-dimensional rectangular domain, prescribing p = 0 to all boundaries, where\n\np(x,y,z,t) is the acoustic pressure, V (x,y,z) is the propagation speed and ?(x,y,z) is\n\nthe media density. The Laplace Operator is discretized by a 12th order finite differences\n\napproximation on each spatial dimension. A 2nd finite differences operator approximates\n\nthe derivatives.\n\n1\n\nV 2\n.\n?2p\n\n?t2\n= ?2p (4.1)\n\n1\n\nV 2\n.\n?2p\n\n?t2\n= ?2p?\n\n??\n?\n\n.?p (4.2)\n\nNext section present the performance evaluation results where we employ the same\n\nexperimental setup and methodology described in Section 3.1. The difference is the work-\n\nload that in this chapter is the seismic exploration application. The seismic code was writ-\n\nten in standard C and leverage from OpenMP directives for shared-memory parallelism.\n\nThe stencil used in the experiments was 1024\u00d7256\u00d7256 resulting in total memory us-\n\nage of 2.5 GB.\n\n4.2 Results\n\nThe following subsections present the optimizations techniques we used to im-\n\nprove the performance of a real-world application and the experiments performed to val-\n\nidate them. We describe the optimizations and analyze how they address the challenges\n\nimposed by multi-core and many-core architectures.\n\n4.2.1 Memory Access Pattern to Improve Data Locality\n\nCurrent computer architectures provide caches and hardware prefetchers to help\n\nprogrammers manage data implicitly (LEE et al., 2010). The loop interchange technique\n\ncan be used to improve the performance of both elements by exchanging the order of two\n\nor more loops. It also reduces memory bank conflicts, improves data locality and helps\n\n\n\n47\n\nto reduce the stride of an array computation. In this way, more data that is fetched to the\n\ncache memories are effectively accessed, the data reuse in the caches is increased, and\n\ncache line prefetchers are able to fetch data from the main memory more accurately. In\n\nthis application, we have three loops that are used to compute the stencil. They can be\n\nexecuted on any order without changing the results. The default loop sequence was xyz.\n\nWe propose to change the loop sequence from xyz to all possible combinations.\n\nThe outermost loop is the one that was always parallelized using threads. In Figure 4.1,\n\nwe show in the X-axis the sequences and in the Y-axis the speedup versus the xyz se-\n\nquence. The bars represent the architecture. Loop sequence zyx has better performance\n\nand combined cache hit ratio results in Broadwell. The speedup compared with the xyz\n\nversion is 5.3\u00d7. This sequence is better than others because the data is accessed in a way\n\nthat benefits more from the caches, as can be observed in the cache hit rates shown in\n\nFigure 4.2(a). The L2 cache hit rate was improved from 14.9% to 77% when the loop\n\nsequence was changed to zyx. The L3 cache hit was also improved from 76.2% to 92%\n\nin Broadwell. However, this was not the case with the L1 cache, as its hit rate decreases\n\nfrom 82.3% to 73%. In Knights Landing, version yzx have better performance and com-\n\nbined cache hit ratio results. The speedups were up to 3.9\u00d7 showing that this optimization\n\nimpact less on the performance of Knights Landing than Broadwell. The cache hit rates\n\nare shown in Figure 4.2(b). The L2 cache hit rate was improved from 9.6% to 95.9%.\n\nAlthough L1 cache hit rate decreases in both architectures, it shows that the best option\n\naiming performance is to increase the last level cache (LLC) hit rates, even when the\n\ncache hit rate of any other level decreases.\n\nThe differences in cache miss happened because the data access stride becomes\n\ndifferent when changing the loop sequence, influencing both spatial and temporal local-\n\nities. Despite the reductions in the L1 hit rate, the increase of the LLC hit rates resulted\n\nin the highest performance improvement and is, therefore, the best choice for this appli-\n\ncation since LLC caches have the highest latencies per access and increase their hit rates\n\nconsequently reduces latency. The performance improvement in the Knights Landing is\n\nlower than in Broadwell because the amount of cache memory available per thread in the\n\nKnights Landing is much lower. The best loop sequence for Broadwell is zyx while for\n\nKnights Landing is yzx. This difference is due the L2 cache size which is 512 KB per\n\ncore Knights Landing and 256 KB in Broadwell.\n\n\n\n48\n\nFigure 4.1: Speedup over the xyz sequence.\n\nxyz xzy yxz yzx zxy zyx\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nLoop sequence\n\nS\npe\n\ned\nup\n\nBroadwell Knights Landing\n\nSource: The Author.\n\nFigure 4.2: Cache hit ratio on different architectures.\n\nxyz xzy yxz yzx zxy zyx\n0%\n\n20%\n\n40%\n\n60%\n\n80%\n\n100%\n\nLoop sequence\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\ne\n\nL1 L2 L3 Combined\n\n(a) Broadwell.\n\nxyz xzy yxz yzx zxy zyx\n0%\n\n20%\n\n40%\n\n60%\n\n80%\n\n100%\n\nLoop sequence\n\nC\nac\n\nhe\nH\n\nit\nR\n\nat\ne\n\nL1 L2 Combined\n\n(b) Knights Landing.\n\nSource: The Author.\n\n4.2.2 Exploiting SIMD for Floating-point Computations\n\nRecent hardware approaches increase performance by integrating more cores with\n\nwider SIMD (single instruction, multiple data) units (SATISH et al., 2012). This data\n\nprocessing technique, called vectorization, has units that perform, in one instruction, the\n\nsame operation on several operands. To maximize the effectiveness of vectorization, the\n\nmemory addresses accessed by the same instruction on consecutive loop iterations must\n\nalso be consecutive. In this way, the compiler can load and store the operands of con-\n\nsecutive iterations using a single load/store instructions, optimizing cache memory usage,\n\nsince data is already fetched in blocks from the main memory. More recent processors in-\n\ntroduce the support for gather and scatter instructions, which reduce the overhead\n\nof loading/storing non-consecutive memory addresses. Nevertheless, the performance is\n\nstill much higher when the addresses are consecutive. In this context, we modified the\n\nsource code such that the memory addresses accessed by the same instruction were con-\n\nsecutive along loop iterations.\n\n\n\n49\n\nFigure 4.3: Performance gain using vectorization.\n\nxyz xzy yxz yzx zxy zyx\n0\n1\n2\n3\n4\n5\n6\n7\n\nLoop sequence\n\nS\npe\n\ned\nup\n\nBroadwell Knights Landing\n\nSource: The Author.\n\nWe used the Advanced Vector Extensions (AVX) instructions, which is an instruc-\n\ntion set architecture extension to use SIMD units to increase the performance of the float-\n\ning point computations. These instructions use specific floating point units that can load,\n\nstore or perform calculations on several operands at once. As previously described, the\n\nefficiency of AVX is better when the elements are accessed in the memory contiguously,\n\nas they can be loaded and stored in blocks. We show the execution time speedup in Fig-\n\nure 4.3. The speedup shown is relative to the loop sequence without AVX. The sequences\n\nyzx and zyx have better results because they have more elements being accessed contigu-\n\nously. The performance gain differs from architecture to architecture. In Broadwell, the\n\nimprovement was up to 1.4\u00d7. In the Knights Landing architecture, the improvement was\n\nup to 6.5\u00d7. These differences are due to the size of each architecture\u2019s vector unit and the\n\nnumber of cores used.\n\n4.2.3 Improving Load Balancing\n\nSome applications have regions with different computation load requirements, e.g.\n\nboundaries, potentially causing unevenness in the computing time among the threads. The\n\ntime to execute a parallel application is determined by the task that takes the most time to\n\nfinish, and thereby by the core with the highest amount of work. Hence, by distributing the\n\nwork more evenly among the cores, we can reduce the execution time of an application.\n\nLoad balancing techniques reduce these disparities and thereby improve resource usage\n\nand performance. In the context of multi-core and many-core systems, load balancing is\n\neven more important due to a large number of cores.\n\nThe OpenMP specification has a directive to indicate whether the scheduling is\n\n\n\n50\n\nstatic, dynamic or guided. The static scheduling is the default value and it assigns chunks\n\nto threads in a round-robin fashion before the computation starts. The dynamic and guided\n\napproaches distribute the work during runtime as thread request, but in dynamic, all the\n\nchunks have the same size, while guided assigns larger chunks first, and their size de-\n\ncreases along the iterations. In addition, loop collapse also helps to improve load balanc-\n\ning, because it increases the total number of iterations partitioned across the threads by\n\ncollapsing two or more loops.\n\nWe investigated the impact of different OpenMP scheduling policies with and\n\nwithout loop collapse. Figures 4.4 and 4.4(b) show the speedup of each combination of\n\nschedule policy in the Broadwell and Knights Landing architectures. The baseline used to\n\ncalculate the speedup is the execution time of the default schedule policy, which is static\n\nwith a chunk size equal to the number of iterations divided by the number of threads.\n\nFigures 4.4 and 4.4(b) present the results for Broadwell and Knights Landing. In\n\nthe experiments without the collapse, we split the outer loop between the threads. The\n\nbest speedup without collapse in Broadwell was using the guided scheduling policy. It\n\nwas up to 1.06\u00d7 faster than the default scheduler. This approach is useful to applications\n\nwhose workload changes in runtime or have some regions with unbalanced workloads. In\n\nKnights Landing, the performance was almost the same as without any scheduler.\n\nA way to improve the performance with larger chunks is collapsing the loops.\n\nThe idea is that, with more work to be divided between the threads, larger chunks can\n\nbe used while keeping a good load balance. In order to investigate this, we collapsed the\n\nouter loops and evaluated different combinations of the scheduler. Results show that for\n\nBroadwell, the best policy is the static scheduler. In Knights Landing, the best policy with\n\ncollapse is the guided scheduler. The performance in Broadwell was improved by up to\n\n1.04\u00d7 while in Knights Landing by up to 1.11\u00d7.\n\n4.2.4 Optimizing Memory Affinity\n\nThe goal of mapping mechanisms is to improve resource usage by arranging\n\nthreads and data according to a fixed policy, where each approach may target different\n\naspects to enhance. For example, there are techniques focused on improving locality, to\n\nreduce cache misses and remote memory accesses, as well as traffic on inter-chip inter-\n\nconnections (CRUZ et al., 2016). Other policies seek a uniform load distribution among\n\nthe cores and memory controllers.\n\n\n\n51\n\nFigure 4.4: Loop collapse and scheduling performance gain.\nwithout collapse collapsing two loops\n\nstatic dynamic guided\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\n1.25\n\nS\npe\n\ned\nup\n\n(a) Broadwell.\n\nstatic dynamic guided\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\n1.25\n\nS\npe\n\ned\nup\n\n(b) Knights Landing.\n\nSource: The Author.\n\nWe evaluate different thread and data mapping techniques. We combine thread and\n\ndata mapping techniques because together they improve the performance even more (DI-\n\nENER et al., 2016a). The following thread mapping policies were evaluated:\n\nDefault (baseline) The default thread mapping of the Linux kernel (WONG et al., 2008),\n\nfocused on load balancing.\n\nCompact Thread Mapping A compact thread mapping that arranges neighbor threads\n\nto closer cores according to the memory hierarchy (EICHENBERGER et al., 2012).\n\nScatter Thread Mapping A Scatter thread mapping distributes the threads as evenly as\n\npossible across the entire system, which is the opposite of compact.\n\nRoundRobin Thread Mapping A RoundRobin thread mapping distributes the threads\n\nto the cores in order from 0 to the number of cores minus 1.\n\nThe following data mapping policies were evaluated:\n\nDefault (baseline) The default data mapping of Linux, the first-touch data mapping pol-\n\nicy, where the page is mapped to the NUMA node of the core that accessed the page\n\nfor the first time.\n\nNUMA Balancing Data Mapping The NUMA Balancing data mapping (CORBET, 2012)\n\nmigrates pages along the execution to the NUMA node of the latest thread that ac-\n\ncessed the page. The NUMA node is detected by introducing artificial page faults\n\nalong the execution.\n\nInterleave Data Mapping The interleave data mapping arranges consecutive pages to\n\nconsecutive NUMA nodes. In the Knights Landing architecture, due to having the\n\nDRAM and MCDRAM NUMA nodes, we also evaluate the Interleave MCDRAM\n\npolicies, which distribute pages only to MCDRAM nodes.\n\n\n\n52\n\nThe results obtained from different mapping policies in the Broadwell and Knights\n\nLanding architectures are shown in Figures 4.5 and 4.6. The speedup was normalized to\n\nthe baseline mapping with the corresponding loop sequence, such that we can measure\n\nthe benefits from mapping more precisely. The results of the cache miss, previously\n\nshown in Figure 4.2, can help us understand the behavior from different mapping policies.\n\nThe reason for this result is that most of the improvements from mapping are due to the\n\nreduction of accesses to the main memory, but these benefits are mitigated if the cache hit\n\nrate is high.\n\nIt can be observed that the xyz variant, which benefited most from mapping, also\n\nhad most cache misses. In the other configurations, since the L3 cache hit rate is very high,\n\nwe have few accesses to the main memory, such that, as explained, the benefit from the\n\nmapping is lower. The usage of an interleaved data mapping provided a better distribution\n\nof the load between the memory controllers, with the cost of additional inter-chip traffic.\n\nDespite the trade-off between the load and inter-chip traffic, the interleaved data mapping\n\nprovided the best improvements overall.\n\nOn the Broadwell architecture, the best speedup was 1.6\u00d7, achieved for round-\n\nrobin with interleave mapping for the xyz version. It combines an even thread distribu-\n\ntion with a balanced distribution of the pages. We combine both thread and data mapping\n\nbecause, in most applications, the effectiveness of data mapping depends on thread map-\n\nping.\n\nFor the experiments in the Knights Landing architecture, we analyze the same\n\ntechniques as in Broadwell plus interleave using MCDRAM nodes. The best speedup\n\nwas 4.4\u00d7 for scatter with interleave MCDRAM for the xyz version. We can observe that\n\nin Knights Landing, the best improvements usually happened with policies that focus on\n\nload balancing, such as the scatter thread mapping and interleave data mapping.\n\nThe difference in the memory locality between the cores affects these architectures\n\nperformance and scalability. In NUMA systems, the time to access the main memory de-\n\npends on the core that requested the memory access and the NUMA node that contains the\n\ndestination memory bank. If the core and destination memory bank belong to the same\n\nnode, we have a local memory access. On the other hand, if the core and the destination\n\nmemory bank belong to different NUMA nodes, we have a remote memory access. Local\n\nmemory accesses are faster than remote memory accesses. By mapping the threads and\n\ndata of the application in such a way that we increase the number of local memory ac-\n\ncesses over remote memory accesses, the average latency of the main memory is reduced.\n\n\n\n53\n\nFigure 4.5: Mapping results on Broadwell.\nRoundRobin + NUMA Balancing Compact + NUMA Balancing\nScatter + NUMA Balancing RoundRobin + Interleave\nCompact + Interleave Scatter + Interleave\n\nxyz yzx zyx\n0\n\n0.5\n\n1\n\n1.5\n\nS\npe\n\ned\nup\n\nSource: The Author.\n\nFigure 4.6: Mapping results on Knights Landing.\nRoundRobin + NUMA Balancing Compact + NUMA Balancing\nScatter + NUMA Balancing RoundRobin + Interleave\nCompact + Interleave Scatter + Interleave\n\nxyz yzx zyx\n0\n\n2\n\n4\n\nS\npe\n\ned\nup\n\nSource: The Author.\n\nFigure 15 shows the number of remote memory accesses that were reduced when we\n\nuse mapping techniques. For xyz version, we reduced up to 150 GB of remote accesses\n\nimproving application performance and scalability.\n\n4.3 Conclusions\n\nIn this chapter, we applied and analyzed the performance of a set of optimization\n\ntechniques on multi-core and many-core architectures. We showed that all techniques\n\ntogether could improve the performance of a real-world seismic exploration application\n\nby up to 22.7\u00d7 and 56.7\u00d7 on Broadwell and Knights Landing, respectively. Regarding the\n\nperformance, the best was 259.9 GFLOPS on the Broadwell architecture. We emphasize\n\nthat the optimizations presented in this chapter can also be applied to other applications\n\nand architectures.\n\n\n\n54\n\n\n\n55\n\n5 CONCLUSION AND FUTURE WORK\n\nThis work performs a detailed analysis of multi-core and many-core architectures\n\nand optimizes their performance using different strategies. We evaluated 18 benchmarks\n\nand used hardware performance counters to gather accurate measurements of the actual\n\nimpact of different factors that influence the performance. This evaluation has shown that\n\nsome metrics help in understand applications performance, but there are cases where a\n\nmetric alone is not representative.\n\nWe use our performance bottlenecks study as a basis for optimizing a real-world\n\ngeophysics model. We applied the following optimization techniques: (1) loop inter-\n\nchange to improve cache memory usage; (2) vectorization to increase the performance of\n\nfloating point computations; (3) loop scheduling and collapse to improve load balancing;\n\nand (4) thread and data mapping to better use the memory hierarchy. These optimizations\n\ncan also be applied to other applications and architectures.\n\nIn our performance optimization experiments, we show that loop interchange is a\n\nuseful technique to improve the performance of different cache memory levels, being able\n\nto improve the performance by up to 5.3\u00d7 and 3.9\u00d7 on Broadwell and Knights Landing,\n\nrespectively. These improvements happened because we were able to increase the last\n\nlevel cache hit ratio by up to 95.9%. Furthermore, by changing the code such that ele-\n\nments are accessed contiguously between loop iterations, we were able to vectorize the\n\ncode, which improved performance by up to 1.4\u00d7 and 6.5\u00d7. Load Balancing and collapse\n\ntechniques were also evaluated, but the application balance mitigated their performance\n\nimprovements. These techniques improve the performance of Knights Landing by up to\n\n1.1\u00d7. Thread and data mapping techniques were also evaluated, with a performance im-\n\nprovement of up to 1.6\u00d7 and 4.4\u00d7 We also compared the best version of each architecture\n\nand showed that we were able to improve the performance of Broadwell by 22.7\u00d7 and\n\nKnights Landing by 56.7\u00d7 compared with a naive version but at the end, Broadwell was\n\n1.2\u00d7 faster than Knights Landing.\n\n5.1 Future Work\n\nFuture work will focus on proposing an automatic mechanism to optimize the per-\n\nformance of multi-core and many-core architectures. Furthermore, we plan on expanding\n\nour evaluation of the performance bottlenecks by using newer architectures and evaluate\n\n\n\n56\n\nenergy consumption.\n\n5.2 Publications\n\nThe following papers were produced during this dissertation. We first list the ones\n\nstrong related to this work, including those under review:\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; NAVAUX, P. O. A.; PANETTA, J. Otimizando\n\numa Aplica\u00e7\u00e3o de Geof\u00edsica com Mapeamento de Threads e Dados. In: WSCAD\n\n2018 - 19th Symposium on Computer Systems, 2018, (Under Review).\n\n\u2022 SERPA, M. S.; KRAUSE, A. M.; CRUZ, E. H. M.; PASIN, M.; FELBER, P.;\n\nNAVAUX, P. O. A. Optimizing Machine Learning Algorithms on Multi-core and\n\nMany-core Architectures using Thread and Data Mapping. In: PDP 2018 - Eu-\n\nromicro International Conference on Parallel, Distributed, and Network-Based Pro-\n\ncessing, 2018, Qualis A2.\n\n\u2022 CRUZ, E. H. M.; DIENER, M.; SERPA, M. S.; NAVAUX, P. O. A.; PILLA, L. L.;\n\nKOREN, I. Improving Communication and Load Balancing with Thread Mapping\n\nin Manycore Systems. In: PDP 2018 - Euromicro International Conference on\n\nParallel, Distributed, and Network-Based Processing, 2018, Qualis A2.\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; DIENER, M.; KRAUSE, A. M.; NAVAUX, P. O.\n\nA; PANETTA, J.; FARR\u00c9S, A.; ROSAS, C.; HANZICH, M. Optimization Strate-\n\ngies for Geophysics Models on Many-core Systems. In: International Journal of\n\nHigh Performance Computing Applications, 2018 (Minor Review), Qualis B1.\n\n\u2022 SERPA, M. S.; KRAUSE, A. M.; CRUZ, E. H. M.; NAVAUX, P. O. A. Otimizando\n\nAlgoritmos de Machine Learning com Mapeamento de Threads e Dados. In: ERAD\n\n2018 - XVIII Escola Regional de Alto Desempenho, 2018.\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; DIENER, M.; ROSAS, C.; PANETTA, J.; HANZICH,\n\nM.; NAVAUX, P. O. A. Strategies to Improve the Performance of a Geophysics\n\nModel for Different Manycore Systems. In: WAMCA 2017 - Workshop on Appli-\n\ncations for Multi-Core Architectures, 2017.\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; MOREIRA, F. B.; DIENER, M.; NAVAUX, P.\n\nO. A. A Deep Analysis of Memory Performance and Bottlenecks in Multicore and\n\nManycore Architectures. In: International Journal of Parallel Programming, 2017\n\n(Major Review), Qualis B1.\n\n\n\n57\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; NAVAUX, P. O. A. Impacto de T\u00e9cnicas de\n\nOtimiza\u00e7\u00e3o de Software em Arquiteturas Multicore e Manycore. In: ERAD 2017 -\n\nXVII Escola Regional de Alto Desempenho, 2017.\n\n\u2022 SERPA, M. S.; CRUZ, E. H. M.; MOREIRA, F. B.; DIENER, M.; NAVAUX, P. O.\n\nA. Impacto do Subsistema de Mem\u00f3ria em Arquiteturas CPU e GPU. In: WSCAD\n\n2016 - 17th Symposium on Computer Systems, 2016.\n\nThe following papers were also published during this dissertation:\n\n\u2022 PAVAN, P. J.; SERPA, M. S.; PADOIN, E.L.; SCHNORR, L.; NAVAUX, P. O.\n\nA.; PANETTA, J. Melhorando o Desempenho de Opera\u00e7\u00f5es de E/S do Algoritmo\n\nRTM Aplicado na Prospec\u00e7\u00e3o de Petr\u00f3leo. In: WSCAD 2018 - 19th Symposium\n\non Computer Systems, 2018, (Under Review).\n\n\u2022 PAVAN, P. J.; SERPA, M. S.; CARRE\u00d1O, E. D.; ABAUNZA, V. E. M.; PADOIN,\n\nE. L.; NAVAUX, P. O. A.; PANETTA, J.; M\u00c9HAUT, J. F. Improving Performance\n\nand Energy Efficiency of Stencil Based Applications on GPU Architectures. In:\n\nLatin America High Performance Computing Conference, Piedecuesta - Colombia,\n\n2018.\n\n\u2022 ABAUNZA, V. E. M.; SERPA, M. S.; NAVAUX, P. O. A.; PADOIN, E. L.; PANETTA,\n\nJ.; M\u00c9HAUT, J. F. Performance Evaluation of Stencil Computations based on Source-\n\nto-Source Transformations. In: Latin America High Performance Computing Con-\n\nference, Piedecuesta - Colombia, 2018.\n\n\u2022 PAVAN, P. J.; SERPA, M. S.; ABAUNZA, V. E. M.; PADOIN, E. L.; NAVAUX, P.\n\nO. A.; PANETTA, J. Strategies to Improve the Performance and Energy Efficiency\n\nof Stencil Computations for NVIDIA GPUs. In: WPerformance 2018 - XVII Work-\n\nshop em Desempenho de Sistemas Computacionais e de Comunica\u00e7\u00e3o, 2018.\n\n\u2022 ABAUNZA, V. E. M.; SERPA, M. S.; NAVAUX, P. O. A.; PADOIN, E. L.; PANETTA,\n\nJ. Performance Prediction of Geophysics Numerical Kernels on Accelerator Archi-\n\ntectures. In: Energy 2018 - The International Conference on Smart Grids, Green\n\nCommunications and IT Energy-aware Technologies, 2018.\n\n\u2022 SCHEPKE, C.; LIMA, J. V. F; SERPA, M. S. Challenges on Porting Lattice Boltz-\n\nmann Method on Accelerators: NVIDIA Graphic Processing Units and Intel Xeon\n\nPhi. In: Analysis and Applications of Lattice-Boltzmann Simulations, p. 30-53,\n\n2018.\n\n\u2022 KRAUSE, A. M.; SERPA, M. S.; NAVAUX, P. O. A. Implementa\u00e7\u00e3o de uma Apli-\n\n\n\n58\n\nca\u00e7\u00e3o de Simula\u00e7\u00e3o Geof\u00edsica em OpenCL. In: ERAD 2018 - XVIII Escola Re-\n\ngional de Alto Desempenho, 2018.\n\n\u2022 CRUZ, E. H. M.; CARRE\u00d1O, E. D.; SERPA, M. S.; NAVAUX, P. O. A; FREITAS,\n\nI. J. F. Intel Modern Code: Programa\u00e7\u00e3o Paralela e Vetorial AVX para o Processador\n\nIntel Xeon Phi Knights Landing. In: WSCAD 2017 - XVIII Simp\u00f3sio em Sistemas\n\nComputacionais de Alto Desempenho, 2017.\n\n\u2022 LORENZONI, R. K.; SERPA, M. S.; PADOIN, E. L.; PANETTA, J.; NAVAUX, P.\n\nO. A.; MEHAUT, J. Otimizando o Uso do Subsistema de Mem\u00f3ria de GPUs para\n\nAplica\u00e7\u00f5es Baseadas em Est\u00eanceis. In: WPerformance 2017 - XVI Workshop em\n\nDesempenho de Sistemas Computacionais e de Comunica\u00e7\u00e3o, 2017.\n\n\u2022 ABAUNZA, V. E. M.; SERPA, M. S.; DUPROS, F.; PADOIN, E. L.; NAVAUX, P.\n\nO. A. Performance Prediction of Acoustic Wave Numerical Kernel on Intel Xeon\n\nPhi Processor. In: CARLA 2017 - Latin America High Performance Computing\n\nConference, 2017.\n\n\u2022 LORENZONI, R. K.; SERPA, M. S.; PADOIN, E. L.; NAVAUX, P. O. A. Melho-\n\nrando o Desempenho da Computa\u00e7\u00e3o de Est\u00eanceis em GPUs. In: CNMAC 2017 -\n\nBrazilian Society of Computational and Applied Mathematics, 2017.\n\n\u2022 LORENZONI, R. K.; SERPA, M. S.; PADOIN, E. L.; NAVAUX, P. O. A.; M\u00c9HAUT,\n\nJ. F. Impacto do Subsistema de Mem\u00f3ria da Arquitetura Kepler no Desempenho de\n\numa Aplica\u00e7\u00e3o de Propaga\u00e7\u00e3o de Onda. In: ERAD 2017 - XVII Escola Regional\n\nde Alto Desempenho, 2017.\n\n\u2022 SILVA, S. A.; SERPA, M. S.; SCHEPKE, C. T\u00e9cnicas de Otimiza\u00e7\u00e3o Computa-\n\ncional em um Algoritmo de Multiplica\u00e7\u00e3o de Matrizes. In: ERAD 2017 - XVII\n\nEscola Regional de Alto Desempenho, 2017.\n\n\u2022 SERPA, M. S.; BEZ, J. L.; CRUZ, E. H. M.; DIENER, M.; ALVES, M. A. Z.;\n\nNAVAUX, P. O. A. Intel Modern Code: Programa\u00e7\u00e3o Vetorial e Paralela em Ar-\n\nquiteturas Intel Xeon e Intel Xeon Phi. In: ERAD 2017 - Escola Regional de Alto\n\nDesempenho, 2017.\n\n\u2022 SILVA, S. A.; SERPA, M. S.; SCHEPKE, C. Derrube Todos os Recordes de Ganho\n\nde Desempenho Otimizando seu C\u00f3digo. In: ERAD 2017 - Escola Regional de Alto\n\nDesempenho, 2017.\n\n\u2022 SILVA, S. A.; SERPA, M. S.; SCHEPKE, C. T\u00e9cnicas de Otimiza\u00e7\u00e3o Loop Un-\n\nrolling e Loop Tiling em Multiplica\u00e7\u00f5es de Matrizes Utilizando OpenMP. In: WS-\n\n\n\n59\n\nCAD 2016 - 17th Symposium on Computer Systems, 2016 (Best Paper Award).\n\n\u2022 KAPELINSKI, K.; SCHEPKE, C.; SERPA, M. S. Uma Abordagem Inicial para\n\na Paraleliza\u00e7\u00e3o de uma Aplica\u00e7\u00e3o de Simula\u00e7\u00e3o de Abla\u00e7\u00e3o por Radiofrequ\u00eancia\n\npara o Tratamento de C\u00e2ncer. In: WSCAD 2016 - 17th Symposium on Computer\n\nSystems, 2016.\n\n\u2022 ROLOFF, E.; CARRE\u00d1O, E. D.; VALVERDE-S\u00c1NCHEZ, J. K. M.; DIENER, M.;\n\nSERPA, M. S.; HOUZEAUX, G.; SCHNORR, L.; MAILLARD, N.; GASPARY,\n\nL. P.; NAVAUX, P. O. A. Performance Evaluation of Multiple Cloud Data Centers\n\nAllocations for HPC. In: CARLA 2016 - Latin America High Performance Com-\n\nputing Conference, 2016.\n\n\n\n60\n\n\n\n61\n\nREFERENCES\n\nANDREOLLI, C. et al. Chapter 23 - Characterization and Optimization Methodology\nApplied to Stencil Computations. In: High Performance Parallelism Pearls. [S.l.]:\nMorgan Kaufmann, 2015.\n\nAUSAVARUNGNIRUN, R. et al. Exploiting inter-warp heterogeneity to improve gpgpu\nperformance. In: IEEE. 2015 International Conference on Parallel Architecture and\nCompilation (PACT). [S.l.], 2015. p. 25\u201338.\n\nBORKAR, S.; CHIEN, A. A. The future of microprocessors. Communications of the\nACM, ACM, v. 54, n. 5, p. 67\u201377, 2011.\n\nCASTRO, M. et al. Seismic wave propagation simulations on low-power and\nperformance-centric manycores. Parallel Computing, v. 54, p. 108 \u2013 120, 2016.\n\nCHE, S. et al. A characterization of the Rodinia benchmark suite with comparison to\ncontemporary CMP workloads. In: IEEE International Symposium on Workload\nCharacterization (IISWC). [S.l.]: Ieee, 2010. ISBN 978-1-4244-9297-8.\n\nCOOK, S. CUDA programming: a developer\u2019s guide to parallel computing with\nGPUs. [S.l.]: Newnes, 2012.\n\nCORBET, J. Toward better NUMA scheduling. 2012. Dispon\u00edvel em:&lt;http:\n//lwn.net/Articles/486858/>.\n\nCOTEUS, P. W. et al. Technologies for exascale systems. IBM Journal of Research and\nDevelopment, IBM, v. 55, n. 5, p. 14\u20131, 2011.\n\nCRUZ, E. H. et al. Lapt: A locality-aware page table for thread and data mapping.\nParallel Computing (PARCO), v. 54, p. 59 \u2013 71, 2016. ISSN 0167-8191.\n\nCRUZ, E. H. et al. Lapt: A locality-aware page table for thread and data mapping.\nParallel Computing, Elsevier, v. 54, p. 59\u201371, 2016.\n\nCRUZ, E. H. et al. Improving communication and load balancing with thread mapping\nin manycore systems. In: IEEE. Parallel, Distributed and Network-based Processing\n(PDP), 2018 26th Euromicro International Conference on. [S.l.], 2018. p. 93\u2013100.\n\nCRUZ, E. H. M. et al. Hardware-Assisted Thread and Data Mapping in Hierarchical\nMulticore Architectures. ACM Trans. Archit. Code Optim., ACM, New York, NY,\nUSA, v. 13, n. 3, set. 2016. ISSN 1544-3566.\n\nDENG, L. et al. Performance Optimization and Comparison of the Alternating Direction\nImplicit CFD Solver on Multi-core and Many-Core Architectures. Chinese Journal of\nElectronics, IET, v. 27, n. 3, p. 540\u2013548, 2018.\n\nDIENER, M. et al. Affinity-Based Thread and Data Mapping in Shared Memory Systems.\nACM Computing Surveys (CSUR), v. 49, n. 4, p. 1\u201338, 2016. ISSN 15577341.\n\nDIENER, M. et al. Kernel-based thread and data mapping for improved memory\naffinity. IEEE Transactions on Parallel and Distributed Systems, IEEE, v. 27, n. 9, p.\n2653\u20132666, 2016.\n\nhttp://lwn.net/Articles/486858/\nhttp://lwn.net/Articles/486858/\n\n\n62\n\nEICHENBERGER, A. E. et al. The design of OpenMP thread affinity. Lecture Notes in\nComputer Science, v. 7312 LNCS, p. 15\u201328, 2012. ISSN 03029743.\n\nFALCH, T. L.; ELSTER, A. C. Register caching for stencil computations on gpus.\nIn: 2014 16th International Symposium on Symbolic and Numeric Algorithms for\nScientific Computing. [S.l.]: IEEE, 2014. p. 479\u2013486.\n\nGROPP, W.; SNIR, M. Programming for exascale computers. Computing in Science\nEngineering, v. 15, n. 6, p. 27\u201335, 2013.\n\nHASIB, A. A. et al. Energy efficiency effects of vectorization in data reuse\ntransformations for many-core processors\u2014a case study. Journal of Low Power\nElectronics and Applications, Multidisciplinary Digital Publishing Institute, v. 7, n. 1,\np. 5, 2017.\n\nHE, J.; CHEN, W.; TANG, Z. Nestedmp: Enabling cache-aware thread mapping for\nnested parallel shared memory applications. Parallel Computing, Elsevier, v. 51, p.\n56\u201366, 2016.\n\nHEINECKE, A. et al. High Order Seismic Simulations on the Intel Xeon Phi\nProcessor (Knights Landing). Cham: Springer International Publishing, 2016.\n343\u2013362 p.\n\nHSU, J. Three paths to exascale supercomputing. IEEE Spectrum, v. 53, n. 1, p. 14\u201315,\n2016.\n\nINTEL. Intel Performance Counter Monitor - A better way to measure CPU\nutilization. 2012. Dispon\u00edvel em:&lt;http://www.intel.com/software/pcm>.\n\nINTEL. Intel VTune Amplifier XE 2016. 2016.\n\nJIA, W.; SHAW, K. A.; MARTONOSI, M. Mrpb: Memory request prioritization for\nmassively parallel processors. In: IEEE. 2014 IEEE 20th International Symposium on\nHigh Performance Computer Architecture (HPCA). [S.l.], 2014. p. 272\u2013283.\n\nKIRK, D. B.; WEN-MEI, W. H. Programming massively parallel processors: a\nhands-on approach. [S.l.]: Morgan kaufmann, 2016.\n\nLASTOVETSKY, A.; SZUSTAK, L.; WYRZYKOWSKI, R. Model-based optimization\nof eulag kernel on intel xeon phi through load imbalancing. IEEE Transactions\non Parallel and Distributed Systems, v. 28, n. 3, p. 787\u2013797, March 2017. ISSN\n1045-9219.\n\nLEE, V. W. et al. Debunking the 100X GPU vs. CPU Myth: An Evaluation of Throughput\nComputing on CPU and GPU. SIGARCH Comput. Archit. News, ACM, New York,\nNY, USA, v. 38, n. 3, jun. 2010. ISSN 0163-5964.\n\nLIU, G. et al. Optimizing thread-to-core mapping on manycore platforms with distributed\ntag directories. In: IEEE. Design Automation Conference (ASP-DAC), 2015 20th Asia\nand South Pacific. [S.l.], 2015. p. 429\u2013434.\n\nhttp://www.intel.com/software/pcm\n\n\n63\n\nMA, L.; AGRAWAL, K.; CHAMBERLAIN, R. D. A memory access model for\nhighly-threaded many-core architectures. Future Generation Computer Systems, v. 30,\np. 202 \u2013 215, 2014. Special Issue on Extreme Scale Parallel Architectures and Systems,\nCryptography in Cloud Computing and Recent Advances in Parallel and Distributed\nSystems, {ICPADS} 2012 Selected Papers.\n\nMARUYAMA, N.; AOKI, T. Optimizing stencil computations for nvidia kepler gpus.\nIn: Proceedings of the 1st International Workshop on High-Performance Stencil\nComputations, Vienna. [S.l.: s.n.], 2014. p. 89\u201395.\n\nMEI, X.; CHU, X. Dissecting GPU memory hierarchy through microbenchmarking.\nCoRR, abs/1509.02308, 2015.\n\nMITTAL, S.; VETTER, J. S. A survey of cpu-gpu heterogeneous computing techniques.\nACM Computing Surveys (CSUR), ACM, v. 47, n. 4, p. 69:1\u201369:35, jul. 2015. ISSN\n0360-0300.\n\nNALAMALPU, A. et al. Broadwell: A family of ia 14nm processors. In: IEEE. VLSI\nCircuits (VLSI Circuits), 2015 Symposium on. [S.l.], 2015. p. C314\u2013C315.\n\nNASCIUTTI, T. C.; PANETTA, J. Impacto da arquitetura de mem\u00f3ria de gpgpus\nna velocidade da computa\u00e7 ao de est\u00eanceis. In: XVII Simp\u00f3sio de Sistemas\nComputacionais (WSCAD-SSC). Aracaju, SE: [s.n.], 2016. p. 1\u20138. ISSN 2358-6613.\n\nSAO, P. et al. A sparse direct solver for distributed memory xeon phi-accelerated\nsystems. In: IEEE International Parallel and Distributed Processing Symposium\n(IPDPS). [S.l.: s.n.], 2015. p. 71\u201381.\n\nSATISH, N. et al. Can traditional programming bridge the ninja performance gap for\nparallel computing applications? In: IEEE. ACM SIGARCH Computer Architecture\nNews. [S.l.], 2012. v. 40, n. 3.\n\nSERPA, M. S. et al. Strategies to improve the performance of a geophysics model for\ndifferent manycore systems. In: IEEE. 2017 International Symposium on Computer\nArchitecture and High Performance Computing Workshops (SBAC-PADW). [S.l.],\n2017. p. 49\u201354.\n\nSERPA, M. S. et al. Optimizing machine learning algorithms on multi-core and many-\ncore architectures using thread and data mapping. In: IEEE. Parallel, Distributed and\nNetwork-based Processing (PDP), 2018 26th Euromicro International Conference\non. [S.l.], 2018. p. 329\u2013333.\n\nSLOTA, G. M.; RAJAMANICKAM, S.; MADDURI, K. High-performance graph\nanalytics on manycore processors. In: IEEE International Parallel and Distributed\nProcessing Symposium (IPDPS). [S.l.: s.n.], 2015. p. 17\u201327. ISSN 1530-2075.\n\nSODANI, A. et al. Knights landing: Second-generation intel xeon phi product. IEEE\nMicro, v. 36, n. 2, 2016.\n\nTOUSIMOJARAD, A.; VANDERBAUWHEDE, W. An efficient thread mapping strategy\nfor multiprogramming on manycore processors. Parallel Computing: Accelerating\nComputational Science and Engineering (CSE), Advances in Parallel Computing,\nv. 25, p. 63\u201371, 2014.\n\n\n\n64\n\nWANG, X. et al. A Fast Tridiagonal Solver for Intel MIC Architecture. In: IEEE\nInternational Parallel and Distributed Processing Symposium (IPDPS). [S.l.: s.n.],\n2016. p. 172\u2013181.\n\nWONG, C. S. et al. Towards achieving fairness in the Linux scheduler. ACM SIGOPS\nOperating Systems Review, v. 42, n. 5, p. 34\u201343, jul 2008.\n\nZIAKAS, D. et al. Intel QuickPath Interconnect - Architectural Features Supporting\nScalable System Architectures. In: Symposium on High Performance Interconnects\n(HOTI). [S.l.: s.n.], 2010. p. 1\u20136.\n\n\n\n65\n\nAPPENDIX A \u2014 RESUMO EM PORTUGU\u00caS\n\nIn this chapter, we present a summary of this master thesis in the portuguese lan-\n\nguage, as required by the PPGC Graduate Program in Computing.\n\nNeste cap\u00edtulo, \u00e9 apresentado um resumo desta disserta\u00e7\u00e3o de mestrado na l\u00edngua\n\nportuguesa, como requerido pelo Programa de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o.\n\nA.1 Introdu\u00e7\u00e3o\n\nA computa\u00e7\u00e3o de alto desempenho (CAD) tem sido respons\u00e1vel por uma grande\n\nrevolu\u00e7\u00e3o cient\u00edfica. Atrav\u00e9s dos computadores, problemas que at\u00e9 ent\u00e3o n\u00e3o podiam ser\n\nresolvidos, ou que demandavam muito tempo para serem solucionados, passaram a estar\n\nao alcance da comunidade cient\u00edfica. A evolu\u00e7\u00e3o das arquiteturas de computadores acar-\n\nretou no aumento do poder computacional, ampliando a gama de problemas que pode-\n\nriam ser tratadas computacionalmente. A introdu\u00e7\u00e3o de circuitos integrados, pipelines,\n\naumento da frequ\u00eancia de opera\u00e7\u00e3o, execu\u00e7\u00e3o fora de ordem e previs\u00e3o de desvios con-\n\nstituem parte importante das tecnologias introduzidas at\u00e9 o final do s\u00e9culo XX. Recente-\n\nmente, tem crescido a preocupa\u00e7\u00e3o com o gasto energ\u00e9tico, com o objetivo de se atingir\n\na computa\u00e7\u00e3o em n\u00edvel exascale de forma sustent\u00e1vel (HSU, 2016). Entretanto, as tec-\n\nnologias at\u00e9 ent\u00e3o mencionadas n\u00e3o possibilitam atingir tal objetivo, devido ao alto custo\n\nenerg\u00e9tico de se aumentar a frequ\u00eancia e est\u00e1gios de pipeline, assim como a chegada nos\n\nlimites de explora\u00e7\u00e3o do paralelismo em n\u00edvel de instru\u00e7\u00e3o (BORKAR; CHIEN, 2011;\n\nCOTEUS et al., 2011).\n\nA fim de se solucionar tais problemas, arquiteturas paralelas e heterog\u00eaneas foram\n\nintroduzidas nos \u00faltimos anos. A principal caracter\u00edstica de arquiteturas paralelas \u00e9 a pre-\n\nsen\u00e7a de v\u00e1rios n\u00facleos de processamento operando concorrentemente, de forma que a\n\naplica\u00e7\u00e3o deve ser programada separando-a em diversas tarefas que se comunicam en-\n\ntre si. Em rela\u00e7\u00e3o \u00e0s arquiteturas heterog\u00eaneas, sua principal caracter\u00edstica \u00e9 a presen\u00e7a\n\nde diferentes arquiteturas em um mesmo sistema, cada um com sua pr\u00f3pria arquitetura\n\nespecializada para um tipo de tarefa. A utiliza\u00e7\u00e3o de aceleradores \u00e9 uma das principais\n\nformas adquiridas por arquiteturas heterog\u00eaneas, no qual um processador gen\u00e9rico \u00e9 re-\n\nspons\u00e1vel principalmente pela ger\u00eancia do sistema, e diversos aceleradores presentes no\n\nsistema realizam a computa\u00e7\u00e3o de determinados tipos de tarefas.\n\nA utiliza\u00e7\u00e3o de arquiteturas paralelas e heterog\u00eaneas imp\u00f5e diversos desafios para\n\n\n\n66\n\nse obter um alto desempenho (MITTAL; VETTER, 2015). As aplica\u00e7\u00f5es precisam ser\n\ncodificadas considerando as particularidades e restri\u00e7\u00f5es de cada arquitetura, assim como\n\nsuas caracter\u00edsticas arquiteturais distintas (GROPP; SNIR, 2013). Por exemplo, na hierar-\n\nquia de mem\u00f3ria, a presen\u00e7a de diversos n\u00edveis de mem\u00f3ria cache, alguns compartilhados\n\ne outros privados, bem como se os bancos de mem\u00f3ria encontram-se centralizados ou\n\ndistribu\u00eddos, introduz tempos de acesso n\u00e3o uniformes, o que gera um grande impacto\n\nno desempenho (CRUZ et al., 2016a). Isso \u00e9 ainda mais cr\u00edtico em arquiteturas het-\n\nerog\u00eaneas, visto que cada acelerador pode possuir sua pr\u00f3pria, e distinta, hierarquia de\n\nmem\u00f3ria. Al\u00e9m disso, nas arquiteturas heterog\u00eaneas, o n\u00famero de unidades funcionais\n\npode variar entre os diferentes aceleradores, sendo que o pr\u00f3prio conjunto de instru\u00e7\u00f5es\n\npode tamb\u00e9m n\u00e3o ser o mesmo. Neste contexto, \u00e9 importante desenvolver t\u00e9cnicas para\n\nan\u00e1lise de desempenho e do comportamento de arquiteturas paralelas e heterog\u00eaneas, a\n\nfim de se propiciar um melhor suporte para otimizar o desempenho de aplica\u00e7\u00f5es.\n\nA.1.1 Contribui\u00e7\u00f5es\n\nO principal objetivo desta pesquisa \u00e9 avaliar arquiteturas multi-core e many-core,\n\ne reduzir os gargalos de desempenho atrav\u00e9s de otimiza\u00e7\u00f5es para c\u00f3digo fonte.\n\nConsiderando os objetivos, as principais contribui\u00e7\u00f5es deste trabalho s\u00e3o:\n\n\u2022 Um conjunto de m\u00e9tricas de desempenho foi analisado sobre aplica\u00e7\u00f5es com carac-\n\nter\u00edsticas distintas de execu\u00e7\u00e3o paralela, com o objetivo de encontrar uma correla\u00e7\u00e3o\n\nentre a m\u00e9trica e o desempenho da aplica\u00e7\u00e3o (IPC).\n\n\u2022 Um conjunto de estrat\u00e9gias de otimiza\u00e7\u00e3o de desempenho foi aplicado, com o obje-\n\ntivo de aumentar o desempenho de uma aplica\u00e7\u00e3o de explora\u00e7\u00e3o s\u00edsmica no mundo\n\nreal. As t\u00e9cnicas empregadas foram: loop interchange para melhorar o uso da\n\nmem\u00f3ria cache; vetoriza\u00e7\u00e3o para aumentar o desempenho de c\u00e1lculos de ponto\n\nflutuante; load balancing e collapse para melhorar o balanceamento de carga; e\n\nmapeamento de threads e dados para melhor usar a hierarquia de mem\u00f3ria.\n\nA.2 Arquiteturas Multi-core e Many-core\n\nDesde 2003, a ind\u00fastria vem seguindo duas abordagens para o projeto de micro-\n\nprocessadores (KIRK; WEN-MEI, 2016). A abordagem multi-core \u00e9 orientada \u00e0 lat\u00eancia,\n\n\n\n67\n\nonde instru\u00e7\u00f5es s\u00e3o executadas em poucos ciclos de clock. Por outro lado, as arquiteturas\n\nmany-core tem uma abordagem focada na vaz\u00e3o, ou seja, um grande n\u00famero de instru\u00e7\u00f5es\n\ns\u00e3o executadas por unidade de tempo.\n\nO projeto das arquiteturas multi-core e many-core \u00e9 diferente ao ponto que, de-\n\npendendo da aplica\u00e7\u00e3o, o desempenho pode ser muito grande em uma arquitetura e muito\n\npequeno na outra (COOK, 2012). A arquitetura multi-core utiliza uma l\u00f3gica de cont-\n\nrole sofisticada para permitir que instru\u00e7\u00f5es de uma \u00fanica thread sejam executadas em\n\nparalelo. Grandes mem\u00f3rias cache s\u00e3o fornecidas para reduzir lat\u00eancias de acesso \u00e0s\n\ninstru\u00e7\u00f5es e dados de aplica\u00e7\u00f5es que tem acesso predominante \u00e0 mem\u00f3ria. Por fim, as\n\nopera\u00e7\u00f5es das Unidades L\u00f3gicas e Aritm\u00e9ticas (ULA) tamb\u00e9m s\u00e3o projetadas visando\n\notimizar a lat\u00eancia.\n\nA arquitetura many-core tira proveito de um grande n\u00famero de threads de exe-\n\ncu\u00e7\u00e3o. Pequenas mem\u00f3rias cache s\u00e3o fornecidas para evitar que m\u00faltiplas threads, aces-\n\nsando os mesmos dados, precisem ir at\u00e9 a mem\u00f3ria principal. Al\u00e9m disso, a maior parte\n\ndo chip \u00e9 dedicada a unidades de ponto flutuante. Arquiteturas desse tipo s\u00e3o projetadas\n\ncomo mecanismos de c\u00e1lculo de ponto flutuante e n\u00e3o para opera\u00e7\u00f5es convencionais,\n\nque s\u00e3o realizadas por arquiteturas multi-core. Algumas aplica\u00e7\u00f5es poder\u00e3o utilizar tanto\n\nmulti-core quanto many-core em conjunto, sendo cada arquitetura melhor para um tipo de\n\nopera\u00e7\u00e3o.\n\nA.2.1 Trabalhos Relacionados\n\nCom a an\u00e1lise dos trabalhos relacionados, foi poss\u00edvel concluir que o conheci-\n\nmento profundo do comportamento da aplica\u00e7\u00e3o no n\u00edvel arquitetural permite desenvolver\n\nt\u00e9cnicas para obter desempenho. Este trabalho vai al\u00e9m da an\u00e1lise e busca uma maior\n\ncompreens\u00e3o do desempenho de diferentes aplica\u00e7\u00f5es em sistemas multi-core e many-\n\ncore. Arquiteturas multi-core e many-core porque seu estilo de programa\u00e7\u00e3o \u00e9 semel-\n\nhante, diferente de uma arquitetura de GPU que tem um estilo de programa\u00e7\u00e3o muito\n\ndiferente e incomum. Os trabalhos relacionados tamb\u00e9m mostram que a maior parte do\n\ntrabalho \u00e9 focada em otimiza\u00e7\u00f5es de mem\u00f3ria, mas v\u00e1rios trabalhos visam a vetoriza\u00e7\u00e3o,\n\nbalanceamento de carga e mapeamento. Desta forma, foram realizadas quatro otimiza\u00e7\u00f5es\n\njuntas em uma aplica\u00e7\u00e3o do mundo real. Os pr\u00f3ximos cap\u00edtulos descrevem as propostas\n\nem detalhes.\n\n\n\n68\n\nA.3 An\u00e1lise dos Gargalos de Desempenho\n\nAtualmente, existem v\u00e1rias arquiteturas diferentes dispon\u00edveis n\u00e3o apenas para a\n\nind\u00fastria, mas tamb\u00e9m para consumidores finais. Processadores multi-core e many-core\n\napresentam caracter\u00edsticas muito diferentes. Essa ampla gama de caracter\u00edsticas repre-\n\nsenta um desafio para os desenvolvedores de aplica\u00e7\u00f5es, porque a mesma aplica\u00e7\u00e3o pode\n\nter um bom desempenho quando executado em uma arquitetura, mas mal em outra ar-\n\nquitetura.\n\nPara explicar melhor a motiva\u00e7\u00e3o, o comportamento de um conjunto de aplica\u00e7\u00f5es\n\nparalelas nessas arquiteturas \u00e9 mostrado. A Figura A.1 mostra a m\u00e9trica IPC (instru\u00e7\u00f5es\n\npor ciclo), que indica o n\u00famero m\u00e9dio de instru\u00e7\u00f5es executadas por ciclo, para 18 apli-\n\nca\u00e7\u00f5es da su\u00edte Rodinia (CHE et al., 2010). Como esperado, o desempenho de cada apli-\n\nca\u00e7\u00e3o depende da arquitetura. Existem tr\u00eas grupos de aplica\u00e7\u00f5es: melhor em Broadwell;\n\nmelhor em Knights Landing; e quase o mesmo desempenho em ambas as arquiteturas.\n\nIsso motiva o estudo de aplica\u00e7\u00f5es e caracter\u00edsticas de arquituras, com o objetivo\n\nde entender por que uma aplica\u00e7\u00e3o funciona melhor em uma arquitetura e como melhorar\n\nseu desempenho. Contadores de desempenho de hardware foram utilizados para coletar\n\nmedidas precisas do impacto real de diferentes fatores que influenciam o desempenho.\n\nAo fazer isso, uma compreens\u00e3o detalhada de como os diferentes aspectos da arquitetura\n\nafetam o desempenho das aplica\u00e7\u00f5es foi obtida. Este estudo serviu de base para a pr\u00f3xima\n\nse\u00e7\u00e3o, onde uma aplica\u00e7\u00e3o de explora\u00e7\u00e3o s\u00edsmica no mundo real foi otimizada.\n\nA.4 Estrat\u00e9gias de Otimiza\u00e7\u00e3o para Multi-core e Many-core\n\nA Se\u00e7\u00e3o A.3 investiga os gargalos de desempenho de arquiteturas multi-core e\n\nmany-core. Os resultados mostraram que um dos aspectos mais importantes \u00e9 o com-\n\nportamento da mem\u00f3ria cache, j\u00e1 que a mem\u00f3ria cache desempenha um papel crucial no\n\ndesempenho. Da mesma forma, a hierarquia de mem\u00f3ria, composta de v\u00e1rias camadas de\n\ncache e controladores de mem\u00f3ria, tem um impacto significativo no desempenho.\n\nCom base nisso, algumas estrat\u00e9gias de otimiza\u00e7\u00e3o de desempenho foram abor-\n\ndadas com o objetivo de reduzir o impacto desses gargalos. Primeiro, a t\u00e9cnica de loop\n\ninterchange foi empregada para melhorar o uso da mem\u00f3ria cache. A seguir, a vetoriza-\n\n\u00e7\u00e3o foi considerada para aumentar o desempenho dos c\u00e1lculos de ponto flutuante. Ap\u00f3s,\n\nload balancing e collapse foram aplicadas para melhorar o balanceamento de carga; Fi-\n\n\n\n69\n\nnalmente, o desempenho da hierarquia de mem\u00f3ria foi aumentado usando o mapeamento\n\nde threads e dados.\n\nPortanto, a fim de validar nossas suposi\u00e7\u00f5es, o desempenho de uma aplica\u00e7\u00e3o de\n\nexplora\u00e7\u00e3o s\u00edsmica real fornecida pela Petrobras foi melhorado. A aplica\u00e7\u00e3o implementa\n\numa aproxima\u00e7\u00e3o de propaga\u00e7\u00e3o de onda ac\u00fastica, que \u00e9 a refer\u00eancia atual para ferra-\n\nmentas de imagens s\u00edsmicas. A aplica\u00e7\u00e3o tem sido amplamente aplicada para gera\u00e7\u00e3o de\n\nimagens de reservat\u00f3rios de petr\u00f3leo e g\u00e1s nos \u00faltimos cinco anos. Esses mecanismos\n\nde propaga\u00e7\u00e3o ac\u00fastica devem ser continuamente portados para o mais novo hardware\n\ndispon\u00edvel para manter a competitividade.\n\nA.4.1 Padr\u00e3o de Acesso \u00e0 Mem\u00f3ria para Melhorar a Localidade dos Dados\n\nAs arquiteturas de computador atuais fornecem caches e prefetchers de hardware\n\npara ajudar os programadores a gerenciar dados implicitamente (LEE et al., 2010). A\n\nt\u00e9cnica de loop interchange pode ser usada para melhorar o desempenho de ambos os el-\n\nementos, trocando a ordem de dois ou mais la\u00e7os. Essa t\u00e9cnica tamb\u00e9m reduz os conflitos\n\ndo banco de mem\u00f3ria, melhora a localidade dos dados e ajuda a reduzir o fluxo de uma\n\ncomputa\u00e7\u00e3o de matriz. Dessa forma, mais dados que s\u00e3o buscados para as mem\u00f3rias de\n\ncache s\u00e3o efetivamente acessados, a reutiliza\u00e7\u00e3o de dados nas caches \u00e9 aumentada, e os\n\nprefetchers de linha de cache s\u00e3o capazes de buscar dados da mem\u00f3ria principal com mais\n\nprecis\u00e3o. Nessa aplica\u00e7\u00e3o, temos tr\u00eas la\u00e7os que s\u00e3o usados para calcular o est\u00eancil. Eles\n\npodem ser executados em qualquer ordem sem alterar os resultados. A sequ\u00eancia de la\u00e7o\n\npadr\u00e3o \u00e9 xyz.\n\nA sequ\u00eancia do la\u00e7o foi alterada de xyz para todas as permuta\u00e7\u00f5es poss\u00edveis. O\n\nla\u00e7o mais externo \u00e9 aquele que foi paralelizado usando threads. A sequ\u00eancia de la\u00e7o zyx\n\ntem um desempenho melhor e resultados de taxa de acertos combinada no Broadwell.\n\nO aumento de desempenho comparado com a vers\u00e3o xyz \u00e9 de 5,3 \u00d7. Essa sequ\u00eancia \u00e9\n\nmelhor que outras porque os dados s\u00e3o acessados de uma maneira que se beneficia mais\n\ndas caches. A taxa de acertos da cache L2 foi melhorada de 14,9% para 77% quando\n\na sequ\u00eancia do la\u00e7o foi alterada para zyx. O impacto do cache L3 tamb\u00e9m melhorou\n\nde 76,2% para 92% no Broadwell. No entanto, esse n\u00e3o foi o caso do cache L1, j\u00e1\n\nque sua taxa de acertos diminui de 82,3% para 73%. Na Knights Landing, a vers\u00e3o yzx\n\napresenta melhor desempenho e de taxa de acertos combinada. Os ganhos de desempenho\n\nforam de at\u00e9 3,9\u00d7, mostrando que essa otimiza\u00e7\u00e3o impacta menos no desempenho da\n\n\n\n70\n\narquitetura Knights Landing do que da Broadwell. A taxa de acertos na cache L2 foi\n\nmelhorada de 9,6% para 95,9%. Embora a taxa de acertos da cache L1 diminua em\n\nambas as arquiteturas, isso mostra que a melhor op\u00e7\u00e3o visando o desempenho \u00e9 aumentar\n\nas taxas de acerto da cache de \u00faltimo n\u00edvel, mesmo quando a taxa de acertos do cache de\n\nqualquer outro n\u00edvel diminui.\n\nA.4.2 Explorando SIMD para Computa\u00e7\u00e3o de Ponto Flutuante\n\nAbordagens recentes de hardware aumentam o desempenho integrando mais n\u00fa-\n\ncleos com unidades mais amplas de SIMD (\u00fanica instru\u00e7\u00e3o, m\u00faltiplos dados) (SATISH\n\net al., 2012). Essa t\u00e9cnica de processamento de dados, denominada vetoriza\u00e7\u00e3o, possui\n\nunidades que executam, em uma \u00fanica instru\u00e7\u00e3o, a mesma opera\u00e7\u00e3o em v\u00e1rios operan-\n\ndos. Para maximizar a efic\u00e1cia da vetoriza\u00e7\u00e3o, os endere\u00e7os de mem\u00f3ria acessados pela\n\nmesma instru\u00e7\u00e3o, em itera\u00e7\u00f5es de la\u00e7o consecutivas, tamb\u00e9m devem ser consecutivos.\n\nDessa forma, o compilador pode carregar e armazenar os operandos de itera\u00e7\u00f5es consec-\n\nutivas usando uma \u00fanica instru\u00e7\u00e3o load / store, otimizando o uso da mem\u00f3ria cache, j\u00e1\n\nque os dados j\u00e1 s\u00e3o buscados em blocos da mem\u00f3ria principal. Os processadores mais\n\nrecentes introduzem o suporte para as instru\u00e7\u00f5es gather e scatter, que reduzem a\n\nsobrecarga de carregar / armazenar endere\u00e7os de mem\u00f3ria n\u00e3o consecutivos. No entanto,\n\no desempenho ainda \u00e9 muito maior quando os endere\u00e7os s\u00e3o consecutivos. Nesse con-\n\ntexto, o c\u00f3digo-fonte foi modificado de modo que os endere\u00e7os de mem\u00f3ria acessados\n\npela mesma instru\u00e7\u00e3o fossem consecutivos ao longo das itera\u00e7\u00f5es do la\u00e7o.\n\nAs instru\u00e7\u00f5es AVX (Advanced Vector Extensions), que \u00e9 uma extens\u00e3o de arquite-\n\ntura de conjunto de instru\u00e7\u00f5es para usar unidades SIMD para aumentar o desempenho dos\n\nc\u00e1lculos de ponto flutuante foi utilizada. Essas instru\u00e7\u00f5es usam unidades espec\u00edficas de\n\nponto flutuante que podem carregar, armazenar ou executar c\u00e1lculos em v\u00e1rios operandos\n\nde uma s\u00f3 vez. Como descrito anteriormente, a efici\u00eancia do AVX \u00e9 melhor quando os\n\nelementos s\u00e3o acessados na mem\u00f3ria de forma cont\u00edgua, pois podem ser carregados e ar-\n\nmazenados em blocos. O aumento de velocidade mostrado \u00e9 relativo \u00e0 sequ\u00eancia do la\u00e7o\n\nsem o AVX. As sequ\u00eancias yzx e zyx t\u00eam melhores resultados porque t\u00eam mais elementos\n\nsendo acessados de forma cont\u00edgua. O ganho de desempenho difere de arquitetura para a\n\narquitetura. Na Broadwell, a melhoria foi de at\u00e9 1,4\u00d7. Na arquitetura Knights Landing, a\n\nmelhoria foi de at\u00e9 6,5 \u00d7. Essas diferen\u00e7as se devem ao tamanho da unidade de vetor de\n\ncada arquitetura e ao n\u00famero de n\u00facleos usados.\n\n\n\n71\n\nA.4.3 Melhorando o Balanceamento de Carga\n\nAlgumas aplica\u00e7\u00f5es t\u00eam regi\u00f5es com diferentes requisitos de carga de computa\u00e7\u00e3o,\n\npor exemplo contornos, potencialmente causando irregularidade no tempo de computa\u00e7\u00e3o\n\nentre as threads. O tempo para executar um aplica\u00e7\u00e3o paralela \u00e9 determinada pela tarefa\n\nque leva mais tempo para ser conclu\u00edda e, portanto, pelo n\u00facleo com a maior quantidade\n\nde trabalho. Portanto, ao distribuir o trabalho de maneira mais uniforme entre os n\u00facleos,\n\no tempo de execu\u00e7\u00e3o da aplica\u00e7\u00e3o \u00e9 reduzido. As t\u00e9cnicas de balanceamento de carga\n\nreduzem essas disparidades e, portanto, melhoram o uso e o desempenho dos recursos.\n\nNo contexto de sistemas com multi-core e many-core, o balanceamento de carga \u00e9 ainda\n\nmais importante devido a um grande n\u00famero de n\u00facleos.\n\nA especifica\u00e7\u00e3o OpenMP possui uma diretiva para indicar se o escalonamento \u00e9\n\nest\u00e1tico, din\u00e2mico ou guiado. O escalonamento est\u00e1tico \u00e9 o padr\u00e3o, e atribui itera\u00e7\u00f5es as\n\nthreads seguindo um algoritmo round-robin antes do in\u00edcio do c\u00e1lculo. As abordagens\n\ndin\u00e2micas e guiadas distribuem o trabalho durante o tempo de execu\u00e7\u00e3o de acordo com\n\nas solicita\u00e7\u00f5es das threads, mas, no din\u00e2mico, todos os blocos t\u00eam o mesmo tamanho,\n\nenquanto no guiado, os primeiros blocos s\u00e3o maiores e seu tamanho diminui ao longo\n\ndas itera\u00e7\u00f5es. Al\u00e9m disso, o loop collapse tamb\u00e9m ajuda a melhorar o balanceamento de\n\ncarga, pois aumenta o n\u00famero total de itera\u00e7\u00f5es particionadas nas threads ao reduzir dois\n\nou mais la\u00e7os.\n\no impacto de diferentes pol\u00edticas de escalonamento do OpenMP com e sem o loop\n\ncollapse foi investigado. Nos experimentos sem collapse, o la\u00e7o externo foi dividido\n\nentre as threads. A melhor acelera\u00e7\u00e3o sem collapse no Broadwell estava usando a pol\u00edtica\n\nguiada. Foi at\u00e9 1,06\u00d7 mais r\u00e1pido que o padr\u00e3o. Essa abordagem \u00e9 \u00fatil para aplica\u00e7\u00f5es\n\ncuja carga de trabalho muda em tempo de execu\u00e7\u00e3o ou possui algumas regi\u00f5es com cargas\n\nde trabalho desequilibradas. No Knights Landing, o melhor desempenho foi o uso do\n\nescalonamento est\u00e1tico com um aumento de velocidade de 1,01\u00d7.\n\nUma maneira de melhorar o desempenho com trechos maiores \u00e9 com collapse. A\n\nideia \u00e9 que, com mais trabalho a ser dividido entre as threads, peda\u00e7os maiores podem\n\nser usados, mantendo um bom balanceamento de carga. A fim de investigar isso, os\n\ndois la\u00e7os mais externos foram agregados e diferentes combina\u00e7\u00f5es do escalonador foram\n\navaliadas. Os resultados mostram que, para o Broadwell, a melhor pol\u00edtica \u00e9 o escalonador\n\nest\u00e1tico. No Knights Landing, a melhor pol\u00edtica com collapse \u00e9 a guiada. O desempenho\n\nno Broadwell foi melhorado em at\u00e9 1,04\u00d7, enquanto no Knights Landing em at\u00e9 1,11\u00d7.\n\n\n\n72\n\nA.4.4 Otimizando a Afinidade de Mem\u00f3ria\n\nO objetivo dos mecanismos de mapeamento \u00e9 melhorar o uso de recursos, orga-\n\nnizando threads e dados de acordo com uma pol\u00edtica fixa, onde cada abordagem pode\n\ndirecionar diferentes aspectos para melhorar. Por exemplo, existem t\u00e9cnicas focadas em\n\nmelhorar localidade, para reduzir os erros de cache e os acessos \u00e0 mem\u00f3ria remota, bem\n\ncomo o tr\u00e1fego nas interconex\u00f5es inter-chips (CRUZ et al., 2016). Outras pol\u00edticas bus-\n\ncam uma distribui\u00e7\u00e3o de carga uniforme entre os n\u00facleos e controladores de mem\u00f3ria.\n\nNa arquitetura de Broadwell, o melhor ganho de desempenho foi de 1,6\u00d7, al-\n\ncan\u00e7ada para t\u00e9cnica round-robin com interleave para a vers\u00e3o xyz. Ele combina uma\n\ndistribui\u00e7\u00e3o de threads uniforme com uma distribui\u00e7\u00e3o equilibrada das p\u00e1ginas. O mapea-\n\nmento de threads e de dados foi combinado porque, na maioria das aplica\u00e7\u00f5es, a efic\u00e1cia\n\ndo mapeamento de dados depende do mapeamento de threads.\n\nNa Knights Landing, as t\u00e9cnicas avalidas foram as mesmas que na Broadwell\n\nmais interleave usando MCDRAM. O melhor ganho foi 4,4\u00d7 para scatter com interleave\n\nMCDRAM para a vers\u00e3o xyz. Podemos observar que, na Knights Landing, as melhores\n\nmelhorias geralmente acontecem com pol\u00edticas que se concentram no balanceamento de\n\ncarga, como o mapeamento de threads de scatter e o mapeamento de dados de interleave.\n\nA.5 Conclus\u00e3o e Trabalhos Futuros\n\nEste trabalho realiza uma an\u00e1lise detalhada dos gargalos de desempenho em multi-\n\ncore e many-core e otimiza seu desempenho usando diferentes estrat\u00e9gias. Avaliamos 18\n\naplica\u00e7\u00f5es e usamos contadores de desempenho de hardware para coletar medi\u00e7\u00f5es pre-\n\ncisas do impacto real de diferentes fatores que influenciam o desempenho. Essa avalia\u00e7\u00e3o\n\nmostrou que algumas m\u00e9tricas ajudam a entender o desempenho das aplica\u00e7\u00f5es, mas h\u00e1\n\ncasos em que uma m\u00e9trica sozinha n\u00e3o \u00e9 representativa.\n\nUsamos nosso estudo de gargalos de desempenho como base para otimizar um\n\nmodelo de geof\u00edsica do mundo real. Aplicamos as seguintes t\u00e9cnicas de otimiza\u00e7\u00e3o: (1)\n\nloop interchange para melhorar o uso da mem\u00f3ria cache; (2) vetoriza\u00e7\u00e3o para aumentar o\n\ndesempenho dos c\u00e1lculos de ponto flutuante; (3) load balancing e collapse para melhorar\n\no balanceamento de carga; e (4) mapeamento de threads e dados para melhor usar a hier-\n\narquia de mem\u00f3ria. Essas otimiza\u00e7\u00f5es tamb\u00e9m podem ser aplicadas a outras aplica\u00e7\u00f5es e\n\narquiteturas.\n\n\n\n73\n\nEm nossos experimentos de otimiza\u00e7\u00e3o de desempenho, mostramos que loop in-\n\nterchange \u00e9 uma t\u00e9cnica \u00fatil para melhorar o desempenho de diferentes n\u00edveis de mem\u00f3ria\n\ncache, sendo capaz de melhorar o desempenho em at\u00e9 5.3\u00d7 e 3.9\u00d7 na Broadwell e\n\nKnights Landing, respectivamente. Essas melhorias ocorreram porque a taxa de acer-\n\ntos da cache de \u00faltimo n\u00edvel foi aumentada em at\u00e9 95,9%. Al\u00e9m disso, alterando o c\u00f3digo\n\nde modo que os elementos sejam acessados de forma cont\u00edgua entre as itera\u00e7\u00f5es do la\u00e7o,\n\no c\u00f3digo foi vetorizado, o que melhorou o desempenho em at\u00e9 1.4\u00d7 e 6.5\u00d7. As t\u00e9cnicas\n\nde balanceamento de carga e de collapse tamb\u00e9m foram avaliadas, mas o balanceamento\n\nda aplica\u00e7\u00e3o atenuou as melhorias de desempenho. Essas t\u00e9cnicas melhoraram o desem-\n\npenho do Knights Landing em at\u00e9 1,1\u00d7. As t\u00e9cnicas de mapeamento de threads e dados\n\ntamb\u00e9m foram avaliadas, com uma melhoria de desempenho de at\u00e9 1,6\u00d7 e 4,4\u00d7. O ganho\n\nde desempenho do Broadwell foi de 22,7\u00d7 e do Knights Landing de 56,7\u00d7 em compara-\n\n\u00e7\u00e3o com uma vers\u00e3o sem otimiza\u00e7\u00f5es, mas, no final, o Broadwell foi 1,2\u00d7 mais r\u00e1pido\n\nque o Knights Landing.\n\nA.5.1 Trabalhos Futuros\n\nOs trabalhos futuros se concentrar\u00e3o em propor mecanismos autom\u00e1ticos para\n\notimizar o desempenho de arquiteturas multi-core e many-core. Al\u00e9m disso, a avalia\u00e7\u00e3o\n\ndos gargalos de desempenho ser\u00e1 expandida usando arquiteturas mais recentes e avaliando\n\no consumo de energia.\n\n\n\n74\n\nFigure A.1: Desempenho de diferentes arquiteturas (maior IPC \u00e9 melhor).\n\nK\nM S\nC\n\nB\nP\n\nS\nR\n\nA\nD\n\nM\nD\n\nN\nW\n\nLU\nD\n\nM\nS\n\nH\nS\n\n2D\n\nC\nF\n\nD\n\nK\nN\n\nN\n\nPA\nT\n\nHLC\n\nH\nS\n\n3DH\nW\n\nB\nF\n\nS\nB\n\n+\n\nP\nF\n\n0\n\n0.5\n\n1\n\n1.5\n\nBenchmarks\n\nIn\nst\n\nru\n\u00e7\u00f5\n\nes\npo\n\nr\nci\n\ncl\no\n\n(I\nP\n\nC\n) Broadwell Knights Landing\n\nFonte: O Autor.\n\n\n\tAgradecimentos\n\tAbstract\n\tResumo\n\tList of Abbreviations and Acronyms\n\tList of Symbols\n\tList of Figures\n\tList of Tables\n\tContents\n\t1 Introduction\n\t1.1 Contributions of this research\n\t1.2 Document Organization\n\n\t2 Multi-core and Many-core: Overview and Related Work\n\t2.1 Multi-core and Many-core Architectures\n\t2.1.1 The Broadwell Architecture\n\t2.1.2 The Knights Landing Architecture\n\n\t2.2 Related Work\n\t2.2.1 Memory Optimization\n\t2.2.2 Vectorization\n\t2.2.3 Load Balancing\n\t2.2.4 Thread and Data Mapping\n\t2.2.5 Combined Different Properties\n\n\t2.3 Summary\n\n\t3 Analysis of Performance Bottlenecks\n\t3.1 Methodology\n\t3.1.1 Workloads\n\n\t3.2 Results on Broadwell\n\t3.3 Results on Knights Landing\n\t3.4 Conclusions\n\n\t4 Optimization Strategies for Multi-core and Many-core\n\t4.1 Seismic Exploration Application\n\t4.2 Results\n\t4.2.1 Memory Access Pattern to Improve Data Locality\n\t4.2.2 Exploiting SIMD for Floating-point Computations\n\t4.2.3 Improving Load Balancing\n\t4.2.4 Optimizing Memory Affinity\n\n\t4.3 Conclusions\n\n\t5 Conclusion and Future Work\n\t5.1 Future Work\n\t5.2 Publications\n\n\tReferences\n\tAppendix A \u2014 Resumo em Portugu\u00eas\n\tA.1 Introdu\u00e7\u00e3o\n\tA.1.1 Contribui\u00e7\u00f5es\n\n\tA.2 Arquiteturas Multi-core e Many-core\n\tA.2.1 Trabalhos Relacionados\n\n\tA.3 An\u00e1lise dos Gargalos de Desempenho\n\tA.4 Estrat\u00e9gias de Otimiza\u00e7\u00e3o para Multi-core e Many-core\n\tA.4.1 Padr\u00e3o de Acesso \u00e0 Mem\u00f3ria para Melhorar a Localidade dos Dados\n\tA.4.2 Explorando SIMD para Computa\u00e7\u00e3o de Ponto Flutuante\n\tA.4.3 Melhorando o Balanceamento de Carga\n\tA.4.4 Otimizando a Afinidade de Mem\u00f3ria\n\n\tA.5 Conclus\u00e3o e Trabalhos Futuros\n\tA.5.1 Trabalhos Futuros"}]}}}