{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.13506"}, {"@name": "filename", "#text": "19510_ulfc120705_tm_Joana_Estevens.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "2016 \n\nUNIVERSIDADE DE LISBOA \n\nFACULDADE DE CI\u00caNCIAS \n\nDEPARTAMENTO DE MATEM\u00c1TICA \n\n \n\nINSTITUTO SUPERIOR DE CI\u00caNCIAS DO TRABALHO E DA EMPRESA \n\nDEPARTAMENTO DE FINAN\u00c7AS \n\n \n\n \n\n \n\n \n\n \n\n \n\nStochastic Modelling of Non-Stationary Financial Assets \n\n \n\n \n\n \n\n \n\nJoana Ribeiro Estevens \n\n \n\n \n\n \n\nMestrado em Matem\u00e1tica Financeira   \n\n  \n\n \n\n \n\nDisserta\u00e7\u00e3o orientada por: \n\nProfessor Doutor Jo\u00e3o Pedro Silva Brito Boto \n\nDoutor Pedro Gon\u00e7alves Lind \n\n \n\n\n\n\n\nAcknowledgements\n\nI would like to thank for the amazing opportunity of finishing my master\u2019s dissertation in\n\nGermany under the programme Erasmus+ sponsored by the European Union. This research\n\nwas partially supported by the grant they gave me.\n\nI would also like to express my sincere gratitude to my colleagues from the department of\n\nStatistical-Physics from the University of Osnabru?ck who gave me a warm welcome in Germany\n\nand provided me with useful comments and remarks about my work. A special thanks to\n\nProfessor Philipp Maass not only for his insights but also for the hard questions that allowed be\n\nto expand my work.\n\nThis dissertation could not have been completed without the guidance, support and encour-\n\nagement that I received from my advisors, Professor Joa?o Boto and Doctor Pedro Lind. A very\n\nspecial and heartfelt thank you for both of them who always kept the office door open and were\n\nalways available whenever I needed it.\n\nI thank my fellow colleague Paulo Rocha for the useful discussions, the data sharing and for\n\nhis infinite patient in helping me overcome my difficulties during this period.\n\nFinally, I would like to thank my family and friends who were always there when I needed.\n\nTo my parents that supported me in each and every imaginable way throughout my entire life.\n\nTo Cristiana who has always given me the moral support that I needed. To my partner in life,\n\nmy everyday support, Gonc?alo, I thank you for giving me more than I could have asked for.\n\ni\n\n\n\nii\n\n\n\nAbstract\n\nIn this dissertation, we propose a framework to model the evolution of the non-stationary\n\ntime series of the volume-price from 2000 companies from the New York Stock Exchange (NYSE).\n\nIt was shown that for each 10 minutes window, the distribution that best fits our data is the\n\nlog-normal distribution. Since the volume-price series is non-stationary, we treat the parameters\n\n? and ? of the log-normal distribution as stochastic variables. We assume that all the time\n\ndependency of the volume-price series is included in the parameters, ?(t) and ?(t). Therefore,\n\nby describing the evolution of ? and ? we are able to model the volume-price series.\n\nOur analysis decomposes the parameters evolution into average daily patterns, ? and ?, and\n\nfluctuations around these patterns, ?? and ??.\n\nThe daily patterns ? and ? are easily modelled using cubic curves. The fluctuations ?? and ??\n\nare modelled using Langevin equations. We show that both parameters fluctuations, ?? and ??,\n\nare weakly correlated, which leads us to consider two different models. One, where we assume\n\nthat the variables are not correlated and therefore yields two independent Langevin equations for\n\nour data. And another, two-dimensioned, where we take into account the correlation between the\n\ntwo variables ?? and ?? which leads to the derivation of a system of coupled Langevin equations.\n\nWe used the first approach because most of the times, the simplest models are the ones used in\n\nthe real world applications. However, we know that the variables are indeed correlated. Thus,\n\nwe also followed the other approach where we took into account this correlation since we expect\n\nit to yield better results.\n\nFinally, we use the system of Langevin equations and the cubic curves describing the daily\n\npatterns to reconstruct two synthetic time series statistically reproducing the evolution of the\n\nparameters ? and ?, and show that they unequivocally determine the expected value of each\n\nstatistical moment of the log-normal distribution of the volume-price. By comparing the prob-\n\nability density function of these expected values using the ? and ? time series obtained from\n\nour model versus the empirical ones, we arrive to the conclusion that our model describes well\n\nthe first moments of the volume-price time series. This framework proposed by us is general\n\nenough to be applied to other fields of study where non-stationary stochastic variables need to\n\nbe modelled, like biology, medicine, geology, among others.\n\nThe data from the NYSE was collected directly from Yahoo Finance between January 2011\n\nand April 2014, with a sampling frequency of 10 minutes, which give us a total of 976 days.\n\nKeywords: Stochastic Differential Equations, Non-Stationarity, Time Series, Brownian Motion.\n\niii\n\n\n\niv\n\n\n\nResumo\n\nNesta dissertac?a?o vamos propor um modelo que nos permite estudar a se?rie temporal na?o\n\nestaciona?ria do volume-prec?o de 2000 empresas cotadas no New York Stock Exchange. Estes\n\ndados foram recolhidos diretamente do Yahoo Finance entre 27/01/2011 e 06/04/2014, com uma\n\nfreque?ncia de amostragem de 10 minutos. O volume-prec?o num dado instante corresponde ao\n\nproduto entre o prec?o de determinada ac?a?o pelo volume transacionado da mesma. Esta e? uma\n\nvaria?vel muito importante na matema?tica financeira pois incorpora a interac?a?o existente entre\n\no volume e o prec?o de uma ac?a?o. Por exemplo, volumes altos te?m tende?ncia a originar prec?os\n\naltos, enquanto volumes baixos esta?o habitualmente associados a prec?os mais baixos. Para ale?m\n\ndisso, quando uma pessoa decide comprar/vender uma ac?a?o, o prec?o da mesma na?o e? a u?nica\n\nvaria?vel importante. Tambe?m e? necessa?rio ter em conta o volume, visto que este esta? associado\n\na? liquidez do t??tulo, isto e?, ao qua?o fa?cil ou dif??cil e? comprar/vender a ac?a?o. Para ale?m disso,\n\na distribuic?a?o do volume-prec?o da?-nos informac?a?o sobre a quantidade de capital que esta? a ser\n\ntransacionada no mercado.\n\nApesar da se?rie temporal do volume-prec?o ser na?o estaciona?ria, esta segue uma forma fun-\n\ncional constante ao longo do tempo. Ja? foi mostrado em trabalhos anteriores [1] que, para cada\n\njanela temporal de 10 minutos, o modelo que melhor explica o volume-prec?o das 2000 empre-\n\nsas da bolsa de Nova Iorque e? uma log-normal com para?metros ? e ?. A me?dia do logaritmo\n\ndo volume-prec?o e? representada pelo para?metro ? enquanto o desvio-padra?o do logaritmo do\n\nvolume-prec?o e? representado pelo para?metro ?.\n\nE? sabido da estat??stica cla?ssica que e? poss??vel chegar a uma fo?rmula fechada para o n-\n\ne?simo momento de uma log-normal e que, sabendo a expressa?o de todos os momentos de uma\n\ndistribuic?a?o, podemos chegar a? sua func?a?o densidade de probabilidade usando transformadas\n\nde Fourier. Portanto, o nosso objetivo de estudar a se?rie na?o estaciona?ria do volume-prec?o\n\nresume-se a estudar as se?ries estaciona?rias dos para?metros ? e ?.\n\nA nossa ana?lise decompo?e a evoluc?a?o dos para?metros ? e ? na soma dos seus padro?es me?dios\n\ndia?rios, ? e ?, com as flutuac?o?es em torno destes para?metros, ?? e ??. Vamos modelar estas duas\n\npartes separadamente e utilizando abordagens diferentes.\n\nModelar os padro?es dia?rios ? e ? e? um procedimento simples, visto que ambos se podem\n\ndescrever por uma func?a?o cu?bica. A modelac?a?o das flutuac?o?es ja? tem que ser mais cuidadosa\n\npois estamos perante se?ries temporais com um comportamento fortemente estoca?stico. Como\n\ntal, vamos modelar estas se?ries atrave?s de equac?o?es diferenciais estoca?sticas, tambe?m conhecidas\n\ncomo equac?o?es de Langevin. E? importante referir que no?s dividimos as se?ries temporais ? e ? em\n\nduas partes porque na?o e? poss??vel modelar dados que contenham algum tipo de periodicidade\n\natrave?s de equac?o?es de Langevin. Portanto, tivemos que retirar a parte perio?dica dos nossos\n\ndados. Utiliza?mos a transformada ra?pida de Fourier para nos certificarmos de que na?o persistia\n\nnenhum tipo de periodicidade nas nossas se?ries temporais das flutuac?o?es.\n\nv\n\n\n\nComec?a?mos por modelar as flutuac?o?es assumindo que as se?ries ?? e ?? sa?o independentes uma\n\nda outra. Apesar de isto na?o se verificar na realidade, o coeficiente de correlac?a?o entre as se?ries\n\ne? muito baixo pelo que podemos supor, para um modelo mais simples, que e? aproximadamente\n\nzero. Para ale?m disso, na maior parte das situac?o?es da vida real, os modelos mais simples sa?o\n\nprefer??veis face aos mais complexos, visto que e? mais fa?cil implementa?-los e interpretar os seus\n\nresultados. Como tal, no?s extra??mos dos nossos dados os coeficientes que regem as duas equac?o?es\n\nde Langevin que modelam as flutuac?o?es dos nossos dados. Em ambos os casos, o coeficiente da\n\nparte determin??stica da equac?a?o, D(1), corresponde a uma func?a?o linear da varia?vel em estudo,\n\nenquanto o coeficiente da parte estoca?stica, D(2), e? uma func?a?o quadra?tica.\n\nApo?s esta primeira abordagem com um modelo mais simples, passa?mos para um modelo que\n\nincorpora a interac?a?o entre ?? e ??. Vamos descrever estas flutuac?o?es atrave?s de um sistema\n\nde duas equac?o?es Langevin acopladas. Agora o coeficiente da parte determin??stica, D(1), e? na\n\nverdade um vetor de duas func?o?es enquanto o coeficiente da parte estoca?stica, D(2), e? uma\n\nmatriz sime?trica com tre?s func?o?es distintas.\n\nVisto que temos um sistema de equac?o?es que nos permite descrever a evoluc?a?o de ?? e ??,\n\npodemos construir as nossas pro?prias se?ries temporais das flutuac?o?es. Somando estas se?ries\n\nteo?ricas ao padra?o me?dio obtido pelas func?o?es cu?bicas, ficamos com se?ries temporais teo?ricas\n\npara o ? e para o ? . Por outras palavras, estes sa?o os valores de ? e ? que o nosso modelo\n\npreve?. Compara?mos estes valores teo?ricos das se?ries do ? e do ? com os valores emp??ricos que\n\nja? t??nhamos. Chega?mos a? conclusa?o de que o nosso modelo e? capaz de explicar muito bem a\n\nevoluc?a?o da se?rie temporal do ?, visto que a densidade da se?rie teo?rica e da se?rie emp??rica sa?o\n\nquase coincidentes. Contudo, a se?rie do ? ja? na?o e? ta?o bem explicada pelo nosso modelo. Isto\n\nja? era esperado visto que, como o ? e? um momento de primeira ordem, enta?o e? mais facilmente\n\nmodelado do que um momento de segunda ordem. Notar que o ? e? a raiz quadrada do segundo\n\nmomento centrado.\n\nComo ja? vimos anteriormente, e? poss??vel chegar a? fo?rmula fechada para o o n-e?simo momento\n\nda distribuic?a?o log-normal que depende apenas de ? e ?. Apo?s termos esta fo?rmula, podemos\n\nsubstituir os valores de ? e ? pelos que obtivemos atrave?s do nosso modelo e obtemos a se?rie\n\ntemporal do n-e?simo momento teo?rico. No?s calcula?mos as se?ries dos primeiros quatro momentos\n\nteo?ricos. Para percebermos se os nossos resultados estavam de acordo com a realidade, fizemos\n\num processo semelhante para as se?ries do ? e do ? emp??ricas. Finalmente, compara?mos as\n\nfunc?o?es densidade de probabilidade de ambas as se?ries e percebemos que o nosso modelo descreve\n\nmuito bem os primeiros momentos da se?rie do volume-prec?o. Mais uma vez, isto tambe?m ja? era\n\nesperado visto que os momentos de ordens superiores esta?o mais dependentes do ? do que aqueles\n\nde ordens inferiores.\n\nHa? inu?meros modelos na literatura que nos permitem estudar se?ries temporais. Uma per-\n\ngunta pertinente seria o porque? de termos escolhido esta ana?lise de Langevin em detrimento dos\n\noutros modelos. Uma particularidade muito interessante sobre o nosso modelo e? que ele na?o\n\nnos permite apenas descrever a evoluc?a?o temporal da se?rie do volume-prec?o. Para ale?m disso,\n\ntambe?m podemos chegar a uma equac?a?o a?s derivadas parciais, de tipo Fokker-Plank, que nos da?\n\na evoluc?a?o da func?a?o densidade de probabilidade do volume-prec?o. A deduc?a?o desta equac?a?o na?o\n\nfoi efetuada no a?mbito desta tese, mas e? um excelente ponto para ser desenvolvido em trabalhos\n\nfuturos.\n\nPara terminar, o modelo que no?s propusemos nesta tese pode ser aplicado a? bolsa de valores\n\nde Nova Iorque para calcular medidas de risco como o Value at Risk. Uma das questo?es que\n\nvi\n\n\n\nse levantaram a partir da realizac?a?o deste trabalho e? se este modelo e? adequado para fazer\n\npreviso?es sobre a evoluc?a?o do volume-prec?o. Este seria um bom ponto de partida para trabalhos\n\nfuturos. Para ale?m disso, este modelo e? suficientemente geral para poder ser aplicado a se?ries\n\ntemporais de outras a?reas cient??ficas, como por exemplo no estudo da variabilidade card??aca na\n\nfisiologia ou no estudo de se?ries s??smicas na geologia. Este trabalho proporcionou-nos uma visa?o\n\nbastante aprofundada do estudo das se?ries tempora?rias na?o estaciona?rias e permitiu-nos propor\n\numa metodologia que sera? bastante u?til em va?rias a?reas.\n\nPalavras-Chave: Equac?o?es Diferenciais Estoca?sticas, Na?o Estacionariedade, Se?ries Temporais,\n\nMovimento Browniano.\n\nvii\n\n\n\nviii\n\n\n\nContents\n\n1 Introduction 1\n\n2 State of the art 5\n\n2.1 Stationary Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n2.2 Brownian Motion and White Noise . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n2.3 Markov Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n2.4 Stochastic Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n2.5 The Fokker-Planck Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n\n2.6 The Black-Scholes Model and its limitations . . . . . . . . . . . . . . . . . . . . . 18\n\n2.7 From Stochastic Volatility to Superstatistics . . . . . . . . . . . . . . . . . . . . . 19\n\n2.8 The Langevin Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n2.9 Statistical Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n3 Getting to know the data 23\n\n3.1 Outliers and Daily Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n3.2 Log-Normal Parameter Fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n3.3 Markov Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n\n4 The Langevin Analysis 31\n\n4.1 A Simple Model without Correlation . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\n4.2 Modelling the Coupling between ?? and ?? . . . . . . . . . . . . . . . . . . . . . . 33\n\n5 Approaching Non-Stationarity 39\n\n6 Discussion and Conclusions 43\n\nix\n\n\n\nx\n\n\n\nList of Figures\n\n1.1 Structure of the thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n\n3.1 Time series without outliers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n3.2 Daily Patterns from ? and ? time series . . . . . . . . . . . . . . . . . . . . . . . 24\n\n3.3 Flucuations time series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n3.4 Power spectrum and autocorrelation function . . . . . . . . . . . . . . . . . . . . 27\n\n3.5 Marginal PDF of the fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n3.6 Joint PDF and contour plot of the fluctuations . . . . . . . . . . . . . . . . . . . 29\n\n3.7 Wilcoxon test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n\n4.1 Computation of the D(1) and D(2) functions . . . . . . . . . . . . . . . . . . . . . 31\n\n4.2 D(1) and D(2) functions assuming independency . . . . . . . . . . . . . . . . . . . 32\n\n4.3 D(4) function assuming independency . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n4.4 D(1) and D(2) functions with correlation . . . . . . . . . . . . . . . . . . . . . . . 34\n\n4.5 g functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n\n5.1 Empirical and modelled series of the moments ?sn? . . . . . . . . . . . . . . . . 40\n5.2 PDF for the modelled and empirical fluctuations and moments ?sn? . . . . . . . 41\n\nxi\n\n\n\nxii\n\n\n\nList of Tables\n\n4.1 Coefficients for the D(1) and D(2) functions with correlation . . . . . . . . . . . . 36\n\n4.2 Coefficients for the g functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\nxiii\n\n\n\nxiv\n\n\n\nChapter 1\n\nIntroduction\n\nIn this dissertation we are going to explore a way of understanding the evolution of the\n\nvolume-price in the stock market. We use data from 2000 companies in the New York Stock\n\nExchange (NYSE). We are going to do this using stochastic differential equations.\n\nVolume-price is an important variable in mathematical finance since it incorporates the\n\ninteraction between volume and price. For instance, the volume has as an important role in\n\nthe assets\u2019 price. High volumes trigger high prices and low volumes are associated with low\n\nprices. Moreover, while we need to know the price of an asset if we want to sell or buy it at\n\nthe right time, we should also know the volume that is being transitioned in the market, since\n\nhigh volumes reflect a good market liquidity, i.e. it is easy to sell or buy the asset. Finally, the\n\ndistribution of the volume-price provides information about the capital that is being transitioned\n\nin the market.\n\nWe want to study the temporal evolution of the volume-price which is a non-stationary\n\nrandom variable. If it would be a stationary variable, it would be very easy to solve this problem.\n\nIn the literature we have numerous ways to model stationary variables [2]. However, since we\n\nhave a non-stationary random variable, it seems almost impossible to fit a good model to it.\n\nRecently, it was shown [1] that although the distribution of the volume-price is non-stationary,\n\nthe series has a constant functional shape (log-normal) throughout time. The parameters of\n\nthis log-normal distribution are time dependent and they are stationary. We will study these\n\nstationary parameters in order to describe the evolution of the non-stationary time series of the\n\nvolume-price.\n\nFigure 1.1 shows a simple scheme of the idea that will be explored in this dissertation. The\n\nstarting points of our study are the volume and price time series of 2000 companies in the New\n\nYork Stock Exchange (NYSE). This data was collected directly from Yahoo Finance, with a\n\nsampling frequency of 10 minutes, starting in January 27th 2011 and ending in April 6th 2014,\n\nwhich yields a total of 976 days (? 105 data points). The data preprocessing was done in a\nprevious work and can be found in Refs. [3, 4]. We can see this time series for just one company\n\nin Figure 1.1a. Multiplying the two series yields the volume-price time series. For the same\n\ntime interval, we have a sample of approximately 2000 companies. Thus, for each 10-minutes\n\nsnapshot, we have 2000 observations of volume-price, one for each company.\n\nIt was shown [1] that the log-normal distribution had the best fit to this data. We can see a\n\n10-minutes snapshot in Figure 1.1b: the dots represent the empirical probability density function\n\n(PDF) of the logarithm of the volume-price time series for the 2000 companies in that particular\n\n10 minutes. The solid line is the PDF of a normal distribution with mean and standard deviation\n\n1\n\n\n\n42\n\n44\n\n46\n\n48\nP\n\nri\nc\ne\n\n800 850 900 950 1000 1050 1100\n\nTime\n\n10\n3\n\n10\n4\n\n10\n5\n\n10\n6\n\nV\no\nlu\n\nm\ne\n\n10\n4\n\n10\n5\n\n10\n6\n\n10\n7\n\nVolume-Price\n\n1\n\n1.1\n\n1.2\n\n1.3\n\n13\n\n14\n\n?\n\n800 850 900 950 1000 1050 1100\n\nTime\n\n1.4\n\n1.6\n\n1.8\n\n?\n\n13\n\n14\n\n?\n\n-1\n\n-0.5\n\n0\n\n0.5\n\n1\n\n?\u2019\n\n800 850 900 950 1000 1050 1100\n\nTime\n\n1.4\n\n1.6\n\n1.8\n\n?\n\n800 850 900 950 1000 1050 1100\n\nTime\n\n-0.2\n\n-0.1\n\n0\n\n0.1\n\n0.2\n\n?\u2019\n\n(a) Single stock\n\nStock market log-normal model\n\n(c)\n\n_\n\n_\n\n(d) (e)\n\nParameter evolution\n\nAverage pattern (daily) Fluctuations\n\n+\n\n(b)\n\ne\n\ne\n\n?\n\n?\n\nFigure 1.1: We start with the price and volume series of 2000 different companies. In (a) we can see the volume\nand price series for just one of the companies. Multiplying the two series yields the volume-price series which\nfollows a log-normal distribution with parameters ? and ?. In (b) we can see the empirical density represented\nby the dots and the adjusted log-normal to the data, solid line, for a particular window of 10 minutes. Each\n10-minutes window yields a log-normal with different parameters. Thus, we will have (c) a time series for the\nparameter ? and another one for ?. Each time series can be decomposed in (d) a daily pattern and (e) fluctuations\naround this pattern. We will describe the evolution observed in (c) by analysing this components separately.\n\nequal to the ones of the volume-price logarithm. However, the series is non-stationary: another\n\n10-minutes window yields a log-normal with different parameters. This means that our volume-\n\nprice series follows a constant functional form throughout time (log-normal) but the parameters\n\nof this distribution are themselves stochastic variables.\n\nWe are going to study the evolution of these parameters illustrated in Figure 1.1c. For each\n\n10-minute window, we will have a value to the mean ? and to the standard deviation ? of the\n\nvolume-price logarithm. We can also see in this figure that both time series appear to have a\n\ndaily pattern. Because of this, we are going to split our analysis in two parts: one considering the\n\naverage daily patterns, Figure 1.1d, and another one with the fluctuations around this pattern,\n\nFigure 1.1e. The original series is the sum of these two parts.\n\nWe will propose a framework to describe the evolution observed in Figure 1.1c, modelling\n\naverage behaviour and stochastic contributions separately. We are going to extract stochastic\n\ndifferential equations for the parameters ? and ? following a recent framework [5]. By knowing\n\nhow the fluctuations from the ? and ? time series behave, we will be able to describe the evolution\n\nof the non-stationary series of volume-price. The main goal of this dissertation is to model the\n\n2\n\n\n\noriginal non-stationary time series of volume-price, s.\n\nIn this introductory chapter we stated the problem that is going to be addressed by this\n\ndissertation, the importance of this subject and what we did, in general lines, to solve this\n\nproblem as we can see in Figure 1.1. In Chapter 2, we will do a quick review of the main\n\nconcepts which are necessary to understand the subsequent chapters. In Chapter 3, we will\n\nexplain the preprocessing of the ? and ? time series. After that, we explore various aspects\n\nof these series and we check if it is possible the analysis of these time series. In Chapter 4\n\nwe will analyse both time series independently and coupled. In Chapter 5, we will use the\n\nresults presented in the previous chapters to derive the equations that govern the volume-price\n\nnon-stationary time series. Finally, the discussions and conclusions are presented in Chapter 6.\n\n3\n\n\n\n4\n\n\n\nChapter 2\n\nState of the art\n\n2.1 Stationary Stochastic Processes\n\nA stochastic process (Xt) is a family of random variables indexed by t belonging to some\n\nindex set T. The index set T can be any abstract set. However, in order to make it simple, we\n\ntake it as time, and hence T can be taken as R or some subset of R. In this case, we often call\nthe stochastic process (Xt) a time series. When we are trying to explain real world phenomena\n\nwith stochastic processes, we can notice that we only have a incomplete sample path of the\n\nprocess. However, we want to figure out the probability structure from this data, i.e. the finite\n\ndimensional distributions. If we do not make more assumptions, this is basically an impossible\n\ntask. In this section, we are going to represent by B(Rn) the ?-algebra generated by the family\nof open intervals from Rn.\n\nDefinition 1 (Finite Dimensional Distributions). Let (Xt)t?0 be a stochastic process de-\n\nfined on some probability space (?,F,P). The probabilities\n\nPXt1,Xt2,...,Xtn (B) = P((Xt1, ...,Xtn) ? B) (2.1)\n\nwhere n ? N, t1, ..., tn ? R, 0 ? t1 &lt;t2 &lt;... &lt;tn &lt;?,B ?B(Rn), i.e. B is a Borel measurable\nset of Rn, are called finite dimensional distributions.\n\nFinite dimensional distributions completely characterize the probability structure of a stochas-\n\ntic process since they involve the knowledge of all marginal distributions. However, if you do not\n\nmake further simplifications or study special cases we will not be able to get tractable models\n\nfor stochastic processes. One special case that is easier to study is a stationary process. A\n\nstationary stochastic process in its simplest form assumes that it is in equilibrium. In fact the\n\nprobability structure of the process does not change in time.\n\nDefinition 2 (Strict Stationarity). A stochastic process (Xt)t?0 is strictly stationary if for\n\nany t1, t2, ..., tn and h, the joint distributions of (Xt1, ...,Xtn) and (Xt1+h, ...,Xtn+h) are identi-\n\ncal, that is\n\nPXt1,Xt2,...,Xtn (B) = PXt1+h,Xt2+h,...,Xtn+h(B) (2.2)\n\nwhere n ? N, t1, ..., tn ? R, 0 ? t1 &lt;t2 &lt;... &lt;tn &lt;?,B ?B(Rn)\n\n5\n\n\n\nIt is important to notice that a strictly stationary process has a probability structure which\n\nis invariant under a time shift. This means the random variables Xt and Xt+h have the same\n\ndistribution. Therefore, if the moment exists then E(Xnt ) = E(X\nn\nt+h). In particular, a strictly\n\nstationary process will have constant mean and variance. Besides that, the covariance function\n\nof a strictly stationary time series depends only on the time interval between the time points,\n\nnot on the specific location of the points along the time axis.\n\nStrict stationarity is a very restrictive property defined on all of the finite dimensional dis-\n\ntributions which is very difficult to verify empirically. There are weaker forms of stationarity,\n\nexpressed in terms of the moments which are often sufficient to construct very useful processes.\n\nDefinition 3 (mth-order Stationarity). The process Xt is said to be stationary up to order\n\nm, if for any t1, t2, ..., tn and for all m1,m2, ...,mn such that\n?n\n\ni=1 mi ? m, all product moments\nexist E(Xm1t1 ...X\n\nmn\ntn\n\n) and for any h ? 0,\n\nE(Xm1t1 ...X\nmn\ntn\n\n) = E(Xm1t1+h...X\nmn\ntn+h\n\n) . (2.3)\n\nSecond-order stationarity, which is also often called weak stationarity or covariance station-\n\narity, is fundamental in studying large classes of processes. Strictly stationary processes do not\n\nneed to be mth-order stationary. However, when moments up to order m exist, then a strictly\n\nstationary process will also be a mth-order stationary. Since the knowledge of full infinite se-\n\nquence of moments under certain conditions define the finite dimensional distributions, loosely\n\nspeaking, strict stationarity corresponds to mth-order stationarity in the limit as m ??.\n\n2.2 Brownian Motion and White Noise\n\nUntil the nineteenth century, it was commonly thought that if we could collect all the initial\n\ndata then we would predict the future with certainty. This is known as the Laplace\u2019s dream: we\n\nwould be able to create a model of the universe which would be completely deterministic [6]. We\n\nnow know that this is not true. The limited predictability may arise in the form of fluctuations\n\ndue to interactions with the environment.\n\nIn 1828, the botanist Robert Brown observed and tried to describe the irregular movement\n\nof pollen, suspended in water [7]. This movement is now known as the Brownian movement and\n\nis attributed to the collisions of the pollen with the water molecules, resulting in a diffusion of\n\nthe pollen in the water. His work was largely ignored by the scientific community at his time.\n\nIn 1900, independently from Brown\u2019s work, L. Bachelier [8] derived the law governing the\n\nposition wt at time t of a single grain of pollen performing a one-dimensional Brownian motion\n\nstarting at a ? R at time t = 0:\n\nPa{Wt ? dx} = p(t,a,x)dx , with p(t,a,x) =\n1\n?\n\n2?t\ne?\n\n(x?a)2\n2t (2.4)\n\nwhere p(t,a,x) is the solution of the heat equation:\n\n6\n\n\n\n?u\n\n?t\n=\n\n1\n\n2\n\n?2u\n\n?a2\n, (2.5)\n\nIn 1905 Albert Einstein addressed the Brown\u2019s question without having knowledge of Bache-\n\nlier work [9]. He predicted what the Brownian motion should be. These predictions are nowadays\n\na part of one of the formal definitions of Brownian motion [10].\n\nHowever, it was only in 1923 that Nobert Wiener proved the existence of Brownian motion\n\nand set down a firm mathematical foundation for its analysis. In his honour, the mathemat-\n\nical formulation of the Brownian motion is know as the Wiener process. Before we give the\n\nmathematical definition of a Brownian motion, it is important to understand the concept of a\n\nfiltration.\n\nDefinition 4 (Filtration). Let X : [0, +?[\u00d7? ? R be a stochastic process on a probability\nspace ? with a ?-algebra F. For each 0 ? t &lt;?, we define a ?-algebra Ft as the sigma algebra\ngenerated by the variables Xs, 0 ? s ? t:\n\nFt = ?(Xs : 0 ? s ? t) (2.6)\n\nIf 0 ? s &lt;t, then Fs ?Ft ?F. Such a family of ?-fields Ft : 0 ? t &lt;? is called a filtration of\nF.\n\nThis filtration is generated by the process Xt since it contains all of its past at each time t.\n\nIn an intuitive way, it is the smallest filtration available to study the process Xt: is contains all\n\nthe information regarding the past of the process, but only that information. Now we can see\n\nwhat a Brownian motion is:\n\nDefinition 5 (Brownian Motion). A Brownian motion is a real valued, continuous stochastic\n\nprocess (Wt)t?0, with the following properties:\n\n\u2022 W0 = 0 almost surely.\n\n\u2022 independent increments: If s ? t, Wt ?Ws is independent of Fs = ?(Wu : 0 ? u ? s).\n\n\u2022 stationary increments: If s ? t, Wt ? Ws and Wt?s ? W0 have the same probability law\nsince they follow a normal distribution with mean zero and variance t?s.\n\nUsing the Kolmogorov theorem [10] it is possible to prove that the function t 7? W(t) is\nalmost surely continuous. However, the randomness allows Brownian motion to also be nowhere\n\ndifferentiable as we are going to see. However, we clarify the concept of limit of a stochastic\n\nvariables that we are going to use here.\n\nDefinition 6 (Almost Sure Convergence). We say that Xn converges to X with probability\n\none (almost surely) if P\n\n(\nlim\n\nn?+?\nXn = X\n\n)\n= 1. That is, for almost every ? ? ? except for a set\n\nof ? ? ? with measure 0, the sequence Xn(?) converges to X(?).\n\nTheorem 1 (Non-differentiability of the Brownian motion). Almost surely, Brownian\n\n7\n\n\n\nmotion is nowhere differentiable. Furthermore, almost surely, for all t, either\n\nlim\nsuph?0\n\nsup\nWt+h ?Wt\n\nh\n= +? (2.7)\n\nor\n\nlim\ninfh?0\n\nsup\nWt+h ?Wt\n\nh\n= ?? . (2.8)\n\nSince the sample paths of a Brownian motion are nowhere differentiable with probability 1,\n\na stochastic process of the form (?t)t?0 with:\n\n?t =\ndWt\ndt\n\n= W ?t or dWt = ?tdt (2.9)\n\ncannot be introduced by taking the almost sure limit in a difference quotient. Nevertheless, it\n\nis possible to arrive to a definition via an integral [11]. In order to approach this definition, let\n\ng(t) be any function with a continuous derivative g?(t) in the interval [a,b] and t0, t1, ..., tn a\n\nsequence of numbers satisfying:\n\na = t0 &lt;t1 &lt;... &lt;tn = b and ?ti = ti+1 ? ti; i = 0, 1, 2...,n? 1\n\nThen, the stochastic integral\n? b\na\ng(t)dWt is defined as the almost sure limit:\n\n? b\na\ng(t)dWt = lim\n\nn?+?, max?ti?0\n\nn?1?\ni=0\n\ng(ti)(Wti+?ti ?Wti) = (2.10)\n\n= lim\nn?+?, max?ti?0\n\ng(b)Wb ?g(a)Wa ?\nn?1?\ni=0\n\nWti+1\ng(ti + ?ti) ?g(ti)\n\n?ti\n?ti (2.11)\n\nThe stochastic integral is an almost sure limit of a sum of normally distributed variables.\n\nTherefore, it also has a normal distribution. It is important to notice that we are going to give a\n\nmore general definition of the stochastic integral later, after we have introduced more concepts.\n\nNow we just want to motivate the following definition by taking the almost sure limit on both\n\nsides of the previous equation.\n\nDefinition 7 (White Noise). Let (Wt)t?0 be a Brownian motion. A stochastic process\n\n(?t)t?0 is called a white noise if it satisfies, for any function g(t), with a continuous deriva-\n\ntive g?(t) in [a,b],a &lt;b, the following relationship:\n\n? b\na\ng(t)?tdt = g(b)Wb ?g(a)Wa ?\n\n? b\na\nWtg\n\n?(t)dt . (2.12)\n\nIt is important to notice that in the left side of Equation (2.12) we are writing an abusive\n\nnotation (although common and inoffensive) since ?t does not exist as a function whereas it\n\nis defined as a linear functional in the space of continuous differentiable functions through the\n\nright side of Equation (2.12).\n\nIf Wt had a first derivative in order to time, then ?t =\ndWt\ndt\n\nwould satisfy the relationship\n\nin the previous definition. Therefore, the white noise ?t can be interpreted as a generalized\n\n8\n\n\n\nderivative of the Brownian motion Wt.\n\nIf s ? t we know that Wt ? Ws ? N(0, t ? s). Therefore E(Wt) = E(Wt ? W0) = 0\nand var(Wt) = var(Wt ? W0) = t ? 0 = t. If s &lt;t, E(WsWt) = E(Ws(Ws + Wt ? Ws)) =\nE(W 2s ) + E(Ws)E(Wt ? Ws) = var(Ws) + 0 = s. We can replicate these calculations if t &lt;s\nand we would get E(WsWt) = t. Therefore, E(WsWt) = min{s,t}.\n\nSince E(Wt) = 0 and E(WsWt) = min{s,t}, we know that Cov(Ws,Wt) = min{s,t}.\nUsing this result for the Brownian motion and realizing that ?t =\n\ndWt\ndt\n\n, it is easy to see that\n\nCov(?s,?t) = 0. Therefore, for s 6= t there is no correlation between ?s and ?t, even if we make\nthe difference |s? t| very small.\n\nAs a result, white noise can be seen as the most random stochastic process and this is why it is\n\nso often used to model random noise in various fields like electronics, engineering, econometrics,\n\nfinance, among others. But also because of this property, the white noise cannot exist in the\n\nreal world.\n\nIt is not plausible for any physical phenomena to generate uncorrelated ?s and ?t even\n\nwhen |s ? t| is very small. This mismatch from physical reality appears in its mathematical\nrepresentation as well: one cannot define a continuous white noise process with non-zero, finite\n\nvariance. The variance of a white noise process is undefined [12].\n\nIn practice, a weakly stationary stochastic process (?t)t?0 can approximately be considered\n\nwhite noise if the covariance between ?t and ?t+? tends extremely fast to 0 with increasing ?.\n\nThe denomination white noise comes from the spectral theory of stationary random pro-\n\ncesses that states the white noise has a power spectrum which is uniformly distributed over all\n\nfrequencies (like white light).\n\nThe range of application of Brownian motion goes far beyond a study of microscopic particles\n\nin suspension and included modelling of stock prices, thermal noise in electrical circuits, limiting\n\nbehaviour in queuing and random perturbations in a variety of other physical, biological, eco-\n\nnomic and management systems. Moreover, integration with respect to Brownian motion gives\n\nus a unifying representation for a large class of diffusion processes.\n\nDiffusion processes represented this way exhibit a rich connection with the theory of partial\n\ndifferential equations. In particular, to each diffusion process there corresponds a second order\n\nparabolic equation which governs the transition probabilities of the process.\n\nBachelier not only was the pioneer in modelling the Brownian motion using stochastic pro-\n\ncesses, but he was also the first one to build a theory for the fluctuations of the stock market using\n\nadvanced mathematics. Besides that, he uncovered the Markovian property of the Brownian\n\nmotion. However, his work went largely unknown for nearly a century.\n\n2.3 Markov Process\n\nThe Brownian motion is an example of a Markov process. This type of processes are very\n\nimportant since we can derive results assuming the Markovian property which we would not be\n\nable to get without it. The Langevin Analysis that we are testing in this thesis is one example\n\nof this. We can only use this approach if our time series are Markovian [13] . Before we define\n\na Markov process, it is important to understand the concept of conditional expectation. It is\n\n9\n\n\n\nknown from basic probability theory that the conditional probability of an event A ? F given\nB ?F and P(B) > 0 is the real number:\n\nP(A|B) =\nP(A?B)\nP(B)\n\n(2.13)\n\nUsing this simple concept, we can create a new probability measure:\n\nTheorem 2 (Conditional probability of an Event). Let us consider a probability space\n\n(?,F,P) and B ?F with P(B) > 0. Thus, the application QB : F ? [0, 1] defined by QB(A) =\nP(A|B) is a new probability measure in (?,F) and it is called the conditional probability of event\nA given event B.\n\nAnd now we are able to define the conditional expectation of a stochastic random variable,\n\ngeneralizing Equation (2.13):\n\nDefinition 8 (Conditional Expectation of a Stochastic Variable given an event). Let\n\nB ? F with P(B) > 0 and let ? : ? ? R be a stochastic integrable variable, i.e., E(|?|) &lt;?.\nThe conditional expectation of ? given an event B, E(?|B), is the real number:\n\nE(?|B) =\nE(1B?)\nP(B)\n\n(2.14)\n\nwhere\n\n1B(x) =\n\n??\n?1, if x ? B0, if x /? B (2.15)\n\nDefinition 9 (Conditional Expectation of a Stochastic Variable given a ?-algebra).\n\nLet G be a sub- ?-algebra from F and let ? : ? ? R be a stochastic integrable variable. The\nconditional expectation of ? given the ?-algebra G is a stochastic integrable variable, E(?|G) that\nsatisfies the following properties:\n\n\u2022 E(?|G) is measurable with respect to G.\n\n\u2022 If A ?G then E(1AE(?|G)) = E(1A?)\n\nSince we are now familiar with the concept of conditional expectation, we can easily under-\n\nstand that E(X|Ft) is the best estimate of X based on observations of the process up to time\nt. The properties of conditional expectations with respect to filtrations define various types of\n\nstochastic processes. One of the most important ones is the Markov process.\n\nDefinition 10 (Markov Process). Let us consider a stochastic process (Xt)t?0 in a probability\n\nspace (?,F,P). We say that (Xt)t?0 is a Markov process if it satisfies the following property\n(Markov property): to all the functions f : R ? R, Borel measurable and bounded and 0 ? s ?\nt &lt;?:\n\nE(f(Xt)|Fs) = E(f(Xt)|Xs) (2.16)\n\n10\n\n\n\nIf X is any random variable, then E(X | Ft) is the best estimate of X based on observations\nof the process up to time t. Intuitively, one interprets Equation (2.16) as follows: given the\n\npresent of the process, the future is independent of its past. A Markov process only cares about\n\nits present state, and has no memory of how it got there.\n\nMarkov processes are very useful in applied mathematics for several reasons. Four of the\n\nmore important reason are:\n\n\u2022 Many real world phenomena can be modeled by a Markov process.\n\n\u2022 Usually the input needed for the application of a Markov process is more easily given than\nfor other non-Markovian processes.\n\n\u2022 There are various computer algorithms for numerical simulations for Markov processes.\n\n\u2022 All stochastic processes that have independent increments also have the Markov property.\n\nTheorem 3 (Brownian Motion as a Markov Process). Let us consider a probability space\n\n(?,F,P). A Brownian motion (Wt)t?0 is a Markov process.\n\nIt is also possible to characterize a Markov process using its finite dimensional distributions.\n\nLet us consider the times:\n\n0 ? t1 &lt;t2 &lt;... &lt;tm &lt;tm+1 &lt;... &lt;tn\n\nThe conditional probability that Xti = xi for m + 1 ? i ? n given that Xti = xi, for\n1 ? i ? m, is given by:\n\np(xn, tn; ...; xm+1, tm+1 | xm, tm; ...; x1, t1) =\np(xn, tn; ...; x1, t1)\n\np(xm, tm; ...; x1, t1)\n(2.17)\n\nWe have a Markov process if these conditional densities depend only on the most recent\n\ntime, which means:\n\np(xn+1, tn+1 | xn, tn; ...; x2, t2; x1, t1) = p(xn+1, tn+1 | xn, tn) (2.18)\n\nTherefore, we also have for a Markov process:\n\np(xn, tn; ...; x2, t2 | x1, t1) = p(xn, tn | xn?1, tn?1)...p(x2, t2 | x1, t1) (2.19)\n\nIt is now possible to deduce all joint finite dimensional probability densities of a Markov\n\nprocess Xt using only the transition density p(x,t | y,s) and the probability density of its initial\nvalue, p0(y):\n\nP(x,t) =\n\n? +?\n??\n\np(x,t | y, 0)p0(y)dy (2.20)\n\nThe transition probabilities of a Markov satisfy the Chapman-Kolmogorov equation:\n\np(x,t | y,s) =\n? +?\n??\n\np(x,t | z,r)p(z,r | y,s)dz , s &lt;r &lt;t (2.21)\n\n11\n\n\n\nIntuitively this means that the probability of a transition from y at time s to x at time t\n\nis equal to the probability of the transition to z at an intermediate time r, multiplied by the\n\nprobability of the transition from z at the time r to x at the time t, summed over all possible\n\nintermediate values z.\n\nDefinition 11 (Homogeneous Markov Process). A Markov process (Xt)t?0 is (time) ho-\n\nmogeneous if\n\np(x,t | y,s) = p(x,t?s | y, 0) (2.22)\n\nIn other words, a Markov process is said to be time homogeneous if its transition probability\n\nis stationary. A homogeneous Markov process is essentially a Markov process with invariable\n\nstochastic properties under a time shift. The probability of a transition from y to x only depends\n\non the time difference t ? s. Therefore, we can write p(x,t|y,s) = p(x,t ? s|y). We can also\nrewrite the Chapman-Kolmogorov equation:\n\np(x,t | y) =\n? +?\n??\n\np(x,t?s | z)p(z,s | y)dz , 0 &lt;s &lt;t (2.23)\n\nTheorem 4 (Brownian Motion as a Homogeneous Markov Process). The Brownian\n\nmotion (Wt)t?0 is a homogeneous Markov process with state space R and transition probability\nfunction given by\n\np(x,t | y) =\n1\n?\n\n2?t\ne?\n\n(x?y)2\n2t t > 0 . (2.24)\n\n2.4 Stochastic Differential Equations\n\nIn Equation (2.9) we established that the white noise can be seen as the time derivative of\n\nthe Brownian motion. In fact, the Brownian motion is the foundation for the constitution of\n\nan extensive class of Markov processes with continuous sample paths, called diffusion processes.\n\nThe white noise was an example. But we can also have more sophisticated examples. For\n\ninstance, we can add a mean drift to the white noise. This diffusion process can be described\n\nby the stochastic differential equation:\n\ndXt\ndt\n\n= b(Xt) + ?t (2.25)\n\nwhere b : R ? R is a given smooth function and ?t is a white noise. We can think about this\nequation as a white noise perturbed by a drift term b or as a deterministic ordinary differential\n\nequation perturbed by an additive white noise [13].\n\nThe white noise is the time-integral of the Brownian motion. Therefore, any differential\n\nequation with a white noise can be rewritten as an integral equation with a Brownian motion.\n\nWe rewrite Equation (2.25) as:\n\n12\n\n\n\nXt = X0 +\n\n? t\n0\nb(X(s))ds + W(t) (2.26a)\n\nor in the differential form\n\ndXt = b(Xt)dt + dWt (2.26b)\n\nThese are stochastic differential equations (SDE) which contain a white noise with a constant\n\nstrength. In order to study SDE where the strength of the white noise depends on the solution,\n\nwe will need to give a meaning to the expression:\n\n? T\n0\nft(?)dWt(?) (2.27)\n\nwhere (?,F,P) is a probability space and (Wt)t?0 is a Brownian motion.\nOne of the most important properties of the Brownian motion is that its paths are almost\n\nsurely not differentiable at any point. Therefore, we cannot define the integral in Equation\n\n(2.27) as a Lebesgue-Stieltjes integral. Nevertheless, we are able to define this type of integral\n\nwith respect to a Brownian motion and we will call them stochastic integrals.\n\nTo start, we will construct the integral for a set of processes called simple processes. After\n\nthat, the definition of the integral will be extended to a more general class of processes by taking\n\na limit [10].\n\nDefinition 12 (Simple Process). (St(?))0?t?T is called a simple process if it can be written\n\nas\n\nSt(?) =\n\np?\ni=1\n\n?i(?)1]ti?1,ti] . (2.28)\n\nwhere 0 = t0 &lt;t1 &lt;... &lt;tp = T and ?i is Fti?1 -measurable and bounded.\n\nDefinition 13 (Stochastic Integral of a Simple Process). The stochastic integral of a\n\nsimple process S defined as in Equation 2.28 is the continuous process:\n\n? T\n0\nSt dWt :=\n\nn?\ni=1\n\n?i(Wti ?Wti?1 ) (2.29)\n\nNow we want to extend the previous definition to a broader class of stochastic processes.\n\nWe will define the stochastic integral for all the stochastic processes that belong to H2([0,T]),\ndefined as:\n\nDefinition 14 (Stochastic Processes in H2([0, T])). Given a time interval [0,T], let H2([0,T])\nbe the class of stochastic processes H = (Ht)t?[0,T ] that satisfy the following conditions:\n\n\u2022 (Ht)t?[0,T ] is a measurable process, i.e. the function [0,T] \u00d7 ? ? R : (t,?) 7? Ht(?) is\nmeasurable.\n\n\u2022 (Ht)t?[0,T ] is adapted to the filtration .(Ft)t?0, i.e. for all t ? [0,T] the variable Ht is\nFt-measurable\n\n\u2022 E(\n?T\n\n0\nH2t ) &lt;? .\n\n13\n\n\n\nFor every process in H2([0,T]) there is a sequence of simple processes that converge to that\none process. This result is going to let us define the stochastic integral for all the processes in\n\nH2([0,T]).\n\nTheorem 5. To every stochastic process H ?H2([0,T]), there is a sequence (Hn)n?N of simple\nstochastic processes that:\n\nlim\nn??\n\nE\n\n(? T\n0\n\n(Ht ?Hnt )\n2\n\n)\n= 0 . (2.30)\n\nTherefore, it is easy to see that if we have a process H ?H2([0,T]) and a sequence (Hn)n?(N)\nin the conditions of the previous theorem then\n\n?T\n0\n\n(Hnt )n?N dWt also converges and the limit\n\ndoes not depend on the sequence (Hn)n?(N). Hence, we are now ready to give a definition of the\n\nstochastic integral in H2([0,T]).\n\nDefinition 15 (Stochastic Integral). For a given process H ? H2([0,T]),we will define the\nstochastic integral of Ito? of H in [0,T] as the almost sure limit of the sequence of stochastic\n\nintegrals\n?T\n\n0\n(Hnt )n?N dWt, where (H\n\nn)n?N is in the conditions of Theorem 5, i.e.? T\n0\nHt dWt = lim\n\nn??\n\n? T\n0\nHnt dWt (2.31)\n\nSince we already know what a stochastic integral is, we can then generalize Equations (2.26)\n\nand define a SDE in a more general way. In Equations (2.26) we examined SDE that contains\n\na white noise with a constant strength. Now we are going to define SDE where the strength of\n\nthe white noise depends on the solution and we are going to allow the coefficients do depend\n\nexplicitly on time.\n\nDefinition 16 (Stochastic Differential Equation). Let us consider a probability space (?,F,P)\nwith a filtration (Ft)t?0 and let (Wt)t?0 be a Brownian motion. A stochastic differential equation\nis an equation with the following form:\n\nXt = ? +\n\n? t\nt0\n\nf(s,Xs)ds +\n\n? t\nt0\n\ng(s,Xs)dWs , t ? t0 ? 0 (2.32)\n\nwhere f : R+\u00d7R ? R, (t,x) 7? f(t,x), g : R+\u00d7R ? R, (t,x) 7? g(t,x) are measurable functions,\n? is a random variable and (Xt)t?0 is a stochastic process. The functions f and g are called the\n\ncoefficients of the equation and ? is the initial condition.\n\nWe can also write a SDE in the differential form:??\n?dXt = f(t,Xt)dt + g(t,Xt)dWt , t ? t0Xt0 = ? (2.33)\n\nA non rigorous explanation of Equation 2.33 (but a very useful one) is that for a little time\n\nincrease dt > 0, the stochastic process (Xt)t?0 changes its value in a random quantity that is\n\nnormally distributed with mean f(t,Xt)dt and variance g(t,Xt)g\nT (t,Xt)dt and it is independent\n\nfrom the past of the process. This happens because the increments of the Brownian motion are\n\n14\n\n\n\nindependent and normally distributed with mean 0 and variance equal to the time increase. In\n\nthis sense, the term g(t,Xt)dWt is used to model the perturbation of the random noise that\n\naffects the deterministic system dXt = f(t,Xt)dt. In a physicist tradition we sometimes refer to\n\na SDE as a Langevin equation. Both designations mean the same.\n\nIf we want to work with SDE, we not only have to know what the stochastic integral means,\n\nbut we also have to know how to work with it. The traditional rules of calculus will not apply\n\nto the stochastic integral.\n\nOne of the key results that allows us to work with the stochastic integral is a new version of\n\nthe chain rule called Ito?\u2019s formula.\n\nTheorem 6 (Ito?\u2019s Formula). Let (Xt)t?0 be a solution of Equation 2.33 and h : R+ \u00d7 R ?\nR, (t,x) 7? h(t,x) a function with continuous derivative with respect to t and continuous deriva-\ntives with respect to x until the second order. In another words, this means h is a function in\n\nC2([0, +?[\u00d7R).\n\nThen (Yt)t?0 where Yt = h(t,Xt) is a stochastic process that satisfies the equation\n\ndYt =\n\n(\n?h\n\n?t\n(t,Xt) + f(t,Xt)\n\n?h\n\n?x\n(t,Xt) +\n\n1\n\n2\ng2(t,Xt)\n\n?2h\n\n?x2\n(t,Xt)\n\n)\ndt + g(t,Xt)\n\n?h\n\n?x\n(t,Xt)dWt .\n\n(2.34)\n\nNow that we have already understood how the Ito?\u2019s formula work in one dimension, we will\n\ngeneralize it to several dimensions [14].\n\nTheorem 7 (Ito?\u2019s Formula in d-dimension). Let Xt = (X1t , ...,X\nd\nt )\nT be a vector process\n\nsatisfying:\n\ndXt = Adt + HdWt (2.35)\n\nwhere Wt is a multi-dimensional independent Brownian motion defined as Wt = (W 1t , ...,W\nn\nt )\n\nT ,\n\nA = (a1(t,Xt), ...,ad(t,Xt))T is the drift vector and H is the diffusion matrix:\n\nH =\n\n?\n?????????\n\nh11(t,Xt) ... h1n(t,Xt)\n\n...\n. . .\n\n...\n\nhd1(t,Xt) ... hdn(t,Xt)\n\n?\n?????????\n\n(2.36)\n\nLet h : R+ \u00d7 Rd ? R be a given bounded function in C2([0, +?[\u00d7Rd). Let Yt = h(t,Xt).\nThen,\n\ndYt =\n\n?\n??h\n?t\n\n+\nd?\ni=1\n\nai\n?h\n\n?xi\n+\n\n1\n\n2\n\nd?\ni,j=1\n\nn?\np=1\n\nhjphip\n?2h\n\n?xi?xj\n\n?\n?dt + d?\n\nj=1\n\nn?\np=1\n\nhjp\n?h\n\n?xj\ndW\n\np\nt (2.37)\n\n15\n\n\n\n2.5 The Fokker-Planck Equation\n\nWe have already seen that the Brownian motion is a time-homogeneous Markov process with\n\na transition density that satisfies Equation (2.5). Fokker and Planck derived this differential\n\nequation for the distribution function describing the Brownian motion and it is now known\n\nas the Fokker-Planck equation for the Brownian motion. However, we can generalize this to\n\nother stochastic processes. In fact, the Fokker-Planck equation is an equation of motion for the\n\ndistribution function of fluctuating macroscopic variables, i.e. it describes the evolution through\n\ntime of the probability density function of macroscopic variables. By solving the Fokker-Planck\n\nequation one obtains distribution functions from which any averages of macroscopic variables\n\nare obtained by integration.\n\nIn order for us to present the Fokker-Planck equation, it is necessary to be familiar with\n\nthe concept of a diffusion process which is a special case of a Markov process with continuous\n\nsample functions which serve as probability-theoretic models of physical diffusion phenomena\n\n[15]. The simplest and oldest example of a diffusion process is the Brownian motion. There are\n\ndifferent approaches to the class of diffusion processes. We are going to define them in terms of\n\nthe conditions on the transition probabilities p(Xt ? B|Xs = x).\n\nDefinition 17 (Diffusion Process). A diffusion process is a Markov process Xt, t0 ? t ? T ,\nwith values in Rd and almost sure continuous sample functions whose transition probability\np(Xt ? B|Xs = x) satisfies the following three conditions for every s ? [t0,T[, x ? Rd and\n? > 0:\n\n\u2022\nlim\ns?t\n\n1\n\nt?s\n\n?\n|y?x|>s\n\np(Xt = y|Xs = x)dy = 0 . (2.38)\n\n\u2022 There exists a Rd-valued function D(1)(s,x) such that:\n\nlim\ns?t\n\n1\n\nt?s\n\n?\n|y?x|?s\n\n(y ?x)p(Xt = y|Xs = x)dy = D(1)(s,x) . (2.39)\n\n\u2022 There exists a d\u00d7d matrix-valued D(2)(s,x) such that:\n\nlim\ns?t\n\n1\n\nt?s\n\n?\n|y?x|?s\n\n(y ?x)(y ?x)?p(Xt = y|Xs = x)dy = D(2)(s,x) . (2.40)\n\nThe functions D(1) and D(2) are called the coefficients of the diffusion process. In particular,\n\nD(1) is called the drift vector and D(2) is called the diffusion matrix. D(2) is non-negative-defined.\n\nNow we can proceed to see what a Fokker-Planck Equation is.\n\nTheorem 8 (Fokker-Planck Equation). Let Xt, t0 ? t ? T ,denote a d-dimensional diffusion\nprocess that satisfies the conditions in Definition 17 and which possesses a transition density\n\np(Xt = y|Xs = x). If the derivatives:\n\n?p\n\n?t\n,\n\n?\n\n?yi\n\n(\nD\n\n(1)\ni (t,y)p\n\n)\n,\n\n?2\n\n?yi?yj\n\n(\nD\n\n(2)\nij (t,y)p\n\n)\n16\n\n\n\nexist and are continuous functions, then for fixed s and x such that s ? t, the transition proba-\nbility p(Xt = y|Xs = x) is a fundamental solution of the Fokker-Planck equation:\n\n?p\n\n?t\n+\n\nd?\ni=1\n\n?\n\n?yi\n\n(\nD\n\n(1)\ni (t,y)p\n\n)\n?\n\n1\n\n2\n\nd?\ni,j=1\n\n?2\n\n?yi?yj\n\n(\nD\n\n(2)\nij (t,y)p\n\n)\n= 0 (2.41)\n\nThe Fokker-Planck equation can be used to get the probability density function associated\n\nwith a SDE. In fact, the Ito?\u2019s formula gives us a simple way of deriving the Fokker-Planck\n\nequation [13]. If we do this, we will see that the coefficients of the SDE are related to the\n\ncoefficients of the Fokker-Planck equation.\n\nAnalytical solutions to the Fokker-Planck equations can only be reached in very special case.\n\nMost of the times we cannot find an analytical solution. However, it is possible to use other\n\nmethods of solutions like simulation methods, transformation of the Fokker-Planck equation in\n\na Schro?dinger equation, numerical integration methods among others.\n\nOne of the main goals of this dissertation is to determine the coefficients D(1) and D(2) that\n\ngovern the Fokker-Planck equation of the probability density function of the fluctuations of the\n\nparameters of the log-normal distribution of volume-price.\n\nAs an example of application of the Fokker-Planck equation, we will study the oldest example\n\nof a stochastic differential equation which describes the Brownian motion of a particle under the\n\ninfluence of friction but no other force field and it is known as the Ornstein-Uhlenbeck process:\n\nDefinition 18 (Ornstein-Uhlenbeck Process). The Ornstein-Uhlenbeck process is the uni-\n\nvariate continuous Markov process Xt that evolves with time t according to the following stochas-\n\ntic differential equation:\n\ndXt = ?(\u00b5?Xt)dt + ?dWt (2.42)\n\nwhere ?,\u00b5,? > 0 and (Wt)(t?0) is a Brownian motion.\n\nWe can solve Equation (2.42) and we get a solution that is the sum of a deterministic\n\nbehaviour plus a stochastic term :\n\nXt = X0e\n??t + \u00b5(1 ?e??t) + ?\n\n? t\n0\ne??(t?s))dWs (2.43)\n\nThe probability density function, p(Xt = x|Xs = y), of the Ornstein-Uhlenbeck process\nsatisfies the Fokker-Planck equation:\n\n?p\n\n?t\n?\n\n?\n\n?x\n(?(x?\u00b5)p) ?\n\n1\n\n2\n\n?2\n\n?x2\n(\n?2p\n)\n\n= 0 (2.44)\n\nIn order to simplify the results, we are going to assume \u00b5 = 0 and D = 1\n2\n?2. Then the\n\nsolution for this Fokker-Planck equation is:\n\np(Xt = x|Xs = y) =\n\n?\n?\n\n2?D(1 ?e?2?(t?s))\nexp\n\n(\n?\n?(x?e??(t?s))y2\n\n2D(1 ?e?2?(t?s))\n\n)\n(2.45)\n\n17\n\n\n\n2.6 The Black-Scholes Model and its limitations\n\nAlthough Bachelier had already applied results from stochastic calculus to the finance world,\n\nthis methodology only started to be widely used when Fischer Black, Myron Scholes [16] and\n\nRobert Merton [17] addressed the problem of pricing and hedging an European option on a\n\nnon-dividend paying stock in 1973. This is one of the most important problems in Financial\n\nMathematics. Black and Scholes worked independently from Merton but arrived to the same\n\nconclusions. The Nobel Prize in Economics for 1997 was awarded to Merton and Scholes. If\n\nBlack had been alive in that year, he would have shared the prize.\n\nAlthough their model have some problems which have been pointed out throughout the years,\n\nit is widely employed as a useful approximation. However, a proper application requires a good\n\nunderstanding of its limitations. Blindly following the model exposes the user to unexpected\n\nrisk.\n\nThe Black-Scholes model assumes that the asset price, St, follows a geometric Brownian\n\nmotion:\n\ndSt = \u00b5Stdt + ?StdWt (2.46)\n\nwhere Wt is a standard Brownian motion, \u00b5 ? R and ? ? R+. This stochastic differential\nequation tells us that the price variation of the asset in the time interval dt follows a normal\n\ndistribution with mean \u00b5dt and variance ?2dt. Equation (2.46) is the main assumption of\n\nthe Black-Scholes model. Besides that, we also assume short selling is allowed, there are no\n\ntransactions costs, the assets are perfectly divisible and pay no dividend, there are no arbitrage\n\nopportunities, trading takes place continuously in time and the risk-free rate is constant and the\n\nsame for all maturities. From all these assumptions, we can derive the time-t fair value of an\n\nEuropean-style option (call or put) on an asset with spot price St [10].\n\nDespite the importance of the Black-Scholes model, various authors have showed that its\n\nunderlying assumptions do not agree with the market\u2019s reality. For example, the geometric\n\nBrownian motion cannot explain the negative skewness and the high kurtosis that are usually\n\nseen in the empirical asset return distributions [18]. Another example is the negative correlation\n\nbetween stock returns and realized volatility, known as leverage effect [19]. However the most\n\nnoticed drawback in the Black-Scholes model is the inverse relationship between the implied\n\nvolatility and the strike price, known as volatility smile [20].\n\nOther models were developed in order to overcome these problems. The Constant Elasticity\n\nof Variance (CEV) model was developed by Cox in 1975 [21]. It is consistent with the leverage\n\neffect and with the volatility smile. The CEV model assumes that the asset price, St, follows\n\nthe following diffusion process:\n\ndSt = \u00b5Stdt + ?S\n?\n2\nt dWt (2.47)\n\nwhere ? is the constant of elasticity that controls the relationship between volatility and price.\n\nWhen ? &lt;2 we have the leverage effect, i.e. the observed variance of stock returns will be\n\ninversely related with the asset\u2019s price: it will increase as the asset\u2019s price decreases and decrease\n\nwhen the price increases.\n\nAnother important and useful alternative to the Black-Scholes model is the Heston model\n\n[22]. This is one of the most well known stochastic volatility models. It assumes that the asset\n\n18\n\n\n\nprice, St, is governed by the following stochastic differential equation:\n\ndSt = \u00b5Stdt +\n?\nvtStdW\n\n(1)\nt (2.48)\n\nwhere Wt is a standard Brownian motion and \u00b5 ? R. The instantaneous variance of the asset\u2019s\nreturns is assumed to follow another stochastic differential equation:\n\ndvt = a(b?vt)dt + ?\n?\nvtdW\n\n(2)\nt (2.49)\n\nwhere b is the long term mean of vt, a ? 0 is the speed of mean reversion, i.e., the rate at which\nvt reverts to b and ? is the volatility of the variance process. Moreover, the parameters have to\n\nobey the Feller condition so the process vt is strictly positive:\n\n2ab\n\n?2\n> 1 (2.50)\n\nThe Brownian motions in Equations (2.48) and (2.49) are correlated, with correlation ?.\n\n2.7 From Stochastic Volatility to Superstatistics\n\nSuperstatistics is a branch of statistics aimed to the study of non-linear and non-equilibrium\n\nsystems. Complex systems often show a behaviour which can be regarded as a superposition of\n\ndifferent dynamics[23]. Superstatistics uses the superposition of multiple statistical models to\n\nexplain the complex system hence the prefix \u201csuper\u201d.\n\nOne example of a superstatistics is the Heston model. We saw that this model assumes that\n\nthe asset price follows a stochastic differential equation (SDE), in particular Equation (2.48).\n\nHowever, the parameter vt of SDE follows another SDE, Equation (2.49). Thus, we have a\n\nsuperposition of statistics that explain the evolution of the asset price St.\n\nIn our case, we have a non-stationary time series: the volume-price series. This is a complex\n\nsystem that we cannot describe using ordinary statistics. We will also use an ensemble of\n\ndifferent statistics.\n\nIt was proved [1] that the volume-price series follows a log-normal distribution throughout\n\ntime as we can see Equation (3.1). If the volume-price time series would be stationary we would\n\nnot need superstatistics. However, the parameters ? and ? are not constants. They follow\n\nanother statistical model. Thus, we are going to study the evolution of these parameters with\n\nthe aim of uncovering the evolution of volume-price.\n\n2.8 The Langevin Analysis\n\nWe are interested in modelling the parameters ? and ? of the log-normal distribution of\n\nvolume-price. We are going to do this using stochastic differential equations (SDE) since we\n\nare going to find the functions D(1) and D(2) governing the Fokker-Planck equation for the\n\n19\n\n\n\nprobability density function of the parameters. The approach that we are going to follow was\n\nintroduced in 1997 [24] and reviewed by Friedrich et al. in a paper from 2011 [5].\n\nWe want to describe the log-normal parameters with a stochastic differential equation of the\n\ntype:\n\ndX\n\ndt\n= D(1)(X,t) + g(X,t)?t , (2.51)\n\nwhere X is a vector which contains our parameters and ?t is a white noise. The Langevin analysis\n\nallow us to extract directly from the data the vector of functions D(1)(X) and the matrix of\n\nfunctions D(2)(X) = g(X)gT (X).\n\nA stochastic process described by Equation (2.51) can be modeled by stochastic evolution\n\nlaws that relate the state vectors X(t) at times ti, ti+1 = ti+?, ti+2 = ti+2?, ..., for small but\n\nfinite values of ?. Here, we deal with the SDE that are defined by the following discrete time\n\nevolutions:\n\nX(ti+1) = X(ti) + D\n(1)(X(ti), ti)? + g(X(ti), ti)\n\n?\n??(ti) . (2.52)\n\nThis discrete SDE must be considered in the limit ? ? 0. We are going to explain how the\ndiscrete time processes are related to Equation (2.51).\n\nIn order to motivate the discrete time approximations, we integrate the Equation (2.51) over\n\na finite but small time increment ?.\n\nX(t + ?) = X(t) +\n\n? t+?\nt\n\nD(1)(X(s),s)ds +\n\n? t+?\nt\n\ng(X(s),s)?(s)ds (2.53)\n\n? X(t) + D(1)(X(t), t)? +\n? t+?\nt\n\ng(X(s),s)?(s)ds (2.54)\n\nThese are the quantities for which a statistical characterization can be given. We are going\n\nto interpret the integral for the wildly fluctuating stochastic quantities ?(t) in the Ito? sense:\n\n? t+?\nt\n\ng(X(s),s)?(s)ds ? g(X(t), t)\n? t+?\nt\n\n?(s)ds = g(X(t), t)\n?\n??(t) (2.55)\n\nwhere ?(t) is a stochastic variable with a standard Gaussian distribution since Wt =\n? t\n\n0\n?(s)ds\n\nand Wt+? ?Wt ?N(0,\n?\n?).\n\nWe have just discussed processes described by stochastic equations. Now we are going to\n\nsummarize the corresponding statistical description needed to the Langevin analysis.\n\nD(1) and D(2) are the drift vector and the diffusion matrix in Equation (2.41). By considering\n\nthe Ito?\u2019s definitions of the stochastic integrals, the coefficients D(1) and D(2) of the Fokker\u2013Planck\n\nequation and the coefficients of the Langevin equation are related. They are defined according\n\nto the following equations, where ?X? is the mean of the variable X:\n\nD\n(1)\ni (x,t) = lim\n\n??0\n\n1\n\n?\n&lt;Xi(t + ?) ?xi >|X(t)=x (2.56a)\n\nD\n(2)\nij (x,t) = lim\n\n??0\n\n1\n\n?\n&lt;(Xi(t + ?) ?xi)(Xj(t + ?) ?xj) >|X(t)=x (2.56b)\n\nThese equations are the discrete versions of Equations (2.39) and (2.40). They show us that\n\n20\n\n\n\nthe drift vector and the diffusion matrix are determined as the first and second moments of the\n\nconditional probability distributions in the small time limit.\n\nWe will know describe the Langevin analysis which allows us to get the drift vector and the\n\ndiffusion matrix directly from the data:\n\n\u2022 The time series are represented in a state space, i.e., the set of values that a process can\ntake.\n\n\u2022 The state space is partitioned into a set of small bins.\n\n\u2022 For each bin ?, located at point x? of the partition we consider the quantity:\n\nx(tj + ?) = x(tj) + D\n(1)(x(tj), tj)? + g(x(tj), tj)\n\n?\n??(tj) , (2.57)\n\nwhere the points x(tj) are taken from the bin located at x?.\n\nThe drift vector assigned to the bin located at x? is determined as:\n\nD(1)(x?, t) = lim\n??0\n\n1\n\n?\nM(1)(x?, t,?) (2.58)\n\non the conditional moment:\n\nM(1)(x?, t,?) =\n1\n\nN?\n\n?\nx(tj)??\n\n[x(tj + ?) ?x(tj)] (2.59)\n\nwhere N? is the number of points contained in the bin ?.\n\nThe diffusion matrix is estimated by:\n\nD(2)(x?, t) = lim\n??0\n\n1\n\n?\nM(2)(x?, t,?) (2.60)\n\non the conditional moment:\n\nM(2)(x?, t,?) =\n1\n\nN?\n\n?\nx(tj)??\n\n([(x(tj + ?) ?x(tj)) ? ?D(1)(xj, t)]2 (2.61)\n\n2.9 Statistical Tests\n\nIn this dissertation we are going to use some statistical tests that we present here in this\n\nsection.\n\nIn order to apply the Langevin Analysis, it is necessary that the time series of the fluctuations,\n\n?? and ??, are Markovian. Most of the physical phenomena can only be considered Markovian\n\nif we take a sufficient large time step. Einstein recognized this for the Brownian motion. He\n\ndiscussed the smallest time scale for which the Brownian motion can be seen as a Markovian\n\nstochastic process [25]. We call this time step the Markov length, ?M .\n\n21\n\n\n\nWe will use the Wilcoxon rank-sum test [26] to compare the conditional probabilities:\n\np(x1,?1|x2,?2) and p(x1,?1|x2,?2; x3,?3) . (2.62)\n\nWe will compute the value of t/t0, where t0 =\n?\n\n2\n?\n\nand t =\n|Q?<Q>|\n?(Q)\n\nwhere Q denotes the\n\ntotal number of inversions in the Wilcoxon test. Q is a Gaussian distributed variable with mean\n\nvalue &lt;Q > and standard deviation ?(Q). Therefore t is the absolute value of a Gaussian-\n\ndistributed random variable with mean value zero and standard deviation one. The expected\n\nvalue of t, where averaging is done with respect to x2 should be\n?\n\n2\n?\n\n. Therefore, values of t/t0\n\nclose to 1, indicates the data has the Markovian property.\n\nShapiro and Wilk\u2019s [27] W-test is a wide used and powerful test of departure from normal-\n\nity. It tests the null hypothesis that the sample x1, ...,xn comes from a gaussian distributed\n\npopulation. The test statistics is:\n\nW =\n\n(?n\ni=1 aix(i)\n\n)2?n\ni=1(xi ?x\n\n2)\n(2.63)\n\nwhere x(i) is the i-th order statistics, i.e., the i-th smallest number in the sample, x =\n?n\ni=1 xi\nn\n\nand:\n\n(a1, ...,an) =\nmTV ?1\n\n?\nmTV ?1V ?1m\n\n(2.64)\n\nwhere m = (m1, ...,mn)\nT and mi, for i = 1, ...,n is the expected value of the order statistics\n\nof independent and identically distributed random variables sampled from the standard normal\n\ndistribution, and V is the covariance matrix of those order statistics.\n\n22\n\n\n\nChapter 3\n\nGetting to know the data\n\nIn previous works, it was shown that, to the volume-price at this time scale and with this\n\ndata, the log-normal distribution had the best fit to the data among other four statistical models\n\n[28]. The probability density function of the log-normal is:\n\np(s) =\n1\n\ns\n?\n\n2??\nexp\n\n[\n?\n\n(log s??)2\n\n2?2\n\n]\n(3.1)\n\nIf volume-price is log-normal distributed with parameters ? and ? then the volume-price\n\nlogarithm has a normal distribution with mean ? and standard deviation ?. This relationship\n\nis easily observed if we plot a log-normal variable in a logarithmic scale, like we did in Figure\n\n1.1b. In this section, we are going to study the time series of the parameters ? and ? of this\n\ndistribution.\n\n3.1 Outliers and Daily Patterns\n\nWe start by removing the outliers from our data. An outlier is an observation that is distant\n\nfrom the other ones. The outliers may be the result of measuring errors. In order for us to have\n\na robust model we should discard the outliers. After we have studied our data series, we decided\n\nto consider as outliers all the data points which do not lie within five standard deviations of the\n\nmean. By doing this, we removed 33 data points from our original series.\n\nFigure 3.1: The first 27 trading days of the (a) ? and (b) ? time series after we have removed the outliers.\n\n23\n\n\n\nFigure 3.2: (a) ?ma represents the 20-day moving average pattern of parameter ? over a period of 9 days and\n(b)?ma represents the same for ?. (c) The average over all trading days of parameters ?, i.e. ?, and (d) ?, with\nthe respective fitting functions.\n\nWe can see the result from this process in Figure 3.1. We can also see that we clearly have a\n\ndaily pattern in both time series. In order to apply the Langevin analysis described in Chapter\n\n2, we will have to remove this pattern in order to obtain the time series of the fluctuations since\n\nthe Langevin analysis does not support any kind of periodicity in the data. The original series\n\n? and ? are the sum of the average daily pattern with the fluctuations around this pattern.\n\nIn Figure 3.2 we can see that both the average of ? and ? are described by the following\n\ncubic curves:\n\n?(td) = a?t\n3\nd + b?t\n\n2\nd + c?td + d? (3.2a)\n\n?(td) = a?t\n3\nd + b?t\n\n2\nd + c?td + d? (3.2b)\n\nwhere td = (t mod 144) in units of 10 min, a? = 8.216 \u00d7 10?5, b? = ?2.316 \u00d7 10?3, c? =\n?2.016\u00d710?2 and d? = 13.52 for the ? times series and a? = ?1.006\u00d710?5, b? = 5.616\u00d710?4,\nc? = ?1.324 \u00d7 10?2 and d? = 1.792 for the ? times series.\n\nIf we observe the behaviour of ? in Figure 3.2a, we can notice that the trading is heavier,\n\ni.e. we have a higher ?, at the beginning and end of the day than during the rest of the day.\n\nThe opening and the closing of the NYSE are very peculiar times: they occur after and before\n\nthe market is closed. This can explain the high volume-prices at the beginning and end of the\n\n24\n\n\n\nday.\n\nIn the beginning of the day, the volume-price series has high values. This happens because\n\nour time series is not continuous. We have a time gap between the end of one day and the\n\nbeginning of the next day. Information arrives during this period and the traders will have\n\ndifferent opinions about what is going to happen in the morning. There is a lot of speculation.\n\nTherefore, when the Stock Market opens, there will be a huge quantity of money transactions.\n\nNotice that the volume-price is essentially the amount of money which is being traded. Everyone\n\nwants to sell or to buy fast so they can make more money than the others. Time is money so\n\nthe faster you give your order, the higher profit you will have. As time passes by, the money\n\ntraded will decrease. Now we do not have a urgency to sell or to buy.\n\nHowever, after lunch time, we see an alteration in this pattern. Now the volume-price series\n\nstarts to grow again. At the closing time, traders anticipate price changes overnight that can\n\nalter their portfolios, so they exchange larger amounts of money. Besides that, there are index\n\nmutual fund managers who need to make trades at the closing prices for administrative purposes\n\nand there are short sellers and hedgers who frequently close and hedge their positions at the end\n\nof the day. The volume-price increases at the end of the day at a higher rate than it decreases\n\nin the beginning of the day and the closing value is typically grater than the opening value.\n\nThis may happen because there is a deadline (i.e. the closing time) that we do not have in the\n\nbeginning of the day.\n\nThe volume traded has a bigger impact in the shape of the volume-price series than the price.\n\nBesides that, the series of volume-price inherit the oscillation-like structure from the series of\n\ntrading volumes that we can see in Figure 1.1a. Rocha [1] showed that the correlation between\n\nthe volume and the volume-price is approximately 0.8. Therefore we can admit that the pattern\n\nfollowed by ? and ? in the volume-price series is approximately the same that we would see in\n\nthe volume series.\n\nThe U-Shaped pattern followed by ? is long known in the finances world [29, 30], it is very\n\ntypical and it has been documented in various studies. This pattern is a characteristic of the\n\nvolume series. However, we saw that the correlation between the volume and volume-price series\n\nis almost one. Therefore, it is expected that the volume-time series follows the same pattern.\n\nVarious authors gave different reasons for the existence of this pattern. Admati and Pfleiderer\n\n[29] argue that high volume in a particular time segment reveals the presence of asymmetric\n\ninformation. Brock and Kleidon [31] defend that different trading strategies at the open and\n\nclose of the markets form these volume patterns.\n\nWe can also see in Figure 3.2d the pattern followed by ?. We have to remember that ?\n\naccounts for the standard deviation of the volume-price series logarithm. In the beginning of\n\nthe day there is a great variance in our data. This may happen because the traders have different\n\nperceptions about what is going to happen. As the time goes by, the standard deviation starts to\n\ndecrease, first slowly and then faster. Finally, the standard deviation is very low at the end of the\n\nday. Apparently the traders exchange similar amounts of money. Maybe this happens because\n\nof the information received during the day. At the beginning of the day, people have different\n\ninformations so they adopt different strategies. However, as the time passes, the information\n\nand the observation of the NYSE behaviour during that day leads the different traders to similar\n\nstrategies.\n\nThese patterns, ? and ?, happen every day and they are easily modelled by Equations (3.2).\n\nHowever, the fluctuations around these patterns, ?? and ??, need to be addressed more carefully\n\n25\n\n\n\nFigure 3.3: The fluctuations time series of the first 27 trading days: (a) ?? and (b) ??.\n\nsince they have a strong stochastic behaviour. We will study the time series of the fluctuations\n\nin the next section.\n\n3.2 Log-Normal Parameter Fluctuations\n\nWe got the time series of the fluctuations by subtracting the 20-day moving average pattern\n\n(Figure 3.2) from the data without the outliers (Figure 3.1). In each day, we subtracted the\n\npattern from the 20 days before that day. We got Figure 3.3.\n\nIn order to make sure that we have removed all the periodicity from our time series, we\n\nbuild a power spectrum for both time series, before and after the process of removing the daily\n\npatterns, using the Fast Fourier Transform.\n\nWhen the time series are viewed in the form of a frequency spectrum, certain aspects of the\n\nunderlying processes are revealed. If the frequency spectrum include distinct periodic peaks, we\n\nmay infer that the original processes have some kind of periodicity.\n\nIn Figures 3.4a and 3.4b we can see the power spectrum in a log-log scale. The solid lines\n\nare the power spectrum before we have removed the daily patterns. We can see some peaks\n\nthat confirm our idea that the original time series are periodic. The power spectrum for the\n\nfluctuations is represented by the dots in the same picture. It is clear, for both time series, that\n\nthe peaks observed in the solid lines disappeared. We have removed efficiently the periodicity\n\nfrom our time series.\n\nWe also computed the autocorrelation function, ?, in a log-lin scale (see Figures 3.4c and\n\n3.4d) for the ?? and ?? fluctuations time series to check the correlation between values of the\n\nseries at different times. The ACF ? has an exponential decay:\n\n26\n\n\n\nFigure 3.4: (a) The power spectrum of the ? and (b) ? time series is represented by the solid lines. The power\nspectrum of the (a) ?? and (b) ?? fluctuations is represented by the dots. (c) Autocorrelation function (dots) and\nlinear function fitted to the data (dashed line) in a log-lin scale for the ?? time series and for the (d) ?? time series.\n??,? represents the slope of the line fitted to the data.\n\n??,? = ??,?e\n? ?\n??,? (3.3)\n\nTherefore the logarithm of the ACF is a line. We fitted linear functions to both ACF. We\n\ngot a R2 bigger than 90% in both cases so we have a good fit. For ??, we have 1\n??\n\n= 0.0192 and\n\nlog(??) = ?0.8496. For ??, we have 1?? = 0.0132 and log(??) = ?0.7776. The\n1\n??,?\n\nconstants\n\n0.0192 and 0.0132 have units that are the inverse of the time because the exponential exponent\n\ncannot have units. Therefore, if we take the inverse of these constants, i.e. ?? = 52.08 and\n\n?? = 75.76 we have the characteristic time for both series after which the process is uncorrelated,\n\nor, in other words, the characteristic time after which the processes have no memory.\n\nWe can see the probability density functions (PDF) of the fluctuations ?? and ?? in Figure\n\n3.5. At first sight, these probability density functions appear to be normally distributed. We\n\ntested this hypothesis using the Shapiro-Wilk Normality test. According to Royston [32], an\n\napproximate p-value below 0.1 is enough to reject the null hypothesis of normality. For both\n\ntime series, the Shapiro-Wilk test rejected the normality hypothesis.\n\nAlthough the Shapiro-Wilk test rejectes the assumption of normality, we still plotted in\n\nFigure 3.5 the probability density function of the fluctuations ?? and ?? in a log-lin scale (dots)\n\nand the adjusted Gaussian PDF (dashed line), i.e., a Gaussian probability density function with\n\nthe same mean and standard deviation as our data. We have a good fit for both time series in the\n\n27\n\n\n\nFigure 3.5: (a) Marginal probability density function (solid line) and Gaussian adjusted PDF (dashed line) of the\nfluctuations ?? and (b) ?? in a log-lin scale.\n\ncentral region of the PDF. The marginal PDF of the ?? series has an almost perfect superposition\n\nwith the Gaussian PDF. However, the fit in the tails is not so good. For the ?? time series, we\n\nalso have a good fitting although it is not as good as for ??. The skewness is almost zero in the\n\nempirical data. However, the kurtosis is approximately 6. This value is very high. Although the\n\nkurtosis has been associated to the \u201cpeakedness\u201d of the distribution, Westafall [33] proved that\n\nthe kurtosis has only an unambiguous interpretation in terms of tail extremity. A high kurtosis\n\n(like the one found in our data) draw our attention to the existence of outliers since there are\n\noutliers that contribute meaningfully to the computation of the kurtosis. In our data, we have\n\nfatter tails than it was expected. This can be related to the process of removing the outliers.\n\nIt looks like some outliers have still remained in our data. Despite this, a Gaussian model is a\n\ngood approach to study the evolution of the fluctuations ?? and ??.\n\nWe have been studying each time series separately. However it is also important to see if\n\nthere is any kind of correlation between our variables. We computed the covariance matrix ?,\n\nwhich has components ??? = 0.0619, ??? = 0.0039, ??? = ??? = ?0.0036. Besides this, we\ncomputed a correlation coefficient of -0.2311 between the two variables which show us that our\n\nvariables are indeed correlated. Correlation and independence are different concepts. If we have\n\nzero correlation that does not imply independence. However, if the correlation is different from\n\nzero then we cannot have independence. In our case, the correlation coefficient is small. One can\n\nargue that it is almost zero. But one can also argue that the variables are negatively correlated.\n\nIn Figure 3.6a we plotted the joint probability density function of the fluctuations time series,\n\n?? and ??. In order to make a comparison, we also plotted in Figure 3.6b a multivariate normal\n\n28\n\n\n\nFigure 3.6: (a) Joint PDF of the empirical time series and (b) a multivariate normal distribution with mean\nvector and covariance matrix equal to the ones of our empirical data. (c) Contour plot for the ?? and ?? time\nseries and (d) for a multivariate normal distribution with mean vector and covariance matrix equal to the ones of\nour empirical data.\n\ndistribution with mean vector and covariance matrix equal to the ones from the fluctuations\n\ntime series. From these figures, we can see that the two plots are remarkably alike.\n\nThe PDF of the multivariate normal distribution represented in Figure 3.6b is:\n\np(x) =\n1\n\n2?\n?\n|?|\n\nexp\n\n(\n?\n\n1\n\n2\n(x?\u00b5)T ??1(x?\u00b5)\n\n)\n(3.4)\n\nwhere x = (?,?), |?| = ????????2?? is the determinant of the matrix ? and \u00b5 is a 2-dimensional\n\n29\n\n\n\nvector of zeros since the mean of ?? and ?? is approximately zero.\n\nWe also draw in Figures 3.6c the contour plots of the fluctuations and (d) the contour plots\n\nof the multivariate normal. We can notice that both contour plots area leaning towards the\n\nleft which indicates a negative correlation between the variables. It is important to notice that\n\nthe contour plot of the multivariate normal is not elliptical (as it was expected) because we\n\nsimulated data from a multivariate normal and, after that, we build the joint distribution and\n\nthe contour plot.\n\n3.3 Markov Tests\n\nTo test the Markovianity of the series, we apply the Wilcoxon test to compare the distribu-\n\ntions p(x1,?1|x2,?2) and p(x1,?1|x2,?2; x3,?3) for a fixed value of x3. We are also going to infer\nthe Markov length, ?M .\n\nIn Figures 3.7 we can see that the values for the quotient t/t0 are close to 1 for both time\n\nseries for all values of ?. Thus, this suggests a Markov length ?M = 600s = 10min which is our\n\ntime scale.\n\nFigure 3.7: (a) Wilcoxon test to verify the Markovian property of the ?? time series and (b) the ?? time series,\nshowing the Markov length of ?M = 600s.\n\n30\n\n\n\nChapter 4\n\nThe Langevin Analysis\n\n4.1 A Simple Model without Correlation\n\nIn this section we will implement the one dimensional Langevin Approach [34] to the two\n\nfluctuations time series represented in Figure 3.3. It is true that our time series are not indepen-\n\ndent. However, the correlation coefficient is small, namely ? = ?0.2311, so we will try to test\na simple model in our data. Usually, the simplest models are the ones people use more often,\n\neven though they are not the ones that describe reality the best (one flagrant example is the\n\nBlack-Scholes model).\n\nWe are going to assume this time series to be independent from each other. We use the\n\nroutine \u201cLangevin1D\u201d applied to the fluctuations as described in Chapter 2. This routine\n\nretrieves M(1) and M(2) from Equations (2.59) and (2.61) which are essentially the means of the\n\nfirst and second conditional moment, in each bin and for each time step ?. We want to derive\n\nfrom our data the functions D(1) and D(2) that govern the following equations:\n\nd??(t) = D(1)(??)dt +\n?\nD(2)(??)dWt (4.1a)\n\nd??(t) = D(1)(??)dt +\n?\nD(2)(??)dWt (4.1b)\n\nThese are the derivatives with respect to time of M(1) and M(2). Thus, we computed the\n\nquotient M\n(n)\n\n?\n[35] for each bin and for n = 1, 2. In Figure 4.1 we can see this quotient for the\n\nFigure 4.1: (a) Computation of D(1) and (b) D(2), in the bin that contains the mean, as the intersection of the\nlinear fit with ? = 0.\n\n31\n\n\n\nbin that contains the mean. We can see that M(n) is linear when we consider the time steps\n\nbetween 10 and 20. Thus, to find the derivatives with respect to time D(n) we just have to make\n\na linear fit toM\n(n)\n\n?\n(when the time steps are between 10 and 20) and see for which value that\n\nline intersects ? = 0. That number is going to be our D(n) in that bin. If we do this for all the\n\nbins, we will have the D(n) function.\n\nNote that we had to use this proxy because, apparently:\n\nlim\n??0\n\nM(n)\n\n?\n= +? . (4.2)\n\nThis may happen because the time series have measurement noise which makes the empirical\n\nlimit to diverge or simply because of round-off errors [36]. In order to surpass this, we use the\n\nfit to a linear region before the limit starts diverging.\n\nApplying this methodology to all bins from our time series we get Figure 4.2. Now it is\n\npossible to write the Langevin equations for our time series using the values we obtained with the\n\nLangevin analysis as we can see in Equations (4.1), where D\n(1)\n? = ?0.08?, D\n\n(2)\n? = 0.006 + 0.07?\n\n2,\n\nD\n(1)\n? = ?8.0 \u00d7 10\n\n?2? and D\n(2)\n? = 3.9 \u00d7 10\n\n?4 ? 7.9 \u00d7 10?4? + 5.6 \u00d7 10?2?2.\n\nFigure 4.2: (a-d) D1 and D2 functions to the ?\n? (left) and ?? (right) time series. (e-f ) Quotient D(1)/\n\n?\nD2 for ?\n\n?\n\nand ??, respectively.\n\n32\n\n\n\nFigure 4.3: (a) D4 function to the ?\n? and (b) ?? time series.\n\nThe D(1) coefficient is linear for both the ?? and ?? time series. This means we have an\n\noscillator with a string constant that corresponds to the line\u2019s slope. The string has a fixed\n\npoint in zero since the line contains the origin of the referential.\n\nIn Figure 4.2e and Figure 4.2f we plotted the quotient D(1)/\n?\nD(2) to check if the order of\n\nmagnitude of the functions that govern the deterministic and stochastic part of Equations (4.1)\n\nis the same. That quotient varies between ?0.3 and 0.3 for both time series, which means that?\nD(2), i.e., the coefficient of the stochastic part, has a heavier weight in the Equations (4.1)\n\nthan D(1), the coefficient of the deterministic part.\n\nWe also computed the D(4) function for both time series. The results obtained are in Figure\n\n4.3. Here we can see that D(4) is almost zero for both time series. According to the Pawula\n\nTheorem [37], if D(4) is zero or very small when compared to D(1) and D(2), then we can stop\n\nthe Kramers-Moyal expansion at n = 2 since the terms of bigger order are zero.\n\n4.2 Modelling the Coupling between ?? and ??\n\nSince our time series are not independent, we are going to perform the Langevin Analysis\n\n2D described in Chapter 2, i.e., assuming there exists dependence between the two time series.\n\n33\n\n\n\nThe routine gives us the values of M(1) and M(2), like we saw in the previous section. Thus,\n\nwe need to compute D(1) and D(2). However, we do not have a linear behaviour of M\n(n)\n\n?\nlike\n\nwe had when we assumed that ?? and ?? were independent from each other. This may happen\n\nbecause our sampling frequency is too high. We are going to use as a proxy the value of M(n)(x)\n\nin ? = 1.\n\nFigure 4.4: Functions from the drift vector and diffusion matrix obtained by the routine Langevin2D: (a) D\n(1)\n? ,\n\n(b) D\n(1)\n? , (c) D\n\n(2)\n?? , (d) D\n\n(2)\n?? , (e) D\n\n(2)\n?? .\n\nAn interval of 10 minutes in the NYSE is huge, considering there are transactions happening\n\nin a microsecond scale. When we have a Markov process, the conditional moments change in a\n\nlinear way to small values of ?. Then, for larger values of ?, they start deviating from linearity.\n\n34\n\n\n\nFigure 4.5: The three components of the g matrix: (a) g??, (b) g??, (c) g??.\n\nThis may be happening in our data but we cannot notice because of the frequency of our time\n\nseries. Maybe we have this linear behaviour for a ? &lt;10min. But our ?1 = 10min so we do\n\nnot notice this linear behaviour. One way of getting over this problem is to approximate the\n\nD(n)(x) by the value of M(n)(x) in ? = 1.\n\nWe can be making an approximation error (or not). We do not know how the variation takes\n\nplace in the linear part. If the linear part is immediately below the 10 minutes, then we should\n\nhave a good approximation. However, if the linear variation is in the order of some seconds,\n\nthen the first 10 minutes are very far from the linear part and our approximation may not be\n\nvery good. Nevertheless, this is the best estimative that we can make with this data. It is better\n\nthan to use the other values of ?.\n\nIn Figure 4.4 we plotted the functions from the drift vector and diffusion matrix (notice that\n\nthis matrix is symmetric) obtained by the routine Langevin2D [34]. For simplicity we are going\n\nto suppress the prime symbol noticing that we only address the fluctuations. Thus, in the two\n\ndimensional case we have a system of coupled Langevin equations that govern the evolution of\n\n? and ?:\n\n?\n????\nd?(t)\n\nd?(t)\n\n?\n???? =\n\n?\n????\nD?\n\n(1)(?,?)\n\nD?\n(1)(?,?)\n\n?\n????dt +\n\n?\n????\ng??(?,?) g??(?,?)\n\ng??(?,?) g??(?,?)\n\n?\n????\n?\n????\ndW\n\n(1)\nt\n\ndW\n(2)\nt\n\n?\n???? (4.3)\n\n35\n\n\n\nwhere ggT = D(2) with:\n\nD(2) =\n\n?\n????\nD\n\n(2)\n?? D\n\n(2)\n??\n\nD\n(2)\n?? D\n\n(2)\n??\n\n?\n???? . (4.4)\n\nWe fitted polynomials of degree one to the functions from the drift vector and polynomials\n\nof degree two to the functions from the diffusion matrix:\n\nD(1) ? a + b? + c? (4.5a)\n\nD(2) ? a + b? + c? + d?2 + e?? + f?2 (4.5b)\n\nwhose coefficients are presented in the next table:\n\nTerm 1 ? ? ?2 ?? ?2\n\nD\n(1)\n? -0.0085 -0.7143 0.2812\n\nD\n(1)\n? -0.0031 0.0293 -0.5023\n\nD\n(2)\n?? -0.1233 0.1107 0.3563 0.9013 0.7350 5.8615\n\nD\n(2)\n?? -0.0017 0.0037 -0.0104 0.0059 -0.0186 0.5253\n\nD\n(2)\n?? -0.0081 -0.0046 -0.0267 -0.0222 0.4385 -0.1901\n\nTable 4.1: Coefficients for the D(1) and D(2) functions, obtained by the Langevin analysis.\n\nIn order to have all of the coefficients of Equation (4.3), we have to compute the g matrix.\n\nThis matrix g is not unique. If we have an orthogonal matrix Q and ggT = D(2) then h = gQ\n\nalso satisfies hhT = D(2) since\n\nhhT = (gQ)(gQ)T = gQQTgT = ggT = D(2) (4.6)\n\nFor now, we will just compute one possible matrix that satisfies this property. If the matrix\n\nD(2) is diagonalizable then we know that there exists an invertible matrix P satisfying:\n\nPD(2)P?1 = d (4.7)\n\nwhere d is a diagonal matrix that contains the eigenvalues of D(2) in the main diagonal and P\n\nis a matrix whose columns are the eigenvectors of D(2). Now it is easy to show that\n\ng = P?1\n?\ndP (4.8)\n\nwhere\n?\nd is the matrix obtained after taking the square root of each element of the diagonal\n\n36\n\n\n\nmatrix d. Notice that:\n\nggT = (P?1\n?\ndP)(P?1\n\n?\ndP)T = P?1\n\n?\ndPP?1\n\n?\ndP = P?1dP = D(2) (4.9)\n\nsince P?1 = PT because symmetric matrices have orthogonal eigenvectors.\n\nAfter implementing this procedure we obtained the three components of the g matrix (notice\n\nthat this matrix is symmetric) and we fitted quadratic forms to this components:\n\ng ? a + b? + c? + d?2 + e?? + f?2 (4.10)\n\nwhose coefficients are presented in the next table:\n\nTerm 1 ? ? ?2 ?? ?2\n\ng?? 0.2185 0.0918 0.2255 0.4850 0.2925 4.0541\n\ng?? 0.0360 0.0174 -0.0128 0.0210 0.0245 1.5197\n\ng?? -0.0111 -0.0051 -0.0158 -0.0134 0.2936 -0.1835\n\nTable 4.2: Coefficients for the g functions.\n\nWe can see in Figure 4.5 the empirical results obtained and the surfaces fitted to this data.\n\n37\n\n\n\n38\n\n\n\nChapter 5\n\nApproaching Non-Stationarity\n\nAfter introducing our framework in Chapters 3 and 4 and applying it to the volume-price\n\nseries, we now deduce the formula of all the moments E [sn] ,n = {0, 1, 2, ...} of the log-normal\ndistribution. We are going to use the notation ?sn? with the same meaning as E [sn]. It is a well\nknown statistical result [38] that, if we have all the moments from a distribution, we can deduce\n\nits probability density function using a Fourier transform. It is possible to have a closed-form for\n\nthe nth-moment of s, since s follows a log-normal distribution whose PDF is given by Equation\n\n(3.1). The moments ?sn?,n = {0, 1, 2, ...} of our distribution are given by:\n\n?sn? =\n? +?\n??\n\nsnp(s,?(t),?(t))ds = en?+\nn2?2\n\n2 . (5.1)\n\nAssuming that all time dependency is incorporated in the distribution parameters ? and\n\n? one is able to fully characterize the non-stationary time series of the volume-price: one just\n\nneeds to model the ? and ? evolution through time. Indeed, these parameters are the sum of a\n\ndaily average pattern ? and ? with the fluctuations ?? and ??:\n\n? = ? + ?? (5.2a)\n\n? = ? + ?? (5.2b)\n\nThe daily patterns ? and ? are the ones observed in Figure 3.2 and can be described by\n\nthe expressions in Equations (3.2). Since we already have the expressions for the daily patterns\n\n? and ?, now we need to describe the fluctuations ?? and ??. In our model we are going to\n\nassume that these fluctuations obey the system of Langevin equations given in Equation (4.3).\n\nFor simplicity we are going to suppress the prime symbol noticing that we only address the\n\nfluctuations.\n\nIn order to integrate these equations for the parameters fluctuations, we will have to use\n\ntheir discrete versions in the Ito?\u2019s description, namely:\n\n?(t + ?t) = ?(t) + D\n(1)\n? (?(t),?(t))?t + g11(?(t),?(t))\n\n?\n?t r1 + g12(?(t),?(t)\n\n?\n?t r2 (5.3a)\n\n?(t + ?t) = ?(t) + D\n(1)\n? (?(t),?(t))?t + g21(?(t),?(t))\n\n?\n?t r1 + g22(?(t),?(t))\n\n?\n?t r2 (5.3b)\n\nwhere r1 and r2 are random numbers from a Gaussian distribution with mean zero and standard\n\ndeviation 1.\n\nHaving generated a sample of fluctuations, we then add the daily patterns in Equations (3.2)\n\n39\n\n\n\nFigure 5.1: (a) Time series of empirical ?s? and (b) corresponding modelled series. Therefore, (c-d) ?s2?, (e-f )\n?s3? and (g-h) ?s4?. Inside each plot, there is a sub-plot with the corresponding entire series.\n\nand obtain the modelled time series for ?mod and ?mod. Inserting ?mod and ?mod in Equation\n\n(5.1) yields the modelled volume-price moments.\n\nWe next compare the modelled time series of the first four moments ?sn?, n = 1, ..., 4, with\nthe empirical ones which are obtained by replacing in Equation (5.1) the original time series of\n\n? and ? represented in Figure 3.1.\n\nIn Figure 5.1 we have the series obtained from our model versus the empirical ones, for the\n\nfirst four moments. If we look to the entire time series, we can see that for n = 1, 2, our model\n\ncan explain the extreme events. However, for n = 3, 4, we have far more extreme events in the\n\nempirical series than in the modeled ones. When we look to the zoom in Figure 5.1, we can see\n\nthe same pattern in both series. In our model, the extreme events do not happen at the same\n\ntime as in the empirical ones, but they will eventually happen.\n\nIn Figure 5.2 we have the empirical and modelled probability density functions for the fluc-\n\ntuations of ? and ?. We can see that our model can explain better the ? fluctuations than the\n\n? fluctuations. In Figure 5.2a we see that the densities are quite close, specially in the central\n\n40\n\n\n\nFigure 5.2: (a) PDFs for the empirical fluctuations of ? (dashed line) and for the modelled fluctuations (solid\nline). (b) PDFs for the empirical fluctuations of ? (dashed line) and for the modelled fluctuations (solid line).\n(c-f ) PDFs for the ?sn? time series. The dashed line represents the empirical PDF and the solid line is the PDF\nobtained from our model.\n\nregion. However, in Figure 5.2b we do not have a fit as good since it is easier to model means\n\nthan standard deviations. This can be explained noticing that ? is a first-order moment while\n\n? is the square root of a second-order central moment.\n\nIn Figure 5.2(c-f) we plotted the empirical and theoretical probability distributions of ?sn?,n =\n1, ..., 4. Our model has a good fit in the first moments and can be used to model them since\n\nthe theoretical and empirical distributions are very close to each other. Since the ? parameter\n\nis better modelled than the ? parameter, it is expected that for higher moments, when ? is\n\ndominant over ?, we do not achieve such good results.\n\n41\n\n\n\n42\n\n\n\nChapter 6\n\nDiscussion and Conclusions\n\nThe main goal of this dissertation was to model the non-stationary time series of the volume-\n\nprice. By assuming that the log-normal had the best fit to the data in each 10-minutes window,\n\nthis goal resumes to the one of studying the parameters ? and ? of this distribution, which\n\nare themselves stochastic variables. We were able to show that we can describe the time series\n\nof these parameters by decomposing the variables as a sum of two terms: one accounting for\n\nthe daily pattern and another regarding the fluctuation around that average pattern. The\n\nfluctuations are modelled using a system of Langevin equations whose coefficients we retrieved\n\nfrom our empirical data. From here, we proposed a framework to reconstruct the evolution of\n\nall the moments of the volume-price distribution.\n\nOur model reproduces well the first moments, being therefore suitable to study the volume-\n\nprice of the stocks from the NYSE. It would be interesting if, in the future, we conduct tests to\n\nsee if this framework can be used to make previsions about the evolution of the volume-price.\n\nIn particular, it could be a suitable approach for the calculation of the Value at Risk (VaR) of\n\na stock\u2019s portfolio.\n\nThis work leaves some open questions to be answered. It is true that we achieved a good\n\nmodel to the ? fluctuations, but we could not match this result to the ? fluctuations. One\n\npossible explanation is related with the outliers: we removed all the points which did not lie in a\n\n5? interval from the mean. However, when we plotted the time series without the outliers, there\n\nwere still some extreme values that look more like measurement errors than fluctuations as we\n\ncan see in Figure 3.1. We chose to use the 5? criterion because we tried to minimize the number\n\nof points taken from our sample in order to let our data as close as possible to the original one.\n\nHowever, if one prefers to choose a stricter criteria, like using a 3? interval, then the time series\n\nwould have lesser outliers and maybe the results would be more easily modelled.\n\nThere are many models in the literature that enable us to study and to model stochastic\n\ntime series such as autoregressive models [39], moving-average models [40] and autoregressive\n\nintegrated moving average models [41]. One may ask, why did we choose the Langevin model\n\ninstead of all the others. One strong argument in favour of this model is that it not only allows us\n\nto describe the evolution of our time series, but it may also give us an equation, Fokker-Planck\n\nequation, to describe the evolution of the volume-price distribution. Further work should be\n\ndone in trying to extract such an equation from the equations we already have. If one is able to\n\ndo this, then we would have much more information about the volume-price evolution and we\n\ncould apply this information to the computation of the Value at Risk or other risk measures.\n\nA comparison between our model and the classic models that have been used for studying\n\n43\n\n\n\ntime series would be an interesting work to develop in the future. It is true that our model\n\nhas the advantage already stated of being able to produce an equation to the evolution of the\n\ndistribution of the volume price. However, the results achieved by our model may be indeed\n\nbetter than the ones achieved by the classical models. In order to test this hypothesis, we should\n\ndo this comparative study.\n\nAnother question that raises from the main findings of this dissertation is related with\n\nthe sampling frequency of our data. In the NYSE, where transactions are happening at the\n\nmicrosecond scale, 10 minutes is a very large interval. If we were able to redo this analysis\n\nwith a smaller sampling frequency, maybe we would be able to extract stochastic differential\n\nequations which explain better the behaviour of the fluctuations in our data.\n\nWe proposed a model that is capable of modeling the central region of the volume-price\n\ndistribution. However, it does not have a good fit in the tails. It would be interesting to develop\n\na two dimensional model to describe the behaviour of the tails. Rocha [1] had already described\n\nthe tails of the volume-price as a one parametric inverse gamma distribution. But if one is able\n\nto fit a two parametric model to the tails, then it may yield better results. After this, it would\n\nbe a good idea to combine these two models and see if we get a better fit to the real data.\n\nFinally, this work gave us important insight in the study of non-stationary time series and\n\nwe have proposed here a methodology that is going to be very useful in numerous fields. This\n\nframework is general enough to be applied to other markets besides the NYSE and also to other\n\nfields of study like physiology, when we are trying to study the heart interbeat intervals or\n\ngeology, in order to study seismic time series.\n\n44\n\n\n\nBibliography\n\n[1] P. Rocha, \u201cStochastic evolution of parameters defining probability density functions: Ap-\n\nplication to the new york stock market,\u201d Master\u2019s thesis, Faculty of Sciences of Lisbon\n\nUniversity, 2014.\n\n[2] H. Furstenberg, Stationary Processes and Prediction Theory. Princeton University Press,\n\n1960.\n\n[3] P. Rocha, F. Raischel, J. Cruz, and P. Lind, in 3rd SMTDA Conference Proceedings, 2015,\n\npp. 619\u2013627.\n\n[4] P. Rocha, F. Raischel, J. Boto, and P. Lind, Journal of Physics: Conference Series, vol.\n\n574, 012148, 2014.\n\n[5] R. Friedrich, J. Peinke, M. Sahimi, and M. Tabar, \u201cApproaching complexity by stochastic\n\nmethods: From biological systems to turbulence,\u201d Physical Review, vol. 506, p. 87, 2011.\n\n[6] P. Laplace, A philosophical essay on probabilities. New York: J. Wiley, 1902.\n\n[7] R. Brown, \u201cA brief account of microscopical observations made in the months of june, july\n\nand august, 1827, on the particles contained in the pollen of plants; and on the general\n\nexistence of active molecules in organic and inorganic bodies.\u201d Philosophical Magazine,\n\nvol. 4, p. 161\u2013173, 1828.\n\n[8] L. Bachelier, \u201cThe?orie de la spe?culation,\u201d Annales Scientifiques de l\u2019 E?cole Normale\n\nSupe?rieure, vol. 3, no. 17, pp. 21\u201386, 1900.\n\n[9] A. Einstein, \u201cU?ber die von der molekularkinetischen theorie der wa?rme geforderte bewegung\n\nvon in ruhenden flu?ssigkeiten suspendierten teilchen,\u201d Annalen der Physik, vol. 17 (8), p.\n\n549\u2013560, 1905.\n\n[10] D. Lamberton and B. Lapeyre, Introduction to Stochastic Calculus Applied to Finance.\n\nChapman and Hall/CRC, 2008.\n\n[11] F. Beichelt, Stochastic Processes in Science, Engineering and Finance. New York: Chap-\n\nman and Hall/CRC, 2006.\n\n[12] M. Vetterli, J. Kovacevic, and V. Goyal, Foundations of Signal Processing. Cambridge\n\nUniversity Press, 2014.\n\n[13] J. Hunter, Lecture Notes on Applied Mathematics: Methods and Models. University of\n\nCalifornia, 2009. [Online]. Available: www.math.ucdavis.edu/?hunter/m280 09/ch.pdf\n\n45\n\nwww.math.ucdavis.edu/~hunter/m280_09/ch.pdf\n\n\n[14] F. Yilmaz, H. O?z, and G. Weber, \u201cIto?-Taylor expansions for systems of stochastic differential\n\nequations with numerical applications,\u201d 2013.\n\n[15] L. Arnold, Stochastic Differential Equations: Theory and Applications. New York: J.\n\nWiley, 1974.\n\n[16] F. Black and M. Scholes, \u201cThe pricing of options and corporate liabilities,\u201d Journal of\n\nPolitical Economy, vol. 81(3), pp. 637\u2013654, 1973.\n\n[17] R. Merton, \u201cTheory of rational option pricing,\u201d The Bell Journal of Economics and Man-\n\nagement Science, vol. 4(1), pp. 141\u2013183, 1973.\n\n[18] J. Jackwerth and M. Rubinstein, \u201cRecovering stochastic processes from option prices,\u201d\n\nContemporary Studies in Economic and Financial Analysis, vol. 94, 2012.\n\n[19] R. Schmalensee and R. Trippi, \u201cCommon stock volatility expectations implied by option\n\npremia,\u201d Journal of Finance, vol. 33, pp. 129\u2013147, 1978.\n\n[20] P. Dennis and S. Mayhew, \u201cRisk-neutral skewness: Evidence from stock options,\u201d Journal\n\nof Financial and Quantitative Analysis, vol. 37, pp. 471\u2013493, 2002.\n\n[21] J. Cox, \u201cNotes on option pricing I: Constant elasticity of variance diffusions,\u201d Unpublished\n\nDraft, 1975.\n\n[22] S. Heston, \u201cA closed-form solution for options with stochastic volatility with applications\n\nto bond and currency options,\u201d The Review of Financial Studies, vol. 6(2), pp. 327\u2013343,\n\n1993.\n\n[23] C. Beck, \u201cSuperstatistics: Theoretical concepts and physical applications,\u201d in Anomalous\n\nTransport. Wiley-VCH Verlag GmbH and Co. KGaA, 2008.\n\n[24] R. Friedrich and J. Peinke, \u201cDescription of a turbulent cascade by a fokker-planck equation,\u201d\n\nPhysical Review Letters, vol. 78 (863), 1997.\n\n[25] R. Stresing, D. Kleinhans, R. Friedrich, and J. Peinke1, \u201cDifferent methods to estimate the\n\nEinstein-Markov coherence length in turbulence,\u201d Physical Review E, vol. 83, 046319, 2011.\n\n[26] C. Renner, J. Peinke, and R. Friedrich, \u201cExperimental indications for markov properties of\n\nsmall-scale turbulence,\u201d Journal of Fluid Mechanics, vol. 443, pp. 383\u2013409, 2001.\n\n[27] S. Shapiro and M. Wilk, \u201cAn analysis of variance test for normality (complete\n\nsamples),\u201d Biometrika, vol. 52, no. 3-4, pp. 591\u2013611, 1965. [Online]. Available:\n\nhttp://biomet.oxfordjournals.org/content/52/3-4/591.short\n\n[28] P. Rocha, F. Raischel, J. Boto, and P. Lind, \u201cUncovering the evolution of nonstationary\n\nstochastic variables: The example of asset volume-price fluctuations,\u201d Physical Review E,\n\nvol. 93,052122, May. [Online]. Available: link.aps.org/doi/10.1103/PhysRevE.93.052122\n\n[29] A. Admati and P. Pfleiderer, \u201cA theory of intraday patterns: Volume and price variability,\u201d\n\nThe Review of Financial Studies, vol. 1, pp. 3\u201340, 1988.\n\n46\n\nhttp://biomet.oxfordjournals.org/content/52/3-4/591.short\nlink.aps.org/doi/10.1103/PhysRevE.93.052122\n\n\n[30] P. Jain and G. John, \u201cThe dependence between hourly prices and trading volume,\u201d Journal\n\nof Financial and Quantitative Analysis, vol. 23, pp. 269\u2013284, 1986.\n\n[31] W. Brocka and A. Kleidon, \u201cPeriodic market closure and trading volume: A model of\n\nintraday bid and asks,\u201d Journal of Economic Dynamics and Control, vol. 16, pp. 451\u2013489,\n\n1991.\n\n[32] J. Royston, \u201cRemark AS R94: A remark on algorithm AS 181: The W test for normality,\u201d\n\nApplied Statistics, vol. 44, pp. 547\u2013551, 1995.\n\n[33] P. Westfall, \u201cKurtosis as peakedness, 1905-2014 R.I.P.\u201d The American Statistician, vol. 68,\n\npp. 191\u2013195, 2014.\n\n[34] P. Rinn, P. Lind, M. Wachter, and J. Peinke, \u201cLangevin: An R package for stochastic data\n\nanalysis,\u201d Journal of Open Research Software, 2016.\n\n[35] P. Lind, M. Haase, F. Bo?ttcher, J. Peinke, D. Kleinhans, and R. Friedrich, \u201cExtracting\n\nstrong measurement noise from stochastic series: applications to empirical data,\u201d Physical\n\nReview E, vol. 81, 041125, 2010.\n\n[36] E. Darulova?, \u201cProgramming with numerical uncertaintie,\u201d Ph.D. dissertation, E?cole Poly-\n\ntechinque Fe?de?rale de Lausanne, 2014.\n\n[37] H. Risken, Fokker-Planck Equation. Berlin: Springer, 1984.\n\n[38] S. Rabbani, \u201cProbability density function in terms of moments,\u201d 2008. [Online]. Available:\n\nhttp://srabbani.com/moments2.pdf\n\n[39] G. Yule, \u201cOn a method of investigating periodicities in disturbed series, with special ref-\n\nerence to wolfer\u2019s sunspot numbers,\u201d Philosophical Transactions of the Royal Society of\n\nLondon, vol. 226, p. 267\u2013298, 1927.\n\n[40] W. Enders, Stationary Time-Series Models. Applied Econometric Time Series. New York:\n\nJ. Wiley, 2004.\n\n[41] J. Contreras, R. Espinola, F. Nogales, and A. Conejo, \u201cArima models to predict next-day\n\nelectricity prices,\u201d Institute of Electrical and Electronics Engineers(IEEE) Transactions on\n\nPower Systems, vol. 18, pp. 1014\u20131020, 2003.\n\n47\n\nhttp://srabbani.com/moments2.pdf\n\n\tIntroduction\n\tState of the art\n\tStationary Stochastic Processes\n\tBrownian Motion and White Noise\n\tMarkov Process\n\tStochastic Differential Equations\n\tThe Fokker-Planck Equation\n\tThe Black-Scholes Model and its limitations\n\tFrom Stochastic Volatility to Superstatistics\n\tThe Langevin Analysis\n\tStatistical Tests\n\n\tGetting to know the data\n\tOutliers and Daily Patterns\n\tLog-Normal Parameter Fluctuations\n\tMarkov Tests\n\n\tThe Langevin Analysis\n\tA Simple Model without Correlation\n\tModelling the Coupling between  and \n\n\tApproaching Non-Stationarity\n\tDiscussion and Conclusions"}]}}}