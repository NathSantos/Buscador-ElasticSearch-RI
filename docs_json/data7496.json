{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.11709"}, {"@name": "filename", "#text": "17110_Disserta%c3%a7%c3%a3o%20J%c3%balio%20Hoffimann%20Mendes.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "T H E I N V E R S E P R O B L E M O F H I S T O R Y M A T C H I N G\n\nA Probabilistic Framework for\nReservoir Characterization and Real Time Updating\n\nBY\n\nj \u00fa l i o h o f f i m a n n m e n d e s\n\nA dissertation submitted in partial fulfillment of\n\nthe requirements for the degree of\n\nMaster of Science\n\nin\n\nCivil Engineering\n\nin the\n\nDepartment of Civil Engineering\n\nFederal University of Pernambuco\n\nRecife, PE 50670-901\n\nMay 28, 2014\n\n\n\na d v i s o r s\n\nRamiro Brito Willmersdorf\n\n\u00c9zio da Rocha Ara\u00fajo\n\nc o m m i t t e e\n\nAlexandre Anoz\u00e9 Emerick\n\nBernardo Horowitz\n\nJ\u00falio Hoffimann Mendes: The Inverse Problem of History Matching, A Probabilistic\n\nFramework for Reservoir Characterization and Real Time Updating\n\nc\u00a9 May 28, 2014\n\n\n\nCataloga\u00e7\u00e3o na fonte\nBibliotec\u00e1ria Margareth Malta, CRB-4 / 1198\n\nM538i Mendes, J\u00falio Hoffimann.\nThe inverse problem of history matching, a probabilistic framework for \n\nreservoir characterization and real time updating / J\u00falio Hoffimann Mendes. \n- Recife: O Autor, 2014.\n\nxv, 121 folhas, il., gr\u00e1fs., tabs.\n\nOrientador: Prof. Dr. Ramiro Brito Willmersdorf.\nCooreintador: Prof. Dr. \u00c9zio da Rocha Ara\u00fajo.\nDisserta\u00e7\u00e3o (Mestrado) \u2013 Universidade Federal de Pernambuco. CTG. \n\nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Engenharia Civil, 2014.\nInclui Refer\u00eancias.\n\n1.   Engenharia  civil.   2.   Simula\u00e7\u00e3o  de  Reservat\u00f3rios.  3.  Ajuste  ao \nHist\u00f3rico I.  Willmersdorf,  Ramiro Brito. (Orientador).  II. Ara\u00fajo,  \u00c9zio da \nRocha. (Coorientador). III. T\u00edtulo.\n\n                   UFPE\n\n624 CDD (22. ed.)       BCTG/2014-164\n\n\n\n?????????\t??\n?????\t?\n??\n\n???\t?????\n\n????\t?\t\n??\n\n?????\t??\t???\n??\n??????\t??\t\n?????\n\n\t\n????????\n?? ??! \"?# \n\" \n??$?? \n\"?\n?????#% &amp;??\n\"?\n???%# \"?\n\n?????????\t?\n????\n?????\t??\n???\t???\n\n???\n??????\t???????\n????\t?\n??\n?\n????\t?\t??\t?\t????\n?\t?????????\n???\t???\n?\n????\n\n???????????\t??\t?\n?\t????????\n?\t????\t?\n?????\t???\n\n\"?$?!\"?\" \n'?#\n\n()*??\n??$$?? !!\n??!\"??\n\n??!??\"?# \n?\n? !\"?\" %?\n\t\n???\t??\n\n????$?+\n,-\n\"?\n? ??\n\"?\n,./0\n\n?#??!% \"?#??1\n\n2222222222222222222222222222222222222222222\n\n#?$3\n?#3\n? ??#?\n?#?%?\n4?**??#?\"?#$\n?\n??\n?\n\n\n5?#??!% \"?#6\n\n2222222222222222222222222222222222222222222\n\n#?$3\n?#3\n78??\n\" \n???9 \n\t# ):?\n?\n??\n?\n\n\n5????#??!% \"?#6\n\n? !? \n?? ??! \"?# 1\n\n2222222222222222222222222222222222222222222\n\n#?$3\n?#3\n? ??#?\n?#?%?\n4?**??#?\"?#$\n?\n??\n?\n\n\n5?#??!% \"?#6\n\n222222222222222222222222222222222222222222\n?#3\n\t*?? !\"#?\n\t!?8;\n???#??<\n=\n\n?%#?>#??\n\n\n5?? ??! \"?#\n??%?#!?6\n\n222222222222222222222222222222222222222222\n\n#?$3\n?#3\n??#! #\"?\n??#?@?%8\n=\n??\n?\n\n\n5?? ??! \"?#\n?!%?#!?6\n\n\n\nDedicated to my family and friends for all their love.\n\nTo one of the pioneers of this theory, Albert Tarantola.\n\n1949 \u2013 2009\n\n\n\nR E S U M O\n\nEm Engenharia de Petr\u00f3leo e outras \u00e1reas da ci\u00eancia, Mitiga\u00e7\u00e3o de Incertezas baseada\n\nem Hist\u00f3rico (MIH) \u00e9 o termo moderno usado por especialistas ao se referirem\n\na ajustes cont\u00ednuos de um modelo matem\u00e1tico dadas observa\u00e7\u00f5es. Tais ajustes\n\ntem maior valor quando acompanhados de diagn\u00f3sticos que incluem intervalos\n\nde confian\u00e7a, momentos estat\u00edsticos, e idealmente caracteriza\u00e7\u00e3o completa das dis-\n\ntribui\u00e7\u00f5es de probabilidade associadas.\n\nNeste trabalho, o bastante conhecido problema de ajuste ao hist\u00f3rico em campos\n\nde petr\u00f3leo \u00e9 revisado sob uma perspectiva Bayesiana que leva em considera\u00e7\u00e3o\n\ntoda poss\u00edvel fonte de incerteza te\u00f3rica ou experimental. \u00c9 uma aplica\u00e7\u00e3o direta\n\nda metodologia geral desenvolvida por Albert Tarantola no seu livro intitulado\n\n\u2018\u2019Inverse Problem Theory and Methods for Model Parameter Estimation\u201d.\n\nNosso objetivo \u00e9 fornecer a pesquisadores da \u00e1rea de \u00d3leo &amp; G\u00e1s um software\n\nescrito em uma linguagem de programa\u00e7\u00e3o moderna (i. e. Python) que possa ser\n\nfacilmente modificado para outras aplica\u00e7\u00f5es; realizar a invers\u00e3o probabil\u00edstica\n\ncom dezenas de milhares de c\u00e9lulas como uma prova de conceito; e desenvolver\n\ncasos de estudo reproduz\u00edveis para que outros interessados neste tema possam\n\nrealizar \u201cbenchmarks\u201d e sugerir melhoramentos.\n\nDiferentemente de outros m\u00e9todos de sucesso para MIH como Ensemble Kalman\n\nFilters (EnKF), o m\u00e9todo proposto, denomidado Ensemble MCMC (EnMCMC),\n\nn\u00e3o assume distribui\u00e7\u00f5es a priori Gaussianas. Pode ser entendido como uma cadeia\n\nde Markov de ensembles e teoricamente \u00e9 capaz de lidar com qualquer distribui\u00e7\u00e3o\n\nde probabilidade multimodal.\n\nDois casos de estudo sint\u00e9ticos s\u00e3o implementados em um cluster de computa\u00e7\u00e3o\n\nde alto desempenho usando o modelo MPI de execu\u00e7\u00e3o paralela para distribuir as\n\ndiversas simula\u00e7\u00f5es de reservat\u00f3rio em diferentes n\u00f3s computacionais. Resultados\n\nmostram que a implementa\u00e7\u00e3o falha em amostrar a distribui\u00e7\u00e3o a posteriori, mas\n\nque ainda pode ser utilizada na obten\u00e7\u00e3o de estimativas maximum a posteriori\n\n(MAP) sem fortes hip\u00f3teses a respeito dos dados (e. g. a priori Gaussianas).\n\nPalavras-chave: simula\u00e7\u00e3o de reservat\u00f3rios, ajuste ao hist\u00f3rico.\n\nvi\n\n\n\nA B S T R A C T\n\nIn Petroleum Engineering and other fields, History-based Uncertainty Mitigation\n\n(HUM) is the modern generic term used by experts when referring to continuous\n\nadjustments of a mathematical model given observations. Such adjustments have\n\ngreater value when accompanied by uncertainty diagnostics including statistical\n\nbounds, moments, and ideally full characterization of underlying distributions.\n\nIn this work, the well-known/ill-posed history matching problem is reviewed\n\nunder a Bayesian perspective which takes into account every possible source of\n\nexperimental and theoretical uncertainty. It is the direct application of the general\n\nframework developed by Albert Tarantola on his textbook entitled \u2018\u2019Inverse Problem\n\nTheory and Methods for Model Parameter Estimation\u201d.\n\nOur aim is to provide other researchers in the field with a software package writ-\n\nten in a modern programming language (i. e. Python) that can be easily adapted to\n\ntheir needs; to perform probabilistic inversion with tens of thousands of cells as a\n\nproof of concept; and to develop reproducible case studies for others to benchmark\n\nand propose improvements.\n\nUnlike other successfull HUM methods such as Ensemble Kalman Filters (EnKF),\n\nthe proposed method here called Ensemble MCMC (EnMCMC) does not assume\n\nGaussian priors. It can be thought as a Markov chain of ensembles and theoretically\n\ncan deal with any (multimodal) probability distribution.\n\nTwo synthetic case studies are implemented in a time-shared high-performance\n\ncomputer cluster using the MPI parallel execution model to distribute reservoir\n\nsimulations among various computational nodes. Results show the implementa-\n\ntion fails to sample the posterior distribution but still can be used to obtain clas-\n\nsical maximum a posteriori (MAP) estimates without strong assumptions on the\n\ndata (e. g. Gaussian priors).\n\nKeywords: reservoir simulation, history matching.\n\nvii\n\n\n\nP U B L I C A T I O N S\n\nSome ideas and figures have appeared in:\n\nMendes, Willmersdorf, and Ara\u00fajo [2013a]\n\nMendes, Willmersdorf, and Ara\u00fajo [2013b]\n\nMendes, Willmersdorf, and Ara\u00fajo [2013c]\n\nSoftware implementations of the methods presented in this thesis are publicly\n\navailable at https://github.com/juliohm/HUM. Refer to Appendix B for additional\n\ncode snippets discussed throughout the text.\n\nPlease cite this work if you plan to use the software.\n\nviii\n\nhttps://github.com/juliohm/HUM\n\n\nOld wood best to burn, old wine to drink, old friends to trust, and old authors to read.\n\nFrancis Bacon\n\nA C K N O W L E D G E M E N T S\n\nMy family and friends are quite important to me. I won\u2019t list all but my parents for\n\neducating me with precious principles. Rigoberto Mendes da Silva &amp; Lindevany\n\nHoffimann de Lima Mendes, I love you. A special thanks goes to my father for his\n\nunconditional support and presence.\n\nI would like to thank my advisor, Ramiro Brito Willmersdorf, for pushing me\n\ntowards hard problems, for his harsh sincerity againist my mistakes and for his\n\nfriendship along these years. He is certaintly one of the greatest finds of my career\n\nand definitely has shaped who I am today.\n\nAlthough rare, the long philosophical discussions at the campus with my co-\n\nadvisor, \u00c9zio da Rocha Ara\u00fajo, were inspiring. His vast knowledge on various\n\nsubjects and clear understanding of the fundamentals contributed to many of the\n\ninsights I had.\n\nThanks to Alexandre Emerick, Fl\u00e1via Pacheco and R\u00e9gis Romeu from c e n p e s for\n\ntheir accreditation in our research; to p e t ro b r a s for the financial support medi-\n\nated by the p f r h - 2 6 program; and to c e na pa d - p e for providing computational\n\nresources.\n\nix\n\n\n\nC O N T E N T S\n\nI i n v e r s e p ro b l e m t h e o r y 1\n\n1 b a s i c c o n c e p t s 2\n\n1.1 What is an inverse problem? . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2 Why inverse problems are hard? . . . . . . . . . . . . . . . . . . . . . 6\n\n1.3 The maximum likelihood principle . . . . . . . . . . . . . . . . . . . . 8\n\n1.4 Tarantola\u2019s postulate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n1.5 Classical vs. probabilistic framework . . . . . . . . . . . . . . . . . . . 10\n\n2 c l a s s i c a l f r a m e w o r k 12\n\n2.1 Basic taxonomy for inverse problems . . . . . . . . . . . . . . . . . . 13\n\n2.2 Linear regression and the least-squares estimate . . . . . . . . . . . . 16\n\n2.3 Tikhonov regularization . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n2.4 Levenberg-Marquardt solution to nonlinear regression . . . . . . . . 24\n\n3 p ro b a b i l i s t i c f r a m e w o r k 28\n\n3.1 Definition of probability . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n3.2 States of information . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n3.3 Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n3.4 Ensemble Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . 42\n\nII h i s t o r y m at c h i n g 55\n\n4 p r e l u d e 56\n\n4.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\n4.2 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n\n4.3 Comments on reproducibility . . . . . . . . . . . . . . . . . . . . . . . 63\n\n5 c h a n n e l i z e d r e s e rv o i r 64\n\n5.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n\n5.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n5.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n6 b ru g g e f i e l d 80\n\n6.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n6.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\nx\n\n\n\nc o n t e n t s xi\n\n6.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n\n7 c o n c l u s i o n 86\n\n7.1 General comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n\n7.2 Technical difficulties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n7.3 Suggested improvements . . . . . . . . . . . . . . . . . . . . . . . . . 89\n\nIII a p p e n d i x 91\n\na o m i t t e d p ro o f s 92\n\na.1 The majority of inverse problems is ill-posed . . . . . . . . . . . . . . 92\n\na.2 Maximum likelihood estimation for i.i.d. Gaussians . . . . . . . . . . 92\n\na.3 System of equations for discrete linear inverse problems . . . . . . . 93\n\na.4 Maximum likelihood and least-squares . . . . . . . . . . . . . . . . . 93\n\na.5 Weighted linear least-squares estimate . . . . . . . . . . . . . . . . . . 94\n\na.6 Levenberg-Marquardt gradient and Hessian . . . . . . . . . . . . . . 94\n\na.7 Conditional probability by conjunction of states . . . . . . . . . . . . 95\n\na.8 Kernel density estimation as a convolution . . . . . . . . . . . . . . . 95\n\nb c o d e s n i p p e t s 97\n\nb.1 Iteratively reweighted least-squares . . . . . . . . . . . . . . . . . . . 97\n\nb.2 Least absolute shrinkage and selection operator . . . . . . . . . . . . 98\n\nb.3 Metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\nb.4 Online Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . 100\n\nb.5 Histogram fitting with kernel density estimation . . . . . . . . . . . . 101\n\nc k e r n e l p c a 103\n\nc.1 Kernel Gramian matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n\nc.2 Centering in the feature space . . . . . . . . . . . . . . . . . . . . . . . 105\n\nc.3 Eigenproblem and normalization . . . . . . . . . . . . . . . . . . . . . 106\n\nc.4 Preimage problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\nb i b l i o g r a p h y 110\n\n\n\nL I S T O F F I G U R E S\n\nFigure 1.1 Geostatistical inference . . . . . . . . . . . . . . . . . . . . . . 4\n\nFigure 1.2 Non-bijective map G : M 7?? D . . . . . . . . . . . . . . . . . 6\nFigure 1.3 3D non-convex surfaces . . . . . . . . . . . . . . . . . . . . . 10\n\nFigure 2.1 Univariate Gaussians . . . . . . . . . . . . . . . . . . . . . . . 14\n\nFigure 2.2 Polynomial regression models . . . . . . . . . . . . . . . . . . 16\n\nFigure 2.3 Bullet trajectory prediction . . . . . . . . . . . . . . . . . . . . 18\n\nFigure 2.4 Ridge regression . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\nFigure 2.5 L-curve criterion . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n\nFigure 2.6 L1-norm regression . . . . . . . . . . . . . . . . . . . . . . . . 23\n\nFigure 2.7 Numerical simulator as a black box . . . . . . . . . . . . . . 26\n\nFigure 3.1 Homogeneous distribution for Jeffreys parameters . . . . . . 32\n\nFigure 3.2 Disjunction for producing histograms . . . . . . . . . . . . . 32\n\nFigure 3.3 Impact clouds on a cathodic screen . . . . . . . . . . . . . . . 33\n\nFigure 3.4 The p-event of an interval for a Jeffreys parameter. . . . . . . 34\n\nFigure 3.5 Conditioning probability densities . . . . . . . . . . . . . . . 35\n\nFigure 3.6 Dirac delta function ?(x; x0) . . . . . . . . . . . . . . . . . . . 37\n\nFigure 3.7 Nescience towards Omniscience . . . . . . . . . . . . . . . . . 37\n\nFigure 3.8 2D Gaussian N\n(\nmprior, Cm\n\n)\n. . . . . . . . . . . . . . . . . . . 38\n\nFigure 3.9 Uncertainty in the forward operator . . . . . . . . . . . . . . 39\n\nFigure 3.10 Graph for a 3-state machine . . . . . . . . . . . . . . . . . . . 44\n\nFigure 3.11 Limiting behavior for a 3-state Markov chain . . . . . . . . . 44\n\nFigure 3.12 Histogram for a Gaussian mixture . . . . . . . . . . . . . . . 47\n\nFigure 3.13 Trace and autocorrelation for a Gaussian mixture . . . . . . 48\n\nFigure 3.14 Probabilistic inversion of y = x2 . . . . . . . . . . . . . . . . . 50\n\nFigure 3.15 KDE for a 2D Gaussian . . . . . . . . . . . . . . . . . . . . . . 51\n\nFigure 3.16 Anisotropic joint distribution . . . . . . . . . . . . . . . . . . 52\n\nFigure 3.17 Stretch move ?(t)\nk\n? ?(t+1)\n\nk\n. . . . . . . . . . . . . . . . . . . 52\n\nFigure 4.1 Mind map of algorithms and concepts . . . . . . . . . . . . . 56\n\nFigure 4.2 Training image of size 250x250 pixels . . . . . . . . . . . . . 58\n\nFigure 4.3 Ten-spot well configuration . . . . . . . . . . . . . . . . . . . 59\n\nxii\n\n\n\nList of Figures xiii\n\nFigure 4.4 Oil/water saturation within channelized reservoir . . . . . . 59\n\nFigure 4.5 Production history for channelized reservoir . . . . . . . . . 59\n\nFigure 4.6 Brugge field . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n\nFigure 4.7 Top view of Brugge field realization . . . . . . . . . . . . . . 61\n\nFigure 4.8 Brugge field permeability curves . . . . . . . . . . . . . . . . 62\n\nFigure 4.9 Brugge field oil production history . . . . . . . . . . . . . . . 62\n\nFigure 5.1 Filtersim realizations . . . . . . . . . . . . . . . . . . . . . . . 66\n\nFigure 5.2 kPCA for increasing polynomial kernel degrees . . . . . . . 68\n\nFigure 5.3 Stretch move: bean plot of prior and posterior log-probabilities 71\n\nFigure 5.4 KDE move: bean plot of prior and posterior log-probabilities 71\n\nFigure 5.5 KDE move: production history for the prior ensemble . . . . 72\n\nFigure 5.6 KDE move: production history for the posterior ensemble . 73\n\nFigure 5.7 KDE move: acceptance fraction for each walker . . . . . . . . 74\n\nFigure 5.8 KDE move: 25 most probable images in prior ensemble . . . 75\n\nFigure 5.9 KDE move: 25 most probable images in posterior ensemble 76\n\nFigure 5.10 KDE move: maximum a posteriori estimate . . . . . . . . . . 77\n\nFigure 5.11 Filtersim: bean plot of prior and posterior log-probabilities . 77\n\nFigure 5.12 Filtersim: acceptance fraction for each walker . . . . . . . . . 77\n\nFigure 5.13 Filtersim: 25 most probable images in posterior ensemble . . 78\n\nFigure 5.14 Filtersim: production history for the posterior ensemble . . 78\n\nFigure 5.15 Filtersim: maximum a posteriori estimate . . . . . . . . . . . 79\n\nFigure 5.16 kPCA reconstruction . . . . . . . . . . . . . . . . . . . . . . . 79\n\nFigure 6.1 Prior on observations changing over time . . . . . . . . . . . 82\n\nFigure 6.2 KDE move: bean plot of prior and posterior log-probabilities 83\n\nFigure 6.3 KDE move: acceptance fraction for each walker . . . . . . . . 84\n\nFigure 6.4 KDE move: production history for the prior ensemble . . . . 84\n\nFigure 6.5 KDE move: production history for the posterior ensemble . 85\n\n\n\nL I S T O F T A B L E S\n\nTable 4.1 Channelized reservoir summary table . . . . . . . . . . . . . 58\n\nTable 4.2 Brugge rock formations . . . . . . . . . . . . . . . . . . . . . 61\n\nTable 5.1 SSIM statistics for Filtersim and KDE-based proposals . . . 75\n\nxiv\n\n\n\nL I S T O F A L G O R I T H M S\n\n2.1 Coordinate descent for sparse regularization . . . . . . . . . . . . . . . 23\n\n3.1 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\n3.2 Stretch move in Rn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n\nxv\n\n\n\nL I S T I N G S\n\nListing 5.1 Filtersim parameters file . . . . . . . . . . . . . . . . . . . . . 65\n\nListing B.1 Iteratively reweighted least-squares . . . . . . . . . . . . . . . 97\n\nListing B.2 Least absolute shrinkage and selection operator . . . . . . . 98\n\nListing B.3 Metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . 99\n\nListing B.4 Online Bayesian inversion . . . . . . . . . . . . . . . . . . . . 100\n\nListing B.5 Histogram fitting with kernel density estimation . . . . . . . 101\n\nxvi\n\n\n\nA C R O N Y M S\n\nSVM Support Vector Machine\n\nMPS Multiple-Point Statistics\n\nSVD Singular Value Decomposition\n\nLASSO Least Absolute Shrinkage and Selection Operator\n\nKKT Karush-Kuhn-Tucker\n\nDFP Davidon-Fletcher-Powell\n\nBFGS Broyden-Fletcher-Goldfarb-Shanno\n\nMCMC Markov chain Monte Carlo\n\nEnKF Ensemble Kalman Filter\n\nMAP Maximum a Posteriori\n\nKDE Kernel Density Estimation\n\nRML Randomized Maximum Likelihood\n\nK-L Karhunen-Lo\u00e8ve\n\nkPCA Kernel Principal Component Analysis\n\nkMAF Kernel Maximum Autocorrelation Factor\n\nMPI Message Passing Interface\n\nGPGPU General-Purpose Computing on Graphics Processing Units\n\nHUM History-based Uncertainty Mitigation\n\nTPFA Two-Point Flux Approximation\n\nEnMCMC Ensemble Markov chain Monte Carlo\n\nSSIM Structural Similarity\n\nxvii\n\n\n\nPart I\n\nI N V E R S E P R O B L E M T H E O R Y\n\nIn which the general problem of estimating parameters of a (physical)\n\nsystem based on real evidence (i. e. observation) is set up. The classi-\n\ncal and probabilistic framework for the solution are briefly reviewed\n\nemphasizing the very different questions they target.\n\n\n\n1\nB A S I C C O N C E P T S\n\nFor certainly it is excellent discipline for an author to feel that he must say all he has to\n\nsay in the fewest possible words, or his reader is sure to skip them; and in the plainest\n\npossible words, or his reader will certainly misunderstand them.\n\nJohn Ruskin\n\n1.1 What is an inverse problem? . . . . . . . . . . . . . . . . . . . . . 3\n\n1.2 Why inverse problems are hard? . . . . . . . . . . . . . . . . . . . 6\n\n1.3 The maximum likelihood principle . . . . . . . . . . . . . . . . . 8\n\n1.4 Tarantola\u2019s postulate . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n1.5 Classical vs. probabilistic framework . . . . . . . . . . . . . . . . 10\n\nWhat is an inverse problem? Why is it so hard to solve? What to expect as a reasonable\n\nsolution? All these questions should be addressed in the following sections.\n\nInverse problems are quite general and don\u2019t require concrete examples to be\n\nunderstood. Like the elementary concept of an inverse image (a. k. a. preimage) of\n\na function, these problems are nothing but abstraction. Indeed, the solution of an\n\ninverse problem is an inverse image1.\n\nWithout even knowing what an inverse problem is, the smart reader is probably\n\nspeculating about ill-posedness and how to deal with it. There isn\u2019t magic and the\n\nmost natural question2 has to be adapted for approximate answers to be inferred,\n\nin a least-squares sense.\n\nHowever, redesigning the questions can be surprisingly innovative and reward\n\none\u2019s mind with new points of view.\n\n1 Or a collection of images with known distribution.\n2 What is x = f?1(y)?\n\n2\n\n\n\n1.1 w h at i s a n i n v e r s e p ro b l e m ? 3\n\n1.1 w h at i s a n i n v e r s e p ro b l e m ?\n\nConsider a function G : M 7?? D that for each parameter m ? M associates a\nresponse d ? D. It\u2019s possible to conceptualize exactly three types of inference [4]:\n\nF O R WA R D P R O B L E M Given m and G; Find d = G(m).\n\nI N V E R S E P R O B L E M Given d and G; Find m\n?\n= G?1(d).\n\nS U P E R V I S E D L E A R N I N G Given m and d; Find G.\n\nRegardless of the meaning of M and D, hereafter called model space and data space,\n\nor the complexity of G, the transfer function3; these problems have rather distinct\n\ntractability. The f o r wa r d p ro b l e m is the easiest as its solution is obtained by\n\ndirect function evaluation. Techniques for solving the i n v e r s e p ro b l e m will be\n\ndiscussed throughout this thesis because clearly the inverse G?1 may not exist\n\nor be available. Finally, s u p e rv i s e d l e a r n i n g4 has being extensively studied\n\nas a Machine Learning subtopic and various successful classification/regression\n\ntechniques such as SVM were developed for solving it [5].\n\ni m p o r ta n t n o t e : In the literature, the term f o r wa r d p ro b l e m is also\n\nused to designate the inductive process of deriving physical laws from experiments.\n\nThis is essentially a human being quality, and it\u2019s much harder, if not impossible,\n\nfor a computer to reproduce. In this text, the transfer function G is given.\n\nFor better understanding, the following concrete examples attach meaning to M,\n\nD and G. They\u2019re all practical applications.\n\nExample 1.1 (Geostatistics)\n\nA random field Z(x; ?) is a stochastic process on spatial coordinates x ? Rn [6].\nFor the oil industry it characterizes the uncertainty on petrophysical properties in\n\na reservoir model [7, 8, 9]. A very common task a professional in this area has to\n\naccomplish is conditional sampling:\n\n3 Often representing an expensive simulation code.\n4 It isn\u2019t the more intuitive name for the third type of inference, s y s t e m i d e n t i f i c at i o n p ro b l e m\n\nis a possible alternative [4].\n\n\n\n1.1 w h at i s a n i n v e r s e p ro b l e m ? 4\n\nA spatial property is measured at few locations within an acceptable deviation\n\nfrom the true (unknown) value. This hard data comes from boreholes as the prod-\n\nuct of laboratory experiments with cores, well tests, or sometimes are indirectly\n\nobtained from well logs, see Figure 1.1a. The task is to fill in the 3D reservoir\n\nmodel with the mentioned property honoring the hard data and prior probability\n\ndistribution of the field. A complementary step not considered in this example is\n\nsoft data integration [10, 11].\n\n(a) Rock porosity at well locations. (b) Ordinary Kriging estimation.\n\nFigure 1.1: Geostatistical inference on a 3D reservoir model.\n\nThe result of applying Ordinary Kriging to the Stanford VI data set [11] is shown\n\nin Figure 1.1b. Similar to Stanford V [12], this stratigraphic model was synthesized\n\nas a fluvial channel system with the purpose of extensively testing (MPS) algo-\n\nrithms for reservoir characterization.\n\nIn spite of a neat deterministic solution, it presents non-physical smoothness [13].\n\nThis inverse problem is better solved by sequential simulation which is a technique\n\nfor drawing random values from univariate distributions sequentially built with\n\nthe hard data [14]. In fact, this subject has been widely discussed [15, 16, 17]. Se-\n\nquential simulation allows multiple realizations to be drawn to characterize the ran-\n\ndom field.\n\nIn mathematical language, m is defined as the flattened array of length nx\u00d7ny\u00d7\nnz containing all porosity values of the 3D grid; d is of reduced length, the number\n\nof cells with hard data; and G is the \u201cselection\u201d operation, it simply discards grid\n\nlocations that aren\u2019t in d, and is represented by a matrix whose entries are either 1\n\nor 0.\n\n\n\n1.1 w h at i s a n i n v e r s e p ro b l e m ? 5\n\nThus, a linear inverse problem of the form:\n\n?\n???\n\n???\n\nd\n\n?\n???\n\n???\n\n=\n\nG\n? ?? ??\n???\n\n1 0 \u00b7 \u00b7 \u00b7 0 0\n0 0\n\n. . . 1 0\n...\n\n...\n...\n\n...\n...\n\n?\n???\n\n?\n????????\n\n????????\n\nm\n\n?\n????????\n\n????????\n\n(1.1)\n\n?\n\nExample 1.2 (Well Testing)\n\nFor assessing a newly discovered reservoir under dynamic conditions, the just\n\ndrilled pilot well is provisionally completed for controlled production during a\n\nshort period of time, and closed until hydrostatic equilibrium is reestablished.\n\nThe well pressure is registered along with the imposed production rate. The\n\nphenomenon can be analytically modelled by the diffusion equation with the ap-\n\npropriate boundary conditions for the transient regime5. The analytical solution\n\nfor vertical wells under constant production rate is [18, 19]:\n\npw = pi ?\nq\u00b5\n\n2??h\n\n(\n1\n\n2\nEi\n\n[\n?\u00b5ctr\n\n2\nw\n\n4?t\n\n])\n(1.2)\n\nwith Ei(x)\ndef\n=\n\n??\nx\n\ne??\n\n?\nd?, the exponential integral function. The ultimate goal of\n\nthe test is to estimate the productivity index for the well in the long term by\n\nfirst estimating the rock permeability\u2014the only unknown in Equation 1.2. Similar\n\nestimation is desired when in situ permanent sensors are installed [20]. ?\n\nExample 1.3 (Physical Measurements)\n\nA collection of instruments is used for measuring parameters of a physical system.\n\nThat may involve human interaction and/or impossible to control environmental\n\nconditions. As a result, instrument readings may be (and generally are) not precise,\n\neven though accurate6.\n\nSuppliers of these instruments then provide a statistical analysis of the uncer-\n\ntainties involved in the measurement process that should be used to describe\n\nthe output. For instance, if parameters m are to be measured by an instrument\n\nI : M 7?? M, the supplier provides the conditional probability Pr(mout | m) or the\ncovariance CM.\n\n5 The reservoir has hypothetical infinite extension.\n6 Precision is about statistical variance, whereas accuracy is the absence of bias in the measurements.\n\n\n\n1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ? 6\n\nIdeally, the instrument is the identity function I ? IdM, the forward and inverse\nproblem share the same solution. In the real world, is common practice to assume\n\nthe (Gaussian) error ? is independent of the input:\n\nmout = I(m) = m + ? (1.3)\n\nThe inverse problem is to find the true values for the physical parameters m\n\ngiven the instrument measurements mout, or as for the later assumption, to sub-\n\ntract m = mout ? ?. Note the error is a random variable. ?\n\nThis work is concerned with the inverse problem of history matching, a much\n\nmore computational expensive problem that the oil industry is encouraging re-\n\nsearchers to devote time thinking. It\u2019ll be introduced in the following chapters\n\nalong with its theoretical and practical difficulties.\n\n1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ?\n\nm1\n\nm2\n\nm3\n\nM\n\nM\n\nd1\n\nd2\n\nd3\n\nD\n\nD\n\nG\n\n?\n?\n\nG?1\n\nFigure 1.2: Non-bijective map between model and data space.\n\nBack in 1902, the French mathematician Jacques Hadamard (1865 \u2013 1963) con-\n\nceived the term well-posed problem to designate a very important concept that can\n\nbe adapted for use in inverse problem theory. A boundary value problem in math-\n\nematical physics is said to be well-posed in the sense of Hadamard if it satisfies all\n\nof the following criteria [21]:\n\n\u2022 There exists a solution\n\n\u2022 The solution is unique\n\n\u2022 The solution depends continuously on the data7\n\n7 Meaning slight changes in the boundary conditions cause little, if any, impact on the solution.\n\n\n\n1.2 w h y i n v e r s e p ro b l e m s a r e h a r d ? 7\n\nA problem that isn\u2019t well-posed in this sense is termed ill-posed problem, see\n\nFigure 1.2 for an illustration.\n\nTheorem 1. The majority of inverse problems is ill-posed.\n\nProof. Given two equinumerous sets |A| = |B|, the number of bijections from A to\n\nB never exceeds the number of non-bijections, except when both sets are empty or\n\nsingleton.\n\ni n f i n i t e c a s e Denote fA7?B, bA 7?B and nA7?B the number of functions, bi-\n\njections and non-bijections from A to B, respectively. It follows that:\n\nbA7?B 6 fA7?B = fA\\{a} 7?B 6 nA7?B\n\nfA7?B = fA\\{a} 7?B because A and A \\ {a} have the same cardinality; fA\\{a} 7?B 6\n\nnA7?B because a function f : A \\ {a} 7?? B can be extended by mapping f(a) ?\nf(A \\ {a}), which is non-injective.\n\nf i n i t e c a s e |A| = |B| = n, fA7?B = nn, bA7?B = n!, nA7?B = nn ? n!\n\nApart from the empty and singleton sets for which only one (bijective) function is\n\ndefined, the result (left for the reader) is proved by induction:\n\nnn ? n! > n! (?n > 2)\n\nPut differently, well-posed inverse problems are accidental. For instance, assume\n\nthe model and data space are finite; if |M| 6= |D|, none of the inverse problems is\nwell-posed, else |M| = |D| = n and the percentage8 of n!\n\nnn\n? 0% vanishes very\n\nrapidly with n.\n\nNot only ill-posed, inverse problems are sometimes computationally expensive\n\nwithin current solving strategies. If a more detailed characterization of the model\n\nspace is desired and the transfer function is a demanding physical simulation,\n\nthe wall time required for the solution increases considerably. Up to date, two\n\nframeworks coexist for the solution of inverse problems, the probabilistic being in\n\ngeneral more expensive than the classical, for reasons that will be clear in next\n\nsections.\n\n8 The third criterion of Hadamard is not taken into account, the exact percentage is smaller.\n\n\n\n1.3 t h e m a x i m u m l i k e l i h o o d p r i n c i p l e 8\n\n1.3 t h e m a x i m u m l i k e l i h o o d p r i n c i p l e\n\nGiven a statistical model for experiment outcomes x1, x2, . . . , xm parameterized by\n\n?, the likelihood function is a reinterpretation of the joint probability density f as\n\nif the observations were \u201cfixed parameters\u201d:\n\nL(? | x1, x2, . . . , xm)\ndef\n= f(x1, x2, . . . , xm | ?) (1.4)\n\nThe maximum likelihood principle states the model parameters are to be set to\n\nmaximize the experiment probability (i. e. an extremum estimator):\n\n?? = arg max\n???\n\nL(? | x1, x2, . . . , xm) (1.5)\n\nThis is to say over all plausible parameters ? ? ?, only ?? is of interest. For\ninstance, if the univariate Gaussian model is assumed and the random variables\n\nare i.i.d., the parameters can be shown to match the sample mean and standard\n\ndeviation (see Appendix A.2):\n\n?? =\n\n{\n\u00b5?\n\n??2\n\n}\n\n=\n\n{\n1\nm\n\n?\ni xi\n\n1\nm\n\n?\ni(xi ? \u00b5?)\n\n2\n\n}\n\n(1.6)\n\nFor inverse problems, the principle is interpreted likewise: find the parameters\n\nm? ? M that best honor the observed data d ? D through the forward operator G\naccording to some loss function (e. g. ?d ? G(m)?L2 ):\n\nm? = arg min\nm?M\n\nloss (d, G(m)) (1.7)\n\nIt can be in general a non-convex optimization problem over a high-dimensional\n\nspace. In Example 1.1, the Kriging model has parameters with 6 million entries\n\n(i. e. cells) and the loss function is the mean square estimation error or estimation\n\nvariance ?2 = E\n[\n(Z?(x; ?) ? Z(x; ?))2\n\n]\nwith the estimator Z?(x; ?)\n\ndef\n=\n\n?\ni ?iZ(xi; ?)\n\na linear combination of the hard data.\n\nThe maximum likelihood principle is widely applied in statistics and real life\n\nengineering applications.\n\n\n\n1.4 ta r a n t o l a\u2019 s p o s t u l at e 9\n\n1.4 ta r a n t o l a\u2019 s p o s t u l at e\n\n\u201cThe most general solution of an inverse problem provides a probability distri-\n\nbution over the model space.\u201d \u2013 Albert Tarantola\n\nOn his book Inverse Problem Theory and Methods for Model Parameter Estimation\n\n[22], Tarantola argued that solutions to inverse problems are of greater value if\n\nthey come attached to probability distributions. He developed a very clever theory\n\nin which all information available about the problem is modeled within a richer\n\nprobabilistic framework. This dissertation is an attempt to apply this framework\n\nto the problem of history matching and, at worst, will serve to clarify what parts\n\nof it requires further work for practical use by the oil industry.\n\nA careful review of the theory is given in Chapter 3. For now, it\u2019s sufficient\n\nto know that once a coordinate system is fixed for the data and model spaces,\n\nprobability densities can be defined, and the Bayesian updating performed:\n\n?M(m) = k ?M(m) L(m) (1.8)\n\nThe posterior distribution over the model space ?M(m) is obtained from the\n\nprior ?M(m) by incorporating the likelihood L(m) which expresses how good the\n\nparameters m are in explaining the data. In Equation 1.8, k is a normalization\n\nconstant.\n\nHaving a continually updated probability distribution for the parameters allows\n\nanswering questions such as What is the most probable parameter?9 What is the proba-\n\nbility of a parameter being in a certain range? and many others. Tarantola\u2019s framework\n\nis therefore, as previously mentioned, \u201cricher\u201d at providing the analyst a variety\n\nof insights.\n\nHowever, it requires higher level of abstraction from the implementer as to iden-\n\ntify and model the various states of information. Not to mention, information isn\u2019t\n\nalways available, specially if it costs billions of dollars to be acquired.\n\nThe probabilistic framework is relatively recent, few researchers have employed\n\nit in large scale problems for assessing its efficiency.\n\n9 Maximum likelihood estimation.\n\n\n\n1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k 10\n\n1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k\n\nIt\u2019s clear that the classical framework, as the direct application of the maximum\n\nlikelihood principle, does not address many important questions. As Tarantola\n\nonce said: \u201cThis is not the solution; it is, rather, the mean of all possible solutions. Looking\n\nat this mean provides much less information than looking at a movie of realizations.\u201d.\n\nReferring to the least-squares solution of an inverse problem [22].\n\nIt\u2019s also clear that, by what was exposed in Section 1.4, the probabilistic frame-\n\nwork is more general as it considers the maximum likelihood estimation as one of\n\nmany possible post-processing steps. All the embedded information is preserved\n\nand accumulates as new data becomes available.\n\nFollowing this reasoning, the comparison c l a s s i c a l v s . p ro b a b i l i s t i c is\n\nunfair, nevertheless it is important to be raised. The point is to make it clear when\n\nit is worth applying one framework over another.\n\nThis dissertation endorses the Tarantola\u2019s postulate with the justification being\n\nmultimodal hypersurfaces in high-dimensional spaces. Figure 1.3 is a collection of\n\n3D synthetic surfaces for which the maximum likelihood estimation isn\u2019t helpful.\n\nFigure 1.3: 3D non-convex surfaces.\n\n\n\n1.5 c l a s s i c a l v s . p ro b a b i l i s t i c f r a m e w o r k 11\n\nThe classical framework is the option for rough estimates, if no data is available\n\nto assemble representative distributions or if the problem is known to be unimodal\n\nbeforehand.\n\nConcerning ill-posedness handling, maximum likelihood estimation does it ex-\n\nplicitly with the help of regularization techniques, whereas Bayesian updating\n\nguarantees the existence of an unique well-defined posterior. Alternatively, the\n\nprobabilistic framework always gives an unique answer: the posterior distribution\n\nover the model space. This e x p l i c i t v s . i m p l i c i t ill-posedness treatment will\n\nbe revisited in more depth when it\u2019s appropriate.\n\nAlgorithmically, Tarantola\u2019s framework relies on randomness for effective explo-\n\nration of high-dimensional spaces. The classical approach uses, in general, deter-\n\nministic optimization routines for solving slightly modified (i. e. biased) problems.\n\nIn summary, what to expect as a reasonable solution? Notwithstanding a subjec-\n\ntive question, a probability distribution over the model space is almost always a\n\nsatisfactory answer.\n\n\n\n2\nC L A S S I C A L F R A M E W O R K\n\nIf you don\u2019t know anything about computers, just remember that they are machines that\n\ndo exactly what you tell them but often surprise you in the result.\n\nRichard Dawkins\n\n2.1 Basic taxonomy for inverse problems . . . . . . . . . . . . . . . . 13\n\n2.2 Linear regression and the least-squares estimate . . . . . . . . . 16\n\n2.3 Tikhonov regularization . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n2.4 Levenberg-Marquardt solution to nonlinear regression . . . . . . 24\n\nLikely to be employed by frequentists1, the classical framework aims at finding the\n\nmost probable parameters for the (physical) system that honors the observations.\n\nIt consists of maximizing the likelihood function, no matter the prior state of in-\n\nformation on the system2. For the practical experimentalist, the framework often\n\ntranslates into minimizing the misfit on the training data, disregarding previous\n\nattempts or externally acquired knowledge.\n\nAs optimization is omnipresent, it makes sense to approach the framework by\n\ninformally classifying which inverse problems reflect \u201cgood\u201d objective functions\n\nand which don\u2019t. A basic taxonomy is presented that groups inverse problems as\n\ndiscrete, continuous, linear and nonlinear.\n\nIt occurs that adding engineer-designed terms to the objective can improve the\n\nsolution process and the estimation itself\u2014a technique generically referred to as\n\nregularization. The most commonly used regularization schemes are presented for\n\nreference, elaborating on this topic is out of scope of the present work.\n\n1 Followers of frequentist statistics.\n2 A virtual prior is often introduced for stability.\n\n12\n\n\n\n2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 13\n\nFinally, the problem of nonlinear inversion is investigated with a specialized\n\nversion of the Gauss-Newton method for unconstrained optimization.\n\nThis chapter is heavily influenced by Aster, Borchers, and Thurber\u2019s \u201cParameter\n\nEstimation and Inverse Problems\u201d [4].\n\n2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s\n\n2.1.1 Discrete vs. continuous\n\nInverse problems for which the model and data space have continuous functions\n\nas elements of study are termed continuous. These problems are often expressed as\n\nintegral operators:\n\n?b\n\na\n\ng(s, x)m(x) dx = d(s) (2.1)\n\nwhere m(x) is the unknown, d(s) the observation, and g(s, x) the kernel function.\n\nEquation 2.1 occurs so often in mathematical models, it has a name\u2014Fredholm\n\nintegral equation of the first kind. For instance, in electrical engineering, the kernel\n\ndepends explicitly on s ? x; convolution/deconvolution arises as one of the most\n\nimportant forward/inverse problems:\n??\n\n??\n\ng(s ? x)m(x) dx = d(s) (2.2)\n\nFor further clarification, consider g(s, x) = 1 over [a, b] ? R, the inverse problem\nhas no solution unless d(s) = C is a constant. Moreover, there are multiple func-\n\ntions m(x) for which the definite integral evaluates to C. The problem is ill-posed.\n\nSince continuous functions cannot be represented by digital computers, these\n\nproblems must be discretized first. The parameters m = (m1, m2, . . . , mn)? and\n\nobservations d = (d1, d2, . . . , dm)? are both represented by a finite3 number of\n\ncoordinates and the inversion is said to be discrete.\n\nParameter estimation is an alternative name for discrete inverse problems, it is\n\noriginated from the fact that discretization is generally achieved by series expan-\n\nsion and parametrization. For example, the univariate Gaussian distribution is en-\n\ntirely determined by two parameters N\n(\n\u00b5, ?2\n\n)\n, no matter the complex approxima-\n\ntions the computer does for reproducing it:\n\n3 More restrictive than just discrete.\n\n\n\n2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 14\n\n?6 ?5 ?4 ?3 ?2 ?1 0 1 2 3 4 5 6\n\n\u00b5 = 0 ?2 = 0.6\n\n\u00b5 = 0 ?2 = 1.0\n\n\u00b5 = 0 ?2 = 3.0\n\n\u00b5 = ?2 ?2 = 0.8\n\nFigure 2.1: Univariate Gaussians parameterized by\n(\n\u00b5, ?2\n\n)?\n.\n\nGiven that large-scale inverse problems are solved by computers, discretization\n\nis of major importance. Few continuous inverse problems have an elegant solution\n\nlike in Example 2.1.\n\nExample 2.1 (Lagrange Interpolation)\n\nFind a polynomial p(X) ? R[X] of degree n with given zeros x1, x2, . . . , xn ?\nR. This problem is inverse to the direct problem of finding the roots of a given\n\npolynomial p(X) ? R[X]. In this case, the inverse problem is conceptually easier to\nsolve: p(X) = c(X ? x1)(X ? x2) \u00b7 \u00b7 \u00b7(X ? xn) for c ? R.\n\nMore generally, find a polynomial p(X) ? R[X] of degree n that assumes given\nvalues y1, y2, . . . , yn ? R at given distinct points x1, x2, . . . , xn ? R. The solution\nis given by the Langrange interpolation theorem.\n\nWhether this example is an i n v e r s e p ro b l e m or s u p e rv i s e d l e a r n i n g, it\n\nis open to philosophical discussion. ?\n\n2.1.2 Linear vs. nonlinear\n\nAn inverse problem is linear if its forward operator G satisfies superposition and\n\nscaling:\n\nG(m1 + m2) = G(m1) + G(m2) (2.3)\n\nG(?m) = ? G(m) (2.4)\n\nIt\u2019s nonlinear otherwise, and also harder to solve. The model and data spaces of\n\ndiscrete inverse problems are assumed to be linear manifolds so that it makes sense\n\nto add and scale coordinates (a. k. a. components). This is a reasonable assumption\n\nwith no penalties to most real applications.\n\n\n\n2.1 b a s i c ta x o n o m y f o r i n v e r s e p ro b l e m s 15\n\nEvery discrete linear inverse problem can be trivially written as a system of\n\nlinear equations by looking up the basis for the associated model and data spaces\n\n(see Appendix A.3):\n\nGm = d (2.5)\n\nwith G the matrix for the linear transformation. The problem then reduces to\n\n\u201csolving\u201d the system for m even when an inverse G?1 does not exist. Various\n\nwell-established results from linear algebra have been applied for the solution of\n\nthese problems, they are reviewed in Section 2.2.\n\nFredholm integral equations of the first kind are a good example of continuous\n\nlinear inverse problems, Equation 2.1 is an integral and as such:\n?b\n\na\n\ng(s, x)\n[\n? m1(x) + m2(x)\n\n]\ndx = ?\n\n?b\n\na\n\ng(s, x)m1(x) dx +\n?b\n\na\n\ng(s, x)m2(x) dx (2.6)\n\nSince coupled multiphysics and complex models are considered in realistic En-\n\ngineering simulations, nonlinear outcomes are obtained in general. Inversion is\n\nmuch difficult and requires iterative optimization for an appropriate solution. The\n\nLevenberg-Marquardt algorithm is discussed in Section 2.4.\n\n2.1.3 Well-posed vs. ill-posed\n\nThe Hadamard definition for well-posedness is revisited for completeness with\n\nnormed spaces [23].\n\nDefinition 2.1 (Well-posedness). Let X and Y be normed spaces, T : X 7?? Y a\n(linear or nonlinear) mapping. The equation T(x) = y is called properly-posed or\n\nwell-posed if the following holds:\n\n\u2022 Existence and uniqueness: ?y ? Y, ?!x ? X, y = T(x).\n\n\u2022 Stability: for every sequence (xn)n?N with T(xn) ? T(x), it follows that\nxn ? x.\n\nEquations for which (at least) one of these properties does not hold are called\n\nimproperly-posed or ill-posed.\n\ni m p o r ta n t n o t e : Discrete linear inverse problems can be further classified\n\nas purely underdetermined, purely overdetermined or mixed determined depending on\n\nthe range and null space of G [24].\n\n\n\n2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 16\n\n2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e\n\nSince no exact preimage exists for noisy observations, parameterized mathematical\n\nmodels are adjusted to fit data with respect to some misfit measure, as illustrated\n\nin Figure 2.2. Linear regression is the term used when the model being fit depends\n\nlinearly on the parameters d? = Gm.\n\nFigure 2.2: Polynomial regression models.\n\nThe widely employed procedure is to minimize the sum of squared errors be-\n\ntween model and observation dobs\n4:\n\nmL2 = arg min\nm?M\n\n(dobs ? Gm)\n?(dobs ? Gm) (2.7)\n\nIt has a closed form solution mL2 = (G\n?G)?1G?dobs when G has full column\n\nrank and probabilistic meaning5 when the noise is Gaussian (see Appendix A.4).\n\nWhen the null space of G denoted N(G) isn\u2019t trivial, the pseudo-inverse defined\n\nthrough SVD is used to compute a least-squares and minimum-length solution:\n\nG\u2020\ndef\n= VpS\n\n?1\np U\n\n?\np (2.8)\n\nwith rank(G) = p in the singular value decomposition for G:\n\nG = USV? =\n[\nUp U0\n\n][Sp 0\n0 0\n\n][\nVp V0\n\n]?\n(2.9)\n\nThe pseudo-inverse solution m\u2020 = G\u2020dobs always exist and, as a least-squares\n\nsolution, satisfies the normal equations:\n\n(G?G)m\u2020 = G\n?dobs (2.10)\n\n4 dobs and d are used indistinguishably.\n5 It\u2019s the maximum likelihood estimate.\n\n\n\n2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 17\n\nMost importantly, it can be shown by pure algebraic manipulation that m\u2020 min-\n\nimizes the length ?m?L2 over all residuals ?d ? Gm?L2 (refer to Aster et al.).\nThe major problem with pseudo-inverses is that they introduce non-negligible bias\n\nto the solution, whereas for instance, the least-squares estimate mL2 is unbiased\n\nunder Gaussian assumption, E [mL2] = mtrue. Bounds are derived for the intro-\n\nduced bias with the concept of model resolution: for the pseudo-inverse G\u2020 of the\n\nforward operator G define the resolution matrix RG\ndef\n= G\u2020G and notice\n\nRGm = G\n\u2020Gm = G\u2020(Gm) ? m (2.11)\n\nis a defeatured version of m that will be exact approximation RGm = m if nothing\n\nis missed in the null space N(G). A simple measure for the bias is the trace Tr(RG);\n\nthe closer to that of the identity matrix, the lower the bias. An exact (but not very\n\nuseful) quantification is obtained comparing the expected value for m\u2020 and the\n\ntrue (unknown) parameters mtrue:\n\nE\n[\nm\u2020\n]\n= E\n\n[\nG\u2020d\n\n]\n= G\u2020E [d] = G\u2020Gmtrue = RGmtrue (2.12)\n\n? BIAS = E\n[\nm\u2020\n]\n? mtrue = (RG ? I)mtrue (2.13)\n\nEquation 2.13 gives a theoretical bound ?BIAS? 6 ?RG ? I??mtrue? since no prior\nknowledge exists for ?mtrue?. It can be further manipulated to incorporate SVD\nfactors RG ? I = VpV?p ? VV\n\n? = ?V0V\n?\n0 .\n\nAnother important issue is the instability of the generalized inverse solution.\n\nSmall singular values cause it to be extremely sensitive to noise in the data as\n\ntranslated by the condition number of G. In practice, all the solutions discussed\u2014\n\nleast-squares and generalized inverse\u2014aren\u2019t implemented directly; techniques for\n\nstabilizing the inverse problem produce considerably better results, the so called\n\nregularization or damped estimation is revised in the next section.\n\nThe regression model can also be very sensitive to outliers, as is the case for\n\nleast-squares. The usual misfit measure for inconsistent data is the L1-norm of\n\nthe residual ?d ? Gm?L1 , its resistance to outliers is often explained by arguing\nthe inconsistency isn\u2019t magnified by taking squares. L1 regression is robust, the\n\nmain reason it\u2019s the second choice is non-differentiability. The minimization is\n\nperformed by iteratively reweighted least-squares [25] which is a sequence of least-\n\nsquares solutions converging to the L1 estimate mL1 . A possible implementation\n\nin the GNU Octave programming language is presented in Appendix B.1.\n\nTherein, the weighted system G?RGm = G?Rd with R\ndef\n= diag\n\n(\n|d ? Gm|\n\np?2\n)\n\ndiminishes the influence of large residuals (e. g. outliers) when 1 6 p &lt;2. It\u2019s\n\nhttp://www.gnu.org/software/octave/\n\n\n2.2 l i n e a r r e g r e s s i o n a n d t h e l e a s t - s q ua r e s e s t i m at e 18\n\nextremely stable for p = 1, specially if a cut-off value is used for rounding up\n\nresiduals that are close to zero.\n\nExample 2.2 (Bullet Trajectory)\n\nThis is the classical problem of fitting a parabola to consecutive snapshots of a\n\nbullet trajectory with the discrete linear model y(t) = m1 + m2t ?\n1\n2\nm3t\n\n2 for the\n\nelevation at instant t > 0. The parameters to be estimated (m1, m2, m3)? have\n\nwell-known physical meaning.\n\nObservations are made and the linear system Gm = d built:\n?\n???\n\n1 t1 ?\n1\n2\nt21\n\n1 t2 ?\n1\n2\nt22\n\n...\n...\n\n...\n\n?\n???\n\n?\n???\n\n???\n\nm1\n\nm2\n\nm3\n\n?\n???\n\n???\n\n=\n\n?\n???\n\n???\n\ny1\n\ny2\n...\n\n?\n???\n\n???\n\nIn the presence of an outlier, the L2 regression underestimates the initial bullet\n\nvelocity and the gravitational field giving a poor trajectory prediction. The L1 is\n\nless sensitive to the inconsistent observation, as shown in Figure 2.3.\n\nTime [t]\n\nE\nle\n\nva\nti\n\no\nn\n\n[y\n(t\n)]\n\nObserved data\nL1 regression\nL2 regression\n\noutlier\n\nFigure 2.3: Bullet trajectory prediction with L1 and L2 regression.\n\nThis plot was reproduced from the already mentioned textbook, example 2.4, as\n\nit illustrates the danger in applying pure least-squares for inversion on arbitrary,\n\nrealistic data sets. ?\n\nWeighted systems like those for the Lp solution naturally arise when uncertainty\n\nin the measurements is stochastically modeled. The Euclidean distance is replaced\n\nby a scale-invariant metric that is better adapted to settings involving non spheri-\n\ncally symmetric distributions.\n\n\n\n2.3 t i k h o n ov r e g u l a r i z at i o n 19\n\nDefinition 2.2 (Mahalanobis distance). The Mahalanobis distance of a multivariate\n\nvector x ? Rn from a group of values with mean \u00b5 ? Rn and covariance ? ? Rn\u00d7n\n\nis defined as:\n\nDM(x) =\n\n?\n(x ? \u00b5)???1(x ? \u00b5)\n\nThe metric in Definition 2.2 is used to formulate a weighted linear least-squares\n\nproblem with Cd the covariance matrix for the measurements:\n\narg min\nm?M\n\n(dobs ? Gm)\n?C?1d (dobs ? Gm) (2.14)\n\nwhere the best estimate6 might have closed form mM = (G?C\n?1\nd\n\nG)?1G?C?1\nd\n\ndobs\n\n(see Appendix A.5). In practice, the noise is assumed to be separate from the input\n\nas in Example 1.3 [26, 27] and the covariance to be diagonal Cd = diag\n(\n?21, ?\n\n2\n2, . . . , ?\n\n2\nm\n\n)\n.\n\nThe least-squares solution is recovered for Cd = I.\n\nTo end this section, it is important to know that uncertainty in the data can be\n\npropagated through all linear estimators m(\u00b7) = G\n(\u00b7)dobs because of a basic result\n\nfrom multivariate statistics.\n\nLemma 1. The covariance of a linear mapping y = Ax + b is Cy = ACxA? with Cx the\n\ncovariance for x [6].\n\nThe covariance for the least-squares estimate is, for instance, derived by setting\n\nA = (G?G)?1G? in Lemma 1:\n\nCmL2\n= (G?G)?1G?CdG(G\n\n?G)?1 (2.15)\n\nand if the measurements are uncorrelated under the same degree of uncertainty\n\nCd = ?\n2I, Equation 2.15 simplifies to:\n\nCmL2\n= ?2(G?G)?1 (2.16)\n\nRemark 1. The classical framework for discrete linear inverse problems gives gen-\n\neralized linear estimates for the parameters and models uncertainty through direct\n\ncovariance propagation.\n\n2.3 t i k h o n ov r e g u l a r i z at i o n\n\nIn the previous section, the general solution to discrete linear inverse problems was\n\nreviewed and, at that time, it was already mentioned that using those estimates\n\n6 Linear minimum-variance unbiased estimate.\n\n\n\n2.3 t i k h o n ov r e g u l a r i z at i o n 20\n\ncould lead to erroneous predictions. This is mainly caused by ill-conditioning as\n\nbriefly explained with SVD on rank-deficient matrices, or by the presence of out-\n\nliers as illustrated by Example 2.2.\n\nRegularization consists of solving the trade-off between resolution and stability\n\nof the estimate. It can be thought as the process of penalizing terms in the SVD of\n\nG that are highly sensitive to noise\u2014terms associated to small singular values.\n\nRecapping Section 2.2, it was mentioned that the pseudo-inverse estimate m\u2020\n\nis also minimum-length, meaning it\u2019s the solution to the following optimization\n\nproblem:\n\nminimize ?m?L2\ns.t. ?d ? Gm?L2 6 t(?)\n\n(2.17)\n\nwhere t(?) is an increasing function of ?. The constraint is usually incorporated\n\ninto the objective in a damped minimization and the resulting problem is known\n\nin statistics as Ridge regression.\n\nminimize (d ? Gm)?(d ? Gm)\n? ?? ?\n\nleast-squares\n\n+ ?2m?m\n? ?? ?\nregularizer\n\n(2.18)\n\nThe regularizer contributes to pushing the solution towards the origin\u2014as ?2\n\nincreases, resolution is lost\u2014and in the case of Ridge regression is represented by\n\nconcentric circles in a 2D model space, see Figure 2.4. For any ?2 ? [0, ?), the\nRidge estimate m? lies on a point of tangency between ellipses (i. e. least-squares)\n\nand circles; in the lower extreme lim\n? 7?0\n\nm? = mL2 .\n\nm1\n\nm2 least-squares\nregularizer\n\n+ mL2\n\n?\n2 ?\n\n0\n\n?\n?\n\n?\n2\n\nm?\n\nFigure 2.4: Ridge estimate m? as a function of ?\n2.\n\n\n\n2.3 t i k h o n ov r e g u l a r i z at i o n 21\n\nThe effect of the regularizer on the normal equations is numerically very intu-\n\nitive, the damped objective is rewritten as augmented norm:\n\nminimize\n\n?????\n\n[\nG\n\n?I\n\n]\nm ?\n\n[\nd\n\n0\n\n]?????\n\n2\n\nL2\n\n(2.19)\n\nand the normal projection gives:\n\n[\nG? ?I\n\n][G\n?I\n\n]\nm =\n\n[\nG? ?I\n\n][d\n0\n\n]\n(2.20)\n\nEquation 2.20 simplifies to\n(\nG?G + ?2I\n\n)\nm = G?d, it\u2019s very clear ?2 is being\n\nadded to the diagonal of G?G to fix its condition number, in which case the esti-\n\nmate can be safely resolved:\n\nm? =\n(\n\nG?G + ?2I\n)?1\n\nG?d (2.21)\n\nWhat is a reasonable value for ?? Once again, it\u2019s a trade-off between resolution\n\nand stability. If the SVD of G ? Rm\u00d7n is substituted in Equation 2.21, a damped\ndecomposition is produced where the damping factors (a. k. a. filter factors) are\n\nwell-determined in terms of the singular values s1, s2, . . . , smin(m,n):\n\nfi\ndef\n=\n\ns2i\ns2\ni\n+ ?2\n\n(2.22)\n\nThe smaller they are s2i ? ?2, the higher the penalty fi ? 0, whereas bigger\nvalues s2i ? ?2 aren\u2019t discarded, fi ? 1. Equation 2.21 can be rewritten in terms of\nthe filter matrix F\n\ndef\n= diag(f1, f2, . . . , fmin(m,n)):\n\nm? =\n(\n\nG?G + ?2I\n)?1\n\nG?d\n\n= G?d\n\n= VFS\u2020U?d\n\n(2.23)\n\nand the resolution RG,?\ndef\n= G?G = VFV? is clearly a function of ?2, the regularizer\n\ndamping constant.\n\nA reasonable candidate for ? can be obtained by the L-curve criterion: the log-\n\nlog cross-plot of ?m??L2 and ?d ? Gm??L2 parameterized by ? has an L-shape.\nThe damping constant should be selected near the corner to minimize both terms\n\nsimultaneously, see Figure 2.57.\n\n7 In Economics, the curve is called the Pareto optimal frontier for the multi-objective minimization.\n\n\n\n2.3 t i k h o n ov r e g u l a r i z at i o n 22\n\n?m??L2\n\n?d\n?\n\nG\nm\n\n?\n? L\n\n2 good candidates\n\nhigh instability\n\nhigh bias\n\nFigure 2.5: L-curve criterion for choosing reasonable damping constants ?2.\n\nAnother very common technique for this selection, mostly applied to s u p e r-\n\nv i s e d l e a r n i n g problems, is k-fold cross-validation: the original data set is ran-\n\ndomly partitioned into k equal size subsets, one of which is retained for validation,\n\nthe other k ? 1 are used for training the model. The process is repeated k times and\n\nthe final estimator is taken as the average of all.\n\ni m p o r ta n t n o t e : Apart from the mentioned techniques\u2014L-curve and k-fold\n\ncross-validation\u2014damping constant selection for Tikhonov regularization can be\n\nmade implicit within an iterative optimization approach [28].\n\nRidge regression (a. k. a. zeroth-order Tikhonov regularization) damps the objec-\n\ntive with the L2-norm of the model parameters. It\u2019s also interesting consider other\n\nnorms on different arguments. The first well-known modification is the use of a L1\n\nregularizer, producing the LASSO:\n\nminimize (d ? Gm)?(d ? Gm) + ?2?m?L1 (2.24)\n\nIt has many advantages over the L2 regularizer, especially when it comes to fea-\n\nture selection (i. e. discarding parameters). The contour lines for the L1-norm have\n\n?-shape and because the minimum occurs at the intersection with least-squares\nellipses, it almost always happens at the diamond corners, see Figure 2.6. The cor-\n\nners are on the axis meaning many parameters in the solution are exactly zero,\n\nthus the name Sparse Regularization.\n\n\n\n2.3 t i k h o n ov r e g u l a r i z at i o n 23\n\nm1\n\nm\n2\n\nleast-squares\nregularizer\n\n0\n\nm?\n\nFigure 2.6: LASSO ?-shape regularizer.\n\nThe augmented objective with L1 regularization is no longer differentiable, the\n\noptimum is derived using the concept of subderivative from convex optimization,\n\ndirectly embedded in Algorithm 2.1. A possible implementation of this shrinkage\n\noperator [29] is presented in Appendix B.2.\n\nIn Tibshirani\u2019s \u201cThe lasso problem and uniqueness\u201d [30], the LASSO solution is\n\ninvestigated from the KKT conditions using the term \u201cwell-defined\u201d in favour of\n\nwell-posed.\n\nAlgorithm 2.1: Coordinate descent for sparse regularization\nInput: G ? Rm\u00d7n, d ? Rm, ? ? R\nOutput: m? = arg min m?M?d ? Gm?\n\n2\nL2\n\n+ ?2?m?L1\n// initialize m randomly or use the Ridge estimate\n\nm ? (G?G + ?2I)?1G?d\nrepeat\n\nforeach column G?j? do update mj:\n\naj ? 2G?j?\n?\n\nG?j?\n\ncj ? 2G?j?\n?\n(d ? Gm + mjG\n\n?j?)\n\nif cj &lt;??\n2 then mj ?\n\ncj + ?\n2\n\naj\n\nelse if cj > ?\n2 then mj ?\n\ncj ? ?\n2\n\naj\n\nelse mj ? 0\nend\n\nuntil converge\n\nreturn m\n\n\n\n2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 24\n\nWhen combined these two regularizers form what is known in the literature as\n\nElastic net regularization. This is mainly done to overcome LASSO limitations for\n\n\u201csmall m, large n\u201d problems.\n\nminimize (d ? Gm)?(d ? Gm) + ?1?m?L1 + ?2?m?\n2\nL2\n\n(2.25)\n\nRidge regression and the LASSO are recovered for ?1 = 0 and ?2 = 0 respectively.\n\nNote that their exponents differ in the objective function.\n\nBias towards the origin isn\u2019t the only choice. If guesses m0 are allowed, they are\n\nincorporated by translation ?m ? m0?L2 . Furthermore, higher-order regularizers\nare produced with the introduction of finite-difference operators.\n\nThe first-order Tikhonov regularization takes the first derivative of the parame-\n\nters into account using the forward difference matrix D1. Similarly, second-order\n\nTikhonov regularization uses central differences D2 to count for second derivatives:\n\nD1\ndef\n=\n\n?\n????????\n\n?1 1\n\n?1 1\n\n\u00b7 \u00b7 \u00b7\n?1 1\n\n?1 1\n\n?\n????????\n\nD2\ndef\n=\n\n?\n????????\n\n1 ?2 1\n\n1 ?2 1\n\n\u00b7 \u00b7 \u00b7\n1 ?2 1\n\n1 ?2 1\n\n?\n????????\n\n(2.26)\n\nThe most general regularizer here discussed ?D(\u00b7)m?L(\u00b7) serves for a variety of\npurposes. For instance, Ridge regression is equivalent to D = I and L2-norm.\n\nThere are no limits to creativity. . .\n\nminimize (d ? Gm)?C?1d (d ? Gm)\n? ?? ?\n\nweighted least-squares\n\n+ ?1?m ? m0?L1\n? ?? ?\n\nregularized towards m0\n\n+ ?2?D1m?2L2\n? ?? ?\nfirst derivative\n\n(2.27)\n\nand a quote by Alan Kay is appropriate to end this section: \u201cThe best way to predict\n\nthe future is to invent it.\u201d\n\n2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n\n\nVery often the forward operator G : M 7?? D is much more complex than a matrix\nmultiplication d = Gm. It might represent an entire engineering system with a\n\nbroad variety of nonlinear interactions. For such systems, linear algebra can\u2019t be\n\napplied directly as in the previous derivations for generalized linear estimators.\n\n\n\n2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 25\n\nOne possible way to tackle a general operator d = G(m) is by iterative optimiza-\n\ntion. First, recall the Newton-Raphson method for finding the roots of a continu-\n\nously differentiable nonlinear square system of equations F(m) = 0:\n\nJ(mk)\n(\nmk+1 ? mk\n\n)\n= ?F(mk) (2.28)\n\nwith J(m) ? ?Fi(m)\n?mj\n\nthe Jacobian matrix. It can be employed to find the local min-\n\nima m? = arg min m?M f(m) of a twice continuously differentiable function f(m)\n\nthrough the necessary condition ?f(m?) = 0:\n\nH(mk)\n(\nmk+1 ? mk\n\n)\n= ??f(mk) (2.29)\n\nwith H(m) ? ?\n2f(m)\n\n?mi?mj\nthe Hessian. Second, note the Newton-Raphson update can-\n\nnot be performed for the system G(m) ? d = 0 even though an analytical expres-\n\nsion may exist. This is because the inverse problem is not guaranteed to have a\n\nsolution G(m?) = d and the system to be square (i. e. m ? n).\nThe Levenberg-Marquardt solution to nonlinear inverse problems consist of ap-\n\nplying Newton-Raphson to a damped least-squares objective:\n\nf(m)\ndef\n=\n\nm?\n\ni=1\n\n(\nG(m)i ? di\n\n?i\n\n)2\n(2.30)\n\nAs with linear regression, if the observations are assumed to be Gaussian, then to\n\nmaximize the likelihood is equivalent to minimize the objective in Equation 2.308.\n\nBy introducing the notation f(m) =\n?m\n\ni=1 fi(m)\n2, and the misfit vector F(m) =\n\n(f1(m), f2(m), . . . , fm(m))?, the gradient and the Hessian needed for the Newton-\n\nRaphson update are given by (see Appendix A.6):\n\n?f(m) = 2J(m)?F(m) (2.31)\n\nH(m) = 2J(m)?J(m) + Q(m) (2.32)\n\nwith J(m) ??F(m) being the Jacobian and Q(m) def= 2\n?m\n\ni=1 fi(m)?2fi(m). A good\napproximation to the Hessian is obtained ignoring the Q(m) term in Equation 2.32,\n\nin this case the update ?m = mk+1 ? mk is such that:\n\nJ(mk)?J(mk)?m = ?J(mk)?F(mk) (2.33)\n\nEquation 2.33 is sometimes referred to as the Gauss-Newton method. The Levenberg-\n\nMarquardt algorithm is introduced with a slight modification:\n(\n\nJ(mk)?J(mk) + ?2I\n)\n?m = ?J(mk)?F(mk) (2.34)\n\n8 The proof in Appendix A.4 is still valid for nonlinear operators G.\n\n\n\n2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 26\n\nwhere ?2 is adjusted during optimization. Larger values leads to steepest decent,\n\nwhereas steps with small penalization ?2I mimic the Gauss-Newton method. The\n\nfinal effect of this penalizer is very much that of the Tikhonov regularization, but\n\nwith a different dynamic interpretation. Refer to Aster et al. for comparing the two.\n\nA specialized derivation for linear operators d = Gm and Gaussian priors can\n\nbe found in Oliver, Reynolds, and Liu\u2019s \u201cInverse Theory for Petroleum Reservoir Char-\n\nacterization and History Matching\u201d [24].\n\ni m p o r ta n t n o t e : More advanced and well-established strategies with simi-\n\nlar adaptive behavior for solving unconstrained optimization exist as part of the\n\nQuasi-Newton family of methods (e. g. DFP, BFGS). They use a numerical approxi-\n\nmation of the Hessian based on successive gradient evaluations.\n\nThe most critical issue with these algorithms is the need for derivatives. The nu-\n\nmerical simulator G is in general a black box either on purpose to avoid complexity\n\nor because its source code isn\u2019t available. Moreover, commercial software doesn\u2019t\n\nnecessarily implement adjoint code [31, 32, 33].\n\nm\n\nG d\n\nu = ??K?p\n\n?\u00b7u = qw\n?w\n\n+\nqo\n?o\n\nFigure 2.7: Numerical simulator as a black box.\n\nFinite differences are very sensitive to the stencil and also very costly. The ap-\n\nproach is unfeasible for demanding simulators in high-dimensions unless a robust\n\nframework such as DAKOTA is used for computing them in parallel, or a low\n\nfidelity proxy is iteratively fitted (e. g. Polynomial, Kriging, Particle filters).\n\nUnlike discrete linear inverse problems\u2014for which inversion is performed by\n\nSVD, Cholesky factorization or conjugate gradients9\u2014nonlinear inverse problems\n\nare challenging due to the lack of characterization of nonlinearities in black box\n\n9 Preferred for large sparse matrices [34].\n\nhttp://dakota.sandia.gov/\n\n\n2.4 l e v e n b e r g - m a rq ua r d t s o l u t i o n t o n o n l i n e a r r e g r e s s i o n 27\n\nsolvers. Introspection and dedicated analysis of the source code for G can over-\n\ncome this actual limitation and further improve performance of existent software.\n\nOther methods such as Kalman filters developed for linear dynamical systems\n\nperform very well in practice even in the presence of nonlinearities. These filters\n\nthat linearize about the current mean and covariance are sometimes referred to as\n\nextended Kalman filters and detailed explanation is out of the scope of this work.\n\nMore recent and powerful variations are obtained with the use of an ensemble,\n\nproducing the EnKF [35, 36, 37] or with probabilistic collocation techniques [38, 39].\n\n\n\n3\nP R O B A B I L I S T I C F R A M E W O R K\n\nInformation can tell us everything. It has all the answers. But they are answers to\n\nquestions we have not asked, and which doubtless don\u2019t even arise.\n\nJean Baudrillard\n\n3.1 Definition of probability . . . . . . . . . . . . . . . . . . . . . . . . 29\n\n3.2 States of information . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n3.3 Bayesian inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n3.4 Ensemble Markov chain Monte Carlo . . . . . . . . . . . . . . . . 42\n\nResponding to the statement that one estimate alone isn\u2019t informative, specially\n\non the richness of high-dimensional spaces, the probabilistic framework relies on\n\na subjective degree of belief to model the plausibility1 of any given preimage. This\n\nmeans that each estimate m ? M is assigned a measure of consistency with previ-\nous observations and expert knowledge (e. g. probability).\n\nThe first important distinction that has to be made against the classical frame-\n\nwork is the retification2 of plausibility distributions. In this chapter, these are the\n\nmain objects of attention and carry all the information about the inverse problem\n\nto be solved\u2014the state of information. Two extreme states are identified, represent-\n\ning maximum uncertainty (i. e. flat shape) and total confidence (i. e. peak shape);\n\nand a Bayesian rule derived from Kolmogorov axioms, suitable for distributions\n\nthat aren\u2019t necessarily normalizable, is used to navigate from one extreme to the\n\nother in the learning direction.\n\nAs in Chapter 2, the specificities of the forward operator G : M 7?? D and the\nassociated spaces are completely hidden so to highlight the core assumptions of\n\n1 The terms plausibility, probability and belief are used interchangeably in this document.\n2 In Programming Languages, retification is the process of creating first-class objects.\n\n28\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 29\n\nthe theory and to make the text accessible to readers coming from different fields.\n\nThey will only be introduced in Part II of the dissertation for the general history\n\nmatching problem or within small simple examples.\n\nAt a higher abstraction level, the solution is naturally formulated as an integral\n\nthat represents the marginalization of the posterior with respect to the noisy output\n\nmeasurements d ? D. It is in general more computationally demanding than the\nmathematical optimization techniques previously presented.\n\nThe MCMC strategy for exact sampling of the posterior is briefly reviewed with\n\na \u201chands on\u201d approach. Among the many variants of the algorithm, only those\n\nwith support for distributed parallel execution should be considered. This is a\n\nstrict and important requirement if the forward operator is expensive.\n\nThis chapter is heavily influenced by Tarantola\u2019s \u201cInverse Problem Theory and\n\nMethods for Model Parameter Estimation\u201d [22].\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y\n\nPlausibility measures follow a multitude of similar axioms depending on the goal\n\nof the theorist [40, 41]. Herein, the term \u201cprobability\u201d is formally redefined and\n\nall other names such as plausibility, belief, etc. assigned the same meaning. This\n\nlinguistic abuse is to avoid confusion with subtle conceptual differences and make\n\nthe reading more pleasant.\n\nDefinition 3.1 (Probability). For a probability space (X, F, P) with X a finite-\n\ndimensional universe, and F the associated ?-algebra, the probability measure of\n\nan event A ? X, denoted P(A) ? R, satisfies the Kolmogorov axioms:\n\n\u2022 P(A?B) = P(A) + P(B) for disjoint events A?B = ?.\n\n\u2022 There is continuity at zero, i. e. , if a non-increasing chain A1 ? A2 ? \u00b7\u00b7\u00b7\ntends to the empty set, then P(Ai) ? 0.\n\nThe function itself P : F 7?? R with no reference to a particular event is called the\nprobability distribution and is written P(\u00b7) for short.\n\nIt follows from Definition 3.1 that the empty set has zero probability P(?) = 0.\nThe universe X is generic notation for either M or D, but it can also represent the\n\nCartesian product X = M\u00d7D. In all cases, there is no guarantee of finite probability3.\n\n3 The distribution may not be normalizable.\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 30\n\nNon-normalizable distributions evaluate to probabilities that can\u2019t be interpreted\n\nintuitively, but are still useful as a relative measure: given two events A, B ? F, it\u2019s\nstill possible to compare P(A), P(B) ? R. This is a crucial point often obfuscated\nby overloaded notation.\n\nDefinition 3.2 (Density). For any probability distribution P(\u00b7) over X and fixed\ncoordinate system, there exists (Radon-Nikodym theorem) f(x), called probability\n\ndensity such that ?A ? X, P(A) =\n?\n\nA\ndx f(x).\n\nExample 3.1 emphasizes that probability densities in Definition 3.2 aren\u2019t nec-\n\nessarily bounded nor intuitive. This exotic behavior will be suppressed with the\n\nnotion of p-events in the following paragraphs.\n\nExample 3.1 (Jeffreys Parameters)\n\nIn Physics, reciprocal variables are usually defined to provide the scientist with\n\ndifferent arguments about the same phenomenon:\n\nx ?? 1/x\nFrequency f ?? Period T = 1/f\nResistivity ? ?? Conductivity ? = 1/?\n\nCompressibility ? ?? Bulk modulus k = 1/?\nConsider that x > 0 is strictly positive like in the pairs in the above diagram.\n\nFor any two samples xa, xb ? X the absolute differences don\u2019t match |xa ? xb| 6=??? 1xa ?\n1\nxb\n\n???, and it would be incorrect to arbitrarily select one of them as the distance\nbetween points. A good distance is invariant under change of coordinates, for\n\ninstance take the resistivity/conductivity pair:\n\nD(?a, ?b)\ndef\n=\n\n????log\n?a\n\n?b\n\n???? =\n????log\n\n?a\n\n?b\n\n???? = D(?a, ?b) (3.1)\n\nEquation 3.1 is accounting for \u201coctaves\u201d4 instead of plain differences. In differential\n\nform, the distance element for D(xa, xb) =\n???log xaxb\n\n??? is given by dL(x) = 1xdx and\ntherefore the unbounded density f(x) = 1/x assigns probabilities proportional to\n\nthe length of the event. These positive reciprocal variables are here called Jeffreys\n\nparameters as suggested by Tarantola [42]. ?\n\nOutside engineering and other applied fields, the notion of density is sometimes\n\ndiscarded in favour of volumetric probabilities. Unlike densities that are affected by\n\nchange of coordinates x? = x?(x):\n\nf?(x?) = f(x)\n\n????\n?x\n\n?x?\n\n???? (3.2)\n\n4 In Music, an octave is the interval between a pitch and another with double its frequency.\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 31\n\nvolumetric measures have fixed shape independently of the coordinate system.\n\nAssume that the notion of volume is supported by X, that is, for any event A ? X\nthe volume V(A) is defined. This is true for most practical inverse problems or if X\n\nhas a metric (see Example 3.1). The volume element dV(x) = v(x) dx is integrated\n\nwith volume density v(x) over the event of interest:\n\nV(A) =\n\n?\n\nA\n\ndx v(x) (3.3)\n\nIt\u2019s then possible to represent any probability distribution P(\u00b7) over X by either\na volumetric probability F(x) or probability density f(x):\n\nP(A) =\n\n?\n\nA\n\ndV(x) F(x) =\n\n?\n\nA\n\ndx f(x) (3.4)\n\nwhere the convention is to use upper case letters for volumetric measures. The\n\nrelation between volumes and densities f(x) = v(x)F(x) is obtained by substituting\n\nthe volume element in Equation 3.4.\n\nEvery reasoning that follows requires a good understanding of these concepts,\n\ninformation is defined in terms of volumes, but implemented in terms of densities to be in\n\nagreement with popular scientific software.\n\nDefinition 3.3 (Homogeneous distribution). The homogeneous probability distribution\n\nM(\u00b7) over X induced by the volume V(\u00b7) is a distribution that assigns for each event\nA ? X a probability proportional to its volume V(A). The homogeneous probability\ndensity is taken proportional to the volume density \u00b5(x) ? v(x):\n\nM(A) =\n\n?\n\nA\n\ndx \u00b5(x)\n\nIf the volume of the universe V =\n?\n\nX\ndx v(x) is finite, the homogeneous density\n\nis normalized \u00b5(x)\ndef\n= v(x)/V, otherwise it only reproduces the shape of the volume\n\ndensity in Definition 3.3, reinforcing the idea that absolute probabilities need not\n\nbe propagated until the posterior is fully characterized.\n\nThe homogeneous distribution in Example 3.1 is directly obtained by looking\n\nup the volume element dL(x) = 1\nx\ndx, its density is proportional to the volume\n\ndensity \u00b5(x) ? 1/x defined over X. Mnemonically, the homogeneous distribution\nfor a Jeffreys parameter x is represented by the density \u00b5(x) = k/x with k ? R+ an\narbitrary positive constant, see Figure 3.1.\n\nLemma 2. If the expression of the volume element dV(x) = v(x) dx is known for a\n\nspace X, the probability density representing the homogeneous distribution is given by\n\n\u00b5(x) = k v(x), k ? R+.\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 32\n\nLemma 3. If there is a metric g in X, then the volume element is dV(x) =\n?\n|det g(x)|dx.\n\nBy Lemma 2, the homogeneous probability density is given by \u00b5(x) = k\n?\n|det g(x)|.\n\nxa xb\n\n\u2022\n\n\u2022\n\nx\n\n\u00b5\n(x\n)\n\nk = 1, 2, 3, . . . ? R+\n\n\u00b5(xa)\n\n\u00b5(xb)\n=\n\nxb\nxa\n\nFigure 3.1: Homogeneous distribution for Jeffreys parameters.\n\nThe dynamics of the system is introduced by conjunction and disjunction of states\n\n(i. e. probability distributions). These two basic operations serve as a mechanism\n\nto manipulate information.\n\nDefinition 3.4 (Disjunction). Given two states P1(\u00b7) and P2(\u00b7) with probability den-\nsity f1 and f2, respectively, the disjunction written (P1 ? P2)(\u00b7) is represented by\ndensity:\n\nf1 ? f2\ndef\n=\n\nf1 + f2\n\n2\n\nThe intuition behind Definition 3.4 is that of a histogram. The mountains and\n\nvalleys of the input distributions are added up producing potentially multimodal\n\nstates, see Figure 3.2.\n\n? =\n\nFigure 3.2: Disjunction for producing histograms.\n\nDisjunction of states alone can be used to approximate complex unknown dis-\n\ntributions as illustrated in Example 3.2. This operation is usually underestimated\n\ncompared to its counterpart\u2014the conjunction.\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 33\n\nExample 3.2 (Cathodic screen)\n\nLet r and ? be the polar coordinates on a cathodic screen and consider a special\n\ndevice emitting electrons onto it continuously. The exact impact location of the\n\nparticle is beyond the instrument capabilities, the best a computer can do is provide\n\na density fi(r, ?) for the coordinates of the i-th impact point.\n\n+ f1(r, ?) + f2(r, ?)\n\n+ f3(r, ?)\n\n+ f4(r, ?)\n\nFigure 3.3: Impact clouds on a cathodic screen.\n\nThe clouds fi(r, ?) in Figure 3.3 are combined by disjunction to approximate the\n\nunknown emission distribution g(r, ?), much like an ordinary histogram:\n\ng(r, ?) ? f1(r, ?) + f2(r, ?) + \u00b7 \u00b7 \u00b7+ fn(r, ?)\nn\n\nThis is an illustration for example 1.11 found in Tarantola [22]. ?\n\nDefinition 3.5 (Conjunction). Given two states P1(\u00b7) and P2(\u00b7) with probability den-\nsity f1 and f2, respectively, and the homogeneous distribution M(\u00b7) over X with\ndensity \u00b5, the conjunction written (P1 ? P2)(\u00b7) is represented by density:\n\nf1 ? f2\ndef\n=\n\n1\n\nv\n\nf1 \u00b7f2\n\u00b5\n\nwith v\ndef\n=\n\n?\n\nX\ndx\n\nf1(x)f2(x)\n\u00b5(x)\n\nthe normalization \u201cconstant\u201d.\n\nThe expression in Definition 3.5 is insensitive to the homogeneous distribution\n\nby design, i. e. , for any distribution P(\u00b7), the conjunction (P ? M)(\u00b7) = P(\u00b7) doesn\u2019t\nadd new information. This is the rationale for defining states of information in the\n\nnext section.\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 34\n\nBoth binary operations can be easily extended to n arguments5:\n\nf1 ? f2 ? \u00b7 \u00b7 \u00b7? fn\ndef\n=\n\nf1 + f2 + \u00b7 \u00b7 \u00b7+ fn\nn\n\n(3.5)\n\nf1 ? f2 ? \u00b7 \u00b7 \u00b7? fn\n\u00b5\n\ndef\n=\n\n1\n\nv\n\nf1\n\n\u00b5\n\nf2\n\n\u00b5\n\u00b7 \u00b7 \u00b7 fn\n\n\u00b5\n(3.6)\n\nwith v\ndef\n=\n\n?\n\nX\ndx \u00b5(x)\n\nf1(x)\n\u00b5(x)\n\nf2(x)\n\u00b5(x)\n\n\u00b7 \u00b7 \u00b7 fn(x)\n\u00b5(x)\n\n. However, this extra typing isn\u2019t necessary in\n\nany of the succeeding derivations.\n\nDefinition 3.6 (p-event). For any event B ? X it\u2019s possible to attach a probability\ndistribution MB(\u00b7), called the p-event of B, through the density:\n\n\u00b5B(x)\ndef\n=\n\n?\n?\n\n?\n\nk \u00b5(x), if x ? B\n\n0, otherwise\n\nwith k ? R+ an arbitrary constant and \u00b5(x) the density for the homogeneous\ndistribution over X.\n\nThe branched expression in Definition 3.6 copies the shape of the homogeneous\n\ndensity inside a particular event (i. e. domain), see Figure 3.4. When evaluated\n\nat an event A ? X, it\u2019s proportional to the volume of the intersection A ? B. In\nmathematical notation this is written MB(A) ? V(A?B).\n\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n\n1\n\n2\n\n3\n\n4\n\n0.5 1 1.5 2 2.5 3 3.5 4 4.5 5\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n\u00b5(x) = 1/x \u00b5B(x), B = [1, 3]\n\nFigure 3.4: The p-event of the interval [1, 3] for a Jeffreys parameter x > 0.\n\nRemark 2. Depending on the choice of the event B ? X, the associated p-event\nMB(\u00b7) is a tool for normalizing distributions within a subdomain of interest.\n\nConditional probabilities are defined in terms of p-events as a special case of\n\nconjunction. The conditioning isn\u2019t done with events, but probability distributions\n\ndirectly. In practical words, information is combined in a fuzzy fashion.\n\nDefinition 3.7 (Conditional probability). Given P(\u00b7) a probability distribution over\nX and an arbitrary event B ? X with associated p-event MB(\u00b7), the conditional\nprobability of P(\u00b7) given B is the conjunction (P ? MB)(\u00b7).\n\n5 The textbook has an error, see list of errata [43].\n\n\n\n3.1 d e f i n i t i o n o f p ro b a b i l i t y 35\n\nIt can be shown that for any event A ? X the expression in Definition 3.7 is\nproportional to the volume of the intersection (see Appendix A.7):\n\n(P ? MB)(A) =\nP(A?B)\nP(B)\n\n(3.7)\n\nand the left-hand side of Equation 3.7 is identified with the usual notation P(A | B).\n\nMoreover, by writing P(A ? B) = P(A | B) P(B) = P(B | A) P(A), the Bayes rule\nrelating conditionals is obtained:\n\nP(A | B) =\nP(B | A) P(A)\n\nP(B)\n(3.8)\n\ni m p o r ta n t n o t e : The Bayes theorem\u2014probability of the causes\u2014showed in\n\nEquation 3.8 is not used in any way by the framework. The update to be defined\n\nin Section 3.3 is performed using simple conjunction of states, and the naming\n\nBayesian inversion remounts only to the fact that probabilities are interpreted as\n\n\u201cdegrees of belief\u201d.\n\nConditioning probability densities is a more complicated topic and some as-\n\nsumptions are needed for the standard formulas to be safely applied. Consider\n\nthe joint density f(m, d) with (m, d) ? M\u00d7D and an application m 7? d(m) from\nM into D. The general idea is to retain only the information of f(m, d) for which\n\nd = d(m), and forget all values for which d 6= d(m), see Figure 3.5. This is done by\ntaking the \u201corthogonal\u201d limit towards d = d(m) since it\u2019s invariant under change\n\nof coordinates [44]. In practical terms, the conditioning of probability densities is\n\nsafe if the forward operator G : M 7?? D isn\u2019t highly nonlinear and the coordinates\nclose to that of a Cartesian system.\n\n-4\n\n-2\n\n 0\n\n 2\n\n 4\n\n-4 -2  0  2  4\n\nd\n=\n\nd\n(m\n\n)\n\n?\n?\n\n?\n?\n\n?\n?\n\nm\n\nd\n\nf(m, d)\n\northogonal\n\nlimit\n\nFigure 3.5: Conditioning the joint density f(m, d) towards the curve d = d(m).\n\n\n\n3.2 s tat e s o f i n f o r m at i o n 36\n\n3.2 s tat e s o f i n f o r m at i o n\n\nProbability distributions are interpreted as states of information to be combined\n\nby means of conjunction/disjunction with the aim of improving the degree of un-\n\nderstanding about a given system. It\u2019s appropriate to formally define two extreme\n\nopposite states for future reference and illustration purposes.\n\nThe first state of information is that of nescience or total ignorance. In Bayesian\n\njargon it would be referred to as noninformative6 and is considered for that school\n\nthe very last option during modeling of random variables. It\u2019s arguably the last\n\noption since priors better be consistent with the current conception of the system\n\nwhich is rarely void.\n\nDefinition 3.8 (Nescience). The state of nescience against the space X is represented\n\nby the homogeneous distribution in Definition 3.3. The notation \u00b5(x) for the prob-\n\nability density is preserved unless a more verbose form like \u00b5X(x) is needed for\n\ndisambiguation.\n\nIt\u2019s important to highlight once more, nescience as homogeneous distribution\n\ndoesn\u2019t imply flat shape for the probability density \u00b5(x). In fact, its shape might\n\nradically change after a change of coordinates x? = x?(x). Scientists that are not\n\nfamiliar with this notion should be careful with their intuition.\n\nThe second extreme state to be defined invokes divinity through the capacity of\n\nfull comprehension of reality or omniscience. It is the choice for deterministic variables\n\nor if uncertainties in the system are to be systematically neglected. Like showed in\n\nthe textbook [22], the solution is here derived for the most general inverse problem,\n\nand only in Part II that uncertainties will possibly be neglected for producing\n\nfeasible case studies.\n\nDefinition 3.9 (Omniscience). The state of omniscience against the space X at point\n\nx0 ? X is represented by the Dirac delta function ?(x; x0).\n\nThe Dirac delta function ?(x; x0) is illustrated in Figure 3.6. It\u2019s such that all the\n\nprobability mass is concentrated at point x0 ? X. For linear spaces, the notation\n?(x ? x0) is commonly used, but since it isn\u2019t particularly advantageous, there is\n\nno need for distinction in this document.\n\nIt was mentioned in the beginning of this chapter that states of information\n\nare first class objects. The general picture to retain in mind when solving inverse\n\n6 Some statisticians disagree with the nomenclature arguing it\u2019s as informative as any other state.\n\n\n\n3.3 b ay e s i a n i n v e r s i o n 37\n\n?\n\nx0\n\nFigure 3.6: Dirac delta function ?(x; x0).\n\nproblems is one in which probability distributions are morphing towards a more\n\npeaked shape, see Figure 3.7. It is noteworthy that because most inverse problems\n\nare ill-posed, there may be various peaks instead of a unique mass concentration\n\npoint. Formally, it is said that states of information converge in distribution.\n\n\u00b5(x)\nD?? ?(x; x0)\n\nFigure 3.7: Nescience towards Omniscience.\n\nThe rules for navigating in the learning direction\u2014for which uncertainty is con-\n\nsistently reduced\u2014are presented in the next section, and even though multimodal\n\ndistributions are the most common answer, they progressively present small vari-\n\nance near the peaks as new evidence is incorporated.\n\nRemark 3. The classical framework seeks the best match to the data, whereas the\n\nprobabilistic framework is only concerned with uncertainty mitigation.\n\n3.3 b ay e s i a n i n v e r s i o n\n\nBayesian inversion translates into uncertainty modeling and mitigation (i. e. setting\n\npriors and incorporating data) in that order. In this section, all sources of uncer-\n\ntainty regarding the general inverse problem G : M 7?? D are identified using a\nconsistent and mnemonic notation; and only after that, information is combined.\n\nUncertainties are either experimental or theoretical. Inversion is precisely the conjunc-\n\ntion of these two kinds together with posterior marginalization, as will be shown\n\nin the following paragraphs. Moreover, a multi-stage update can be performed in\n\nan outer loop using disjunction of states if the time interval at which (reliable) data\n\nis observed is considerably greater than the time spent with marginalization.\n\n\n\n3.3 b ay e s i a n i n v e r s i o n 38\n\n3.3.1 Uncertainty in the model space\n\nThe first prior that can be extremely hard to set is the initial belief about the real\n\nmodel mtrue ? M. This state of information should be supported by experts in\nthe field and extensively proven to be plausible according to all available evidence.\n\nThis a priori state is represented by probability density ?M(m) which can be equal\n\nto nescience \u00b5M(m) in absence of good characterization.\n\nExample 3.3 (Gaussian prior)\n\nConsider models m ? Rn, the Gaussian centered at mprior = (m1, m2, . . . , mn)?\n\nwith covariance Cm has the form:\n\n?M(m) = ((2?)\nn det Cm)\n\n?1/2 exp\n(\n?\n1\n\n2\n(m ? mprior)\n\n?C?1m (m ? mprior)\n\n)\n\nand is illustrated for the 2D case in Figure 3.8.\n\n?M(m2) ?M(m1)\n\nm1 m2\n\n?\nM\n(\nm\n\n1\n,m\n\n2\n)\n\nFigure 3.8: 2D Gaussian N\n(\nmprior, Cm\n\n)\n.\n\nFrom the modeler\u2019s perspective, the guess that best supports the available data\n\nbefore inversion is the center mprior of the Gaussian, it\u2019s subject to precision C?1m .\n\n?\n\n3.3.2 Uncertainty in the data space\n\nSimilarly to the model space, the prior state for the observed data dobs ? D is\ndenoted ?D(d). It is hardly the nescience state or considered free of uncertainty\u2014\n\nomniscience ?(d; dobs). It can take the shape of a multivariate Gaussian N (dobs, Cd)\n\nas in Example 3.3 or any other distribution.\n\n\n\n3.3 b ay e s i a n i n v e r s i o n 39\n\n3.3.3 Uncertainty in the forward operator\n\nIf the theory behind the forward operator G : M 7?? D was very solid, free of\nrounding errors and consistent for any conceivable input m ? M, the graph of\nG itself would be sufficient for modeling uncertainty (or the lack of it). However,\n\nin general, the numerical simulator is only trusted up to a certain degree, and\n\ndefining a conditional probability ?(d | m) on the output given the input is more\n\nsatisfactory approach, see Figure 3.9.\n\nm\n\nd\n\n?(d | m)\n\nG\n\nFigure 3.9: Graph of G replaced by the conditional distribution ?(d | m).\n\n3.3.4 Combining information\n\nThe joint state ?(m, d) over M\u00d7D accounts for all experimental uncertainty in the\ninverse problem. By design, ?M(m) assembled from static data and ?D(d) guessed\n\nfrom initial evidence are independent, and thus, it follows ?(m, d) = ?M(m) ?D(d).\n\nThe joint state ?(m, d) over M\u00d7D accounts for all theoretical uncertainty derived\nby conditioning the forward operator to its input ?(m, d) = ?(d | m) \u00b5M(m).\n\nExample 3.4 (Perfect Forwarding)\n\nIf the scientist neglects uncertainty in the forward operator G on purpose, the\n\ndistribution ?(m, d) = const. ?(d; G(m)) concentrates all the mass in the curve\n\nd = G(m). Both marginals ?M(m) =\n?\n\nD\ndd ?(m, d) and ?D(d) =\n\n?\n\nM\ndm ?(m, d)\n\nare constant and don\u2019t carry rich information about neither M nor D separately.\n\nThis assumption is designated here perfect forwarding7. ?\n\n7 The name has no relation with the same term in the context of programming languages (e. g. C++).\n\n\n\n3.3 b ay e s i a n i n v e r s i o n 40\n\nThe conjunction of both theoretical and experimental states is taken with \u00b5(m, d)\n\nthe homogeneous density over M\u00d7D and k some normalization constant:\n\n?(m, d)\ndef\n= k\n\n?(m, d) ?(m, d)\n\u00b5(m, d)\n\n(3.9)\n\nBy the same reason ?(m, d) is decoupled as a product of states, it is true that\n\n\u00b5(m, d) = \u00b5M(m) \u00b5D(d). The desired solution to the general inverse problem is\n\nthe result of marginalization:\n\n?M(m) =\n\n?\n\nD\n\ndd ?(m, d) (3.10)\n\nEquation 3.10 defines the posterior density over the model space M. It has all\n\nthe information before and during the application of G : M 7?? D, and as Bayesian\ninversion is a fixed-point iteration, ?M(m) is the next prior to be used in the right-\n\nhand side of Equation 3.9. Furthermore, it can be rewritten as prior times likeli-\n\nhood (remember Equation 1.8):\n\n?M(m) = k ?M(m)\n\n?\n\nD\n\ndd\n?(d | m) ?D(d)\n\n\u00b5D(d)\n(3.11)\n\nConsidering that the forward operator is a fully-featured engineering simulation,\n\nthe likelihood in Equation 3.11 is a very computationally expensive integral over\n\nthe data space D. There are two relevant scenarios to emphasize at this point de-\n\npending on how negligible is the theoretical uncertainty for the particular inverse\n\nproblem being studied.\n\np e r f e c t f o r wa r d i n g The more tractable inversion where perfect forward-\n\ning, defined in Example 3.4, is assumed to hold true. The conditional distribution\n\nover the linear space D is concentrated ?(d | m) = ?(d ? G(m)) with homogeneous\n\ndistribution \u00b5D(d) = const. The forward operator G is evaluated only once in the\n\nlikelihood:\n\nL(m) =\n\n?\n\nD\n\ndd\n?(d ? G(m)) ?D(d)\n\n\u00b5D(d)\n= const. ?D(G(m)) (3.12)\n\ng e n e r a l c a s e Perfect forwarding doesn\u2019t hold. The forward operator is a\n\nblack box and no analytical antiderivative is available. Thus, the integral L(m) is\n\napproximated using quadrature rules or Monte Carlo integration. For example:\n\nL(m) ? 1\nN\n\nN?\n\ni=1\n\n?(di | m)\n\n\u00b5D(di)\n, di ? ?D(d) (3.13)\n\n\n\n3.3 b ay e s i a n i n v e r s i o n 41\n\nThe g e n e r a l c a s e with a Cartesian system is illustrated in Figure 3.9. The\n\nforward operator is still evaluated once as the mean of the conditional ?(d | m) for\n\na given m ? M, this is made clear by a Gaussian distribution with mean G(m) and\ntheoretical precision C?1t :\n\n?(d | m) = ((2?)m det Ct)\n?1/2 exp\n\n(\n?\n1\n\n2\n(d ? G(m))?C?1t (d ? G(m))\n\n)\n(3.14)\n\ni m p o r ta n t n o t e : The prior ?D(d) might also be Gaussian with probability\n\ndensity N(dobs, Cd), however it does not involve any call to the forward operator G.\n\nAccording to this exposition, the probabilistic framework seeks the posterior\n\ndistribution ?M(m) ? ?M(m) L(m) as the solution to the general inverse problem\nintroduced in Chapter 1. After data d ? D is observed and compared to the output\nof the theoretical transfer function G : M 7?? D, the posterior gives a rich descrip-\ntion on the model parameters m ? M in terms of probabilities. Practical questions\ncan be answered with such description:\n\nWhat is the most probable model after observation is made? It\u2019s known as the MAP\n\nestimate, or the sample with greater probability:\n\nmMAP = arg max\nm?M\n\n?M(m)\n\n\u00b5M(m)\n(3.15)\n\nWhat is the probability an event E has happened? The posterior integrated over all\n\nsamples m ? E:\n\nPr(E) =\n?\n\nE\n\ndm ?M(m) (3.16)\n\nWhat is the expected value for the model space? By definition, it\u2019s an integral or\n\nweighted summation over all samples m ? M:\n\nE[m] =\n\n?\n\nM\n\ndm m ?M(m) (3.17)\n\nAlthough far more general than the classical framework, this theory faces one\n\nmajor challenge\u2014computational efficiency. Except for specific problems like those\n\nwith Gaussian priors and linear mapping where ?M(m) is analytically obtained\n\n(refer to Tarantola), the full characterization of the posterior can be very hard to\n\ntackle. In high dimensions, the likelihood needs to be evaluated multiple times in\n\na Markov chain, making the implementation yet more costly.\n\nIn the g e n e r a l c a s e, the likelihood L(m) is the integral over D shown in\n\nEquation 3.11. Whenever dim D ? dim M and the prior ?D(d) is addressed by\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 42\n\nwhat is called the Wiener-Askey scheme8, L(m) can be accurately approximated\n\nusing advanced sparse grid integration techniques [39, 45, 46, 47, 48]. Otherwise,\n\neither dim D is large or ?D(d) is not handled by the polynomial chaos expansion,\n\nretaining the correctness of transitions in this Markov process can be very difficult.\n\nIgnoring performance issues for a moment, the posterior does always exist and\n\nis uniquely defined. It\u2019s common to say the probabilistic framework enjoys implicit\n\nregularization rather than explicit, see Section 2.3. If ?M(m) ? 0 is identically null,\nit means uncertainties were likely underestimated or that the conjunction of states\n\nwas unable to deal with information inconsistency.\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o\n\nThe ultimate role of MCMC in this chapter is to sample complex posterior dis-\n\ntributions in high-dimensional spaces assuming candidates can be easily drawn\n\nfrom the correspondent prior or proposal distribution. In traditional Monte Carlo,\n\nspace exploration is quite ineffective in capturing the distribution in its entirety,\n\nand some of the probability modes might never be visited or be poorly sampled\n\ndue to the \u201ctotally random\u201d strategy of the sampler (e. g. direct sampling, rejection\n\nsampling, importance sampling).\n\nRandom exploration in high dimensions can sometimes be improved9 by simply\n\nrecording the very last candidate drawn, the Markov chain assumption states in\n\nsimple words future is independent of the past given the present and leads to one\n\nof the most influential family of algorithms developed in this century (e. g. Gibbs\n\nsampling, Metropolis-Hastings, etc.). For the sake of clarity, the definitions are here\n\nlimited to discrete time stochastic processes with discrete state space. The reader is\n\npointed to Gamerman and Lopes\u2019s \u201cMarkov Chain Monte Carlo: Stochastic Simulation\n\nfor Bayesian Inference\u201d [49] for a more general presentation of the subject.\n\nFollowing the notation in that textbook, a discrete time stochastic process is a\n\nfamily of random quantities {?(t) : t ? T} over a state space S with countable index\nset T. Without loss of generality, indices are taken from the set of natural numbers\n\nN and generally represent iterations of a simulation scheme. The states ?(t) ? S\nare usually vectors in Rn, but can also be matrices or any other multidimensional\n\nabstraction. Special interest is paid to the dynamic and limiting behavior of the\n\n8 Most usual parametric distributions: uniform, triangular, gamma, etc.\n9 In the sense of visiting and sampling high probability regions.\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 43\n\nsequence (?(1), ?(2), ?(3), . . .) as this is what the computer will be sampling for\n\nlong runs.\n\nDefinition 3.10 (Markov chain). The discrete time stochastic process {?(n) : n ? N}\nover S is a Markov chain if it satisfies\n\nPr\n(\n?(n+1) ? A | ?(n) = x, ?(n?1) ? An?1, . . . , ?(0) ? A0\n\n)\n\n= Pr\n(\n?(n+1) ? A | ?(n) = x\n\n)\n\nfor all A0, . . . , An?1, A ? S and x ? S.\n\nIf the state space S is further assumed to be discrete, Definition 3.10 is usually\n\nrewritten on a point basis x0, . . . , xn?1, x, y ? S:\n\nPr\n(\n?(n+1) = y | ?(n) = x, ?(n?1) = xn?1, . . . , ?(0) = x0\n\n)\n\n= Pr\n(\n?(n+1) = y | ?(n) = x\n\n) (3.18)\n\nThe notion of state transition x ? y is evident in Equation 3.18. If it doesn\u2019t\ndepend on the iteration counter n = 0, 1, 2, . . ., the chain is said to be homogeneous.\n\nDefinition 3.11 (Transitional probability). For a homogeneous Markov chain, the\n\ntransitional probability P(\u00b7, \u00b7) is a function such that:\n\n\u2022 ?x ? S, P(x, \u00b7) is a probability distribution over S\n\n\u2022 ?A ? S the function x 7? P(x, A) can be evaluated\n\nP(\u00b7, \u00b7) is also named the transition or kernel function. If S = {x1, x2, . . . , xr} is\nfinite, the transition matrix defined by Pij\n\ndef\n= P(xi, xj) summarizes the behavior of\n\nthe chain in jumping between states xi ? xj:\n\nP =\n\n?\n???\n\nP(x1, x1) \u00b7 \u00b7 \u00b7 P(x1, xr)\n...\n\n. . .\n...\n\nP(xr, x1) \u00b7 \u00b7 \u00b7 P(xr, xr)\n\n?\n??? (3.19)\n\nand since Definition 3.11 was established here for homogeneous chains, the matrix\n\nP is fixed during simulation. Its rows add up to unit\n?\n\nj Pij = 1, or equivalently,\n\n1\ndef\n= (1, 1, . . . , 1)? is an eigenvector of P with eigenvalue 1.\n\nProbabilities are assigned to each point10 x1, x2, . . . , xr ? S per iteration, the\ninitial distribution ?(0) =\n\n(\n?(0)(x1), ?(0)(x2), . . . , ?(0)(xr)\n\n)\nis multiplied on the\n\nright by the transition matrix P to get the next state ?(1) = ?(0)P and the process\n\nis repeated ?(n) = ?(0)Pn towards an equilibrium distribution that is guaranteed to\n\nexist if the chain is irreducible and aperiodic [50].\n\n10 In history matching, each point is a property map such as a permeability field.\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 44\n\nExample 3.5 (Finite State Machine)\n\nConsider a non-deterministic 3-state machine with transitional probabilities illus-\n\ntrated in Figure 3.10. This automata is such that at any given time n = 1, 2, . . .\n\nthere is a chance ?(n) =\n(\n?(n)(x1), ?(n)(x2), ?(n)(x3)\n\n)\nof being somewhere in\n\n{x1, x2, x3}.\n\nx1\n\nx2\n\nx3\n\n1.0 0.9\n\n0.6\n\n0.4\n\n0.1\n\nFigure 3.10: Graph for a 3-state {x1, x2, x3} machine.\n\nThe transition matrix P =\n[\n\n0 1 0\n0 0.1 0.9\n0.6 0.4 0\n\n]\nfor the graph admits an equilibrium dis-\n\ntribution ?(?) = ( 27\n122\n\n, 50\n122\n\n, 45\n122\n\n) no matter the initial guess ?(0). Hence, the graph\n\nis connected (i. e. irreducible) and aperiodic. The ternary plot in Figure 3.11 illus-\n\ntrates the limiting behavior of the chain limn?? ?(n) starting at ?(0) = (1, 0, 0).\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 0.2 0.4 0.6 0.8 1\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n?(n)(x1)?\n(n)(x2)\n\n?(n)(x3)\n\nFigure 3.11: limn?? ?(n) starting at ?(0) = (1, 0, 0).\n\nIn Linear Dynamical Systems, this convergence is fully understood. The station-\n\nary or invariant distribution ?(?) is the normalized non-trivial solution to ?P = ?\n\nor equivalent homogeneous system ?(P ? I) = 0. ?\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 45\n\nExample 3.5 has its value in clarifying notation and helps introducing some\n\nimportant concepts in MCMC simulation. First, the hitting time for a particular\n\nevent A ? S defined as TA\ndef\n= min{n > 0 : ?(n) ? A} is not easy to predict in\n\ngeneral. Most available implementations of MCMC allow for a burn-in period which\n\nmeans to throw away the initial steps of the chain afterwards avoiding samples\n\nfrom distributions other than that of equilibrium. This policy is criticized by some\n\nresearchers in the field who prefer to put effort designing representative priors and\n\nnever resort to burn-in, anyways it is still useful feature to have in software. Second,\n\nthe limiting or equilibrium distribution ?(?) is what the chain aims to mimic.\n\nAfter various iterations (see Figure 3.11), this target distribution is sampled almost\n\nperfectly in a convergence rate guided by the transitional probability P(x, y) in\n\nEquation 3.20 for discrete state spaces or transition kernel p(x, y) in Equation 3.21\n\nfor continuous state spaces.\n\n?(n+1)(y) =\n?\n\nx?S\nP(x, y)?(n)(x) (3.20)\n\n?(n+1)(y) =\n\n?\n\nS\n\np(x, y) ?(n)(x) dx (3.21)\n\nIrreducibility and aperiodicity, and therefore uniqueness of the stationary distri-\n\nbution ?, are not always possible to assert in practice. A sufficient condition for the\n\nexistence of ? = ?P known as detailed balance is often used to design transitions.\n\nDefinition 3.12 (Reversible chain). A Markov chain is said to be reversible if its\n\ntransitional probability satisfies detailed balance:\n\n?x, y ? S, ?(x)P(x, y) = ?(y)P(y, x)\nIntuitively, Definition 3.12 is saying that the rate at which the system moves from\n\nx to y when in equilibrium, ?(y)P(x, y), is the same as the rate at which it moves\n\nfrom y to x, ?(x)P(y, x). Such reversible chains can be built using Hastings idea of\n\nacceptance probability [51].\n\nThe transition kernel p(x, y) is factored into p(x, y) = q(x, y)?(x, y), x 6= y,\nthat is, a proposal q(x, y) times an acceptance ?(x, y)\n\ndef\n= min\n\n{\n\n1, ?(y)q(y,x)\n?(x)q(x,y)\n\n}\n\n. The\n\nproposed move x ? y is quite general as no strong requirements are imposed on\nq(x, y), it\u2019s accepted or not depending on the probability ratio with the current\n\nposition, thus a Markov chain.\n\nRemark 4. The proposal q(x, y) is \u201cany\u201d (often random) procedure to generate y\n\ngiven the current position is x (e. g. y ? N(x, ?2)).\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 46\n\nAlgorithm 3.1: Metropolis-Hastings\nInput: posterior ?(\u00b7), proposal q(\u00b7,\u00b7), N ? N\nOutput: samples xn ? ?, n = 1, 2, . . . , N\n\n// initialize the chain intelligently\n\nx0 ? xguess\nforeach n = 0, 1, 2, . . . , N ? 1 do sample xn+1:\n\ndraw u ? U[0,1]\ndraw x? ? q(xn, x?)\n\n?(xn, x?) ? min\n{\n\n1,\n?(x?)q(x?, xn)\n?(xn)q(xn, x?)\n\n}\n\nif u &lt;?(xn, x?) then xn+1 ? x?\n\nelse xn+1 ? xn\nend\n\nreturn x1, x2, . . . , xN\n\nAlgorithm 3.1 is one of the most popular implementations of the last decade\n\nin statistical-related fields (e. g. Data Analysis, Machine Learning). The posterior\n\ndensity ?(\u00b7) is only evaluated in a ratio ?(x?)\n?(xn)\n\nwhich has the advantage that no full\n\ncharacterization (i. e. normalizing constants) is needed to generate samples.\n\nThe Metropolis-Hastings algorithm solves the difficult problem of sampling a\n\ncomplex distribution represented by ?(\u00b7) with the introduction of an auxiliary pro-\nposal q(\u00b7, \u00b7). In the algorithm, acceptance probabilities can be rewritten:\n\n?(xn, x\n?) = min\n\n{\n\n1,\n?(x?)/q(xn, x?)\n?(xn)/q(x\n\n?, xn)\n\n}\n\n(3.22)\n\nso to notice the ratio gets close to unit when the proposal is a good approximation\n\nof the target q ? ?. In Example 3.6, different proposals are employed to show the\nimpact on MCMC sampling.\n\nExample 3.6 (Gaussian mixture)\n\nConsider the target distribution is proportional to the conjunction of two Gaussian\n\nmodes ?(x) ? 0.3e?x2 + 0.7e?(x?10)2 and that the move x ? y is also Gaussian\ncentered at the current position of the chain q(x, y) = 1\n\n?\n?\n2?\n\ne\n? 1\n\n2\n(y?x)2\n\n?2 . For such\n\ncases where the proposal is symmetric q(x, y) = q(y, x), the acceptance takes the\n\nsimpler form ?(x, y) = min\n{\n\n1, ?(y)\n?(x)\n\n}\n\nas originally introduced by Metropolis in a\n\nchemical physics problem [52]. A possible implementation in the R programming\n\nlanguage is found in Appendix B.3, it should not be compared to/preferred over\n\nwidely tested packages available on CRAN.\n\nhttp://www.r-project.org/\nhttp://cran.r-project.org/\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 47\n\nThe standard deviation ? in the proposal q(x, y) is the tuning parameter used\n\nto produce different histograms in Figure 3.12. In all three settings, the chain is\n\ninitialized in the left mode x = 0 and run for N = 5000 iterations.\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n?5 0 5 10 15\nx\n\n?(\nx)\n\n? = 1\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n?5 0 5 10 15\nx\n\n?(\nx)\n\n? = 10\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n?5 0 5 10 15\nx\n\n?(\nx)\n\n? = 100\n\nFigure 3.12: Metropolis sampling with different proposals ? = 1, 10, 100.\n\nFor small perturbations ? = 1, the chain is unable to get out of the left mode\n\nand the bimodal characteristic of the target is totally lost. For a wide proposal ? =\n\n100, both modes are captured, but the rejection is too high (? 98%). Satisfactory\nhistograms are produced in between these extremes with acceptances as good as\n\n50% and honoring the Gaussian mixture model. Since the target distribution isn\u2019t\n\nexpensive, automatic tuning can be easily achieved with Bayesian optimization.\n\n?\n\nThe most natural-to-ask challenging-to-answer question regarding MCMC is\n\nwhen to stop the chain? Convergence diagnostics exist, but are sometimes misleading\n\nand require careful inspection to avoid fallacy. Widely used visual diagnostics are\n\nthat of trace and autocorrelation plots. The former is the history of samples while the\n\nlater measures the dependence across successive MCMC iterations as separated by\n\nlag distances. Both illustrated in Figure 3.13 for the three chains in Example 3.6.\n\nDefinition 3.13 (Autocorrelation). The autocorrelation of a chain {?(t) : t ? T} for\na lag distance k ? N is the correlation coefficient between ?(t) and ?(t+k):\n\n?(k)\ndef\n=\n\ncov\n(\n?(t), ?(t+k)\n\n)\n\n?t ?t+k\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 48\n\n0 1000 2000 3000 4000 5000\n\n?\n3\n\n?\n2\n\n?\n1\n\n0\n1\n\n2\n\n? = 1\n\nIterations\n\nx\n\n0 5 10 15 20 25 30 35\n\n?\n1\n\n.0\n?\n\n0\n.5\n\n0\n.0\n\n0\n.5\n\n1\n.0\n\n? = 1\n\nLag\n\nA\nu\n\nto\nco\n\nrr\ne\n\nla\ntio\n\nn\n\n0 1000 2000 3000 4000 5000\n\n?\n2\n\n0\n2\n\n4\n6\n\n8\n1\n\n0\n1\n\n2\n\n? = 10\n\nIterations\n\nx\n\n0 5 10 15 20 25 30 35\n\n?\n1\n\n.0\n?\n\n0\n.5\n\n0\n.0\n\n0\n.5\n\n1\n.0\n\n? = 10\n\nLag\n\nA\nu\n\nto\nco\n\nrr\ne\n\nla\ntio\n\nn\n\n0 1000 2000 3000 4000 5000\n\n?\n2\n\n0\n2\n\n4\n6\n\n8\n1\n\n0\n1\n\n2\n\n? = 100\n\nIterations\n\nx\n\n0 5 10 15 20 25 30 35\n\n?\n1\n\n.0\n?\n\n0\n.5\n\n0\n.0\n\n0\n.5\n\n1\n.0\n\n? = 100\n\nLag\n\nA\nu\n\nto\nco\n\nrr\ne\n\nla\ntio\n\nn\n\nFigure 3.13: Trace and autocorrelation for the chains in Example 3.6.\n\nThe samples for ? = 100 are highly correlated even for large lags as indicated\n\nby the plots. This chain is therefore inferior in quality compared to the other two.\n\nHowever, autocorrelation alone doesn\u2019t tell much about the mixing of the chain (i. e.\n\n\u201ccloseness\u201d to steady state) as the trace for ? = 1 is trapped in the left mode.\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 49\n\nMore recently, sophisticated visualization techniques were introduced for diag-\n\nnosing MCMC convergence11 in high-dimensional spaces [53]. They complement\n\nthe commonly used potential scale reduction factor diagnostic developed by Gelman\n\nand Rubin [54] and as such, also rely on the assessment of multiple chains. Vari-\n\nous other visual and nonvisual diagnostics exist in the literature [55], all with some\n\nimperfection. Nonetheless, they provide great insight about the process dynamics.\n\nThe idea of multiple chains in MCMC is inspired by general optimization of\n\nmultimodal objectives. Different starting points are fed in iterative procedures that\n\nare only guaranteed to find local minima. These locals are then considered together\n\nto reveal the global minimum, or a very good candidate for it. In general Bayesian\n\ninference, multiple chains are started from an overdispersed distribution that is an\n\ninitial approximation of the target, they might overlap in the long run or capture\n\ndifferent modes of the stationary distribution if lucky.\n\nIn inverse problem theory, the target distribution to be sampled with MCMC is\n\nthe posterior ?M(m) over the model space M. As already explained in Section 3.3.4,\n\neach sample involves a call to the forward operator G : M 7?? D on a point m? ? M\nthat now can be understood as a proposal m? ? ?M(m).\n\nExample 3.7 (Root-finding)\n\nConsider the forward operator f : R 7?? R+\n0\n\nis a parabola f(x) = x2 that is being\n\nrepeatedly evaluated on a point x? ? R. The point is unknown, but the output is\nvery poorly monitored\u2014modeled by a Gaussian centered at y = 4. Probabilistic\n\ninversion for x consists in the following settings:\n\n\u2022 x ? U[?10,10] (a prior state)\n\n\u2022 y = x2 (a forward operator)\n\n\u2022 y ? N(4, 1) (observations)\n\nThe solution in Figure 3.14 was implemented in the Python programming lan-\n\nguage using the development version of the PyMC12 module which contains the\n\nfunction stochastic_from_data() contributed by myself. The code in Appendix B.4\n\nis an infinite loop where, at each iteration, y is observed and state of information\n\nabout x is updated.\n\n11 Or better, the lack of it.\n12 PyMC version 2.\n\nhttp://www.python.org/\nhttps://github.com/pymc-devs/pymc\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 50\n\nIt can be seen the analytical roots to x2 = 4 are recovered in the histogram as\n\nmodes \u00b12 even for noisy output measurements. More generally, given a function\nx 7? f(x) and a target y?, the solution space f(x) = y? can be characterized by a\nprobability distribution.\n\nFigure 3.14: Finding the roots of x2 = 4.\n\nEven though f(x) = x2 is not invertible, all the roots were recovered. In the clas-\n\nsical framework, only one of {?2, 2} would be found depending on the algorithm\n\n(e. g. Bisection, Newton, Secant) and its starting point.\n\nThe reader is encouraged to experiment with the code, in particular try differ-\n\nent priors (e. g. rnormal(0,1,10000)) to see the learning process in action. For\n\nconvenience, the function stochastic_from_data() was copied from the PyMC\n\nrepository to Appendix B.5. Although root-finding was previously formulated as\n\na forward problem (see Example 2.1), it is solved here, on a different perspective,\n\nby inverse problem theory. ?\n\nIn Example 3.7, arbitrary histograms (a. k. a. non-parametric distributions) are\n\nfitted with kernel density estimation [56, 57], a linear combination of symmetric\n\nbasis functions Kh centered at each sample xi ? R:\n\nf?h(x) =\n1\n\nn\n\nn?\n\ni=1\n\nKh(x ? xi) (3.23)\n\nwith h the kernel bandwidth. It can be thought as a convolution (see Appendix A.8)\n\nand generalized to multiple dimensions as depicted in Figure 3.15.\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 51\n\nFigure 3.15: Kernel density estimation for a 2D Gaussian.\n\nAll single-chain samplers in the previous examples share a serious limitation in\n\nthat they cannot be parallelized. The likelihood function needs to be evaluated in\n\nsequence as, for any Markov process, the future depends on the present. It was\n\nthis simple claim that caused researchers to develop multi-chain samplers for taking\n\nadvantage of multicore processors and computer clusters.\n\nConsider a Markov chain of ensembles {?(t) : t ? T}, that is, each point ?(t) ? Snw\n\nis a collection of nw points (or walkers) in S, written ?(t) =\n{\n\n?\n(t)\n1\n\n, ?(t)\n2\n\n, . . . , ?(t)nw\n}\n\n.\n\nAt first, it might seem a bad idea replace the sampling in S by the one in Snw ,\n\nspecially if S is high-dimensional (e. g. permeability maps). However, the ensemble\n\ncarries rich information about the target distribution at each iteration and more\n\nimportantly, some of the walkers can advance in parallel without violating detailed\n\nbalance. This is the rationale behind Markov chain ensemble samplers proposed here\n\nfor the history matching problem.\n\nIn such samplers, the jumps for a given walker are dictated by the ensemble of\n\nthe corresponding iteration. In other words, the proposal distribution takes into\n\nconsideration all the chains being run in parallel to decide where to move next.\n\nMoves ?(t)\nk\n? ?(t+1)\n\nk\nare proposed for each chain ?(t)\n\nk\n? ?(t) based on what is\n\ncalled the complementary ensemble:\n\n?\n(t)\n\n[k]\n\ndef\n=\n\n{\n\n?\n(t+1)\n1\n\n, . . . , ?(t+1)\nk?1\n\n, ?(t)\nk+1\n\n, . . . , ?(t)nw\n}\n\n(3.24)\n\nwhich can be represented by a matrix with dim S rows and nw ? 1 columns. The\n\nnotation indicates all chains up to k ? 1 were moved before iteration k and that\n\nthese new positions are used to update ?(t)\nk\n\n, similar to Gibbs sampling [49].\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 52\n\nFor problems where no physical interpretation is available to design reasonable\n\nproposals, the stretch move works incredibly well [58]. It was designed to be affine\n\ninvariant meaning its performance is unaffected by highly anisotropic distributions\n\n(e. g. highly correlated variables) as the one illustrated in Figure 3.16.\n\nx1\n\nx\n2\n\nFigure 3.16: Anisotropic joint distribution for x1 and x2.\n\nThe stretch is made along a line connecting two distinct walkers in the ensemble\n\nusing a scaling random variable Z with density g(z) such that g(1\nz\n) = z g(z). This\n\nconstraint ensures the proposal is symmetric [59]. Researchers in the field suggest,\n\nfor instance:\n\ng(z) ?\n\n?\n?\n\n?\n\n1?\nz\n\n, if z ?\n[\n1\na\n\n, a\n]\n\n0, otherwise\n(3.25)\n\nwith a > 1 a tuning parameter. Algorithm 3.2 gives a better description of the\n\nmove happening in Figure 3.17.\n\n?\n\n?\n(t)\nk\n\n??\n\nFigure 3.17: Stretch move ?(t)\nk\n? ?(t+1)\n\nk\nfor the ensemble sampler.\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 53\n\nAlgorithm 3.2: Stretch move in Rn\n\nInput: posterior ?(\u00b7), ensemble ?(t) =\n{\n\n?\n(t)\n1\n\n, ?(t)\n2\n\n, . . . , ?(t)nw\n}\n\nOutput: updated ensemble ?(t+1)\n\nforeach k = 1, 2, . . . , nw do move ?\n(t)\nk\n? ?(t+1)\n\nk\n:\n\ndraw u ? U[0,1]\nchoose ? ? ?(t)\n\n[k]\nat random\n\npropose ?? = ? + Z(?(t)\nk\n\n? ?)\n\n?(?\n(t)\nk\n\n, ??) ? min\n{\n\n1, Zn?1\n?(??)\n\n?(?\n(t)\nk\n\n)\n\n}\n\nif u &lt;?(?\n(t)\nk\n\n, ??) then ?(t+1)\nk\n\n? ??\n\nelse ?\n(t+1)\nk\n\n? ?(t)\nk\n\nend\n\nreturn ?(t+1)\n\nThe complementary ensemble is represented by gray dots, one of which is cho-\n\nsen at random. It\u2019s denoted ? in the sketch and determines the direction ?(t)\nk\n\n? ?\n\nfor the line search with random step size Z. Similar picture and a more detailed\n\nexplanation of the algorithm can be found in the literature, as well as convergence\n\ntests showing the superior performance of this MCMC method [58].\n\nIt\u2019s tempting to parallelize the loop k = 1, 2, . . . , nw by simultaneously advanc-\n\ning each walker in the ensemble. This practice violates detailed balance and there-\n\nfore the correctness of the sampling for the stretch move. To overcome such barrier,\n\na partition trick was proposed that presents an ideal speed-up of nw\n2\n\n[60].\n\nThe ensemble is partitioned into two halves ?(t) = ?l ??r, all walkers in the\nleft half ?l are updated in parallel based on walkers in the right half only, then\n\nthe update is performed for the other half ?r the same way using the just updated\n\nensemble ?l. These consecutive updates ?l ? ?r and ?r ? ?l each involve nw2\nwalkers, hence the speed-up in evaluating the likelihood.\n\nThe parallel stretch move outperforms the classical Metropolis-Hastings algo-\n\nrithm considerably, either with respect to autocorrelation or acceptance fraction\n\nof the moves. The tuning with a > 1 in Equation 3.25 is analogous to the usual\n\ntuning of the covariance matrix in a Gaussian proposal. If the acceptance fraction\n\nis too low, it can be raised by decreasing a; if it\u2019s too high, the parameter better be\n\nincreased. In the paper, the authors suggest to fix a = 2 inspired by the excellent\n\nresults with various target distributions modeled in Astrophysics [60].\n\nBeyond trial and error with the parameter a, the parallel-tempered version of\n\nthe sampler proves to be very effective strategy in the presence of many discon-\n\n\n\n3.4 e n s e m b l e m a r k ov c h a i n m o n t e c a r l o 54\n\nnected modes [61]. Such target distributions are difficult to sample from because\n\nin general likely zones are separated by extremely low probability regions.\n\nA parallel-tempered ensemble sampler uses the same physical analogy as does sim-\n\nulated annealing [62] but with multiple replicas running at different temperatures\n\nin parallel. The mixing is improved by exchanging \u201cenergy\u201d between hot and\n\ncold configurations. The higher is the temperature, the better is the exploration,\n\nwhereas lower temperatures are tied to local investigation of the state space. Paral-\n\nlel tempering can tremendously improve the performance of the sampler for multi-\n\nmodal distributions [63]; temperature scheduling and other important features of\n\nthe method [64] aren\u2019t discussed here to avoid deviating too much from inverse\n\nproblem theory.\n\nAs a matter of fact, the most successful methods for history matching today (e. g.\n\nEnKF, RML) are ensemble samplers developed under the assumption of Gaussian\n\ndistributions. Although nonlinearity of the forward operator is sometimes hidden\n\nusing a trick with extended state vectors, it is still an open issue for these types\n\nof filters [35, 36, 65], specially smoothers for which data is assimilated all at once\n\ninstead of sequentially in time [66].\n\nIn Part II, ensemble samplers will be used to approximate the evolving state of\n\ninformation on model parameters present in petroleum reservoir history matching.\n\nThe chapters therein seek a probability distribution over the reservoir description\n\nthat is consistent with production levels and other relevant performance indicators.\n\nImplementation difficulties regarding computational performance will surely\n\narise\u2014that is the main point of this work.\n\n\n\nPart II\n\nH I S T O R Y M A T C H I N G\n\nIn which the state of information about the petroleum reservoir (e. g.\n\nporosity/permeability maps, net-to-gross, depth of oil\u2013water contact)\n\nis updated using inverse problem theory. The discussion is restricted to\n\nspecific case studies designed to mimic important features of realistic\n\npetroleum fields.\n\n\n\n4\nP R E L U D E\n\nNo problem is too small or too trivial if we really do something about it.\n\nRichard Feynman\n\n4.1 Problem description . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\n4.2 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n\n4.3 Comments on reproducibility . . . . . . . . . . . . . . . . . . . . 63\n\nHistory\n\nMatching\n\nHigh\n\nPerformance\n\nComputing Multicore\n\nCPU\n\nMPI\n\nGPGPU\n\nInverse\n\nProblem\n\nTheory\n\nParallel\n\nTempering\n\nEnsemble\n\nMCMC\nEnKF\n\nMachine\n\nLearning\n\nKernel\n\nMAF\n\nKernel\n\nPCA\n\nGeostatisticsfiltersim\n\nsnesim\n\nFigure 4.1: Mind map of algorithms and concepts.\n\nThe mind map in Figure 4.1 gives an overview of some of the algorithms that will\n\nbe implemented or used in the following chapters. Not all leaves will necessarily\n\nbe utilized, and the picture is better seen as a non-exhaustive visual organization\n\nof today\u2019s history matching technology.\n\n56\n\n\n\n4.1 p ro b l e m d e s c r i p t i o n 57\n\nLeaves in red color represent the computational expensive core of the solution,\n\nthese algorithms require massive evaluation of the reservoir simulator (i. e. forward\n\noperator) and therefore determine the feasibility of the framework. In green color\n\nare the leaves that makes this discussion possible: distributed-memory execution\n\nusing the Message Passing Interface (MPI); and correct usage of modern, shared-\n\nmemory, multicore CPUs.\n\nLeaves in orange color are either auxiliary algorithms for parametrization and\n\ndimensionality reduction (e. g. kPCA, kMAF), advanced sampling of the proposal\n\ndistribution (e. g. filtersim, snesim); or represent the usage of graphics cards to\n\nfasten geostatistical simulation (i. e. GPGPU).\n\nAll these acronyms will be revisited right before their application within case\n\nstudies in order to clarify their exact contribution. The text is written in the form\n\nof a tutorial to ease understanding of the material and enlight cause/effect of the\n\ntaken decisions.\n\n4.1 p ro b l e m d e s c r i p t i o n\n\nThe modern formulation of the history matching problem is exactly that of the\n\nprobabilistic framework introduced in Part I. It consists of three elements:\n\nM O D E L S PA C E The parameters m ? M include permeability/porosity\nmaps, depth of oil\u2013water contact, relative permeability curves, or any other\n\nuncertain physical property of the petroleum reservoir.\n\nD A T A S PA C E The measurements d ? D include well production rates\n(oil, gas, water), bottom-hole pressure (BHP), recovery factors, or any other\n\ndynamic output available during production.\n\nF O R WA R D O P E R A T O R The forward operator G : M 7?? D is a fully-\nfeatured numerical reservoir simulator.\n\nin the quest for information [67] about the reservoir description. Uncertainties in\n\nthe petrophysical properties of the reservoir are mitigated based on the production\n\nhistory of the petroleum field, and there is no direct attempt to find the \u201cbest\u201d model\n\nfor the data1.\n\n1 Perhaps History-based Uncertainty Mitigation (HUM) is a better term for modern history matching.\n\n\n\n4.2 c a s e s t u d i e s 58\n\n4.2 c a s e s t u d i e s\n\n4.2.1 Channelized reservoir\n\nA 2D training image extensively used at Stanford university for testing MPS sim-\n\nulation algorithms [11, 68, 69, 70, 71] is interpreted as the permeability field of a\n\nsynthetic fluvial reservoir. The channels in Figure 4.2 present high permeability\n\nand determine the pattern of the flow in this black-oil model.\n\nFigure 4.2: Training image of size 250x250 pixels.\n\nGrid specifications and petrophysical properties are summarized in Table 4.1.\n\nThe OPM based reservoir simulator written for this case study is free software,\n\nits source code is the best resource for exact reproduction of additional settings\n\nsuch as relative permeability function, convergence criteria, etc. These won\u2019t be\n\nreplicated here. At a higher abstraction level, the incompressible flow is modelled\n\nwith a simple TPFA pressure solver and implicit Euler transport.\n\nTable 4.1: Channelized reservoir summary table.\n\nDescription Value Description Value\n\nNumber of cells 250\u00d7250\u00d71 Porosity (?e) 30%\nCell size 10 m \u00d710 m \u00d71 m Low permeability (?l) 10 mD\n\nFluid phases water + oil High permeability (?h) 10 D\n\nWater density (?w) 1000 kg/m3 Oil density (?o) 890 kg/m3\n\nWater viscosity (\u00b5w) 0.89 cP Oil viscosity (\u00b5o) 1 cP\n\nThere are two groups of wells, 8 producers in the first and 2 injectors in the\n\nsecond, all illustrated in Figure 4.3. The total liquid rate for each group is controlled\n\nhttp://www.opm-project.org/\nhttps://github.com/juliohm/HUM/blob/master/case_studies/channelized/simulator.cpp\n\n\n4.2 c a s e s t u d i e s 59\n\nto never exceed 2000 m3/day at reservoir conditions. The simulation runs for 20\n\ntime steps of 90 days each, totaling 5 years of production. Refer to Figure 4.4 for a\n\nstatic movie of oil/water saturation and to Figure 4.5 for the production history.\n\n?\nProd1\n\n?\nProd2\n\n?\nProd3\n\n?\nProd4\n\n?\nProd5\n\n?\nProd6\n\n?\nProd7\n\n?\nProd8\n\n?\nInj1\n\n?\nInj2\n\nFigure 4.3: Ten-spot well configuration.\n\nFigure 4.4: Oil/water saturation for various time steps within channelized reservoir.\n\n0 5 10 15 20\n\n242\n\n244\n\n246\n\n248\n\n250\n\n252\n\n254\n\nProd1 Prod2 Prod3 Prod4\n\n0 5 10 15 20\n\n245\n\n250\n\n255\n\nProd5 Prod6 Prod7 Prod8\n\n0 5 10 15 20\n\n?1,020\n\n?1,010\n\n?1,000\n\n?990\n\n?980\n\n?970\n\nInj1 Inj2\n\nTime step\n\nP\nro\n\nd\nu\n\nct\nio\n\nn\n[m\n\n3\n/\n\nd\nay\n\n]\n\nFigure 4.5: Production history for channelized reservoir.\n\ni m p o r ta n t n o t e : The simulator can write VTK files if requested by passing\n\nthe --vtk option. For further functionality, --help.\n\nThe training image in Figure 4.2 is the true (unknown) model mtrue, only used\n\nto generate the production history dobs in Figure 4.5 as if measured on a real\n\nplatform. Any permeability value which varies between [?l, ?h] is scaled to the\n\nunit interval [0, 1] for keeping numbers small in numerical computations. In terms\n\nhttp://www.vtk.org/\n\n\n4.2 c a s e s t u d i e s 60\n\nof implementation, the reservoir simulator expects input data to be a whitespace\n\n(or end of line) separated list of values in the interval [0, 1].\n\nHence, the model space is the unit hypercube M ? [0, 1]62500; the data space has\ndimension dim D = 20 time steps \u00d78 producers = 160; and the forward operator\nG takes m ? M, scales it to the range [?l, ?h], and perform the flow simulation\nreturning d ? D, the production history.\n\n4.2.2 Brugge field\n\nThe Brugge field is a benchmark case developed and made available by TNO for\n\nassessing closed-loop control strategies and methods for history matching [72].\n\nVarious researchers in the field participated in this comparative study and shared\n\ntheir results in the form of journal publications [73, 74].\n\nFigure 4.6: Brugge field.\n\nThe field in Figure 4.6 consists of an E\u2013W elongated half-dome with a large\n\nboundary fault at its northern edge, and an internal fault with a modest throw\n\nat an angle of 20? to the former. Its extension is roughly 10 km \u00d73 km including\nfour rock formations (a. k. a. reservoir zones). Formation properties are averaged\n\nin Table 4.2 along with some remarks.\n\nThere are 60048 cells (44550 active). Prior realizations for porosity, horizontal\n\nand vertical permeability, net-to-gross and connate water saturation are given. Poro-\n\nperm regression curves for each rock formation and entire reservoir are provided\n\nand can be used to correlate these petrophysical properties during generation of\n\nnew realizations [72]:\n\nKx = 0.01 e\n45.633?mD (4.1)\n\nhttps://www.tno.nl\n\n\n4.2 c a s e s t u d i e s 61\n\nTable 4.2: Brugge averaged properties per rock formation.\n\nFormation Thickness Porosity Permeability Net-to-gross Remarks\n\nSchelde 10 m 20.7% 1105 mD 60% Discrete sand\n\nbodies in shale\n\nWaal 26 m 19.0% 90 mD 88% Contains loggers:\n\ncarbonate\n\nconcretions\n\nMaas 20 m 24.1% 814 mD 97% \u2014\n\nSchie 5 m 19.4% 36 mD 77% Irregular carbonate\n\npatches\n\nThe 20 producer and 10 injector wells in Figure 4.7 are all concentrated near\n\nthe top of the dome. The free water level is at 1678 m and the model is initialized\n\nwith reference pressure of 2466 psi at 1700 m. Verbose settings such as PVT table\n\nand well scheduling are found in the source code and won\u2019t be replicated here.\n\nRelative permeability curves shown in Figure 4.8 for reference are considered free\n\nof uncertainty.\n\nFigure 4.7: Top view of Brugge field porosity realization.\n\nThe true reservoir model is kept in secret by TNO on purpose and only its\n\nhistory is made available for 10 years of production\u2014BHP, oil/water production\n\nrate per well and production/injection for the entire field. Oil production rates\n\nfor all producer wells are collected at each time step t of CMGTM IMEX R\u00a9 as the\nobservation vector dtobs. This means dim D = 20 oil rates and that the Bayesian\n\ninference is performed multiple times (e. g. number of time steps).\n\nhttps://github.com/juliohm/HUM/blob/master/case_studies/brugge/brugge.tmpl\n\n\n4.2 c a s e s t u d i e s 62\n\n0.2 0.4 0.6 0.8 1\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nk r\nw\n\nk\nro\nw\n\nSw\nk\nr\n\nFigure 4.8: Brugge field relative permeability curves.\n\nOil production is shown in Figure 4.9 for all 20 producers (BR-P-1 to BR-P-20)\n\nand 122 time steps.\n\n0 1,000 2,000 3,000 4,000\n\n0\n\n100\n\n200\n\n300\n\nBR-P-1 BR-P-2 BR-P-3\nBR-P-4 BR-P-5\n\n0 1,000 2,000 3,000 4,000\n\n0\n\n100\n\n200\n\n300\n\nBR-P-6 BR-P-7 BR-P-8\nBR-P-9 BR-P-10\n\n0 1,000 2,000 3,000 4,000\n\n0\n\n100\n\n200\n\n300\n\nBR-P-11 BR-P-12 BR-P-13\nBR-P-14 BR-P-15\n\n0 1,000 2,000 3,000 4,000\n\n0\n\n100\n\n200\n\n300\n\nBR-P-16 BR-P-17 BR-P-18\nBR-P-19 BR-P-20\n\nTime step\n\nP\nro\n\nd\nu\n\nct\nio\n\nn\n[m\n\n3\n/\n\nd\nay\n\n]\n\nFigure 4.9: Brugge field oil production history.\n\nIn summary, this inverse problem consists of porosity values m ? M for all\n44550 active cells in the grid, IMEX as the forward operator G and observations\n\ndtobs collected in an online fashion (i. e. for each time step).\n\n\n\n4.3 c o m m e n t s o n r e p ro du c i b i l i t y 63\n\n4.3 c o m m e n t s o n r e p ro du c i b i l i t y\n\nMost of the scientific work produced today isn\u2019t reproducible. If reproducibility is\n\njust a matter of data and software availability, then various tools and services exist\n\nto make results reachable by other researchers and interested readers.\n\nThe chapters herein serve as documentation for the case studies made available\n\non the web2. Although all studies were designed with reproducibility in mind,\n\nsome of them require commercial software for evaluation. Namely, the CMGTM\n\nIMEX R\u00a9 reservoir simulator and Results Report R\u00a9 tool are both required by the\nBrugge field case.\n\n2 Check https://github.com/juliohm/HUM for software.\n\nhttps://github.com/juliohm/HUM\n\n\n5\nC H A N N E L I Z E D R E S E R V O I R\n\nImagination is more important than knowledge. For knowledge is limited, whereas\n\nimagination embraces the entire world, stimulating progress, giving birth to evolution. It\n\nis, strictly speaking, a real factor in scientific research.\n\nAlbert Einstein\n\n5.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n\n5.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . 69\n\n5.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\nWe\u2019re given observations dobs and a conceptual model mguess in the form of a\n\ntraining image, and are asked for a probability distribution over M. Although, for\n\nthis case study, the given model equals the true model mtrue, this information is\n\nnot used anywhere in this chapter because the true model is unknown in practice.\n\nThe Bayesian approach starts regularizing the inverse problem by setting priors\n\nover both the spaces, M and D. It\u2019s the most critical component towards a good\n\napproximation of the posterior, and is here attempted with MPS simulation, kPCA\n\nparametrization and KDE.\n\nAfter priors have been set, inversion is performed with the further assumption of\n\nperfect forwarding (see Example 3.4). Updated ensembles and a few convergence\n\nindicators such as the ensemble sampler mean acceptance fraction are analyzed to\n\nassess the effectiveness of the method.\n\nThe software exploits multiple levels of parallelism, including distributed-memory\n\nexecution with MPI on a time-shared computer cluster and multicore computing\n\nby spawning processes.\n\n64\n\n\n\n5.1 s e t t i n g p r i o r s 65\n\n5.1 s e t t i n g p r i o r s\n\n5.1.1 Prior on model parameters\n\nThe conceptual model mguess in Figure 4.2 is simulated with Filtersim [71, 75] for\n\ntaking high-order statistics into consideration. Elaborating on this MPS simulation\n\nalgorithm is considered out of scope, but guidelines for tuning its parameters are\n\navailable on the web [76]. The initial ensemble of 200 realizations is generated with\n\nthe following XML parameter file in SGeMS:\n\nListing 5.1: filtersim.xml\n\n1&lt;parameters>&lt;algorithm name=\" filtersim_cont \" />\n\n2&lt;GridSelector_Sim value=\" filtersimGrid \" region=\" \" />\n\n3&lt;Property_Name_Sim value=\"perm\" />\n\n4&lt;Nb_Realizations value=\" 200 \" />\n\n5&lt;Seed value=\" 211175 \" />\n\n6&lt;PropertySelector_Training grid=\"mguess\" property=\" permeability \" region=\" \" />\n\n7&lt;Patch_Template_ADVANCED value=\"7 7 1\" />\n\n8&lt;Scan_Template value=\" 11 11 1\" />\n\n9&lt;Trans_Result value=\"0\" />\n\n10&lt;Hard_Data grid=\" \" property=\" \" region=\" \" />\n\n11&lt;Use_SoftField value=\"0\" />\n\n12&lt;Region_Indicator_Prop value=\" \" />\n\n13&lt;Active_Region_Code value=\" \" />\n\n14&lt;Use_Previous_Simulation value=\"0\" />\n\n15&lt;Previous_Simulation_Prop value=\" \" />\n\n16&lt;Use_Region value=\"0\" />\n\n17&lt;Nb_Multigrids_ADVANCED value=\"3\" />\n\n18&lt;Debug_Level value=\"0\" />\n\n19&lt;Cmin_Replicates value=\" 10 10 10 \" />\n\n20&lt;Data_Weights value=\" 0.5 0.3 0.2 \" />\n\n21&lt;CrossPartition value=\"1\" />\n\n22&lt;KMeanPartition value=\"0\" />\n\n23&lt;Nb_Bins_ADVANCED value=\"5\" />\n\n24&lt;Nb_Bins_ADVANCED2 value=\"2\" />\n\n25&lt;Use_Normal_Dist value=\"0\" />\n\n26&lt;Use_Score_Dist value=\"1\" />\n\n27&lt;Filter_Default value=\"1\" />\n\n28&lt;Filter_User_Define value=\"0\" />\n\n29&lt;/parameters>\n\nRandomly selected realizations are shown in Figure 5.1 for reference. They all\n\npresent channels similar to that of the training image as intended. Only the first\n\n100 out of 200 realizations are used in the study.\n\n\n\n5.1 s e t t i n g p r i o r s 66\n\nFigure 5.1: Filtersim realizations for channelized training image.\n\nRemark 5. The prior state of information about the petroleum reservoir mtrue is\n\nintroduced with (MPS) simulation.\n\ni m p o r ta n t n o t e : I ported the SGeMS build maintained by AR2Tech R\u00a9 to\nthe GNU/Linux operating system. Please refer to https://github.com/juliohm/\n\nar2tech-SGeMS-public for installation instructions.\n\nThe prior density ?M(m) needs to be evaluated anywhere in the model space M,\n\nnot just at the ensemble members\n{\n\nm(k)\n}100\nk=1\n\n. This is a requirement imposed by\n\nthe MCMC algorithm with arbitrary proposals explained in Section 3.4. Moreover,\n\ndim M = 62500 is far too high for KDE. A simple attempt can show that all RAM\n\nof a modern computer (e. g. 8GB Linux 64bits) is exhausted by the current SciPy\n\nGaussian KDE implementation.\n\nParametrization with dimensionality reduction is employed to circumvent both\n\nof these issues. Among the available methods, PCA [77, 78] and MAF [79, 80]\n\nwere investigated in their kernel-based versions [81, 82, 83]. kPCA performed very\n\nwell for the ensemble under study, whereas kMAF, on the other hand, ended up\n\nproducing numerically unstable generalized eigenproblems1, therefore, the later\n\nwas not used with this dataset.\n\nIn a different context, MAF (without kernels) has been successfully applied in\n\nmining [84, 85] to avoid the modelling of cross-covariance in the Linear Model of\n\nCoregionalization [13]. This technique is particularly powerful when more than two\n\nrandom fields are to be cosimulated (e. g. grade estimation in multivariate deposits).\n\n1 Of course, the code might have bugs.\n\nhttps://github.com/juliohm/ar2tech-SGeMS-public\nhttps://github.com/juliohm/ar2tech-SGeMS-public\nhttp://scipy.org/\n\n\n5.1 s e t t i n g p r i o r s 67\n\nRegardless of the parametrization technique adopted, the idea is the same: write\n\nthe permeability map m ? M that has dimension dim M = 62500 in terms of a\nreduced number (e. g. 50) of uncorrelated coordinates ? = (?1, ?2, . . . , ?50)?:\n\nm = m(?) (5.1)\n\nIn linear PCA (a. k. a. K-L decomposition [86, 87, 88]), the parametrization is a\n\nproduct m = E?1/2? that maximizes the variance of the ensemble. Here, E and ?\n\nare truncated matrices of eigenvectors and eigenvalues of the ensemble covariance.\n\nAs a side note, the covariance is never computed directly because of its size, for\n\ninstance 62500\u00d762500. A dual formulation with kernels is implemented instead,\nthat solves eigenproblems of size 100\u00d7100 (i. e. number of realizations squared).\n\nFor nonlinear parametrization m = m(?), vectors are mapped2 onto an abstract\n\nvery high-dimensional (possibly infinite) feature space F where the ensemble is in\n\nsome sense \u201clinearly separable\u201d [81]. This mapping ? : M 7?? F introduces a new\npreimage problem [89, 90, 91] that is attacked with the classical framework, either by\n\ntransport of metrics or fixed-point iteration. Previous attempts have shown little\n\ndifference between these two variants [92], and the later is used in this work.\n\nThe minor details of kPCA parametrization and associated preimage problem\n\nare treated in Appendix C. A specialized matricial formulation is developed for\n\nthe polynomial kernel purposed in Sarma\u2019s \u201cEfficient Closed-loop Optimal Control of\n\nPetroleum Reservoirs under Uncertainty\u201d Ph.D. thesis [70] and derived publications\n\n[88]. It\u2019s a sum up to degree d for preserving up to 2d-th order statistics:\n\nk(x, y)\ndef\n= ?x, y?+ ?x, y?2 + \u00b7 \u00b7 \u00b7+ ?x, y?d (5.2)\n\nFigure 5.2 shows how Equation 5.1 can be used to reconstruct 250\u00d7250 pixels,\nchannelized images m from 50 uncorrelated coordinates ?. The channel patterns\n\nare consistently recovered as the degree is increased, and this is made clear by\n\ndenoised versions shown for each reconstruction. More importantly, it proves that\n\nlinear PCA (i. e. d = 1) isn\u2019t able to capture channel patterns in the ensemble.\n\nRemark 6. MPS simulation and nonlinear kPCA parametrization are responsible\n\ntogether for generating and preserving high-order statistics.\n\nAfter parametrization with dimensionality reduction is performed, the initial\n\nensemble\n{\n\nm(k)\n}100\nk=1\n\nis mapped to the feature space\n{\n\n?\n(k)\n\n}100\n\nk=1\nand the forward\n\noperator is replaced by the function composition:\n\nG?\ndef\n= G ? m (5.3)\n\n2 The mapping is never actually evaluated.\n\n\n\n5.1 s e t t i n g p r i o r s 68\n\nd = 1 d = 2 d = 3 d = 4\n\nKernel PCA for increasing degrees: reconstruction above and denoised version below\n\nFigure 5.2: kPCA for increasing polynomial kernel degrees.\n\nThus, EnMCMC will be updating lower-dimensional ensembles in a reduced\n\nspace ? through the application of G?(?) = G(m(?)). This inverse problem is\n\nrepresented by the triple (?, D, G?) which is not equivalent to (M, D, G) due to\n\nparametrization losses, but can be implemented with today\u2019s computers.\n\nThe prior state of information on model parameters is obtained by Gaussian\n\nKDE on the initial ensemble of features\n{\n\n?\n(k)\n\n}100\n\nk=1\nusing a bandwidth computed\n\nby 10-fold cross-validation:\n\n?M(m(?)) = KDE(?) (5.4)\n\nThis data-driven approach with cross-validation is preferred over Scott\u2019s rule of\n\nthumb [56] implemented in SciPy by default as it does not impose any assumption\n\non the ensemble. Among the available implementations (e. g. SciPy, scikit-learn,\n\nStatsmodels), only scikit-learn\u2019s KernelDensity correctly exploits log-scale to avoid\n\nthe \u201cvanishing problem\u201d3, and is therefore used.\n\n5.1.2 Prior on observations\n\nThe history for each of the 8 producer wells in Figure 4.5 is concatenated into a\n\nvector dobs of size 8 wells \u00d720 time-steps = 160. The prior state of information on\nobservations is assumed to be multivariate Gaussian centered at dobs:\n\n?D(d) = N(dobs, I) (5.5)\n\n3 Probabilities vanish in high dimensions (e. g. p = 0.1 =? p50 ? 0).\n\nhttp://scipy.org\nhttp://scikit-learn.org\nhttp://statsmodels.sourceforge.net\n\n\n5.2 p ro b a b i l i s t i c i n v e r s i o n 69\n\n5.2 p ro b a b i l i s t i c i n v e r s i o n\n\nPerfect forwarding is assumed to hold true which means the posterior probability\n\nis simply the product of shapes in Equation 5.4 and Equation 5.5:\n\n?M(?) ? ?D(G?(?)) ?M(?) (5.6)\n\nIn log-scale, this update rule is written log ?M(?) = log ?D(G?(?)) + log ?M(?)\n\nwhere the constant term is dropped for simplicity. It will be canceled out in the\n\nMCMC acceptance criterion ultimately.\n\nThe number of walkers in EnMCMC is equal to the number of members in the\n\nensemble, in this case 100 walkers are used to explore R50. It must be an even\n\nnumber since the ensemble is divided into two halves for parallelization by the\n\nemcee Python package. That is, 50 reservoir simulations are run in parallel and\n\nthe ensemble is updated in two consecutive steps with total theoretical wall-time\n\nof twice that of a single reservoir simulation.\n\nThree proposal distributions are investigated: the stretch move in ? explained\n\nin Section 3.4, KDE sampling of the prior, and a Filtersim-based proposal inspired\n\nin Hansen et al.\u2019s work [15, 93, 94, 95, 96]. In the later case, Filtersim is used to\n\ngenerate 100 new realizations that are mapped to the feature space F.\n\nThe Hastings acceptance probability for the (symmetric) stretch move ? ? ?? is\na function of the posterior ?M(?):\n\n?(?, ??) = min\n{\n\n1,\n?M(?\n\n?\n)\n\n?M(?)\n\n}\n\n(5.7)\n\nThe KDE and Filtersim based proposals are assumed to sample the true prior\n\ninformation q(?, ??) ? ?M(??) and therefore their acceptance only depends on the\nlikelihood:\n\n?(?, ??) = min\n{\n\n1,\n?M(?\n\n?\n) ?M(?)\n\n?M(?) ?M(?\n?\n)\n\n}\n\n= min\n{\n\n1,\nL(?\n\n?\n) ?M(?\n\n?\n) ?M(?)\n\nL(?) ?M(?) ?M(?\n?\n)\n\n}\n\n= min\n{\n\n1,\nL(?\n\n?\n)\n\nL(?)\n\n}\n\n(5.8)\n\nThe prior is canceled out in Equation 5.8 but this does not mean it\u2019s not being\n\nconsidered by the framework. In fact, it\u2019s being sampled as the ideal proposal.\n\nFurthermore, custom proposals like these present ideal theoretical speed-up of nw\n\ncompared to the stretch move which can handle at most nw\n2\n\nreservoir simulations\n\nat a time.\n\nhttp://dan.iel.fm/emcee\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 70\n\n5.3 a na ly s i s o f t h e r e s u lt s\n\nResults are reported for each of the three proposal distributions mentioned in\n\nthe previous section. Possible causes for bad/good performance of the applied\n\nmethods are highlighted whenever possible. The intent here is to reason against\n\nthe applicability of the framework and illustrate implementation difficulties.\n\nDue to deadline constraints and long job queues in the cluster4, Markov chain\n\nwalkers advanced no more than 1000 steps and are quite probably in high autocor-\n\nrelation regime. Each step consists of 100 evaluations of the forward operator, that\n\nis 1000\u00d7100 = 100000 evaluations in total.\n\n5.3.1 Parallel stretch move\n\nThis configuration proved to be ineffective. Stretch moves in ? were such that the\n\nassociated preimages did have extremely low probability according to the KDE fit\n\nwith the prior ensemble. In other words, doing such moves in the ensemble\n{\n\n?\n(k)\n\n}\n\nused for KDE training produces candidates that are very unlikely in 50 dimensions.\n\nThis is one possible manifestation of the \u201cvanishing problem\u201d.\n\nRemark 7. Generic moves that are independent of the target problem, such as the\n\nparallel stretch move, will surely present inferior performance in high dimensions.\n\nThe acceptance fraction is exactly zero for all of the walkers in the ensemble and\n\ntherefore the state of information is unchanged. In any case, log-probabilities are\n\nshown in Figure 5.3 for giving the reader a sense of magnitude.\n\nPrior and posterior distribution coincide. Values around ?800 show the real need\n\nfor working in log-scale as in finite floating-point precision exp(?800) = 0 and all\n\nensemble members would be equally implausible; which is clearly not true.\n\n5.3.2 KDE-based proposal\n\nThis time candidates are directly proposed by KDE assuming that it\u2019s the prior\n\nstate of information on ?. This is reasonable assumption since the training step\n\nwas done with a featurized version of the prior ensemble\n{\n\nm(k)\n} ???\n\n{\n\n?\n(k)\n\n}\n\n.\n\n4 c e na pa d - p e: http://www.cenapad-pe.ufpe.br\n\nhttp://www.cenapad-pe.ufpe.br\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 71\n\nprior posterior\n\nprior vs. posterior\n\n2000\n\n1800\n\n1600\n\n1400\n\n1200\n\n1000\n\n800\n\n600\n\n400\n\nlo\ng\n-p\n\nro\nb\na\nb\nil\nit\n\ny\n\nFigure 5.3: Stretch move: bean plot of prior and posterior log-probabilities.\n\nThe acceptance criterion depends only on the log-likelihood and no more on\n\nthe evaluation of the prior, alleviating slightly the vanishing of probabilities. In\n\nFigure 5.4, the posterior ensemble members are more likely to represent the true\n\nunknown model than their prior versions before history assimilation as expected.\n\nprior posterior\n\nprior vs. posterior\n\n1200\n\n1100\n\n1000\n\n900\n\n800\n\n700\n\n600\n\nlo\ng\n-p\n\nro\nb\na\nb\nil\nit\n\ny\n\nFigure 5.4: KDE move: bean plot of prior and posterior log-probabilities.\n\nWhen forwarded to the data space, the initial ensemble presents a bias around\n\n250 m3/day illustrated in Figure 5.5. This is not an issue for the Metropolis-Hastings\n\nalgorithm itself, but it seriously compromise the way samples are drawn from the\n\nKDE-based proposal in high-dimensions: in such scenarios, probabilities are quite\n\nlow and the only candidates that survive are those in a pretty small vicinity of\n\nthe training ensemble (i. e. initial ensemble). The direct consequence is incorrect\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 72\n\ndata fitting5: the ensemble shrinks to the wrong curve in Figure 5.6 after history is\n\nassimilated all at once (i. e. smoother).\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 1\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 2\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 3\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 4\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 5\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 6\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 7\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 8\n\nhistory for prior ensemble\n\ntimestep\n\np\nro\n\nd\nu\nc\nti\n\no\nn\n r\n\na\nte\n\n [\nm\n\n\u00b3/\nd\n]\n\nFigure 5.5: KDE move: production history for the prior ensemble.\n\nRemark 8. KDE was not designed for very high-dimensional problems and is in\n\ngeneral quite sensitive to the initial ensemble used for training.\n\nThe mean acceptance fraction of 1% is unacceptably low and for sure the pos-\n\nterior distribution is not being sampled given the prior, see Figure 5.7. Increasing\n\nthe number of EnMCMC iterations is not helpful at all because of the bias already\n\nillustrated in Figure 5.6.\n\nThis configuration was run with 71 (70 slaves + 1 master) processes but could\n\nhave been run with up to 101 for optimal speed-up. It took 1 day + 10 hours + 43\n\nminutes to complete. In the Oil &amp; Gas industry, real time large-scale history-based\n\nuncertainty mitigation happens on a daily basis\u2014the usual time scale for a field\n\nmodel under production in commercial simulators.\n\nPrior and posterior ensemble members are shown in Figure 5.8 and Figure 5.9,\n\nrespectively. The MAP estimate is also shown side by side with the true (unknown)\n\nreservoir in Figure 5.10.\n\nThese results show how challenging this problem of finding the posterior state of\n\ninformation in high-dimensional spaces actually is when no assumption is made\n\n5 Assuming the OPM simulator and OPM itself are free of bugs.\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 73\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 1\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 2\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 3\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 4\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 5\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 6\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 7\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 8\n\nMAP\n\nhistory for posterior ensemble\n\ntimestep\n\np\nro\n\nd\nu\nc\nti\n\no\nn\n r\n\na\nte\n\n [\nm\n\n\u00b3/\nd\n]\n\nFigure 5.6: KDE move: production history for the posterior ensemble.\n\nabout the underlying distribution (e. g. Gaussian prior). They also give hints on\n\nthe technical difficulties involved in the computer implementation, particularly\n\nthe sampling of the proposal.\n\nAs can be seen, although the initial ensemble moves towards higher probability\n\nregions, the implementation fail to sample the posterior distribution. It\u2019s important\n\nto stress that the framework and its implementation are different entities and that\n\nthe failure of the later doesn\u2019t invalidate the former.\n\n5.3.3 Filtersim proposal\n\nIn this attempt, candidates are drawn with the Filtersim geostatistical algorithm.\n\nSo far, this algorithm has only been used to generate the initial ensemble in a pre-\n\nprocessing step, see Section 5.1.1. The difference here is that a modified version\n\nof SGeMS6 is run at every EnMCMC iteration for generating a completely new\n\nensemble, resulting in a non-trivial move.\n\nSuch ensembles proposed by Filtersim are \u201cmapped\u201d to the feature space in the\n\nsame fashion as was the initial ensemble. The chain proceeds with its exploration\n\nof ? as before with the only additional effort of calling SGeMS 1000 times.\n\n6 https://github.com/juliohm/ar2tech-SGeMS-public\n\nhttps://github.com/juliohm/ar2tech-SGeMS-public\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 74\n\n0 20 40 60 80 100\n\nwalker index\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\na\nc\nc\ne\np\nta\n\nn\nc\ne\n f\n\nra\nc\nti\n\no\nn\n\nmean acceptance = 1.0 %\n\nFigure 5.7: KDE move: acceptance fraction for each walker.\n\ni m p o r ta n t n o t e : Unlike in the previous attempts, this non-trivial proposal\n\nrequires a modified version of SGeMS installed on the cluster. I fixed some minor\n\nbugs in the original software and added a command LoadCartesianGrid so that\n\nit could be fully run in non-interactive mode without its graphical user interface.\n\nThe file format the command expects is documented in the source code.\n\nIn Figure 5.11, posterior log-probabilities reach higher values compared to the\n\nprevious KDE-based attempt. This indicates that space exploration with Filtersim\n\nis more effective, though it still presents unacceptably low acceptance fractions as\n\nshown in Figure 5.12.\n\nThe posterior ensemble is illustrated in Figure 5.13 and forwarded in Figure 5.14.\n\nA naive visual comparison with the KDE-based posterior ensemble in Figure 5.9\n\nratifies Filtersim is doing a slightly better job in exploring the model space. This\n\nstatement is made rigorous here with the aid of a measure of structural similarity\n\nbetween images\u2014the SSIM index [97].\n\nPosterior SSIM statistics are shown in Table 5.1 for both proposals using a scan\n\nwindow size of 7 in accordance with the Filtersim patch size in Listing 5.1. The\n\ninterquartile range for the SSIM index in the Filtersim proposal is slightly greater\n\nthan the one for the KDE-based proposal, which means more diversity of patterns.\n\nRemark 9. Filtersim does better exploration of the model space compared to the\n\nKDE-based attempt with the channelized reservoir under study.\n\nhttps://github.com/juliohm/HUM/blob/master/case_studies/channelized/mtrue.dat\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 75\n\nprior ensemble\n\nFigure 5.8: KDE move: 25 most probable images in prior ensemble.\n\nTable 5.1: SSIM statistics for Filtersim and KDE-based proposals.\n\nSSIM index\n\nmean std min 25% 50% 75% max interquartile range\n\nKDE-based 0.214 0.021 0.189 0.203 0.203 0.224 0.291 0.02129\n\nFiltersim 0.218 0.021 0.189 0.208 0.216 0.230 0.291 0.02233\n\nThe MAP estimate is shown in Figure 5.15 side by side with the true reservoir.\n\nComparing its SSIM index of 0.212 with the SSIM index of 0.203 for the KDE-based\n\nMAP estimate, Filtersim gets closer to the true reservoir model.\n\ni m p o r ta n t n o t e : Space exploration is also affected by the way images are\n\nreconstructed from the lower-dimensional space ?. Various members in the final\n\nensemble when reconstructed with kPCA present completely different patterns as\n\nshown in Figure 5.16.\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 76\n\nposterior ensemble\n\nFigure 5.9: KDE move: 25 most probable images in posterior ensemble.\n\nThis configuration with Filtersim as the proposal distribution was run with 101\n\n(100 slaves + 1 master) processes and took 2 days + 18 hours + 33 minutes to\n\ncomplete. According to experiments, at least 16 hours are spent outside reservoir\n\nsimulation. Such considerable waste of time is due to serialization/deserialization\n\nof large Python objects for message passing.\n\nAmong the three attempts\u2014stretch move, KDE-based, Filtersim\u2014the Filtersim\n\nalgorithm had the best, yet not acceptable, results.\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 77\n\ntrue reservoir MAP estimate\n\nFigure 5.10: KDE move: maximum a posteriori estimate.\n\nprior posterior\n\nprior vs. posterior\n\n1100\n\n1000\n\n900\n\n800\n\n700\n\n600\n\n500\n\nlo\ng\n-p\n\nro\nb\na\nb\nil\nit\n\ny\n\nFigure 5.11: Filtersim: bean plot of prior and posterior log-probabilities.\n\n0 20 40 60 80 100\n\nwalker index\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\na\nc\nc\ne\np\nta\n\nn\nc\ne\n f\n\nra\nc\nti\n\no\nn\n\nmean acceptance = 0.9 %\n\nFigure 5.12: Filtersim: acceptance fraction for each walker.\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 78\n\nposterior ensemble\n\nFigure 5.13: Filtersim: 25 most probable images in posterior ensemble.\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 1\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 2\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 3\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 4\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 5\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 6\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 7\n\nMAP\n\n0 5 10 15 20\n235\n\n240\n\n245\n\n250\n\n255\n\n260\n\n265\nwell 8\n\nMAP\n\nhistory for posterior ensemble\n\ntimestep\n\np\nro\n\nd\nu\nc\nti\n\no\nn\n r\n\na\nte\n\n [\nm\n\n\u00b3/\nd\n]\n\nFigure 5.14: Filtersim: production history for the posterior ensemble.\n\n\n\n5.3 a na ly s i s o f t h e r e s u lt s 79\n\ntrue reservoir MAP estimate\n\nFigure 5.15: Filtersim: maximum a posteriori estimate.\n\noriginal reconstruction original reconstruction\n\nKernel PCA reconstruction\n\nFigure 5.16: kPCA reconstruction.\n\n\n\n6\nB R U G G E F I E L D\n\nSome people say, \u201cHow can you live without knowing?\u201d I do not know what they mean. I\n\nalways live without knowing. That is easy. How you get to know is what I want to know.\n\nRichard Feynman\n\n6.1 Setting priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n\n6.2 Probabilistic inversion . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n6.3 Analysis of the results . . . . . . . . . . . . . . . . . . . . . . . . . 83\n\nThis case study is an example of online inversion. We\u2019re given observations dtobs at\n\nvarious time steps t and are asked for a probability distribution over M along with\n\nthe reservoir simulation. That is, the inversion is performed multiple times, each\n\ncorresponding to an instantaneous set of measurements from producer wells.\n\nThe prior state of information about the petroleum reservoir changes along the\n\nprocess whenever an observation is assimilated. This is different than what was\n\ndone in Chapter 5 in which the prior distribution was fixed and the inversion was\n\nperformed only once for the entire history.\n\n3 time steps were chosen for assimilation by visual inspection of the production\n\ncurves. The perfect forwarding assumption is again exploited as this problem is\n\nmuch more expensive than the previous flow in a synthetic channelized reservoir.\n\nPriors are first set with the initial ensemble provided by TNO, and most of what\n\nwas said earlier regarding parametrization and dimensionality reduction could be\n\nreplicated here. Inversion is then performed in an online fashion producing results\n\nthat are analyzed in the last section of the chapter.\n\nDue to time constraints and other licensing issues, code testing and a more\n\ncareful analysis of the results were compromised. Nevertheless, the software is\n\navailable to anyone who is eager to improve the applicability of the framework.\n\n80\n\n\n\n6.1 s e t t i n g p r i o r s 81\n\n6.1 s e t t i n g p r i o r s\n\n6.1.1 Prior on model parameters\n\nTNO provided to all participants of the benchmark an initial ensemble of 104\n\nrealizations constructed with various geostatistical algorithms. Some of the layers\n\nwere built with MPS simulation whereas others were not. The result is a complex\n\nreservoir that presents all sorts of physical (and non-physical) characteristics.\n\nWere we simulating layers (or rock facies) separately, it would make sense to\n\nparametrize with nonlinear kPCA. However, petrophysical maps are being gener-\n\nated here for all 44550 active cells without any pre-processing, what makes linear\n\nkPCA a good alternative. Furthermore, the linear parametrization has closed form\n\nwhich is preferred over the general fixed-point iteration discussed in Appendix C.\n\nThe initial ensemble\n{\n\nm(k)\n}100\nk=1\n\nis mapped to the feature space ? exactly as in\n\nChapter 5. The resulting ensemble\n{\n\n?\n(k)\n\n}100\n\nk=1\nis made of members that have 50\n\ncomponents each, an appreciable dimensionality reduction 44550 ? 50.\nKDE is performed to approximate a nonparametric distribution that represents\n\nthe prior state of information on the model space:\n\n?M(m(?)) = KDE(?) (6.1)\n\n6.1.2 Prior on observations\n\n3 time steps were chosen for assimilation: 1812, 2421 and 3029. These correspond\n\nto dates in IMEX (i. e. TIME keyword) for which a considerable mismatch was\n\nreported in comparing the true history with predictions from the prior ensemble.\n\nAn observation dtobs at time step t is assumed to be the center of a multivariate\n\nGaussian. This is illustrated in Figure 6.1 and written in mathematical notation as:\n\n?tD(d) = N(d\nt\nobs , 0.1\u00d7 I) (6.2)\n\nNo attempt is made to model correlation between different time steps. It might\n\nbe that in alternative settings, the prior state ?tD(d) is actually affected by previous\n\nmeasurements dt?1obs or assimilation.\n\n\n\n6.2 p ro b a b i l i s t i c i n v e r s i o n 82\n\ndtobs\n\nt\n?\nt D\n(d\n)\n\nFigure 6.1: Prior on observations changing over time.\n\n6.2 p ro b a b i l i s t i c i n v e r s i o n\n\nPerfect forwarding is assumed to hold true. At every time step, the posterior is\n\nsimply the product of Equation 6.1 and Equation 6.2:\n\n?M(?) ? ?tD(G?(?)) ?M(?) (6.3)\n\nwith G?\ndef\n= G?m the composition that maps ? 7? m 7? d. For the next assimilation\n\nat t + 1, the prior ?M is replaced with the posterior ?M just computed. This fixed-\n\npoint rule is such that at the end of 3 assimilations:\n\n?M(m) ?\n3?\n\nt=1\n\n?tD(G\n?(?))?M(?) (6.4)\n\nor in log-scale log ?M(m) =\n?3\n\nt=1 log ?\nt\nD(G\n\n?(?)) + log ?M(?) with the constant\n\nterm ultimately dropped in the Metropolis-Hastings algorithm.\n\ni m p o r ta n t n o t e : Although Equation 6.4 or its log-scale version are correct,\n\nit\u2019s safer to implement Equation 6.3 in a loop to avoid extremely low probabilities.\n\nThe resulting code is also more flexible to future generalizations.\n\n10 EnMCMC iterations are run for each of the 3 history data. For sure, no MCMC\n\nalgorithm can ever converge with such a small number, and unfortunately, no time\n\nis left for longer runs at this moment of writing. These 30 EnMCMC iterations\n\ngives a total of 30\u00d7100 = 3000 very expensive reservoir simulations.\n\nhttps://github.com/juliohm/HUM/blob/master/case_studies/brugge/main.py\n\n\n6.3 a na ly s i s o f t h e r e s u lt s 83\n\n6.3 a na ly s i s o f t h e r e s u lt s\n\nAlthough more expensive, this case study was easier to tackle compared to the\n\nchannelized reservoir in Chapter 5 in the sense that the presence of patterns (e. g.\n\nchannels, vugs) was not considered at all. The only attempt with the KDE-based\n\nproposal presented higher mean acceptance fraction, but due to time constraints\n\nthe chains could not be run for longer and escape the burn-in period.\n\nPrior and posterior log-probabilities are shown in Figure 6.2. It can be noticed\n\nthat the ensemble members with lower probability were replaced by candidates in\n\nbetter accordance with the production history and prior state of information.\n\nprior posterior\n\nprior vs. posterior\n\n7500000\n\n7000000\n\n6500000\n\n6000000\n\n5500000\n\n5000000\n\n4500000\n\n4000000\n\n3500000\n\n3000000\n\nlo\ng\n-p\n\nro\nb\na\nb\nil\nit\n\ny\n\nFigure 6.2: KDE move: bean plot of prior and posterior log-probabilities.\n\nThe mean acceptance fraction of 39.8% shows the ensemble is changing at an\n\nacceptable rate in these initial EnMCMC iterations as expected. This performance\n\nis likely to degrade as more iterations are run, or in other words, the rate of change\n\nis likely to decrease after the burn-in period. See Figure 6.3.\n\nThe predicted history for prior and posterior ensemble are shown in Figure 6.4\n\nand Figure 6.5 respectively, and for 8 selected wells in the field\u2014BR-P-1 to BR-P-8.\n\nThe MAP estimate is also shown, it fits the data reasonably well on that macroscale.\n\nMore investigation and careful analysis are needed before concluding anything\n\nabout the implementation feasibility in this case. Longer runs and convergence\n\nchecks are required before stating it did actually succeed. Other important history\n\nsuch as BHP and water rates were not considered and must be included for more\n\nrealistic scenarios.\n\n\n\n6.3 a na ly s i s o f t h e r e s u lt s 84\n\n0 20 40 60 80 100\n\nwalker index\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\na\nc\nc\ne\np\nta\n\nn\nc\ne\n f\n\nra\nc\nti\n\no\nn\n\nmean acceptance = 39.8 %\n\nFigure 6.3: KDE move: acceptance fraction for each walker.\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-1\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-2\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-3\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-4\n\n0 20 40 60 80 100 120\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\nBR-P-5\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-6\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-7\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-8\n\nhistory for prior ensemble\n\ntimestep\n\np\nro\n\nd\nu\nc\nti\n\no\nn\n r\n\na\nte\n\n [\nm\n\n\u00b3/\nd\n]\n\nFigure 6.4: KDE move: production history for the prior ensemble.\n\n\n\n6.3 a na ly s i s o f t h e r e s u lt s 85\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-1\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-2\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-3\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-4\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\nBR-P-5\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-6\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-7\n\nMAP\n\n0 20 40 60 80 100 120\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\nBR-P-8\n\nMAP\n\nhistory for posterior ensemble\n\ntimestep\n\np\nro\n\nd\nu\nc\nti\n\no\nn\n r\n\na\nte\n\n [\nm\n\n\u00b3/\nd\n]\n\nFigure 6.5: KDE move: production history for the posterior ensemble.\n\n\n\n7\nC O N C L U S I O N\n\nWhat we think or what we know or what we believe is in the end of little consequence.\n\nThe only thing of consequence is what we do.\n\nJohn Ruskin\n\n7.1 General comments . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n\n7.2 Technical difficulties . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n7.3 Suggested improvements . . . . . . . . . . . . . . . . . . . . . . . 89\n\nThis masters dissertation and associated software development ended up being a\n\npurely academic exercise, serving to identify some of the technology barriers that\n\nexist in doing probabilistic inversion with realistic petroleum reservoirs.\n\nVarious enhancements to the current implementation are needed before it can\n\npossibly be employed by the industry with tens or hundreds of thousands of grid\n\ncells. They are not trivial as Chapter 5 and Chapter 6 have shown, but the source\n\ncode made available on the web is a good start point.\n\n7.1 g e n e r a l c o m m e n t s\n\nAs previously mentioned, the failure of this particular implementation in sampling\n\nthe posterior state of information correctly doesn\u2019t imply that the framework is\n\ninfeasible in practice. It just means more tweaks are necessary to overcome current\n\nhardware limitations and to better represent in a computer the prior information\n\nthat we have about the true (physical) model. In spite of that, the implementation\n\nmight still be useful for obtaining classical MAP estimates without strong, and\n\nsometimes unrealistic assumptions (e. g. Gaussian prior).\n\n86\n\n\n\n7.1 g e n e r a l c o m m e n t s 87\n\nThe simplicity of the code is an evidence of the framework generality where\n\nstates of information are updated without complicated programming paths. In\n\nfact, it\u2019s a simple generic loop with the EnMCMC sampler.\n\nInternals of the target simulator G are totally ignored in each of the case studies.\n\nThis can be interpreted as a feature for hiding complexity, but also as an implemen-\n\ntation deficiency; it actually depends on the quality of the obtained results. With\n\nthat said, I argue it\u2019s a deficiency in both chapters and that the simulator internals\n\nshould have been used to add those very high-dimensional ill-posed problems\n\nwith extra rich information.\n\nDissertation results and the software used to generate them were made available\n\nin such a way other researchers can investigate and contribute new ideas. The\n\nchannelized reservoir case study is fully open source whereas the Brugge field\n\nbenchmark case relies on proprietary software, and therefore is only reproducible\n\nto those with a software license.\n\nAnother major difference between Chapter 5 and Chapter 6 resides in the way\n\nthe history was assimilated\u2014offline vs. online inversion. Although the two case\n\nstudies have about the same dimensionality dim M, the number of wells in the\n\nBrugge field is considerably greater. The observations were considered per time\n\nstep so that dim D was low enough for fitting a multivariate Gaussian.\n\nThe channelized reservoir benchmark case was designed for assessing the per-\n\nformance of advanced history-based uncertainty mitigation methods. It has the\n\ninteresting appeal of visually comparing patterns before and after inversion, and\n\ncan be used to guide the development of new clever heuristics for history-matching\n\nproblems with complex geology.\n\nThe novelty of this work is in the nonparametric representation of the prior\n\nstate of information with nonlinear kPCA and KDE; and introduction of MCMC\n\nensemble samplers to petroleum engineering history-matching problems.\n\nThe posterior probability under perfect forwarding assumption was rewritten in\n\nlog-scale and directly used in the EnMCMC algorithm in order to overcome the\n\nvanishing of probabilities on digital computer finite precision\u2014a crucial step for\n\ndealing with high-dimensional model spaces (e. g. R50).\n\nkPCA proved to be an effective method for dimensionality reduction that is\n\nparticularly powerful for preserving high-order statistics. Together with KDE and\n\nk-fold cross-validation, the software was able to encode information without strong\n\nprobabilistic assumptions on the current ensemble.\n\n\n\n7.2 t e c h n i c a l d i f f i c u lt i e s 88\n\nEnMCMC showed itself as a high-performance Markov chain sampling algo-\n\nrithm that can exploit MPI pools to distribute work among various computational\n\nnodes. Its theoretical speed-up of either nw\n2\n\nfor the parallel stretch move or nw\n\nfor custom proposal distributions1 allowed the execution of expensive case studies\n\nwithin 2\u20134 days (i. e. wall time).\n\nAs measured by the SSIM index, and among the three attempts in Chapter 5, the\n\nFiltersim-based proposal did the most successful exploration of the model space.\n\nThe final ensemble however remained trapped at a local minimum and the full\n\ncharacterization of the posterior distribution was never achieved.\n\nSimilar conclusion was made in Chapter 6, in which the expensive probabilistic\n\ninversion of the Brugge field was set up and implemented with the KDE-based\n\nproposal. The posterior state of information was not captured because of the small\n\nnumber of EnMCMC iterations, but a MAP estimate was succesfully found with-\n\nout any Gaussian assumption on the petrophysical properties of the reservoir.\n\n7.2 t e c h n i c a l d i f f i c u lt i e s\n\nDeveloping the software in a high-level programming language such as Python\n\nwas a quite pleasurable experience; porting the code together with all its depen-\n\ndencies to an old Red Hat R\u00a9 computer cluster was not. I had to recompile all third-\nparty libraries (e. g. NumPy/SciPy, mpi4py, VTK, Qt, . . . ) with a custom toolchain\n\nand link against the MPI implementation supported by the c e na pa d - p e cluster\n\nfor everything to work as expected. The process involved various obscure error\n\nmessages that gave little clue about what the actual misconfiguration was.\n\nThe SLURM resource manager installed on the cluster was also misconfigured\n\ncausing some symbols to not be resolved during runtime. I solved the issue by\n\nmanual preload of the missing libraries right before job scheduling.\n\nSome of the computing nodes in the cluster were damaged causing the crash\n\nof the entire job with non-deterministic errors. They were identified and simply\n\nexcluded from subsequent executions.\n\nCMG licensing was another issue I faced while working on the second case study.\n\nThe license manager was having trouble with multiple requests in parallel close\n\nto the available limit (e. g. 100). No solution was presented by the support and I\n\nforcibly reduced the number of processes in the MPI pool.\n\n1 Assuming as many walkers as the ensemble size.\n\nhttps://computing.llnl.gov/linux/slurm/\n\n\n7.3 s u g g e s t e d i m p rov e m e n t s 89\n\nNot really a technical difficulty and more like a caveat, users must be aware that\n\nthe OPM initiative is very recent and that its API is subject to radical modifications.\n\nFinally, minor issues regarding RAM limits and other cluster resources were\n\nsolved on demand by implementing alternative memory-friendly code in response\n\nto the large amount of data being transmitted and transformed for each realization.\n\n7.3 s u g g e s t e d i m p rov e m e n t s\n\nVarious issues within the current software implementation should be addressed\n\nbefore it can actually be employed for sampling the posterior state of information\n\nin high-dimensional petroleum history-matching problems.\n\nThe major room for improvements that comes to mind lies in the design of\n\nproposal distributions (or moves) that are aware of the current ensemble position.\n\nIn all implemented attempts, the current ensemble is discarded and replaced by\n\na new one as if the current realizations never existed. In mathematical notation,\n\nthe move x ? y that should be dependent on the current state x is not using this\ninformation to propose a new state y in a \u201cgood direction\u201d: q(x, y) = q(y).\n\nAssuming better proposals are available, other Machine Learning tools could\n\nbe investigated for fitting the ensemble just proposed. KDE was not originally\n\ndesigned for high-dimensional problems and even in log-scale might present poor\n\napproximation quality.\n\nPerfect forwarding is assumed to hold true in both case studies. In terms of com-\n\nputational requirements, it\u2019s extremely difficult to throw this assumption away and\n\ntake theoretical uncertainties into account. Nonetheless, it is a possible research\n\ntopic to dive in.\n\nStill related to computational performance, online inversion in the Brugge field\n\nbenchmark case doesn\u2019t need the reservoir simulation to run all the way to the end.\n\nFor the initial time steps the simulation should be interrupted right away as the\n\nremaining history is useless for data assimilation.\n\nAnother very important feature that wasn\u2019t implemented is the possibility of\n\nrestarting the algorithm anywhere in the chain. Specially if the target simulator\n\nterminates abnormally very often with the proposed realizations as inputs, it\u2019s of\n\nenormous interest to fix the bug and continue from where the chain has stopped.\n\n\n\n7.3 s u g g e s t e d i m p rov e m e n t s 90\n\nIn Part II, no attempt was made to assess the predictive capability of the posterior\n\nensemble or how each of its members perform for future timesteps beyond the\n\nassimilated history. This can be done with little programming effort.\n\nFinally, the research community is invited to benchmark other HUM methods\n\n(e. g. EnKF, RML) with the case studies developed here and send me pull requests\n\nto make them available to future readers.\n\n\n\nPart III\n\nA P P E N D I X\n\n\n\nA\nO M I T T E D P R O O F S\n\na.1 t h e m a j o r i t y o f i n v e r s e p ro b l e m s i s i l l - p o s e d\n\nFollowing Theorem 1, it remains to proof by induction nn ? n! > n! for all n > 2:\n\n22 ? 2! > 2! X\n\n(n + 1)n+1 ? (n + 1)! = (n + 1)\n[\n(n + 1)n ? n!\n\n]\n\n= (n + 1)\n[((\n\nn\n\n0\n\n)\nnn +\n\n(\nn\n\n1\n\n)\nnn?1 + \u00b7 \u00b7 \u00b7+\n\n(\nn\n\nn\n\n))\n? n!\n\n]\n\n= (n + 1)\n[\nnn ? n! +\n\n((\nn\n\n1\n\n)\nnn?1 + \u00b7 \u00b7 \u00b7+\n\n(\nn\n\nn\n\n))]\n\n> (n + 1)\n[\nn! +\n\n(\nn\n\n1\n\n)\nnn?1 + \u00b7 \u00b7 \u00b7+\n\n(\nn\n\nn\n\n)]\n\n> (n + 1)! X\n\na.2 m a x i m u m l i k e l i h o o d e s t i m at i o n f o r i . i . d. g au s s i a n s\n\nAssume x1, x2, . . . , xm are independent and identically distributed Gaussian ran-\n\ndom variables with xi ? N\n(\n\u00b5, ?2\n\n)\n. The likelihood is given by the joint probability:\n\nL(\u00b5, ? | x1, x2, . . . , xm) = f(x1, x2, . . . , xm | \u00b5, ?)\n\n=\n?\n\ni\n\n1\n\n?\n?\n2?\n\nexp\n[\n?\n(xi ? \u00b5)\n\n2\n\n2?2\n\n]\n\n=\n(2?)?m/2\n\n?m\nexp\n\n[\n?\n\n?\ni(xi ? \u00b5)\n\n2\n\n2?2\n\n]\n\nThe log-likelihood is then:\n\nln L(\u00b5, ? | x1, x2, . . . , xm) = ?\n1\n\n2\nm ln 2? ? m ln ? ?\n\n?\ni(xi ? \u00b5)\n\n2\n\n2?2\n\n92\n\n\n\nDeriving the log-likelihood w.r.t. \u00b5 and ? individually and setting to zero gives:\n\n\u00b5? =\n1\n\nm\n\n?\n\ni\n\nxi and ??\n2 =\n\n1\n\nm\n\n?\n\ni\n\n(xi ? \u00b5?)\n2\n\nBecause the logarithm is monotonically increasing function, this is the maximum\n\nlikelihood estimation.\n\na.3 s y s t e m o f e q uat i o n s f o r d i s c r e t e l i n e a r i n v e r s e p ro b l e m s\n\nEvery parameter m = m1e1 + m2e2 + \u00b7 \u00b7 \u00b7+ mnen ? M is a linear combination of\nvectors in the basis. The forward operator satisfies superposition and scaling:\n\nG(m) = m1G(e1) + m2G(e2) + \u00b7 \u00b7 \u00b7+ mnG(en)\n\nLet G be the matrix whose columns are the coordinates of G(e1), G(e2), . . . , G(en)\n\nin the data space.\n\n? d =\n\n?\n???G(e1) G(e2) \u00b7 \u00b7 \u00b7 G(en)\n\n?\n???\n\n?\n??????\n\n??????\n\nm1\n\nm2\n...\n\nmn\n\n?\n??????\n\n??????\n\n= Gm\n\na.4 m a x i m u m l i k e l i h o o d a n d l e a s t - s q ua r e s\n\nAssume the observations have independent Gaussian errors di = (Gm)i + ?i with\n\n?i ? N\n(\n0, ?2i\n\n)\n. The likelihood is given by:\n\nL(m | d) =\n(2?)?m/2\n?\n\ni ?i\nexp\n\n[\n?\n?\n\ni\n\n(di ? (Gm)i)\n2\n\n2?2\ni\n\n]\n\nand the maximum occurs at:\n\narg max\nm?M\n\nL(m | d) = arg min\nm?M\n\n?\n\ni\n\n(di ? (Gm)i)\n2\n\n?2\ni\n\nIt\u2019s the least-squares estimate to Gm = d except for the 1/?2i factors. Scaling\n\nthe system of equations with W\ndef\n= diag(1/?1, 1/?2, . . . , 1/?m), Gw = WG and\n\ndw = Wd causes the least-squares to match the maximum likelihood estimate:\n\n?dw ? Gwm?2L2 =\n?\n\ni\n\n(dwi ? (Gwm)i)\n2\n=\n\n?\n\ni\n\n(di ? (Gm)i)\n2\n/?2i\n\n93\n\n\n\n? arg max\nm?M\n\nL(m | d) = arg min\nm?M\n\n?dw ? Gwm?2L2\n\nThis is equivalent to incorporating data uncertainty Cd\ndef\n= diag(?21, ?\n\n2\n2, . . . , ?\n\n2\nm)\n\ninto the objective (d ? Gm)?C?1\nd\n\n(d ? Gm).\n\na.5 w e i g h t e d l i n e a r l e a s t - s q ua r e s e s t i m at e\n\nLet O(m) = (dobs ? Gm)\n?C?1\n\nd\n(dobs ? Gm) be the objective function, it can be ex-\n\npanded to:\n\nO(m) = d?obsC\n?1\nd dobs ? d\n\n?\nobsC\n\n?1\nd Gm ? m\n\n?G?C?1d dobs + m\n?G?C?1d Gm\n\nand the gradient w.r.t. m derived:\n\n?m O(m) = ?G?(C?1d )\n?dobs ? G\n\n?C?1d dobs + 2G\n?C?1d Gm\n\nThe inverse of the covariance is symmetric (C?1\nd\n\n)? = C?1\nd\n\nand then:\n\n?m O(m) = ?2G?C?1d dobs + 2G\n?C?1d Gm\n\nBy setting ?m O(m) = 0, the result follows if G?C?1d G is invertible:\n\nmCd = (G\n?C?1d G)\n\n?1G?C?1d dobs\n\nIn particular, a proof for pure least-squares is obtained erasing all occurrences\n\nof C?1\nd\n\n.\n\na.6 l e v e n b e r g - m a rq ua r d t g r a d i e n t a n d h e s s i a n\n\nLet f(m) =\n?m\n\ni=1 fi(m)\n2 with fi(m) =\n\nG(m)i?di\n?i\n\nbe the objective for the Newton-\n\nRaphson update and F(m) = (f1(m), f2(m), . . . , fm(m))? the misfit vector. The\n\ncomponents of the gradient are given by:\n\n?f(m)j =\nm?\n\ni=1\n\n2?fi(m)jF(m)j\n\nor in matrix notation with J(m) ??F(m) the Jacobian:\n\n?f(m) = 2J(m)?F(m)\n\nSimilarly, consider the components of the Hessian:\n\nH(m) = ?2f(m) =\nm?\n\ni=1\n\n?2\n(\nfi(m)\n\n2\n)\n=\n\nm?\n\ni=1\n\nHi(m)\n\n94\n\n\n\nThe j, k entry of Hi(m) has the form:\n\nHij,k(m) =\n?2 (fi(m)\n\n2\n)\n\n?mj?mk\n\n=\n?\n\n?mj\n\n(\n2fi(m)\n\n?fi(m)\n\n?mk\n\n)\n\n= 2\n\n(\n?fi(m)\n\n?mj\n\n?fi(m)\n\n?mk\n+ fi(m)\n\n?2fi(m)\n\n?mj?mk\n\n)\n\nor in matrix notation with Q(m)\ndef\n= 2\n\n?m\ni=1 fi(m)?2fi(m):\n\nH(m) = 2J(m)?J(m) + Q(m)\n\na.7 c o n d i t i o na l p ro b a b i l i t y b y c o n j u n c t i o n o f s tat e s\n\nLet P(\u00b7) be a probability distribution with density f(x), and MB(\u00b7) the p-event of B\nwith density \u00b5B(x). Also, let \u00b5(x) be the homogeneous probability density over X.\n\nAccording to Definition 3.5, the conjunction (P ? MB)(\u00b7) is represented by density:\n\ng(x) =\n1\n\nv\n\nf(x) \u00b5B(x)\n\n\u00b5(x)\n\nFrom Definition 3.6, the p-event nulls out g(x) outside B:\n\ng(x) =\n\n?\n?\n\n?\n\n1\nv\nf(x), if x ? B\n\n0, otherwise\n\nThe normalizing constant is (provided the integral is finite) v =\n?\n\nB\ndx f(x) =\n\nP(B). Thus, evaluating (P ? MB)(\u00b7) on any event A ? X is the classical ratio of\nprobabilities:\n\n(P ? MB)(A) =\n\n?\n\nA\n\ndx g(x) =\n1\n\nv\n\n?\n\nA?B\ndx f(x) =\n\nP(A?B)\nP(B)\n\na.8 k e r n e l d e n s i t y e s t i m at i o n a s a c o n v o l u t i o n\n\nFor any batch of data X = {x1, x2, . . . , xn}, the empirical density function is defined\n\nas the average of Dirac delta functions:\n\nfX(x)\ndef\n=\n\n1\n\nn\n\nn?\n\ni=1\n\n?(x ? xi)\n\n95\n\n\n\nThe empirical cumulative distribution, shown here for completeness, is derived by\n\nremembering that\n?\n\nR\ng(x)?(x)dx = g(0) for any function g:\n\n?x\n\n??\n\nfX(y)dy =\n\n?x\n\n??\n\n1\n\nn\n\nn?\n\ni=1\n\n?(y ? xi)dy\n\n=\n1\n\nn\n\nn?\n\ni=1\n\n?x\n\n??\n\n?(y ? xi)dy\n\n=\n1\n\nn\n\nn?\n\ni=1\n\n?\n\nR\n\n1(y 6 x) ?(y ? xi)dy\n\n=\n1\n\nn\n\nn?\n\ni=1\n\n1(xi 6 x)\n\n= FX(x)\n\nwith 1 the indicator function. Given any other function k, the convolution with fX\n\nis written by definition:\n\n(fX ?k)(x)\ndef\n=\n\n?\n\nR\n\nfX(x ? y)k(y)dy\n\n=\n\n?\n\nR\n\n1\n\nn\n\nn?\n\ni=1\n\n?(x ? y ? xi)k(y)dy\n\n=\n1\n\nn\n\nn?\n\ni=1\n\n?\n\nR\n\n?(x ? y ? xi)k(y)dy\n\n=\n1\n\nn\n\nn?\n\ni=1\n\nk(xi ? x)\n\nLet k(x) = Kh(?x) = Kh(x) be a symmetric kernel and the result follows:\n\n(fX ?Kh)(x) =\n1\n\nn\n\nn?\n\ni=1\n\nKh(x ? xi)\n\n96\n\n\n\nB\nC O D E S N I P P E T S\n\nb.1 i t e r at i v e ly r e w e i g h t e d l e a s t - s q ua r e s\n\nListing B.1: lp_solve.m\n\n1 % Copyright (c) 2013 J\u00falio Hoffimann Mendes\n\n2 %\n\n3 % This program is free software: you can redistribute it and/or modify\n\n4 % it under the terms of the GNU General Public License as published by\n\n5 % the Free Software Foundation, either version 3 of the License, or\n\n6 % (at your option) any later version.\n\n7\n\n8 % usage: x = lp_solve (A,b,tol,maxiter,cutoff,p)\n\n9 %\n\n10 % Find the L_p solution to Ax=b by iterating least-squares.\n\n11 function x = lp_solve (A,b,p,tol,maxiter,cutoff)\n\n12\n\n13 iter = 1; x = A\\b; % least-squares estimate as initial guess\n\n14 do\n\n15 r = abs (A*x-b); % current residual\n\n16 r(r &lt;cutoff) = cutoff; % fix small entries\n\n17 R = diag (r.^(p-2));\n\n18\n\n19 newx = (A\u2019*R*A)\\(A\u2019*R*b); % solution to the weighted system\n\n20 err = norm (newx-x)/(1+norm (x));\n\n21\n\n22 x = newx;\n\n23 until (err &lt;tol || ++iter > maxiter)\n\n24\n\n25 endfunction\n\n26 % References:\n\n27 % SCALES, J. A.; GERSZTENKORN, A; TREITEL, S., 1988. Fast Lp Solution\n\n28 % of Large, Sparse, Linear Systems: Application to Seismic Travel\n\n29 % Time Tomography.\n\n97\n\n\n\nb.2 l e a s t a b s o l u t e s h r i n k ag e a n d s e l e c t i o n o p e r at o r\n\nListing B.2: lasso.m\n\n1 % Copyright (c) 2013 J\u00falio Hoffimann Mendes\n\n2 %\n\n3 % This program is free software: you can redistribute it and/or modify\n\n4 % it under the terms of the GNU General Public License as published by\n\n5 % the Free Software Foundation, either version 3 of the License, or\n\n6 % (at your option) any later version.\n\n7\n\n8 % usage: x = lasso (A,b,d,tol,maxiter)\n\n9 %\n\n10 % Find the minimum of (Ax-b)\u2019*(Ax-b) + d^2 ||x||_1.\n\n11 function x = lasso (A,b,d,tol,maxiter)\n\n12\n\n13 iter = 1; n = columns (A); a = 2*sum (A.^2);\n\n14 x = (A\u2019*A + d^2 * eye (n)) \\ A\u2019*b; % Ridge estimate as initial guess\n\n15 do\n\n16 oldx = x;\n\n17 for j = 1:n\n\n18 cj = 2*A(:,j)\u2019*(b - A*x + x(j)*A(:,j));\n\n19\n\n20 % subderivative for |xj|\n\n21 if (cj &lt;-d^2)\n\n22 x(j) = (cj + d^2) / a(j);\n\n23 elseif (cj > d^2)\n\n24 x(j) = (cj - d^2) / a(j);\n\n25 else\n\n26 x(j) = 0;\n\n27 endif\n\n28 endfor\n\n29 err = norm (x-oldx) / (1 + norm (oldx));\n\n30 until (err &lt;tol || ++iter > maxiter)\n\n31\n\n32 endfunction\n\n33 % References:\n\n34 % HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J., 2009. The Elements of\n\n35 % Statistical Learning: Data Mining, Inference, and Prediction.\n\n98\n\n\n\nb.3 m e t ro p o l i s a l g o r i t h m\n\nListing B.3: metropolis.R\n\n1 # Copyright (c) 2013 J\u00falio Hoffimann Mendes\n\n2 #\n\n3 # This program is free software: you can redistribute it and/or modify\n\n4 # it under the terms of the GNU General Public License as published by\n\n5 # the Free Software Foundation, either version 3 of the License, or\n\n6 # (at your option) any later version.\n\n7\n\n8 # usage: x = metropolis (pi,N,s)\n\n9 #\n\n10 # Generate N samples from the target density pi using Metropolis-Hastings\n\n11 # with a symmetric proposal q(x,y) = Gaussian(y;x,s).\n\n12 metropolis&lt;- function(pi,N,s) {\n\n13 # initialize the chain\n\n14 x&lt;- numeric(N); x[1]&lt;- 0\n\n15\n\n16 for (n in 1:(N-1)) {\n\n17 # propose a move xn --> y\n\n18 y&lt;- rnorm(1,x[n],s)\n\n19\n\n20 # acceptance probability\n\n21 alpha&lt;- min(1,pi(y)/pi(x[n]))\n\n22\n\n23 if (runif(1) &lt;alpha)\n\n24 x[n+1]&lt;- y\n\n25 else\n\n26 x[n+1]&lt;- x[n]\n\n27 }\n\n28\n\n29 return(x)\n\n30 }\n\n31 # References:\n\n32 # GAMERMAN, D.; LOPES, H. F., 2006. Markov Chain Monte Carlo:\n\n33 # Stochastic Simulation for Bayesian Inference.\n\n99\n\n\n\nb.4 o n l i n e b ay e s i a n i n v e r s i o n\n\nListing B.4: online inversion.py\n\n1 # -*- coding: utf8 -*-\n\n2 # Copyright (c) 2013 J\u00falio Hoffimann Mendes\n\n3 #\n\n4 # This program is free software: you can redistribute it and/or modify\n\n5 # it under the terms of the GNU General Public License as published by\n\n6 # the Free Software Foundation, either version 3 of the License, or\n\n7 # (at your option) any later version.\n\n8\n\n9 from pymc import *\n\n10 import matplotlib.pyplot as plt\n\n11\n\n12 prior = runiform(-10,10,10000) # initial guess\n\n13\n\n14 while True: # online inversion\n\n15 x = stochastic_from_data( \u2019x \u2019, prior)\n\n16 y = x*x\n\n17 obs = Normal( \u2019 obs \u2019, y, .1, 4 + rnormal(0,1), observed=True)\n\n18\n\n19 model = Model([x,y,obs])\n\n20 mcmc = MCMC(model)\n\n21 mcmc.sample(10000)\n\n22\n\n23 posterior = mcmc.trace( \u2019x \u2019)\n\n24 Matplot.plot(posterior)\n\n25 plt.show()\n\n26\n\n27 prior = posterior[:]\n\n100\n\n\n\nb.5 h i s t o g r a m f i t t i n g w i t h k e r n e l d e n s i t y e s t i m at i o n\n\nListing B.5: pymc patch.py\n\n1 # -*- coding: utf8 -*-\n\n2 # Copyright (c) 2013 J\u00falio Hoffimann Mendes\n\n3 #\n\n4 # This program is free software: you can redistribute it and/or modify\n\n5 # it under the terms of the GNU General Public License as published by\n\n6 # the Free Software Foundation, either version 3 of the License, or\n\n7 # (at your option) any later version.\n\n8\n\n9 import numpy as np\n\n10 from scipy.stats.kde import gaussian_kde\n\n11 from pymc import Stochastic\n\n12\n\n13 def stochastic_from_data(name, data, lower=-np.inf, upper=np.inf,\n\n14 value=None, observed=False, trace=True,\n\n15 verbose=-1, debug=False):\n\n16 \"\"\"\n\n17 Return a Stochastic subclass made from arbitrary data.\n\n18\n\n19 The histogram for the data is fitted with Kernel Density Estimation.\n\n20\n\n21 :Parameters:\n\n22 - \u2018data\u2018 : An array with samples (e.g. trace[:])\n\n23 - \u2018lower\u2018 : Lower bound on possible outcomes\n\n24 - \u2018upper\u2018 : Upper bound on possible outcomes\n\n25\n\n26 :Example:\n\n27 >>> from pymc import stochastic_from_data\n\n28 >>> pos = stochastic_from_data(\u2019posterior\u2019, posterior_samples)\n\n29 >>> prior = pos # update the prior with arbitrary distributions\n\n30\n\n31 :Alias:\n\n32 Histogram\n\n33 \"\"\"\n\n34 pdf = gaussian_kde(data) # automatic bandwidth selection\n\n35\n\n36 # account for tail contribution\n\n37 lower_tail = upper_tail = 0.\n\n38 if lower > -np.inf: lower_tail = pdf.integrate_box(-np.inf, lower)\n\n39 if upper &lt;np.inf: upper_tail = pdf.integrate_box(upper, np.inf)\n\n40 factor = 1./(1. - (lower_tail + upper_tail))\n\n41\n\n42 def logp(value):\n\n43 prob = factor*pdf(value)\n\n44 if value &lt;lower or value > upper:\n\n101\n\n\n\n45 return -np.inf\n\n46 elif prob&lt;= 0.:\n\n47 return -np.inf\n\n48 else:\n\n49 return np.log(prob)\n\n50\n\n51 def random():\n\n52 res = pdf.resample(1)[0][0]\n\n53 while res &lt;lower or res > upper:\n\n54 res = pdf.resample(1)[0][0]\n\n55 return res\n\n56\n\n57 if value == None: value = random()\n\n58\n\n59 return Stochastic(logp = logp,\n\n60 doc = \u2019Non?parametric density with Gaussian Kernels . \u2019,\n\n61 name = name,\n\n62 parents = {},\n\n63 random = random,\n\n64 trace = trace,\n\n65 value = value,\n\n66 dtype = float,\n\n67 observed = observed,\n\n68 verbose = verbose)\n\n69\n\n70 # Alias following Stochastics naming convention\n\n71 Histogram = stochastic_from_data\n\n102\n\n\n\nC\nK E R N E L P C A\n\nConsider an ensemble in the form of a matrix X ? RNf\u00d7Nr with Nf the number\nof features (e. g. number of cells in the reservoir model) and Nr the number of\n\nrealizations. Each column xj, j = 1, 2, . . . , Nr is in a very high-dimensional space\n\nNf ? Nr. Consider also the ensemble is centered (i. e.\n?\n\nj xj = 0) for the moment.\n\nLinear PCA in its primal form diagonalizes the covariance matrix C = 1\nNr\n\n?\nj xjx\n\n?\nj\n\nas an ordinary eigenproblem:\n\n?v = Cv\n\nwith eigenvalues ? > 0 and where every solution v ? RNf \\ {0} lies in the span of\nthe ensemble x1, x2, . . . , xNr . This argument is made clear by writing:\n\nCv =\n(?\n\nj\n\nxjx\n?\nj\n\n)\nv =\n\n?\n\nj\n\nxj(x\n?\nj v)\n\nas a linear combination of realizations. The direct conclusion from this fact is that\n\n?v = Cv is equivalent to:\n\n?x?k v = x\n?\nk Cv, for all k = 1, 2, . . . , Nr\n\nRepeat the previous steps now in a feature space F reached by some mapping\n\n? : RNf 7?? F. The covariance C? = 1\nNr\n\n?\nj ?(xj)?(xj)\n\n? is decomposed with eigen-\n\nvalues ?? > 0 and eigenvectors v? in the span of ?(x1), ?(x2), . . . , ?(xNr):\n\n???(xk)\n?v? = ?(xk)\n\n?C?v? for all k = 1, 2, . . . , Nr\n\nSubstitute v? =\n?\n\ni ?i?(xi) and C? =\n1\nNr\n\n?\nj ?(xj)?(xj)\n\n? in the previous equation\n\nto get for all k = 1, 2, . . . , Nr:\n\n??\n?\n\ni\n\n?(xk)\n??(xi) =\n\n1\n\nNr\n\n?\n\ni\n\n?i\n(\n?(xk)\n\n?\n?\n\nj\n\n?(xj)\n)(\n?(xj)\n\n??(xi)\n)\n\n103\n\n\n\nand define Kij\ndef\n= ?(xi)\n\n??(xj) the kernel Gramian matrix for compact notation:\n\nNr??K? = K\n2\n?\n\nAs K ? RNr\u00d7Nr is symmetric, the coordinates ? def= (?1, ?2, . . . , ?Nr)? of the\neigenvector v? ? F \\ {0} in the ensemble basis ?(x1), ?(x2), . . . , ?(xNr) are found\nwith the now tractable ordinary eigenproblem:\n\nNr??? = K?\n\nThus, the intractable primal problem of diagonalizing C ? RNf\u00d7Nf is replaced\nby the tractable dual problem of diagonalizing K ? RNr\u00d7Nr [81].\n\nNote the kernel Gramian matrix only depends on inner products ?(xi)??(xj)\n\nbetween points in the ensemble. This is crucial because ? : RNf 7?? F itself is\ngenerally expensive to evaluate or is possibly unknown, dismissing the whole pur-\n\npose of the method. The \u201ckernel trick\u201d replaces inner products by mind-created\n\nfunctions k(xi, xj) that are valid (i. e. ??, k(xi, xj) = ?(xi)??(xj)) and that can be\neasily computed. Such functions can be thought as a measure of similarity between\n\ntwo images xi, xj ? RNf , for instance the dot product x?i xj (i. e. linear PCA).\nThe sections below describe all the steps for implementing kPCA. The algorithm\n\nis divided into two (or three) subroutines that correspond to training and actually\n\ndoing prediction. The initial assumption that observations should be centered is\n\ndropped with an additional step performed in the feature space.\n\nc.1 k e r n e l g r a m i a n m at r i x\n\nThe polynomial kernel k(x, y)\ndef\n= ?x, y?+ ?x, y?2 + \u00b7 \u00b7 \u00b7+ ?x, y?d [70, 88] is evaluated\n\nat the columns of X ? RNf\u00d7Nr without any assumption about centering. The\nresultant Gramian matrix K ? RNr\u00d7Nr is symmetric positive semidefinite:\n\nKij\ndef\n= k(xi, xj)\n\n104\n\n\n\nc.2 c e n t e r i n g i n t h e f e at u r e s pac e\n\nUp to this point, mapped observations were assumed to be centered in F, that is,\n?\n\nj ?(xj) = 0. Consider the more general case where this assumption does not\n\nhold true and center the data explicitly as in:\n\n?0(xj)\ndef\n= ?(xj) ?\n\n1\n\nNr\n\n?\n\nj\n\n?(xj)\n\nThis new ensemble ?0(x1), ?0(x2), . . . , ?0(xNr) is such that the covariance or\n\ndot product matrix K?ij = ?0(xi)??0(xj) is in accordance with the derivation in\n\nthe beginning of this appendix:\n\n???? = K???\n\nwith v? =\n?\n\nj ??j?0(xj) written in terms of centered observations. Perfect, except\n\nthat the function ? : RNf 7?? F isn\u2019t actually available.\nThe centered Gramian K? is obtained from its non-centered version K using some\n\nalgebraic manipulations instead:\n\nK?ij = ?0(xi)\n??0(xj)\n\n=\n\n(\n?(xi) ?\n\n?\n\nm\n\n?(xm)\n\n)?(\n?(xj) ?\n\n?\n\nn\n\n?(xn)\n\n)\n\n= ?(xi)\n??(xj) ?\n\n1\n\nNr\n\n?\n\nm\n\n?(xm)\n??(xj) ?\n\n1\n\nNr\n\n?\n\nn\n\n?(xi)\n??(xn) +\n\n1\n\nNr\n2\n\n?\n\nm,n\n\n?(xm)\n??(xn)\n\nor in matrix form with 1Nr\ndef\n= 1\n\nNr\nI ? RNr\u00d7Nr :\n\nK? = K ? 1Nr K ? K1Nr + 1Nr K1Nr\n\nThus, after K is computed using the kernel function k(x, y), it\u2019s centered with\n\nthe formula derived above. No assumption about centering in the original space\n\nis required and the ensemble X ? RNf\u00d7Nr is used as is. The centered K? is still\nsymmetric positive semidefinite:\n\nK? =\n[\n?0(x1) ?0(x2) \u00b7 \u00b7 \u00b7 ?0(xNr)\n\n]?[\n?0(x1) ?0(x2) \u00b7 \u00b7 \u00b7 ?0(xNr)\n\n]\n\n105\n\n\n\nc.3 e i g e n p ro b l e m a n d n o r m a l i z at i o n\n\nThe ordinary symmetric eigenproblem ???? = K??? is solved with well-tested linear\n\nalgebra software and the solutions ?? are normalized so that the corresponding\n\neigenvectors v? ? F satisfy v??v? = 1:\n\n1 = v??v?\n\n=\n\n(\n?\n\ni\n\n??i?0(xi)\n\n)?(\n?\n\ni\n\n??i?0(xi)\n\n)\n\n=\n?\n\ni,j\n\n??i??j?0(xi)\n??0(xj)\n\n=\n?\n\ni,j\n\n= ??i??jK?ij\n\n= ???(K???)\n\n= ???????\n\nIf the eigenproblem solver (e. g. LAPACK) returns ?? such that ????? = 1, it\n\nremains to scale the basis as:\n\nA\ndef\n=\n\n[\n??1?\n??1\n\n??2?\n??2\n\u00b7 \u00b7 \u00b7 ??Nc?\n\n??Nc\n\n]\n\nwith 1 6 Nc 6 min(Nf, Nr) the desired number of components to be retained.\n\nThe matrix A ? RNr\u00d7Nc is saved as the only information necessary for future\nreconstructions.\n\nThis first training part of kPCA is independent of the kernel function family\n\nbeing used (e. g. polynomial, sigmoid), and with A stored in memory, there are at\n\nleast two possible goals:\n\nc o m p r e s s i n g Express an image of size Nf in terms of Nc ? Nf components\n\nd e n o i s i n g Purge undesired modes in a valid image of size Nf\n\nDimensionality reduction is achieved by compressing the data in the feature\n\nspace F. The problem of reconstructing a valid preimage x ? RNf in the original\nspace, given the compression coordinates of ?(x) ? F is known as the preimage\nproblem. It\u2019s formulated and \u201csolved\u201d next for the polynomial kernel family.\n\n106\n\n\n\nc.4 p r e i m ag e p ro b l e m\n\nConsider a point x ? RNf is projected onto a normalized eigenbasis v1, . . . , vNc ? F,\nthat is, its image ?(x) ? F is approximately written:\n\n?(x) ? Proj(x) def=\nNc?\n\nk=1\n\n?kvk\n\nwith ?k = ??(x), vk? the dual coefficients. The eigenbasis of interest is stored in\nthe columns of A =\n\n[\n?1 ?2 \u00b7 \u00b7 \u00b7 ?Nc\n\n]\nin terms of the mapped ensemble, and is\n\nnever computed explicitly:\n\nvk =\n\nNr?\n\ni=1\n\n?ki ?(xi) for k = 1, 2, . . . , Nc\n\nGiven the projection coordinates ? = (?1, ?2, . . . , ?Nc)\n?, how to reconstruct\n\nthe corresponding preimage x ? RNf ? This ill-posed inverse problem is usually\n\u201csolved\u201d by minimizing the distance to the projection in F:\n\nx? = arg min\nz?RNf\n\n?Proj(x) ? ?(z)?2\n\nSuch objective function can be rewritten in terms of the kernel k(x, y) by simple\n\nsubstitution of the expressions for Proj(x) and vk:\n\n?(z)\ndef\n= ?Proj(x) ? ?(z)?2\n\n= k(z, z) ? 2\nNc?\n\nk=1\n\n?k\n\nNr?\n\ni=1\n\n?ki k(xi, z) + C\n\nwith C a term that doesn\u2019t depend on z ? RNf . In matrix form, by dropping the\nconstant and defining Kz\n\ndef\n= (k(x1, z), k(x2, z), . . . , k(xNr , z))\n\n?:\n\n?(z) = k(z, z) ? 2??A?Kz\n\nthe only term that needs further work is ?. It can also be written in terms of the\n\n(symmetric) kernel:\n\n?k = ??(x), vk? =\nNr?\n\ni=1\n\n?ki k(xi, x) = ?\nk?Kx for k = 1, 2, . . . , Nc\n\nwith Kx evaluated at x the same way Kz is evaluated at z. Thus, ? = A?Kx and\n\nfinally:\n\n?(z) = k(z, z) ? 2K?x AA\n?Kz\n\nA row vector b?\ndef\n= ?\n\n?A? = K?x AA\n? is defined to simplify expressions in the\n\nfollowing proofs.\n\n107\n\n\n\nc.4.1 Linear kernel\n\nThe objective function for the linear kernel k(x, y)\ndef\n= x?y is given by:\n\n?(z) = k(z, z) ? 2b?Kz\n\n= z?z ? 2b?X?z\n\nTake the gradient w.r.t. z and equate to zero:\n\n?z?(z) = 2z ? 2Xb = 0\n\nThe preimage x? ? RNf is therefore a linear combination of the ensemble X:\n\nx? = arg min\nz?RNf\n\n?(z) = Xb\n\nThe coefficients b might be normalized before multiplication (i. e.\n?\n\ni bi = 1) to\n\nhonor hard data (e. g. well logs).\n\nc.4.2 Monomial kernel\n\nThe objective for the monomial kernel k(x, y)\ndef\n= (x?y)d is given by:\n\n?(z) = k(z, z) ? 2b?Kz\n\n= (z?z)d ? 2b?(X?z)d\n\nwhere (X?z)d is entrywise pow(). Take the gradient w.r.t. z and equate to zero:\n\n?z?(z) = 2d(z?z)d?1z ? 2dX diag\n(\n(X?z)d?1\n\n)\nb = 0\n\nThere is no closed-form solution for the equation above. It\u2019s approximated with\n\nfixed-point iteration z = f(z):\n\nz = X diag\n\n((\nX?z\nz?z\n\n)d?1)\nb\n\nAt each iteration, the preimage is still a linear combination of the ensemble (i. e.\n\nz = Xc) where coefficients are normalized to honor hard data. Note the linear\n\nkernel is recovered for d = 1.\n\n108\n\n\n\nc.4.3 Polynomial kernel\n\nThe objective for the polynomial kernel k(x, y)\ndef\n= x?y + (x?y)2 + \u00b7 \u00b7 \u00b7+ (x?y)d is\n\ngiven by:\n\n?(z) = k(z, z) ? 2b?Kz\n\n= z?z + (z?z)2 + \u00b7 \u00b7 \u00b7+ (z?z)d\n\n? 2b?\n(\n\nX?z + (X?z)2 + \u00b7 \u00b7 \u00b7+ (X?z)d\n)\n\nTake the gradient w.r.t. z and equate to zero:\n\n?z?(z) = 2\n(\n1 + 2z?z + 3(z?z)2 + \u00b7 \u00b7 \u00b7+ d(z?z)d?1\n\n)\nz\n\n? 2X\n\n(\nI + 2 diag\n\n(\nX?z\n\n)\n+ 3 diag\n\n(\n(X?z)2\n\n)\n+ \u00b7 \u00b7 \u00b7+ d diag\n\n(\nX?z\n\n)d?1)\nb = 0\n\nAgain, there is no closed-form solution to the equation above. It\u2019s approximated\n\nwith fixed-point iteration z = f(z):\n\nz = X diag\n(\n\nI + 2X?z + \u00b7 \u00b7 \u00b7+ d(X?z)d?1\n1 + 2z?z + \u00b7 \u00b7 \u00b7+ d(z?z)d?1\n\n)\nb\n\nThe preimage is a linear combination of the ensemble X where coefficients are\n\nnormalized to honor the hard data. Previous solutions for linear and monomial\n\nkernels are trivially recovered.\n\nRegardless of the kernel, the difference between compressing and denoising resides\n\nin the column vector b:\n\nb\ndef\n= A? = AA?Kx\n\nIn denoising, a valid image x ? RNf is mapped to the feature space with Kx\nand then b = AA?Kx is computed. In compressing, the (uncorrelated) coordinates\n\n? ? RNc are given and b = A? is used instead.\nkPCA is used in this dissertation basically for compressing permeability maps\n\nx = x(?), but denoising might also be used as an optional post-processing step.\n\ni m p o r ta n t n o t e : For all kernels here discussed, the preimage is an affine\n\ncombination of the ensemble (i. e. a linear combination with coefficients adding up\n\nto unit), but it isn\u2019t convex. This means realizations might have values outside the\n\noriginal range of the data (e. g. negative permeabilities) that should be truncated if\n\nnecessary.\n\n109\n\n\n\nB I B L I O G R A P H Y\n\n[1] J\u00falio H. Mendes, Ramiro B. Willmersdorf, and \u00c9zio R. Ara\u00fajo. Ajuste de\n\nhist\u00f3rico como problema inverso e conex\u00e3o com geostat\u00edstica. In I Workshop\n\ndos Programas de Forma\u00e7\u00e3o de Recursos Humanos nas \u00c1reas de Petr\u00f3leo, G\u00e1s, Bio-\n\ncombust\u00edveis, Naval e Offshore da UFPE, Av. Prof. Moraes Rego, 1235, Mar\u00e7o\n\n2013a. UFPE. (Cited on page viii.)\n\n[2] J\u00falio H. Mendes, Ramiro B. Willmersdorf, and \u00c9zio R. Ara\u00fajo. Multiple-point\n\nstatistics and kpca parametrization for reservoir characterization. Semin\u00e1rios\n\nPeri\u00f3dicos PRH-26/ANP/UFPE, May 2013b. (Cited on page viii.)\n\n[3] J\u00falio H. Mendes, Ramiro B. Willmersdorf, and \u00c9zio R. Ara\u00fajo. Ajuste de\n\nhist\u00f3rico sob \u00f3tica bayesiana - caracteriza\u00e7\u00e3o geostat\u00edstica de m\u00faltiplos pontos\n\ne invers\u00e3o quasi-cont\u00ednua via markov chain monte carlo. Proposta de tese\n\napresentada no CENPES, Junho 2013c. (Cited on page viii.)\n\n[4] Richard C. Aster, Brian Borchers, and Clifford H. Thurber. Parameter Estima-\n\ntion and Inverse Problems (International Geophysics). Academic Press, 2005. ISBN\n\n0120656043. URL http://www.ees.nmt.edu/outside/courses/GEOP529_book.\n\nhtml. (Cited on pages 3, 13, 17, and 26.)\n\n[5] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998. ISBN\n\n0471030031. (Cited on page 3.)\n\n[6] Jos\u00e9 Paulo de Almeida e Albuquerque, Jos\u00e9 Mauro Pedro Fortes, and\n\nWeiler Alves Finamore. Probabilidade, Vari\u00e1veis Aleat\u00f3rias e Processos Estoc\u00e1s-\n\nticos. T\u00e9cnico-cient\u00edfica n.8. PUC-Rio, Rua Marqu\u00eaz de S\u00e3o Vicente, 225 - Casa\n\nEditora G\u00e1vea, RJ, 22.451-900, 2008. URL http://www.editora.vrc.puc-rio.\n\nbr/probabilidade.html. (Cited on pages 3 and 19.)\n\n[7] P. K. Kitanidis. Introduction to Geostatistics: Applications in Hydrogeology\n\n(Stanford-Cambridge Program). Cambridge University Press, 1997. ISBN\n\n0521587476. (Cited on page 3.)\n\n110\n\nhttp://www.ees.nmt.edu/outside/courses/GEOP529_book.html\nhttp://www.ees.nmt.edu/outside/courses/GEOP529_book.html\nhttp://www.editora.vrc.puc-rio.br/probabilidade.html\nhttp://www.editora.vrc.puc-rio.br/probabilidade.html\n\n\nb i b l i o g r a p h y 111\n\n[8] Ricardo A. Olea. Geostatistics for Engineers and Earth Scientists. Springer, 1999.\n\nISBN 0792385233. (Cited on page 3.)\n\n[9] Nicolas Remy, Alexandre Boucher, and Jianbing Wu. Applied Geostatistics with\n\nSGeMS: A User\u2019s Guide. Cambridge University Press, 2011. ISBN 1107403243.\n\n(Cited on page 3.)\n\n[10] Yongshe Liu. Geostatistical Integration of Linear Coarse Scale and Fine Scale Data.\n\nPhD thesis, Department of Petroleum Engineering - Stanford University, 2007.\n\n(Cited on page 4.)\n\n[11] Scarlet A. Castro. A Probabilistic Approach to Jointly Integrate 3D/4D Seismic, Pro-\n\nduction Data and Geological Information for Building Reservoir Models. PhD thesis,\n\nDepartment of Energy Resources - Stanford University, June 2007. DOWN-\n\nLOAD. (Cited on pages 4 and 58.)\n\n[12] Shuguang Mao and Andr\u00e9 G. Journel. Generation of a reference petrophysi-\n\ncal/seismic data set: the stanford v reservoir. Technical report, Stanford Uni-\n\nversity, 1999. (Cited on page 4.)\n\n[13] Jean-Paul Chil\u00e8s and Pierre Delfiner. Geostatistics: Modeling Spatial Uncer-\n\ntainty (Wiley Series in Probability and Statistics). Wiley, 2012. ISBN 0470183152.\n\nURL http://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/\n\nsummary. (Cited on pages 4 and 66.)\n\n[14] Nicolas Remy. Gstl: The geostatistical template library in c++. Master\u2019s thesis,\n\nStanford University, March 2001. (Cited on page 4.)\n\n[15] Thomas M. Hansen, Klaus Mosegaard, and Knud S. Cordua. Using geostastis-\n\ntics to describe complex a priori information for inverse problems. In Juli\u00e1n M.\n\nOrtiz and Xavier Emery, editors, Proceedings of the Eighth International Geo-\n\nstatistics Congress, pages 329\u2013338. University of Copenhagen, 2008. (Cited on\n\npages 4 and 69.)\n\n[16] Thomas M. Hansen and Klaus Mosegaard. Visim: Sequential simulation for\n\nlinear inverse problems. Computers &amp; Geosciences, (34):53\u201376, March 2008.\n\n(Cited on page 4.)\n\n[17] Thomas M. Hansen, Andr\u00e9 G. Journel, Albert Tarantola, and Klaus\n\nMosegaard. Linear inverse gaussian theory and geostatistics. Geophysics, 71\n\n(6), December 2006. (Cited on page 4.)\n\nhttp://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_PhD_Castro.pdf\nhttp://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_PhD_Castro.pdf\nhttp://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/summary\nhttp://onlinelibrary.wiley.com/doi/10.1002/9780470316993.refs/summary\n\n\nb i b l i o g r a p h y 112\n\n[18] Adalberto Jos\u00e9 Rosa, Renato de Souza Carvalho, and Jos\u00e9 Augusto Daniel\n\nXavier. Engenharia de Reservat\u00f3rios de Petr\u00f3leo. Interci\u00eancia Ltda., Rua\n\nVerna Magalh\u00e3es, 66 \u2013 Engenho Novo \u2013 RJ, 2006. URL http://www.\n\neditorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;idprod=82.\n\n(Cited on page 5.)\n\n[19] Tiago Almeida Costa. Aplica\u00e7\u00e3o do m\u00e9todo dos elementos finitos (mef) para\n\nmodelos de testes de forma\u00e7\u00e3o em po\u00e7os de petr\u00f3leo. Master\u2019s thesis, UNI-\n\nCAMP, Mar\u00e7o 2013. (Cited on page 5.)\n\n[20] Faruk O. Alpack, Carlos Torres-Verd\u00edn, and Kamy Sepehrnoori. Estima-\n\ntion of axisymmetric spatial distributions of permeability and porosity from\n\npressure-transient data acquired with in situ permanent sensors. Journal of\n\nPetroleum Science and Engineering, 44:231\u2013267, March 2004. (Cited on page 5.)\n\n[21] Jacques Hadamard. La Theorie Des Equations Aux Derivees Partielles. Imprime\n\nEn Chine, 1964. (Cited on page 6.)\n\n[22] Albert Tarantola. Inverse Problem Theory and Methods for Model Parameter Esti-\n\nmation. SIAM: Society for Industrial and Applied Mathematics, 2004. ISBN\n\n0898715725. URL http://www.ec-securehost.com/SIAM/ot89.html. (Cited\n\non pages 9, 10, 29, 33, 36, and 41.)\n\n[23] Andreas Kirsch. An Introduction to the Mathematical Theory of In-\n\nverse Problems (Applied Mathematical Sciences, Vol. 120). Springer, 2011.\n\nISBN 1441984739. URL http://www.springer.com/mathematics/dynamical+\n\nsystems/book/978-1-4419-8473-9. (Cited on page 15.)\n\n[24] Dean S. Oliver, Albert C. Reynolds, and Ning Liu. Inverse Theory for Petroleum\n\nReservoir Characterization and History Matching. Cambridge University Press,\n\n2008. ISBN 052188151X. URL http://www.cambridge.org/gb/knowledge/\n\nisbn/item1174799. (Cited on pages 15 and 26.)\n\n[25] John A. Scales, Adam Gersztenkorn, and Sven Treitel. Fast lp solution of large,\n\nsparse, linear systems: Application to seismic travel time tomography. Journal\n\nof Computational Physics, 75:314\u2013333, 1988. (Cited on page 17.)\n\n[26] Justyna K. Przybysz-Jarnut, Remus G. Hanea, Jan-Dirk Jansen, and Arnold W.\n\nHeemink. Application of the representer method for parameter estimation in\n\nnumerical reservoir models. Computers &amp; Geosciences, 2007. (Cited on page 19.)\n\nhttp://www.editorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;idprod=82\nhttp://www.editorainterciencia.com.br/index.asp?pg=prodDetalhado.asp&amp;idprod=82\nhttp://www.ec-securehost.com/SIAM/ot89.html\nhttp://www.springer.com/mathematics/dynamical+systems/book/978-1-4419-8473-9\nhttp://www.springer.com/mathematics/dynamical+systems/book/978-1-4419-8473-9\nhttp://www.cambridge.org/gb/knowledge/isbn/item1174799\nhttp://www.cambridge.org/gb/knowledge/isbn/item1174799\n\n\nb i b l i o g r a p h y 113\n\n[27] Zhao Hui, Li Yang, Yao Jun, and Zhang Kai. Theoretical research on reser-\n\nvoir closed-loop production management. SCIENCE CHINA - Technological\n\nSciences, 54(10):2815\u20132824, 2011. (Cited on page 19.)\n\n[28] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge\n\nUniversity Press, 2004. ISBN 9780521833783. URL http://www.stanford.edu/\n\n~boyd/cvxbook. (Cited on page 22.)\n\n[29] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Sta-\n\ntistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer\n\nSeries in Statistics). Springer, 2009. ISBN 0387848576. URL http://www-stat.\n\nstanford.edu/~tibs/ElemStatLearn. (Cited on page 23.)\n\n[30] Ryan J. Tibshirani. The lasso problem and uniqueness. Eletronic Journal of\n\nStatistics, 7:1456\u20131490, May 2013. (Cited on page 23.)\n\n[31] Andrew M. Bradley. Pde-constrained optimization and the adjoint method,\n\nJune 2013. DOWNLOAD. (Cited on page 26.)\n\n[32] Ruijian Li, A. C. Reynolds, and D. S. Oliver. History matching of three-phase\n\nflow production data. SPE Journal, December 2003. (Cited on page 26.)\n\n[33] Zhan Wu, A. C. Reynolds, and D. S. Oliver. Conditioning geostatistical models\n\nto two-phase production data. SPE Journal, 4, June 1999. (Cited on page 26.)\n\n[34] Jonathan Richard Shewchuk. An introduction to the conjugate gradient\n\nmethod without the agonizing pain. Carnegie Mellon University - Pittsburg,\n\nPA 15213, August 1994. DOWNLOAD. (Cited on page 26.)\n\n[35] Alexandre Anoz\u00e9 Emerick. History Matching and Uncertainty Characteriza-\n\ntion using Ensemble-based Methods. PhD thesis, The university of Tulsa, 2012.\n\nDOWNLOAD. (Cited on pages 27 and 54.)\n\n[36] Alexandre A. Emerick and Albert C. Reynolds. Investigation of the sampling\n\nperformance of ensemble-based methods with a simple reservoir model. Com-\n\nputational Geosciences, November 2012. (Cited on pages 27 and 54.)\n\n[37] P. Sarma and W. H. Chen. Generalization of the ensemble kalman filter using\n\nkernels for non-gaussian random fields. SPE Journal, (119177), February 2009.\n\n(Cited on page 27.)\n\nhttp://www.stanford.edu/~boyd/cvxbook\nhttp://www.stanford.edu/~boyd/cvxbook\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn\nhttp://www.stanford.edu/~ambrad/adjoint_tutorial.pdf\nhttp://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf\nhttp://www.tuprep.utulsa.edu/comparative_study/Emerick_PhD.pdf\n\n\nb i b l i o g r a p h y 114\n\n[38] Dongxiao Zhang, Heng Li, and Haibin Chang. History matching for non-\n\ngaussian random fields using the probabilistic collocation based kalman filter.\n\nSPE Journal, 2011. (Cited on page 27.)\n\n[39] J\u00falio Hoffimann Mendes and Ramiro Brito Willmersdorf. Sparse probabilis-\n\ntic collocation for uncertainty quantification in reservoir engineering. 10-th\n\nWorld Congress on Computational Mechanics, July 2012. DOWNLOAD. (Cited\n\non pages 27 and 42.)\n\n[40] Nir Friedman and Joseph Y. Halpern. Plausibility measures: A user\u2019s guide.\n\nIn Eleventh Conference on Uncertainty in Artificial Intelligence, volume 95, pages\n\n175\u2013184, 1995. DOWNLOAD. (Cited on page 29.)\n\n[41] Nir Friedman and Joseph Y. Halpern. Plausibility measures and default rea-\n\nsoning. ACM Journal, 48(4):648\u2013685, 2001. DOWNLOAD. (Cited on page 29.)\n\n[42] Albert Tarantola. Logarithmic parameters, November 2001. DOWNLOAD.\n\n(Cited on page 30.)\n\n[43] Albert Tarantola. Inverse Problem Theory, chapter List of Errata. SIAM: Society\n\nfor Industrial and Applied Mathematics, July 2005. DOWNLOAD. (Cited on\n\npage 34.)\n\n[44] Klaus Mosegaard and Albert Tarantola. Probabilistic Approach to Inverse Prob-\n\nlems, chapter Part A, pages 237\u2013265. Academic Press, November 2002. DOWN-\n\nLOAD. (Cited on page 35.)\n\n[45] Nitin Agarwal and N. R. Aluru. A domain adaptive stochastic collocation\n\napproach for analysis of mems under uncertaintes. Journal of Computational\n\nPhysics, 2009. (Cited on page 42.)\n\n[46] M. S. Eldred, C. G. Webster, and P. G. Constantine. Evaluation of non-intrusive\n\napproaches for wiener-askey generalized polynomial chaos. American Institute\n\nof Aeronautics and Astronautics, 2008. (Cited on page 42.)\n\n[47] Thomas Gerstner and Michael Griebel. Numerical integration using sparse\n\ngrids. (Cited on page 42.)\n\n[48] Sergey Smolyak. Quadrature and interpolation formulas for tensor products\n\nof certain classes of functions. Doklady Akademii Nauk SSSR, 1963. (Cited on\n\npage 42.)\n\nhttp://www.academia.edu/1956459/Sparse_Probabilistic_Collocation_for_Uncertainty_Quantification_in_Reservoir_Engineering\nhttp://robotics.stanford.edu/people/nir/Papers/uai95.pdf\nhttp://robotics.stanford.edu/people/nir/Papers/FrH5Full.pdf\nhttp://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/Music.pdf\nhttp://www.ipgp.fr/~tarantola/Files/Professional/Books/ErrataIP/Errata.pdf\nhttp://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/InverseProblemHandbk.pdf\nhttp://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/InverseProblemHandbk.pdf\n\n\nb i b l i o g r a p h y 115\n\n[49] Dani Gamerman and Hedibert F. Lopes. Markov Chain Monte Carlo: Stochastic\n\nSimulation for Bayesian Inference, Second Edition (Chapman &amp; Hall/CRC Texts in\n\nStatistical Science). Chapman and Hall/CRC, 2006. ISBN 1584885874. URL\n\nhttp://www.crcpress.com/product/isbn/9781584885870. (Cited on pages 42\n\nand 51.)\n\n[50] Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and Michael I. Jordan.\n\nAn introduction to mcmc for machine learning. Machine Learning, (50):5\u201343,\n\n2003. DOWNLOAD. (Cited on page 43.)\n\n[51] W. K. Hastings. Monte carlo sampling methods using markov chains and\n\ntheir applications. Biometrika, 57(1):97\u2013109, April 1970. DOWNLOAD. (Cited\n\non page 45.)\n\n[52] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Au-\n\ngusta H. Teller, and Edward Teller. Equation of state calculations by fast com-\n\nputing machines. The Journal of Chemical Physics, 21(6):1087\u20131092, June 1953.\n\n(Cited on page 46.)\n\n[53] Jaakoo Peltonen, Jarkoo Venna, and Samuel Kaski. Visualization for assessing\n\nconvergence and mixing of markov chain monte carlo simulations. Compu-\n\ntational Statistics and Data Analysis, 53(12):4453\u20134470, October 2009. DOWN-\n\nLOAD. (Cited on page 49.)\n\n[54] Andrew Gelman and Donald B. Rubin. Inference from iterative simulation\n\nusing multiple sequences. Statistical Science, 7(4):457\u2013511, 1992. DOWNLOAD.\n\n(Cited on page 49.)\n\n[55] Mary Kathryn Cowles and Bradley P. Carlin. Markov chain monte carlo con-\n\nvergence diagnostics: A comparative review. Journal of the American Statistical\n\nAssociation, 91(434):883\u2013904, June 1996. (Cited on page 49.)\n\n[56] Emanuel Parzen. On estimation of a probability density function and mode.\n\nThe Annals of Mathematical Statistics, 33(3):1065\u20131076, 1962. DOWNLOAD.\n\n(Cited on pages 50 and 68.)\n\n[57] George R. Terrell and David W. Scott. Variable kernel density estimation. The\n\nAnnals of Statistics, 20(3):1236\u20131265, 1992. DOWNLOAD. (Cited on page 50.)\n\nhttp://www.crcpress.com/product/isbn/9781584885870\nhttp://www.cs.ubc.ca/~nando/papers/mlintro.pdf\nhttp://www.jstor.org/stable/2334940\nhttp://www.cis.hut.fi/projects/mi/papers/csda09_preprint.pdf\nhttp://www.cis.hut.fi/projects/mi/papers/csda09_preprint.pdf\nhttp://www.stat.columbia.edu/~gelman/research/published/itsim.pdf\nhttp://projecteuclid.org/euclid.aoms/1177704472\nhttp://projecteuclid.org/euclid.aos/1176348768\n\n\nb i b l i o g r a p h y 116\n\n[58] Jonathan Goodman and Jonathan Weare. Ensemble samplers with affine in-\n\nvariance. Communications in Applied Mathematics and Computational Science, 5\n\n(1):65\u201380, 2010. DOWNLOAD. (Cited on pages 52 and 53.)\n\n[59] J. Andr\u00e9s Christen and Colin Fox. A general purpose sampling algorithm for\n\ncontinuous distributions (the t-walk). Bayesian Analysis, 5(2):263\u2013282, Decem-\n\nber 2010. DOWNLOAD. (Cited on page 52.)\n\n[60] D. Foreman-Mackey, D. W. Hogg, D. Lang, and J. Goodman. emcee: The\n\nMCMC Hammer. 125:306\u2013312, March 2013. doi: 10.1086/670067. DOWN-\n\nLOAD. (Cited on page 53.)\n\n[61] D. J. Earl and M. W. Deem. Parallel tempering: Theory, applications, and new\n\nperspectives. Physical Chemistry Chemical Physics (Incorporating Faraday Trans-\n\nactions), 7, 2008. doi: 10.1039/b509983h. DOWNLOAD. (Cited on page 54.)\n\n[62] Radford M. Neal. Annealed importance sampling. Technical Report 9805,\n\nUniversity of Toronto, Ontario, CA, September 1998. DOWNLOAD. (Cited\n\non page 54.)\n\n[63] Thomas Romary. Integrating production data under uncertainty by parallel\n\ninteracting markov chains on a reduced dimensional space. Journal of Compu-\n\ntational Geosciences, 13:103\u2013122, 2009. (Cited on page 54.)\n\n[64] A. Malakis and T. Papakonstantinou. Comparative study of selected parallel\n\ntempering methods. 88(1), 2013. doi: 10.1103/PhysRevE.88.013312. DOWN-\n\nLOAD. (Cited on page 54.)\n\n[65] Alexandre A. Emerick and Albert C. Reynolds. History matching time-lapse\n\nseismic data using the ensemble kalman filter with multiple data assimi-\n\nlations. Computational Geostatistics, 16(3), 2012. DOWNLOAD. (Cited on\n\npage 54.)\n\n[66] Alexandre A. Emerick and Albert C. Reynolds. Ensemble smoother with mul-\n\ntiple data assimilation. Computers &amp; Geosciences, 2012. DOWNLOAD. (Cited\n\non page 54.)\n\n[67] Albert Tarantola and Bernard Valette. Inverse problems = quest for informa-\n\ntion. Journal of Geophysics, 50:159\u2013170, 1982. DOWNLOAD. (Cited on page 57.)\n\nhttp://msp.org/camcos/2010/5-1/camcos-v5-n1-p04-p.pdf\nhttp://ba.stat.cmu.edu/journal/2010/vol05/issue02/christen.pdf\nhttp://arxiv.org/pdf/1202.3665v4\nhttp://arxiv.org/pdf/1202.3665v4\nhttp://arxiv.org/pdf/physics/0508111v2\nhttp://arxiv.org/pdf/physics/9803008v2\nhttp://arxiv.org/pdf/1305.4907v3\nhttp://arxiv.org/pdf/1305.4907v3\nhttp://enkf.nersc.no/Publications/eme12a.pdf\nhttp://www.academia.edu/4799481/Ensemble_Smoother_with_Multiple_Data_Assimilation\nhttp://www.ipgp.fr/~tarantola/Files/Professional/Papers_PDF/IP_QI_original.pdf\n\n\nb i b l i o g r a p h y 117\n\n[68] Guven Burc Arpat. Sequential Simulation with Patterns. PhD thesis, Department\n\nof Petroleum Engineering - Stanford University, January 2005. DOWNLOAD.\n\n(Cited on page 58.)\n\n[69] Mehrdad Honarkhah. Stochastic Simulation of Patterns using Distance-based Pat-\n\ntern Modeling. PhD thesis, Department of Energy Resources Engineering -\n\nStanford University, April 2011. DOWNLOAD. (Cited on page 58.)\n\n[70] Pallav Sarma. Efficient Closed-loop Optimal Control of Petroleum Reservoirs under\n\nUncertainty. PhD thesis, Department of Petroleum Engineering - Stanford\n\nUniversity, September 2006. DOWNLOAD. (Cited on pages 58, 67, and 104.)\n\n[71] Tuanfeng Zhang. Filter-based Training Pattern Classification for Spatial Pattern\n\nSimulation. PhD thesis, Department of Geological and Environmental Sciences\n\n- Stanford University, March 2006. DOWNLOAD. (Cited on pages 58 and 65.)\n\n[72] Kees Geel. Description of the Brugge Field and Property Realizations. TNO, Febru-\n\nary 2008. (Cited on page 60.)\n\n[73] Yan Chen and Dean S. Oliver. Ensemble-based closed-loop optimization ap-\n\nplied to brugge field. SPE Reservoir Evaluation &amp; Engineering, pages 56\u201371,\n\nFebruary 2010. (Cited on page 60.)\n\n[74] E. Peters, R. J. Arts, G. K. Brouwer, C. R. Geel, S. Cullick, R. J. Lorentzen,\n\nY. Chen, K. N. B. Dunlop, F. C. Vossepoel, R. Xu, P. Sarma, A. H. Alhutali,\n\nand A. C. Reynolds. Results of the brugge benchmark study for flooding\n\noptimization and history matching. SPE Reservoir Evaluation &amp; Engineering,\n\npages 391\u2013405, June 2010. (Cited on page 60.)\n\n[75] Sebastien Strebelle. Multiple-point geostatistics: from theory to practice. In\n\nExpanded Abstract Collection from Ninth International Geostatistics Congress. Nor-\n\nwegian Computing Center, 2012. DOWNLOAD. (Cited on page 65.)\n\n[76] Bruno Dujardin. Sensitivity analysis of filtersim and histogram reproduction.\n\nMaster\u2019s thesis, Department of Energy Resources Engineering - Stanford Uni-\n\nversity, June 2007. DOWNLOAD. (Cited on page 65.)\n\n[77] Jon Shlens. A tutorial on principal component analysis - derivation, discussion\n\nand singular value decomposition. Technical report, University of California,\n\nSan Diego, March 2003. DOWNLOAD. (Cited on page 66.)\n\nhttp://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2005_PhD_Arpat.pdf\nhttp://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2010-2019/2011_PhD_Mehrdad_Honarkhah.pdf\nhttps://pangea.stanford.edu/ERE/db/pereports/record_detail.php?filename=Sarma06.pdf\nhttp://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2006_PhD_Zhang.pdf\nhttp://geostats2012.nr.no/pdfs/1744859.pdf\nhttps://pangea.stanford.edu/departments/ere/dropbox/scrf/documents/Theses/SCRF-Theses/2000-2009/2007_MS_Dujardin.pdf\nhttp://www.snl.salk.edu/~shlens/pca.pdf\n\n\nb i b l i o g r a p h y 118\n\n[78] C. G. Wu, Y. C. Liang, W. Z. Lin, H. P. Lee, and S. P. Lim. A note on equivalence\n\nof proper orthogonal decomposition methods. Journal of Sound and Vibration,\n\n265:1103\u20131110, January 2003. (Cited on page 66.)\n\n[79] Allan Aasbjerg Nielsen. Orthogonal transformations. Technical report, De-\n\npartment of Mathematical Modelling - Techinical University of Denmark,\n\n2000. (Cited on page 66.)\n\n[80] J. Antonio Vargas-Guzm\u00e1n and Roussos Dimitrakopoulos. Computational\n\nproperties of min/max autocorrelation factors. Computers &amp; Geosciences, 29:\n\n715\u2013723, January 2003. DOWNLOAD. (Cited on page 66.)\n\n[81] Bernhard Sch\u00f6lkopf, Alexander Smola, and Klaus-Robert M\u00fcller. Nonlinear\n\ncomponent analysis as a kernel eigenvalue problem. Technical Report 44,\n\nDecember 1996. (Cited on pages 66, 67, and 104.)\n\n[82] Allan Aasbjerg Nielsen. Kernel maximum autocorrelation factor and mini-\n\nmum noise fraction transformations. In IEEE Transactions on Image Processing,\n\nvolume 20, pages 612\u2013624, March 2011. DOWNLOAD. (Cited on page 66.)\n\n[83] Allan A. Nielsen and Morton J. Canty. Kernel principal component and maxi-\n\nmum autocorrelation factor analyses for change detection. In Image and Signal\n\nProcessing for Remote Sensing XV, 2009. DOWNLOAD. (Cited on page 66.)\n\n[84] Camilla Zacch\u00e9 da Silva. A descorrela\u00e7\u00e3o de vari\u00e1veis com fatoriza\u00e7\u00e3o maf\n\nem estimativa de teores. Master\u2019s thesis, Department of Mining - Federal\n\nUniversity of Rio Grande do Sul, 2013. DOWNLOAD. (Cited on page 66.)\n\n[85] Mar\u00eda Noel Morales Boezio. Estudo das Metodologias Alternativas da Geostat\u00eds-\n\ntica Multivariada Aplicadas a Estimativa de Teores de Dep\u00f3sitos de Ferro. PhD\n\nthesis, Department of Mining - Federal University of Rio Grande do Sul, 2010.\n\nDOWNLOAD. (Cited on page 66.)\n\n[86] Kwangwon Park. Seeing invisible properties of subsurface oil and gas reser-\n\nvoir through extensive uses of machine learning algorithms. Technical report,\n\nDepartment of Energy Resources Engineering - Stanford University, Septem-\n\nber 2007. DOWNLOAD. (Cited on page 67.)\n\n[87] Pallav Sarma, Louis J. Durlofsky, Khalid Aziz, and When H. Chen. A new ap-\n\nproach to automatic history matching using kernel pca. SPE Journal, (106176),\n\nFebruary 2007. (Cited on page 67.)\n\nhttps://www.researchgate.net/publication/222899505_Computational_properties_of_minmax_autocorrelation_factors\nhttp://www2.imm.dtu.dk/pubdb/views/edoc_download.php/5925/pdf/imm5925.pdf\nhttp://www2.imm.dtu.dk/pubdb/views/edoc_download.php/5757/pdf/imm5757.pdf\nhttp://www.lume.ufrgs.br/bitstream/handle/10183/75914/000891754.pdf\nhttp://www.lume.ufrgs.br/bitstream/handle/10183/33664/000761391.pdf\nhttp://cs229.stanford.edu/proj2007/Park-SeeingInvisiblePropertiesofSubsurfaceOilandGasReservoir.pdf\n\n\nb i b l i o g r a p h y 119\n\n[88] Pallav Sarma, Louis J. Durlofsky, and Khalid Aziz. Kernel principal compo-\n\nnent analysis for efficient, differentiable parametrization of multipoint geo-\n\nstatistics. Math Geosciences, 40:3\u201332, December 2007. (Cited on pages 67\n\nand 104.)\n\n[89] Bernhard Sch\u00f6lkopf, Sebastian Mika, Alex Smola, Gunnar R\u00e4tsch, and Klaus-\n\nRobert M\u00fcller. Kernel pca pattern reconstruction via approximate pre-images,\n\n1998. DOWNLOAD. (Cited on page 67.)\n\n[90] Sebastian Mika, Bernhard Sch\u00f6lkopf, Alex Smola, Klaus-Robert M\u00fcller,\n\nMatthias Scholz, and Gunnar R\u00e4tsch. Kernel pca and de-noising in feature\n\nspaces, 1999. DOWNLOAD. (Cited on page 67.)\n\n[91] James T. Kwok and Ivor W. Tsang. The pre-image problem in kernel methods.\n\nIn Twentieth International Conference on Machine Learning, 2003. DOWNLOAD.\n\n(Cited on page 67.)\n\n[92] Kwangwon Park. Modeling Uncertainty in Metric Space. PhD thesis, Depart-\n\nment of Energy Resources Engineering - Stanford University, January 2011.\n\nDOWNLOAD. (Cited on page 67.)\n\n[93] Thomas Mejer Hansen, Knud Skou Cordua, and Klaus Mosegaard. Inverse\n\nproblems with non-trivial priors: efficient solution through sequential gibbs\n\nsampling. Computers &amp; Geosciences, (16):593\u2013611, January 2012. DOWNLOAD.\n\n(Cited on page 69.)\n\n[94] Thomas Mejer Hansen, Knud Skou Cordua, Majken Caroline Looms, and\n\nKlaus Mosegaard. Sippi: A matlab toolbox for sampling the solution to in-\n\nverse problems with complex prior information part 1: Methodology. Com-\n\nputers &amp; Geosciences, 52:470\u2013480, September 2012. DOWNLOAD. (Cited on\n\npage 69.)\n\n[95] Thomas Mejer Hansen, Knud Skou Cordua, Majken Caroline Looms, and\n\nKlaus Mosegaard. Sippi: A matlab toolbox for sampling the solution to in-\n\nverse problems with complex prior information part 2: Application to cross-\n\nhole gpr tomography. Computers &amp; Geosciences, 52:481\u2013492, 2012. DOWN-\n\nLOAD. (Cited on page 69.)\n\n[96] Knud Skou Cordua, Thomas Mejer Hansen, and Klaus Mosegaard. Monte\n\ncarlo full-waveform inversion of crosshole gpr data using multiple-point geo-\n\nhttp://www.kernel-machines.org/papers/denoising.ps\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.5268&amp;rep=rep1&amp;type=pdf\nhttp://citeseer.ist.psu.edu/viewdoc/download?doi=10.1.1.330.8185&amp;rep=rep1&amp;type=pdf\nhttp://pangea.stanford.edu/~jcaers/Thesis_PhD_KPark.pdf\nhttps://www.researchgate.net/publication/257550612_Inverse_problems_with_non-trivial_priors_efficient_solution_through_sequential_Gibbs_sampling\nhttps://www.researchgate.net/publication/256505077_SIPPI_A_Matlab_toolbox_for_Sampling_the_solution_to_Inverse_Problems_with_complex_Prior_Information_Part_1__Methodology\nhttps://www.researchgate.net/publication/256505491_SIPPI_A_Matlab_toolbox_for_sampling_the_solution_to_inverse_problems_with_complex_prior_information_Part_2Application_to_crosshole_GPR_tomography\nhttps://www.researchgate.net/publication/256505491_SIPPI_A_Matlab_toolbox_for_sampling_the_solution_to_inverse_problems_with_complex_prior_information_Part_2Application_to_crosshole_GPR_tomography\n\n\nb i b l i o g r a p h y 120\n\nstatistical a priori information. Journal of Geophysics, 77(2):H19\u2013H31, February\n\n2012. DOWNLOAD. (Cited on page 69.)\n\n[97] Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simon-\n\ncelli. Image quality assessment: From error visibility to structural similarity.\n\nIEEE Transactions on Image Processing, 13(4):600\u2013612, April 2004. DOWNLOAD.\n\n(Cited on page 74.)\n\nhttp://www.researchgate.net/publication/258647313_Monte_Carlo_full-waveform_inversion_of_crosshole_GPR_data_using_multiple-point_geostatistical_a_priori_information\nhttp://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf\n\n\nc o l o p h o n\n\nThis document was typeset using the typographical look-and-feel classicthesis\n\ndeveloped by Andr\u00e9 Miede. The style was inspired by Robert Bringhurst\u2019s seminal\n\nbook on typography \u201cThe Elements of Typographic Style\u201d. classicthesis is available\n\nfor both LATEX and LYX:\n\nhttp://code.google.com/p/classicthesis/\n\nFinal Version as of June 3, 2014 (classicthesis version 1.0).\n\nhttp://code.google.com/p/classicthesis/\n\n\nD E C L A R A T I O N\n\nI undertake that all the material presented for examination is my own work and\n\nhas not been written for me, in whole or in part, by any other person. I also\n\nundertake that any quotation or paraphrase from the published or unpublished\n\nwork of another person has been duly acknowledged in the work which I present\n\nfor examination.\n\nRecife, PE 50670-901, May 28, 2014\n\nJ\u00falio Hoffimann Mendes,\n\nJune 3, 2014\n\n\n\tDedication\n\tAbstract\n\tPublications\n\tAcknowledgements\n\tContents\n\tList of Figures\n\tList of Tables\n\tList of Algorithms\n\tListings\n\tAcronyms\n\tInverse Problem Theory\n\t1 Basic Concepts\n\t1.1 What is an inverse problem?\n\t1.2 Why inverse problems are hard?\n\t1.3 The maximum likelihood principle\n\t1.4 Tarantola's postulate\n\t1.5 Classical vs. probabilistic framework\n\n\t2 Classical Framework\n\t2.1 Basic taxonomy for inverse problems\n\t2.2 Linear regression and the least-squares estimate\n\t2.3 Tikhonov regularization\n\t2.4 Levenberg-Marquardt solution to nonlinear regression\n\n\t3 Probabilistic Framework\n\t3.1 Definition of probability\n\t3.2 States of information\n\t3.3 Bayesian inversion\n\t3.4 Ensemble Markov chain Monte Carlo\n\n\n\tHistory Matching\n\t4 Prelude\n\t4.1 Problem description\n\t4.2 Case studies\n\t4.3 Comments on reproducibility\n\n\t5 Channelized Reservoir\n\t5.1 Setting priors\n\t5.2 Probabilistic inversion\n\t5.3 Analysis of the results\n\n\t6 Brugge Field\n\t6.1 Setting priors\n\t6.2 Probabilistic inversion\n\t6.3 Analysis of the results\n\n\t7 Conclusion\n\t7.1 General comments\n\t7.2 Technical difficulties\n\t7.3 Suggested improvements\n\n\n\tAppendix\n\tA Omitted Proofs\n\tA.1 The majority of inverse problems is ill-posed\n\tA.2 Maximum likelihood estimation for i.i.d. Gaussians\n\tA.3 System of equations for discrete linear inverse problems\n\tA.4 Maximum likelihood and least-squares\n\tA.5 Weighted linear least-squares estimate\n\tA.6 Levenberg-Marquardt gradient and Hessian\n\tA.7 Conditional probability by conjunction of states\n\tA.8 Kernel density estimation as a convolution\n\n\tB Code Snippets\n\tB.1 Iteratively reweighted least-squares\n\tB.2 Least absolute shrinkage and selection operator\n\tB.3 Metropolis algorithm\n\tB.4 Online Bayesian inversion\n\tB.5 Histogram fitting with kernel density estimation\n\n\tC Kernel PCA\n\tC.1 Kernel Gramian matrix\n\tC.2 Centering in the feature space\n\tC.3 Eigenproblem and normalization\n\tC.4 Preimage problem\n\n\tBibliography\n\tColophon\n\tDeclaration"}]}}}