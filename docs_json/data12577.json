{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.16628"}, {"@name": "filename", "#text": "23293_ferro_l_dr_rcla.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE ESTADUAL PAULISTA \n\nInstituto de Geoci\u00eancias e Ci\u00eancias Exatas \n\nCampus de Rio Claro \n\n \n\n \n\nLUCIANO FERRO \n\n \n\n \n\n \n\nAPLICA\u00c7\u00c3O DA REDE NEURAL MLP (MULTILAYER PERCEPTRON) EM \n\nIND\u00daSTRIA DE PISOS E REVESTIMENTOS DO P\u00d3LO CER\u00c2MICO DE SANTA \n\nGERTRUDES \u2013 SP \n\n \n\n \n\n \n\nTese de Doutorado apresentada ao Instituto \n\nde Geoci\u00eancias e Ci\u00eancias Exatas do \n\nCampus de Rio Claro, da Universidade \n\nEstadual Paulista J\u00falio de Mesquita Filho, \n\ncomo parte dos requisitos para obten\u00e7\u00e3o do \n\nt\u00edtulo de  Doutor em Geoci\u00eancias e Meio  \n\nAmbiente \n\n \n\n \n\n \n\nOrientador: Prof. Dr. Jos\u00e9 Ricardo Sturaro  \n\n \n\nRio Claro \u2013 SP \n\n2013\n\n\n\n \n\nFerro, Luciano\n     Aplica\u00e7\u00e3o da rede neural MLP (Multilayer Perceptron) em\nind\u00fastria de pisos e revestimentos do p\u00f3lo cer\u00e2mico de Santa\nGertrudes - SP / Luciano Ferro. - Rio Claro, 2013\n     143 f. : il., figs., gr\u00e1fs., tabs.\n\n     Tese (doutorado) - Universidade Estadual Paulista,\nInstituto de Geoci\u00eancias e Ci\u00eancias Exatas\n     Orientador: Jos\u00e9 Ricardo Sturaro\n\n     1. Redes neurais (Computa\u00e7\u00e3o). 2. Redes neurais\nartificiais. 3. Rede MLP. 4. Argila. 5. Vari\u00e1veis f\u00edsicas. I.\nT\u00edtulo.\n\n \n006.32\nF395a\n\n\t Ficha Catalogr\u00e1fica elaborada pela STATI - Biblioteca da UNESP\nCampus de Rio Claro/SP\n\n\n\n \n\nCOMISS\u00c3O EXAMINADORA \n\n \n\n \n\nDr. JOS\u00c9 RICARDO STURARO \n\nIGCE/UNESP/Rio Claro (SP) \n\n \n\nDr. PAULO MILTON BARBOSA LANDIM \n\nIGCE/UNESP/Rio Claro (SP) \n\n \n\nDr. RICARDO EGYDIO DE CARVALHO \n\nIGCE/UNESP/Rio Claro (SP) \n\n \n\nDr. ALESSANDRO FIRMIANO DE JESUS \n\nDivis\u00e3o de Ensino/Academia da For\u00e7a A\u00e9rea/Pirassununga (SP) \n\n \n\nDr. ALEXANDRE CAMPANE VIDAL \n\nIG/UNICAMP/Campinas (SP) \n\n \n\n \n\n \n\nDoutorando: LUCIANO FERRO \n\n \n\nRESULTADO: APROVADO \n\n \n\nData da Defesa: 25/04/2013 \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nDEDICAT\u00d3RIA \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nAos meus familiares e, em \n\nespecial, \u00e0 Maria, ao Gastone e \n\nVioletta (in memoriam), e Claudia, \n\nRodrigo, Giovanna e Gustavo.\n\n\n\nAGRADECIMENTOS \n\n \n\nAo Prof. Dr. Jos\u00e9 Ricardo Sturaro pela orienta\u00e7\u00e3o deste trabalho, pela dedicada \n\namizade e confian\u00e7a, pelo apoio e est\u00edmulos e pelos conhecimentos transmitidos. \n\n \n\n\u00c0 COPEMA (Comiss\u00e3o Permanente do Magist\u00e9rio), da Academia da For\u00e7a A\u00e9rea, \n\npelo apoio e pelas facilidades concedidas para que pud\u00e9ssemos concluir este trabalho. \n\n \n\nAos professores e funcion\u00e1rios do Instituto de Geoci\u00eancias e Ci\u00eancias Exatas da \n\nUNESP de Rio Claro (SP) pelo apoio, incentivos e ensinamentos transmitidos. \n\n \n\nAos colegas de curso Luiz Batista Castanheira, Natale Chierice J\u00fanior e Roseli \n\nAparecida Fernandes Chierice pela amizade e incentivos e ao Rogers R. da Rocha, \n\npropriet\u00e1rio das Cer\u00e2micas Triunfo e Rochaforte que, al\u00e9m da amizade, proporcionou os \n\nmateriais e os resultados das vari\u00e1veis f\u00edsicas, da sua tese de doutorado, para a nossa \n\nconsequente aplica\u00e7\u00e3o atrav\u00e9s das Redes Neurais Artificiais. \n\n \n\nAo Sr. Mario Castellano Pieroni, da Minera\u00e7\u00e3o Pieroni Ltda., aos Srs. Antonio Vitti, \n\nAntonio Cl\u00e1udio Vitti e F\u00e1bio Ramos Vitti, da Santa Am\u00e1bile Agropecu\u00e1ria e Minera\u00e7\u00e3o \n\nLtda., ambas de Rio Claro, por permitirem a visita e a tomada de fotos das minas para este \n\ntrabalho. Ao amigo Aldo Jos\u00e9 Colabone, Diretor de Meio Ambiente da Santa Am\u00e1bile \n\nAgropecu\u00e1ria e Minera\u00e7\u00e3o Ltda., por ter intermediado a permiss\u00e3o das visitas e ao seu t\u00e9cnico \n\nceramista S\u00e9rgio Antonio Castilho. \n\n \n\nA todos aqueles que, de alguma forma, colaboraram para a realiza\u00e7\u00e3o deste trabalho. \n\n \n\n \n\n\n\nRESUMO \n\n \n\nAs Redes Neurais Artificiais se constituem numa alternativa \u00e0 computa\u00e7\u00e3o \n\nprogramada tradicional e foram aplicadas em quase todos os ramos do conhecimento humano. \n\nEm Geotecnologia, no entanto, ainda s\u00e3o escassas as aplica\u00e7\u00f5es de maneira que, com este \n\ntrabalho, procura-se mostrar que elas tamb\u00e9m podem ser aplicadas em ind\u00fastrias de pisos e \n\nrevestimentos cer\u00e2micos do P\u00f3lo Cer\u00e2mico de Santa Gertrudes, Estado de S\u00e3o Paulo. Para \n\nisso, foram utilizados corpos-de-prova elaborados, testados e analisados nas ind\u00fastrias \n\nTriunfo Cer\u00e2mica e Rochaforte Cer\u00e2mica, com argilas oriundas de nove minas da regi\u00e3o que \n\nconstitui o P\u00f3lo Cer\u00e2mico de Santa Gertrudes, dentre aquelas que representavam toda a \n\ncoluna estratigr\u00e1fica da Forma\u00e7\u00e3o Corumbata\u00ed com amostras bem diferenciadas. Os dados \n\nobtidos relativos \u00e0s vari\u00e1veis f\u00edsicas foram gentilmente cedidos pelo propriet\u00e1rio das \n\nind\u00fastrias acima citadas e as vari\u00e1veis f\u00edsicas usadas neste estudo s\u00e3o a Densidade de \n\nPrensagem (DP), a Densidade Aparente de Corpos-de-Prova Secos (DAS), a Retra\u00e7\u00e3o Linear \n\nde Secagem (RLS), a Retra\u00e7\u00e3o Linear de Queima (RLQ), a Perda ao Fogo (PF), a Carga de \n\nRuptura (CR), a Absor\u00e7\u00e3o de \u00c1gua (Abs) e o M\u00f3dulo de Resist\u00eancia \u00e0 Flex\u00e3o (MRF). Para a \n\nan\u00e1lise, os corpos-de-prova foram submetidos a quatro temperaturas de queima 1000\u00b0C, \n\n1020\u00b0C, 1040\u00b0C e 1060\u00b0C, onde cada um destes valores deu origem a uma rede neural MLP \n\n(Multilayer Perceptron) de tr\u00eas camadas, para as quais foi usada a Regra do Aprendizado de \n\nRetropropaga\u00e7\u00e3o do Erro (Backpropagation, do original em ingl\u00eas). \n\n \n\nPalavras-chave: redes neurais artificiais, rede MLP, argila e vari\u00e1veis f\u00edsicas. \n\n \n\n \n\n \n\n\n\nABSTRACT \n\n \n\nArtificial Neural Networks constitute an alternative to traditional programmed \n\ncomputation and have been applied in almost all branches of human knowledge. However, \n\nthey are rarely applied in Geotechnology, so this work aims to show that they can be applied \n\nin the flooring and ceramic tile industries in the Principial Ceramic Region of Saint Gertrudes, \n\nS\u00e3o Paulo State. For this purpose, proof specimens elaborated, tested and analyzed in the \n\nindustries of Triunfo Cer\u00e2mica and Rochaforte Cer\u00e2mica were used. These proof specimens \n\nwere composed of well differentiated clays from nine mines in the Principial Ceramic Region \n\nof Saint Gertrudes, and these mines are representative of all the stratigraphic column of the \n\nCorumbata\u00ed Formation. The data relative to physical variables were graciously provided by \n\nthe owner of the above mentioned industries, and the physical variables used in this study are \n\nPressing Density (DP), Bulk Density of Dry Specimens (DAS), Linear Shrinkage Drying \n\n(RLS), Linear Shrinkage Firing (RLQ), Loss on Ignition (PF), Tensile Strength (CR), Water \n\nAbsorption (Abs) and Flexural Modulus of Resistance (MRF). For analysis, the proof \n\nspecimens were subjected to four firing temperatures, 1000\u00b0 C, 1020\u00b0 C, 1040\u00b0 C and \n\n1060\u00b0C. Each one of these values gave rise to a neural network MLP (Multilayer Perceptron) \n\nof three tiers for which the Backpropagation rule of learning was used. \n\n \n\nKey words: artificial neural networks, MLP network, clay and physical variables. \n\n\n\nLISTA DE FIGURAS \n\n \n\nFigura 1 - O Neur\u00f4nio Biol\u00f3gico Natural.................................................................................21 \n\nFigura 2 - Uma rede direta. ....................................................................................................... 23 \n\nFigura 3 - Um neur\u00f4nio artificial. ............................................................................................. 25 \n\nFigura 4 - A fun\u00e7\u00e3o log\u00edstica e a sua derivada. ........................................................................ 25 \n\nFigura 5 - A fun\u00e7\u00e3o tangente hiperb\u00f3lica e a sua derivada. ..................................................... 26 \n\nFigura 6 - O neur\u00f4nio de McCulloch/Pitts. .............................................................................. 28 \n\nFigura 7 - O Perceptron de Rosenblatt. .................................................................................... 29 \n\nFigura 8 - Uma rede direta de tr\u00eas camadas. ............................................................................ 34 \n\nFigura 9 - Fotos da mina Pieroni. ............................................................................................. 46 \n\nFigura 10 - Fotos da mina Santa Am\u00e1bile. ............................................................................... 47 \n\nFigura 11 - Resultado encontrado para os dados do ANEXO E, com N = 7 e 242 epochs. .... 59 \n\nFigura 12 - Resultado encontrado para os dados do ANEXO E, com N =34 e 164 epochs. ... 62 \n\nFigura 13 - Resultado encontrado para os dados do ANEXO F, com N = 27 e 88 epochs. ..... 66 \n\nFigura 14 - Resultado encontrado para os dados do ANEXO F, com N = 33 e 83 epochs. ..... 67 \n\nFigura 15 - Resultado encontrado para os dados do ANEXO F, com N = 90 e 60 epochs. ..... 68 \n\nFigura 16 - Resultado encontrado para os dados do ANEXO G, com N= 27 e 85 epochs. ..... 70 \n\nFigura 17 - Resultado encontrado para os dados do ANEXO G, com N = 33 e 82 epochs. .... 71 \n\nFigura 18 - Resultado encontrado para os dados do ANEXO G, com N = 34 e 73 epochs. .... 72 \n\nFigura 19 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 215 epochs. .. 74 \n\nFigura 20 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 94 epochs. .... 75 \n\nFigura 21 - Resultado encontrado para os dados do ANEXO H, com N =40 e 127 epochs. ... 76 \n\nFigura 22 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 126 epochs. .. 77 \n\nFigura 23 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 118 epochs. .. 78 \n\nFigura 24 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 78 epochs. .... 79 \n\nFigura 25 - Resultado encontrado para os dados do ANEXO I, com N = 56 e 64 epochs. ...... 84 \n\nFigura 26 - Resultado encontrado para os dados do ANEXO I, com N = 58 e 158 epochs. .... 85 \n\nFigura 27 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 263 epochs. .... 86 \n\nFigura 28 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 269 epochs. .... 87 \n\nFigura 29 - Resultado encontrado para os dados do ANEXO J, com N = 60 e 80 epochs. ..... 89 \n\nFigura 30 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 74 epochs. ..... 90 \n\nFigura 31 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 81 epochs. ..... 91 \n\nFigura 32 - Resultado encontrado para os dados do ANEXO J, com N = 63 e 66 epochs. ..... 92 \n\nFigura 33 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 75 epochs. ..... 93 \n\n\n\nFigura 34 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 167 epochs. ... 94 \n\nFigura 35 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 76 epochs. .... 96 \n\nFigura 36 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 92 epochs. .... 97 \n\nFigura 37 - Resultado encontrado para os dados do ANEXO K, com N = 65 e 65 epochs. .... 98 \n\nFigura 38 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 69 epochs. .... 99 \n\nFigura 39 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 97 epochs. .. 100 \n\nFigura 40 - Resultado encontrado para os dados do ANEXO K, com N = 70 e 81 epochs. .. 101 \n\nFigura 41 - Resultado encontrado para os dados do ANEXO K, com N = 71 e 60 epochs. .. 102 \n\n \n\n \n\n\n\nLISTA DE TABELAS \n\nTabela 1 - Classifica\u00e7\u00e3o das placas cer\u00e2micas quanto \u00e0 porosidade. ....................................... 40 \n\nTabela 2 - Especifica\u00e7\u00f5es da NBR 13818 (ABNT, 1997). ....................................................... 41 \n\nTabela 3 - Coordenadas UTM obtidas por GPS das minas estudadas (Datum: SAD 69). ....... 43 \n\nTabela 4 - M\u00e9dias dos ANEXOS A, B, C e D, m\u00e9dia geral e o padr\u00e3o de produto estabelecido \nneste trabalho. .......................................................................................................... 51 \n\nTabela 5 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H,com \nN = 7 e 242 epochs. ................................................................................................. 61 \n\nTabela 6 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas as dados dos ANEXOS E, F, G e H, com \nN = 34 e 164 epochs. ............................................................................................... 63 \n\nTabela 7 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com \nN = 35 e 122 epochs. ............................................................................................... 63 \n\nTabela 8 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO F. ...................................... 64 \n\nTabela 9 - Complementa\u00e7\u00e3o da TABELA 8. ........................................................................... 65 \n\nTabela 10 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 27 e 88 epochs. ....................................................................................... 66 \n\nTabela 11 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 33 e 83 epochs. ....................................................................................... 67 \n\nTabela 12 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 90 e 60 epochs. ....................................................................................... 68 \n\nTabela 13 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO G..................................... 69 \n\nTabela 14 - Complementa\u00e7\u00e3o dos dados da TABELA 13. ...................................................... 69 \n\nTabela 15 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 27 e 85 epochs. ....................................................................................... 71 \n\nTabela 16 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 33 e 82 epochs. ....................................................................................... 72 \n\nTabela 17 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 34 e 73 epochs. ....................................................................................... 73 \n\nTabela 18 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO H..................................... 73 \n\nTabela 19 - Complementa\u00e7\u00e3o dos dados da TABELA 18. ...................................................... 74 \n\nTabela 20 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 38 e 215 epochs. ..................................................................................... 75 \n\nTabela 21 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 38 e 94 epochs. ....................................................................................... 76 \n\nTabela 22 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 40 e 127 epochs. ..................................................................................... 77 \n\nTabela 23 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 40 e 126 epochs. ..................................................................................... 78 \n\n\n\nTabela 24 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 40 e 118 epochs. ..................................................................................... 79 \n\nTabela 25 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, \ncom N = 40 e 78 epochs. ....................................................................................... 80 \n\nTabela 26 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO I. ..................................... 82 \n\nTabela 27 - Complementa\u00e7\u00e3o dos dados da TABELA 26. ...................................................... 83 \n\nTabela 28 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 56 e 64 epochs. ................................................................................................... 84 \n\nTabela 29 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 58 e 158 epochs. ................................................................................................. 85 \n\nTabela 30 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 60 e 263 epochs. ................................................................................................. 86 \n\nTabela 31 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 60 e 269 epochs. ................................................................................................. 87 \n\nTabela 32 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO J. ..................................... 88 \n\nTabela 33 - Complementa\u00e7\u00e3o dos dados da TABELA 32. ...................................................... 88 \n\nTabela 34 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 60 e 80 epochs. ................................................................................................... 89 \n\nTabela 35 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 62 e 74 epochs. ................................................................................................... 90 \n\nTabela 36 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 62 e 81 epochs. ................................................................................................... 91 \n\nTabela 37 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 63 e 66 epochs. ................................................................................................... 92 \n\nTabela 38 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 65 e 75 epochs. ................................................................................................... 93 \n\nTabela 39 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 65 e 167 epochs. ................................................................................................. 94 \n\nTabela 40 - Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO K..................................... 95 \n\nTabela 41 - Complementa\u00e7\u00e3o dos dados da TABELA 40. ...................................................... 95 \n\nTabela 42 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 55 e 76 epochs. ................................................................................................... 97 \n\nTabela 43 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 55 e 92 epochs. ................................................................................................... 98 \n\nTabela 44 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 65 e 65 epochs. ................................................................................................... 99 \n\nTabela 45 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 67 e 69 epochs. ................................................................................................. 100 \n\nTabela 46 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 67 e 97 epochs. ................................................................................................. 101 \n\n\n\nTabela 47 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 70 e 81 epochs. ................................................................................................. 102 \n\nTabela 48 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N \n= 71 e 60 epochs. ................................................................................................. 103 \n\nTabela 49 - Bloco B1 da matriz IW, f\u00f3rmula (4.9). ............................................................... 106 \n\nTabela 50 - Bloco B2 da matriz IW, f\u00f3rmula (4.9). ............................................................... 107 \n\nTabela 51 - Bloco B3 da matriz IW, f\u00f3rmula (4.9). ............................................................... 108 \n\nTabela 52 - Bloco B4 da matriz IW, f\u00f3rmula (4.9). ............................................................... 109 \n\nTabela 53 - C\u00e1lculo dos desvios, D(%), para os valores da TABELA 46.............................. 110 \n\nTabela 54 - C\u00e1lculo dos desvios, D(%), para os valores da TABELA 47.............................. 111 \n\nTabela 55 - Bloco B1 da matriz IW , f\u00f3rmula (4.12). ............................................................ 112 \n\nTabela 56 - Bloco B2 da matriz IW, f\u00f3rmula (4.12). ............................................................. 113 \n\nTabela 57 - Bloco B3 da matriz IW, f\u00f3rmula (4.12). ............................................................. 114 \n\nTabela 58 - Bloco B4 da matriz IW, f\u00f3rmula (4.12). ............................................................. 115 \n\nTabela 59 - Bloco B5 da matriz IW, f\u00f3rmula (4.12). ............................................................. 116 \n\nTabela 60 - Bloco B6 da matriz IW, f\u00f3rmula (4.12). ............................................................. 117 \n\n \n\n\n\nLISTA DE ANEXOS \n\nAnexo A - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1000\u00b0C ........................... 133\u00a0\n\nAnexo B - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C ........................... 134\u00a0\n\nAnexo C - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C ........................... 135\u00a0\n\nAnexo D - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. .......................... 136\u00a0\n\nAnexo E - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1000\u00b0C. (Minas CF, CR, \nPG, PT e PI) .......................................................................................................... 137\u00a0\n\nAnexo F - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C. (Minas CF, CR, \nPG, PT e PI) .......................................................................................................... 138\u00a0\n\nAnexo G - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C. (Minas CF, CR, \nPG, PT e PI) .......................................................................................................... 139\u00a0\n\nAnexo H - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. (Minas CF, CR, \nPG, PT e PI) .......................................................................................................... 140\u00a0\n\nAnexo I - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C. (Minas CF, CR, PG, \nPT, PI e TU).......................................................................................................... 141\u00a0\n\nAnexo J - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C. (Minas CF, CR, PG, \nPT, PI e TU).......................................................................................................... 142\u00a0\n\nAnexo K - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. (Minas CF, CR, \nPG, PT, PI e TU) .................................................................................................. 143 \n\n \n\n \n\n \n\n\n\nLISTA DE ABREVIATURAS E SIGLAS \n\n \n\nABNT   Associa\u00e7\u00e3o Brasileira de Normas T\u00e9cnicas \n\nAbs   Absor\u00e7\u00e3o de \u00c1gua \n\nAPL   Arranjo Produtivo Local \n\nASPACER   Associa\u00e7\u00e3o Paulista das Cer\u00e2micas de Revestimento \n\nCF 1   Cristofoletti Mina 1 \n\nCF 2   Cristofoletti Mina 2 \n\nCR   Carga de Ruptura \n\nCR 1   Cruzeiro Frente 1 \n\nCR 2   Cruzeiro Frente 2 \n\nDAS   Densidade Aparente de Corpos de Prova Secos \n\nDP   Densidade de Prensagem \n\nGPS   Global Positioning System ou Sistema de Posicionamento Global \n\nMRF   M\u00f3dulo de Resist\u00eancia \u00e0 Flex\u00e3o \n\nPCSG  P\u00f3lo Cer\u00e2mico de Santa Gertrudes \n\nPF   Perda ao Fogo \n\nPG  Mina Paganoti \n\nPI   Mina Pieroni \n\nPT 1  Partezani Mina 1 \n\nPT 2   Partezani Mina 2 \n\nPT 3   Partezani Mina 3 \n\nRLQ   Retra\u00e7\u00e3o Linear de Queima \n\nRLS   Retra\u00e7\u00e3o Linear de Secagem \n\nTU   Mina Tute \n\nUTM   Sistema Universal de Coordenadas Transverso de Mercator \n\n    \n\n \n\n\n\nSUM\u00c1RIO \n\n1.\u00a0 INTRODU\u00c7\u00c3O ................................................................................................................ 16\u00a0\n\n1.1. Objetivo Geral e Hip\u00f3tese .......................................................................................... 17\u00a0\n\n1.2. Estrutura ..................................................................................................................... 18\u00a0\n\n2.\u00a0 REVIS\u00c3O BIBLIOGR\u00c1FICA ......................................................................................... 19\u00a0\n\n2.1. O Neur\u00f4nio Biol\u00f3gico Natural ................................................................................... 19\u00a0\n\n2.2. As Redes Neurais Artificiais ...................................................................................... 21\u00a0\n\n2.3. Breve Hist\u00f3rico sobre as Redes Neurais Artificiais ................................................... 27\u00a0\n\n2.4. Regra Delta e Regra Delta Generalizada ................................................................... 32\u00a0\n\n2.4.1. Regra Delta ....................................................................................................... 32\u00a0\n\n2.4.2. Regra Delta Generalizada ................................................................................. 33\u00a0\n\n3.\u00a0 MATERIAIS E M\u00c9TODOS ............................................................................................. 39\u00a0\n\n3.1. O P\u00f3lo Cer\u00e2mico de Santa Gertrudes ........................................................................ 39\u00a0\n\n3.2. Caracteriza\u00e7\u00e3o das Argilas e das Cer\u00e2micas .............................................................. 39\u00a0\n\n3.3. Os Corpos-de-Prova ................................................................................................... 43\u00a0\n\n3.4. Determina\u00e7\u00e3o das Vari\u00e1veis F\u00edsicas dos Corpos-de-Prova........................................ 48\u00a0\n\n3.5. Metodologia ............................................................................................................... 50\u00a0\n\n4.\u00a0 RESULTADOS E DISCUSS\u00d5ES .................................................................................... 55\u00a0\n\n4.1. Introdu\u00e7\u00e3o .................................................................................................................. 55\u00a0\n\n4.2. Regra de Aprendizado Backpropagation (Formato B\u00e1sico) ...................................... 55\u00a0\n\n4.3. Regra de Aprendizado Backpropagation com Momento (traingdm) ........................ 57\u00a0\n\n4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp) ................................... 58\u00a0\n\n4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm) .......... 103\u00a0\n\n4.6. Conclus\u00f5es ............................................................................................................... 104\u00a0\n\n5.\u00a0 CONSIDERA\u00c7\u00d5ES FINAIS ......................................................................................... 121\u00a0\n\nREFER\u00caNCIA BIBLIOGR\u00c1FICA........................................................................................123 \n\nBIBLIOGRAFIA COMPLEMENTAR..................................................................................127 \n\nANEXOS ................................................................................................................................ 132\u00a0\n\n \n \n\n \n\n\n\n16 \n \n\n1. INTRODU\u00c7\u00c3O \n \n\nPara uma explora\u00e7\u00e3o economicamente vi\u00e1vel e sustentada dos recursos minerais e sua \n\nconsequente industrializa\u00e7\u00e3o com uma minimiza\u00e7\u00e3o, na medida do poss\u00edvel, dos impactos \n\ncausados inevitavelmente ao meio ambiente, h\u00e1 atualmente a possibilidade de se aplicar \n\nalguns dos modernos conceitos da Matem\u00e1tica, da Estat\u00edstica, da Computa\u00e7\u00e3o. Neste trabalho, \n\nconsideram-se a minera\u00e7\u00e3o de argila e, em especial, a industrializa\u00e7\u00e3o correspondente de \n\npisos e revestimentos cer\u00e2micos da regi\u00e3o do Estado de S\u00e3o Paulo denominada de P\u00f3lo \n\nCer\u00e2mico de Santa Gertrudes (PCSG) formado pelos munic\u00edpios de Limeira, Cordeir\u00f3polis, \n\nRio Claro, Ipe\u00fana, Piracicaba, Araras e Santa Gertrudes. Este segmento da ind\u00fastria, \n\nconforme dados da Associa\u00e7\u00e3o Paulista das Cer\u00e2micas de Revestimento, ASPACER, \n\n(ASPACER, 2012), \u00e9 respons\u00e1vel por aproximadamente 15000 empregos diretos e 200000 \n\nempregos indiretos. Est\u00e1 inserido no Arranjo Produtivo Local (APL) de Pisos e \n\nRevestimentos Cer\u00e2micos de Santa Gertrudes, o qual de acordo com Poletto (2007), \u00e9 o maior \n\nprodutor da Am\u00e9rica Latina e o quarto maior do mundo. \u00c9 formado por ind\u00fastrias de pequeno \n\ne de m\u00e9dio porte, utiliza basicamente o processo de monoqueima r\u00e1pida, com produ\u00e7\u00e3o, \n\npreponderantemente, pela chamada Via Seca usando argilas de boa qualidade. \n\nA preocupa\u00e7\u00e3o com o meio ambiente tem-se intensificado sobremaneira nos \u00faltimos \n\nanos em vista das recentes pesquisas sobre a polui\u00e7\u00e3o das \u00e1guas e da atmosfera, a ocupa\u00e7\u00e3o \n\ndos espa\u00e7os nas terras e nos mares, o desmatamento das florestas remanescentes, a explora\u00e7\u00e3o \n\ndos recursos minerais dispon\u00edveis, recursos estes indispens\u00e1veis \u00e0 esp\u00e9cie humana. \n\nParalelamente, a ind\u00fastria de transforma\u00e7\u00e3o procura meios para aumentar a produ\u00e7\u00e3o \n\ndiminuindo os custos dos processos produtivos, aumentando os lucros e melhorando a \n\nqualidade do seu produto final. Atualmente, ela tamb\u00e9m busca por certifica\u00e7\u00f5es ambientais \n\ncomo uma forma de demonstrar que tamb\u00e9m est\u00e1 consciente dos impactos ambientais que ela \n\nmesma provoca ao meio ambiente, utilizando-se para isso de medidas mitigadoras (planos de \n\nrecupera\u00e7\u00e3o, de remedia\u00e7\u00e3o, de reabilita\u00e7\u00e3o), mesmo que, \u00e0s vezes, de forma incipiente, \n\napesar dos baixos custos, quando bem planejadas. Contudo, persistem ainda os conflitos entre \n\nas mineradoras e ind\u00fastrias e a comunidade onde elas est\u00e3o inseridas. Se, por um lado, as \n\nmineradoras e as ind\u00fastrias provocam a polui\u00e7\u00e3o ambiental, por outro lado promovem o \n\ndesenvolvimento econ\u00f4mico gerando empregos e renda, que s\u00e3o muito importantes para a \n\npopula\u00e7\u00e3o. O controle de qualidade do produto final, o controle de qualidade nos processos e \n\nnos mecanismos de industrializa\u00e7\u00e3o associados a uma diminui\u00e7\u00e3o do consumo de energia \n\n\n\n17 \n \n\npodem ser, quando bem planejados, instrumentos de promo\u00e7\u00e3o na busca pelas certifica\u00e7\u00f5es \n\nambientais. \n\nDentro deste contexto, pode-se admitir que com a escassez de aplica\u00e7\u00f5es do conceito \n\nestat\u00edstico-matem\u00e1tico das Redes Neurais Artificiais (RNAs) na ind\u00fastria cer\u00e2mica, \n\nvislumbrou-se um farto e interessante material para esta pesquisa, tendo em vista que com \n\neste conceito pode-se estabelecer com bastante rigor o valor das vari\u00e1veis, dentro dos seus \n\npr\u00f3prios limites de toler\u00e2ncia de especifica\u00e7\u00e3o, estabelecidos pela Associa\u00e7\u00e3o Brasileira de \n\nNormas T\u00e9cnicas (ABNT), as quais determinam as caracter\u00edsticas do produto final, no caso, \n\npiso e revestimento cer\u00e2mico. Ademais, dentro das sete ferramentas estat\u00edsticas para o \n\ncontrole de qualidade de um produto industrial introduzidas por Kaoru Ishikawa, podem ser \n\nestabelecidos o limite inferior de controle e o limite superior de controle com valores bem \n\npr\u00f3ximos um do outro, diminuindo sensivelmente a amplitude do intervalo. Para este \n\nprop\u00f3sito ser\u00e1 utilizado o software MATLAB\u00ae (MATrix LABoratory) 7.0, da \u201cMathWorks, \n\nInc.\u201d \u2013 User\u2019s Guide : Neural Networks Toolbox. \n\nOriginariamente, em fun\u00e7\u00e3o dos dados e do problema apresentados, delimitou-se a \n\naplica\u00e7\u00e3o entre o conceito das RNAs e o conjunto de t\u00e9cnicas de otimiza\u00e7\u00e3o conhecido como \n\nMetodologia de Superf\u00edcie de Resposta, introduzido por G. E. P. Box e K. B. Wilson no in\u00edcio \n\ndos anos cinquenta do s\u00e9culo passado, j\u00e1 que ambos se inserem, em alguns casos, no conceito \n\nde Aproxima\u00e7\u00e3o de Fun\u00e7\u00f5es da An\u00e1lise Num\u00e9rica. \n\nA escolha recaiu sobre as RNAs tendo em vista a facilidade de aplica\u00e7\u00e3o, o baixo \n\ncusto para a obten\u00e7\u00e3o e o rigor na determina\u00e7\u00e3o das especifica\u00e7\u00f5es do produto industrial, o \n\nqual \u00e9 o pr\u00f3prio padr\u00e3o de sa\u00edda da RNA. \n\n \n\n1.1.  Objetivo Geral e Hip\u00f3tese \n\n \n\nO objetivo geral deste trabalho \u00e9 o de encontrar uma rede neural artificial, dentre as \n\nv\u00e1rias arquiteturas e as diversas regras de aprendizado, aquela que melhor se adapta para \n\nmaximizar o aproveitamento das argilas na ind\u00fastria cer\u00e2mica do PCSG e melhorar a \n\nqualidade dos pisos e revestimentos cer\u00e2micos ali produzidos, mediante uma rigorosa \n\ncaracteriza\u00e7\u00e3o. \n\nEste objetivo geral \u00e9 consequ\u00eancia da seguinte hip\u00f3tese: \n\n\n\n18 \n \n\n\u201cMostrar que \u00e9 poss\u00edvel aplicar o conceito das redes neurais artificiais na ind\u00fastria de pisos e \n\nrevestimentos cer\u00e2micos\u201d. \n\n \n\n1.2.  Estrutura \n\n \n\nEste trabalho foi dividido em cinco cap\u00edtulos, onde, al\u00e9m deste introdut\u00f3rio, no \n\nCap\u00edtulo 2, ser\u00e1 desenvolvida uma revis\u00e3o bibliogr\u00e1fica sobre o conceito estat\u00edstico-\n\nmatem\u00e1tico das Redes Neurais Artificiais, dentro daquilo que se concebeu como Intelig\u00eancia \n\nArtificial. \n\nEm seguida, no Cap\u00edtulo 3, ser\u00e3o apresentados os materiais utilizados, a localiza\u00e7\u00e3o \n\ndas minas e a metodologia usada. \n\nNo Cap\u00edtulo 4, ser\u00e3o comentados e discutidos os resultados encontrados com a \n\naplica\u00e7\u00e3o da rede neural artificial MLP (Multilayer Perceptron), mostrando que, de fato, elas \n\npodem ser utilizadas, inclusive em futuro pr\u00f3ximo, para melhorar a qualidade dos pisos e \n\nrevestimentos cer\u00e2micos dentro das especifica\u00e7\u00f5es t\u00e9cnicas do produto j\u00e1 previamente \n\nestabelecidas. Finalizando este cap\u00edtulo, foi acrescentado um roteiro para os interessados em \n\naplicar as RNAs. \n\nNo Cap\u00edtulo 5, ser\u00e3o mostradas as conclus\u00f5es encontradas e feitas as considera\u00e7\u00f5es \n\nfinais com sugest\u00f5es para trabalhos futuros. \n\nPor fim, s\u00e3o indicados na Refer\u00eancia Bibliogr\u00e1fica os livros, os artigos e outras \n\npublica\u00e7\u00f5es e as publica\u00e7\u00f5es pesquisadas junto \u00e0 rede \u201cinternet\u201d, usados neste trabalho. Foi \n\ntamb\u00e9m acrescentada uma Bibliografia Complementar, importante para o entendimento das \n\nRNAs. \n\n\n\n19 \n \n\n2. REVIS\u00c3O BIBLIOGR\u00c1FICA \n\n2.1. O Neur\u00f4nio Biol\u00f3gico Natural \n\n \n\nO c\u00e9rebro humano \u00e9 constitu\u00eddo principalmente por dois tipos diferentes de c\u00e9lulas, as \n\nc\u00e9lulas glias e os neur\u00f4nios. As c\u00e9lulas glias, com um n\u00famero aproximado dez vezes maior do \n\nque o n\u00famero de neur\u00f4nios, s\u00e3o respons\u00e1veis pela sustenta\u00e7\u00e3o do c\u00e9rebro, enquanto os \n\nneur\u00f4nios s\u00e3o basicamente as unidades ou elementos de processamento dos sinais (ou pulsos) \n\ne est\u00edmulos que recebe e sobre eles est\u00e3o concentrados os principais estudos no sentido de se \n\nsimular ou de se modelar mediante um algoritmo o seu comportamento. O neur\u00f4nio \u00e9 \n\nconstitu\u00eddo por um corpo celular ou soma, pelo ax\u00f4nio e pelos dendritos. O c\u00e9rebro humano \n\natrav\u00e9s da rede de neur\u00f4nios e de suas interconex\u00f5es, onde ocorrem sinapses, \u00e9 respons\u00e1vel \n\npelo pensamento, emo\u00e7\u00e3o, cogni\u00e7\u00e3o. As sinapses ocorrem entre ax\u00f4nios de diferentes \n\nneur\u00f4nios, entre o ax\u00f4nio e o soma e, tamb\u00e9m, entre os dendritos de um mesmo neur\u00f4nio, \n\nsegundo Wasserman (1989). Ainda, de acordo com o mesmo autor, o c\u00e9rebro humano, que \n\npesa somente 2% da massa corporal, consome 20% de todo o oxig\u00eanio liberado no corpo \n\nhumano e somente de 20 a 30 W de pot\u00eancia para o seu saud\u00e1vel funcionamento. \n\n\u00c9 atrav\u00e9s das sinapses que um neur\u00f4nio se interconecta a at\u00e9 outros 10.000 neur\u00f4nios, \n\nnum total de aproximadamente 100 bilh\u00f5es de neur\u00f4nios dentro do nosso c\u00e9rebro. Eles n\u00e3o \n\ns\u00e3o todos id\u00eanticos, diferem quimicamente, estruturalmente ou funcionalmente e j\u00e1 foram \n\nencontrados mais de 250 tipos diferentes de neur\u00f4nios. A regi\u00e3o intersin\u00e1ptica \u00e9 \n\neletroquimicamente ativa, local onde j\u00e1 foram descobertas mais de 50 subst\u00e2ncias qu\u00edmicas \n\nque executam a r\u00e1pida intercomunica\u00e7\u00e3o, via corrente i\u00f4nica, entre um neur\u00f4nio pr\u00e9-sin\u00e1ptico \n\ne um p\u00f3s-sin\u00e1ptico, por isso essas subst\u00e2ncias s\u00e3o denominadas apropriadamente de \n\nneurotransmissores. \n\nA membrana do neur\u00f4nio \u00e9 de grande import\u00e2ncia, em fun\u00e7\u00e3o da intensa \n\nintercomunica\u00e7\u00e3o neuronal, porque \u00e9 atrav\u00e9s dela que os neurotransmissores atuam podendo \n\nser excitadores quando a despolarizam mediante a atua\u00e7\u00e3o do potencial el\u00e9trico de a\u00e7\u00e3o ou \n\nimpulso nervoso ou podendo ser inibidores quando a hiperpolarizam, visto que em repouso a \n\nmembrana encontra-se a -70 mV, segundo Eccles (1957). A complexidade do funcionamento \n\nde um \u00fanico neur\u00f4nio reside, principalmente, no fato de que a concentra\u00e7\u00e3o dos \n\nneurotransmissores depende cada um deles de uma s\u00e9rie de fatores que at\u00e9 o presente \n\nmomento n\u00e3o s\u00e3o bem conhecidos e por isso encontra-se, ainda, em fase de intensos estudos. \n\n\n\n20 \n \n\n\u00c9 importante observar que o mesmo neurotransmissor pode ser excitat\u00f3rio para uma sinapse e \n\ninibit\u00f3rio para outra, de acordo com Wasserman (1989). \n\nO neur\u00f4nio foi identificado pelo neurologista espanhol Ram\u00f3n y Cajal; as suas \n\nmanifesta\u00e7\u00f5es el\u00e9tricas foram observadas pela primeira vez por DuBois Reymond, com o \n\naux\u00edlio de galvan\u00f4metros (KOV\u00c1CS, 2002). Identificado pelo pesquisador E. D. Adrian, o \n\npotencial de a\u00e7\u00e3o consiste em um pacote de ondas e \u00e9 obtido pela despolariza\u00e7\u00e3o da \n\nmembrana do neur\u00f4nio com o aux\u00edlio da chamada bomba de s\u00f3dio/pot\u00e1ssio, a qual permite a \n\nentrada do s\u00f3dio e a consequente sa\u00edda do pot\u00e1ssio. Depois de emitido o impulso, ocorre a \n\nhiperpolariza\u00e7\u00e3o com os \u00edons fazendo o caminho inverso. Toda esta opera\u00e7\u00e3o ocorre no \n\nintervalo de tempo de 1 a 3 ms, com a superposi\u00e7\u00e3o de um per\u00edodo refrat\u00e1rio e um estado de \n\nrepouso ou relaxa\u00e7\u00e3o, o que impossibilita uma retomada pelo neur\u00f4nio de um novo impulso \n\ndurante esse per\u00edodo de tempo (WASSERMAN, 1989), como pode ser visto na Figura 1. O \n\nneur\u00f4nio natural apresenta muitas entradas (conex\u00f5es sin\u00e1pticas), mas uma s\u00f3 sa\u00edda (um \u00fanico \n\nimpulso nervoso), de forma que, dependendo dos sinais ou est\u00edmulos que recebe, um neur\u00f4nio \n\npode ou n\u00e3o emitir um pulso. A superposi\u00e7\u00e3o de todos esses sinais ou est\u00edmulos, excitat\u00f3rios \n\nou inibit\u00f3rios, \u00e9 o que constitui a atividade cerebral. \n\nOs neur\u00f4nios organizados em sistemas s\u00e3o adaptativos, visto que com mudan\u00e7as nas \n\ninterconex\u00f5es sin\u00e1pticas, eles aprendem e se auto-organizam fazendo emergir ordem da \n\ndesordem e, tamb\u00e9m, o que \u00e9 mais importante, estes sistemas apresentam propriedades \n\ncoletivas (mem\u00f3ria), mediante a competi\u00e7\u00e3o e coopera\u00e7\u00e3o entre os neur\u00f4nios constituintes, as \n\nquais diferem das propriedades individuais. \n\nO c\u00e9rebro humano \u00e9 um sistema din\u00e2mico complexo com uma grande quantidade de \n\nelementos organizados em sistemas que, por sua vez, tamb\u00e9m se organizam em outros \n\nsistemas cada vez mais complexos, de maneira similar \u00e0 pr\u00f3pria origem da vida e a evolu\u00e7\u00e3o \n\ndas esp\u00e9cies, conforme Dam\u00e1sio (1996). De acordo com Haykin (2001), o c\u00e9rebro humano \n\nprocessa informa\u00e7\u00f5es como se fosse um computador altamente complexo, n\u00e3o linear e \n\nparalelo. Em fun\u00e7\u00e3o dessa complexidade, atualmente h\u00e1 pelo menos dois grandes ramos da \n\nneuroci\u00eancia; o primeiro que estuda a estrutura e o funcionamento dos sistemas nervosos da \n\nmaneira como interessam aos bi\u00f3logos e aos profissionais de estudos correlacionados, \n\nenquanto que o segundo estuda o processamento computacional dos dados de maior interesse \n\npara os f\u00edsicos e matem\u00e1ticos no intuito de se constru\u00edrem computadores \u00fateis ou m\u00e1quinas \n\nmais efetivas, segundo Nussenzveig (1999). \n\n \n\n\n\n21 \n \n\nFigura 1 - O potencial de a\u00e7\u00e3o durante a aplica\u00e7\u00e3o de um est\u00edmulo nervoso. \n \n\n \n\nFonte: Dispon\u00edvel em:&lt;http://faculty.washington.edu.chudler/ap.html>. \nAcesso em: 03 mar. 2007. Tradu\u00e7\u00e3o nossa.  \n\n \n\n2.2. As Redes Neurais Artificiais \n\nAs redes neurais artificiais, constitu\u00eddas de neur\u00f4nios artificiais em analogia com os \n\nneur\u00f4nios naturais humanos, s\u00e3o modelos estat\u00edstico-matem\u00e1ticos que buscam simular alguns \n\nprocessos biol\u00f3gicos do Sistema Nervoso Central. Elas s\u00e3o modelos adapt\u00e1veis, da mesma \n\nmaneira que os neur\u00f4nios naturais, e \u00e9 dessa forma que devemos entend\u00ea-las, pois atrav\u00e9s da \n\nvaria\u00e7\u00e3o de alguns par\u00e2metros de controle elas conseguem aprender e, consequentemente, \n\nrealizar opera\u00e7\u00f5es de controle, de classifica\u00e7\u00e3o e de reconhecimento de padr\u00f5es previamente \n\nfixados. A simula\u00e7\u00e3o deve ser feita com o aux\u00edlio de algoritmos e de programas \n\ncomputacionais. Estes modelos surgiram na mesma \u00e9poca em que foram constru\u00eddos os \n\nprimeiros computadores, d\u00e9cada de 40 do s\u00e9culo passado, para se lidar com problemas \n\ncomplexos, especialmente, aqueles da teoria matem\u00e1tica dos sistemas din\u00e2micos, onde grande \n\n\n\n22 \n \n\nquantidade de dados deve ser modelada e analisada estatisticamente e computacionalmente, \n\nde acordo com Abdi et al. (1999), Haykin (2001), Kov\u00e1cs (2002) e Braga et al. (2007). As \n\nRNAs  constituem uma alternativa \u00e0 computa\u00e7\u00e3o programada tradicional conduzida pela \n\nelabora\u00e7\u00e3o de um algoritmo, por um conjunto de rotinas e por uma s\u00f3lida base l\u00f3gica, visto \n\nque s\u00e3o capazes de reagir e de se auto-organizar; elas aprendem, sua maior virtude, mas, \n\ninfelizmente, como qualquer ser humano, tamb\u00e9m esquecem. Elas s\u00e3o capazes de gerar as \n\nsuas pr\u00f3prias regras e \u00e9 importante ressaltar que no processo de adapta\u00e7\u00e3o elas podem gerar \n\nregras internas desconhecidas. S\u00e3o muito utilizadas, em fun\u00e7\u00e3o destas caracter\u00edsticas, como \n\nsimula\u00e7\u00e3o dos processos mentais na ci\u00eancia da cogni\u00e7\u00e3o, conforme Abdi et al.(1999), Haykin \n\n(2001) e Braga et al. (2007). Braga et al. (2007) ressaltam que as redes neurais s\u00e3o utilizadas \n\nem sistemas de processamento paralelo e distribu\u00eddo. \n\nA ideia original foi a de se aproximar o m\u00e1ximo poss\u00edvel do funcionamento dos \n\nsistemas de neur\u00f4nios humanos, mas apesar de todo o avan\u00e7o tecnol\u00f3gico que se seguiu e da \n\nsua r\u00e1pida transfer\u00eancia aos modernos computadores, esta ideia teve que ser deixada de lado, \n\npois paralelamente avan\u00e7ou-se tamb\u00e9m na compreens\u00e3o da fisiologia de um \u00fanico neur\u00f4nio \n\nbiol\u00f3gico, onde se percebeu que o seu funcionamento era demasiadamente complexo, o qual \n\nreunido a outros neur\u00f4nios em estruturas cada vez mais complexas dentro do Sistema Nervoso \n\nCentral ainda n\u00e3o permite que esta desejada aproxima\u00e7\u00e3o se concretize. No entanto, o estudo \n\ndas redes neurais artificiais revelou-se muito importante na resolu\u00e7\u00e3o de problemas de \n\ncontrole, pois mesmo quando a informa\u00e7\u00e3o \u00e9 parcial, a rede pode escolher um padr\u00e3o (ou \n\ninforma\u00e7\u00e3o de sa\u00edda) mais pr\u00f3ximo ao desejado, pois uma caracter\u00edstica muito importante das \n\nredes neurais \u00e9 a de que elas s\u00e3o tolerantes a falhas (NUSSENZVEIG, 1999). As redes \n\nneurais, atualmente, s\u00e3o aplicadas em in\u00fameros campos do conhecimento humano, onde se \n\ndestacam a filtragem de ru\u00eddos (em telefonia), a an\u00e1lise do eletrocardiograma e do \n\neletroencefalograma, a compress\u00e3o de imagens e a realiza\u00e7\u00e3o de efeitos especiais na ind\u00fastria \n\nde entretenimento, a simula\u00e7\u00e3o de v\u00f4o e dos sistemas de controle de avi\u00f5es na ind\u00fastria \n\naeroespacial, a avalia\u00e7\u00e3o da aplica\u00e7\u00e3o de cr\u00e9ditos no sistema banc\u00e1rio. Mas, conforme foi \n\nsalientado no cap\u00edtulo anterior, h\u00e1 poucas aplica\u00e7\u00f5es notadamente na ind\u00fastria cer\u00e2mica, \n\nobjeto deste estudo.  \n\nNa constitui\u00e7\u00e3o de uma rede neural artificial, as unidades ou elementos de \n\nprocessamento, denominados apropriadamente de neur\u00f4nios artificiais, s\u00e3o organizados \n\nbasicamente em camadas: uma camada de entrada, que recebe os dados ou padr\u00e3o de entrada, \n\numa camada de sa\u00edda, que fornece as respostas da rede ou padr\u00e3o de sa\u00edda, ambas interligadas \n\n\n\n23 \n \n\nao exterior e, entre elas, podem ser inclu\u00eddas uma ou mais camadas chamadas de \n\nintermedi\u00e1rias ou ocultas (internas). A cada elemento de processamento \u00e9 associado um peso \n\nsin\u00e1ptico, que \u00e9 o respons\u00e1vel pelas conex\u00f5es em uma analogia com a realidade das sinapses. \n\n\u00c9 atrav\u00e9s das mudan\u00e7as destes pesos sin\u00e1pticos de liga\u00e7\u00e3o entre as conex\u00f5es que uma rede \n\nneural aprende. Esta rede, pela sua arquitetura ou topologia, recebe o nome de rede direta ou \n\nfeedforward ou, ainda, de acordo com Haykin (2001), rede alimentada para frente (Figura 2). \n\nAl\u00e9m desta, h\u00e1 outras arquiteturas ou topologias.  \n\n \n\nFigura 2 - Uma rede direta. \n\n \nFonte: Abdi et al. (1999). \n \n           A primeira simplifica\u00e7\u00e3o introduzida no modelo do elemento de processamento foi a \n\ndetermina\u00e7\u00e3o de que cada neur\u00f4nio podia assumir somente dois estados: ativo, quando o \n\nax\u00f4nio emite um sinal ou inativo quando o ax\u00f4nio n\u00e3o emite sinal. \n\n           Uma rede neural artificial exibe duas fases de opera\u00e7\u00e3o: o aprendizado e a \n\nrecapitula\u00e7\u00e3o. No aprendizado, os pesos sin\u00e1pticos s\u00e3o adaptados ou modificados no sentido \n\nde se obter o resultado desejado. Quando a resposta (padr\u00e3o de sa\u00edda) \u00e9 conhecida, diz-se que \n\no aprendizado \u00e9 supervisionado e quando a resposta n\u00e3o \u00e9 conhecida, mas \u00e9 esperada uma \n\nsa\u00edda \u00f3tima para resolver o problema, diz-se que o aprendizado \u00e9 n\u00e3o supervisionado, porque \n\nneste caso n\u00e3o h\u00e1 a necessidade de um supervisor (ou um professor) que encaminhe a rede \n\npara o resultado esperado. Complementando, a rede pode ser heteroassociativa quando o vetor \n\n(padr\u00e3o) de sa\u00edda for diferente do vetor (padr\u00e3o) de entrada e autoassociativa quando eles \n\nforem iguais. Na fase de recapitula\u00e7\u00e3o (ou opera\u00e7\u00e3o propriamente dita), tem-se que o \n\n\n\n24 \n \n\nresultado \u00e9 obtido em resposta a um dado exemplo, sem que ocorra qualquer modifica\u00e7\u00e3o nos \n\npesos das conex\u00f5es, de acordo com Campanha (1994). \n\nEm resumo, pode-se dizer que as caracter\u00edsticas fundamentais de uma rede neural \n\nartificial s\u00e3o a sua arquitetura ou topologia (a forma como as unidades de processamento \n\nest\u00e3o interligadas), as suas pr\u00f3prias unidades de processamento e as suas regras de \n\naprendizado. \n\nQuanto \u00e0 arquitetura, dentre os v\u00e1rios tipos de redes, destacam-se as redes diretas ou \n\nfeedforward, do original em ingl\u00eas, ou rede alimentada adiante com camada \u00fanica ou com \n\nm\u00faltiplas camadas, conforme denomina\u00e7\u00e3o de Haykin (2001), onde os elementos de \n\nprocessamento aparecem interconectados em camadas sequencialmente colocadas da entrada \n\nat\u00e9 a sa\u00edda, passando pelas camadas ocultas ou intermedi\u00e1rias (ou ainda, hidden) - que ser\u00e1 \n\nutilizada neste trabalho -, e as redes recorrentes, em especial a rede de Hopfield, a qual \u00e9 uma \n\nrede sim\u00e9trica (a matriz dos pesos sin\u00e1pticos \u00e9 sim\u00e9trica) com ciclos ou com realimenta\u00e7\u00e3o \n\n(ou retroalimenta\u00e7\u00e3o), como quando um neur\u00f4nio se auto realimenta da mesma forma que um \n\nneur\u00f4nio biol\u00f3gico natural. \n\nBasicamente, uma unidade de processamento \u00e9 constitu\u00edda de v\u00e1rios sinais de entrada \n\n(x 1 ,...,x n ), cada qual com o seu peso sin\u00e1ptico (conex\u00e3o) associado (w 1 ,...,w n ). A soma \n\nponderada \n\n? ix . w i              (2.1) \n\n\u00e9 a entrada efetiva que dar\u00e1 origem a uma s\u00f3 sa\u00edda. Se a soma ponderada (2.1) for superior a \n\num limiar (threshold) previamente estabelecido, quando necess\u00e1rio, ent\u00e3o o sinal de sa\u00edda \u00e9 \n\ngerado mediante a aplica\u00e7\u00e3o de uma fun\u00e7\u00e3o de transfer\u00eancia (ativa\u00e7\u00e3o) ou fun\u00e7\u00e3o de sa\u00edda do \n\nneur\u00f4nio, a qual deve ser uma fun\u00e7\u00e3o que mais se aproxima do efeito de ativa\u00e7\u00e3o do potencial \n\nde a\u00e7\u00e3o do neur\u00f4nio natural. Pode ser estabelecida por qualquer fun\u00e7\u00e3o cont\u00ednua, \n\nmonotonicamente crescente, com imagens reais dentro dos intervalos [0,1] ou [-1,1]. Caso \n\ncontr\u00e1rio, se a soma (2.1) for inferior ao limiar, nada acontece, ou seja, n\u00e3o h\u00e1 sinal de sa\u00edda \n\n(Figura 3). \n\nAs principais fun\u00e7\u00f5es de transfer\u00eancia s\u00e3o: a fun\u00e7\u00e3o linear \n\nf(x) = kx,            (2.2)  \n\na fun\u00e7\u00e3o log\u00edstica (Figura 4) \n\n\n\n25 \n \n\nf(x) = ( 1 + e kx? ) 1?            (2.3)  \n\n \n\nFigura 3 - Um neur\u00f4nio artificial. \n\n \n\nFonte: Abdi et al. (1999). \n\n \nFigura 4 \u2013 A fun\u00e7\u00e3o log\u00edstica e a sua derivada. \n \n\n-10 -8 -6 -4 -2 0 2 4 6 8 10\n-0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFun\u00e7\u00e3o log\u00edstica\n\n-6 -4 -2 0 2 4 6\n-0.05\n\n0\n\n0.05\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\nDerivada da fun\u00e7\u00e3o log\u00edstica\n\nFonte: Ferro (2007). \n\ne a fun\u00e7\u00e3o tangente hiperb\u00f3lica (Figura 5), \n\nf(x) = tanh(kx) .                               (2.4) \n\n \n\n \n\n \n\n \n\n\n\n26 \n \n\nFigura 5 \u2013 A fun\u00e7\u00e3o tangente hiperb\u00f3lica e a sua derivada. \n \n\n-10 -8 -6 -4 -2 0 2 4 6 8 10\n\n-1\n\n-0.8\n\n-0.6\n\n-0.4\n\n-0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFun\u00e7\u00e3o tangente hiperb\u00f3lica\n\n-6 -4 -2 0 2 4 6\n-0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nDerivada da fun\u00e7\u00e3o tangente hiperb\u00f3lica\n\n \n\nFonte: Ferro (2007). \n\n \n\nEm todas estas fun\u00e7\u00f5es, k \u00e9 uma constante (em especial, o MATLAB usa k = 1). As \n\nderivadas das duas \u00faltimas fun\u00e7\u00f5es apresentam caracter\u00edsticas sigmoidais. \n\nDe acordo com Abdi et al. (1999), o processamento do conhecimento \u00e9 conseguido \n\natrav\u00e9s das mudan\u00e7as nos pesos das conex\u00f5es (sin\u00e1pticos) entre os elementos de \n\nprocessamento. Em termos estat\u00edsticos, este processo \u00e9 equivalente \u00e0 interpreta\u00e7\u00e3o de que os \n\nvalores dos pesos das conex\u00f5es entre os elementos de processamento s\u00e3o par\u00e2metros \n\nestimadores. O processo de aprendizado especifica o algoritmo usado para estimar esses \n\npar\u00e2metros. O aprendizado, ent\u00e3o, est\u00e1 ligado a uma regra que modifica ou adapta os pesos \n\ndas conex\u00f5es.  \n\nEm 1949, o neuropsic\u00f3logo Donald Hebb prop\u00f4s a regra, chamada de Regra do \n\nAprendizado de Hebb, a qual estabelece que \u201cquando um ax\u00f4nio da c\u00e9lula A est\u00e1 pr\u00f3ximo o \n\nsuficiente para excitar uma c\u00e9lula B e repetidamente ou persistentemente a estimula, algum \n\nprocesso de crescimento ou mudan\u00e7a metab\u00f3lica ocorre em uma ou em ambas as c\u00e9lulas de \n\ntal forma que a efici\u00eancia de A, como uma das c\u00e9lulas que estimula B, aumenta\u201d, conforme \n\nAbdi et al. (1999), tradu\u00e7\u00e3o nossa. Na d\u00e9cada de 60, do s\u00e9culo passado, B. Widrow e M. Hoff \n\ndesenvolveram a Regra do Aprendizado Delta, que \u00e9 baseada no m\u00e9todo dos m\u00ednimos \n\nquadrados (WIDROW &amp; HOFF, 1960), dentro daquilo que \u00e9 chamado em matem\u00e1tica de \n\najustamento de curvas ou aproxima\u00e7\u00e3o polinomial de fun\u00e7\u00f5es; no caso particular em que o \n\npolin\u00f4mio de aproxima\u00e7\u00e3o \u00e9 do primeiro grau, o m\u00e9todo recebe o nome bem conhecido de \n\n\n\n27 \n \n\nregress\u00e3o linear. Neste m\u00e9todo, o m\u00ednimo da fun\u00e7\u00e3o do erro acontece no v\u00e9rtice da sua \n\nsuperf\u00edcie de resposta, visto que o erro de aproxima\u00e7\u00e3o \u00e9 quadr\u00e1tico com curva ou superf\u00edcie \n\nde m\u00ednimo. A Regra de Aprendizado Delta foi generalizada por Rumelhart e outros \n\npesquisadores, ainda na d\u00e9cada de 80 do s\u00e9culo passado, com o nome de Regra Delta \n\nGeneralizada ou Regra de Retropropaga\u00e7\u00e3o do Erro (Backpropagation), tornando-se a mais \n\npoderosa regra de aprendizado das redes neurais artificiais. Esta regra foi desenvolvida \n\noriginalmente por P. Werbos, (1974), e redescoberta, independentemente, por  D. Parker,  \n\n(1982), Y. LeCun, (1985), e por Rumelhart et al., (1986), de acordo com os seus pr\u00f3prios \n\nidealizadores. Ela usa o m\u00e9todo do gradiente descendente para a corre\u00e7\u00e3o do erro. Como o \n\ngradiente \u00e9 um vetor cujo sentido aponta sempre na dire\u00e7\u00e3o do crescimento m\u00e1ximo da \n\nfun\u00e7\u00e3o sobre a qual ele \u00e9 aplicado, quando o seu sentido \u00e9 invertido, ele passa a apontar na \n\ndire\u00e7\u00e3o do decrescimento m\u00e1ximo da fun\u00e7\u00e3o. Assim, o m\u00ednimo da fun\u00e7\u00e3o de erro, neste caso, \n\ndepende do gradiente com sinal negativo. A Regra Delta e a Regra Delta Generalizada ser\u00e3o \n\napresentadas ainda neste cap\u00edtulo. Acrescentam-se a estas regras, dentre outras, a Regra de \n\nAprendizagem de Kohonen, (KOHONEN, 1982), a Regra de Aprendizagem por Coopera\u00e7\u00e3o-\n\nCompeti\u00e7\u00e3o de von der Malsburg e Grossberg, segundo seus pr\u00f3prios criadores von der \n\nMalsburg (1973) e Grossberg (1976, 1980). \n\n \n\n2.3. Breve Hist\u00f3rico sobre as Redes Neurais Artificiais \n\nA seguir destacam-se os fatos hist\u00f3ricos relevantes para o objetivo geral deste trabalho \n\ne para isto foram adotados os textos de Campanha (1994), Abdi et al. (1999), Azevedo et al. \n\n(2000), Haykin (2001) e Kov\u00e1cs (2002). \n\nEm 1943, o neurologista Warren S. McCulloch junto com o seu aluno Walter Pitts, \n\nestat\u00edstico, publicaram o artigo \u201cA logical calculus of the ideas immanent in nervous activity\u201d \n\nno Bulletin of Mathematical Biophysics, considerado o marco zero no desenvolvimento das \n\nredes neurais artificiais. Acredita-se que este artigo tenha influenciado cientistas como John \n\nvon Neumann que se voltou para a constru\u00e7\u00e3o de c\u00e9rebros eletr\u00f4nicos ou computadores, \n\nMarvin Minsky que se dedicou a aplica\u00e7\u00e3o da intelig\u00eancia artificial em sistemas aut\u00f4nomos, \n\nNorbert Wiener que se preocupou com aquilo que veio a denominar-se cibern\u00e9tica e, \n\nprincipalmente, Frank Rosenblatt que, preocupado com o aspecto computacional da vis\u00e3o, \n\ncriou o \u201cPerceptron\u201d, em 1957. Alguns historiadores acreditam que McCulloch e Pitts foram, \n\n\n\n28 \n \n\npor seu turno, tamb\u00e9m influenciados pelas pesquisas de Alan Turing e do pr\u00f3prio John von \n\nNeumann. \n\nO neur\u00f4nio de McCulloch/Pitts \u00e9 do tipo l\u00f3gico, que admite somente dois estados: \n\ndispara ou n\u00e3o dispara o pulso ou sinal. As suas entradas pertencem ao conjunto formado \n\npelos n\u00fameros 0 e 1, possui um limiar constante igual a 1 e as suas sinapses tanto excitat\u00f3rias \n\nquanto inibit\u00f3rias possuem valores id\u00eanticos. Neste caso, as sinapses s\u00e3o excitat\u00f3rias quando \n\no peso da conex\u00e3o \u00e9 positivo e s\u00e3o inibit\u00f3rias quando o peso da conex\u00e3o \u00e9 negativo. Se a \n\nsoma ponderada for superior ao limiar o neur\u00f4nio dispara o sinal e em, caso contr\u00e1rio, n\u00e3o \n\ndispara o sinal, conforme estabelecido pelos pr\u00f3prios autores McCulloch &amp; Pitts (1943), \n\n(Figura 6). \n\n \n\nFigura 6 \u2013 O neur\u00f4nio de McCulloch/Pitts. \n\n \n\nFonte: McCulloch/Pitts (1943). \n\n \n\nFrank Rosenblatt, na Universidade de Cornell, construiu o Perceptron, Figura 7, \n\nequipamento que se constitu\u00eda em uma rede neural de uma s\u00f3 camada e era um classificador \n\nde padr\u00f5es capaz de identificar formas geom\u00e9tricas. Tamb\u00e9m foi constru\u00eddo com neur\u00f4nios do \n\ntipo l\u00f3gico, entretanto j\u00e1 apresentava conex\u00f5es modific\u00e1veis, mas a sa\u00edda fornecia os mesmos \n\nresultados 0 e 1. Usava a regra Hebbiana para o aprendizado. Os neur\u00f4nios do Perceptron \n\neram discriminadores (ou separadores) lineares, tendo em vista que a soma ponderada: \n\n? iw . x i  = ? ,                      (2.5) \n\n\n\n29 \n \n\nonde ?  \u00e9 o limiar, comportava-se no caso em que n = 2, onde n \u00e9 o n\u00famero de entradas, \n\ncomo a equa\u00e7\u00e3o de uma reta no plano formado pelos eixos x 1  e x 2 . Assim, se  \n\nw 1 .x 1  + w 2 .x 2  ?  ?            (2.6) \n\n\u00e9 obtida uma parte do plano (acima da reta) e se \n\nw 1 .x 1  + w 2 .x 2  &lt;?           (2.7) \n\n\u00e9 encontrada a outra parte do plano (abaixo da reta). Quando n = 3 o elemento separador \n\nw 1 x 1 + w 2 x 2 + w 3 x 3  = ?  passa a ser um plano no espa\u00e7o gerado pelas coordenadas x 1 , x 2 e \n\nx 3 . Quando n \u00e9 superior a 3, tem-se aquilo que os matem\u00e1ticos chamam de hiperplanos \n\n(ROSENBLATT,1958). \n\n \n\nFigura 7 \u2013 O Perceptron de Rosenblatt. \n\n \n\nFonte: Rosenblatt (1958). \n\nNa Universidade de Stanford, em 1959, Bernard Widrow desenvolveu filtros que \n\neliminaram os ru\u00eddos nas linhas telef\u00f4nicas e se confirmou como a primeira aplica\u00e7\u00e3o das \n\nredes neurais artificiais ao mundo real. Estudou tamb\u00e9m a aplica\u00e7\u00e3o das redes neurais no \n\nreconhecimento da fala e de objetos, na previs\u00e3o do tempo, no ajuste de antenas parab\u00f3licas, \n\nna regula\u00e7\u00e3o da press\u00e3o sangu\u00ednea. Criou o ADALINE (ADAptative LINear Element), onde as \n\nunidades de processamento s\u00e3o adaptadores lineares e tamb\u00e9m o MADALINE (Multiple \n\nAdaline) com v\u00e1rias camadas ADALINE. Com Marcian Hoff desenvolveu a Regra de \n\n\n\n30 \n \n\nAprendizado Delta (ou de Widrow-Hoff), aquela que estabeleceu que \u201cquando voc\u00ea comete \n\num erro, preste menos aten\u00e7\u00e3o \u00e0 c\u00e9lula de entrada que levou voc\u00ea a cometer este erro e preste \n\nmais aten\u00e7\u00e3o \u00e0 c\u00e9lula de entrada que n\u00e3o levou voc\u00ea a cometer este mesmo erro\u201d. Esta regra \n\ncausou uma grande euforia junto a outros pesquisadores crentes na imensa potencialidade de \n\naplica\u00e7\u00f5es das redes neurais, acreditando, naqueles tempos, que havia sido descoberta uma \n\nchave da intelig\u00eancia humana (WIDROW e HOFF, 1960). \n\nEntretanto, M. Minsky e S. Papert provaram, posteriormente, que o Perceptron n\u00e3o \n\nseria capaz de discriminar a opera\u00e7\u00e3o l\u00f3gica do tipo \u201cou exclusivo (XOR)\u201d e o seu \n\ncomplemento e, significava que, para os conjuntos \u201cmais interessantes\u201d (linearmente n\u00e3o \n\nsepar\u00e1veis) ele n\u00e3o conseguiria exibir nenhum conjunto de pesos sin\u00e1pticos (w i ) para efetuar \n\na classifica\u00e7\u00e3o de padr\u00f5es (MINSKY e PAPERT, 1969). Isto provocou uma estagna\u00e7\u00e3o nos \n\nestudos sobre as redes neurais, principalmente, pela indisponibilidade de verbas para as \n\npesquisas por quase duas d\u00e9cadas, de acordo com as refer\u00eancias indicadas no in\u00edcio deste \n\nitem. \n\nMesmo assim, isto n\u00e3o impediu que Teuvo Kohonen, Stephen Grossberg, James \n\nAnderson, Igor Aleksander (redes sem pesos), Kunihiko Fukushima (cognitron e \n\nneocognitron), notadamente, continuassem os seus trabalhos de pesquisa dentro da \n\ncomputa\u00e7\u00e3o neural. De acordo com Wasserman (1989), o choque provocado pelo livro \n\n\u201cPerceptrons\u201d de Minsky e Papert permitiu que houvesse um per\u00edodo de lat\u00eancia de modo a se \n\natingir a necess\u00e1ria maturidade neste campo de estudos. Hoje as redes neurais artificiais \n\nrotineiramente resolvem alguns dos problemas que foram colocados no livro de Minsky e \n\nPapert. As redes diretas exigem apenas camadas intermedi\u00e1rias para solucionar os problemas \n\nn\u00e3o linearmente separ\u00e1veis. Provou-se, mais tarde, que bastava uma s\u00f3 camada, no entanto, a \n\nsolu\u00e7\u00e3o ficaria ainda na depend\u00eancia da quantidade de neur\u00f4nios desta camada oculta. \n\nDestaca-se que Kohonen, da Universidade de Helsinque, nos anos 70 do s\u00e9culo \n\npassado, desenvolveu a Regra do Aprendizado Competitivo e a rede neural correspondente, \n\nna qual as unidades de processamento competem entre si. Tamb\u00e9m desenvolveu alguns \n\nalgoritmos adaptativos locais (mapa auto-organiz\u00e1vel). James Anderson, na Universidade de \n\nBrown, desenvolveu um Modelo Associativo Linear similar aos modelos biol\u00f3gicos de \n\nmem\u00f3ria e reconhecimento. Por seu turno, Stephen Grossberg utilizando-se da cl\u00e1ssica \n\nexperi\u00eancia de Pavlov sobre reflexos condicionados, construiu modelos em computa\u00e7\u00e3o \n\nneural. Desenvolveu, tamb\u00e9m, a partir dos anos 60, a Teoria da Resson\u00e2ncia Adaptativa \n\n(ART), para classificar padr\u00f5es, com os modelos ART1 (padr\u00f5es bin\u00e1rios), ART2 (padr\u00f5es \n\n\n\n31 \n \n\nanal\u00f3gicos) e, junto com Gail Carpenter, o ART3, redes diretas espa\u00e7o temporais, esta em \n\n1978.  \n\nCom a publica\u00e7\u00e3o do artigo \u201cNeural networks and physical systems with emergent \n\ncollective computational abilities\u201d na renomada Proceedings of the National Academy of \n\nSciences por J. J. Hopfield, da Caltech, em 1982, promoveu-se o retorno \u00e0 respeitabilidade \n\ndos estudos sobre as redes neurais artificiais. Hopfield desenvolveu uma Rede Associativa \n\ncom neur\u00f4nios do tipo l\u00f3gico, introduzindo o conceito de fun\u00e7\u00e3o de energia (ou fun\u00e7\u00e3o de \n\nLyapunov) para analisar a evolu\u00e7\u00e3o da rede. O mecanismo de resposta da rede \u00e9 uma \n\nparticulariza\u00e7\u00e3o dos trabalhos de Grossberg, no entanto, ele concebeu uma not\u00e1vel s\u00edntese de \n\nideias com um adequado tratamento matem\u00e1tico, conforme destaca Wasserman (1989). \n\nComplementando, dentre outros precursores, pode-se destacar o neuroanatomista \n\nBrian G. Gragg e o f\u00edsico H. N. V. Temperley que, ainda em 1954, estabeleceram uma \n\nanalogia entre as Redes de Neur\u00f4nios e as Redes de \u00c1tomos com Spins. Analogia que foi \n\naproveitada por William A. Little, em 1974, que associou as redes neurais e os sistemas de \n\nspins (Ising). Em 1976, D. Marr e T. Poggio estudando como o c\u00e9rebro humano constru\u00eda a \n\nno\u00e7\u00e3o de profundidade, que permite a vis\u00e3o tridimensional, desenvolveram um algoritmo \n\ncooperativo semelhante ao modelo de Hopfield . \n\nEm 1986, com a publica\u00e7\u00e3o do cl\u00e1ssico \u201cParallel Distributed Processing\u201d por D. \n\nRumelhart e J. McClelland consolidou-se definitivamente o avan\u00e7o nos estudos sobre as redes \n\nneurais com a cria\u00e7\u00e3o de associa\u00e7\u00f5es de pesquisadores e estudiosos, com a funda\u00e7\u00e3o de \n\nempresas de software para a sua explora\u00e7\u00e3o comercial e com uma prolifera\u00e7\u00e3o de publica\u00e7\u00f5es \n\ne de peri\u00f3dicos. Com a introdu\u00e7\u00e3o do algoritmo de Retropropaga\u00e7\u00e3o do Erro \n\n(Backpropagation) houve um aumento substancial nas aplica\u00e7\u00f5es das redes neurais. \n\nAtualmente, h\u00e1 diversos grupos utilizando-as em Neurologia, Psicologia, Medicina, Ci\u00eancia \n\nda Computa\u00e7\u00e3o e em outros ramos do conhecimento humano, o que demonstra a grande \n\ncapacidade que elas possuem para a resolu\u00e7\u00e3o de problemas de grande complexidade com \n\nresultados impressionantes. Para finalizar, em 1987, os pesquisadores Sejnowsky e Rosenberg \n\ndesenvolveram redes para a convers\u00e3o de texto e a correspondente representa\u00e7\u00e3o fon\u00e9tica. \n\nBurr desenvolveu redes para o reconhecimento de caracteres escritos manualmente enquanto \n\nCottrell, Munro e Zipser desenvolveram redes para a compress\u00e3o de imagens. Acrescentam-se \n\nas redes de Fun\u00e7\u00f5es de Base Radial, desenvolvida por M. J. D. Powell, em 1985, Renals, em \n\n1989, Moody e Darken, em 1990, Poggio e Girosi, tamb\u00e9m em 1990 e Park e Sandberg, em \n\n\n\n32 \n \n\n1991, que s\u00e3o utilizadas para a classifica\u00e7\u00e3o e para a aproxima\u00e7\u00e3o de fun\u00e7\u00f5es introduzidas \n\npela fun\u00e7\u00e3o de ativa\u00e7\u00e3o gaussiana nos neur\u00f4nios das camadas intermedi\u00e1rias. \n\n \n\n2.4. Regra Delta e Regra Delta Generalizada \n\n2.4.1 Regra Delta \n\n \nBasicamente as regras de aprendizado aqui analisadas consistem em um processo \n\niterativo de ajuste dos pesos das conex\u00f5es da rede. Na forma matricial, se a matriz dos pesos \n\ndas conex\u00f5es (pesos sin\u00e1pticos) for W, ent\u00e3o o processo iterativo \u00e9 desenvolvido pela \n\nf\u00f3rmula: \n\nW(n + 1) = W(n) + ? W(n),             (2.8) \n\nonde W(n + 1) representa a matriz dos pesos no instante (ou passo) n + 1, W(n) a matriz no \n\ninstante n e ? W(n) representa a matriz de ajustes aplicado aos pesos das conex\u00f5es. Para cada \n\nmaneira diferente de se calcular ? W(n) h\u00e1 um algoritmo de aprendizado diferente. \n\nA regra delta elaborada por B. Widrow e M. Hoff (WIDROW &amp; HOFF, 1960), com \n\nbase na regra de Hebb, \u00e9 um processo de minimiza\u00e7\u00e3o do erro calculado entre a sa\u00edda \n\ndesejada (alvo) e a sa\u00edda obtida atrav\u00e9s da aproxima\u00e7\u00e3o pelo m\u00e9todo dos m\u00ednimos quadrados. \n\n\u00c9 uma regra supervisionada, segundo Abdi et al. (1999), Haykin (2001) e Braga et al. (2007). \n\nA regra b\u00e1sica (F\u00d3RMULA 2.8) na forma matricial pode ser decomposta para cada peso, \n\nconforme a equa\u00e7\u00e3o a seguir  \n\n)()()1( nwnwnw ijijij ???? ,        (2.9) \n\nonde i representa a c\u00e9lula de entrada e j a c\u00e9lula de sa\u00edda. Considerando jt  o valor desejado, \n\njt?  o valor encontrado pela rede no instante n , o erro de aproxima\u00e7\u00e3o \u00e9  \n\njjj tte \u02c6?? .                  (2.10) \n\nAssim, \n\n)(..)()1( nxenwnw ijijij ????                               (2.11) \n\n\n\n33 \n \n\nonde ?  \u00e9 uma constante positiva denominada de taxa de aprendizado, ??  [0,1], e )(nxi  \u00e9 o \n\nvalor de ativa\u00e7\u00e3o (entrada) i do neur\u00f4nio. Ainda, de acordo com Braga et al., 2007, a Equa\u00e7\u00e3o \n\n(2.11) aparece tamb\u00e9m no algoritmo de treinamento do Perceptron, no algoritmo de \n\ntreinamento das redes ADALINE, de B. Widrow, e na generaliza\u00e7\u00e3o do algoritmo de \n\nRetropropaga\u00e7\u00e3o do Erro (Backpropagation). \n\nA minimiza\u00e7\u00e3o do erro deve ocorrer quando se aplicam as derivadas \n\nparciais na soma dos erros quadr\u00e1ticos das sa\u00eddas \n\n ?\n?\n\n?\np\n\nj\njee\n\n1\n\n22\n\n2\n\n1\n,                     (2.12) \n\nonde p \u00e9 o n\u00famero de exemplos de treinamento da rede, o n\u00famero \u00bd \u00e9 um fator de controle e \n\nn\u00e3o afeta o resultado, de acordo com o ajustamento pelo M\u00e9todo dos M\u00ednimos Quadrados. No \n\ncaso do aprendizado Hebbiano, tem-se:  \n\n)().(. nxnyw ijij ??? ,                    (2.13) \n\nonde )(ny j  \u00e9 a sa\u00edda j do neur\u00f4nio e )(nxi  \u00e9 a entrada i do neur\u00f4nio. Especificamente para o \n\nPerceptron, os autores Braga et al.  (2007), apresentam este algoritmo de aprendizado com o \n\nauxilio do que chamam de Portas de Limiar (Threshold gates) Linear, Quadr\u00e1tica e \n\nPolinomial e, novamente, neste caso, fica bem evidente a similaridade entre os conceitos de \n\nPortas de Limiar e a An\u00e1lise de Regress\u00e3o, com f\u00f3rmulas semelhantes e com a minimiza\u00e7\u00e3o \n\nefetuada com a f\u00f3rmula da soma quadr\u00e1tica dos erros. Ressaltam ainda que o ADALINE \u00e9 \n\ntamb\u00e9m um aproximador linear de fun\u00e7\u00f5es. \n\n \n\n2.4.2 Regra Delta Generalizada \n\n \n\nA Regra Delta Generalizada ou Regra de Retropropaga\u00e7\u00e3o do Erro ou, ainda, Regra \n\nBackpropagation, \u00e9 de aprendizado supervisionado e pode ser aplicada \u00e0 qualquer rede direta \n\nmulticamadas com unidades de processamento n\u00e3o lineares. A diferen\u00e7a entre uma unidade \n\nlinear e outra n\u00e3o linear \u00e9 a fun\u00e7\u00e3o de transfer\u00eancia ou de ativa\u00e7\u00e3o, que no segundo caso \u00e9 n\u00e3o \n\nlinear. \n\n\n\n34 \n \n\nO desenvolvimento do algoritmo ser\u00e1 feito, sem perda de generalidade, com uma \n\ncamada de entrada, uma camada oculta (intermedi\u00e1ria ou hidden) e uma camada de sa\u00edda, \n\nconforme a Figura 8. \n\n \n\nFigura 8 \u2013 Uma rede direta de tr\u00eas camadas. \n\n \n\nFonte: Abdi et al. (1999). \n\n \n\nNa Figura 8, a camada de entrada \u00e9 constitu\u00edda por I neur\u00f4nios, a camada oculta \u00e9 \n\nconstitu\u00edda por L neur\u00f4nios e a camada de sa\u00edda \u00e9 constitu\u00edda por J neur\u00f4nios. A matriz dos \n\npesos sin\u00e1pticos Z, que conecta os neur\u00f4nios da camada de entrada aos neur\u00f4nios da camada \n\noculta (no MATLAB, esta matriz \u00e9 denotada por IW) \u00e9 de ordem I X L, a matriz de pesos \n\nsin\u00e1pticos W, que conecta os neur\u00f4nios da camada oculta aos neur\u00f4nios da camada de sa\u00edda \n\n(no MATLAB, ela \u00e9 denotada por LW) \u00e9 de ordem L X J, o vetor do padr\u00e3o de entrada x k  \u00e9 \n\nde ordem I X 1. Da mesma forma, o vetor h k  \u00e9 de ordem L X 1, kt?  \u00e9 de ordem J X 1. A sa\u00edda \n\ndesejada \u00e9 o vetor kt , tamb\u00e9m de ordem J X 1, de acordo com Abdi et al. (1999). \n\nA diferen\u00e7a entre a resposta obtida (estimada) kt?  de uma unidade de sa\u00edda e a resposta \n\ndesejada kt  \u00e9 o erro produzido pela rede. As c\u00e9lulas da camada de sa\u00edda usam este erro \n\n\n\n35 \n \n\ndiretamente para corrigir os seus pesos nas conex\u00f5es. O mesmo n\u00e3o acontece com as c\u00e9lulas \n\nda camada oculta, que n\u00e3o est\u00e3o em contato direto com alguma f\u00f3rmula de erro, de forma que \n\nelas estimam os pr\u00f3prios erros. Neste ponto entra o erro de retropropaga\u00e7\u00e3o. O crescimento \n\ndo erro cometido \u00e9 inicialmente um sinal de erro proporcional \u00e0 raz\u00e3o de mudan\u00e7a (tangente \n\nou derivada) da fun\u00e7\u00e3o de ativa\u00e7\u00e3o n\u00e3o linear. Este sinal de erro \u00e9 passado para tr\u00e1s atrav\u00e9s \n\ndos pesos das conex\u00f5es da unidade oculta. Em seguida, todos os pesos das conex\u00f5es s\u00e3o \n\natualizados, conforme Abdi et al. (1999).  \n\nNa Regra Delta de unidades lineares a corre\u00e7\u00e3o dos pesos \u00e9 proporcional ao erro \n\ncometido e ao valor da c\u00e9lula de entrada, enquanto na Regra Delta Generalizada, a corre\u00e7\u00e3o \n\ndos pesos sin\u00e1pticos \u00e9 proporcional \u00e0 raz\u00e3o de mudan\u00e7a da fun\u00e7\u00e3o de transfer\u00eancia n\u00e3o linear \n\n(ABDI et al.,1999). Na camada de sa\u00edda, tem-se, ent\u00e3o, \n\n? W lj  = f \u00b4(a j ). ? ?jj tt \u02c6?  .h l  =                   (2.14) \n\n = f \u00b4(a j ).e j .h l  =                     (2.15) \n\n= lj h.? ,                      (2.16) \n\nonde a j \u00e9 a sa\u00edda para a fun\u00e7\u00e3o de transfer\u00eancia. O sinal de erro \u00e9 j?  = f\u2019(a j ).e j . Na camada \n\noculta o erro \u00e9 estimado mediante a aplica\u00e7\u00e3o da Equa\u00e7\u00e3o (2.17) \n\nle?  = ?\nj\n\njljw ?  e,                      (2.17)  \n\nconsequentemente, na camada de entrada tem-se \n\n? Z il  = [f \u00b4(a l ). le? ].x i  =                    (2.18) \n\n= il x.? ,                      (2.19) \n\n onde a l  \u00e9 a sa\u00edda para a fun\u00e7\u00e3o de transfer\u00eancia. \n\nNa modalidade de \u00fanico est\u00edmulo (termos com \u00edndice k), de acordo com Abdi et al. \n\n(1999), \u00e9 mostrado o funcionamento da Regra Delta Generalizada atrav\u00e9s da sequ\u00eancia das \n\nequa\u00e7\u00f5es que seguem abaixo. Na ida ou no sentido da propaga\u00e7\u00e3o do erro, a sequ\u00eancia se \n\ncaracteriza pelos seguintes passos, onde o s\u00edmbolo T,  que acompanha as matrizes Z e W, \n\nsignifica que a matriz \u00e9 transposta: \n\n\n\n36 \n \n\nCamada de entrada x k ; \n\nCamada oculta h k  = f(Z\nT x k ); \n\nCamada de sa\u00edda kt?  = f(W\nT h k ); \n\nErro de sinal kt?  = f(W\nT h k ). \n\nAgora, no retorno, no sentido da retropropaga\u00e7\u00e3o do erro, tem-se: \n\nCamada de sa\u00edda (s) e k  = kk tt \u02c6? ; \n\nks,? = f \u00b4(W\nT .h k ).e k ; \n\nW(n +1) = W(n) + ? .h k .\nT\n\nks,?  = W(n) + ? W(n). \n\nCamada oculta (o) ko,? = f \u00b4(Z\nT .x k ).e k ; \n\nZ(n + 1) = Z(n) + ? .x k .\nT\n\nko,?  = Z(n) + ? Z(n). \n\nAo retornar \u00e0 camada de entrada, inicia-se novamente o processo de ida e de volta at\u00e9 \n\nque o erro seja minimizado para resolver o problema. No MATLAB, cada uma dessas \n\npassagens \u00e9 considerada como um passo ou um epoch, do original em ingl\u00eas. Para facilitar os \n\nc\u00e1lculos, pode-se tamb\u00e9m colocar, de forma simplificada (Formulas 2.20), \n\nb = Z T .x                      (2.20) \nh = f(b)  \na = W T .h  \nt?  = f(a)  \ne = tt \u02c6?   \n\ns?  = f \u00b4(a).e  \ne?  = W. s?   \n\no?  = f \u00b4(b). e?   \n \n\nZ(n + 1) = Z(n) + ? .x. To?   \n\nW(n + 1) = W(n) + ? .h. Ts?  \n\n\n\n37 \n \n\nNestas f\u00f3rmulas, o \u00edndice s est\u00e1 relacionado \u00e0 camada de sa\u00edda, o \u00edndice o est\u00e1 \n\nrelacionado \u00e0 camada oculta e ?  \u00e9 a taxa de aprendizado. \n\nO objetivo deste algoritmo \u00e9 o de encontrar iterativamente o m\u00ednimo da fun\u00e7\u00e3o do erro \n\n(superf\u00edcie de erro), usando o m\u00e9todo do gradiente descendente. O gradiente \u00e9 um operador \n\ndiferencial que aplicado a uma fun\u00e7\u00e3o de no m\u00ednimo duas vari\u00e1veis aponta, sempre, no \n\nsentido do crescimento m\u00e1ximo dessa fun\u00e7\u00e3o. No sentido contr\u00e1rio, encontra-se o m\u00ednimo. \n\nEsta regra est\u00e1 intimamente relacionada com a t\u00e9cnica estat\u00edstica da An\u00e1lise de Regress\u00e3o \n\nN\u00e3o Linear. Considerando novamente a forma matricial do erro e usando a regra da cadeia \n\nadaptada para matrizes, demonstra-se o m\u00e9todo do gradiente descendente conforme \n\ndesenvolvimento a seguir. Assim, se \n\n? ? ? ? ? ?kTkkTkkTkkkTkkk ttttttttttE \u02c62\u02c6\u02c6\n2\n\n1\u02c6\u02c6\n2\n\n1\n??????                 (2.21) \n\n\u00e9 a fun\u00e7\u00e3o de erro, ent\u00e3o o gradiente aplicado a esta fun\u00e7\u00e3o \u00e9 \n\nW\n\nhW\n\nhW\n\nt\n\nt\n\nE\n\nW\n\nE\nE k\n\nT\n\nk\nT\nk\n\nk\n\nkk\nkW ?\n\n?\n?\n?\n\n?\n?\n\n?\n?\n?\n\n??\n\u02c6\n\n\u02c6\n,                 (2.22) \n\nonde cada derivada parcial, calculada em separado, fornece os resultados \n\n? ?kk\nk\n\nk tt\nt\n\nE\n\u02c6\n\n\u02c6\n???\n\n?\n?\n\n,                    (2.23) \n\n? ? ? ?kT\nk\n\nT\nk\n\nT\n\nk\nT\nk hWf\n\nhW\n\nhWf\n\nhW\n\nt\n'\n\n\u02c6\n?\n\n?\n?\n\n?\n?\n? , e                     (2.24) \n\nT\nk\n\nk\nT\n\nh\nW\n\nhW\n2?\n\n?\n? .                      (2.25) \n\nDonde se conclui que \nT\nkkskW hE ,???? , e                 (2.26) \n\n? ? T kskhnW ,???? .                    (2.27) \n\nNovamente, usando a regra da cadeia adaptada para matrizes e considerando que: \n\n? ?? ?kTTk xZfWft ?\u02c6  ,                    (2.28) \n\n\n\n38 \n \n\n \nZ\n\nxZ\n\nxZ\n\nh\n\nZ\n\nxZ\n\nxZ\n\nh\n\nh\n\nhW\n\nhW\n\nt\n\nt\n\nE\n\nZ\n\nE\nE k\n\nT\n\nk\nT\n\nk\nko\n\nk\nT\n\nk\nT\n\nk\n\nk\n\nk\nT\n\nk\nT\nk\n\nk\n\nkk\nkZ ?\n\n?\n?\n?\n\n??\n?\n\n?\n?\n?\n\n??\n?\n\n?\n??\n?\n\n?\n?\n\n?\n?\n?\n\n?\n?\n\n?\n?\n?\n\n?? ,\n\u02c6\n\n\u02c6\n? .               (2.29)  \n\nCalculando cada derivada parcial em separado, encontram-se as express\u00f5es: \n\nW\nh\n\nhW\n\nk\n\nk\nT\n\n?\n?\n\n? ,                      (2.30) \n\n? ? ? ?kT\nk\n\nT\nk\n\nT\n\nk\nT\n\nk xZf\nxZ\n\nxZf\n\nxZ\n\nh\n'?\n\n?\n?\n\n?\n?\n?  , e                   (2.31) \n\nT\nk\n\nk\nT\n\nx\nZ\n\nxZ\n2?\n\n?\n? .                      (2.32) \n\nConsequentemente, \n\nT\nkkokZ xE ,????  , e                    (2.33) \n\n? ? T kokxnZ ,???? .                     (2.34) \n\nCom estes resultados conclui-se a demonstra\u00e7\u00e3o do m\u00e9todo do gradiente descendente, \n\naplicado na elabora\u00e7\u00e3o da Regra Delta Generalizada, conforme Abdi et al. (1999). \n\nAl\u00e9m das tr\u00eas regras de aprendizado aqui apresentadas, Haykin (2001) acrescenta o \n\nprocesso de aprendizagem baseado em mem\u00f3ria e a aprendizagem de Boltzmann com \n\nexcelente detalhamento. Tamb\u00e9m complementa com outros t\u00f3picos relacionados com as redes \n\nneurais artificiais. \n\nPor seu turno, Braga et al. (2007) apresenta em detalhes o modelo ADALINE, as redes \n\nauto-organiz\u00e1veis, o desenvolvimento dos Sistemas Neurais H\u00edbridos, tema de pesquisa mais \n\nrecente, onde um dos subsistemas \u00e9 uma rede neural artificial, e as redes neurais sem pesos. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n39 \n \n\n3. MATERIAIS E M\u00c9TODOS \n\n3.1. O P\u00f3lo Cer\u00e2mico de Santa Gertrudes \n\n \n\nO P\u00f3lo Cer\u00e2mico de Santa Gertrudes (PCSG) constitu\u00eddo pelos munic\u00edpios de Limeira, \n\nCordeir\u00f3polis, Santa Gertrudes, Rio Claro, Ipe\u00fana, Piracicaba e Araras \u00e9 integrado por 34 \n\nind\u00fastrias cer\u00e2micas, de um total de 47, dentro do Estado de S\u00e3o Paulo, que s\u00e3o filiadas \u00e0 \n\nAssocia\u00e7\u00e3o Paulista das Cer\u00e2micas de Revestimento, ASPACER, (ASPACER, 2012). \n\nPor ter acompanhado as inova\u00e7\u00f5es e mudan\u00e7as tecnol\u00f3gicas pela importa\u00e7\u00e3o, \n\nprincipalmente, de m\u00e1quinas e equipamentos de pa\u00edses sobre os quais foram impostas severas \n\nregras para o controle ambiental da produ\u00e7\u00e3o industrial de transforma\u00e7\u00e3o, ainda de acordo \n\ncom Poletto (2007), os impactos ambientais causados pelas ind\u00fastrias do j\u00e1 citado APL de \n\nSanta Gertrudes sofreram uma sens\u00edvel redu\u00e7\u00e3o, contr\u00e1rio ao das mineradoras que ainda, na \n\nocasi\u00e3o em que foi elaborada a sua pesquisa, n\u00e3o haviam modernizado os seus processos de \n\nlavra. Contudo, o mesmo autor pondera que com uma fiscaliza\u00e7\u00e3o mais rigorosa por parte dos \n\n\u00f3rg\u00e3os e das entidades governamentais associadas a melhorias t\u00e9cnicas advindas das \n\nmodernas m\u00e1quinas e dos equipamentos de minera\u00e7\u00e3o e, tamb\u00e9m, com mudan\u00e7as na forma de \n\nextra\u00e7\u00e3o das argilas, inclusive as mineradoras est\u00e3o se esfor\u00e7ando no sentido de diminuir os \n\nimpactos ambientais por elas causados, dentro das possibilidades e orienta\u00e7\u00f5es de cada uma \n\ndelas. Novamente, Poletto (2007) considera que o maior impacto ambiental causado pelas \n\nmineradoras \u00e9 aquele da desfigura\u00e7\u00e3o topogr\u00e1fica, para a qual sugere algumas solu\u00e7\u00f5es como, \n\npor exemplo, a revegeta\u00e7\u00e3o, al\u00e9m da revers\u00e3o da \u00e1rea j\u00e1 explorada em terras produtivas e \n\nautosustent\u00e1veis. \n\n \n\n3.2. Caracteriza\u00e7\u00e3o das Argilas e das Cer\u00e2micas \n\n \n\nDe acordo com Christofoletti e Moreno (2004), as argilas da Forma\u00e7\u00e3o Corumbata\u00ed \n\nconstitu\u00edda basicamente de argilitos, siltitos, arenitos, calc\u00e1rios, predominante no APL de \n\nSanta Gertrudes, s\u00e3o argilominerais que permitem, dentre outros, o seu aproveitamento \n\ndentro, principalmente, do grupo BIIb, segundo a norma NBR 13818 da Associa\u00e7\u00e3o Brasileira \n\nde Normas T\u00e9cnicas, ABNT, (ABNT, 1997). Conforme os textos de Christofoletti et al. \n\n(2005), Poletto (2007b) e Prado et al. (2008), as argilas de boa qualidade dessa forma\u00e7\u00e3o \n\nocorrem no interior do Estado de S\u00e3o Paulo, onde se localiza o PCSG com aproximadamente \n\n\n\n40 \n \n\n90 m de espessura (altura) numa largura que atinge aproximadamente 10 km. De acordo com \n\na ASPACER (2012) esta forma\u00e7\u00e3o possui uma continuidade aproximada de 200 km. \n\nEm sua composi\u00e7\u00e3o mineral\u00f3gica destacam-se a illita, a montmorillonita, a clorita, o \n\nquartzo, o feldspato, os carbonatos (em n\u00edveis localizados), os compostos de Fe e Ti , al\u00e9m da \n\npresen\u00e7a de argilominerais interestratificados, segundo Gaidzinski (2006) e Prado et al. \n\n(2008), ressaltando que a ocorr\u00eancia das argilas n\u00e3o \u00e9 homog\u00eanea. Ocorrem varia\u00e7\u00f5es nos \n\nteores de uma mina para outra e, mesmo dentro de uma cava de lavra, a composi\u00e7\u00e3o das \n\nargilas pode variar tanto horizontalmente quanto verticalmente, esta, por\u00e9m, menos frequente. \n\nEm complemento, de acordo com Christofoletti et al. (2005), Gaidzinski (2006), Poletto \n\n(2007) e Prado et al. (2008), essas argilas apresentam em sua composi\u00e7\u00e3o qu\u00edmica alguns \n\n\u00f3xidos, principalmente, SiO 2  e AlO 3 , ambos respons\u00e1veis pelas propriedades refrat\u00e1rias, os \n\ncarbonatos (terrosos) CaO, MgO, Na 2 O, que juntos ao \u00f3xido K 2 O s\u00e3o respons\u00e1veis pelas \n\npropriedades de queima (ou sinteriza\u00e7\u00e3o). Completam esta lista Fe 2 O 3 , TiO 2 , MnO, P 2 O 5  e \n\nMn 2 O 3 . Da mesma maneira h\u00e1 varia\u00e7\u00f5es, visto que a porcentagem destes \u00f3xidos presentes \n\nnas argilas \u00e9 diferente em cada cava de lavra. \n\nPrado (2007) verificou que as ind\u00fastrias cer\u00e2micas misturam, no m\u00e1ximo, argilas \n\nprovenientes de tr\u00eas fontes diferentes e que na prepara\u00e7\u00e3o da massa para moagem executam \n\numa mistura de mat\u00e9rias-primas. Tamb\u00e9m, complementa que o grupo BIIb especificado na \n\nnorma NBR 13818 (ABNT, 1997) caracteriza um produto semi-poroso, com alta absor\u00e7\u00e3o de \n\n\u00e1gua mas com baixa resist\u00eancia mec\u00e2nica. Devido \u00e0 porosidade, medida em porcentagem pela \n\nvari\u00e1vel f\u00edsica absor\u00e7\u00e3o de \u00e1gua (Abs), a classifica\u00e7\u00e3o das placas cer\u00e2micas \u00e9 apresentada na \n\nTabela 1. \n\n \n\nTabela 1 \u2013 Classifica\u00e7\u00e3o das placas cer\u00e2micas quanto \u00e0 porosidade. \nPRODUTO Abs (%) \n\nPoroso 10 \u2013 20 \n\nSemi-poroso 6 \u2013 10 \n\nSemi-gr\u00eas 3 \u2013 6 \n\nGr\u00eas 0.5 \u2013 3 \n\nPorcelanato &lt;0.5 \n\nFonte: Prado (2007). \n\n \n\n\n\n41 \n \n\nO porcelanato \u00e9 um produto cer\u00e2mico de alto valor agregado e a sua produ\u00e7\u00e3o \u00e9 a que \n\nmais cresce no mundo. Paralelamente aos dados da Tabela 1, Prado (2007) acrescenta a \n\nTabela 2 (como consta da norma NBR 13818, ABNT,1997). \n\n \n\nTabela 2 \u2013 Especifica\u00e7\u00f5es da NBR 13818 (ABNT, 1997). \n M\u00c9TODOS DE FABRICA\u00c7\u00c3O \n\nAbs(%) Extrudado (A) Prensado (B)  Outros (C) \n\n    \n\n    \n\nAbs ?  0.5 AI BIa CI \n\n0.5 &lt;Abs ?  3.0 AI BIb CI \n\n3.0 &lt;Abs ?  6.0 AIIa BIIa CIIa \n\n6.0 &lt;Abs ?  10.0 AIIb BIIb CIIb \n\nAbs > 10.0 AIII BIII  CIII \n\nFonte: (ABNT, 1997) e (PRADO, 2007). \n\n \n\nA ind\u00fastria de revestimentos cer\u00e2micos est\u00e1 inserida na ind\u00fastria de materiais de \n\nconstru\u00e7\u00e3o que, por sua vez, faz parte da ind\u00fastria de transforma\u00e7\u00e3o. No APL de Santa \n\nGertrudes, o principal processo de fabrica\u00e7\u00e3o \u00e9 o chamado Processo Via Seca (CORREIA et \n\nal., 2007), com o qual, em princ\u00edpio, n\u00e3o h\u00e1 misturas e nem homogeneiza\u00e7\u00e3o, mas inferioriza \n\na qualidade do produto final, ou seja, os pisos e revestimentos cer\u00e2micos. Por\u00e9m, este \n\nprocesso industrial \u00e9 largamente usado porque \u00e9 aplicado com menores custos energ\u00e9ticos, \n\nmenores custos de manuten\u00e7\u00e3o, menores impactos ambientais como um importante resultado, \n\napesar de apresentar limita\u00e7\u00f5es tecnol\u00f3gicas. Conforme Moreno et al. (2009), a argila \u00e9 mo\u00edda \n\na seco, os ciclos s\u00e3o de monoqueima r\u00e1pida com intervalos de tempo de 20 a 30 minutos, com \n\numa temperatura m\u00e1xima superior a 1000\u00b0 C. Christofoletti (1999) refor\u00e7a que na moagem a \n\nseco a umidade \u00e9 de valor aproximado de 5%, enquanto na prensagem a seco a umidade \u00e9 de \n\nordem aproximada a 10%. A monoqueima r\u00e1pida \u00e9 considerada melhor do que a biqueima, \n\noutro processo de fabrica\u00e7\u00e3o. Recentemente foi implantado por algumas ind\u00fastrias brasileiras, \n\num terceiro processo de fabrica\u00e7\u00e3o denominado de terceira queima, o qual permite alguns \n\nefeitos especiais na produ\u00e7\u00e3o. Em resumo, Christofoletti (1999) e Prado (2007) consideram as \n\nseguintes fases nas opera\u00e7\u00f5es de minera\u00e7\u00e3o: \n\n1. Pesquisa mineral; \n\n2. Remo\u00e7\u00e3o da cobertura vegetal (ou desmatamento); \n\n\n\n42 \n \n\n3. Decapeamento; \n\n4. Desmonte (por explosivos ou por meios mec\u00e2nicos); \n\n5. Carregamentos; \n\n6. Britagem (ou tritura\u00e7\u00e3o); \n\n7. Sazonamento (em pilhas) ou \n\n8. P\u00e1tio de secagem e de homogeneiza\u00e7\u00e3o; e \n\n9. Armazenamento (para o posterior transporte de argila). \n\nE, para as opera\u00e7\u00f5es industriais: \n\n1. Extra\u00e7\u00e3o e transporte da argila; \n\n2. Prepara\u00e7\u00e3o da massa (via seca) e moagem; \n\n3. Atomiza\u00e7\u00e3o; \n\n4. Prensagem (ou moldagem) e secagem; \n\n5. Prepara\u00e7\u00e3o do esmalte e esmalta\u00e7\u00e3o (ou n\u00e3o esmaltados) \u2013 faz parte do conceito de \n\ndecora\u00e7\u00e3o; \n\n6. Queima (monoqueima r\u00e1pida); \n\n7. Ret\u00edfica e polimento; \n\n8. Classifica\u00e7\u00e3o do produto e embalagem; \n\n9. Transporte interno (empilhamento e carregamento de caixas); \n\n10. Lavagem de equipamentos e pisos; \n\n11. Gera\u00e7\u00e3o de res\u00edduos s\u00f3lidos; e \n\n12. Sele\u00e7\u00e3o, expedi\u00e7\u00e3o e comercializa\u00e7\u00e3o. \n\n \n\nCom rela\u00e7\u00e3o \u00e0 homogeneiza\u00e7\u00e3o das argilas, Gaidzinski (2006) fez uma an\u00e1lise de \n\nsazonamento, que consiste na estocagem das argilas a c\u00e9u aberto por per\u00edodos de tempo que \n\nvariam de 6 meses a 2 anos, e concluiu que para se conseguir esta homogeneiza\u00e7\u00e3o o per\u00edodo \n\nde tempo deve ser superior a um ano, com as pilhas mantidas em local aberto. Para os seus \n\nestudos, tamb\u00e9m usou argilas provenientes do APL de Santa Gertrudes. \n\nFinalmente, a argila, como qualquer outro mineral, \u00e9 um recurso n\u00e3o renov\u00e1vel, \n\nsignificando que uma mina \u00e9 tempor\u00e1ria e, levando isto em considera\u00e7\u00e3o, pode-se dizer que \n\nela possui um ciclo de vida (AMBIENTE BRASIL, 2009). Este ciclo de vida de uma mina \u00e9 \n\nconstitu\u00eddo pelas seguintes fases: \n\n1. Planejamento (pesquisa mineral, estudos ambientais, estudos de viabilidade); \n\n\n\n43 \n \n\n2. Implanta\u00e7\u00e3o; \n\n3. Opera\u00e7\u00e3o; \n\n4. Desativa\u00e7\u00e3o (prepara\u00e7\u00e3o para o fechamento); \n\n5. Fechamento; e \n\n6. Transfer\u00eancia de responsabilidade. \n\nNeste momento, conv\u00e9m salientar que o Ambiente Brasil (2009) sugere que para se \n\naumentar o ciclo de vida de uma mina, as alternativas s\u00e3o a redu\u00e7\u00e3o de consumo e a \n\nreciclagem das mat\u00e9rias-primas. \n\n \n\n3.3. Os Corpos-de-Prova \n\n \n\nAs amostras para a confec\u00e7\u00e3o e moldagem dos corpos-de-prova foram retiradas das \n\nnove minas: Cristofoletti minas 1 e 2, Cruzeiro frentes 1 e 2, Paganoti, Partezani minas 1, 2 e \n\n3, Pieroni e Tute, identificadas e localizadas conforme os dados que constam da Tabela 3. \n\n \n\nTabela 3 \u2013 Coordenadas UTM obtidas por GPS das minas estudadas (Datum: SAD 69). \nMINAS SIGLA COORDENADA\n\nN - S \n\nCOORDENADA \n\nE \u2013 W \n\nALTITUDE \n\n(m) \n\nCristofoletti Mina 1 CF1 0231532 7521059 540 \n\nCristofoletti Mina 2 CF2 0230592 7526781 598 \n\nCruzeiro Frente 1 CR1 0248481 7505118 654 \n\nCruzeiro Frente 2 CR2 0249023 7505458 641 \n\nPaganoti PG 0251306 7531241 690 \n\nPartezani Mina 1 PT1 0234656 7506449 573 \n\nPartezani Mina 2 PT2 0235424 7507416 567 \n\nPartezani Mina 3 PT3 0234843 7507990 560 \n\nPieroni PI  0232239 7512374 552 \n\nTute TU 0241575 7535652 639 \n\nFonte: Rocha (2012). \n\n \n\nA Tabela 3 apresenta as coordenadas de localiza\u00e7\u00e3o das minas, zona 23K e, tamb\u00e9m, \n\nas altitudes das minas escolhidas, as quais foram obtidas com a utiliza\u00e7\u00e3o do Sistema de \n\nPosicionamento Global, GPS (Global Positioning System). \n\n\n\n44 \n \n\nA Mina Cruzeiro pertence ao munic\u00edpio de Limeira, a Mina Paganoti faz parte do \n\nmunic\u00edpio de Araras, enquanto as Minas Cristofoletti, as Minas Partezani, a Mina Pieroni e a \n\nMina Tute pertencem ao munic\u00edpio de Rio Claro. As \u00e1reas das minas Cruzeiro Frente 1 e \n\nCruzeiro Frente 2, embora pr\u00f3ximas, possuem caracter\u00edsticas distintas e dessa maneira foram \n\nconsideradas em separado. As minas Partezani 1, 2 e 3 encontram-se dentro da mesma \n\npropriedade, mas est\u00e3o localizadas em \u00e1reas bem distintas devido a grande extens\u00e3o territorial \n\nda propriedade. De acordo com Rocha (2012), a escolha destas minas foi feita em fun\u00e7\u00e3o de \n\nque elas permitiram a obten\u00e7\u00e3o de amostras bem diferenciadas e, principalmente, por \n\nrepresentarem toda a coluna estratigr\u00e1fica da Forma\u00e7\u00e3o Corumbata\u00ed. \n\nAinda, conforme Rocha (2012), para a coleta sistem\u00e1tica das amostras foi utilizada \n\numa perfuratriz de acionamento pneum\u00e1tico e m\u00e1quinas escavadeiras. O material recolhido \n\nfoi armazenado em sacos pl\u00e1sticos com a respectiva identifica\u00e7\u00e3o, a qual consistiu das \n\nseguintes informa\u00e7\u00f5es: o nome da amostra (com refer\u00eancia \u00e0 bancada \u2013 designadas por B1, \n\nB2, B3, B4 e B5, respectivamente, da base ao topo da mina \u2013 e a mina de onde ela foi \n\nrecolhida), a data da coleta, a profundidade do furo e o posicionamento obtido pelo GPS. Foi \n\nestabelecida uma quantidade m\u00ednima de 30 kg por amostra de forma a permitir que fossem \n\nfeitos todos os testes necess\u00e1rios durante a etapa laboratorial. Os corpos-de-prova foram \n\npreparados pelo m\u00e9todo de moagem pela Via Seca, \u00e0 semelhan\u00e7a do m\u00e9todo empregado no \n\nPCSG. \n\nA Figura 9 apresenta duas fotos da mina Pieroni, as quais n\u00e3o exp\u00f5em claramente a \n\nsuperposi\u00e7\u00e3o das bancadas. Por isto, foi acrescentada a Figura 10 que apresenta duas fotos da \n\nmina Santa Am\u00e1bile, tamb\u00e9m de Rio Claro que, apesar de n\u00e3o constar da rela\u00e7\u00e3o das minas \n\nestudadas, no momento das fotos, exp\u00f4s de forma bem clara a superposi\u00e7\u00e3o das bancadas. \n\nNestas fotos, entre a base e o topo tem-se aproximadamente um desn\u00edvel de 45 m, onde a \n\nbancada B1 de colora\u00e7\u00e3o bem escura e contaminada com calc\u00e1rio, possui uma espessura \n\naproximada de 25 m, a seguir s\u00e3o vis\u00edveis   bancadas de colora\u00e7\u00e3o roxa, todas elas com uma \n\nespessura total de 15 a 20 m. Ainda, a bancada B2 apresenta argilas duras, B3 argilas \n\nintermedi\u00e1rias e, por fim, a bancada B4 apresenta argilas moles, seguindo o modelo utilizado \n\npelas ind\u00fastrias da regi\u00e3o do PCSG, de acordo com Rocha (2012). \n\nNa prepara\u00e7\u00e3o dos corpos-de-prova, as mat\u00e9rias-primas foram previamente mo\u00eddas e \n\nhomogeneizadas. Em seguida, foi adicionada \u00e1gua para que a umidade atingisse um teor de \n\n10%, em cujo controle foi utilizada uma balan\u00e7a com infravermelho (marca Gehaka, modelo \n\nBK 6000) e, tamb\u00e9m, com uma peneira de malha 8 ABNT (2.36 mm). Em sequ\u00eancia, as \n\n\n\n45 \n \n\nmat\u00e9rias-primas foram completamente homogeneizadas, com o teor de umidade em 10%, \n\nap\u00f3s um per\u00edodo de 24 horas. Finalmente, utilizando-se do m\u00e9todo de prensagem a seco num \n\nmolde de dimens\u00f5es 10.0 X 3.5 cm e com uma espessura m\u00ednima inferior a 7.5 mm foi \n\naplicada uma press\u00e3o de 250 kgf/cm 2 numa prensa hidr\u00e1ulica, cujo controle foi obtido com    \n\na determina\u00e7\u00e3o da densidade aparente do corpo-de-prova \u00famido em  2g/cm 3 , com massa de \n\n50 g. \n\n \n\n \n\n \n\n\n\n46 \n \n\nFigura 9 \u2013 Fotos da mina Pieroni. \n\n \n\n \n\nFonte: Autor (2012). \n\n \n\n \n\n \n\n \n\n\n\n47 \n \n\nFigura 10 \u2013 Fotos da mina Santa Am\u00e1bile. \n\n \n\n \n\nFonte: Autor (2012). \n\n \n\n\n\n48 \n \n\n3.4. Determina\u00e7\u00e3o das Vari\u00e1veis F\u00edsicas dos Corpos-de-Prova \n\n \n\nPara amostras secas foram calculadas as vari\u00e1veis f\u00edsicas Retra\u00e7\u00e3o Linear de Secagem \n\n(RLS), Densidade de Prensagem (DP) e a Densidade Aparente dos Corpos-de-Prova Secos \n\n(DAS). Para as amostras sinterizadas foram calculadas as vari\u00e1veis f\u00edsicas M\u00f3dulo de \n\nResist\u00eancia \u00e0 Flex\u00e3o (MRF), Retra\u00e7\u00e3o Linear de Queima (RLQ), Absor\u00e7\u00e3o de \u00c1gua (Abs), \n\nCarga de Ruptura (CR) e Perda ao Fogo (PF). Os ensaios para o c\u00e1lculo das vari\u00e1veis Abs , \n\nCR e MRF seguiram as especifica\u00e7\u00f5es t\u00e9cnicas da norma NBR 13818 (ABNT, 1997), cujos \n\nvalores s\u00e3o: 6 ?  Abs &lt;10 (grupo BIIb), CR > 500 N e MRF > 18 MPa. \n\nTanto a vari\u00e1vel DP como a vari\u00e1vel DAS foram calculadas pela f\u00f3rmula \n\nDensidade = \nV\n\nm\n ,           (3.1) \n\nonde m \u00e9 a massa do corpo-de-prova, em g , e V \u00e9 o volume do corpo-de-prova, em cm 3 . A \n\nunidade de densidade \u00e9 g/cm 3 . \n\nMedindo-se com um paqu\u00edmetro o comprimento, em mm, dos corpos-de-prova \u00famidos \n\nap\u00f3s a prensagem ( pL ) e o comprimento dos mesmos corpos-de-prova ap\u00f3s a secagem ( sL ) \n\nem uma estufa mantida \u00e0 temperatura constante de 110\u00b0 C por 24 horas, a vari\u00e1vel RLS foi \n\ncalculada, em porcentagem, usando-se a express\u00e3o: \n\n.100*\np\n\nps\n\nL\n\nLL\nRLS\n\n?\n?                      (3.2) \n\nEm seguida, os corpos-de-prova foram secados at\u00e9 o teor de umidade atingir um valor \n\ninferior a 1% e queimados       (forno de rolo de laborat\u00f3rio \u2013 marca Cifel) em um ciclo de 30 \n\nminutos nas temperaturas de 1000\u00b0 C, 1020\u00b0 C, 1040\u00b0 C e 1060\u00b0 C. \n\nPara o c\u00e1lculo do MRF, em MPa, os corpos-de-prova j\u00e1 prensados, secos e queimados, \n\nutilizou-se de um flex\u00edmetro (marca Nannetti, modelo FM/96) durante os ensaios. A f\u00f3rmula \n\nusada foi  \n\n,8066.9*\n2\n\n3\n2\nminbe\n\nFL\nMRF ?           (3.3) \n\n \n\n\n\n49 \n \n\nonde F \u00e9 a for\u00e7a de ruptura, em kgf (1 kgf = 9.9066 N \u2013 fator de corre\u00e7\u00e3o das unidades na \n\nf\u00f3rmula (3.3)), L \u00e9 a dist\u00e2ncia entre as barras de apoio, em mm, b \u00e9 a largura do corpo-de-\n\nprova ao longo da ruptura ap\u00f3s o ensaio, em mm, e mine  \u00e9 a espessura m\u00ednima do corpo-de-\n\nprova, tamb\u00e9m em mm. \n\nAnalogamente, foram calculadas: \n\n,100*\n1\n\n12\n\nm\n\nmm\nAbs\n\n?\n?            (3.4) \n\nonde Abs \u00e9 a absor\u00e7\u00e3o de \u00e1gua, em porcentagem, 1m  \u00e9 a massa seca, em g, e 2m  \u00e9 a massa \n\n\u00famida ou saturada, em g ; \n\n,\nb\n\nFL\nCR ?             (3.5) \n\nonde CR \u00e9 a carga de ruptura, em N, F \u00e9 a for\u00e7a de ruptura, em N, L \u00e9 a dist\u00e2ncia entre as \n\nbarras de apoio, em mm e b \u00e9 a largura do corpo-de-prova ao longo da ruptura ap\u00f3s o ensaio, \n\nem mm; \n\n,100*\ns\n\nsq\n\nL\n\nLL\nRLQ\n\n?\n?           (3.6) \n\nonde RLQ \u00e9 a retra\u00e7\u00e3o linear de queima, em porcentagem, qL  \u00e9 o comprimento do corpo-de-\n\nprova queimado, em mm, e sL  \u00e9 o comprimento do corpo-de-prova seco, em mm; e, \n\nfinalmente \n\n,100*\ns\n\nsq\n\nm\n\nmm\nPF\n\n?\n?            (3.7)  \n\nonde PF \u00e9 a perda ao fogo, em porcentagem, qm  \u00e9 a massa do corpo-de-prova queimado, em \n\ng, e sm  \u00e9 a massa do corpo-de-prova seco, em g. Neste caso, foi usada uma balan\u00e7a de \n\nprecis\u00e3o (mediu a massa com tr\u00eas casas decimais). \n\nOs dados dos ensaios realizados, de acordo com Rocha (2012), constam dos ANEXOS \n\nA (1000\u00b0C), B (1020\u00b0C), C (1040\u00b0C) e D (1060\u00b0C). No ANEXO A entre as minas e suas \n\nbancadas foram encontrados um total de 24 valores para as vari\u00e1veis f\u00edsicas RLQ, PF, Abs, \n\n\n\n50 \n \n\nCR e MRF. Da mesma maneira, nos ANEXOS B, C e D as vari\u00e1veis s\u00e3o DP, DAS, RLS, \n\nRLQ, PF, Abs, CR e MRF, sendo que o n\u00famero de valores encontrados foram 25, 24 e 23, \n\nrespectivamente. Nestes ANEXOS, foram calculadas tamb\u00e9m a amplitude, a m\u00e9dia aritm\u00e9tica \n\ne o desvio padr\u00e3o. \n\n \n\n3.5. Metodologia \n\n \n\nEm princ\u00edpio, o m\u00e9todo empregado neste trabalho ser\u00e1 o dedutivo. \n\nA ind\u00fastria cer\u00e2mica busca se estabelecer em um nicho de mercado e para isso precisa \n\ndeterminar para o seu produto um padr\u00e3o de qualidade, dentro das normas t\u00e9cnicas brasileiras \n\nemanadas pela ABNT. Por outro lado, de acordo com Rocha (2012), o Brasil j\u00e1 atingiu o \n\nsegundo lugar no mundo com a produ\u00e7\u00e3o de pisos e revestimentos cer\u00e2micos, mas ainda \n\nencontra-se em quinto lugar dentro do pequeno grupo de pa\u00edses exportadores, por isso h\u00e1 a \n\nesperan\u00e7a de que num futuro pr\u00f3ximo o Brasil tamb\u00e9m se aproxime dos primeiros lugares. \n\nEm busca de novos mercados, as ind\u00fastrias cer\u00e2micas brasileiras ter\u00e3o que se preparar para \n\nenfrentar uma dura concorr\u00eancia num mundo cada vez mais globalizado, competitivo e cada \n\nvez mais carente de produtos e servi\u00e7os. Em fun\u00e7\u00e3o da sua pol\u00edtica e dos seus interesses em \n\ncomo penetrar nestes novos mercados, cada ind\u00fastria cer\u00e2mica, via de regra, dever\u00e1 \n\nestabelecer um padr\u00e3o de especifica\u00e7\u00e3o diferente para cada um dos seus produtos. Em vista \n\ndisso, optou-se neste trabalho de pesquisa por um padr\u00e3o de sa\u00edda (especifica\u00e7\u00e3o) da rede \n\nneural (particular e independente dos valores especificados por qualquer ind\u00fastria do PCSG) \n\nbaseado nos valores pr\u00f3ximos aos da m\u00e9dia aritm\u00e9tica geral das m\u00e9dias aritm\u00e9ticas dos \n\nANEXOS A (1000\u00b0C), B (1020\u00b0C), C (1040\u00b0C) e D (1060\u00b0C), que s\u00e3o aqueles que \n\napresentam os dados completos dos ensaios dos corpos-de-prova do maior n\u00famero de \n\nbancadas e de minas, que fazem parte da Tabela 4, a seguir: \n\n \n\n\n\n51 \n \n\nTabela 4 \u2013 M\u00e9dias dos ANEXOS A, B, C e D, m\u00e9dia geral e o padr\u00e3o de produto estabelecido neste trabalho. \nM\u00c9DIAS DP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(Mpa) \n\nANEXO A - - - 3.7030 3.3355 10.2555 473.34 37.34 \n\nANEXO B 2.0288 1.8666 0.3078 5.2342 3.9364 7.3188 599.79 50.77 \n\nANEXO C 2.0241 1.8559 0.3052 6.1957 4.1189 5.8655 649.04 56.13 \n\nANEXO D 2.0272 1.8604 0.2926 6.6995 4.5172 4.5365 684.4 59.41 \n\n         \n\nM\u00e9dia \n\nGeral \n\n2.0267 1.8610 0.3019 5.4581 3.9770 6.9941 601.64 50.91 \n\n         \n\nPADR\u00c3O 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\u00c9 interessante observar que na Tabela 4, com exce\u00e7\u00e3o das vari\u00e1veis f\u00edsicas DP e DAS, \n\nas vari\u00e1veis RLQ, PF, CR e MRF apresentam valores crescentes em fun\u00e7\u00e3o do aumento da \n\ntemperatura, contr\u00e1rio ao das vari\u00e1veis f\u00edsicas RLS e Abs, que apresentaram valores \n\ndecrescentes, tend\u00eancia que ser\u00e1 utilizada para refor\u00e7ar a aplica\u00e7\u00e3o das redes neurais \n\nartificiais. \n\nEsta escolha garante uma independ\u00eancia com rela\u00e7\u00e3o aos padr\u00f5es especificados nas \n\nind\u00fastrias do PCSG. Como h\u00e1 um padr\u00e3o pr\u00e9-estabelecido, com rela\u00e7\u00e3o \u00e0 aplica\u00e7\u00e3o mediante \n\na utiliza\u00e7\u00e3o das redes neurais artificiais, optou-se pela rede que apresenta um padr\u00e3o (ou \n\nvetor) de sa\u00edda desejado. A escolha recaiu ent\u00e3o na rede MLP (Multilayer Perceptron) ou rede \n\nPerceptron de m\u00faltiplas camadas, conforme Haykin (2001). A vantagem desta escolha \u00e9 a sua \n\narquitetura, a qual permite que a rede neural seja montada com uma camada de entrada (ou \n\npadr\u00e3o de entrada), uma camada intermedi\u00e1ria ou oculta e uma camada de sa\u00edda (ou padr\u00e3o de \n\nsa\u00edda) que corresponde ao j\u00e1 citado padr\u00e3o de especifica\u00e7\u00e3o. Com isto, a diferencia\u00e7\u00e3o entre \n\numa rede e outra para encontrar o padr\u00e3o de sa\u00edda desejado fica por conta do n\u00famero de \n\nneur\u00f4nios da camada oculta. Al\u00e9m da arquitetura, pode-se dizer que outra vantagem \u00e9 que a \n\nrede MLP permite a aplica\u00e7\u00e3o da Regra de Aprendizado Delta Generalizada (ou regra \n\nbackpropagation, do original em ingl\u00eas) ou Regra de Retropropaga\u00e7\u00e3o do Erro, uma das mais \n\npoderosas regras de aprendizado das redes neurais (ABDI et al., 1999). Outra vantagem \u00e9 a \n\nfacilidade de aplica\u00e7\u00e3o desta rede dentro dos softwares conhecidos atualmente. \n\n\n\n52 \n \n\nPara aplicar a regra de aprendizado backpropagation na rede MLP foi utilizado o \n\nsoftware MATLAB\u00ae (MATrix LABoratory), da MathWorks, Inc., cujo programa detalhado \n\nconsta do Neural Networks Toolbox User\u2019s Guide. Neste trabalho, al\u00e9m da regra b\u00e1sica foram \n\nescolhidas outras tr\u00eas varia\u00e7\u00f5es da mesma regra, dentre outras que constam do mesmo \n\nsoftware, que s\u00e3o a Regra Backpropagation com Momento, a Regra Backpropagation \n\nResiliente e a Regra Backpropagation de Levenberg-Marquadt, sendo que cada uma destas \n\nvaria\u00e7\u00f5es apresenta uma velocidade diferente de converg\u00eancia. Inicialmente, foram \n\npadronizadas as tabelas para que as mesmas vari\u00e1veis f\u00edsicas fossem mostradas nas mesmas \n\nbancadas das mesmas minas e submetidas \u00e0s mesmas temperaturas de queima. Dessa forma, \n\ncom cinco vari\u00e1veis f\u00edsicas (RLQ, PF, Abs, CR e MRF) reuniram-se as tabelas de dados dos \n\nANEXOS E (1000\u00b0C), F (1020\u00b0C), G (1040\u00b0C) e H (1060\u00b0C), as quais apresentam 16 linhas \n\nde amostras de dados, e com as oito vari\u00e1veis f\u00edsicas (DP, DAS, RLS, RLQ, PF, Abs, CR e \n\nMRF) apresentam-se as tabelas de dados dos ANEXOS I (1020\u00b0C), J (1040\u00b0C) e K (1060\u00b0C), \n\nnas quais constam 20 linhas de amostras de dados. Tamb\u00e9m, nestas tabelas, foram calculadas \n\nas respectivas amplitudes, m\u00e9dias aritm\u00e9ticas e o desvio padr\u00e3o correspondente. Os dados que \n\nconstam dos ANEXOS E, F, G, H, I, J e K s\u00e3o inseridos como o padr\u00e3o (ou vetor) de entrada \n\nque, no MATLAB, \u00e9 designado pela letra latina p (ANEXO L). \n\nPara empregar o MATLAB foram considerados os seguintes par\u00e2metros de controle \n\nem fun\u00e7\u00e3o das quatro varia\u00e7\u00f5es da regra de aprendizado backpropagation. S\u00e3o eles: show, \n\nepochs, lr, mc e goal. \n\nO par\u00e2metro show significa que os resultados de sa\u00edda do programa ser\u00e3o apresentados \n\nno gr\u00e1fico correspondente ao erro (mse: erro m\u00e9dio quadr\u00e1tico) de acordo com um valor pr\u00e9-\n\nestabelecido (o valor utilizado foi 200, para visualiza\u00e7\u00e3o conveniente do gr\u00e1fico); o par\u00e2metro \n\nlr (learning rate, do original em ingl\u00eas) \u00e9 a taxa de aprendizado e que para dar estabilidade ao \n\nprocesso deve ser pequena, de acordo com sugest\u00e3o inserida no software (foi usado lr = 0.05); \n\no par\u00e2metro mc \u00e9 o do momento, para o qual o programa sugere um valor alto (neste caso, mc \n\n= 0.9). Tanto o par\u00e2metro lr como o par\u00e2metro mc assumem valores que pertencem ao \n\nintervalo de n\u00fameros reais [0,1]. Os par\u00e2metros epochs e goal s\u00e3o par\u00e2metros de parada do \n\nprocesso, isto \u00e9, independentemente de ter sido atingido ou n\u00e3o o ponto de converg\u00eancia \n\n\u00f3timo, eles finalizam o processo de execu\u00e7\u00e3o do programa. No caso do epochs, ele significa \n\nquantas vezes o processo deve ser propagado (no sentido da camada de entrada para a de \n\nsa\u00edda) e retropropagado (no sentido contr\u00e1rio) para se calcular o erro entre o valor obtido e o \n\n\n\n53 \n \n\nvalor desejado (iniciou-se com o valor 2000). E, finalmente, o par\u00e2metro goal \u00e9 o valor do \n\nerro m\u00e9dio quadr\u00e1tico (mse), que o software calcula mediante a aplica\u00e7\u00e3o da f\u00f3rmula: \n\n])},21[({\n*\n\n1\nMMsquaresummation\n\nnn\nmse ??            (3.8) \n\nonde n \u00e9 o n\u00famero de dados, M1 \u00e9 a matriz correspondente ao padr\u00e3o de sa\u00edda esperado e M2 \n\n\u00e9 a matriz correspondente ao padr\u00e3o de sa\u00edda encontrado durante o processo. Em fun\u00e7\u00e3o dos \n\ndados das tabelas constantes dos ANEXOS E, F, G, H, I, J e K (tr\u00eas casas decimais no \n\nm\u00e1ximo), o valor do par\u00e2metro goal foi fixado em 1e-5. \n\nEm seguida, foi estabelecido que a camada oculta de cada rede neural seria montada \n\ncom um n\u00famero que iria de 2 a 35 neur\u00f4nios (N=2,3,4,...,35) ou N= 5,10,15,20,25,30,35, em \n\nalguns casos, e o processo, para cada n\u00famero de neur\u00f4nios, deveria ser aplicado em tr\u00eas testes \n\ndiferentes para verificar a converg\u00eancia. O n\u00famero de testes de converg\u00eancia foi colocado \n\ntendo em vista que \u00e9 muito comum a superf\u00edcie de resposta do erro (relacionado ao mse), \n\nal\u00e9m de apresentar um m\u00ednimo global, n\u00e3o necessariamente, indicando uma otimizada \n\nconverg\u00eancia, pode tamb\u00e9m apresentar v\u00e1rios m\u00ednimos locais (diferentes do m\u00ednimo global), \n\npor vezes intranspon\u00edveis durante a execu\u00e7\u00e3o do processo, constituindo-se em verdadeiras \n\narmadilhas, antag\u00f4nicas \u00e0 converg\u00eancia esperada. Dessa forma, verificado que nos tr\u00eas testes \n\nde converg\u00eancia os erros, localizados na sua pr\u00f3pria superf\u00edcie de resposta de erro, ficaram \n\npresos nestas armadilhas, a aplica\u00e7\u00e3o do processo era encerrado no momento em que o \n\nn\u00famero de neur\u00f4nios da camada oculta atingisse o valor de N=35. Caso contr\u00e1rio, se entre os \n\ntr\u00eas testes fosse detectada alguma possibilidade de converg\u00eancia na dire\u00e7\u00e3o de um m\u00ednimo \n\nglobal, o n\u00famero de testes poderia ser ampliado at\u00e9 para dez, e, da mesma maneira, o n\u00famero \n\nde neur\u00f4nios na camada oculta tamb\u00e9m poderia ser aumentado e, de fato, o foi alcan\u00e7ando o \n\nvalor N=100, desde que se atingisse a estabilidade da rede neural; o mesmo poderia acontecer \n\ncom o n\u00famero de epochs que, em alguns casos, chegou a ser ampliado para 5000, 10000 ou \n\nat\u00e9 20000. \n\nComo todos os dados s\u00e3o positivos, as fun\u00e7\u00f5es de transfer\u00eancia usadas foram a \n\nlog\u00edstica (no MATLAB, logsig) entre a camada de entrada e a camada oculta e a linear (no \n\nMATLAB, purelin) entre a camada oculta e a camada de sa\u00edda. Nestes programas, o software \n\ncoloca como padr\u00e3o de sa\u00edda a letra latina t. Em tempo, ele usa como matriz dos pesos entre a \n\ncamada de entrada e a camada oculta a sigla IW (no cap\u00edtulo 2, a nota\u00e7\u00e3o usada foi Z, \n\nconforme a Figura 8) e a matriz de limiar correspondente a nota\u00e7\u00e3o b e como matriz dos pesos \n\n\n\n54 \n \n\nentre a camada oculta e a camada de sa\u00edda foi usada a sigla LW (novamente, no cap\u00edtulo 2 a \n\nnota\u00e7\u00e3o usada foi W, de acordo com a Figura 8) e a matriz de limiar correspondente a mesma \n\nnota\u00e7\u00e3o b. O resultado da simula\u00e7\u00e3o \u00e9 denotado pela letra latina a. \n\n \n\nPara a valida\u00e7\u00e3o desta metodologia e consequentemente para garantir a efetiva \n\naplica\u00e7\u00e3o das redes neurais artificiais \u00e0 ind\u00fastria de pisos e revestimentos cer\u00e2micos do \n\nPCSG, a solu\u00e7\u00e3o foi a de que encontrada, numa temperatura de queima (1000\u00b0 C, 1020\u00b0 C, \n\n1040\u00b0 C e 1060\u00b0 C) para as cinco vari\u00e1veis f\u00edsicas, ou nas temperaturas de queima de 1020\u00b0 \n\nC, 1040\u00b0 C e 1060\u00b0 C, para as oito vari\u00e1veis, um valor convergente compat\u00edvel com o erro \n\n(mse) pr\u00e9-estabelecido (goal), ou ainda, compat\u00edvel com o padr\u00e3o especificado originalmente \n\ndeterminado, as matrizes de pesos encontradas, LW e IW, junto com as matrizes de limiares, \n\nseriam ent\u00e3o aplicadas \u00e0s outras temperaturas de queima. Caso os resultados fossem \n\nconformes com o padr\u00e3o de sa\u00edda, ou ainda, dentro das especifica\u00e7\u00f5es da norma NBR 13818 \n\n(ABNT, 1997), a valida\u00e7\u00e3o desta metodologia estaria comprovada.  \n\n \n\n \n\n\n\n55 \n \n\n4. RESULTADOS E DISCUSS\u00d5ES \n\n4.1. Introdu\u00e7\u00e3o \n\n \n\nEm testes preliminares para  viabilizar se uma rede neural artificial e o algoritmo de \n\naprendizado, contudo com uma padroniza\u00e7\u00e3o dos valores das tabelas de dados para que todos \n\neles ficassem restritos ao intervalo real [0,1] (observado que os valores das tabelas s\u00e3o todos \n\npositivos e que a fun\u00e7\u00e3o log\u00edstica assume somente valores positivos dentro do mesmo \n\nintervalo real), iniciou-se o processo de an\u00e1lise com a regra de aprendizado backpropagation, \n\nno seu formato b\u00e1sico mas, como n\u00e3o se conseguiu um resultado satisfat\u00f3rio, avan\u00e7ou-se com \n\na inclus\u00e3o das varia\u00e7\u00f5es ao m\u00e9todo at\u00e9 que fosse encontrado um resultado que pudesse \n\ngarantir a validade do m\u00e9todo e, por consequ\u00eancia, ficasse comprovada a hip\u00f3tese deste \n\ntrabalho. Al\u00e9m disto, o processo deve ser vi\u00e1vel, isto \u00e9, que o trabalho de pesquisa por parte \n\nda ind\u00fastria consuma pouco tempo, seja de f\u00e1cil manipula\u00e7\u00e3o e que os resultados sejam \n\nrigorosos para que se atinja um excelente controle de qualidade dos pisos e revestimentos \n\ncer\u00e2micos nas ind\u00fastrias inclu\u00eddas no PCSG. \n\n \n\n4.2. Regra de Aprendizado Backpropagation (Formato B\u00e1sico) \n\n \n\nEscolhidas a rede neural artificial Perceptron de tr\u00eas camadas, a regra de aprendizado \n\nbackpropagation e o software MATLAB 7.0, o processo de aplica\u00e7\u00e3o envolveu inicialmente a \n\nregra de aprendizado na sua forma b\u00e1sica (traingd) \u00e0s tabelas de dados dos ANEXOS E, F, G, \n\nH, I, J e K. Para os dados do ANEXO E que envolve cinco vari\u00e1veis f\u00edsicas e temperatura de \n\nqueima em 1000\u00b0 C foram determinados os par\u00e2metros show = 200, lr = 0.05, epochs = 2000 \n\ne goal = 1e-5. A performance do programa que corresponde ao c\u00e1lculo do erro m\u00e9dio \n\nquadr\u00e1tico (mse) e aparece como  valor inferior ao par\u00e2metro goal foi encerrado ap\u00f3s a \n\nrealiza\u00e7\u00e3o de tr\u00eas testes, pois os resultados obtidos foram de que para N = 2 neur\u00f4nios at\u00e9 N = \n\n29 neur\u00f4nios na camada oculta n\u00e3o ocorreu nenhum ind\u00edcio de converg\u00eancia e o mse ficou \n\npreso num m\u00ednimo local (armadilha) com um valor extremamente alto igual a 55090.8 . Com \n\nN = 30, 31 e 34 neur\u00f4nios, al\u00e9m do valor  anterior, foi encontrado um segundo valor como \n\nm\u00ednimo local, tamb\u00e9m muito alto. Com N = 32, 33 e 35 neur\u00f4nios foram encontrados at\u00e9 tr\u00eas \n\nm\u00ednimos locais diferentes, sempre com valores alt\u00edssimos, e, inclusive, uma tend\u00eancia de \n\nm\u00e1ximo absoluto em completa diverg\u00eancia. \n\n\n\n56 \n \n\nPara os dados do ANEXO F (cinco vari\u00e1veis f\u00edsicas) com temperatura de queima igual \n\na 1020\u00b0 C foram utilizados os mesmos par\u00e2metros e as mesmas condi\u00e7\u00f5es. Os resultados \n\nencontrados foram de que para N = 2 at\u00e9 N = 31 neur\u00f4nios na camada oculta, o mse ficou \n\npreso no mesmo m\u00ednimo local do caso anterior (55090.8), da mesma forma que com N = 33, \n\n34 e 35 neur\u00f4nios. Quando N = 32 neur\u00f4nios ocorreu uma tend\u00eancia de diverg\u00eancia (m\u00e1ximo \n\nabsoluto). \n\nEm virtude destes resultados, para os dados das tabelas dos ANEXOS G e H, \n\nsubmetidos aos mesmos par\u00e2metros e condi\u00e7\u00f5es o processo foi aplicado somente para N = 5, \n\n10, 15, 20, 25, 30 e 35 neur\u00f4nios na camada oculta e, conforme o que se esperava, em todos \n\nos casos, o mse (goal) ficou preso no mesmo m\u00ednimo local de 55090.8, com exce\u00e7\u00e3o da tabela \n\ndo ANEXO H, onde quando N = 30 neur\u00f4nios, ocorreu novamente um resultado divergente. \n\nEm seguida, para as tabelas de dados dos ANEXOS I, J e K, agora com oito vari\u00e1veis \n\nf\u00edsicas, mas com os mesmos par\u00e2metros e condi\u00e7\u00f5es, o processo foi iniciado com os dados da \n\ntabela do ANEXO I (temperatura de queima de 1020\u00b0 C) e aplicados de N = 2 at\u00e9 N = 35 \n\nneur\u00f4nios na camada oculta, mas nada mudou, o mse ficou da mesma forma preso num \n\nm\u00ednimo local, agora com o valor igual a 38546.7, ainda muito alto, com uma \u00fanica exce\u00e7\u00e3o \n\nquando N = 34 neur\u00f4nios, onde ocorreu uma diverg\u00eancia. Com estes resultados, para os \n\noutros ANEXOS, o processo foi aplicado somente para N = 5, 10, 15, 20, 25, 30 e 35 \n\nneur\u00f4nios na camada oculta e todos indistintamente ficaram presos no mesmo m\u00ednimo local \n\nde 38546.7, com uma \u00fanica exce\u00e7\u00e3o, para N = 30 neur\u00f4nios do ANEXO J onde, novamente, \n\nocorreu a diverg\u00eancia. \n\nOnde se concluiu que depois de efetuados todos os testes para as tabelas de dados dos \n\nANEXOS E, F, G, H, I, J e K, a regra de aprendizado backpropagation no seu formato b\u00e1sico \n\nn\u00e3o permitiu a aplica\u00e7\u00e3o das redes neurais artificiais \u00e0 ind\u00fastria cer\u00e2mica, tendo em vista que \n\nem quase todos os casos o erro m\u00e9dio quadr\u00e1tico (mse) ficou sempre preso num m\u00ednimo local \n\nde valor muito alto. Houve alguns casos excepcionais, onde ao inv\u00e9s da esperada \n\nconverg\u00eancia viu-se a ocorr\u00eancia da diverg\u00eancia do processo. Isto assim colocado, este \n\nprocesso foi abandonado e, em seu lugar, foi testado uma varia\u00e7\u00e3o que, teoricamente acelera a \n\nconverg\u00eancia, que \u00e9 o backpropagation com momento dentro do pr\u00f3prio MATLAB, \n\nconforme item a seguir. \n\n \n\n\n\n57 \n \n\n4.3. Regra de Aprendizado Backpropagation com Momento (traingdm) \n\n \n\nO m\u00e9todo seguinte aplicado (com converg\u00eancia acelerada) foi o da regra de \n\naprendizado backpropagation com momento e, da mesma forma que o software sugere uma \n\ntaxa de aprendizado pequena (pr\u00f3xima de zero) para a estabilidade da rede neural, ele tamb\u00e9m \n\nsugere que o momento deve ser grande (pr\u00f3ximo de um). O valor utilizado foi mc = 0.9. \n\nAssim, para a modelagem da rede neural os par\u00e2metros utilizados foram show = 200,             \n\nlr = 0.05, mc = 0.9, epochs = 2000 e goal (mse) = 1e-5, com tr\u00eas testes para verificar se h\u00e1 \n\nalgum ind\u00edcio de que a rede neural converge. \n\nPara a tabela de dados do ANEXO E (5 vari\u00e1veis, 1000\u00b0 C) foram usados de N = 2 at\u00e9 \n\nN = 35 neur\u00f4nios na camada oculta e, sem exce\u00e7\u00e3o, todos os testes ficaram presos no mesmo \n\nm\u00ednimo local e sempre igual a mse = 55090.8, como no item anterior deste cap\u00edtulo. \u00c9 \n\ninteressante observar que no caso em que N = 34 e 35 o n\u00famero de epochs ficou entre 440 e \n\n570 e da mesma maneira o processo ficou preso na armadilha do m\u00ednimo local em dois testes \n\ndos tr\u00eas previstos. \n\nAnalogamente, para a tabela de dados do ANEXO F (5 vari\u00e1veis, 1020\u00b0 C) com N = 2, \n\n3, 4,..., 35, o resultado foi sempre o mesmo, isto \u00e9, mse = 55090.8, inclusive com o fato de \n\nque o n\u00famero de epochs n\u00e3o atingia o m\u00e1ximo previsto de 2000; ficou entre 440 e 570. \n\nCom isto, para as tabelas de dados dos ANEXOS G (5 vari\u00e1veis, 1040\u00b0 C) e H (5 \n\nvari\u00e1veis, 1060\u00b0 C), o n\u00famero de neur\u00f4nios da camada oculta foi pr\u00e9-fixado em N = 5, 10, 15, \n\n20, 25, 30 e 35 e, novamente, n\u00e3o ocorreu nenhuma tend\u00eancia de converg\u00eancia. \n\nPara a tabela de dados do ANEXO I (8 vari\u00e1veis, 1020\u00b0 C), inicialmente, com N = 2, \n\n3, 4, ..., 35 neur\u00f4nios na camada oculta n\u00e3o ocorreu nenhum ind\u00edcio de converg\u00eancia em todos \n\nos testes. Todos eles ficaram presos no mesmo m\u00ednimo local igual a mse = 38546.7 e da \n\nmesma maneira, para N = 8, 17, 18, 19, 22, 23, 25, 26, 28, 29 e 32 neur\u00f4nios o valor do erro \n\nfoi atingido com valores entre 440 e 570 epochs, n\u00e3o atingindo o m\u00e1ximo pr\u00e9-estabelecido de \n\n2000 epochs. \n\nPara as tabelas dos ANEXOS J (8 vari\u00e1veis, 1040\u00b0 C) e K (8 vari\u00e1veis, 1060\u00b0 C), o \n\nm\u00ednimo local (armadilha) atingido ficou em mse = 38546.7 quando N = 5, 10, 15, 20, 25, 30 e \n\n35 neur\u00f4nios na camada oculta. No caso dos dados do ANEXO J e N = 10, 20, 30 e 35 \n\nneur\u00f4nios e dos dados do ANEXO K com N = 25 e 35 neur\u00f4nios o m\u00ednimo local foi atingido \n\nquando o n\u00famero de epochs ficou entre 440 e 570. \n\n\n\n58 \n \n\nDesta forma, como no caso do item anterior, esta regra n\u00e3o apresentou nenhum ind\u00edcio \n\nde converg\u00eancia. Em fun\u00e7\u00e3o destes resultados, fizeram-se necess\u00e1rios a aplica\u00e7\u00e3o de \n\nprocessos com converg\u00eancia bem mais acelerada do que este processo com momento. Foram \n\nent\u00e3o selecionadas as regras de aprendizado backpropagation resiliente e de Levenberg-\n\nMarquadt, varia\u00e7\u00f5es da regra no formato b\u00e1sico, sempre dentro do MATLAB. Aqui, conv\u00e9m \n\nlembrar que Cintra (2003) aplicou a regra de Levenberg-Marquadt para o controle de teores \n\nde cobre e ouro no dep\u00f3sito de Chapada (GO); a rede se estabilizou com  N = 32 neur\u00f4nios na \n\ncamada oculta. \n\n \n\n4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp) \n\n \n\nPara a aplica\u00e7\u00e3o desta regra de aprendizado, os par\u00e2metros de controle s\u00e3o apenas tr\u00eas: \n\nshow = 200, epochs = 2000 e goal(mse) = 1e-5 e, como sempre inicialmente, foram realizados \n\ntr\u00eas testes. \n\nCom rela\u00e7\u00e3o \u00e0 tabela de dados do ANEXO E (5 vari\u00e1veis, 1000\u00b0 C), iniciou-se o \n\nprocesso com N = 2 neur\u00f4nios na camada oculta. Novamente, o processo ficou preso no \n\nmesmo m\u00ednimo local (armadilha) igual a 55090.8. Com N= 3 neur\u00f4nios, o primeiro ind\u00edcio de \n\nconverg\u00eancia surgiu, apesar de ser muito lento. Por isto, o n\u00famero de epochs foi ampliado \n\npara 5000 e o n\u00famero de testes aumentado para 5. Para N = 4 neur\u00f4nios, a tend\u00eancia de \n\nconverg\u00eancia persistiu e com isso o n\u00famero de epochs foi gradativamente sendo aumentado \n\npara 5000, 10000 e 20000, com o intuito de verificar a converg\u00eancia do processo na dire\u00e7\u00e3o \n\ndo m\u00ednimo global. O n\u00famero de testes permaneceu em 5. Mesmo assim, a converg\u00eancia \n\nmostrou-se muito lenta. Os mesmos resultados se repetiram para N = 5 e 6 neur\u00f4nios. \n\nQuando foi fixado N = 7 neur\u00f4nios na camada oculta, o n\u00famero de testes mantido em \n\n5, e em contrapartida o n\u00famero de epochs sendo aumentado gradativamente para 5000, 10000 \n\ne 20000, apesar de algumas falhas, houve converg\u00eancia r\u00e1pida com apenas 242 epochs \n\n(Figura 11). \n\n \n\n\n\n59 \n \n\nFigura 11 \u2013 Resultado encontrado para os dados do ANEXO E com N = 7 e 242 epochs. \n\n0 50 100 150 200\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n242 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 8.76838e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nA Figura 11 acima mostra que o erro m\u00e9dio quadr\u00e1tico (mse), dentro da performance \u00e9 \n\nde 8.76838e-6. A simula\u00e7\u00e3o obtida pela aplica\u00e7\u00e3o da regra de aprendizado resiliente \u00e9 \n\nrecuperada pelo comando \n\na  = sim (net,p),            (4.1) \n\nonde net significa a rede, p \u00e9 o padr\u00e3o de entrada e matriz encontrada a \u00e9 dada por \n\na = [5.5008 4.0002 7.0064 601.9992 50.9986]. \n\nPara efeito de compara\u00e7\u00e3o, o padr\u00e3o pr\u00e9-estabelecido para estas cinco vari\u00e1veis f\u00edsicas \n\nfoi fixado atrav\u00e9s da matriz: \n\nPADR\u00c3O(5) = [5.5 4 7 602 51]. \n\nDesignando por p1 a matriz do padr\u00e3o de entrada referente aos dados da tabela do \n\nANEXO E e, da mesma forma, p2 para os dados da tabela do ANEXO F, p3 para os dados da \n\ntabela do ANEXO G e p4 para os dados da tabela do ANEXO H e recuperando a matriz de \n\n\n\n60 \n \n\npesos IW, entre os neur\u00f4nios da camada de entrada e os neur\u00f4nios da camada oculta, atrav\u00e9s \n\ndo comando \n\ncelldisp(net.IW),           (4.2)  \n\na matriz de pesos LW, entre os neur\u00f4nios da camada oculta e os neur\u00f4nios da camada de \n\nsa\u00edda, atrav\u00e9s do comando \n\ncelldisp(net.LW) e           (4.3) \n\nas matrizes de limiares b1 e b2, ambas atrav\u00e9s do mesmo comando \n\ncelldisp(net.b),           (4.4)  \n\npode-se aplicar os resultados das f\u00f3rmulas dos comandos (4.2), (4.3) e (4.4) aos dados das \n\ntabelas dos outros ANEXO F, G e H com o aux\u00edlio das seguintes f\u00f3rmulas, dentro das \n\ndenomina\u00e7\u00f5es definidas no MATLAB, \n\n \n\nIW * pi + b1 * ones (1,5),                   (4.5) \n\nAi = logsig (IW *pi + b1 * ones(1,5)),                             (4.6) \n\nTi = LW * Ai + b2,                     (4.7) \n\ni = 1, 2, 3 e 4,  \n\n \n\na matriz Ai indica a aplica\u00e7\u00e3o da fun\u00e7\u00e3o de transfer\u00eancia \n\n \n\ny = logsig (x),           (4.8) \n\n \n\nna forma matricial, e Ti, \u00e9 a matriz dos padr\u00f5es de sa\u00edda. Neste caso, T1 \u00e9 o resultado para os \n\ndados da tabela do ANEXO E, T2 referente ao ANEXO F, T3 referente ao ANEXO G e T4 \n\nreferente do ANEXO H. \n\nAplicando-se as f\u00f3rmulas (4.5), (4.6) e (4.7) \u00e0s matrizes de pesos e de limiares \n\nencontradas e aos dados das tabelas dos ANEXOS E, F, G e H, chegam-se aos resultados que \n\nconstam da Tabela 5. \n\n \n\n\n\n61 \n \n\nTabela 5 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H,com N = 7 e 242 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\n      \n\na 5.5008 4.0002 7.0064 601.9992 50.9986 \n\n      \n\nT1 (1000\u00b0C) 5.4943 3.9915 6.9803 601.2047 50.8284 \n\nT2 (1020\u00b0C) 6.4059 3.8566 3.6228 296.1997 21.5664 \n\nT3 (1040\u00b0C) 6.3043 3.9856 4.4235 257.7497 12.6877 \n\nT4 (1060\u00b0C) 5.3570 4.3453 6.5846 227.7777 -4.9783 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborado pelo autor. \n\n \n\nO valor a \u00e9 o resultado da simula\u00e7\u00e3o da rede neural, com N =7 neur\u00f4nios e 242 \n\nepochs, feito pelo pr\u00f3prio software. O valor a e o valor encontrado quando a temperatura \u00e9 de \n\n1000\u00b0 C (T1), referente ao teste, est\u00e3o bem pr\u00f3ximos, mas n\u00e3o s\u00e3o os mesmos valores em \n\nvirtude da propaga\u00e7\u00e3o do erro em fun\u00e7\u00e3o dos truncamentos e arredondamentos, quando o \n\nprocesso \u00e9 calculado diretamente. No caso desta Tabela, tem-se: \n\ne(RLQ) = a(RLQ) \u2013 T1(RLQ) = 5.5008 \u2013 5.4943 = 0.0065. \n\nAnalogamente, \n\ne(PF) = 4.0002 \u2013 3.9915 = 0.0087; \n\ne(Abs) = 7.0064 \u2013 6.9803 = 0.0261; \n\ne(CR) = 601.9992 \u2013 601.2047 = 0.7945; e \n\ne(MRF) = 50.9986 \u2013 50.8284 = 0.1702. \n\n \n\n\u00c9 um erro que, infelizmente, \u00e9 inerente ao processo e n\u00e3o h\u00e1 como evit\u00e1-lo. Este \n\nresultado n\u00e3o pode ser usado em virtude dos dados das linhas 3, 4 e 5 e das colunas 5 e 6, \n\nconstantes da Tabela 5, onde apareceu at\u00e9 um valor negativo para a vari\u00e1vel f\u00edsica MRF \n\nquando a temperatura de queima \u00e9 de 1060\u00b0 C (T4). \u00c9 bom lembrar que a norma NBR 13818 \n\n(ABNT, 1997) especifica que 6 ?  Abs  &lt;10 , que CR > 500N e que MRF > 18MPa. \n\n\n\n62 \n \n\nContinuando o processo, para N = 8 e 9 neur\u00f4nios, os resultados repetiram uma \n\nconverg\u00eancia muito lenta e nada pode ser aproveitado. Com N = 10, 11,..., 22 neur\u00f4nios, o \n\nn\u00famero de testes foi aumentado para 10, enquanto que em alguns casos o n\u00famero de epochs \n\nchegou a 20000, mas nenhum resultado pode ser usado. A partir de N = 23 neur\u00f4nios na \n\ncamada oculta o m\u00e9todo apresentou novamente uma converg\u00eancia r\u00e1pida e com isso o n\u00famero \n\nde epochs para os 10 testes foi fixado em 5000. A melhor solu\u00e7\u00e3o ocorreu quando N = 34 \n\nneur\u00f4nios, com 164 epochs e performance (mse) = 4.75064e-7, cujos dados podem ser vistos \n\nna Figura 12 e constam da Tabela 6. \n\nNesta Tabela 6, o resultado inconsistente \u00e9 o valor da vari\u00e1vel f\u00edsica Abs para a \n\ntemperatura de queima em 1060\u00b0 C, pois contrariando a tend\u00eancia de queda, ultrapassou o \n\nvalor da escala de classifica\u00e7\u00e3o do grupo BIIb. \u00c9 bom observar que para N = 35 neur\u00f4nios, a \n\nconverg\u00eancia ocorreu com apenas 122 epochs, mas os resultados n\u00e3o foram convenientes, \n\ncomo pode ser visto na Tabela 7. Tamb\u00e9m, devem ser observados os erros m\u00e9dios quadr\u00e1ticos \n\n(mse). \n\n \n\nFigura 12 \u2013 Resultado encontrado para os dados do ANEXO E com N =34 e 164 epochs. \n\n0 20 40 60 80 100 120 140 160\n\n10\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n164 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \nG\n\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 4.75064e-007, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n63 \n \n\nTabela 6 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas as dados dos ANEXOS E, F, G e H, com N = 34 e 164 \nepochs. \n\nVari\u00e1veis \nF\u00edsicas \n\nRLQ \n(%) \n\nPF \n(%) \n\nAbs \n(%) \n\nCR \n(N) \n\nMRF \n(MPa) \n\n      \nT1 (1000\u00b0C) 5.5129 4.0126 7.0096 602.3708 51.2143 \n\nT2 (1020\u00b0C) 6.2948 3.5409 3.8028 513.9377 35.6558 \n\nT3 (1040\u00b0C) 6.9630 3.5255 3.2918 539.5973 44.8043 \n\nT4 (1060\u00b0C) 6.6550 3.6279 12.3915 550.7220 39.3721 \n\n      \nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nTabela 7 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 35 e 122 \nepochs. \n\nVari\u00e1veis \nF\u00edsicas \n\nRLQ \n(%) \n\nPF \n(%) \n\nAbs \n(%) \n\nCR \n(N) \n\nMRF \n(MPa) \n\nT1 (1000\u00b0C) 5.4958 3.9969  6.9857 601.9150 50.9923 \n\nT2 (1020\u00b0C) 7.7770 4.2938 4.9043 446.7413 36.9043 \n\nT3 (1040\u00b0C) 8.2346 4.2443 2.9948 452.1809 44.2455 \n\nT4 (1060\u00b0C) 7.8101 4.6059 1.7728 480.7872 38.6263 \n\nFonte: Elaborada pelo autor. \n\n \n\nEm fun\u00e7\u00e3o dos resultados anteriores, a partir da tabela de dados do ANEXO F (cinco \n\nvari\u00e1veis f\u00edsicas e temperatura de queima de 1020\u00b0 C), fixaram-se os par\u00e2metros de controle \n\ndo MATLAB em show = 200, epochs = 5000 e goal (mse) = 1e-5 e, tamb\u00e9m, o n\u00famero de \n\ntestes em 10. Com isto, para o ANEXO F, encontraram-se os dados da Tabela 8. \n\nPara analisar a estabilidade da rede, o n\u00famero de neur\u00f4nios na camada oculta foi \n\nsendo aumentado gradativamente e os resultados foram inclu\u00eddos na                     Tabela 9       \n\n(complemento da anterior). \n\n \n\n\n\n64 \n \n\nTabela 8 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO F. \nN Performance (mse) Observa\u00e7\u00f5es \n\n2 M\u00ednimo local (10 testes) 55090.8 \n\n3 M\u00ednimo local (10 testes) 55090.8 \n\n4 1 teste convergente - \n\n5 1 teste convergente - \n\n6 2 testes convergentes - \n\n7 2 testes convergentes - \n\n8 2 testes convergentes - \n\n9 4 testes convergentes  - \n\n10 6 testes convergentes - \n\n11 6 testes convergentes - \n\n12 7 testes convergentes -  \n\n13 7 testes convergentes - \n\n14 8 testes convergentes - \n\n15  10 testes convergentes N\u00famero de epochs muito alto (3997) \n\n16 8 testes convergentes - \n\n17 9 testes convergentes - \n\n18 9 testes convergentes - \n\n19  9 testes convergentes - \n\n20 10 testes convergentes N\u00famero de epochs muito alto \n\n21 10 testes convergentes N\u00famero de epochs muito alto (2259) \n\n22 10 testes convergentes N\u00famero de epochs muito alto \n\n23 10 testes convergentes N\u00famero de epochs muito alto \n\n24 10 testes convergentes  N\u00famero de epochs muito alto \n\n25 10 testes convergentes N\u00famero de epochs muito alto \n\n26  10 testes convergentes N\u00famero de epochs muito alto \n\n27 10 testes convergentes (88) \n\n28 10 testes convergentes N\u00famero de epochs muito alto \n\nFonte: Elaborada pelo autor.  \n\n\n\n65 \n \n\nTabela 9 - Complementa\u00e7\u00e3o da Tabela 8. \nN Performance Observa\u00e7\u00f5es \n\n29 10 testes convergentes N\u00famero de epochs muito alto \n\n30 10 testes convergentes N\u00famero de epochs muito alto \n\n31 10 testes convergentes N\u00famero de epochs muito alto \n\n32 10 testes convergentes N\u00famero de epochs muito alto \n\n33 9 testes convergentes (83) \n\n34 10 testes convergentes N\u00famero de epochs muito alto \n\n35 10 testes convergentes N\u00famero de epochs muito alto \n\n40 10 testes convergentes N\u00famero de epochs muito alto \n\n45 10 testes convergentes N\u00famero de epochs muito alto \n\n50 10 testes convergentes N\u00famero de epochs muito alto \n\n60 10 testes convergentes N\u00famero de epochs muito alto \n\n70 10 testes convergentes N\u00famero de epochs muito alto \n\n80 10 testes convergentes N\u00famero de epochs muito alto \n\n90 10 testes convergentes (60) \n\n100 10 testes convergentes N\u00famero de epochs muito alto \n\nFonte: Elaborada pelo autor. \n\n \n\nFicou bem evidente nas Tabelas 8 e 9 que a regra de aprendizado backpropagation \n\nresiliente \u00e9 de converg\u00eancia r\u00e1pida, pois na maioria dos casos em 10 testes foram encontrados \n\n10 testes convergentes, apesar de que na maioria das vezes o n\u00famero de epochs era muito alto \n\n(acima de 2259, no caso, pois quando N = 15 neur\u00f4nios, o menor n\u00famero de epochs foi de \n\n3997, para N =21 neur\u00f4nios, 2259 epochs, para N = 24 neur\u00f4nios, 2403 epochs e para N = 28 \n\nneur\u00f4nios, 3611 epochs). Quando foram aplicadas as f\u00f3rmulas (4.5), (4.6) e (4.7) com estes \n\ndados os resultados n\u00e3o foram nem mesmo satisfat\u00f3rios. Entretanto, quando N = 27 neur\u00f4nios \n\nna camada oculta e com 88 epochs, quando N = 33 neur\u00f4nios e 83 epochs e, finalmente, \n\nquando N =90 neur\u00f4nios e 60 epochs apareceram as melhores solu\u00e7\u00f5es e elas constam das \n\nTabelas 10, 11 e 12. As Figuras 13, 14 e 15 mostram os gr\u00e1ficos em correspond\u00eancia com as \n\nTabelas 10, 11 e 12, respectivamente. \n\n\n\n66 \n \n\nFigura 13 \u2013 Resultado encontrado para os dados do ANEXO F, com N = 27 e 88 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n88 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 3.81476e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 10 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 27 e 88 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 3.8176 4.3852 9.8580 341.7696 20.7748 \n\nT2 (1020\u00b0C) 5.4917 4.0012  6.9926 601.7553 50.8131 \n\nT3 (1040\u00b0C) 7.2374 4.0660 10.9442 598.5219 55.0452 \n\nT4 (1060\u00b0C) 7.0804 4.1405 19.0759 597.0924 55.0565 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n\n\n67 \n \n\nFigura 14 \u2013 Resultado encontrado para os dados do ANEXO F, com N = 33 e 83 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n83 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.05677e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 11 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 33 e 83 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 4.2713 4.2152 9.9552 492.0558 34.1350 \n\nT2 (1020\u00b0C) 5.4974 3.9935 6.9875 601.9989 50.9171 \n\nT3 (1040\u00b0C) 7.2452 4.0279 5.7282 607.0999 59.1992 \n\nT4 (1060\u00b0C) 8.1126 4.0150 11.0738 610.0570 54.9349 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n68 \n \n\nFigura 15 \u2013 Resultado encontrado para os dados do ANEXO F, com n = 90 e 60 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n60 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.26147e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 12 \u2013 As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 90 e 60 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 4.1489 3.6117 9.0583 572.1501 33.9442 \n\nT2 (1020\u00b0C) 5.4955 4.0015 7.0058 601.8821 50.9997 \n\nT3 (1040\u00b0C) 5.7914 3.9837 4.1276 603.5703 63.6037 \n\nT4 (1060\u00b0C) 6.2612 4.0810 1.3956 603.4972 73.9983 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nMesmo assim, fica evidente, que as Tabelas 10, 11 e 12 mostram valores que n\u00e3o \n\natendem as especifica\u00e7\u00f5es t\u00e9cnicas da ABNT, apesar de terem sido obtidos como sendo os \n\nmelhores resultados dos testes. \n\n\n\n69 \n \n\nCom rela\u00e7\u00e3o aos dados da tabela do ANEXO G (5 vari\u00e1veis f\u00edsicas e temperatura de \n\nqueima de 1040\u00b0C), foram mantidos os mesmos par\u00e2metros de controle do MATLAB, ou \n\nseja, show = 200, epochs = 5000 e goal (mse) = 1e-5. O processo foi aplicado inicialmente \n\npara N = 5, 10, 15, 20, 25, 30 e 35 neur\u00f4nios na camada oculta, cujos resultados s\u00e3o \n\nmostrados na Tabela 13. \n\n \n\nTabela 13 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO G. \n\nN Performance (mse) Observa\u00e7\u00f5es \n5 M\u00ednimo local 55090.8 \n10 M\u00ednimo local 55090.8 \n15 M\u00ednimo local 55090.8 \n20 6 testes convergentes N\u00famero de epochs muito alto \n25 10 testes convergentes N\u00famero de epochs muito alto \n30 9 testes convergentes N\u00famero de epochs muito alto \n35 10 testes convergentes N\u00famero de epochs muito alto \n\nFonte: Elaborada pelo autor. \n\n \nEm virtude dos resultados nada satisfat\u00f3rios mostrados pela Tabela 13, foi realizada \n\numa complementa\u00e7\u00e3o para testar a estabilidade da rede neural e os dados constam da Tabela \n\n14. \n\n \nTabela 14 \u2013 Complementa\u00e7\u00e3o dos dados da Tabela 13. \n\nN Performance (mse) Observa\u00e7\u00f5es \n\n21 9 testes convergentes N\u00famero de epochs muito alto \n\n22 8 testes convergentes N\u00famero de epochs muito alto \n\n23 9 testes convergentes N\u00famero de epochs muito alto \n\n24 10 testes convergentes N\u00famero de epochs muito alto \n\n26 10 testes convergentes N\u00famero de epochs muito alto \n\n27 10 testes convergentes (85) \n\n28 8 testes convergentes N\u00famero de epochs muito alto \n\n29 10 testes convergentes N\u00famero de epochs muito alto \n\n31 9 testes convergentes N\u00famero de epochs muito alto \n\n32 10 testes convergentes N\u00famero de epochs muito alto \n\n33 10 testes convergentes (82) \n\n34 10 testes convergentes (73) \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n70 \n \n\nAs melhores solu\u00e7\u00f5es aconteceram quando N = 27 neur\u00f4nios com 85 epochs, quando \n\nN = 33 neur\u00f4nios com 82 epochs e quando N = 34 com 73 epochs. Novamente, quando o \n\nn\u00famero de epochs \u00e9 muito alto, os resultados n\u00e3o permitem as suas aplica\u00e7\u00f5es no processo, \n\npor isso, os resultados anteriores foram considerados os melhores. As Tabelas 15, 16 e 17 \n\nmostram os dados encontrados quando da aplica\u00e7\u00e3o das f\u00f3rmulas (4.5), (4.6) e (4.7) \u00e0s tabelas \n\nde dados dos ANEXOS E, F, G e H. Em correspond\u00eancia aos dados destas Tabelas, s\u00e3o \n\nmostradas as Figuras 16, 17 e 18. \n\n \n\nFigura 16 - Resultado encontrado para os dados do ANEXO G, com N= 27 e 85 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n85 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 3.51056e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n71 \n \n\nTabela 15 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 27 e 85 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 3.3854 4.5142 9.9596 580.4700 39.6761 \n\nT2 (1020\u00b0C) 3.9784 4.0617 5.2503 596.7161 44.6927 \n\nT3 (1040\u00b0C) 5.5046 4.0054 7.0062 601.9684 51.0574 \n\nT4 (1060\u00b0C) 6.1106 4.0444 12.5082 603.4387 52.6749 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 17 - Resultado encontrado para os dados do ANEXO G, com N = 33 e 82 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n82 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 6.92398e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n72 \n \n\nTabela 16 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 33 e 82 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 5.3218 2.9787 29.6079 601.0522 16.5579 \n\nT2 (1020\u00b0C) 9.3116 3.7019 17.2727 597.8778 29.0875 \n\nT3 (1040\u00b0C) 5.4951 3.9957 6.9866 602.0411 50.6939 \n\nT4 (1060\u00b0C) 0.4765 3.5444 7.9963 602.7652 93.1350 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborado pelo autor. \n\n \n\nFigura 18 - Resultado encontrado para os dados do ANEXO G, com N = 34 e 73 epochs. \n\n0 10 20 30 40 50 60 70\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n73 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 7.39822e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n73 \n \n\nTabela 17 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 34 e 73 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\nT1 (1000\u00b0C) 2.3421 3.1560 30.2824 469.8984 16.1915 \n\nT2 (1020\u00b0C) 6.1257 3.7758 15.6994 599.6521 48.3919 \n\nT3 (1040\u00b0C) 5.4791 4.0018 6.9984 602.0983 50.7668 \n\nT4 (1060\u00b0C) 6.4976 4.0933 0.1437 601.9720 101.8092 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborado pelo autor. \n\n \n\nDentre as Tabelas anteriores, \u00e9 vis\u00edvel que o melhor resultado ocorreu com os dados \n\nda Tabela 15. Os dados das outras duas Tabelas est\u00e3o, em alguns casos, bem afastados dos \n\nvalores especificados em norma da ABNT. \n\nNeste momento, chega-se ao ANEXO H que exibe cinco vari\u00e1veis f\u00edsicas e \n\ntemperatura de queima de 1060\u00b0 C. Os par\u00e2metros de controle permanecem com os mesmos \n\nvalores, ou seja, show = 200, epochs = 5000 e goal (mse) = 1e-5. Da mesma maneira com      \n\nN = 5, 10, 15, 20, 25, 30 e 35 foram efetuados 10 testes, conforme Tabela 18. \n\n \n\nTabela 18 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO H. \nN Performance (mse) Observa\u00e7\u00f5es \n\n5 M\u00ednimo local 55090.8 \n\n10 M\u00ednimo local 55090.8 \n\n15 4 testes convergentes Muito lenta \n\n20 4 testes convergentes Muito lenta \n\n25 5 testes convergentes Muito lenta \n\n30 8 testes convergentes Lenta \n\n35 7 testes convergentes Lenta \n\nFonte: Elaborado pelo autor. \n\n \n\nEm fun\u00e7\u00e3o dos ind\u00edcios de converg\u00eancia do processo, a Tabela 18 foi complementada \n\ncom a Tabela 19, a seguir. \n\n\n\n74 \n \n\nTabela 19 \u2013 Complementa\u00e7\u00e3o dos dados da Tabela 18. \nN Performance (mse) Observa\u00e7\u00f5es \n\n36 9 testes convergentes  \n\n37 7 testes convergentes  \n\n38 10 testes convergentes (94), (215) \n\n39 8 testes convergentes  \n\n40 9 testes convergentes (118), (126), (127) \n\nFonte: Elaborada pelo autor. \n\n \n\nOs resultados encontrados est\u00e3o expostos nas Tabelas 20, 21, 22, 23, 24 e 25 e, em \n\ncorrespond\u00eancia, os erros m\u00ednimos quadr\u00e1ticos (mse) junto \u00e0s Figuras 19, 20, 21, 22, 23 e 24. \n\n \n\nFigura 19 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 215 epochs. \n\n0 20 40 60 80 100 120 140 160 180 200\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n215 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 7.12127e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n75 \n \n\nTabela 20 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 38 e 215 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.4978 4.0028 6.9992 602.0013 51.0046 \n\nT1 (1000\u00b0C) -1.4555 3.4268 14.7742 15.5033 1.8843 \n\nT2 (1020\u00b0C) 0.0025 4.1779 18.7575 596.9313 8.7448 \n\nT3 (1040\u00b0C) 2.8703 4.3912 14.3653 596.4031 6.8872 \n\nT4 (1060\u00b0C) 5.4825 3.9943 6.9943 601.5592 50.9025 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 20 - Resultado encontrado para os dados do ANEXO H, com N = 38 e 94 epochs. \n\n0 10 20 30 40 50 60 70 80 90\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n94 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.44677e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n76 \n \n\nTabela 21 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 38 e 94 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.5026 3.9951 7.0000 602.0000 51.0040 \n\nT1 (1000\u00b0C) 3.5936 4.0055 6.4765 489.6407 21.5601 \n\nT2 (1020\u00b0C) 4.7546 3.9940 6.1944 537.3637 32.5325 \n\nT3 (1040\u00b0C) 5.4814 3.8803 5.0144 582.0974 44.3654 \n\nT4 (1060\u00b0C) 5.5026 3.9953 6.9990 601.6696 50.9433 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 21 - Resultado encontrado para os dados do ANEXO H, com N =40 e 127 epochs. \n\n0 20 40 60 80 100 120\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n127 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 5.56055e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n77 \n \n\nTabela 22 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 127 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.4990 4.0050 7.0012 602.0003 51.0005 \n\nT1 (1000\u00b0C) 4.1497 3.9542 6.8460 541.4200 20.0360 \n\nT2 (1020\u00b0C) 4.7328 4.0004 4.6428 586.6600 32.9680 \n\nT3 (1040\u00b0C) 5.6274 4.0113 5.6135 599.3200 45.1340 \n\nT4 (1060\u00b0C) 5.5071 4.0079 7.0021 602.2700 51.2140 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 22 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 126 epochs. \n\n0 20 40 60 80 100 120\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n126 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 5.23196e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n\n\n78 \n \n\nTabela 23 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 126 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.4997 4.0040 6.9970 601.9994 50.9991 \n\nT1 (1000\u00b0C) 3.7022 3.8332 7.3212 599.5800 18.8490 \n\nT2 (1020\u00b0C) 4.4276 3.9802 5.8040 602.0100 32.0970 \n\nT3 (1040\u00b0C) 5.0879 3.9885 4.7899 602.0100 35.6620 \n\nT4 (1060\u00b0C) 5.5001 4.0043 6.9975 602.0000 51.1200 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 23 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 118 epochs. \n\n0 20 40 60 80 100\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n118 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 4.02038e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n\n\n79 \n \n\nTabela 24 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 118 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.5026 3.9968 6.9996 602.0003 51.0016 \n\nT1 (1000\u00b0C) 6.0240 4.6738 8.0256 433.9165 29.3281 \n\nT2 (1020\u00b0C) 6.1732 4.1264 3.6558 314.9238 9.6733 \n\nT3 (1040\u00b0C) 6.3421 3.8372 3.2981 556.7295 35.5169 \n\nT4 (1060\u00b0C) 5.4954 4.0008 7.0008 602.1945 50.9625 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 24 - Resultado encontrado para os dados do ANEXO H, com N = 40 e 78 epochs.  \n\n0 10 20 30 40 50 60 70\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n78 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 8.99296e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n \n\n\n\n80 \n \n\nTabela 25 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS E, F, G e H, com N = 40 e 78 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 5.5063 4.0001 6.9978 601.9992 50.9997 \n\nT1 (1000\u00b0C) 4.7132 4.3448 5.3693 547.1919 21.5839 \n\nT2 (1020\u00b0C) 5.2488 4.0439 3.4622 502.2522 25.9255 \n\nT3 (1040\u00b0C) 5.7853 4.0347 4.0992 585.9482 41.8421 \n\nT4 (1060\u00b0C) 5.5167 4.0045 7.0017 602.0606 51.0640 \n\n      \n\nPADR\u00c3O 5 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nEm resumo, com cinco vari\u00e1veis f\u00edsicas, como constam das tabelas de dados dos \n\nANEXOS E, F, G e H, os melhores resultados ocorreram para a temperatura de queima de \n\n1000\u00b0 C, Tabela 6 (Figura 12), quando N = 34 neur\u00f4nios  com 164 epochs, entretanto, tr\u00eas \n\nvalores da vari\u00e1vel Abs ficaram fora dos limites preconizados pela ABNT (6 ?  Abs &lt;10). Na  \n\ntemperatura de queima de 1020\u00b0 C , Tabela 11 (Figura 14), a melhor solu\u00e7\u00e3o apresentada pela \n\nrede neural com o m\u00e9todo resiliente surgiu quando N = 33 e 83 epochs, apesar de que dois \n\nvalores da vari\u00e1vel Abs ficaram fora dos limites citado anteriormente e de que um valor da \n\nvari\u00e1vel CR ficou abaixo de 500N. Com a temperatura de queima em 1040\u00b0 C, Tabela 15 \n\n(Figura 17), encontraram-se somente dois valores da vari\u00e1vel Abs fora dos limites j\u00e1 citado e, \n\nneste caso, pode-se dizer que a resposta foi relativamente boa porque as vari\u00e1veis CR e MRF \n\nencontradas, ambas, est\u00e3o na ordem crescente. E, finalmente, com a temperatura de queima \n\nem 1060\u00b0 C, Tabela 22 (Figura 21), ocorreu a melhor de todas as solu\u00e7\u00f5es quando N = 40 \n\nneur\u00f4nios e 126 epochs, tendo em vista que os desvios entre a simula\u00e7\u00e3o a e a resposta \n\nfornecida pela rede T4 foram muito pequenos, as respostas da rede para a vari\u00e1vel Abs \n\napareceram no sentido decrescente, com exce\u00e7\u00e3o do \u00faltimo dado em T4, (os valores da \n\nvari\u00e1vel ficaram bem pr\u00f3ximos), enquanto que os valores da vari\u00e1vel MRF, apesar de estarem \n\nafastados, da mesma forma que a vari\u00e1vel CR se apresentaram dentro dos limites impostos \n\npela norma NBR 13818 (ABNT, 1997) e no sentido crescente. Com estes resultados pode-se \n\nconcluir que a melhor resposta da rede neural artificial, com a regra de aprendizado \n\nbackpropagation resiliente, para as cinco vari\u00e1veis RLQ, PF, Abs, CR e MRF, foi obtida \n\nquando a temperatura de queima foi de 1060\u00b0 C, a maior de todas, o que valida o processo \n\npara todos os anexos com cinco vari\u00e1veis. \n\n\n\n81 \n \n\nIsto colocado, em sequ\u00eancia continuou-se com a aplica\u00e7\u00e3o do processo para as tabelas \n\nde dados dos ANEXOS I, J e K, agora com oito vari\u00e1veis f\u00edsicas, que s\u00e3o: DP, DAS, RLS, \n\nRLQ, PF, Abs, CR e MRF. As temperaturas de sinteriza\u00e7\u00e3o s\u00e3o tr\u00eas: 1020\u00b0 C, 1040\u00b0 C e \n\n1060\u00b0 C, respectivamente, para cada um dos anexos. \n\nPelos ind\u00edcios de converg\u00eancia do processo resiliente nos quatro casos j\u00e1 tratados, os \n\npar\u00e2metros de controle continuam com os mesmos valores, ou seja, show = 200, epochs = \n\n5000 e goal (mse) = 1e-5. Continuam, tamb\u00e9m, os testes num total de 10 e o n\u00famero de \n\nneur\u00f4nios na camada oculta para a temperatura de queima de 1020\u00b0 C ser\u00e1 N = 2, 3, 4,..., 35 e \n\npara as outras duas temperaturas de queima, a an\u00e1lise ser\u00e1 feita, inicialmente, com N = 5, 10, \n\n15, 20, 25, 30 e 35 neur\u00f4nios. \n\nCome\u00e7ando com a tabela de dados do ANEXO I, a aplica\u00e7\u00e3o do m\u00e9todo resiliente \n\nforneceu os dados da Tabela 26. \n\n \n\n\n\n82 \n \n\nTabela 26 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO I. \nN Performance (mse) Observa\u00e7\u00f5es \n2 V\u00e1rios m\u00ednimos locais Desiguais \n3 1 teste convergente - \n4 V\u00e1rios m\u00ednimos locais Desiguais \n5 V\u00e1rios m\u00ednimos locais Desiguais \n6 V\u00e1rios m\u00ednimos locais Desiguais \n7 V\u00e1rios m\u00ednimos locais Desiguais \n8 1 teste convergente - \n9 2 testes convergentes - \n\n10 V\u00e1rios m\u00ednimos locais Desiguais \n11 V\u00e1rios m\u00ednimos locais Desiguais \n12 V\u00e1rios m\u00ednimos locais Desiguais \n13 1 teste convergente - \n14 2 testes convergentes - \n15 2 testes convergentes - \n16 V\u00e1rios m\u00ednimos locais Desiguais \n17 V\u00e1rios m\u00ednimos locais Desiguais \n18 4 testes convergentes - \n19 2 testes convergentes N\u00famero de epochs muito alto \n20 3 testes convergentes N\u00famero de epochs muito alto \n21 3 testes convergentes N\u00famero de epochs muito alto \n22 2 testes convergentes N\u00famero de epochs muito alto \n23 3 testes convergentes - \n24 V\u00e1rios m\u00ednimos locais Desiguais \n25 3 testes convergentes - \n26 3 testes convergentes - \n27 4 testes convergentes - \n28 3 testes convergentes - \n29 2 testes convergentes - \n30 6 testes convergentes - \n31 3 testes convergentes - \n32 3 testes convergentes - \n33 5 testes convergentes - \n34 3 testes convergentes - \n35 4 testes convergentes - \n\nFonte: Elaborada pelo autor. \n\n \n\nH\u00e1 converg\u00eancia, mas com muitas falhas e com um n\u00famero de epochs muito alto, por \n\nisto esta Tabela foi complementada com a seguinte Tabela 27. \n\n \n\n \n\n\n\n83 \n \n\nTabela 27 \u2013 Complementa\u00e7\u00e3o dos dados da Tabela 26. \nN Performance (mse)  Observa\u00e7\u00f5es \n\n36  6 testes convergentes N\u00famero de epochs muito alto \n\n37 5 testes convergentes N\u00famero de epochs muito alto \n\n38 4 testes convergentes N\u00famero de epochs muito alto \n\n39 6 testes convergentes N\u00famero de epochs muito alto \n\n40 4 testes convergentes N\u00famero de epochs muito alto \n\n41 5 testes convergentes N\u00famero de epochs muito alto \n\n42 6 testes convergentes N\u00famero de epochs muito alto \n\n43 5 testes convergentes N\u00famero de epochs muito alto \n\n44 7 testes convergentes N\u00famero de epochs muito alto \n\n45 5 testes convergentes N\u00famero de epochs muito alto \n\n50 5 testes convergentes N\u00famero de epochs muito alto \n\n55 5 testes convergentes N\u00famero de epochs muito alto \n\n56 7 testes convergentes (64) \n\n57 6 testes convergentes N\u00famero de epochs muito alto \n\n58 8 testes convergentes (158) \n\n59 8 testes convergentes N\u00famero de epochs muito alto \n\n60 10 testes convergentes (263), (269) \n\n61 8 testes convergentes N\u00famero de epochs muito alto \n\n62 6 testes convergentes N\u00famero de epochs muito alto \n\nFonte: Elaborado pelo autor. \n\n \n\nForam selecionados os melhores resultados como sendo aqueles que ocorreram \n\nquando N = 56 neur\u00f4nios na camada oculta com 64 epochs, quando N = 58 neur\u00f4nios com \n\n158 epochs e quando N = 60 neur\u00f4nios com 263 e 269 epochs. Eles constituem as Tabelas 28, \n\n29, 30 e 31. As f\u00f3rmulas (4.5), (4.6) e (4.7) podem ser usadas agora com i = 1, 2 e 3. Em \n\nsequ\u00eancia, as Figuras 25, 26, 27 e 28 correspondentes aos dados das Tabelas 28, 29, 30 e 31. \n\n \n\n\n\n84 \n \n\nFigura 25 - Resultado encontrado para os dados do ANEXO I, com N = 56 e 64 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n64 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \nG\n\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.04311e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 28 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 56 e 64 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9962 1.9063 0.3006 5.4959 4.0002 7.0001 601.9990 51.0000 \n\nT1 (1020\u00b0C) 2.0005 1.9102 0.3007 5.5018 4.0099 7.0311 601.7692 51.1454 \n\nT2 (1040\u00b0C) 1.9994 1.9104 0.3460 6.8791 4.1394 5.0437 605.7800 59.1417 \n\nT3 (1060\u00b0C) 2.0037 1.9147 0.2936 7.4347 4.1704 2.2264 607.8878 64.8783 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborado pelo autor. \n\n \n\n\n\n85 \n \n\nFigura 26 - Resultado encontrado para os dados do ANEXO I, com N = 58 e 158 epochs. \n\n0 50 100 150\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n158 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.70568e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 29 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 58 e 158 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0052 1.8937 0.3000 5.4980 4.0023 6.9986 602.0000 51.0002 \n\nT1 (1020\u00b0C) 2.0062 1.8947 0.3001 5.5012 4.0022 7.0027 601.9548 51.0176 \n\nT2 (1040\u00b0C) 2.0100 1.9111 0.3287 6.9740 4.0842 4.7241 601.9124 62.5422 \n\nT3 (1060\u00b0C) 2.0185 1.9173 0.2894 7.5741 4.0828 3.0673 583.6256 68.3699 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n\n\n86 \n \n\nFigura 27 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 263 epochs.  \n\n0 50 100 150 200 250\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n263 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.91424e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 30 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 263 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0070 1.8947 0.3004 5.4988 3.9994 7.0000 602.0000 51.0003 \n\nT1 (1020\u00b0C) 1.9989 1.8872 0.2995 5.4776 3.9852 6.9785 601.8094 50.7172 \n\nT2 (1040\u00b0C) 1.9983 1.8591 0.2901 5.9603 3.8205 5.9671 579.4804 64.9740 \n\nT3 (1060\u00b0C) 2.0018 1.8705 0.3239 5.8842 4.0053 4.2970 571.7003 85.8344 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n87 \n \n\nFigura 28 - Resultado encontrado para os dados do ANEXO I, com N = 60 e 269 epochs. \n\n0 50 100 150 200 250\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n269 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.95808e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 31 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 269 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0066 1.8940 0.3002 5.5000 4.0000 6.9998 602.0000 51.0001 \n\nT1 (1020\u00b0C) 2.0054 1.8928 0.2997 5.4953 3.9968 6.9973 601.5843 50.9632 \n\nT2 (1040\u00b0C) 2.0059 1.8857 0.2903 6.7786 3.9716 3.9624 600.7590 67.0975 \n\nT3 (1060\u00b0C) 2.0107 1.8934 0.3022 7.6396 4.1245 1.8284 589.5212 99.2379 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nCom rela\u00e7\u00e3o aos dados da tabela do ANEXO J (8 vari\u00e1veis f\u00edsicas e temperatura de \n\nqueima de 1040\u00b0 C), continuou-se com show = 200, epochs = 5000, goal (mse) = 1e-5, 10 \n\ntestes e N = 5, 10, 15, 20, 25, 30 e 35, onde foram encontrados os valores da Tabela 32. \n\n \n\n\n\n88 \n \n\nTabela 32 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO J. \nN  Performance (mse) Observa\u00e7\u00f5es \n\n5  1 teste convergente - \n\n10 1 teste convergente - \n\n15 V\u00e1rios m\u00ednimos locais Desiguais \n\n20 1 teste convergente - \n\n25 6 testes convergentes - \n\n30 2 testes convergentes - \n\n35 3 testes convergentes - \n\nFonte: Elaborada pelo autor. \n\n \n\nHouve necessidade de complementa\u00e7\u00e3o devido aos ind\u00edcios de converg\u00eancia e os \n\ndados constam da Tabela 33. \n\n \n\nTabela 33 \u2013 Complementa\u00e7\u00e3o dos dados da Tabela 32. \nN  Performance (mse) Observa\u00e7\u00f5es \n\n40 6 testes convergentes - \n\n45 6 testes convergentes - \n\n50 6 testes convergentes - \n\n55 8 testes convergentes - \n\n60 7 testes convergentes (80) \n\n61 7 testes convergentes - \n\n62 10 testes convergentes (74), (81) \n\n63 9 testes convergentes (66) \n\n64 4 testes convergentes - \n\n65 9 testes convergentes (75), (167) \n\n66 8 testes convergentes - \n\n67 9 testes convergentes - \n\nFonte; Elaborada pelo autor. \n\n \n\nOs resultados foram compensadores quando N = 60 neur\u00f4nios na camada oculta e com \n\n80 epochs; analogamente, para N = 62 neur\u00f4nios com 74 e 81 epochs, N = 63 neur\u00f4nios com \n\n66 epochs e N= 65 neur\u00f4nios com 75 e 167 epochs. Eles constam das Tabelas 34, 35, 36, 37, \n\n38 e 39, enquanto as performances podem ser vistas atrav\u00e9s das Figuras 29, 30, 31, 32, 33 e \n\n34. \n\n \n\n \n\n \n\n\n\n89 \n \n\nFigura 29 - Resultado encontrado para os dados do ANEXO J, com N = 60 e 80 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n80 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.49536e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 34 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 60 e 80 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0066 1.8949 0.3005 5.5014 3.9990 6.9985 601.9999 51.0001 \n\nT1 (1020\u00b0C) 2.0082 1.9004 0.3111 4.2524 3.9722 11.6430 601.6900 43.3420 \n\nT2 (1040\u00b0C) 2.0072 1.8954 0.3010 5.5052 3.9991 6.9931 602.0100 51.0660 \n\nT3 (1060\u00b0C) 2.0118 1.9005 0.2938 6.6704 4.0913 2.7039 601.0700 59.7200 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n\n\n90 \n \n\nFigura 30 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 74 epochs. \n\n0 10 20 30 40 50 60 70\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n74 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.99068e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 35 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 62 e 74 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0068 1.8965 0.3011 5.4992 3.9963 7.0025 601.9999 51.0001 \n\nT1 (1020\u00b0C) 2.0106 1.9011 0.3219 4.4838 4.0869 6.8584 596.5225 43.1884 \n\nT2 (1040\u00b0C) 2.0102 1.8997 0.3019 5.5122 4.0058 7.0106 602.8015 51.2447 \n\nT3 (1060\u00b0C) 2.0190 1.9102 0.3302 6.0188 4.0890 4.1550 562.3329 64.5364 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n\n\n91 \n \n\nFigura 31 - Resultado encontrado para os dados do ANEXO J, com N = 62 e 81 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n81 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.00362e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 36 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 62 e 81 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0016 1.8980 0.2996 5.4975 4.0054 7.0054 601.9999 51.0001 \n\nT1 (1020\u00b0C) 2.0082 1.9035 0.3304 4.0838 3.9501 9.7248 599.7006 44.4462 \n\nT2 (1040\u00b0C) 2.0090 1.9049 0.3004 5.5243 4.0201 7.0245 602.0086 51.4463 \n\nT3 (1060\u00b0C) 2.0149 1.9137 0.3390 6.3542 4.0528 4.2268 602.4022 60.6326 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n92 \n \n\nFigura 32 - Resultado encontrado para os dados do ANEXO J, com N = 63 e 66 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n66 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 4.12249e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 37 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 63 e 66 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0025 1.9001 0.3034 5.5032 3.9981 6.9986 602.0000 50.9999 \n\nT1 (1020\u00b0C) 2.0010 1.9005 0.3052 3.8416 3.9267 10.8099 601.7305 41.6519 \n\nT2 (1040\u00b0C) 2.0013 1.8990 0.3032 5.5010 3.9936 6.9879 602.0000 50.9413 \n\nT3 (1060\u00b0C) 2.0037 1.9022 0.3006 6.6933 4.0041 3.6489 595.6186  64.3518 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n93 \n \n\nFigura 33 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 75 epochs. \n\n0 10 20 30 40 50 60 70\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n75 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 7.93299e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\nTabela 38 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 75 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9999 1.9029 0.2991 5.5055 3.9976 7.0041 601.9993 50.9999 \n\nT1 (1020\u00b0C) 2.0043 1.9150 0.3395 3.8566 4.0272 11.3280 582.1600 39.4020 \n\nT2 (1040\u00b0C) 1.9998 1.9028 0.2988 5.5059 3.9965 7.0027 603.2200 50.9320 \n\nT3 (1060\u00b0C) 2.0108 1.9132 0.3398 6.9754 4.0909 1.5772 618.0100 86.4140 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n94 \n \n\nFigura 34 - Resultado encontrado para os dados do ANEXO J, com N = 65 e 167 epochs. \n\n0 20 40 60 80 100 120 140 160\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n167 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.96187e-006, Goal is 1e-005\n\n \nFonte : MATLAB\u00ae 7.0. \n\n \n\nTabela 39 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 167 \nepochs. \n\nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9932 1.9056 0.2998 5.5001 4.0010 7.0002 602.0000 51.0000 \n\nT1 (1020\u00b0C) 1.9830 1.8752 0.3076 3.8657 3.6533 10.5070 599.0200 42.6860 \n\nT2 (1040\u00b0C) 1.9926 1.9052 0.2999 5.4961 4.0017 7.0030 602.6100 50.9790 \n\nT3 (1060\u00b0C) 2.0014 1.9084 0.2950 7.4494 3.7336 3.5958 578.0500 54.4640 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFinalmente, para os dados da tabela do ANEXO K (8 vari\u00e1veis f\u00edsicas e temperatura \n\nde sinteriza\u00e7\u00e3o de 1060\u00b0 C), usaram-se os mesmos valores show = 200, epochs = 5000 e goal \n\n(mse) = 1e-5, 10 testes e N = 5, 10, 15, 20, 25, 30 e 35. Quando da aplica\u00e7\u00e3o do m\u00e9todo \n\nresiliente foram encontrados os valores que constam da Tabela 40. \n\n \n\n\n\n95 \n \n\nTabela 40 \u2013 Aplica\u00e7\u00e3o do m\u00e9todo resiliente aos dados do ANEXO K. \nN Performance (mse)  Observa\u00e7\u00f5es \n\n5 V\u00e1rios m\u00ednimos locais Desiguais \n\n10 1 teste convergente - \n\n15 3 testes convergentes - \n\n20 2 testes convergentes - \n\n25 4 testes convergentes - \n\n30 4 testes convergentes - \n\n35 8 testes convergentes - \n\nFonte: Elaborada pelo autor. \n\n \n\nEm fun\u00e7\u00e3o dos ind\u00edcios de converg\u00eancia, houve a necessidade de complementa\u00e7\u00e3o, \n\nconforme TABELA 41. \n\n \n\nTabela 41 \u2013 Complementa\u00e7\u00e3o dos dados da Tabela 40. \nN Performance (mse) Observa\u00e7\u00f5es \n\n40 6 testes convergentes - \n\n45 6 testes convergentes  - \n\n50 7 testes convergentes - \n\n55 8 testes convergentes (76), (92) \n\n60 7 testes convergentes - \n\n65 9 testes convergentes (65) \n\n66 7 testes convergentes - \n\n67 10 testes convergentes (69), (97) \n\n68 9 testes convergentes - \n\n69 9 testes convergentes - \n\n70  10 testes convergentes (81) \n\n71 8 testes convergentes (60) \n\n72 9 testes convergentes - \n\n73 9 testes convergentes - \n\n74 9 testes convergentes - \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n96 \n \n\nOs resultados mostraram-se \u00fateis quando N = 55 neur\u00f4nios na camada oculta, com 76 \n\ne 92 epochs, quando N = 65 neur\u00f4nios com 65 epochs, quando N = 67 neur\u00f4nios com 69 e 97 \n\nepochs, quando N = 70 neur\u00f4nios com 81 epochs e quando N = 71 neur\u00f4nios com 60 epochs. \n\nNas observa\u00e7\u00f5es, espa\u00e7o reservado nestas tabelas, nada foi colocado porque o n\u00famero de \n\nepochs era muito alto; o mesmo aconteceu em todas as tabelas anteriores. Estas solu\u00e7\u00f5es da \n\nrede neural constam das Tabelas 42, 43, 44, 45, 46, 47 e 48  em correspond\u00eancia com as \n\nFiguras 35, 36, 37, 38, 39, 40 e 41. \n\n \n\nFigura 35 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 76 epochs. \n\n0 10 20 30 40 50 60 70\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n76 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.17852e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n97 \n \n\nTabela 42 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 55 e 76 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9946 1.9045 0.2990 5.4992 4.0043 6.9981 601.9998 51.0001 \n\nT1 (1020\u00b0C) 1.9897 1.8735 0.3225 3.2483 3.9584 9.6778 599.8834 35.1371 \n\nT2 (1040\u00b0C) 1.9910 1.8985 0.3356 4.2641 3.9230 8.7417 601.8029 40.2199 \n\nT3 (1060\u00b0C) 1.9939 1.9039 0.2985 5.4968 3.9999 6.9905 601.9747 51.0167 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 36 - Resultado encontrado para os dados do ANEXO K, com N = 55 e 92 epochs. \n\n0 10 20 30 40 50 60 70 80 90\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n92 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.36375e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n\n\n98 \n \n\nTabela 43 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 55 e 92 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9957 1.9063 0.2975 5.5018 3.9977 7.0009 602.0000 50.9977 \n\nT1 (1020\u00b0C) 1.9568 1.8129 0.3608 1.5532 3.9402 11.1489 545.1705 -10.0082 \n\nT2 (1040\u00b0C) 1.9731 1.8988 0.3144 2.9577 4.0003 8.8076 473.5459 -13.1180 \n\nT3 (1060\u00b0C) 1.9936 1.9045 0.2973 5.4953 3.9947 6.9984 601.9981 50.6856 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 37 - Resultado encontrado para os dados do ANEXO K, com N = 65 e 65 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n65 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 3.27321e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n\n\n99 \n \n\nTabela 44 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 65 e 65 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9977 1.9005 0.2997 5.4997 4.0007 7.0004 602.0045 50.9999 \n\nT1 (1020\u00b0C) 1.9878 1.8885 0.3090 3.5373 4.0022 5.8352 588.9179 39.2767 \n\nT2 (1040\u00b0C) 1.9914 1.8932 0.4073 4.7389 4.0610 6.1922 599.2251 44.6545 \n\nT3 (1060\u00b0C) 1.9972 1.9001 0.2994 5.4961 3.9981 6.9995 602.4032 50.9507 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 38 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 69 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n69 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 4.16672e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n100 \n \n\nTabela 45 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 67 e 69 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 2.0043 1.8998 0.3028 5.5011 3.9989 6.9981 602.0006 50.9998 \n\nT1 (1020\u00b0C) 1.9987 1.8922 0.3055 3.2714 3.9207 12.0186 591.9261 38.5242 \n\nT2 (1040\u00b0C) 2.0070 1.8964 0.3493 4.1951 4.0123 9.9698 600.2105 45.7710 \n\nT3 (1060\u00b0C) 2.0045 1.9000 0.3028 5.4998 3.9985 7.0007 601.8002 50.9753 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 39 - Resultado encontrado para os dados do ANEXO K, com N = 67 e 97 epochs. \n\n0 10 20 30 40 50 60 70 80 90\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n97 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.06144e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n\n\n101 \n \n\nTabela 46 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 67 e 97 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9935 1.9052 0.2998 5.5002 4.0008 7.0014 602.0000 51.0000 \n\nT1 (1020\u00b0C) 1.9906 1.8988 0.2768 6.1225 3.9056 7.2003 574.5857 55.2187 \n\nT2 (1040\u00b0C) 1.9943 1.9087 0.3497 6.2407 4.0325 7.6008 597.0215 59.8810 \n\nT3 (1060\u00b0C) 1.9947 1.9063 0.2999 5.019 4.0016 7.0037 602.3042 51.0358 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 40 - Resultado encontrado para os dados do ANEXO K, com N = 70 e 81 epochs. \n\n0 10 20 30 40 50 60 70 80\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n81 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.61698e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n\n\n102 \n \n\nTabela 47 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 70 e 81 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9938 1.9062 0.2994 5.5002 4.0000 6.9999 602.0000 51.0000 \n\nT1 (1020\u00b0C) 1.9875 1.8856 0.2838 4.8097 3.9479 6.5638 596.6497 39.0613 \n\nT2 (1040\u00b0C) 1.9872 1.8999 0.2849 5.5000 3.9549 6.6073 600.6902 45.8705 \n\nT3 (1060\u00b0C) 1.9931 1.9055 0.2986 5.4944 3.9982 7.0027 602.0094 50.8983 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nFigura 41 - Resultado encontrado para os dados do ANEXO K, com N = 71 e 60 epochs. \n\n0 10 20 30 40 50 60\n10\n\n-6\n\n10\n-4\n\n10\n-2\n\n10\n0\n\n10\n2\n\n10\n4\n\n60 Epochs\n\nT\nra\n\nin\nin\n\ng\n-B\n\nlu\ne\n  \n\nG\no\na\nl-\nB\n\nla\nc\nk\n\nPerformance is 9.06948e-006, Goal is 1e-005\n\n \nFonte: MATLAB\u00ae 7.0. \n\n \n\n \n\n\n\n103 \n \n\nTabela 48 - As f\u00f3rmulas (4.5), (4.6) e (4.7) aplicadas aos dados dos ANEXOS I, J e K, com N = 71 e 60 epochs. \nVari\u00e1veis \n\nF\u00edsicas \n\nDP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF \n\n(%) \n\nAbs \n\n(%) \n\nCR \n\n(N) \n\nMRF \n\n(MPa) \n\na 1.9971 1.8997 0.3018 5.4945 4.0016 6.9947 602.0001 50.9999 \n\nT1 (1020\u00b0C) 1.9751 1.8621 0.3109 3.6305 4.1029 5.4755 601.6643 35.2209 \n\nT2 (1040\u00b0C) 1.9810 1.8857 0.3811 4.7575 4.1775 6.5362 601.7453 44.0534 \n\nT3 (1060\u00b0C) 1.9960 1.8988 0.3015 5.4909 4.0006 6.9935 602.0735 50.9426 \n\n         \n\nPADR\u00c3O 8 2 1.9 0.3 5.5 4 7 602 51 \n\nFonte: Elaborada pelo autor. \n\n \n\nResumindo, os resultados para as tabelas de dados dos ANEXOS I, J e K que melhor \n\ncorresponderam, \u00e0s aplica\u00e7\u00f5es da regra de aprendizado backpropagation resiliente, foram \n\npara a temperatura de queima de 1020\u00b0 C, os dados constantes da Tabela 30 (Figura 27), para \n\na temperatura de queima de 1040\u00b0 C, os dados que constam da Tabela 36 (Figura 31) e para a \n\ntemperatura de queima de 1060\u00b0 C, os dados apresentados na Tabela 47 (Figura 40). Destas \n\ntr\u00eas solu\u00e7\u00f5es, o melhor resultado surgiu para os dados correspondentes \u00e0 temperatura de \n\nqueima de 1060\u00b0 C do ANEXO K e, novamente, para a maior temperatura de sinteriza\u00e7\u00e3o e \n\ndemonstra definitivamente a valida\u00e7\u00e3o do processo. \n\n \n\n4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm) \n\n \n\nNa aplica\u00e7\u00e3o da regra de aprendizado backpropagation de Levenberg-Marquadt os \n\npar\u00e2metros de controle do MATLAB s\u00e3o apenas tr\u00eas: show = 200, epochs = 5000 e goal = 1e-\n\n5, com no caso do m\u00e9todo anterior. Foram realizados 10 testes para verificar a estabilidade e a \n\nconverg\u00eancia do m\u00e9todo. Iniciou-se o processo com o n\u00famero de neur\u00f4nios na camada oculta, \n\nN = 2, 3,..., 25, acrescidos dos valores N = 30 e 35, para os dados da tabela do ANEXO E, \n\nonde as vari\u00e1veis f\u00edsicas s\u00e3o cinco e a temperatura de queima de 1000\u00b0 C. \n\nApesar da grande expectativa neste processo ele apresentou em todos os valores de N \n\numa converg\u00eancia extremamente lenta, exigindo, no m\u00ednimo 40000 epochs para se chegar a \n\num valor satisfat\u00f3rio, al\u00e9m de demandar um tempo razo\u00e1vel de utiliza\u00e7\u00e3o em um computador \n\npessoal. Para se mostrar uma estimativa da lentid\u00e3o da converg\u00eancia, quando N = 22 \n\nneur\u00f4nios, a cada 200 epochs a queda no valor do mse (erro m\u00e9dio quadr\u00e1tico) era da ordem \n\n\n\n104 \n \n\nde 2 em 44000 e quando N = 23 neur\u00f4nios, com 200 epochs o valor do mse era de 51064.8, \n\nenquanto que com 5000 epochs, o valor do mse era ligeiramente menor e igual a 51059.1, \n\nmostrando que, neste caso, o n\u00famero de epochs, para se alcan\u00e7ar um m\u00ednimo global na \n\nvizinhan\u00e7a de zero, deveria ser de ordem aproximada de 43000000 (quarenta e tr\u00eas milh\u00f5es!). \n\nOutra situa\u00e7\u00e3o inesperada quando da aplica\u00e7\u00e3o deste processo foi o fato de que em \n\nalgumas passagens as matrizes de pesos ou de limiares alcan\u00e7aram valores muito pr\u00f3ximos de \n\natingirem determinante nulo, o que leva a resultados n\u00e3o confi\u00e1veis, ou ainda, valores ruins de \n\nescala. \n\nPara as tabelas de dados dos ANEXOS F, G, H, I, J e K, os dez testes foram aplicados \n\ncom N = 5, 10, 15, 20, 25, 30 e 35 neur\u00f4nios na camada oculta e observou-se em todos os \n\ncasos a persist\u00eancia da converg\u00eancia extremamente lenta, o que n\u00e3o permite uma aplica\u00e7\u00e3o \n\nr\u00e1pida deste processo \u00e0 ind\u00fastria de pisos e revestimentos cer\u00e2micos do PCSG. Dessa forma, \n\neste m\u00e9todo foi abandonado. Para a sua aplica\u00e7\u00e3o os dados do padr\u00e3o de entrada devem ser \n\nnormalizados de maneira que eles fiquem contidos dentro do intervalo real [0,1]. Como, no \n\nentanto, o m\u00e9todo resiliente atendeu ao objetivo geral deste trabalho e mostrou tamb\u00e9m que a \n\nhip\u00f3tese, integrante deste trabalho enunciada no cap\u00edtulo 1, \u00e9 verdadeira, este trabalho foi \n\nencerrado, n\u00e3o obstante, sabendo que al\u00e9m destes m\u00e9todos, o pr\u00f3prio software MATLAB \n\napresenta v\u00e1rios outros m\u00e9todos, varia\u00e7\u00f5es do m\u00e9todo backpropagation.  \n\n \n\n4.6. Conclus\u00f5es  \n\n \n\nO objetivo geral deste trabalho foi alcan\u00e7ado na medida em que foi encontrada a rede \n\nneural artificial direta de tr\u00eas camadas chamada perceptron (rede MLP: Multilayer \n\nPerceptron, do original em ingl\u00eas), sendo uma camada de entrada (padr\u00e3o de entrada), uma \n\ncamada intermedi\u00e1ria ou oculta e uma camada de sa\u00edda (ou padr\u00e3o de sa\u00edda). No caso, o \n\npadr\u00e3o de entrada (pi) foram todas as tabelas de dados dos ANEXOS E, F, G, H, I, J e K,  o \n\npadr\u00e3o de sa\u00edda para as cinco vari\u00e1veis f\u00edsicas foi fixado pelo vetor linha \n\nPADR\u00c3O (5) = [5.5 4 7 602 51], \n\ne o padr\u00e3o de sa\u00edda para as oito vari\u00e1veis f\u00edsicas foi estabelecido por outro vetor linha  \n\nPADR\u00c3O (8) = [2 1.9 0.3 5.5 4 7 602 51]. \n\n\n\n105 \n \n\nA diferencia\u00e7\u00e3o entre uma rede e outra foi condicionada ao n\u00famero de neur\u00f4nios da \n\ncamada oculta que, na maioria dos casos, era de N = 2, 3, 4,..., 35 ou, somente, os m\u00faltiplos \n\nde 5. Em alguns casos,  atingiu o valor N = 100 na busca pela estabilidade e pela \n\nconverg\u00eancia. \n\nDentre quatro varia\u00e7\u00f5es da regra de aprendizado delta generalizada (ou simplesmente \n\nbackpropagation), todas inclu\u00eddas no MATLAB, aquela que atendeu ao objetivo geral foi a \n\nregra resiliente. Com a rede MLP e a regra de aprendizado backpropagation resiliente, o \n\nobjetivo geral foi alcan\u00e7ado. \n\nCom rela\u00e7\u00e3o \u00e0 hip\u00f3tese deste trabalho, resta apresentar as matrizes de pesos e de \n\nlimiares encontradas, as quais permitem  a aplica\u00e7\u00e3o das redes neurais artificiais para um \n\ncontrole mais rigoroso da qualidade dos produtos industriais dos pisos e revestimentos \n\ncer\u00e2micos das ind\u00fastrias do PCSG. Esta \u00e9 uma das grandes vantagens da aplica\u00e7\u00e3o das redes \n\nneurais, pois leva em considera\u00e7\u00e3o que o padr\u00e3o de sa\u00edda pode ser determinado com bastante \n\nprecis\u00e3o. Os melhores resultados foram alcan\u00e7ados tanto para as cinco vari\u00e1veis f\u00edsicas como \n\npara as oito vari\u00e1veis sempre na maior temperatura de queima, isto \u00e9, 1060\u00b0 C. A Tabela 23 e \n\nem correspond\u00eancia a Figura 22 mostram os dados para cinco vari\u00e1veis. Nesta Tabela, as \n\nvari\u00e1veis RLQ, PF, CR e MRF s\u00e3o crescentes, com uma ligeira igualdade da vari\u00e1vel CR, \n\nenquanto a vari\u00e1vel Abs \u00e9 decrescente com exce\u00e7\u00e3o daquele dado referente \u00e0 temperatura de \n\nqueima de 1060\u00b0 C. \n\nA matriz de peso IW \u00e9 de ordem 40 X 16, por isso ela foi dividida em blocos. Assim, \n\n ?\n?\n\n?\n?\n?\n\n?\n?\n\n43\n\n21\n\nBB\n\nBB\nIW ,           (4.9) \n\nonde cada bloco Bi, i = 1, 2, 3 e 4, \u00e9 de ordem 20 X 8. As outras matrizes LW, b1 e b2 s\u00e3o de \n\nmais f\u00e1cil manipula\u00e7\u00e3o e n\u00e3o necessitam da divis\u00e3o em blocos. Os blocos B1, B2, B3 e B4 \n\nconstam, respectivamente, das Tabelas 49, 50, 51 e 52. \n\n\n\n106 \n \n\nTabela 49 \u2013 Bloco B1 da matriz IW, f\u00f3rmula (4.9). \n2.4010 2.4007 2.3999 2.3982 2.4052 2.4007 0.4302 2.4034 \n\n5.0451 5.0454 5.0428 5.0438 5.0399 5.0452 0.8628 5.0402 \n\n0.3729 1.9929 1.9954 1.9946 0.3751 0.3710 0.3761 0.3778 \n\n-0.0153 -0.0145 -0.0180 -0.0163 -0.0139 -0.0143 -0.0152 -0.0135 \n\n-2.5245 -2.5292 -2.5264 -2.5276 -2.5261 -2.5280 -2.5291 -2.5279 \n\n-5.5401 -5.5385 -5.5382 -5.5433 -5.5424 -5.5426 -5.5385 -5.5425 \n\n1.9493 1.8100 1.8130 1.8107 1.9526 1.9553 1.8105 1.9526 \n\n-2.2676 -2.2696 -2.2669 -2.2695 -2.2675 -2.2719 -2.2729 -2.2719 \n\n1.7399 1.6031 1.6024 1.5999 1.7405 1.7464 1.6008 1.7460 \n\n-0.5408 -0.0603 -0.0220 -0.0278 -0.1075 -0.3273 -0.0228 -0.4687 \n\n-0.0074 -0.0021 -0.0025 -0.0045 -0.0043 -0.0015 -0.0054 -0.0213 \n\n-4.9680 -4.8266 -4.8222 -4.8291 -4.9740 -4.9699 04.8268 -4.9675 \n\n0.2961 1.5459 1.5437 1.5459 0.2943 1.5494 1.5416 0.2966 \n\n0.3636 0.3674 2.1730 2.1679 0.3634 0.3654 2.1749 0.3661 \n\n-0.4748 -0.0630 -0.0266 -0.0252 -0.1046 -0.3166 -0.0174 -0.4672 \n\n4.8215 1.0123 1.0118 1.0086 4.9629 4.8209 1.0135 4.8235 \n\n0.0048 -0.0009 0.0027 0.0038 -0.0003 -0.0037 0.0033 -0.0016 \n\n-3.2759 -0.3557 -0.3512 -0.3544 -3.1361 -3.1315 -0.3529 -3.2741 \n\n5.5207 5.5170 5.5141 5.5202 5.5166 5.5187 0.9450 5.5171 \n\n-0.5194 -2.8481 -2.8479 -2.8487 -0.5232 -0.5192 -2.8516 -0.5254 \n\nFonte: Elaborada pelo autor. \n\n\n\n107 \n \n\nTabela 50 \u2013 Bloco B2 da matriz IW, f\u00f3rmula (4.9). \n2.3994 2.4019 0.4275 2.3991 2.4006 0.4339 2.3997 0.4319 \n\n5.0425 5.0411 0.8651 5.0450 5.0453 0.8585 5.0405 0.8646 \n\n0.3710 0.3729 1.9963 0.3766 0.3711 1.9979 0.3723 1.9981 \n\n-0.0174 -0.0189 -0.0194 -0.0175 -0.0128 -0.0139 -0.0186 -0.0179 \n\n-2.5305 -2.5266 -2.5232 -2.5308 -2.5292 -2.5246 -2.5283 -2.5293 \n\n-5.5441 -5.5438 -0.8338 -5.5426 -5.5453 -0.8364 -5.5384 -0.8306 \n\n1.9536 1.9516 1.8124 1.9544 1.8055 0.1719 1.8057 1.8093 \n\n-2.2715 -2.2716 -2.2673 -2.2719 -2.2721 -2.2732 -2.2708 -2.2723 \n\n1.7458 1.7428 1.6038 1.7455 1.7433 1.5977 1.7415 1.5974 \n\n-0.4738 -0.1026 -0.0191 -0.0114 0.0790 0.1872 0.0327 -0.0179 \n\n-0.0144 -0.0495 -0.0064 -0.0039 -0.0039 -0.0170 -0.0078 -0.0081 \n\n-4.9719 -4.9741 -0.6902 -4.8265 -4.8285 -0.6905 -4.8299 -0.6903 \n\n0.2980 0.2978 1.5482 0.2940 0.2920 1.5411 1.5462 1.5444 \n\n0.3675 0.3635 2.1704 0.3686 0.3689 2.1708 0.3650 2.1725 \n\n-0.4733 -0.1108 -0.0193 -0.0090 -0.0661 0.1890 0.0782 -0.0166 \n\n4.8229 4.9648 1.0531 4.9613 4.9671 1.0487 4.9619 1.0491 \n\n-0.0030 -0.0038 -0.0030 -0.0044 0.0014 -0.0027 -0.0006 0.0039 \n\n-3.2755 -3.1301 -0.3513 -0.4003 -0.3512 -0.3566 -0.3575 -0.3513 \n\n5.5197 5.5204 0.9435 5.5198 5.5177 0.9433 5.5183 0.9477 \n\n-0.5254 -0.5255 -2.8488 -0.5191 -0.5184 -2.8466 -2.8487 -2.8474 \n\nFonte: Elaborada pelo autor. \n\n\n\n108 \n \n\nTabela 51 - Bloco B3 da matriz IW, f\u00f3rmula (4.9). \n1.5436 1.5428 1.5451 1.5490 1.5429 1.5434 1.5481 1.5449 \n\n-2.6653 -0.5142 -0.5189 -0.5149 -2.6640 -2.6638 -0.5133 -2.6700 \n\n0.3491 0.3559 0.3504 0.3522 0.3556 0.3507 0.3552 0.3561 \n\n-1.7977 -0.3644 -0.3675 -0.3635 -1.7999 -1.7943 -0.3695 -1.7984 \n\n1.4411 1.5799 1.5831 1.5843 1.4409 1.4376 1.5846 1.4404 \n\n-2.6065 -2.6068 -2.6012 -2.6008 -2.5992 -2.6039 -2.6021 -2.6032 \n\n-2.8453 -2.8458 -2.8507 -2.8473 -2.8500 -2.8507 -2.8465 -2.8492 \n\n2.1115 2.1092 2.1106 2.1091 2.1133 2.1136 2.1091 2.1122 \n\n2.1129 2.1095 2.1149 2.1078 2.1115 2.1131 2.1084 2.1135 \n\n5.9267 0.8535 0.8523 0.8535 5.7781 5.9222 0.8590 5.9261 \n\n-3.5899 -3.5904 -3.4528 -3.4495 -3.5894 -3.5879 -3.4460 -3.5876 \n\n2.9366 2.9402 2.9357 2.9375 2.9405 2.9425 2.8013 2.9372 \n\n0.3628 0.0203 -0.0470 -0.0160 0.3748 0.2588 -0.0523 0.2883 \n\n0.2292 -0.1327 -0.1281 -0.1324 0.0889 0.2265 -0.1772 0.2292 \n\n-0.0088 -0.0082 -0.0075 -0.0098 0.0159 -0.0095 -0.0075 -0.0030 \n\n6.4154 0.9455 0.9440 0.9410 6.2712 6.4123 0.9378 6.4186 \n\n0.0009 -0.0011 -0.0028 0.0001 0.0042 -0.0011 -0.0017 -0.0025 \n\n-0.6008 0.0825 0.1032 0.1059 -0.3547 -0.3710 0.2448 -0.3691 \n\n-0.9250 -0.9287 -0.9236 -0.9263 -0.9287 -0.9255 -0.9302 -0.9235 \n\n0.1192 -0.0668 -0.0962 -0.0678 -0.0212 0.0645 -0.0967 1.8221 \n\nFonte: Elaborada pelo autor. \n\n\n\n109 \n \n\nTabela 52 \u2013 Bloco B4 da matriz IW, f\u00f3rmula (4.9). \n1.5443 1.5490 1.5492 1.5483 1.5454 1.5460 1.5440 1.5475 \n\n-2.6657 -2.6667 -0.4795 -2.6667 -0.5174 -0.4821 -2.6682 -0.4848 \n\n0.3541 0.3536 0.3532 0.3527 0.3525 0.0599 0.3516 0.0576 \n\n-1.7985 -1.8008 -0.3313 -1.7941 -0.3655 -0.3299 -1.7969 -0.3345 \n\n1.4354 1.4384 1.5809 1.4359 1.4409 1.5781 1.4397 1.5788 \n\n-2.6046 -2.6040 -2.5982 -2.6034 -2.6024 -2.6003 -2.6039 -2.6052 \n\n-0.5249 -2.8464 -2.8472 -2.8478 -2.8450 -2.8455 -2.8517 -2.8502 \n\n2.1139 2.1100 2.1148 2.1072 2.1147 2.1115 2.1129 2.1093 \n\n2.1096 2.1094 2.1094 2.1136 2.1104 2.1143 2.1082 2.1142 \n\n5.9231 5.7805 0.8171 5.7807 5.7805 0.8181 5.7816 0.8138 \n\n-3.5928 -3.5909 -3.4493 -3.5927 -3.5881 -3.4459 -3.5908 -3.4444 \n\n2.9413 2.9398 0.3829 2.9394 2.9386 0.3787 2.9405 0.3790 \n\n0.2875 0.2428 -0.2852 0.3118 0.1188 -0.0872 0.0907 -0.2876 \n\n0.2368 0.0831 -0.1715 0.0871 -0.1317 -0.1790 -0.1325 -0.1717 \n\n-0.0018 0.0170 -0.0066 0.0196 -0.0071 0.1382 -0.0028 -0.0041 \n\n6.4135 6.2756 0.9012 6.2740 6.2715 0.8998 6.2720 0.8970 \n\n0.0064 -0.0023 -0.0008 0.0027 0.0030 0.0071 -0.0008 0.0024 \n\n-0.3735 -0.3588 0.1458 -0.2037 -0.1372 0.1530 -0.1427 0.1444 \n\n-0.9246 -0.9250 -0.9273 -0.9288 -0.9295 -0.9312 -0.9240 -0.9252 \n\n1.8224 0.2247 -0.1825 -0.0024 -0.0262 -0.2320 -0.0280 -0.1754 \n\nFonte: Elaborada pelo autor. \n\n \n\nA seguir, s\u00e3o fornecidas as matrizes LW (vetor linha), b1 (vetor coluna) e b2 (vetor linha), \n\n \n\nLW =[10.6507 -1.6113 -8.0526 -12.1897 1.0834 4.6068 1.5350 0.7478 -8.2536 1.5361           \n\n-9.6545 5.2655 -8.0779 1.6610 1.4393 2.5157 -1.3052 3.5816 -2.5204 1.0854 -8.4389 1.0781 \n\n0.5190 0.5594 6.4596 1.0913 1.0803 -7.5189 -7.5213 -0.1267 2.2879 -9.5623 -5.9951 -2.4220 \n\n575.5548 1.6027 4.5744 2.8892 1.2824 -1.0867], \n\n \n\nTb1  = [11.1182 -6.7321 3.3964 10.0635 -11.8536 -4.5271 4.4333 2.6438 4.4016 -0.0967 \n\n1.8429 -3.1783 3.5161 5.1964 -0.1652 -0.8158 -3.9578 -1.4680 -7.6699  -4.6255 -2.6878         \n\n-3.9224 -4.0715 -1.5730 -1.3933 -2.4638 -4.5618 0.9091 2.2839 2.6582 -6.5349 3.7130 \n\n5.6143 4.4312 -8.6249 2.7774 1.7058 -3.0905 -3.5703 1.6306],  \n\n\n\n110 \n \n\ne b2 = [65.7042 65.7042 65.7042 65.7042 65.7042], \n\n \n\nonde o s\u00edmbolo T indica matriz transposta. \n\n \n\nPara a aplica\u00e7\u00e3o s\u00e3o usadas as f\u00f3rmulas (4.5), (4.6) e (4.7), repetidas abaixo: \n\nIW * pi + b1 * ones(1,5), \n\nAi = logsig(IW * pi + b1 * ones(1,5)), \n\nTi = LW * Ai + b2, \n\nonde i = 1, 2, 3 e 4, pi \u00e9 o padr\u00e3o de entrada, ones(1,5) = [1 1 1 1 1], Ti \u00e9 o padr\u00e3o de sa\u00edda e \n\nb2 \u00e9 a matriz de limiar, de ordem 1 X 5, com o valor encontrado 65.7042 repetido 5 vezes. \n\n \n\nCom rela\u00e7\u00e3o \u00e0 mesma temperatura de queima de 1060\u00b0 C, agora com oito vari\u00e1veis, a \n\nsolu\u00e7\u00e3o foi decidida entre dois resultados, aqueles fornecidos pelas Tabelas 46 e 47. Para isso, \n\nfoi calculado o desvio entre o valor mostrado pela Tabela, designado por vt e o valor do \n\npadr\u00e3o de resposta esperado, PADR\u00c3O (8), ou de forma simplificada P(8). Em valor \n\nabsoluto, o desvio \u00e9 obtido da express\u00e3o. \n\nd = | vt \u2013 P(8) |,                     (4.10)  \n\ne, transformado em porcentagem, pela f\u00f3rmula \n\nD = .100*\n)8(P\n\nd\n                     (4.11) \n\nOs resultados foram inclu\u00eddos nas Tabelas 53 e 54. \n\n \n\nTabela 53 \u2013 C\u00e1lculo dos desvios, D(%), para os valores da Tabela 46. \nD(%) DP DAS RLS RLQ PF  Abs CR MRF \n\n1020 0.47 0.06 7.73 11.32 2.36 2.86 4.55 8.27 \n\n1040 0.29 0.46 16.57 13.47 0.81 8.58 0.83 17.41 \n\n1060 0.27 033 0.03 0.04 0.04 0.05 0.05 0.07 \n\nFonte: Elaborada pelo autor. \n\n \n\n\n\n111 \n \n\nTabela 54 \u2013 C\u00e1lculo dos desvios, D(%), para os valores da Tabela 47. \nD(%) DP  DAS RLS RLQ PF Abs CR MRF \n\n1020 0.63 0.76 5.40 12.55 1.30 6.23 0.89 23.41 \n\n1040 0.64 0.01 5.03 0 1.13 5.61 0.22 10.06 \n\n1060 0.35 0.29 0.47 0.10 0.05 0.04 0 0.20 \n\nFonte: Elaborada pelo autor. \n \n\nComo a norma NBR 13818 (ABNT, 1997) especifica as vari\u00e1veis f\u00edsicas Abs, CR e \n\nMRF, a escolha recaiu sobre os dados da Tabela 47 combinado com os dados da Tabela 54, \n\nporque esta Tabela suplantou em seis dos nove resultados os dados da Tabela 53, com desvios \n\nmenores. Conv\u00e9m observar que a Tabela 54 apresentou somente um valor, D = 23.41%, \n\nrelativamente alto com rela\u00e7\u00e3o aos outros valores encontrados. \n\nCom isto colocado, a matriz de peso IW ficou sendo de ordem 70 X 20 e, tamb\u00e9m, foi \n\ndividida em blocos, conforme a matriz abaixo: \n\n,\n\n65\n\n43\n\n21\n\n?\n?\n?\n\n?\n\n?\n\n?\n?\n?\n\n?\n\n?\n?\n\nBB\n\nBB\n\nBB\n\nIW                      (4.12) \n\nonde B1 e B2 s\u00e3o de ordem 20 X 10, enquanto B3, B4, B5 e B6 s\u00e3o de ordem 25 X 10. As \n\noutras matrizes n\u00e3o necessitam da divis\u00e3o em blocos. Os blocos B1, B2, B3, B4, B5 e B6 \n\nconstam, respectivamente, das Tabelas 55, 56, 57, 58, 59 e 60. \n\n \n\n\n\n112 \n \n\nTabela 55 \u2013 Bloco B1 da matriz IW, f\u00f3rmula (4.12). \n0.0404 0.0204 0.0106 0.0021 0.0220 0.0141 -0.0058 0.0436 0.0108 0.0429 \n\n-0.0594 -0.0569 -0.0556 -0.0629 -0.0551 -0.0792 -0.0771 -0.0640 -0.0750 -0.0743 \n\n0.3525 0.3484 0.3494 0.2949 0.3544 0.3163 0.3063 0.3538 0.3063 0.3067 \n\n-0.0578 -0.0565 -0.0566 -0.0560 -0.0564 -0.0812 -0.0771 -0.0622 -0.0747 -0.0749 \n\n0.0317 0.0322 0.0312 0.0258 0.0272 0.0246 0.0250 0.0293 0.0263 0.0268 \n\n-0.2090 -0.3408 -0.3393 -0.1198 -0.3426 -0.4490 -0.1168 -0.2099 -0.6812 -0.2055 \n\n-0.5425 0.0348 0.0324 -0.0047 0.0353 -1.0021 -0.0036 -0.4455 -0.0087 -0.7400 \n\n0.0009 -0.0018 -0.0037 -0.0097 -0.0030 -0.0041 -0.0036 0.0008 -0.0012 -0.0037 \n\n-0.0570 -0.623 -0.0542 -0.0561 -0.0545 -0.0768 -0.0783 -0.0628 -0.0794 -0.0786 \n\n0.0015 -0.0006 0.0041 0.0040 0.0001 0.0006 -0.0017 0.0009 0.0025 0.0029 \n\n-0.0581 -0.0591 -0.0550 -0.0156 -0.0557 -0.0952 -0.1083 -0.0605 -0.1129 -0.0794 \n\n0.0483 0.0714 0.0704 0.0289 0.0690 0.0467 0.0407 0.0422 0.0713 0.0495 \n\n0.0022 0.0021 0.0030 -0.0087 0.0005 -0.0010 0.0031 -0.0015 0.0023 0.0006 \n\n0.4360 -0.0434 -0.0414 -0.0343 -0.0397 0.0546 -0.1238 0.3675 -0.0923 0.3569 \n\n-0.0573 -0.0572 -0.0590 -0.0572 -0.0543 -0.0786 -0.0775 -0.0589 -0.0730 -0.0769 \n\n0.0505 0.0239 0.0198 -0.1151 0.0222 0.0269 0.0189 0.0331 0.0232 0.0512 \n\n0.1041 0.1493 0.1481 0.0937 0.0791 0.0987 0.0527 0.0971 0.1184 0.0941 \n\n0.5310 0.1815 -0.0512 -0.0391 -0.0513 0.1170 -0.0284 0.3094 0.0005 0.3092 \n\n-2.7032 -0.2043 -0.2061 -0.0889 -0.2059 -0.3154 -0.0955 -2.7081 -0.5518 -2.7056 \n\n0.0031 0.0008 -0.0001 0.0048 0.0019 -0.0015 0.0021 -0.0023 0.0004 0.0023 \n\nFonte: Elaborada pelo autor. \n\n\n\n113 \n \n\nTabela 56 \u2013 Bloco B2 da matriz IW, f\u00f3rmula (4.12). \n\n0.0440 0.0196 -0.0081 0.0407 0.0182 -0.0081 0.0167 0.0025 0.0040 -0.0106 \n\n-0.0770 -0.0772 -0.0629 -0.0584 -0.0643 -0.1217 -0.0584 -0.0724 -0.1127 -0.1029 \n\n0.2809 0.3078 0.3539 0.3486 0.3540 0.1847 0.3478 0.3481 0.2586 0.1555 \n\n-0.0756 -0.0785 -0.0601 -0.0628 -0.0644 -0.1224 -0.0595 -0.0736 -0.1091 -0.1093 \n\n0.0230 0.0235 0.0230 0.0279 0.0291 0.0103 0.0279 0.0234 0.0127 0.0904 \n\n-0.2049 -0.3415 -0.1136 -0.3411 -0.3386 -0.1198 -0.3378 -0.1148 -0.1183 -0.8532 \n\n-0.4486 -1.0019 -0.0039 0.0297 0.0325 -0.0018 0.0306 -0.0022 -0.0062 -0.1491 \n\n0 -0.0037 -0.0034 -0.0027 -0.0020 0.0036 0.0008 0.0072 0.0039 -0.0013 \n\n-0.0775 -0.0755 -0.0727 -0.0611 -0.0614 -0.1232 -0.0572 -0.0754 -0.1093 -0.1207 \n\n0.0017 -0.0007 0.0045 -0.0011 -0.0016 -0.0016 0.0041 0.0033 0.0003 -0.0009 \n\n-0.0754 -0.0780 -0.0945 -0.0578 0.0568 -0.1478 -0.0605 -0.1094 -0.1323 -0.0182 \n\n0.0478 0.0469 0.0426 0.0435 0.0451 0.0452 0.0747 0.0414 0.0362 0.0167 \n\n-0.0008 0.0007 0.0025 0.0023 -0.0024 -0.0002 -0.0037 0.0032 -0.0115 -0.0139 \n\n0.3824 0.2023 -0.0322 0.2530 -0.0442 -0.0158 -0.1059 -0.0311 -0.0197 -0.0075 \n\n-0.0757 -0.0783 -0.0597 -0.0603 -0.0650 -0.1252 -0.0588 -0.0781 -0.1093 -0.1081 \n\n0.2251 0.0192 -0.1166 -0.0124 0.0226 -0.1207 0.0207 -0.1152 -0.1109 -0.0779 \n\n0.0995 0.0940 0.1032 0.1181 0.1211 0.0971 0.1213 0.0990 0.1047 0.0919 \n\n0.3121 0.1681 -0.0346 0.1664 0.1705 -0.0391 0.1677 -0.0404 -0.0355 -0.0270 \n\n-2.7048 -0.2034 -0.0922 -0.2034 -0.2036 -0.0923 -0.2026 -0.0972 -0.0944 -0.7171 \n\n0.0029 0.0025 0.0037 -0.0016 -0.0019 -0.0030 0.0007 0.0003 0.0050 -0.0004 \n\nFonte: Elaborada pelo autor \n\n\n\n114 \n \n\nTabela 57 \u2013 Bloco B3 da matriz IW, f\u00f3rmula (4.12). \n0.0677 0.0286 0.0297 -0.0374 0.0253 -0.0654 -0.0574 0.0725 -0.0602 0.0718 \n\n0.0028 0.0003 0.0029 -0.0036 0.0007 0.0051 -0.0157 -0.0090 0.0025 0.0022 \n\n-0.1352 -0.1920 -0.1937 -0.0416 -0.1928 -0.1350 -0.1579 -0.1355 -0.1951 -0.1320 \n\n-0.1710 -0.1838 -0.1863 -0.0498 -0.1831 -0.1663 -0.1934 -0.1714 -0.1945 -0.1698 \n\n-0.0270 -0.0277 -0.0328 0.0088 -0.0309 -0.0288 0.0163 -0.0267 -0.0289 -0.0253 \n\n0.0333 0.0166 0.0149 0.0071 0.0162 0.0156 0.0004 0.0273 0.0122 0.0275 \n\n0.0874 0.0694 0.0689 0.0120 0.0660 -0.1259 0.0274 0.0875 0.0316 0.0917 \n\n-0.2085 -0.3440 -0.3390 -0.1147 -0.3408 -0.4480 -0.1144 -0.2087 -0.1072 -0.2092 \n\n0.0044 0.0018 0.0018 -0.0068 0.0026 0.0013 -0.0002 -0.0003 0.0024 0.0021 \n\n0.0008 0.0019 0.0037 0.0030 0.0022 -0.0022 -0.0003 0.0036 -0.0016 -0.0017 \n\n0.3863 -0.0416 -0.0387 -0.0260 -0.0405 0.0597 -0.0303 0.3818 -0.1332 0.3588 \n\n0.2928 0.2937 0.2963 0.2106 0.2439 0.2944 0.2457 0.2971 0.2937 0.2933 \n\n-0.0288 -0.0254 -0.0196 -0.0200 -0.0228 -0.0273 -0.0285 -0.0262 -0.0244 -0.0289 \n\n-0.0765 -0.0276 -0.0243 -0.0550 -0.0290 0.0384 -0.0442 -0.0754 -0.0479 0.0280 \n\n-0.0619 -0.6360 -0.0538 -0.0590 -0.0595 -0.0772 -0.0749 -0.0611 -0.0756 -0.0774 \n\n0.1078 0.1478 0.1439 0.0997 0.0788 0.0983 0.0508 0.0984 0.1238 0.0978 \n\n-0.0371 -0.0247 -0.0283 -0.0175 -0.0239 -0.0416 -0.0222 -0.0351 -0.0252 -0.0359 \n\n-0.0562 -0.0541 -0.0532 -0.1135 -0.0584 -0.0666 -0.1189 -0.0550 -0.0620 -0.0605 \n\n0.3238 0.1774 0.1781 0.0256 0.1827 0.1857 0.1016 0.3189 0.0246 0.3644 \n\n0.0125 0.0014 -0.0285 -0.0202 -0.0228 0.0120 -0.0242 0.0072 -0.0273 0.0126 \n\n0.0752 0.0718 0.0697 0.0281 0.0763 0.0544 0.0174 0.0716 0.0432 0.0642 \n\n-0.0598 -0.0576 -0.0610 -0.0553 -0.0564 -0.0780 -0.0750 -0.0621 -0.0752 -0.0762 \n\n-0.0001 0.0022 0.0038 -0.0022 0.0013 -0.0026 -0.0019 0.0011 0.0026 -0.0018 \n\n-0.0454 -0.0119 -0.0145 -0.0063 -0.0122 -0.0161 0.0111 -0.0408 -0.0133 -0.0414 \n\n0.3129 0.2564 0.2590 0.0766 0.2625 2.7096 0.0739 0.3121 0.4481 0.3139 \n\nFonte: Elaborada pelo autor. \n\n\n\n115 \n \n\nTabela 58 \u2013 Bloco B4 da matriz IW, f\u00f3rmula (4.12). \n0.0702 -0.0615 -0.0567 -0.0655 -0.0622 -0.0580 -0.0674 -0.0585 -0.0714 -0.0260 \n\n0.0010 0.0023 -0.0131 0.0061 0.0065 -0.0017 0.0050 -0.0020 -0.0034 0.0022 \n\n-0.1347 -0.1316 -0.1567 -0.1350 -0.1358 -0.1578 -0.1337 -0.1621 -0.1476 -0.0392 \n\n-0.1676 -0.1678 -0.1807 -0.1703 -0.1699 -0.1908 -0.1650 -0.0587 -0.0486 -0.0325 \n\n-0.0256 -0.0265 0.0159 -0.0253 -0.0259 0.0181 -0.0282 0.0070 0.0107 0.0029 \n\n0.0228 0.0263 0.0058 0.0232 0.0116 0.0055 0.0125 0.0048 0.0065 -0.0009 \n\n0.0003 0.0031 0.0278 0.0368 -0.1229 -0.0650 -0.1266 0.0270 -0.0697 -0.0915 \n\n-0.2083 -0.3385 -0.1195 -0.3437 -0.3387 -0.1128 -0.3400 -0.1151 -0.1185 -0.8537 \n\n0.0014 0 0.0026 0.0005 0.0039 0.0045 -0.0013 0.0014 0.0008 0.0050 \n\n-0.0009 -0.0023 0.0004 0.0008 -0.0038 0.0030 0.0021 0.0014 0.0044 0.0034 \n\n0.0767 0.3591 -0.0288 0.3602 -0.0779 -0.0247 -0.0770 -0.0333 -0.0166 -0.0074 \n\n0.2964 0.2954 0.2115 0.2988 0.2939 0.2133 0.2957 0.2079 0.2096 1.1022 \n\n-0.0307 -0.0300 -0.0226 -0.0250 -0.0290 -0.0633 -0.0229 -0.0228 -0.0595 -0.4179 \n\n0.0264 0.0249 -0.0611 -0.0744 -0.1337 -0.0200 -0.0826 -0.0609 -0.0287 -0.0489 \n\n-0.0777 -0.0752 -0.0766 -0.0573 -0.0596 -0.1206 -0.0584 -0.0728 -0.1082 -0.1223 \n\n0.1017 0.1018 0.1024 0.1251 0.1193 0.0997 0.1236 0.1037 0.1046 0.0987 \n\n-0.0024 -0.0049 -0.0246 -0.0369 -0.0365 -0.0257 -0.0379 -0.0236 -0.0205 -0.0190 \n\n-0.0608 -0.0651 -0.1019 -0.0550 -0.0566 -0.1805 -0.0558 -0.1028 -0.1458 -0.1586 \n\n0.3455 0.1749 0.0314 0.3618 0.1864 -0.0747 0.1826 0.0271 -0.0759 -0.0864 \n\n0.0123 0.0082 -0.0236 0.0085 0.0095 -0.0276 0.0123 -0.0272 -0.0194 -0.0061 \n\n0.0594 0.0560 0.0155 0.0752 0.0644 0.0210 0.0771 0.0266 0.0280 0.0160 \n\n-0.0753 -0.0761 -0.0749 -0.0606 -0.0591 -0.1188 -0.0579 -0.0732 -0.1134 -0.1222 \n\n0.0023 0.0004 0.0021 -0.0014 -0.0024 0.0036 0.0021 -0.0002 0.0044 0.0051 \n\n-0.0454 -0.0193 0.0100 -0.0099 -0.0142 0.0121 -0.0155 0.0061 -0.0001 0.0052 \n\n0.3146 0.3119 0.0746 0.3164 0.2570 0.0799 0.3145 0.0766 0.0769 06128 \n\nFonte: Elaborada pelo autor. \n\n\n\n116 \n \n\nTabela 59 \u2013 Bloco B5 da matriz IW, f\u00f3rmula (4.12). \n-0.0007 0.0013 -0.0008 -0.0014 0.0016 0.0012 -0.0008 -0.0035 -0.0027 0.0002 \n\n0.0275 0.0320 0.0345 0.0211 0.0316 0.0307 0.0404 0.0304 0.0317 0.0302 \n\n0.0283 0.0136 0.0153 0.0043 0.0132 0.0111 0.0003 0.0232 0.0143 0.0245 \n\n-0.0311 0.0060 0.0094 0.0059 0.0085 -0.0203 0.0084 -0.0325 0.0107 -0.0279 \n\n-0.0377 -0.0276 -0.0267 -0.0183 -0.0217 -0.0247 -0.0195 -0.0361 -0.0215 -0.0388 \n\n-0.0157 0.0148 0.0199 0.0245 0.0243 -0.0104 0.0241 -0.0104 0.0131 -0.0138 \n\n0.4300 -0.0276 -0.0282 -0.0049 -0.0315 0.0989 -0.0164 0.3900 -0.0260 0.3657 \n\n0.0024 0.0026 0.0023 0.0031 0.0026 0.0016 -0.0022 -0.0009 -0.0007 -0.0029 \n\n-0.0375 -0.0254 -0.0268 -0.0237 -0.0218 -0.0250 -0.0187 -0.0374 -0.0262 -0.0376 \n\n0.0195 0.0212 0.0208 0.0194 0.0195 0.0153 0.0213 0.0180 0.0168 -0.0499 \n\n-0.0381 -0.0222 -0.0209 -0.0211 -0.0219 -0.0225 -0.0255 -0.0403 -0.0225 -0.0401 \n\n0.0401 -0.0207 -0.0184 -0.0403 -0.0188 0.0441 -0.0222 0.0446 -0.0180 0.0969 \n\n-0.0366 -0.0216 -0.0238 -0.0183 -0.0266 -0.0233 -0.0206 -0.0370 -0.0239 -0.0349 \n\n-0.0599 -0.0596 -0.0563 -0.0561 -0.0576 -0.0799 -0.0797 -0.0588 -0.0732 -0.0767 \n\n0.0221 0.0272 0.0690 -0.0237 -0.0335 0.0226 -0.0194 0.0250 0.0733 0.0262 \n\n-0.0440 -0.0748 -0.0993 -0.0787 -0.0994 -0.0493 -0.1000 -0.0483 -0.0983 -0.0393 \n\n-0.0011 -0.0026 -0.0023 0.0055 0.0003 -0.0002 0.0026 -0.0031 0.0040 0.0009 \n\n0.0033 0.0019 -0.0030 0.0072 -0.0030 -0.0002 -0.0011 -0.0022 -0.0025 0.0011 \n\n0.0009 -0.0048 -0.0017 0.0042 -0.0028 -0.0051 -0.0005 -0.0007 0.0008 -0.0049 \n\n0.0080 0.0097 0.0057 0.0013 0.0071 0.0121 0.0129 0.0121 0.0077 0.0102 \n\n0.3624 0.2319 0.2293 -0.1034 0.2351 -0.2296 -0.1022 0.0106 -0.0712 0.0120 \n\n0.0304 0.0304 0.0385 0.2377 0.0386 0.0230 0.0262 0.0290 0.0220 0.0214 \n\n0.1020 -0.0267 -0.0645 -0.0621 -0.0682 -0.0185 -0.3610 0.1100 0.0529 0.1142 \n\n0.0022 -0.0017 0.0016 -0.0064 0.0008 -0.0039 -0.0030 0.0016 0.0015 -0.0022 \n\n-0.0344 -0.0035 -0.0060 0.0067 -0.0046 0.0003 0.0161 -0.0257 -0.0035 -0.0307 \n\nFonte: Elaborada pelo autor. \n\n\n\n117 \n \n\nTabela 60 \u2013 Bloco B6 da matriz IW, f\u00f3rmula (4.12). \n-0.0026 -0.0018 -0.0005 -0.0021 -0.0031 -0.0047 0.0018 0.0060 0.0118 -0.0118 \n\n0.0266 0.0258 0.0399 0.0310 0.0330 0.0395 0.0266 0.0274 0.0255 0.0001 \n\n0.0240 0.0250 0.0038 0.0257 0.0165 0.0112 0.0265 0.0092 0.0060 0.0041 \n\n-0.0285 -0.0271 0.0345 -0.0167 0.0093 0.0315 0.0053 0.0348 0.0322 0.0164 \n\n-0.0374 -0.0395 -0.0220 -0.0354 -0.0230 -0.0213 -0.0252 -0.0214 -0.0247 -0.0191 \n\n0.0106 -0.0124 0.0195 0.0099 -0.0137 0.0228 -0.0116 0.0241 0.0232 0.0208 \n\n0.3676 0.3660 -0.0151 -0.0861 -0.1285 -0.0143 0.0041 -0.0196 -0.0033 -0.0049 \n\n-0.0007 0.0005 0.0003 -0.0026 0.0028 0.0026 0.0028 -0.0018 -0.0017 -0.0462 \n\n-0.0355 -0.0385 -0.0232 -0.0370 -0.0255 -0.0217 -0.0219 -0.0260 -0.0173 -0.0164 \n\n0.0142 -0.0522 0.0191 0.0155 0.0182 0.0153 0.0189 0.0094 0.0106 0.0060 \n\n-0.0412 -0.0372 -0.0258 -0.0228 -0.0268 -0.0263 -0.0250 -0.0219 -0.0225 0.0080 \n\n0.0968 0.1019 -0.0397 0.0093 -0.0220 -0.0457 -0.0189 -0.0412 -0.0406 -0.2023 \n\n-0.0359 -0.0369 -0.0218 -0.0361 -0.0263 -0.0243 -0.0282 -0.0212 -0.0203 -0.0225 \n\n-0.0755 -0.0757 -0.0732 -0.0596 -0.0643 -0.1195 -0.0569 -0.0776 -0.1097 -0.1062 \n\n0.0221 0.0236 -0.0386 0.0243 0.0265 -0.0373 0.0210 -0.0211 -0.0230 -0.1906 \n\n-0.0534 -0.0378 -0.0922 -0.0496 -0.0462 -0.1021 -0.0460 -0.0921 -0.0840 -0.0283 \n\n0.0036 0.0015 0.0011 0.0024 0.0009 0.0011 0.0011 0.0036 -0.0006 0.0056 \n\n0.0007 0.0008 -0.0007 -0.0010 -0.0029 -0.0038 0.0002 -0.0014 0.0062 0.0120 \n\n-0.0039 -0.0035 -0.0051 -0.0022 -0.0021 0.0018 -0.0053 -0.0025 0.0088 0.0331 \n\n0.0063 0.0101 0.0082 0.0073 0.0080 0.0115 0.0121 0.0102 0.0006 0.0310 \n\n0.0104 -0.1215 -0.0804 0.2307 -0.1201 -0.0785 0.2287 -0.1002 -0.1028 -0.7607 \n\n0.0221 0.0221 0.0326 0.0330 0.0269 0.0241 0.0273 0.0281 0.0203 0.1192 \n\n0.1150 -0.0286 -0.0573 0.1116 -0.0314 -0.0621 -0.0272 -0.0620 -0.0587 -0.5267 \n\n-0.0023 -0.0029 0.0011 -0.0009 -0.0027 -0.0022 0.0020 0.0016 -0.0023 -0.0015 \n\n-0.0360 -0.0360 0.0148 -0.0046 -0.0050 0.0181 -0.0056 -0.0118 0.0045 0.0208 \n\nFonte: Elaborada pelo autor. \n\n \n\nA seguir, as outras matrizes,  \n\nLW = [-1.2516 -27.0481 -0.0985 -27.1744 -0.5460 0.8034 -3.7941 -6.5422  -27.6216 95.9035 \n\n-26.8871 1.8888 -0.4168 -2.5810 -26.8337 2.3459 -0.7838 -0.3164 0.5908 96.6923 0.5864      \n\n-9.0630 -0.9755 -0.8091 0.9583 -0.7615 -0.0532 0.7978 95.9943 96.7776 -1.6576 -0.8905      \n\n-15.0051 0.6144 -27.0444 -0.1060 1.1710 -0.4133 0.1325 -0.6060 -3.7948 -27.3438 96.1249 \n\n0.8995 -3.3421 0.8964 1.9142 -0.5593 2.1019 1.5406 2.1179 -3.3831 -5.0903 1.3298 2.4226  \n\n-1.1270 2.4355 1.4058 -27.3181 2.4678 -0.4543 96.5362 5.0686 -7.0072 2.2331 -0.8964 \n\n0.3360 0.2627 -0.4157 1.2539], \n\n\n\n118 \n \n\nTb1  = [2.4410 -7.9358 -5.0742 -7.2658 2.6508 0.4874 1.5836 1.5013 -8.6979 -3.0517 -5.6962 \n\n-2.7558 -7.1611 -0.6151 -6.5599 -0.5943 -5.7552 -0.5321 -2.0322 -3.5029 -0.0488 0.7850 \n\n3.2023 2.8671 -1.4401 4.0182 1.3002 -0.0167 -4.8233 -3.3370 -0.9908 4.8597 -2.2629           \n\n-0.2202 -6.6958 -5.9316 -4.0406 -5.0494 1.4031 2.4255 1.7494 -7.9289 -4.0354 -2.3067         \n\n-0.4935 1.8228 -1.2622 4.6173 -4.3329 -3.9924 -3.3867 0.5023 8.6083 -3.5537 -1.9742          \n\n-2.7788 -2.9090 -4.6417 -8.9476 -3.6206 3.5346 -4.6819 10.6586 2.2981 -4.3918 1.0575 \n\n2.5938 1.5923 2.6691 -4.0129] e \n\n \n\nb2 = [22.2270 22.2270 22.2270 22.2270 22.2270 22.2270 22.2270 22.2270],  \n\nonde o s\u00edmbolo T indica  matriz transposta. \n\nPara a aplica\u00e7\u00e3o da rede neural artificial aos dados das tabelas dos ANEXOS I, J e K \n\ns\u00e3o usadas as mesmas f\u00f3rmulas (4.5), (4.6) e (4.7), agora com i = 1, 2 e 3, b2 \u00e9 a matriz de \n\nlimiar, de ordem 1 X 8, com o valor encontrado 22.2270 repetido 8 vezes e, por fim,  a matriz \n\nones(1,8) = [1 1 1 1 1 1 1 1]. \n\nCom estes resultados fica provada a hip\u00f3tese formulada no cap\u00edtulo introdut\u00f3rio deste \n\ntrabalho, isto \u00e9, \u00a8\u00e9 poss\u00edvel aplicar as redes neurais artificiais na ind\u00fastria de pisos e \n\nrevestimentos cer\u00e2micos\u00a8. \n\n\u00c9 importante ressaltar que o processo de aplica\u00e7\u00e3o das RNAs foi tamb\u00e9m validado na \n\nmedida em que as matrizes encontradas para a temperatura de sinteriza\u00e7\u00e3o de 1060\u00b0 C, tanto \n\npara cinco como para oito vari\u00e1veis, puderam ser aplicadas \u00e0s outras temperaturas de queima \n\n(1000\u00b0 C para 5 vari\u00e1veis, 1020\u00b0 C e 1040\u00b0 C para 5 e 8 vari\u00e1veis).  \n\nPode-se concluir, ent\u00e3o, que a estabilidade e a converg\u00eancia n\u00e3o ocorreram quando o \n\nn\u00famero de neur\u00f4nios na camada oculta \u00e9 grande e nem mesmo quando o n\u00famero de itera\u00e7\u00f5es \n\n(epochs) \u00e9 muito elevado. Ocorreu quando houve uma rela\u00e7\u00e3o \u00f3tima entre os dois n\u00fameros, \n\nna temperatura de sinteriza\u00e7\u00e3o de 1060\u00b0 C. Ainda, levando em considera\u00e7\u00e3o que uma rede \n\nneural \u00e9 um conceito desenvolvido para operar com grande volume de dados, observou-se que \n\na melhor solu\u00e7\u00e3o foi encontrada quando, al\u00e9m da temperatura mencionada, o n\u00famero de \n\nvari\u00e1veis atingiu o n\u00famero de oito e quando o n\u00famero de linhas de dados era de vinte, de \n\nacordo com as tabelas dos ANEXOS I, J e K. \n\nAp\u00f3s ter contratado os fornecedores de argilas e de outros insumos e definido o \n\nproduto com as especifica\u00e7\u00f5es t\u00e9cnicas das vari\u00e1veis de controle, o industrial interessado em \n\n\n\n119 \n \n\naplicar as RNAs, para se estabelecer em algum nicho de mercado, deve proceder de acordo \n\ncom as seguintes etapas: \n\n1 \u2013 Recolher amostras das minas e bancadas de argilas; \n\n2 \u2013 Confeccionar os corpos-de-prova; \n\n3 \u2013 Testar em laborat\u00f3rio os corpos-de-prova, para encontrar os dados amostrais das          \n\nvari\u00e1veis de controle; \n\n4 \u2013 Elaborar a tabela (ou tabelas) com os dados amostrais, que ser\u00e1 definida como o padr\u00e3o de \n\nentrada das RNAs; e \n\n5 \u2013 Aplicar as RNAs. \n\n Para a aplica\u00e7\u00e3o das RNAs, seguem-se as seguintes etapas: \n\n1 \u2013 Disponibilizar um software (MATLAB); \n\n2 \u2013 Definir a rede (MLP de tr\u00eas camadas); \n\n3 \u2013 Definir a regra de aprendizado da rede (regra delta generalizada ou backpropagation com \n\nquatro varia\u00e7\u00f5es); e \n\n4 \u2013 Estabelecer as fun\u00e7\u00f5es de transfer\u00eancia da rede (log\u00edstica e linear). \n\n Entre par\u00eanteses foram colocados o software e os dados utilizados neste trabalho. O \n\nprograma usado para o backpropagation resiliente (trainrp) consiste dos seguintes passos: \n\n1. p = [2.058 ... 61.3; 2.006 ... 59.7; ... ; 1.984 ... 22.9]; \n\n2. t = [2  1.9  0.3  5.5  4  7  602  51]; \n\n3. net = newff(minmax(p),[3,1],{\u2018logsig\u2019,\u2019purelin\u2019},\u2019trainrp\u2019); \n\n4. net.trainParam.show = 200; \n\n5. net.trainParam.epochs = 5000; \n\n6. net.trainParam.goal = 1e-5; \n\n7. [net,tr] = train(net,p,t); \n\n      Realizados os testes e encontrados os melhores valores de parada (epochs ou goal), \n\nindicando que houve converg\u00eancia, o comando a = sim(net,p) fornece a simula\u00e7\u00e3o executada \n\npelo programa. \n\n\n\n120 \n \n\n Mais importante, os comandos celldisp(net.IW), celldisp(net.LW) e celldisp(net.b)   \n\nrecuperam as matrizes de pesos IW e LW e as matrizes de limiares b(duas). Aplicando as \n\nf\u00f3rmulas (4.5), (4.6) e (4.7), p. 60, podem ser encontrados os valores estimados para o \n\nproduto industrial, os quais se apresentam aproximadamente iguais ao padr\u00e3o de sa\u00edda da \n\nrede, designado por t no programa. Conv\u00e9m observar que a matriz p corresponde aos dados \n\namostrais do ANEXO I.  \n\n  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n121 \n \n\n5. CONSIDERA\u00c7\u00d5ES FINAIS \n \n\nAo concluir este trabalho \u00e9 muito importante refor\u00e7ar alguns aspectos quando da \n\naplica\u00e7\u00e3o de uma rede neural artificial. A ideia da aplica\u00e7\u00e3o do conceito das redes neurais \n\nartificiais \u00e0s ind\u00fastrias de pisos e revestimentos cer\u00e2micos do PCSG mostrou-se bastante \n\npromissora tendo em vista que, mesmo operando com uma pequena quantidade de dados, \n\ncontrariando a sua proposta original de trabalhar com grande volume de dados em problemas \n\ncomplexos, as redes forneceram solu\u00e7\u00f5es com pequenos desvios com rela\u00e7\u00e3o ao padr\u00e3o de \n\nsa\u00edda previamente estabelecido. Mais ainda, a escolha das minas para fornecer as amostras \n\nrecaiu sobre aquelas que apresentaram as maiores diferencia\u00e7\u00f5es nas argilas e representavam \n\ntoda a coluna estratigr\u00e1fica da Forma\u00e7\u00e3o Corumbata\u00ed. Por isso, mesmo diferentes e com dados \n\namostrais que variavam em m\u00e9dia de 0.3 a 602, todos os dados dos ANEXOS A, B, C e D \n\nforam usados desde que pertencessem \u00e0s mesmas vari\u00e1veis f\u00edsicas e \u00e0s mesmas temperaturas \n\nde queima. Estes dados tamb\u00e9m n\u00e3o interferiram nas solu\u00e7\u00f5es encontradas. \n\nProvavelmente, uma ind\u00fastria cer\u00e2mica n\u00e3o utiliza argilas t\u00e3o diferentes para a sua \n\nlinha de produ\u00e7\u00e3o. Com valores mais homog\u00eaneos \u00e9 poss\u00edvel estabelecer um padr\u00e3o de \n\nqualidade bem mais rigoroso, que pode se constituir no padr\u00e3o de sa\u00edda pr\u00e9-estabelecido para \n\no funcionamento de uma rede neural.  \n\nEste trabalho pode ser ampliado, futuramente, incluindo tamb\u00e9m outras vari\u00e1veis, \n\nal\u00e9m das vari\u00e1veis f\u00edsicas aqui usadas, que s\u00e3o as vari\u00e1veis qu\u00edmicas e mineral\u00f3gicas. Isto \n\npossibilita  um melhor desempenho por parte das redes neurais. \n\n\u00c9 bom refor\u00e7ar tamb\u00e9m que as redes neurais artificiais apresentam outras arquiteturas \n\ne tamb\u00e9m outras regras de aprendizado. Elas podem ser operadas isoladamente ou associadas \n\na outro conceito ou m\u00e9todo, em especial, daqueles que j\u00e1 fazem parte do que \u00e9 concebido \n\ncomo Intelig\u00eancia Artificial. \n\nPor outro lado, \u00e9 tamb\u00e9m igualmente importante ressaltar que com o inevit\u00e1vel \n\naumento da popula\u00e7\u00e3o humana, ocorrer\u00e1 uma press\u00e3o cada vez maior para a ocupa\u00e7\u00e3o dos \n\nespa\u00e7os ainda inexplorados na face deste planeta Terra. A humanidade em busca por \u00e1gua, \n\npor alimentos e por outros recursos naturais, for\u00e7osamente provocar\u00e1 um aumento dos \n\nnefastos impactos causados ao meio ambiente. Especificamente, no caso das mineradoras e da \n\nindustrializa\u00e7\u00e3o correspondente, Poletto (2007) e AMBIENTE BRASIL (2009), sugerem \n\nalternativas no sentido de se diminuir o consumo e de se aumentar a reciclagem e, no caso das \n\nminas, de se promover a revegeta\u00e7\u00e3o e a revers\u00e3o das \u00e1reas degradadas em terras produtivas. \n\n\n\n122 \n \n\nNo caso da revegeta\u00e7\u00e3o, como um trabalho para o futuro, poder-se-ia sugerir a aplica\u00e7\u00e3o de \n\numa rede Kohonen, as quais n\u00e3o exibem uma camada de sa\u00edda, contudo os neur\u00f4nios da \n\ncamada de entrada competem entre si e o vencedor leva tudo (ou vencedores). Com esta rede \n\npoderiam ser resolvidos alguns problemas relativos \u00e0 flora, promovendo uma competi\u00e7\u00e3o \n\ncontrolada biologicamente entre as esp\u00e9cies vegetais para que aquelas que estiverem melhores \n\nadaptadas ao local possam se estabelecer. \n\n\n\n123 \n \n\nREFER\u00caNCIA BIBLIOGR\u00c1FICA \n\n \n\nABDI, H.; VALENTIN, D.; EDELMAN, B. Neural networks. Sage University Papers Series \n\non Quantitative Applications in the Social Sciences, 07 \u2013 124. Thousand Oaks, CA, 1999. \n\nAMBIENTE Brasil. Dispon\u00edvel em: http://www.ambientebrasil.com.br. Acesso em: 11 nov. \n\n2009. \n\nASSOCIA\u00c7\u00c3O BRASILEIRA DE NORMAS T\u00c9CNICAS. NBR 13818: placas cer\u00e2micas \n\npara revestimentos: especifica\u00e7\u00e3o e m\u00e9todos de ensaio. Rio de Janeiro, 1997. 78 p. \n\nASSOCIA\u00c7\u00c3O PAULISTA DAS CER\u00c2MICAS DE REVESTIMENTOS \u2013 ASPACER. \n\nDispon\u00edvel em: http://www.aspacer.com.br. Acesso em: 10 fev. 2012. \n\nAZEVEDO, F. M.; BRASIL, L.; OLIVEIRA, R. C. L. Redes neurais com aplica\u00e7\u00f5es em \n\ncontrole e em sistemas especialistas. Florian\u00f3polis: Bookstore, 2000. \n\nBRAGA, R. L.; CARVALHO, A. P. L.; LURDEMIR, T. B. Redes neurais artificiais: teoria e \n\naplica\u00e7\u00f5es. 2. ed. Rio de Janeiro: Livros T\u00e9cnicos, 2007. \n\nCAMPANHA, J. R. Determina\u00e7\u00e3o dos par\u00e2metros de ordem de redes neurais pelo m\u00e9todo \n\ndos cumulantes. 1994. 129p. Tese (Doutorado em Engenharia El\u00e9trica) \u2013 Faculdade de \n\nEngenharia El\u00e9trica e de Computa\u00e7\u00e3o, Universidade Estadual de Campinas, Campinas, 1994. \n\nCHRISTOFOLETTI, S. R. Estudo mineral\u00f3gico, qu\u00edmico e textural das rochas sedimentares \n\nda Forma\u00e7\u00e3o Corumbata\u00ed \u201cJazida Cruzeiro\u201d, e suas implica\u00e7\u00f5es nos produtos e processos \n\ncer\u00e2micos. 1999. 120p. Tese (Doutorado em Geoci\u00eancias) \u2013 Instituto de Geoci\u00eancias e \n\nCi\u00eancias Exatas, Universidade Estadual Paulista, Rio Claro, 1999. \n\nCHRISTOFOLETTI, S. R.; MORENO, M. M. T. Caracter\u00edsticas das rochas da Forma\u00e7\u00e3o \n\nCorumbata\u00ed utilizadas na ind\u00fastria de revestimento cer\u00e2mico. Geoci\u00eancias, S\u00e3o Paulo, v. 23, \n\nn. 1/2, p.79-88, 2004. \n\nCHRISTOFOLETTI, S. R.; THOMAZELLA, H. R.; MORENO, M. M. T.; MASSON, M. R. \n\nUtiliza\u00e7\u00e3o da an\u00e1lise estat\u00edstica multivariada no tratamento de dados aplicados \u00e0s mat\u00e9rias-\n\nprimas cer\u00e2micas. Revista do Instituto Geol\u00f3gico, S\u00e3o Paulo, v. 26, n. 1/2, p. 19-29, 2005. \n\n\n\n124 \n \n\nCINTRA, E. C. Aplica\u00e7\u00e3o de redes neurais no controle de teores de cobre e ouro no dep\u00f3sito \n\nde Chapada (GO). 2003. 170f. Tese (Doutorado em Geologia) \u2013 Instituto de Geoci\u00eancias e \n\nCi\u00eancias Exatas, Universidade Estadual Paulista, Rio Claro, 2003. \n\nCORREIA, S. L.; TOMAZI, F. C.; FOLGUERAS, M. V. Influ\u00eancia da composi\u00e7\u00e3o \n\nmineral\u00f3gica e temperatura de queima nas propriedades tecnol\u00f3gicas de massas cer\u00e2micas. In: \n\nCONGRESSO BRASILEIRO DE CER\u00c2MICA, 51., 2007, Salvador. Anais ..., Salvador, \n\n2007, p. 12. \n\nDAM\u00c1SIO, A. R. O erro de Descarte: emo\u00e7\u00e3o, raz\u00e3o e o c\u00e9rebro humano. Traduzido por \n\nDora Vicente e Georgina Segurado. Revis\u00e3o T\u00e9cnica de Carmem S. da Costa e Ana Maria \n\nBarbosa. S\u00e3o Paulo: Companhia das Letras, 1996. \n\nFERRO, L. Modelo do bulbo olfativo baseado em redes neurais recorrentes. 2007. 97f. \n\nDisserta\u00e7\u00e3o (Mestrado em F\u00edsica Aplicada) \u2013 Instituto de Geoci\u00eancias e Ci\u00eancias Exatas, \n\nUniversidade Estadual Paulista, Rio Claro, 2007. \n\nECCLES, J. C. The physiology of nerve cells. John Hopkins University Press, [S.l.], 1957 \n\nGAIDZINSKI, R. Fatores envolvidos no sazonamento e suas implica\u00e7\u00f5es nas propriedades \n\nde argilas para a Ind\u00fastria Cer\u00e2mica. 2006. 179p. Tese (Doutorado em Ci\u00eancias, em \n\nEngenharia Metal\u00fargica e de Materiais) \u2013 Engenharia Metal\u00fargica e de Materiais, COPPE, \n\nUniversidade Federal do Rio de Janeiro, Rio de Janeiro, 2006. \n\nGROSSBERG, S. Adaptive pattern classification and universal recording. Biological \n\nCybernetics, [S.l.], v. 23, n. 1, p. 121-134, 1976. \n\nGROSSBERG, S. How does the brain build a cognitive code? Psychological Review, [S.l.], \n\nv.89, n. 1, p. 529-572, 1980. \n\nHAYKIN, S. Redes neurais: princ\u00edpios e pr\u00e1tica. Tradu\u00e7\u00e3o de Paulo Martins Engel. 2. ed. \n\nPorto Alegre: Bookman, 2001. \n\nHEBB, D. The organization of behavior: a neurophysiological theory. New York: John Wiley \n\n&amp; Sons, 1949. \n\n\n\n125 \n \n\nHOPFIELD, J. J. Neural networks and physical systems with emergent collective \n\ncomputational abilities. Proceedings of the National Academy of Sciences, [S.l.], v. 79, p. \n\n2554-2558, 1982. \n\nKOHONEN, T. Clustering, taxonomy and topological maps of pattern: pattern recognition. \n\n[S.l.], [s.n.], 1982. \n\nKOV\u00c1CS, Z. L. Redes neurais artificiais: fundamentos e aplica\u00e7\u00f5es: um texto b\u00e1sico. 3. ed. \n\nS\u00e3o Paulo: Livraria da F\u00edsica, 2002. \n\nLE CUN, Y. A learning procedure for asymmetric threshold network. Proceedings of \n\nCognitiva, [S.l.], n. 85, p. 599-604, 1985. \n\nMcCULLOCH, W.; PITTS, W. A logical calculus of the ideas immanent in nervous activity. \n\nBulletin of Mathematical Biophysics, [S.l.], v. 5, n.1, p. 115-133, 1943. \n\nMINSKY, M.; PAPERT, S. Perceptrons: an introduction to computational geometry. \n\nCambridge: The MIT Press, 1969. \n\nMORENO, M. M. T.; BARTOLOMEU, D.; LIMA, R. H. C. An\u00e1lise do comportamento de \n\nqueima de argilas e formula\u00e7\u00f5es para revestimento cer\u00e2mico. Cer\u00e2mica, [S.l.], n.55, p.286-\n\n295, 2009. \n\nNUSSENZVEIG, H. M. (Org.). Complexidade e caos. Rio de Janeiro: UFRJ/COPEA, 1999. \n\nPARKER, D. Learning logic. Invention report, Stanford University, File 1, Office \n\nTechnology Licensing, 1982. \n\nPOLETTO, E. R. A inova\u00e7\u00e3o tecnol\u00f3gica e a utiliza\u00e7\u00e3o de tecnologias ambientais como fator \n\nde diminui\u00e7\u00e3o de impactos ambientais na ind\u00fastria cer\u00e2mica: o caso do APL de pisos e \n\nrevestimentos cer\u00e2micos de Santa Gertrudes (SP). Geografia, Londrina, v. 16, n. 2, p. 25-47, \n\n2007. \n\nPOLETTO, E. R. O desenvolvimento territorial e a utiliza\u00e7\u00e3o dos recursos naturais: a \n\nminera\u00e7\u00e3o de argila no APL de pisos e revestimentos cer\u00e2micos de Santa Gertrudes (SP). In: \n\nF\u00d3RUM AMBIENTAL DA ALTA PAULISTA, 2007, [S.l.]. Anais..., v. III, 2007b, p. 19. \n\n\n\n126 \n \n\nPRADO, A. C. A. Placas cer\u00e2micas para revestimento de baixa absor\u00e7\u00e3o de \u00e1gua e \n\nestabilidade dimensional confeccionadas por moagem a seco usando o material da Forma\u00e7\u00e3o \n\nCorumbata\u00ed. 2007. 203f. Tese (Doutorado em Geologia Regional) \u2013 Instituto de Geoci\u00eancias \n\ne Ci\u00eancias Exatas, Universidade Estadual Paulista, Rio Claro, 2007. \n\nPRADO, A. C. A.; ZANARDO, A.; MORENO, M. M. T.; MENEGAZZO, A. P. M. Redu\u00e7\u00e3o \n\nda susceptibilidade \u00e0 deforma\u00e7\u00e3o piropl\u00e1stica das argilas do P\u00f3lo Cer\u00e2mico de Santa \n\nGertrudes atrav\u00e9s da adi\u00e7\u00e3o de mat\u00e9rias-primas. Cer\u00e2mica, [S.l.], n. 54, p. 7-20, 2008. \n\nROCHA, R. R. Propriedades qu\u00edmico-mineral\u00f3gicas de rochas da Forma\u00e7\u00e3o Corumbata\u00ed: \n\naplica\u00e7\u00e3o na classifica\u00e7\u00e3o de produtos. 2012. 203p. Tese (Doutorado em Geologia Regional) \n\n\u2013 Instituto de Geoci\u00eancias e Ci\u00eancias Exatas, Universidade Estadual Paulista, Rio Claro, \n\n2012. \n\nROSENBLATT, F. The perceptron: a probabilistic model for information storage and \n\norganization in the brain. Psychological Review, [S.l.], v. 65, n. 1, p. 386-408, 1958. \n\nRUMELHART, D. E.; HINTON, G. E.; McCLELAND, J. L. A general framework for \n\nparallel distributed processing. Cambridge: The MIT Press, 1986. \n\nVON DER MARLSBURG, C. Self-organizing of orientation sensitive cells in the striated \n\ncortex. Kibernetic, [S.l.], v. 14, n. 1, p. 66-82, 1973. \n\nWASSERMAN, P. D. Neural computing: theory and practice. New York: Van Nostrand \n\nReinhold, 1989. \n\nWERBOS, P. Beyond regression: new tools for prediction and analysis in the behavioral \n\nsciences. 1974. Ph. D. Thesis. Harvard University, Cambridge, 1974. \n\nWIDROW, B.; HOFF, M. Adaptive switching circuits. In: IRE WESCON CONVENTION \n\nRECORD, 1960, New York, Neurocomputing, p. 96-107. \n\n \n\n \n\n \n\n\n\n127 \n \n\nBIBLIOGRAFIA COMPLEMENTAR \n\n \n\nAIHARA, K.; TAKABE, T.; TOYODA, M. Chaotic neural networks. Phys. Lett. A, n. 144, p. \n\n333-340, [S.l.], 1990. \n\nBAIRD, B.; HIRSH, M. W.; ECKMAN, F. A neural network associative memory for \n\nhandwritten character recognition using multiple Chua attractors. IEEE Transactions on \n\nCircuits and Systems II: analog &amp; digital signal processing, 40, p. 667-674, 1993. \n\nBARROS DE ANDRADE, L. Mapeamento do potencial mineral para n\u00edquel e ouro no \n\nCintur\u00e3o Metassedimentar Nova Brasil\u00e2ndia \u2013 Rond\u00f4nia por meio de l\u00f3gica nebulosa (fuzzy) \n\ne redes neurais artificiais. 2008. Disserta\u00e7\u00e3o (Mestrado em geoci\u00eancias) \u2013 Instituto de \n\nGeoci\u00eancias, Universidade Estadual de Campinas, Campinas, 2008. \n\nBIONDI NETO, L.; SIEIRA, A. C. C.; DANZIGER, B. R.; DA SILVA, J. G. S. Neuro - \n\nCPT: classifica\u00e7\u00e3o de solos usando redes neurais artificiais. Engevista, Rio de Janeiro, v. 8, n. \n\n1, p.37-48, 2006. \n\nBISHOP, C. M. Neural networks for pattern recognition. New York: Oxford University Press \n\nInc., 1995. \n\nBRASIL. Minist\u00e9rio das Minas e Energia. Secretaria de Geologia, Minera\u00e7\u00e3o e \n\nTransforma\u00e7\u00e3o Mineral. Anu\u00e1rio estat\u00edstico 2009. Dispon\u00edvel em: http://www.mme.gov.br. \n\nAcesso em: 17 jan. 2010. \n\nBURDEN, R. L.; FAIRES, J. D. An\u00e1lise num\u00e9rica. Tradu\u00e7\u00e3o de All Taskes. Revis\u00e3o T\u00e9cnica \n\nde Helena Castro. S\u00e3o Paulo: Cengage Learning, 2008. \n\nCABRAL J\u00daNIOR, M.; DEL MONTE, E.; MOTTA, J. F. M.; SINTONI, A.; SUSLICK, S.  \n\nArranjos produtivos minero-cer\u00e2micos e o desenvolvimento econ\u00f4mico: caso do APL de \n\nSocorro \u2013 SP. Cer\u00e2mica Industrial, [S.l.], n.11(2), p.1-6, 2006. \n\nCAMPANHA, J. R.; TANCREDO, A. Um modelo f\u00edsico para redes neurais. Cad. Cat. Ens. \n\nFis., Rio Claro, v. 8, n.1. p.56-63, 1991. \n\n\n\n128 \n \n\nCESSAC, B. Increase in complexity in random neural networks. J. Phys. I France 5, [S.l.], p. \n\n409-432, 1995. \n\nCHAPMAN, S. J. Programa\u00e7\u00e3o em MATLAB\u00ae para engenheiros. Tradu\u00e7\u00e3o de Fl\u00e1vio Soares \n\nCorrea da Silva. S\u00e3o Paulo: Pioneira Thomson Learning. 2003. \n\nCHUTCHFIELD, J. et al. Power spectral analysis of a dynamical system. Physics Letters, \n\n[S.l.], v. 76A, n. 1, p. 1-4, 1980. \n\nCORREIA, S. L.; HOTZA, D.; SEGAD\u00c3ES, A. M. Otimiza\u00e7\u00e3o da resist\u00eancia mec\u00e2nica de \n\ncorpos cer\u00e2micos em fun\u00e7\u00e3o das mat\u00e9rias-primas e restri\u00e7\u00f5es de propriedades tecnol\u00f3gicas. \n\nCer\u00e2mica, [S.l.], n. 51, p.230-238, 2005. \n\nCRISANTI, A.; SOMPOLINSKY, H. Dynamics of spins systems with randomly asymmetric \n\nbonds ising spins and Glauber dynamics. Physical Review A, [S.l.], v. 37, n. 12, p. 4865-4874, \n\n1988. \n\nEKELAND, I. O c\u00e1lculo e o imprevisto. Tradu\u00e7\u00e3o de Maria Clara Constantino. Revis\u00e3o \n\nT\u00e9cnica de Hugo Vicente Capelato. S\u00e3o Paulo: Martins Fontes, 1987. \n\nGORINI, A. P. F.; CORREA, A. R. Cer\u00e2mica para revestimento. Cer\u00e2mica, Rio de Janeiro, n. \n\n10, p. 201-252, 1999. \n\nGROSSBERG, S. A theory of visual coding, memory and development: formal theories of \n\nvisual perception. New York: John Wiley &amp; Sons, 1978. \n\nGUTFREUND, H.; REGER, J. D.; YOUNG, A. P. The nature of attractors in an asymmetric \n\nspin glass with deterministic dynamics. J. Phys. A. Math. Gen., [S.l.], n. 1, p. 2775-2797, \n\n1988. \n\nHANSELMAN, D.; LITTLEFIELD, B. Matlab\u00ae 6: curso completo. Tradu\u00e7\u00e3o de Cl\u00e1udia \n\nSant\u2019Ana Martins. Revis\u00e3o T\u00e9cnica de Alberto Saa, Francisco A. M. Gomes e M. Aparecida \n\nDiniz Ehrhardt. S\u00e3o Paulo: Prentice Hall, 2003. \n\nHIRSCH, M. W.; BAIRD, B. Computing with dynamic attractors in neural networks. \n\nBiosystems, [S.l.], n. 34, p. 173-195, 1995. \n\n\n\n129 \n \n\nIFEACHOR, E. C.; JERVIS, B. W. Digital signal processing: a practical approach. New \n\nYork: Wesley Publishing Company, 1993. \n\nKLOTZ, A.; BR\u00c4UER, K. A small-size neural networks and fuzzy logic: basic concepts and \n\napplications. The Institute of Electrical and Eletronics Engineers (IEEE) Press, Inc., New \n\nYork, 1996. \n\nKOHONEN, T. Self organization and associative memory. 1st. ed. Berlin: Springer-Verlag, \n\n1984. \n\nLANDIM, P. M. B. O grupo Passa Dois na Bacia do Rio Corumbata\u00ed. S\u00e3o Paulo: DNPM \u2013 \n\nDGM, 1970. Boletim 252. \n\nLAU, C. (Ed.). Neural networks: theoretical foundations and analysis. New York: The \n\nInstitute of Electrical and Eletronics Engineers (IEEE) Press, Inc., 1992. \n\nLI, Z.; DAYAN, P. Computational differences between asymmetrical and symmetrical \n\nnetworks. Network: Comput. Neural Syst., [S.l.], n.10, p. 59-77, 1999. \n\nMASSON, M. R. Caracteriza\u00e7\u00e3o de jazidas usando a garantia de qualidade de mat\u00e9rias-\n\nprimas para ind\u00fastria cer\u00e2mica de revestimento. 2002. 267f. Tese (Doutorado em Geologia) \n\n\u2013 Instituto de Geoci\u00eancias e Ci\u00eancias Exatas, Universidade Estadual Paulista, Rio Claro, \n\n2002. \n\nMIRZAI, A. R. (Org.). Artificial intelligence: concepts and applications in engineering. \n\nCambridge: The MIT Press, [19-]. \n\nMORETTIN, P. A. An\u00e1lise harm\u00f4nica de processos estoc\u00e1sticos. In: COL\u00d3QUIO \n\nBRASILEIRO DE MATEM\u00c1TICA, 12., 1979, Po\u00e7os de Caldas. Anais..., Po\u00e7os de Caldas, \n\n1979. \n\nMOTTA, J. F. M. As mat\u00e9rias-primas e o estudo de tr\u00eas casos de rochas fundentes. 2000. \n\n208f. Tese (Doutorado em Geologia) \u2013 Instituto de Geoci\u00eancias e Ci\u00eancias Exatas, \n\nUniversidade Estadual Paulista, Rio Claro, 2000. \n\nMOTTA, J. F. M. et al. Caracter\u00edsticas do p\u00f3lo de revestimentos cer\u00e2micos de Santa \n\nGertrudes \u2013 SP: com \u00eanfase na produ\u00e7\u00e3o de argilas. Cer\u00e2mica Industrial, S\u00e3o Paulo, v. 9, n.1, \n\np. 7-13, 2004. \n\n\n\n130 \n \n\nNOSE FILHO, K.; LOTUFO, A. D. P.; LOPES, M. L. M. Utiliza\u00e7\u00e3o de redes neurais \n\nartificiais e redes neuro fuzzy para previs\u00e3o de cargas el\u00e9tricas. In: BRAZILIAN \n\nCONFERENCE ON DYNAMICS, CONTROL AND APPLICATIONS, 7., 2008, Presidente \n\nPrudente. Anais\u2026, Presidente Prudente, 2008, p.6. \n\nOLIVEIRA, M. C.; MAGANHA, M. F. B. Guia t\u00e9cnico ambiental da ind\u00fastria cer\u00e2mica \n\nbranca e de revestimento. S\u00e3o Paulo: CETESB, s\u00e9rie P + L, 2006, p.90. \n\nOSLER, T. J. A quick look at Liapunov space. New Jersey Mathematics and Computer \n\nEducation, New Jersey, v. 28, n. 2, p. 183-197, [19-]. \n\nPANDORFI, H.; SILVA, I. J. O.; SARNIGHAUSEN, V. C. R.; VIEIRA, F. M. C.; \n\nNASCIMENTO, S. T.; GUISELINI, C. Uso de redes neurais artificiais para predi\u00e7\u00e3o de \n\n\u00edndices zoot\u00e9cnicos nas fases de gesta\u00e7\u00e3o e maternidade na suinocultura. Revista Brasileira de \n\nZootecnia, [S.l.], v. 40, n. 3, p. 676-681, 2011. \n\nPARK, J.; SANDBERG, W. Universal approximation using radial basis functions. Neural \n\nComputation, [S.l.], [s.n.], n. 3, 1991. \n\nPOGGIO, T.; GIROSI, F. Networks for approximation and learning. Proceedings of the \n\nElectrical and Eletronics Engineers, Detroit, v. 78, p. 1481-1497, 1990. \n\nPOGGIO, T.; GIROSI, F. Regularization algorithms for learning that are equivalent to \n\nmultilayer networks. Science, [S.l.], v, 247, n. 1, p. 978-982, [19-]. \n\nPORWAL, A.; CARRANZA, E. J. M.; HALE, M. Artificial neural networks for mineral-\n\npotential mapping: a case study from Aravalli Province, Western India. Natural Resources \n\nResearch, [S.l.], 12(3), p. 155-171, 2003. \n\nRENALS, S.; ROHWER, R. A study of network dynamics. Journal of Statistical Physics, \n\n[S.l.], v. 58, n. 5/6, 1990. \n\nRUMELHART, D. E.; HINTON, G. E.; WILLIANS, R. J. Learning internal representations \n\nby error propagation. Parallel Distributed Processing, Cambridge, MIT Press, v. 1, p. 319-\n\n362, 1986b. \n\n\n\n131 \n \n\nSHAFFER, W. M. Can nonlinear dynamics elucidate mechanisms in ecology and \n\nepidemiology? IMA Journal of Mathematics Applied in Medicine and Biology, [S.l.], n. 2, p. \n\n221-252, 1985. \n\nSHERRINGTON, D. Magnets, microchips and memories: from spin glasses to the brain, \n\n[S.l.], p.319-330, [19-]. \n\nVIEIRA, V. M. Redes neurais artificiais: uma aplica\u00e7\u00e3o em petrof\u00edsica e estudo dos efeitos \n\nde est\u00edmulos persistentes. 2007. 73p. Disserta\u00e7\u00e3o (Mestrado em F\u00edsica) \u2013 Instituto de F\u00edsica, \n\nUniversidade Federal de Alagoas, Macei\u00f3, 2007. \n\nWANDRESEN, R.; MITISHITA, E. A.; DE ANDRADE, J. B. Identifica\u00e7\u00e3o de pontos de \n\napoio pr\u00e9-sinalizados com o uso de redes neurais artificiais e correla\u00e7\u00e3o. Bol. Ci\u00eanc. Geod., \n\nsec. Artigos, Curitiba, v. 9, n. 2, p. 179-198, 2003. \n\nWANG, X. Chaos-based learning. Complex Systems, [S.l.], n . 5, p. 359-370, 1991. \n\nWIDROW, B. Generalization and information storage in networks of adaline neurons. Self-\n\norganizing Systems. Washington, Spartan Books, p. 435-461, 1962. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n132 \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nANEXOS \n\n\n\n133 \n \n\nAnexo A - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1000\u00b0C \n \n\nAMOSTRA RLQ (%) PF (%)  Abs (%)  CR(N) MRF (MPa) \nCF1 B1 5.501 3.505 5.724 662.3 55.5 \nCF1 B2 5.504 4.945 6.625 537.8 47.0 \nCF1 B3 5.517 5.229 6.449 226.5 20.1 \nCF2 B2 5.473 5.682 8.583 619.3 49.8 \nCR1 B3 2.720 2.575 11.032 464.7 36.5 \nCR2 B1 3.838 2.605 8.955 520.1 41.7 \nCR2 B2 4.085 2.666 8.810 596.7 48.2 \nCR2 B3 3.152 3.259 10.964 438.6 35.3 \nPG B1 3.545 2.093 10.179 575.3 42.8 \nPG B2 3.093 1.972 10.432 498.1 37.8 \nPG B3 2.232 1.988 13.460 478.4 34.3 \nPG B4 4.159 5.055 11.884 561.4 45.0 \nPT1 B1 2.613 2.768 11.295 336.3 25.5 \nPT1 B2 4.483 3.260 8.337 587.0 48.5 \nPT1 B3 3.909 2.970 8.932 548.8 43.6 \nPT1 B4 2.966 2.975 10.656 468.2 36.9 \nPT2 B1 2.170 2.449 12.417 288.5 20.7 \nPT2 B2 3.459 2.565 10.135 506.3 39.4 \nPT2 B3 3.976 2.933 9.804 278.3 21.4 \nPT3 B1 2.452 5.051 18.466 306.6 20.4 \nPI B1 2.012 1.997 14.113 343.7 24.0 \nPI B2 2.929 2.775 10.007 409.5 31.6 \nPI B3 5.335 4.001 6.934 499.0 42.7 \nPI B4 3.750 4.735 11.938 608.7 47.5 \n\n      \nAmplitude 3.505 3.710 12.742 435.8 35.4 \n\nM\u00e9dia 3.7030 3.3355 10.2555 473.34 37.34 \nDesvio \nPadr\u00e3o \n\n1.13556 1.16162 2.77118 121.216 10.600 \n\n \n \n \n\n\n\n134 \n \n\nAnexo B \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C \n \n\nAMOSTRA DP \n(g/cm 3 ) \n\nDAS \n(g/cm 3 ) \n\nRLS \n(%) \n\nRLQ \n(%) \n\nPF (%) Abs \n(%) \n\nCR (N) MRF \n(MPa) \n\nCF1 B1 2.058 1.985 0.381 7.121 3.506 3.202 677.4 61.3 \nCF1 B2 2.006 1.863 0.668 6.722 5.261 4.843 673.3 59.7 \nCF1 B3 1.973 1.848 0.794 6.630 5.577 5.567 589.1 52.5 \nCF2 B1 2.008 1.867 0.500 2.246 7.598 13.863 428.0 33.6 \nCF2 B2 2.013 1.846 0.851 6.854 5.650 5.423 766.8 69.8 \nCR1 B3 2.076 1.887 0.094 5.579 2.752 5.572 602.4 51.2 \nCR1 B4 2.119 1.935 0.136 1.398 3.993 12.527 445.9 34.8 \nCR2 B1 2.071 1.893 0.172 5.900 2.965 4.669 588.1 49.4 \nCR2 B2 2.088 1.897 0.279 6.316 3.153 4.269 643.7 57.6 \nCR2 B3 2.073 1.897 0.207 5.031 3.644 7.218 566.0 47.3 \nPG B1 2.022 1.845 0.173 6.298 2.393 4.905 695.6 58.3 \nPG B2 2.044 1.853 0.159 5.942 2.138 5.183 681.1 56.7 \nPG B3 1.928 1.751 0.118 5.451 2.346 9.251 642.1 46.1 \nPG B4 1.991 1.823 0.344 5.674 5.642 9.374 593.3 50.3 \nPT1 B2 2.017 1.864 0.381 6.913 3.060 3.757 684.3 60.2 \nPT1 B3 2.081 1.888 0.299 6.331 3.001 3.998 646.7 57.9 \n\nPT1 B4  2.052 1.893 0.359 5.257 3.124 6.283 612.2 52.0  \n\nPT2 B2 2.045 1.864 0.146  5.994 2.625 4.665 661.7 59.0 \nPT2 B3 2.005 1.850 0.275 6.192 2.952 4.972 606.8 50.9 \nPI B1 1.963 1.800 0.056 4.130 2.318 9.486 515.2 42.9 \nPI B2 2.050 1.873 0.239 4.332 3.265 7.044 384.6 29.9 \nPI B3 2.003 1.871 0.548 6.988 4.256 3.897 715.3 64.1 \nPI B4 1.999 1.850 0.317 5.027 5.407 9.154 768.9 65.3 \nTU B2 2.051 1.906 0.118 1.724 5.742 15.129 478.2 35.6 \nTU B3 1.984 1.816 0.082 0.806 6.041 18.718 328.1 22.9 \n\n         \nAmplitude 0.191 0.234 0.795 6.315 5.460 15.516 440.8 46.9 \n\nM\u00e9dia 2.0288 1.8666 0.3078 5.2342 3.9364 7.3188 599.79 50.77 \nDesvio \nPadr\u00e3o \n\n0.04264 0.04504 0.21738 1.82845 1.48738 4.00806 113.806 11.872\n\n \n \n \n\n\n\n135 \n \n\nAnexo C \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C \n \n\nAMOSTRA DP \n(g/cm 3 ) \n\nDAS \n(g/cm 3 ) \n\nRLS \n(%) \n\nRLQ \n(%)  \n\nPF (%) Abs \n(%) \n\nCR (N) MRF \n(MPa) \n\nCF1 B1 2.063 1.896 0.357 7.969 3.480 1.421 727.2 67.0 \nCF1 B2 2.018 1.873 0.674 7.497 5.227 3.147 766.4 69.3 \nCF1 B3 1.981 1.857 0.825 7.451 5.626 4.014 609.7 56.1 \nCF2 B1  1.985 1.849 0.501 3.511 7.558 12.135 478.6 37.4 \nCF2 B2 2.008 1.852 0.928 8.184 6.338 3.629 763.0 69.5 \nCR1 B3 2.075 1.885 0.098 7.104 2.735 2.437 738.9 64.2 \nCR1 B4 2.114 1.930 0.138 2.974 4.045 9.689 575.4 47.1 \nCR1 B5 2.064 1.853 0.288 0.225 3.253 16.795 134.3 9.8 \nCR2 B1 2.071 1.892 0.176 6.755 3.094 2.354 651.3 54.4 \nCR2 B2 2.083 1.892 0.341 7.491 3.052 1.798 743.2 67.4 \nCR2 B3 2.076 1.898 0.209 6.689 3.682 4.067 629.1 56.4 \nPG B1 2.019 1.839 0.114 8.195 2.454 1.758 751.0 66.3 \nPG B2 2.030 1.854 0.136 7.601 2.170 2.134 792.5 70.3 \nPG B3 1.951 1.781 0.094 7.974 2.247 4.057 771.5 63.3 \nPG B4 2.006 1.813 0.360 6.916 5.280 7.096 669.4 59.7 \nPT1 B3 2.073 1.892 0.297 7.743 3.166 1.435 739.4 66.4 \nPT1 B4 2.046 1.887 0.349 6.969 3.263 2.794 725.4 64.6 \nPT2 B3 2.006 1.848 0.267 7.743 2.980 2.252 635.1 57.3 \nPT3 B1 1.899 1.745 0.056 4.196 5.238 14.211 641.7 48.8 \nPI B1 1.967 1.800 0.062 5.934 2.386 6.039 576.3 45.5 \nPI B3 2.003 1.870 0.514 8.078 4.455 1.959 752.8 68.2 \nPI B4  1.998 1.843 0.317 6.109 5.091 6.761 765.8 67.4 \nTU B2  2.051 1.875 0.098 3.487 5.839 12.025 558.8 43.5 \nTU B3 1.992 1.818 0.126 1.901 6.194 16.765 380.2 27.3 \n\n         \nAmplitude 0.215 0.185 0.872 7.970 5.388 15.374 658.2 60.5 \n\nM\u00e9dia 2.0241 1.8559 0.3052 6.1957 4.1189 5.8655 649.04 56.13 \nDesvio \nPadr\u00e3o \n\n0.04958 0.04189 0.23681 2.23705 1.51295 4.99129 150.641 15.067\n\n \n \n \n\n\n\n136 \n \n\nAnexo D \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. \n \n\nAMOSTRA  DP \n(g/cm 3 ) \n\nDAS \n(g/cm 3  \n\nRLS \n(%) \n\nRLQ \n(%)  \n\nPF (%) Abs \n(%) \n\nCR (N) MRF \n(MPa) \n\nCF1 B1 2.065 1.910 0.419 8.167 3.733 0.202 760.9 70.6 \nCF1 B2 2.029 1.881 0.658 8.231 5.334 1.641 738.7 69.0 \nCF1 B3 2.018 1.879 0.753 8.047 5.366 2.001 643.7 60.8 \nCF2 B1 2.018 1.879 0.508 5.024 7.720 7.531 556.4 46.5 \nCF2 B2 2.013 1.861 0.886 8.885 6.179 2.195 775.8 65.8 \nCR1 B3 2.082 1.890 0.086 7.202 2.776 0.360 745.6 64.2 \nCR1 B4 2.126 1.944 0.170 6.131 4.093 2.689 764.3 69.3 \nCR1 B5 2.041 1.851 0.193 0.742 3.096 15.557 172.5 12.5 \nCR2 B2 2.086 1.896 0.287 7.115 3.115 0.210 744.1 54.8 \nCR2 B3 2.083 1.909 0.219 7.619 3.942 1.811 706.0 65.8 \nPG B1 2.022 1.848 0.148 8.675 2.582 0.154 786.3 69.3 \nPG B2 2.040 1.855 0.146 7.785 2.261 0.138 926.6 82.2 \nPG B3 1.960 1.786 0.122 9.527 2.162 0.229 929.3 79.6 \nPG B4 2.005 1.836 0.342 8.043 5.605 4.475 723.0 66.8 \nPT1 B4 2.047 1.895 0.359 7.556 3.287 0.581 818.3 73.0 \nPT2 B3 2.006 1.853 0.283 8.275 3.033 0.741 720.4 65.3 \nPT3 B1 1.940 1.747 0.038 5.172 5.289 12.070 677.7 53.1 \nPI B1 1.965 1.802 0.042 7.012 2.505 3.976 598.1 49.3 \nPI B3 2.009 1.865 0.482 8.585 4.213 0.927 739.4 68.7 \nPI B4 2.007 1.851 0.328 7.087 5.507 4.890 785.4 69.0 \nTU B1 2.032 1.860 0.068 0.106 10.146 21.738 247.6 17.0 \nTU B2 2.047 1.873 0.110 6.162 5.790 5.814 724.5 60.2 \nTU B3 1.984 1.819 0.082 2.941 6.161 14.410 456.5 33.6 \n\n         \nAmplitude 0.186 0.197 0.848 9.421 7.984 21.600 756.8 69.7 \n\nM\u00e9dia 2.0272 1.8604 0.2926 6.6995 4.5172 4.5365 684.4 59.41 \nDesvio \nPadr\u00e3o \n\n0.04362 0.04316 0.23398 2.45970 1.94883 5.91185 181.683 17.703\n\n \n \n\n \n\n\n\n137 \n \n\nAnexo E - Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1000\u00b0C. (Minas CF, CR, PG, PT e PI) \n \nAMOSTRA RLQ (%) PF (%)  Abs (%) CR (N) MRF (MPa) \n\nCF1 B1 5.501 3.505 5.724 662.3 55.5 \n\nCF1 B2 5.504 4.945 6.625 537.8 47.0 \n\nCF1 B3 5.517 5.229 6.449 226.5 20.1 \n\nCF2 B2 5.473 5.682 8.583 619.3 49.8 \n\nCR1 B3 2.720 2.575 11.032 464.7 36.5 \n\nCR2 B2 4.085 2.666 8.810 596.7 48.2 \n\nCR2 B3 3.152 3.259 10.964 438.6 35.3 \n\nPG B1 3.545 2.093 10.179 575.3 42.8 \n\nPG B2 3.093 1.972 10.432 498.1 37.8 \n\nPG B3 2.232 1.988 13.460 478.4 34.3 \n\nPG B4 4.159 5.055 11.884 561.4 45.0 \n\nPT1 B4 2.966 2.975 10.656 468.2 36.9 \n\nPT2 B2 3.459 2.565 10.135 506.3 39.4 \n\nPI B1 2.012 1.997 14.113 343.7 24.0 \n\nPI B3 5.335 4.001 6.934 499.0 42.7 \n\nPI B4 3.750 4.735 11.938 608.7 47.5 \n\n      \n\nAmplitude 3.505 3.710 8.389 435.8 35.4 \n\nM\u00e9dia 3.9064 3.4526 9.8699 505.31 40.18 \n\nDesvio \n\nPadr\u00e3o \n\n1.22611 0.49096 2.49924 108.912 9.245 \n\n \n\n \n\n \n\n\n\n138 \n \n\nAnexo F \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C. (Minas CF, CR, PG, PT e PI) \n \n\nAMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) \n\nCF1 B1 7.121 3.506 3.202 677.4 61.3 \n\nCF1 B2 6.722 5.261 4.843 673.3 59.7 \n\nCF1 B3 6.630 5.577 5.567 589.1 52.5 \n\nCF2 B2 6.854 5.650 5.423 766.8 69.8 \n\nCR1 B3 5.579 2.752 5.572 602.4 51.2 \n\nCR2 B2 6.316 3.153 4.269 643.7 57.6 \n\nCR2 B3 5.031 3.644 7.218 566.0 47.3 \n\nPG B1 6.298 2.393 4.905 695.6 58.3 \n\nPG B2 5.942 2.138 5.183 681.1 56.7 \n\nPG B3 5.451 2.346 9.251 642.1 46.1 \n\nPG B4 5.674 5.642 9.374 593.3 50.3 \n\nPT1 B4 5.257 3.124 6.283 612.2 52.0 \n\nPT2 B3 6.192 2.952 4.972 606.8 50.9 \n\nPI B1 4.130 2.318 9.486 515.2 42.9 \n\nPI B3 6.988 4.256 3.897 715.3 64.1 \n\nPI B4 5.027 5.407 9.154 768.9 65.3 \n\n      \n\nAmplitude 2.991 3.512 6.284 253.7 26.9 \n\nM\u00e9dia 5.9508 3.7574 6.1624 646.83 55.38 \n\nDesvio \n\nPadr\u00e3o \n\n0.84024 1.33480 2.07216 70.255 7.490 \n\n \n\n \n\n \n\n\n\n139 \n \n\nAnexo G \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C. (Minas CF, CR, PG, PT e PI) \n \n\nAMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) \n\nCF1 B1 7.969 3.480 1.421 727.2 67.0 \n\nCF1 B2 7.497 5.227 3.147 766.4 69.3 \n\nCF1 B3 7.451 5.626 4.014 609.7 56.1 \n\nCF2 B2 8.184 6.338 3.629 763.0 69.5 \n\nCR1 B3 7.104 2.735 2.437 738.9 64.2 \n\nCR2 B2 7.491 3.052 1.798 743.2 67.4 \n\nCR2 B3 6.689 3.682 4.067 629.1 56.4 \n\nPG B1 8.195 2.454 1.758 751.0 66.3 \n\nPG B2 7.601 2.170 2.134 792.5 70.3 \n\nPG B3 7.974 2.247 4.057 771.5 63.3 \n\nPG B4 6.916 5.280 7.096 669.4 59.7 \n\nPT1 B4 6.969 3.263 2.794 725.4 64.6 \n\nPT2 B3 7.743 2.980 2.252 635.1 57.3 \n\nPI B1 5.934 2.386 6.039 576.3 45.5 \n\nPI B2 8.078 4.455 1.959 752.8 68.2 \n\nPI B4 6.109 5.091 6.761 765.8 67.4 \n\n      \n\nAmplitude 2.261 4.168 5.675 216.2 24.0 \n\nM\u00e9dia 7.3690 3.7791 3.4602 713.58 63.28 \n\nDesvio \n\nPadr\u00e3o \n\n0.70007 1.35970 1.80192 66.937 6.691 \n\n \n\n \n\n \n\n\n\n140 \n \n\nAnexo H \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. (Minas CF, CR, PG, PT e PI) \n \n\nAMOSTRA RLQ (%) PF (%) Abs (%) CR (N) MRF (MPa) \n\nCF1 B1 8.167 3.733 0.202 760.9 70.6 \n\nCF1 B2 8.231 5.334 1.641 738.7 69.0 \n\nCF1 B3 8.047 5.366 2.001 643.7 60.8 \n\nCF2 B2 8.885 6.179 2.195 775.8 65.8 \n\nCR1 B3 7.202 2.776 0.360 745.6 64.2 \n\nCR2 B2 7.115 3.115 0.210 744.1 54.8 \n\nCR2 B3 7.619 3.942 1.811 706.0 65.8 \n\nPG B1 8.675 2.582 0.154 786.3 69.3 \n\nPG B2 7.785 2.261 0.138 926.6 82.2 \n\nPG B3 9.527 2.162 0.229 929.3 79.6 \n\nPG B4 8.043 5.605 4.475 723.0 66.8 \n\nPT1 B4 7.556 3.287  0.581 818.3 73.0 \n\nPT2 B3 8.275 3.033 0.741 720.4 65.3 \n\nPI B1 7.012 2.505 3.976 598.1 49.3 \n\nPI B3 8.585 4.213 0.927 739.4 68.7 \n\nPI B4 7.087 5.507 4.890 785.4 69.0 \n\n      \n\nAmplitude 2.515 4.017 4.752 331.2 32.9 \n\nM\u00e9dia 7.9882 3.8500 1.5332 758.85 67.14 \n\nDesvio \n\nPadr\u00e3o \n\n0.71600 1.35089 1.61216 84.975 8.037 \n\n \n\n \n\n\n\n141 \n \n\nAnexo I \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1020\u00b0C. (Minas CF, CR, PG, PT, PI e \nTU) \n\n \n\nAMOSTRA DP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%) \n\nPF (%) ABS \n\n(%) \n\nCR (N) MRF \n\n(MPa) \n\nCF1 B1 2.058 1.985 0.381 7.121 3.506 3.202 677.4 61.3 \n\nCF1 B2 2.006 1.863 0.668 6.722 5.261 4.843 673.3 59.7 \n\nCF1 B3 1.973 1.848 0.794 6.630 5.577 5.567 589.1 52.5 \n\nCF2 B1 2.008 1.867 0.500 2.246 7.598 13.863 428.0 33.6 \n\nCF2 B2 2.013 1.846 0.851 6.854 5.650 5.423 766.8 69.8 \n\nCR1 B3 2.076 1.887 0.094 5.579 2.752 5.572 602.4 51.2 \n\nCR1 B4 2.119 1.935 0.136 1.398 3.993 12.527 445.9 34.8 \n\nCR2 B2 2.088 1.897 0.279 6.316 3.153 4.269 643.7 57.6 \n\nCR2 B3 2.073 1.897 0.207 5.031 3.644 7.218 566.0 47.3 \n\nPG B1 2.022 1.845 0.173 6.298 2.393 4.905 695.6 58.3 \n\nPG B2 2.044 1.853 0.159 5.942 2.138 5.183 681.1 56.7 \n\nPG B3 1.928 1.751 0.118 5.451 2.346 9.251 642.1 46.1 \n\nPG B4 1.991 1.823 0.344 5.674 5.642 9.374 593.3 50.3 \n\nPT1 B4 2.052 1.893 0.359 5.257 3.124 6.283 612.2 52.0 \n\nPT2 B3 2.005 1.850 0.275 6.192 2.952 4.972 606.8 50.9 \n\nPI B1 1.963 1.800 0.056 4.130 2.318 9.486 515.2 42.9 \n\nPI B3 2.003 1.871 0.548 6.988 4.256 3.897 715.3 64.1 \n\nPI B4 1.999 1.850 0.317 5.027 5.407 9.154 768.9 65.3 \n\nTU B2 2.051 1.906 0.118 1.724 5.742 15.129 478.2 35.6 \n\nTU B3 1.984 1.816 0.082 0.806 6.041 18.718 328.1 22.9 \n\n         \n\nAmplitude 0.191 0.234 0.795 6.315 5.460 15.516 440.8 46.9 \n\nM\u00e9dia 2.0228 1.8642 0.3230 5.0693 4.1747 7.9418 601.47 50.65 \n\nDesvio \n\nPadr\u00e3o \n\n0.04681 0.04992 0.23783 1.97134 1.57618 4.22980 114.976 12.003\n\n \n\n \n \n\n\n\n142 \n \n\nAnexo J \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1040\u00b0C. (Minas CF, CR, PG, PT, PI e \nTU) \n\n \n\nAMOSTRA  DP \n(g/cm 3 ) \n\nDAS \n(g/cm 3 ) \n\nRLS \n(%) \n\nRLQ \n(%) \n\nPF (%) Abs \n(%) \n\nCR (N) MRF \n(MPa) \n\nCF1 B1 2.063 1.896 0.357 7.969 3.480 1.421 727.2 67.0 \nCF1 B2 2.018 1.873 0.674 7.497 5.227 3.147 766.4 69.3 \nCF1 B3 1.981 1.857 0.825 7.451 5.626 4.014 609.7 56.1 \nCF2 B1 1.985 1.849 0.501 3.511 7.558 12.135 478.6 37.4 \nCF2 B2 2.008 1.852 0.928 8.184 6.338 3.629 763.0 69.5 \nCR1 B3 2.075 1.885 0.098 7.104 2.735 2.437 738.9 64.2 \nCR1 B4 2.114 1.930 0.138 2.974 4.045 9.689 575.4 47.1 \nCR2 B2 2.083 1.892 0.341 7.491 3.052 1.798 743.2 67.4 \nCR2 B3 2.076 1.898 0.209 6.689 3.682 4.067 629.1 56.4 \nPG B1 2.019 1.839 0.114 8.195 2.454 1.758 751.0 66.3 \nPG B2 2.030 1.854 0.136 7.601 2.170 2.134 798.5 70.3 \nPG B3 1.951 1.781 0.094 7.974 2.247 4.057 771.5 63.3 \nPG B4 2.006 1.813 0.360 6.916 5.280  7.096 669.4 59.7 \nPT1 B4 2.046 1.887 0.349 6.969 3.263 2.794 725.4 64.6 \nPT2 B3 2.006 1.848 0.267 7.743 2.980 2.252 635.1  57.3 \nPI B1 1.967 1.800 0.062 5.934 2.386 6.039 576.3 45.5 \nPI B3 2.003 1.870 0.514 8.078 4.455 1.959 752.8 68.2 \nPI B4 1.998 1.843 0.317 6.109 5.091 6.761 765.8 67.4 \nTU B2 2.051 1.875 0.098 3.487 5.839 12.025 558.8 43.5 \nTU B3 1.992 1.818 0.126 1.901 6.194 16.765 380.2 27.3 \n\n         \nAmplitude 0.163 0.149 0.866 6.294 5.388 15.344 418.3 43.0 \n\nM\u00e9dia 2.0236 1.8580 0.3254 6.4889 4.2051 5.2989 670.82 58.39 \nDesvio \nPadr\u00e3o \n\n0.04274 0.03646 0.25157 1.93347 1.83622 4.26439 110.377 11.944\n\n \n \n \n \n\n \n \n\n\n\n143 \n \n\nAnexo K \u2013 Vari\u00e1veis f\u00edsicas obtidas na temperatura de queima de 1060\u00b0C. (Minas CF, CR, PG, PT, PI e \nTU) \n\n \n\nAMOSTRA DP \n\n(g/cm 3 ) \n\nDAS \n\n(g/cm 3 ) \n\nRLS \n\n(%) \n\nRLQ \n\n(%)  \n\nPF (%) Abs \n\n(%) \n\nCR (N) MRF \n\n(MPa) \n\nCF1 B1 2.065 1.910 0.419 8.167 3.733 0.202 760.9 70.6 \n\nCF1 B2 2.029 1.881 0.658 8.231 5.334 1.641 738.7 69.0 \n\nCF1 B3 2.018 1.879 0.753 8.047 5.366 2.001 643.7 60.8 \n\nCF2 B1 2.018 1.879 0.508 5.024 7.720 7.531 556.4 46.5 \n\nCF2 B2 2.013 1.861 0.886 8.885 6.179 2.195 775.8 65.8 \n\nCR1 B3 2.082 1.890 0.086 7.202 2.776 0.360 745.6 64.2 \n\nCR1 B4 2.126 1.944 0.170 6.131 4.093 2.689 764.3 69.3 \n\nCR2 B2 2.086 1.896 0.287 7.115 3.115 0.210 744.1 54.8 \n\nCR2 B3 2.083 1.909 0.219 7.619 3.942 1.811 706.0 65.8 \n\nPG B1 2.022 1.848 0.148 8.675 2.582 0.154 786.3 69.3 \n\nPG B2 2.040 1.855 0.146 7.785 2.261 0.138 926.6 82.2 \n\nPG B3 1.960 1.786 0.122 9.527 2.162 0.229 929.3 79.6 \n\nPG B4 2.005 1.836 0.342 8.043 5.605 4.475 723.0 66.8 \n\nPT1 B4 2.047 1.895 0.359 7.556 3.287 0.581 818.3 73.0 \n\nPT2 B3 2.006 1.853 0.283 8.275 3.033 0.741 720.4 65.3 \n\nPI B1 1.965 1.802 0.042 7.012 2.505 3.976 598.1 49.3 \n\nPI B3 2.009 1.865 0.482 8.585 4.213 0.927 739.4 68.7 \n\nPI B4 2.007 1.851 0.328 7.087 5.507 4.890 785.4 69.0 \n\nTU B2 2.047 1.873 0.110 6.162 5.790 5.814 724.5 60.2 \n\nTU B3 1.984 1.819 0.082 2.941 6.161 14.410 456.5 33.6 \n\n         \n\nAmplitude 0.166 0.158 0.844 6.586 5.558 14.272 472.8 48.6 \n\nM\u00e9dia 2.0306 1.8666 0.3215 7.4035 4.2682 2.7488 732.17 64.19 \n\nDesvio \n\nPadr\u00e3o \n\n0.04219 0.03788 0.23648 1.48494 1.58971 3.48596 109.468 7.995 \n\n \n\n \n\n \n\n \n\n\n\tCAPA\n\tFICHA CATALOGR\u00c1FICA\n\tCOMISS\u00c3O EXAMINADORA\n\tDEDICAT\u00d3RIA\n\tAGRADECIMENTOS\n\tRESUMO\n\tABSTRACT\n\tLISTA DE FIGURAS\n\tLISTA DE TABELAS\n\tLISTA DE ANEXOS\n\tLISTA DE ABREVIATURAS E SIGLAS\n\tSUM\u00c1RIO\n\t1. INTRODU\u00c7\u00c3O\n\t1.1. Objetivo Geral e Hip\u00f3tese\n\t1.2. Estrutura\n\n\t2. REVIS\u00c3O BIBLIOGR\u00c1FICA\n\t2.1. O Neur\u00f4nio Biol\u00f3gico Natural\n\t2.2. As Redes Neurais Artificiais\n\t2.3. Breve Hist\u00f3rico sobre as Redes Neurais Artificiais\n\t2.4. Regra Delta e Regra Delta Generalizada\n\n\t3. MATERIAIS E M\u00c9TODOS\n\t3.1. O P\u00f3lo Cer\u00e2mico de Santa Gertrudes\n\t3.2. Caracteriza\u00e7\u00e3o das Argilas e das Cer\u00e2micas\n\t3.3. Os Corpos-de-Prova\n\t3.4. Determina\u00e7\u00e3o das Vari\u00e1veis F\u00edsicas dos Corpos-de-Prova\n\t3.5. Metodologia\n\n\t4. RESULTADOS E DISCUSS\u00d5ES\n\t4.1. Introdu\u00e7\u00e3o\n\t4.2. Regra de Aprendizado Backpropagation (Formato B\u00e1sico)\n\t4.3. Regra de Aprendizado Backpropagation com Momento (traingdm)\n\t4.4. Regra de Aprendizado Backpropagation Resiliente (trainrp)\n\t4.5. Regra de Aprendizado Backpropagation de Levenberg-Marquadt (trainlm)\n\t4.6. Conclus\u00f5es\n\n\t5. CONSIDERA\u00c7\u00d5ES FINAIS\n\tREFER\u00caNCIA BIBLIOGR\u00c1FICA\n\tBIBLIOGRAFIA COMPLEMENTAR\n\tANEXOS"}]}}}