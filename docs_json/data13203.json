{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.17062"}, {"@name": "filename", "#text": "23789_VITORHIRAYAMA.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "VITOR HIRAYAMA \n \n \n \n \n \n \n \n \n \n \n \n \n\nClassificador de Qualidade de \u00c1lcool \nCombust\u00edvel e Poder Calor\u00edfico de G\u00e1s \n\nGLP \n \n \n \nDisserta\u00e7\u00e3o apresentada \u00e0 \nEscola Polit\u00e9cnica da \nUniversidade de S\u00e3o Paulo \npara obten\u00e7\u00e3o do T\u00edtulo de \nMestre em Engenharia \n\n \n \n \n \n \n \n \n\nS\u00c3O PAULO \n2004 \n\n\n\nVITOR HIRAYAMA \n \n \n \n \n \n \n \n \n \n \n \n \n\nClassificador de Qualidade de \u00c1lcool \nCombust\u00edvel e Poder Calor\u00edfico de G\u00e1s \n\nGLP \n \n \n \nDisserta\u00e7\u00e3o apresentada \u00e0 \nEscola Polit\u00e9cnica da \nUniversidade de S\u00e3o Paulo \npara obten\u00e7\u00e3o do T\u00edtulo de \nMestre em Engenharia \n\n \n\u00c1rea de Concentra\u00e7\u00e3o: \nMicroeletr\u00f4nica \n\n \nOrientador: \nProf. Dr. \nWalter Jaimes Salcedo \n\n \n \n\nS\u00c3O PAULO \n2004 \n\n\n\n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\nFICHA CATALOGR\u00c1FICA \n\n \n \nHirayama, Vitor \n\nClassificador de qualidade de \u00e1lcool combust\u00edvel e poder  \nCalor\u00edfico de g\u00e1s GLP / Vitor Hirayama -- S\u00e3o Paulo, 2004. \n\n82 p.  \n \n\nDisserta\u00e7\u00e3o (Mestrado) \u2013 Escola Polit\u00e9cnica da Universidade  \nde S\u00e3o Paulo. Departamento de Microeletr\u00f4nica \n \n\n1. Redes neurais 2. Reconhecimento de padr\u00f5es 3. Fuzzy \n4. Componentes principais (an\u00e1lise) 5. Combust\u00edveis gasosos \n(classifica\u00e7\u00e3o) I. Universidade de S\u00e3o Paulo. Escola Polit\u00e9cnica. \nDepartamento de Microeletr\u00f4nica II.t \n\n \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \nAos meus pais e familiares, que tanto me \n\n \n\napoiaram na realiza\u00e7\u00e3o deste trabalho. \n\n\n\n \n\nAGRADECIMENTOS \n\n \nGostaria de agradecer o apoio do meu orientador e amigo prof. Walter \n\nJaimes Salcedo, e tamb\u00e9m ao meu amigo prof. Francisco Javier Ramirez-\n\nFernandes por dividir o seu imenso intelecto para que fosse poss\u00edvel realizar \n\nesta obra. \n\nAgrade\u00e7o aos meus familiares, pelo suporte e incentivo. \u00c0 empresa \n\nAlstom Brasil LTDA. Unidade Casa Verde, por me incentivar e permitir a \n\nrealiza\u00e7\u00e3o deste trabalho. \n\nTamb\u00e9m agrade\u00e7o a todos os meus amigos do grupo SIM: Germ\u00e1n, \n\nDaniel, S\u00edlvia, Wanderson, Maur\u00edcio, G\u00e9rson, Henrique, Thiago, Christian e \n\nPaulo; al\u00e9m de outras pessoas que diretamente ou indiretamente me \n\najudaram a realizar esta Disserta\u00e7\u00e3o. \n\n \n\n\n\n \n\nResumo \n\nEste trabalho apresenta os resultados obtidos com o desenvolvimento de um \n\nsistema robusto como uma alternativa de reconhecimento da qualidade de \n\nvapor de \u00e1lcool combust\u00edvel e do poder calor\u00edfico do g\u00e1s combust\u00edvel GLP \n\nem um nariz eletr\u00f4nico. Foram implementadas duas metodologias \n\nexperimentais para a extra\u00e7\u00e3o de atributos dos padr\u00f5es de vapor de \u00e1lcool \n\ncombust\u00edvel e de g\u00e1s GLP. Na primeira abordagem de tratamento dos \n\ndados, foram usados um Sistema de Infer\u00eancia Fuzzy (FIS), e dois \n\nalgoritmos de treinamento de Redes Neurais Artificiais (RNA) para \n\nreconhecer padr\u00f5es de vapor de \u00e1lcool combust\u00edvel: a Backpropagation e \n\nLearning Vector Quantization. A segunda abordagem para o tratamento dos \n\ndados foi desenvolver um sistema reconhecedor do poder calor\u00edfico do g\u00e1s \n\nGLP robusto \u00e0 perda aleat\u00f3ria de um dos sensores. Foram usados tr\u00eas \n\nsistemas. No primeiro foi implementada uma RNA para reconhecer todos os \n\ndados que simulavam a falha de um sensor aleat\u00f3rio. O resultado desse \n\nsistema foi de 97% de acertos. O segundo implementou sete RNA\u2019s \n\ntreinadas com subconjuntos dos dados de entrada, tais que seis RNA\u2019s \n\nforam treinadas com um sensor diferente com falha; e a s\u00e9tima RNA foi \n\ntreinada com dados dos sensores sem falhas. O resultado desse sistema foi \n\nde 99% de acertos. O terceiro implementou uma M\u00e1quina de Comit\u00ea \n\nEst\u00e1tica Ensemble constitu\u00edda de dez RNA\u2019s em paralelo para resolver o \n\nproblema. O resultado foi de 97% de acertos. As RNA\u2019s tiveram melhores \n\nrespostas que os FIS. Foram sugeridas algumas formas de implementa\u00e7\u00e3o \n\nem hardware do sistema reconhecedor em sistemas pr\u00e9-fabricados com \n\nDSP\u2019s e micro-controladores. \n\n\n\n \n\nAbstract \nThis work shows the results of a robust system development as an \n\nalternative to recognize the quality of an alcohol fuel vapor sample and Liquid \n\nPetrol Gas (LPG) heat power in an electric nose. Two experimental \n\nmethodologies were implemented to extract the features of alcohol fuel vapor \n\nand LPG gas patterns. The first approach to process the data used an Fuzzy \n\nInference System (FIS) and two training algorithms of Artificial Neural \n\nNetworks (ANN) to recognize alcohol fuel vapor patterns: Backpropagation \n\nand Learning Vector Quantization. The second approach consists of process \n\ndata to develop an LPG heat power recognizing system robust to one-\n\nrandom-sensor-loss. Three systems were used. The first implemented an \n\nANN to recognize all data that simulated the failure of a random sensor. This \n\nsystem had 97% of right responses. The second implemented seven ANN\u2019s \n\ntrained with input data subsets, such that six ANN\u2019s were trained with a \n\ndifferent failure sensor, and the seventh ANN was trained with data of all \n\nsensors without failure. This system had 99% of right responses. The third \n\nimplemented an Ensemble Static Learning Machine containing ten parallel \n\nRNA\u2019s to solve the problem. The result were 97% of right responses. RNA\u2019s \n\nhad better results than FIS. Some ways of hardware implementation of the \n\nrecognizing system were suggested in DSP and micro-controllers pre-built \n\nsystems. \n\n \n\n\n\n \n\nSUM\u00c1RIO: \n\n \nLISTA DE TABELAS \n\nLISTA DE FIGURAS \n\n1 Introdu\u00e7\u00e3o................................................................................................................1 \n\n1.1 Objetivos .....................................................................................................1 \n\n1.2 Justificativa .................................................................................................2 \n\n1.3 Estrutura do Texto .....................................................................................3 \n\n2 Fundamentos Te\u00f3ricos ..........................................................................................6 \n\n2.1 Princ\u00edpio de Funcionamento do Sensor de G\u00e1s Taguchi ...................6 \n\n2.2 Sensores que utilizam tecnologias comuns e n\u00e3o-comuns ................ 8 \n\n2.3 Sensores inteligentes ................................................................................ 9 \n\n2.3.1 Sensores que utilizam Redes Neurais Artificiais ..................................9 \n\n2.3.2 Sensores que utilizam L\u00f3gica Difusa ...................................................11 \n\n2.4 Sensores inteligentes para reconhecer a qualidade do \u00e1lcool ........ 13 \n\n3 As Redes Neurais Artificiais ................................................................................15 \n\n3.1 As M\u00e1quinas de Comit\u00ea ......................................................................... 21 \n\n3.2 A L\u00f3gica Fuzzy......................................................................................... 23 \n\n4 Metodologia Experimental...................................................................................27 \n\n4.1 Procedimento para coleta da amostra de \u00e1lcool combust\u00edvel..........29 \n\n4.2 Primeiro m\u00e9todo de extra\u00e7\u00e3o de caracter\u00edsticas do g\u00e1s GLP ..........31 \n\n4.3 Procedimento para extra\u00e7\u00e3o e tratamento dos dados do GLP........ 33 \n\n4.3.1 Procedimento para coleta da amostra de g\u00e1s GLP ...........................33 \n\n4.3.2 Procedimento para efetuar as medi\u00e7\u00f5es de g\u00e1s GLP.......................35 \n\n5 Pr\u00e9-Processamento dos Dados .........................................................................37 \n\n5.1 Reconheciment o da qualidade de \u00e1lcool com Redes Neurais ........ 43 \n\n5.2 Implementa\u00e7\u00e3o da RNA em DSP (Digital Signal Processor) ...........45 \n\n5.2.1 O programa implementado no DSP ..................................................... 47 \n\n\n\n \n\n5.2.2 Compila\u00e7\u00e3o e Linkagem ......................................................................... 48 \n\n5.2.3 Implementa\u00e7\u00e3o da Rede Neural em Hardware ..................................48 \n\n5.2.4 Resultados do programa implementado no DSP ...............................49 \n\n5.3 Implementa\u00e7\u00e3o do Sistema de Infer\u00eancia Fuzzy ...............................50 \n\n5.3.1 FIS com os dados sem pr\u00e9-processamento e PCA...........................51 \n\n5.3.1.1 Modelamento das Vari\u00e1veis Ling\u00fc\u00edsticas ........................................ 53 \n\n5.3.1.2 Base de Conhecimento do FIS ......................................................... 55 \n\n5.3.1.3 Metodologia utilizada para realizar os tratamentos dos dados .... 56 \n\n5.3.1.4 Resultados Obtidos com o Sistema de Infer\u00eancia Fuzzy............. 57 \n\n5.3.2 FIS utilizando dados com pr\u00e9-processamento e PCA.......................59 \n\n5.3.2.1 Modelamento das Vari\u00e1veis Ling\u00fc\u00edsticas ........................................ 61 \n\n5.3.2.2 Metodologia utilizada para realizar os tratamentos dos dados .... 63 \n\n5.3.2.3 Resultados Obtidos com o Sistema de Infer\u00eancia Fuzzy............. 64 \n\n5.4 Reconhecimento do poder calor\u00edfico do GLP em Redes Neurais ...65 \n\n5.4.1 Tratamento dos dados experimentais do GLP...................................68 \n\n5.4.2 Primeiro Experimento: Rede Neural Simples ..................................... 70 \n\n5.4.3 Tratamento dos dados que simulam um sensor com falha ..............73 \n\n5.4.4 Segundo Experimento: RNA de sistema com um sensor em falha 73 \n\n5.4.5 Terceiro Experimento: Implementa\u00e7\u00e3o das M\u00e1quinas de Comit\u00ea ...74 \n\n5.4.6 Quarto Experimento: Redes Ensemble ...............................................75 \n\n6 Conclus\u00f5es e Perspectivas Futuras ..................................................................77 \n\n7 Bibliografia: ............................................................................................................79 \n\n \n\n\n\n \n\nLISTA DE FIGURAS: \n \n\nFig. 2.1a: Caracter\u00edsticas do Sensor Taguchi TGS822.......................................7 \n\nFig. 2.1b: Polariza\u00e7\u00f5es dos sens ores Taguchi por divis\u00e3o de tens\u00e3o e a que \n\nevita a resist\u00eancia por contatos.......................................................................7 \n\nFig. 2.2a: Assinaturas de cores a concentra\u00e7\u00f5es baixas de gases. .................8 \n\nFig. 2.3.2a: Reconhecimento de sinais de olfato com l\u00f3gica fuzzy. ................ 12 \n\nFig. 2.3.2b: An\u00e1lises feitas do sinal de olfato pelo sistema fuzzy. ..................12 \n\nFig. 2.4a: Sistema para reconhecimento de qualidade de \u00e1lcoois ..................13 \n\nFig. 2.4b: Sistema para reconhecer a qualidade de \u00e1lcool combust\u00edvel. .......14 \n\nFig. 3a: Modelo de um neur\u00f4nio artificial............................................................. 16 \n\nFig 4.1a: C\u00e2mara de medidas. ..............................................................................29 \n\nFig. 5.4a: Comportamento t\u00edpico dos sensores na inje\u00e7\u00e3o da amostra......... 32 \n\nFig. 5.4b: Dados Sint\u00e9ticos usados para a cria\u00e7\u00e3o da Rede Neural e o FIS 32 \n\nFig. 4.3.1a:  C\u00e2mara de medidas...........................................................................33 \n\nFig. 4.3.1b: Sistema de coleta do g\u00e1s ..................................................................34 \n\nFig. 4.3.1c: VI que controla a mistura de g\u00e1s com nitrog\u00eanio.......................... 35 \n\nFig. 5a: O aplicativo Data Sculptor .......................................................................37 \n\nFig. 5b: Gr\u00e1fico obtido com o Data Sculptor antes do pr\u00e9-processamento...38 \n\nFig. 5c: Gr\u00e1fico obtido com o Data Sculptor ap\u00f3s o pr\u00e9-processamento.......41 \n\nFig. 5.1a: Varia\u00e7\u00e3o da resist\u00eancia versus resist\u00eancia inicial (sensor 6) ......... 45 \n\nFig. 5.3a: Clusteriza\u00e7\u00e3o Subtrativa utilizada no FIS ..........................................51 \n\nFig. 5.3.1a: Dados sint\u00e9ticos de cada sensor usados no FIS.......................... 52 \n\nFig. 5.3.1b: Sistema Fuzzy com os dados sem pr\u00e9 -processamento e PCA..53 \n\nFig. 5.3.1.1a: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Resist\u00eancia Inicial ............. 54 \n\nFig. 5.3.1.1b: Fun\u00e7\u00f5es de Pertin\u00eancia ajustadas para cada sensor...............55 \n\nFig. 5.3.1.1c: Fun\u00e7\u00f5es de pertin\u00eancia da sa\u00edda do FIS. .................................... 55 \n\nFig. 5.3.2a: Gr\u00e1fico dos dois Componentes Principais com os Clusters dos \n\ndados de treino ................................................................................................ 60 \n\nFig. 5.3.2b: Dados sint\u00e9ticos de teste usados no FIS. ......................................60 \n\nFig. 5.3.2c: Sistema FIS utilizado com os dados de treino submetidos ao pr\u00e9 -\n\nprocessados e an\u00e1lise de componentes principais ...................................61 \n\n\n\n \n\nFig. 5.3.2.1a: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Ling\u00fc\u00edstica PrinComp1..... 61 \n\nFig. 5.3.2.1b: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Ling\u00fc\u00edstica PrinComp2..... 62 \n\nFig. 5.3.2.1c: Fun\u00e7\u00f5es de pertin\u00eancia da sa\u00edda do FIS. .................................... 62 \n\nFig. 5.3.2.3a: Resultado do FIS para os dados com pr\u00e9 -processamento e \n\nan\u00e1lise de componentes principais. ............................................................. 65 \n\nFig. 5.4b: Amostras do G\u00e1s GLP e da mistura GLP com Nitrog\u00eanio.............. 69 \n\nFig. 5.4.2a: RNA usada no Reconhecimento dos dados experimentais ........ 71 \n\nFig. 5.4.2b: Exemplo de curva de erro obtido dos dados experimentais .......71 \n\nFig. 5.4.2c: Rede Neural com PCA treinada com os dados experimentais ...72 \n\nFig. 5.4.2d: Exemplo de curva de erro da RNA com dados experimentais ...72 \n\nFig. 5.4.5a: Primeira proposta de m\u00e1quina de comit\u00ea .......................................74 \n\nFig. 5.4.6a: M\u00e1quina de Comit\u00ea Ensemble......................................................... 76 \n\n \n\n\n\n \n\nLISTA DE TABELAS: \n \n\nTabela 4a: Detalhamento dos experimentos realizados ...................................27 \n\nTabela 4a: Compostos Detectados para cada sensor ......................................28 \n\nTabela 5a: Valores Desejados adotados para as sa\u00eddas ................................. 38 \n\nTabela 5b: Separa\u00e7\u00e3o dos dados feita pelo aplicativo Data Sculptor ............ 41 \n\nTabela 5.1a: Resultados obtidos das RNA\u2019s no reconhecimento de \u00e1lcool ..45 \n\nTabela 5.2a: Coeficientes utilizados no aplicativo Neural Works .................... 46 \n\nTabela 5.2b: Itera\u00e7\u00f5es necess\u00e1rias para o treinamento da rede neural ........ 46 \n\nTabela 5.2.4a: Resultados obtidos nas simula\u00e7\u00f5es das amostras de \u00e1lcool 50 \n\nTabela 5.4a: Treinamentos das RNA\u2019s com Matlab ..........................................66 \n\nTabela 5.4b: Treinamento das RNA\u2019s com Neural Works ................................ 67 \n\nTabela 5.4.2a: Propriedades da RNA sem PCA ................................................ 71 \n\nTabela 5.4.2b: Par\u00e2metros utilizados na RNA com PCA..................................72 \n\nTabela 5.4.6a: Resultados de RNA simples ou m\u00e1quinas de comit\u00ea, que \n\nsimulavam a falha de um dos sensores. ..................................................... 76 \n\n \n\n\n\n \n\n \n\nAcr\u00f4nimos, Abreviaturas e Nota\u00e7\u00f5es Utilizadas: \n\nID \u2013 identification number \u2013  n\u00famero de identifica\u00e7\u00e3o do equipamento. \n\n? \u2013  Taxa de aprendizado; \n\n? \u2013 Momentum. Os pesos s\u00e3o atualizados na propor\u00e7\u00e3o do erro (e) e as \n\nentradas na conex\u00e3o (x). O termo momentum \u00e9 usado para ajudar a suavizar as \n\nmudan\u00e7as dos pesos. \n\n? \u2013  Limiar ou threshold. O aprendizado pode ser limitado a um limiar de erro, \n\nque corresponde ao valor de ?. Caso ? for positivo, qualquer valor absoluto de erro \n\nmenores que ? ser\u00e3o ignorados. \n\nI \u2013  Soma ponderada para os neur\u00f4nios da camada corrente. Os componentes \n\ndesse vetor s\u00e3o I = (I1, I2, ..., In), onde Ii \u00e9 a soma ponderada do i-\u00e9simo neur\u00f4nio da \n\ncamada corrente. \n\nY \u2013 Sa\u00edda da camada corrente. \u00c9 o resultado da computa\u00e7\u00e3o da soma \n\nponderada, fun\u00e7\u00e3o de transfer\u00eancia, e ent\u00e3o a sa\u00edda. As componentes de Y s\u00e3o \n\nY=(y1,y2....yn), onde yi \u00e9 a sa\u00edda do i-\u00e9simo neur\u00f4nio da camada. \n\nD \u2013 Sa\u00edda desejada para a camada corrente. A estrat\u00e9gia de controle \n\nusualmente assume que di est\u00e1 contido no campo erro do i-\u00e9simo neur\u00f4nio.  \n\nE \u2013  Vetor de erro. Caso seja a camada de sa\u00edda, o erro ser\u00e1 o corrente ou o \n\ntransformado pela derivada da fun\u00e7\u00e3o de transfer\u00eancia. Caso seja de outras camadas, \n\no erro ser\u00e1 acumulado, ou o erro transformado retropropagado.  \n\nXi \u2013 Conex\u00f5es de entrada para o i-\u00e9simo neur\u00f4nio da camada. S\u00e3o as sa\u00eddas \n\ndos neur\u00f4nios nos quais o neur\u00f4nio corrente \u00e9 conectado.  \n\nWi \u2013  Vetor inicial dos pesos na camada. Somente pesos vari\u00e1veis participam \n\ndo processo de aprendizado. Os outros tipos (fixed, set &amp; mod) n\u00e3o aprendem.  \n\nWi' \u2013 Vetor de erro depois que ele foi atualizado pela regra de aprendizado. \n\nMi \u2013 Mem\u00f3ria da \u00faltima mudan\u00e7a nos pesos. Muitas regras de aprendizado \n\nchamam esse termo de momentum.  \n\n\n\n \n\nAi \u2013 Campo auxiliar do peso, que \u00e9 usado para o momentum em algumas \n\nregras de aprendizado. \n\nn \u2013  N\u00famero de entradas do neur\u00f4nio corrente. \n\nRNA \u2013 Abrevia\u00e7\u00e3o de Redes Neurais Artificiais. \n\n[x]\u00b9 - Fun\u00e7\u00e3o especial definida como [x]\u00b9 = 1 se x > ?; 0 caso contr\u00e1rio. \n\n[x]p \u2013 Fun\u00e7\u00e3o definida como [x]p = 1 se x>0; 0 caso contr\u00e1rio. \n\ne-nose \u2013 Abrevia\u00e7\u00e3o de Eletronic Noses. S\u00e3o os chamados narizes \n\neletr\u00f4nicos. \n\nFIS \u2013  Abrevia\u00e7\u00e3o de Fuzzy Inference System, ou Sistema de Infer\u00eancia \n\nFuzzy. \n\nGLP \u2013  Abrevia\u00e7\u00e3o para G\u00e1s Liquefeito de Petr\u00f3leo \n\nMLP \u2013  Abrevia\u00e7\u00e3o para Multilayer Perceptron ou Perceptron de M\u00faltiplas \n\nCamadas. \n\nPCA \u2013  Abrevia\u00e7\u00e3o de Principal Component Analysis ou An\u00e1lise de \n\nComponentes Principais. \n\nSOM \u2013 Abrevia\u00e7\u00e3o para Self Organizing Maps, ou Mapas Auto Organiz\u00e1veis. \n\n \n\n\n\n1 \n\n \n\n1 Introdu\u00e7\u00e3o \n\nA possibilidade de realizar medidas diretas com poucos refinamentos e \n\nfacilidade de implementa\u00e7\u00e3o [1], levou ao crescimento do interesse dos chamados \n\nnarizes eletr\u00f4nicos, tanto no meio acad\u00eamico como na ind\u00fastria. Um nariz eletr\u00f4nico \n\ntem diversas aplica\u00e7\u00f5es, tais como reconhecimento de cheiros de alimentos s\u00f3lidos e \n\nl\u00edquidos [2, 3, 4], perfumes e reagentes qu\u00edmicos [4], detec\u00e7\u00e3o de c\u00e2ncer de pulm\u00e3o \n\natrav\u00e9s do ar expirado do paciente [5], medida do teor alco\u00f3lico do h\u00e1lito de um \n\nmotorista [1], monitora\u00e7\u00e3o da qualidade de \u00e1gua pot\u00e1vel [6], reconhecimento de padr\u00f5es \n\nde combust\u00edveis [7], entre outros. \n\nUm nariz eletr\u00f4nico \u00e9 em geral implementado com o aux\u00edlio de Redes Neurais \n\nArtificiais, por causa da sua robustez a ru\u00eddos que podem estar presentes nas amostras \n\nanalisadas [8], al\u00e9m de sua grande capacidade de generaliza\u00e7\u00e3o, o qual promove a \n\ninfer\u00eancia de reconhecimento de amostras novas, muitas vezes correta, fora do universo \n\nde amostras usadas na fase do seu treinamento. A aplica\u00e7\u00e3o discutida neste trabalho ser\u00e1 \n\no reconhecimento da qualidade do vapor de \u00e1lcool e do poder calor\u00edfico do g\u00e1s GLP.  \n\nQuando h\u00e1 muitos atributos a serem analisados, em geral \u00e9 usada a an\u00e1lise de \n\ncomponentes principais para reduzir a dimensionalidade do sistema, de modo a manter \n\no m\u00e1ximo de informa\u00e7\u00e3o presente nos dados [1, 3, 4]. As medidas obtidas pelos seis \n\nsensores foram analisadas em tr\u00eas abordagens: dados crus, dados submetidos \u00e0 an\u00e1lise \n\nde componentes principais, e dados que simulam um sensor com falh a. \n\nOs m\u00e9todos tradicionais de obten\u00e7\u00e3o do poder calor\u00edfico do g\u00e1s combust\u00edvel \n\npodem ser divididos em tr\u00eas categorias [9]: a combust\u00e3o de uma amostra gasosa em \n\numa bomba calorim\u00e9trica, combust\u00e3o do g\u00e1s em um queimador  de chama aberta, e a \n\ncombust\u00e3o catal\u00edtica sem chama. Em geral esses m\u00e9todos requerem um maquin\u00e1rio \n\ncaro. Um sistema embarcado de reconhecimento que utiliza sensores n\u00e3o seletivos e de \n\nbaixo custo pode ser uma alternativa com uma boa rela\u00e7\u00e3o custo-benef\u00edcio para \n\nclassificar o poder calor\u00edfico de um dado g\u00e1s combust\u00edvel. \n\n1.1 Objetivos \n\nO primeiro objetivo deste trabalho \u00e9 reconhecer o poder calor\u00edfico de uma \n\namostra de g\u00e1s combust\u00edvel. As amostras utilizadas ser\u00e3o de vapor de \u00e1lcool e de g\u00e1s \n\ncombust\u00edvel GLP, a pa rtir de padr\u00f5es diferentes. As redes neurais ser\u00e3o estudadas como \n\n\n\n2 \n\n \n\numa alternativa de solu\u00e7\u00e3o ao problema de reconhecimento. As resist\u00eancias dos \n\nsensores de gases Taguchi ser\u00e3o os atributos utilizados neste trabalho. \n\nO segundo objetivo \u00e9 implementar um sistema robusto capaz de reconhecer um \n\ndado padr\u00e3o mesmo com a perda de um dos sensores de g\u00e1s, isto \u00e9, quando algum dos \n\nsensores parar de responder a est\u00edmulos da entrada do sistema. Com isso, poder\u00e1 ser \n\nsimulada uma poss\u00edvel falha de um dos sensores de g\u00e1s. Para tentar contornar este \n\nproblema ser\u00e3o utilizadas as redes neurais em comit\u00ea [8,10], pelo m\u00e9todo de Ensemble \n\ne ser\u00e1 usado um sistema de sete redes neurais treinadas com subconjuntos dos dados \n\ncada um simulando nenhum ou um sensor com falha. \n\nSer\u00e1 discutida tamb\u00e9m a l\u00f3gica Fuzzy como substituto da rede neural para \n\nsolucionar o problema. A abordagem utilizada neste trabalho ser\u00e1 a implementa\u00e7\u00e3o da \n\nclusteriza\u00e7\u00e3o subtrativa nos dados de entrada do sistema. \n\nTamb\u00e9m ser\u00e3o discutidos o pr\u00e9-processamento e a an\u00e1lise de componentes \n\nprincipais dos dados de treinamento da rede, e qual o impacto desses m\u00e9todos na etapa \n\nde treinamento da rede neural e tamb\u00e9m na generaliza\u00e7\u00e3o da mesma. \n\nA implementa\u00e7\u00e3o em hardware da solu\u00e7\u00e3o obtida tamb\u00e9m ser\u00e1 discutida. O \n\nreconhecimento dos padr\u00f5es de \u00e1lcool combust\u00edvel com uma rede neural ser\u00e1 \n\nimplementado em uma placa DSP da Analog Devices. Ser\u00e1 analisado o uso de um \n\ndispositivo dedicado baseado em um microcontrolador. \n\n1.2 Justificativa \n\nAtualmente a an\u00e1lise dos compostos presentes em um determinado g\u00e1s \n\ncombust\u00edvel \u00e9 feita em geral com o aux\u00edlio dos cromat\u00f3grafos. Os resultados da an\u00e1lise \n\nde um cromat\u00f3grafo s\u00e3o as propor\u00e7\u00f5es dos diversos componentes do combut\u00edvel em \n\nan\u00e1lise. Estas propor\u00e7\u00f5es s\u00e3o convertidas em quantidades, que s\u00e3o aplicadas em \n\nf\u00f3rmulas matem\u00e1ticas para obter o poder calor\u00edfico do combust\u00edvel em quest\u00e3o. \n\nApesar do aparelho cromat\u00f3grafo ser muito preciso, ele \u00e9 muito caro. O \n\nproblema proposto neste trabalho \u00e9 de reconhecer o poder calor\u00edfico de um dado g\u00e1s \n\ncombust\u00edvel. Deste modo, um conjunto de sensores de concentra\u00e7\u00f5es de g\u00e1s de baixo \n\ncusto e pouco seletivo poder\u00e1 ser uma alternativa muito mais barata. Como o sistema \n\nreconhecedor n\u00e3o precisa ser muito preciso, a rela\u00e7\u00e3o custo \u2013 benef\u00edcio desta alternativa \n\npoder\u00e1 ser vi\u00e1vel. \n\n\n\n3 \n\n \n\nEste trabalho ter\u00e1 como \u00eanfase o uso das redes neurais artificiais e a l\u00f3gica fuzzy \n\npara a etapa de reconhecimento e condicionamento de sinais do sistema embarcado \n\nreconhecedor de padr\u00f5es de vapor de \u00e1lcool e de GLP. Essas novas tecnologias est\u00e3o \n\nganhando espa\u00e7o no mercado de consumo, pelos seus resultados satisfat\u00f3rios [22]. \n\n1.3 Estrutura do Texto \n\nEsta Disserta\u00e7\u00e3o est\u00e1 dividida em 6 cap\u00edtulos organizados com a estrutura \n\ndetalhada a seguir. \n\n \n\nCap\u00edtulo 1: Introdu\u00e7\u00e3o \n\n\u2022 Descri\u00e7\u00e3o do problema a ser resolvido, detalhamento dos objetivos, e justificativas \n\ndo trabalho proposto. \n\n \n\nCap\u00edtulo 2: Fundamentos Te\u00f3ricos \n\n\u2022 Estudo do funcionamento dos sensores de gases utilizados no experimento. \n\n\u2022 Pesquisa de sensores tradicionais e n\u00e3o tradicionais para o reconhecimento de \n\npadr\u00f5es de combust\u00edveis. \n\n \n\nCap\u00edtulo 3: As Redes Neurais Artificiais \n\n\u2022 Breve introdu\u00e7\u00e3o das Redes Neurais Perceptron de M\u00faltiplas Camadas e SOM, e \n\nsuas principais caracter\u00edsticas. \n\n\u2022 Introdu\u00e7\u00e3o ao pr\u00e9-processamento dos dados e \u00e0 an\u00e1lise de componentes principais. \n\n\u2022 Breve explica\u00e7\u00e3o das abordagens de sistemas de Redes Neurais Artificiais e \n\nsistemas de L\u00f3gica Difusa para resolver o problema de reconhecimento de padr\u00f5es. \n\n3.1 As M\u00e1quinas de Comit\u00ea \n\n\u2022 Introdu\u00e7\u00e3o breve das m\u00e1quinas de comit\u00ea est\u00e1ticas: M\u00e9dia de Ensemble e Refor\u00e7o \n\n(Boosting) \n\n\n\n4 \n\n \n\n \n\n3.2 A L\u00f3gica Fuzzy \n\n\u2022 Breve introdu\u00e7\u00e3o das defini\u00e7\u00f5es e principais caracter\u00edsticas da l\u00f3gica Fuzzy. Ser\u00e3o \n\ndescritos os funcionamentos de um controlador l\u00f3gico fuzzy e de um sistema de \n\ninfer\u00eancia fuzzy.  \n\n \n\nCap\u00edtulo 4: Metodologia Experimental \n\n\u2022 Descri\u00e7\u00e3o dos distintos procedimentos utilizados para coletar as amostras de \u00e1lcool \n\ne de g\u00e1s combust\u00edvel.  \n\n \n\nCap\u00edtulo 5: Pr\u00e9-Processamento de Dados  \n\n\u2022 Descri\u00e7\u00e3o do uso do aplicativo Data Sculptor para realizar o pr\u00e9-processamento dos \n\ndados obtidos do \u00e1lcool combust\u00edvel. \n\n\u2022 Implementa\u00e7\u00e3o do pr\u00e9-processamento de dados e an\u00e1lise de componentes principais \n\natrav\u00e9s do aplicativo Matlab para os dados obtidos do g\u00e1s GLP. \n\n \n\n5.1  Reconhecimento da qualidade de \u00e1lcool com Redes Neurais \n\n\u2022 Descri\u00e7\u00e3o da implementa\u00e7\u00e3o do sistema reconhecedor da qualidade do vapor de \n\n\u00e1lcool combust\u00edvel utilizando um Kit de DSP da Analog Devices. \n\n\u2022 An\u00e1lise dos resultados obtidos e limita\u00e7\u00f5es da implementa\u00e7\u00e3o do sistema no \n\nhardware. \n\n \n\n5.2  Implementa\u00e7\u00e3o do Sistema de Infer\u00eancia Fuzzy \n\n\u2022 Implementa\u00e7\u00e3o do FIS para resolver o problema de reconhecimento do poder \n\ncalor\u00edfico do g\u00e1s GLP. \n\n\u2022 An\u00e1lise dos resultados obtidos com o sistema FIS utilizando os dados com ou sem \n\npr\u00e9-processamento e an\u00e1lise de componentes principais. \n\n \n\n5.3  Reconhecimento do poder calor\u00edfico do GLP em Redes Neurais \n\n\u2022 Descri\u00e7\u00e3o das transforma\u00e7\u00f5es dos dados de entrada do sistema para que eles possam \n\nser usados nas Redes Neurais Artificiais. \n\n\n\n5 \n\n \n\n\u2022 Implementa\u00e7\u00e3o da primeira abordagem de RNA em que foram usados somente os \n\ndados experimentais originais de treinamento. Foi feita a an\u00e1lise das redes com e \n\nsem pr\u00e9-processamento e an\u00e1lise de componentes principais. \n\n\u2022 As tr\u00eas outras abordagens de RNA\u2019s utilizaram dados sint\u00e9ticos que simulavam o \n\ncomportamento de falha de um dos sensores. A primeira delas utilizou uma rede \n\nneural para realizar todo o reconhecimento. J\u00e1 as outras duas abordagens \n\nimplementaram dois tipos de m\u00e1quinas de comit\u00ea est\u00e1ticas. \n\n \n\nCap\u00edtulo 6  Conclus\u00f5es e Perspectivas Futuras  \n\n\u2022 An\u00e1lise global dos resultados obtidos experimentalmente e balan\u00e7o do que foi \n\ndesenvolvido neste trabalho. \n\n\u2022 Listagem de algumas id\u00e9ias a serem desenvolvidas para enriquecer ou melhorar as \n\natividades desenvolvidas. \n\n \n\nBibliografia:  \n\nLista de refer\u00eancias bibliogr\u00e1ficas utilizadas. \n\n \n\n \n\n\n\n6 \n\n \n\n2 Fundamentos Te\u00f3ricos \n\n2.1 Princ\u00edpio de Funcionamento do Sensor de G\u00e1s Taguchi \n\nO sensor de g\u00e1s \u00e9 feito de um \u00f3xido de metal (SnO2, por exemplo). Ao se \n\naquecer um \u00f3xido de metal, mol\u00e9culas de g\u00e1s oxig\u00eanio s\u00e3o adsorvidos na superf\u00edcie do \n\nmesmo com uma carga negativa. H\u00e1 ent\u00e3o a forma\u00e7\u00e3o de uma camada de deple\u00e7\u00e3o na \n\nsuperf\u00edcie do metal, fazendo com que sua resist\u00eancia aumente. Isso porque \u00e9 criada uma \n\nbarreira de potencial, impedindo o fluxo de el\u00e9trons pelo metal [12]. \n\nNa presen\u00e7a de um g\u00e1s (subst\u00e2ncia redutora), a densidade de mol\u00e9culas de g\u00e1s \n\noxig\u00eanio carregadas negativamente diminui, fazendo com que a barreira de potencial \n\nseja reduzida. E, conseq\u00fcentemente, a resist\u00eancia do sensor tamb\u00e9m diminui. \n\nA rela\u00e7\u00e3o entre a resist\u00eancia do sensor e a concentra\u00e7\u00e3o do g\u00e1s \u00e9 dada pela \n\nequa\u00e7\u00e3o (2.1a) sobre uma determinada faixa de concentra\u00e7\u00e3o de g\u00e1s: \n\nRs = A [C]\n??                                                                                                   (2.1a) \n\nOnde: \n\nRs = Resist\u00eancia el\u00e9trica do sensor. \n\nA = Constante. \n\n[C] = Concentra\u00e7\u00e3o do g\u00e1s. \n\n? = Inclina\u00e7\u00e3o da curva Rs. \n\n \n\nCaracter\u00edsticas dos sensores de g\u00e1s \n\nOs sensores de g\u00e1s Taguchi t\u00eam as seguintes caracter\u00edsticas [12]: \n\n\u2022 A sua resist\u00eancia diminui com concentra\u00e7\u00f5es reduzidas de oxig\u00eanio; \n\n\u2022 Sua sensitividade varia de acordo com o g\u00e1s usado; \n\n\u2022 Ao ser exposto a um g\u00e1s, sua resist\u00eancia varia rapidamente. Quando o g\u00e1s \u00e9 retirado, \n\na varia\u00e7\u00e3o para o estado inicial \u00e9 mais lenta. \n\nA taxa das rea\u00e7\u00f5es qu\u00edmicas feitas na superf\u00edcie do sensor depende da \n\ntemperatura. A umidade tamb\u00e9m afeta o funcionamento do sensor, por causa da poss\u00edvel \n\nadsor\u00e7\u00e3o de vapor de \u00e1gua na superf\u00edcie do \u00f3xido de metal. \n\n\n\n7 \n\n \n\nAs curvas caracter\u00edsticas relativas ao sensor TGS 822 demonstram essas \n\ncaracter\u00edsticas. O sensor tem sensibilidade diferente para cada subst\u00e2ncia que \u00e9 aplicada \n\nao sensor, como mostrado na Fig. 2.1a, que mostra Rs/Ro em rela\u00e7\u00e3o \u00e0 concentra\u00e7\u00e3o de \n\ng\u00e1s e Rs/Ro em rela\u00e7\u00e3o \u00e0 temperatura do ambiente. Rs \u00e9 a resist\u00eancia do sensor ap\u00f3s ele \n\nter sido submetido ao g\u00e1s e estar em regime permanente; e Ro \u00e9 a resist\u00eancia inicial do \n\nsensor em regime  permanente. \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.1a: Caracter\u00edsticas do Sensor Taguchi TGS822 \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.1b: Polariza\u00e7\u00f5es dos sensores Taguchi por divis\u00e3o de tens\u00e3o e a que evita a \n\nresist\u00eancia por contatos \n\nH\u00e1 duas formas de polarizar o sensor Taguchi [13], conforme mostrado na Fig. \n\n2.1b: da esquerda para a direita s\u00e3o apresentadas a polariza\u00e7\u00e3o por divis\u00e3o de tens\u00e3o, e a \n\npolariza\u00e7\u00e3o que evita resist\u00eancia por contatos. A polariza\u00e7\u00e3o por divis\u00e3o de tens\u00e3o \u00e9 a \n\nmais usual. Ela faz a liga\u00e7\u00e3o em s\u00e9rie do sensor com uma resist\u00eancia de carga. A tens\u00e3o \n\na ser medida ser\u00e1 obtida pela queda de tens\u00e3o no resistor de carga. A outra forma de \n\npolariza\u00e7\u00e3o do sensor faz uso da tens\u00e3o medida, obtida diretamente da queda de tens\u00e3o \n\nno sensor. Essa outra configura\u00e7\u00e3o de polariza\u00e7\u00e3o evita o s\u00e9rio problema de resist\u00eancia \n\nde contato nas medidas de tens\u00e3o do sensor de g\u00e1s. O fabricante fornece as seguintes \n\nespecifica\u00e7\u00f5es do sensor Taguchi: \n\n\n\n8 \n\n \n\n\u2022 Tens\u00e3o de polariza\u00e7\u00e3o do sensor (Vs): 24Vm\u00e1x. AC ou DC \n\n\u2022 Tens\u00e3o para o aquecimento do sensor (Va): 5,0 \u00b1 0,2V AC ou DC \n\n2.2 Sensores que utilizam tecnologias comuns e n\u00e3o-comuns \n\nO uso de sensores comuns \u00e9 a mais barata e intuitiva solu\u00e7\u00e3o do problema de \n\nreconhecimento de gases. Por\u00e9m, como os sensores em geral n\u00e3o s\u00e3o lineares com \n\nrela\u00e7\u00e3o \u00e0 concentra\u00e7\u00e3o de gases detectados, h\u00e1 a necessidade de um condicionamento \n\ndos sinais medidos. Ou seja, quando \u00e9 necess\u00e1rio maior precis\u00e3o, a lineariza\u00e7\u00e3o dos \n\ndados medidos \u00e9 essencial.  \n\nCom a incorpora\u00e7\u00e3o de circuitos de condicionamento de sinais, o projeto do \n\nsistema tradicional come\u00e7a a ficar cada vez mais complicado e caro. H\u00e1 v\u00e1rios fatores a \n\nserem considerados, como o ambiente de opera\u00e7\u00e3o do sistema, as especifica\u00e7\u00f5es \n\nt\u00e9cnicas do sensor utilizado, a precis\u00e3o requerida do sistema, etc. \n\nEm contraste, h\u00e1 sensores em estado da arte que utilizam tecnologias n\u00e3o \n\ntradicionais. S\u00e3o os sensores que reconhecem odores a partir da visualiza\u00e7\u00e3o de cores \n\nem uma matriz de amostras de metais espec\u00edficos [14]. A resposta qu\u00edmica seletiva de \n\num conjunto de vapores imobilizados em um grande conjunto de metais especiais \n\npermite a visualiza\u00e7\u00e3o de um gr ande conjunto de gases (\u00e1lcoois, aminas, \u00e9teres, dentre \n\noutros). Ou seja, \u00e9 um sensor de an\u00e1lises qualitativas [15], que reconhece a identidade \n\ndo g\u00e1s utilizado, e n\u00e3o reconhece quantidades individuais. A Fig. 2.2a abaixo mostra a \n\nresposta de uma matriz minimizada de 4 metais especiais (Sn(TPP)(Cl2), Co(TPP)(Cl), \n\nZn(TPP) e Fe(TFPP)(Cl), vistos no sentido hor\u00e1rio, de cima para baixo) quando eles \n\nforam submetidos aos gases n-octilamina, dodecanetiol, e tri-n-butilfosfina a 1.8 p.p.m. \n\n \n\n \n\n \n\nFig. 2.2a: Assinaturas de cores a concentra\u00e7\u00f5es baixas de gases. \n\nVapores de gases n\u00e3o afetam a performance do dispositivo, o qual mostra boa \n\nresposta linear a amostras. Assinaturas de cor \u00fanicas podem ser obtidas desde \n\nconcentra\u00e7\u00f5es de amostras abaixo de duas partes por milh\u00e3o at\u00e9 respostas de 100 partes \n\npor milh\u00e3o. \n\n\n\n9 \n\n \n\nA \u00fanica desvantagem destes sensores \u2013 e a que os impossibilita de serem \n\nutilizados em sistemas de controle tradicionais \u2013 \u00e9 que eles oferecem somente respostas \n\nvisuais. N\u00e3o \u00e9 poss\u00edvel captar sinais el\u00e9tricos para odores aplicados \u00e0 matriz de \n\nsensores. Uma forma de utilizar esses sensores seria monitor\u00e1-los atrav\u00e9s de c\u00e2meras de \n\nv\u00eddeo. O que  n\u00e3o compensaria, pois o sistema ficaria mais complexo, e seria necess\u00e1rio \n\num sistema de reconhecimento de imagens para automatizar o sistema. \n\n2.3 Sensores inteligentes \n\nOs sensores inteligentes t\u00eam v\u00e1rias funcionalidades adicionais [16] aos sensores \n\ncomuns, e s\u00e3o capazes de tomarem decis\u00f5es por si pr\u00f3prios. Os sistemas de sensores \n\ninteligentes devem realizar ao menos uma das seguintes atividades listadas a seguir: \n\n\u2022 Calibra\u00e7\u00e3o autom\u00e1tica: realizar sozinhos a sua calibra\u00e7\u00e3o; \n\n\u2022 Processamento do sinal: deve ser capaz de tratar o sinal de medi\u00e7\u00e3o de forma a \n\nfornecer seu valor final. \n\n\u2022 Tomada de decis\u00f5es: podem tomar decis\u00f5es para realizar a\u00e7\u00f5es que fazem com que \n\no sensor cumpra a sua tarefa, independentemente do comando principal do sistema. \n\n\u2022 Fus\u00e3o com outros sensores: podem ser combinados ou adicionados sinais de outros \n\nsensores a fim de melhorar a robustez do resultado.  \n\n\u2022 Capacidade de aprendizado: pode aprender com experi\u00eancias passadas, a fim de \n\naumentar sua performance e robustez.  \n\nUm sensor inteligente pode ser descrito como um sistema composto de uma \n\ncombina\u00e7\u00e3o de sensores convencionais, processamento de sinais, m\u00e9todos de extra\u00e7\u00e3o \n\nde caracter\u00edsticas, algoritmos de aprendizado ou m\u00e9todos de representa\u00e7\u00e3o de \n\nconhecimento integrados. A intelig\u00eancia do sensor inteligente pode ser implementada \n\ncom t\u00e9cnicas de redes neurais, l\u00f3gica difusa, algoritmos gen\u00e9ticos, sistemas \n\nespecialistas ou \u00e1rvores de decis\u00e3o.  \n\n \n\n2.3.1 Sensores que utilizam Redes Neurais Artificiais \n\nSistemas embarcados de reconhecimento de padr\u00f5es baseados em redes neurais \n\nartificiais em geral utilizam sensores comerciais comuns. Portanto, as condi\u00e7\u00f5es de \n\nopera\u00e7\u00e3o do sistema reconhecedor obedecem \u00e0s condi\u00e7\u00f5es de opera\u00e7\u00e3o do sensor \n\n\n\n10 \n\n \n\nutilizado. As redes neurais servem basicamente para substituir as etapas de modifica\u00e7\u00e3o \n\ne condicionamento de sinais. \n\nHouve v\u00e1rios estudos de aplica\u00e7\u00f5es de sensores que utilizam tecnologia de redes \n\nneurais para o reconhecimento do g\u00e1s [13, 17, 18, 19 e 20]. Em [21] \u00e9 enfatizado que \n\nmuitos modelos de redes neurais usua is, tais como o MLP, n\u00e3o podem ser usados para \n\nan\u00e1lises quantitativas de gases. Por\u00e9m eles podem ser usados para reconhecimento de \n\npadr\u00f5es que se aplica na \u00e1rea de qualifica\u00e7\u00e3o de an\u00e1lise qu\u00edmica. Ou seja, para \n\nreconhecer qual g\u00e1s foi submetido \u00e0 matriz de  sensores, com uma determinada \n\nquantidade de g\u00e1s aplicada. \n\nPortanto, o modelo da rede neural artificial MLP deve ser modificado para tratar \n\no problema real de quantifica\u00e7\u00e3o da concentra\u00e7\u00e3o de gases. V\u00e1rios estudos foram feitos \n\npara modificar a rede neural,  dentre elas [21] recomenda o uso do conceito do \n\n\u2018ChemNet\u2019, em que teorias qu\u00edmicas s\u00e3o incorporados na estrutura da rede neural, \n\nfazendo com que a rede guarde um conhecimento a priori quando um modelo \u00e9 \n\nconstru\u00eddo.  Tamb\u00e9m foi apresentado o conceito qu\u00edmico de erro relativo em an\u00e1lises \n\nquantitativas, e discutido a sua aplica\u00e7\u00e3o na an\u00e1lise da mistura gasosa real. \n\nPor\u00e9m, o problema objeto de estudo deste trabalho n\u00e3o envolve an\u00e1lise \n\nquantitativa. N\u00e3o ser\u00e3o criadas v\u00e1rias sa\u00eddas da rede neural para medir com exatid\u00e3o a \n\nconcentra\u00e7\u00e3o das amostras de gases que s\u00e3o submetidas \u00e0 rede neural. Para o caso do \n\nreconhecimento de vapor de \u00e1lcool combust\u00edvel, por exemplo, ser\u00e1 criada somente uma \n\nsa\u00edda da rede neural indicando n\u00edvel l\u00f3gico \u201c1\u201d caso a amostra de \u00e1lcool tenha \n\nconcentra\u00e7\u00f5es entre 92.6% e 94.3% (\u00e1lcool de boa qualidade), e n\u00edvel l\u00f3gico \u201c0\u201d caso \n\ncontr\u00e1rio (\u00e1lcool de m\u00e1 qualidade ou desperd\u00edcio). A rede neural artificial MLP, \n\nportanto, reconhecer\u00e1 dois padr\u00f5es: \u00e1lcool bom ou \u00e1lcool ruim.  \n\nO treinamento de uma rede neural MLP \u00e9 um processo rand\u00f4mico. \n\nPrimeiramente, escolhe -se aleatoriamente os pesos iniciais para as sinapses. As \n\namostras s\u00e3o apresentadas \u00e0 rede uma a uma em ordem aleat\u00f3ria, ou em lotes. Portanto, \n\na especifica\u00e7\u00e3o de um sistema que utiliza redes neurais \u00e9 dif\u00edcil de ser obtida. \u00c9 \n\nnecess\u00e1ria uma avalia\u00e7\u00e3o estat\u00edstica dos experimentos de redes neurais [22]. Deve -se \n\ntreinar v\u00e1rias vezes uma rede neural, separando aleatoriamente o conjunto de dados \n\ndispon\u00edveis em amostras de treino e de teste, e somente ap\u00f3s v\u00e1rios experimentos, pode -\n\nse estimar uma distribui\u00e7\u00e3o gaussiana de caracter\u00edsticas da rede neural. Mas isso \n\n\n\n11 \n\n \n\nocorrer\u00e1, somente se os valores rand\u00f4micos utilizados no treinamento da rede neural \n\nseguirem uma distribui\u00e7\u00e3o gaussiana, ou qualquer outra distribui\u00e7\u00e3o definida.  \n\nPor\u00e9m, uma vez treinados os par\u00e2metros do sensor inteligente por Redes Neurais \n\nArtificiais, o seu uso \u00e9 trivial. Isso porque a arquitetura da rede e os pesos sin\u00e1pticos j\u00e1 \n\nest\u00e3o determinados. Somente \u00e9 necess\u00e1rio inserir os dados medidos nas entradas da rede \n\nneural, e as sa\u00eddas s\u00e3o obtidas por meio de simples contas matem\u00e1ticas de adi\u00e7\u00e3o e \n\nmultiplica\u00e7\u00e3o com n\u00fameros reais. \n\n \n\n2.3.2 Sensores que utilizam L\u00f3gica Difusa \n\nUm exemplo de sistema que utiliza tecnologia de l\u00f3gica difusa \u00e9 apresentado em \n\n[23]. Trata -se de um reconhecedor de sinais de olfato baseado em l\u00f3gica fuzzy, que usa \n\nsensores de gases polim\u00e9ricos. Neste sistema, os sinais de olfato s\u00e3o amostrados com \n\numa freq\u00fc\u00eancia de amostragem de 2Hz, e o valor inicial de cada sinal \u00e9 subtra\u00eddo de \n\ncada amostra do sinal. Esse pr\u00e9-processamento simples elimina os efeitos de deriva de \n\ncorrente produzidos pela dopagem e temperatura dos sensores polim\u00e9ricos condutores. \n\nE os sinais s\u00e3o analisados pelo reconhecedor fuzzy.  \n\nO sistema est\u00e1 mostrado na Fig. 2.3.2a. \u00c9 composto por dois reconhecedores: o \n\nreconhecedor baseado na forma do sinal, e o reconhecedor baseado na faixa \n\ndin\u00e2mica. O reconhecedor baseado na forma \u00e9 formado por um pr\u00e9-processador \n\nling\u00fc\u00edstico, um reposit\u00f3rio de modelos ling\u00fc\u00edsticos e um comparador. O pr\u00e9-\n\nprocessador ling\u00fc\u00edstico converte a resposta da matriz de sensores em uma representa\u00e7\u00e3o \n\nling\u00fc\u00edstica do odorante a ser testado. A transforma\u00e7\u00e3o \u00e9 baseada na parti\u00e7\u00e3o ling\u00fc\u00edstica \n\ndo espa\u00e7o ocupado pelo sinal. O comparador compara essa representa\u00e7\u00e3o ling\u00fc\u00edstica \n\ncom a representa\u00e7\u00e3o contida no reposit\u00f3rio de modelos ling\u00fc\u00edsticos. Todos os modelos \n\ncontidos no reposit\u00f3rio de modelos ling\u00fc\u00edsticos s\u00e3o obtidos na fase de treinamento. \n\nDetalhes do reconhecedor baseado na forma (Shape -Based Recogniser) est\u00e3o mostrados \n\nna figura da direita. \n\nAs varia\u00e7\u00f5es em amplitude do sinal produzidas pelos odorantes podem ser \n\nexploradas para determinar a identidade do odorante utilizado. O reconhecedor baseado \n\nna faixa din\u00e2mica constr\u00f3i um modelo fuzzy para tentar detectar o odorante de acordo \n\ncom sua resposta em amplitude do sinal. \n\n\n\n12 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.3.2a: Reconhecimento de sinais de olfato com l\u00f3gica fuzzy. \n\nPela Fig. 2.3.2b pode-se notar as defini\u00e7\u00f5es de fun\u00e7\u00f5es de pertin\u00eancia para a \n\namplitude do sinal e tamb\u00e9m para o eixo do tempo. Mostrou-se que esse sistema obteve \n\numa taxa de acerto entre 85 e 93%. \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.3.2b: An\u00e1lises feitas do sinal de olfato pelo sistema fuzzy.  \n\nUma outra abordagem que utiliza um sistema FIS, e que foi usado neste trabalho \n\npara resolver o problema de classifica\u00e7\u00e3o do poder calor\u00edfico do g\u00e1s combust\u00edvel, foi \n\nfeita de modo que a base de conhecimento do FIS fosse extra\u00edda a partir dos pr\u00f3prios \n\ndados sint\u00e9ticos. \n\nA especifica\u00e7\u00e3o de um sistema que utiliza l\u00f3gica fuzzy depende de diversos \n\nfatores, tais como: fun\u00e7\u00f5es de pertin\u00eancia utilizadas, conjunto de regras fuzzy \n\nimplementadas, o m\u00e9todo de defuzzifica\u00e7\u00e3o do sistema, m\u00e9todo da intersec\u00e7\u00e3o dos \n\nconjuntos fuzzy, entre outros. S\u00e3o escolhidos os par\u00e2metros que melhor resolvem o \n\nproblema proposto. Para um sistema de controle, a especifica\u00e7\u00e3o de um sistema que \n\n\n\n13 \n\n \n\nutiliza l\u00f3gica fuzzy \u00e9 feita por caracter\u00edsticas externas do mesmo, como por exemplo: \n\ntempos de resposta, amortecimento do sinal, ativa\u00e7\u00e3o, entre outros. \n\nAp\u00f3s as defini\u00e7\u00f5es dos par\u00e2metros do sistema fuzzy utilizado, o uso do sistema \n\ntamb\u00e9m \u00e9 trivial. Por\u00e9m, dependendo da estrat\u00e9gia de defuzzifica\u00e7\u00e3o utilizada, pode -se \n\nexigir maior poder computacional, pois as contas matem\u00e1ticas empregadas poder\u00e3o n\u00e3o \n\nser triviais. Operacionalmente, somente ser\u00e1 necess\u00e1rio apresentar os dados de entrada; \n\ne os dados de sa\u00edda s\u00e3o produzidos pelo sistema.  \n\n2.4 Sensores inteligentes para reconhecer a qualidade do \u00e1lcool \n\nEm [13] foi proposto um sistema para reconhecimento de subst\u00e2ncias arom\u00e1ticas \n\naplicadas a uma matriz de sensores de g\u00e1s. O sistema foi baseado em um \n\nmicrocontrolador 80C552 e um conjunto de sensores de g\u00e1s de \u00f3xido de estanho. As \n\nrespostas dos sensores foram analisadas com o aux\u00edlio da transforma\u00e7\u00e3o r\u00e1pida de \n\nFourier (FFT), e foram efetuados ensaios com subst\u00e2ncias qu\u00edmicas em dois grupos \n\ndiferentes. O primeiro grupo de subst\u00e2ncias utilizadas foram \u00e1lcool de arroz, \u00e1lcool de \n\nerva doce, mistura de \u00e1lcool com erva doce e mistura de \u00e1lcool de arroz com anis. O \n\nsegundo grupo foi composto por aguardentes brasileiras e pisco chileno.  \n\nO aparato experimental utilizado foi composto de uma c\u00e2mara de ensaio, um \n\nconjunto de rot\u00e2meros para controlar o fluxo de gases Nitrog\u00eanio e Oxig\u00eanio e um \n\nsistema de aquisi\u00e7\u00e3o de dados, como mostra a Fig. 2.4a. Dentro da c\u00e2mara de ensaio foi \n\ncolocada uma placa de circuito impresso com o conjunto de sensores de g\u00e1s, umidade e \n\ntemperatura. A tens\u00e3o sobre os sensores foi medida com uma placa de convers\u00e3o A/D e \n\nos dados gravados em um computador pessoal. O reconhecimento foi realizado com \n\nredes neurais artificiais treinadas com diferentes taxas de aprendizado.  \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.4a: Sistema para reconhecimento de qualidade de \u00e1lcoois \n\n\n\n14 \n\n \n\nO sistema adotado neste trabalho para o reconhecimento da qualidade de \u00e1lcool \n\ncombust\u00edvel \u00e9 similar ao descrito anteriormente. A proposta foi reconhecer vapores de \n\n\u00e1lcoois com concentra\u00e7\u00f5es classificadas como bom (de concentra\u00e7\u00f5es entre 92,6% e \n\n94,3% de \u00e1lcool) ou ruim (concentra\u00e7\u00f5es fora desta faixa). O sistema \u00e9 mostrado na Fig. \n\n2.4b. \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 2.4b: Sistema para reconhecer a qualidade de \u00e1lcool combust\u00edvel. \n\nAmostras de vapor de g\u00e1s s\u00e3o injetados na c\u00e2mara por meio de uma seringa. A \n\nc\u00e2mara cont\u00e9m uma matriz de seis sensores de gases. Por meio de um sistema de \n\naquisi\u00e7\u00e3o de sinais, s\u00e3o medidas as resist\u00eancias dos sensores de gases antes e depois da \n\ninje\u00e7\u00e3o das amostras de gases. S\u00e3o ent\u00e3o obtidos valores iniciais e finais de resist\u00eancia \n\ndos sensores, e s\u00e3o obtidos tamb\u00e9m os atributos que ser\u00e3o usados como par\u00e2metros de \n\nentrada do sistema de reconhecimento. Os dados obtidos passaram por uma etapa de \n\npr\u00e9-processamento de dados, onde houve uma normaliza\u00e7\u00e3o dos dados para valores no \n\nintervalo entre zero e um.  \n\nO sistema de reconhecimento escolhido foi a rede neural, por sua simplicidade \n\nde implementa\u00e7\u00e3o, e robustez \u00e0 perda de sensores (propriedade de generaliza\u00e7\u00e3o da rede \n\nneural). Os resultados obtidos tiveram acertos entre 80 e 95% das amostras utilizadas \n\ncomo teste. \n\nTempo\n\nResist\u00eancia\n\nVapor \ndo \n\n\u00e1lcool\n\nA1 A2 A3 A4 SD (*)\n\n...................    Bom\n\n...................    Ruim\n\n...................    Ruim\n\n...................    Bom\n\n...................    ....\n\nPr\u00e9-Processamento de dados\n\nRede\nNeural\nArtificial\n\nReconhecimento\nDa Qualidade\n\nDo \u00c1lcool\n\n\u00c1lcool Bom\n\n\u00c1lcool Ruim\n\nTi\nInstante da inje\u00e7\u00e3o do vapor Tempo\n\nResist\u00eancia\n\nVapor \ndo \n\n\u00e1lcool\n\nVapor \ndo \n\n\u00e1lcool\n\nA1 A2 A3 A4 SD (*)\n\n...................    Bom\n\n...................    Ruim\n\n...................    Ruim\n\n...................    Bom\n\n...................    ....\n\nPr\u00e9-Processamento de dados\n\nRede\nNeural\nArtificial\n\nReconhecimento\nDa Qualidade\n\nDo \u00c1lcool\n\n\u00c1lcool Bom\n\n\u00c1lcool Ruim\n\nTi\nInstante da inje\u00e7\u00e3o do vapor\n\n\n\n15 \n\n \n\n3 As Redes Neurais Artificiais \n\nSegundo [8], uma rede neural \u00e9 um sistema processador paralelamente \n\ndistribu\u00eddo constitu\u00eddo de unidades de processamento simples, que t\u00eam a propens\u00e3o \n\nnatural para armazenar conhecimento experimental e torn\u00e1-lo dispon\u00edvel para o uso. O \n\nconhecimento \u00e9 adquirido pela rede atrav\u00e9s de um processo de aprendizado. O \n\narmazenamento do conhecimento \u00e9 feito nas conex\u00f5es entre neur\u00f4nios, conhecidos \n\ncomo pesos sin\u00e1pticos. \n\nEm [24] \u00e9 exposto que al\u00e9m do modelo do neur\u00f4nio artificial, os paradigmas da \n\nrede incluem tamb\u00e9m a topologia da rede neural, ou seja, o modo como os neur\u00f4nios \n\nest\u00e3o interligados entre si; e os processos de aprendizado, que obedecem aos algoritmos  \n\nde aprendizado  descritas por express\u00f5es matem\u00e1ticas, as equa\u00e7\u00f5es de aprendizado. \n\nAs equa\u00e7\u00f5es de aprendizado descrevem as regras de aprendizado, que por sua vez dita \n\ncomo \u00e9 feito o processo de auto-ajustar seus pesos sin\u00e1pticos. O tipo de aprendizado \n\nnormalmente utilizado \u00e9 o aprendizado supervisionado , em que a resposta ao est\u00edmulo \n\nde uma entrada \u00e9 comparada com seu respectivo valor desejado. Caso sejam diferentes, \n\na rede gera um sinal de erro, o qual \u00e9 usado para calcular os ajustes que dever\u00e3o ser \n\nfeitos nos pesos sin\u00e1pticos da rede. E esse processo necessita de um professor ou \n\nsupervisor. \n\nAntes de uma rede neural ser utilizada como reconhecedor de padr\u00f5es, a mesma \n\ndever\u00e1 ser treinada. E para isso, deve-se escolher uma regra de aprendizado. A regra \n\nnormalmente utilizada \u00e9 a chamada backpropagation. Trata-se de um aprendizado \n\nsupervisionado, em que o erro obtido na sa\u00edda da  rede neural \u00e9 utilizado para atualizar \n\nos pesos das entradas de todos os neur\u00f4nios da rede neural. \n\nOs dados de treinamento utilizados pela rede neural devem ser os mais \n\nrepresentativos do sistema a ser projetado. Se forem escolhidos dados ruins na etapa de \n\ntreinamento da rede, ela n\u00e3o realizar\u00e1 uma generaliza\u00e7\u00e3o adequada. Segundo [8], o \n\ntermo generaliza\u00e7\u00e3o se refere \u00e0 capacidade de uma rede neural conseguir acertar o \n\nreconhecimento de um dado padr\u00e3o, utilizando amostras que n\u00e3o foram apresentadas \u00e0 \n\nrede neural em sua fase de treinamento. \n\nDependendo da fun\u00e7\u00e3o de ativa\u00e7\u00e3o utilizada pela rede neural, os dados de \n\ntreinamento podem ser normalizados em um intervalo real entre 0 e 1 (para a fun\u00e7\u00e3o de \n\n\n\n16 \n\n \n\nativa\u00e7\u00e3o de sigm\u00f3ide, por exemplo) ou entre \u2013 1 e 1 (para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o de \n\ntangente hiperb\u00f3lico, por exemplo). As sa\u00eddas resultantes da rede neural dever\u00e3o ser \n\ntransformadas de modo a compensar a normaliza\u00e7\u00e3o imposta nos dados de entrada. \n\nAp\u00f3s o treinamento da rede neural, ela se assemelha a um circuito l\u00f3gico. Isso \n\nporque o uso da rede torna -se direto, necessitando somente apresentar as amostras nas \n\nentradas para que a rede neural produza as sa\u00eddas, de acordo com o seu treinamento. \n\n \n\nModelo de um Neur\u00f4nio  \n\nO neur\u00f4nio \u00e9 o processador b\u00e1sico de uma rede neural. Cada neur\u00f4nio tem uma \n\nsa\u00edda, o qual pode alimentar muitos outros neur\u00f4nios. Cada neur\u00f4nio recebe muitas \n\nentradas por essas conex\u00f5es, chamadas sinapses. As entradas s\u00e3o multiplicadas pelos \n\npesos das sinapses, e somadas entre si. A ativa\u00e7\u00e3o do neur\u00f4nio \u00e9 computada aplicando \n\numa fun\u00e7\u00e3o de limiar a essa soma. Um modelo abstrato \u00e9 mostrado na Fig. 3a. \n\nFig. 3a: Modelo de um neur\u00f4nio artificial \n\n \n\nFun\u00e7\u00f5es de Ativa\u00e7\u00e3o \n\nA fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 geralmente uma fun\u00e7\u00e3o n\u00e3o-linear. Uma fun\u00e7\u00e3o simples \n\nn\u00e3o linear e que \u00e9 apropriada para redes neurais discretas \u00e9 a fun\u00e7\u00e3o degrau. Uma \n\nvariante da fun\u00e7\u00e3o degrau est\u00e1 descrita abaixo: \n\n?\n?\n?\n\n<\n?\n\n=\n0 xse ,        1-\n\n0 xse ,          1\n)(xf                        (3a) \n\nonde x \u00e9 a soma dos produtos das ativa\u00e7\u00f5es dos neur\u00f4nios de entrada com seus \n\nrespectivos pesos sin\u00e1pticos. \n\n                                                                                                                                 (3b) \n\n?\nFun\u00e7\u00e3o de\n\nAdi\u00e7\u00e3o\n\n?\nFun\u00e7\u00e3o de\nLimiar\n\nWo\n.\n.\n.\n\nWi\n.\n.\n.\n.\n\nWn\n\nAo\n.\n.\n.\n\nAi\n.\n.\n.\n.\n\nAn\n\nAtiva\u00e7\u00e3o de sa\u00edda\n\n?\n=\n\n=\nn\n\ni\niAix\n\n0\n\n* ?\n\n\n\n17 \n\n \n\nonde n \u00e9 o n\u00famero de neur\u00f4nios de entrada mais o valor do bias de entrada, A \u00e9 \n\no vetor de neur\u00f4nios de entrada, e w \u00e9 o vetor dos pesos sin\u00e1pticos que conectam os \n\nneur\u00f4nios de entrada que est\u00e3o sendo examinados. \n\nOutra classe popular de fun\u00e7\u00e3o de ativa\u00e7\u00e3o mais apropriada para redes \n\nanal\u00f3gicas com valores de sa\u00edda positivos, \u00e9 a fun\u00e7\u00e3o sigm\u00f3ide. Um exemplo \u00e9 a fun\u00e7\u00e3o \n\nlog\u00edstica mostrada abaixo: \n\n                                                                                                                      (3c) \n\nUma outra op\u00e7\u00e3o, que considera valores de sa\u00edda positivos e negativos \u00e9 a \n\ntangente hiperb\u00f3lica: \n\n                                                                                                                      (3d) \n\nA caracter\u00edstica mais importante da fun\u00e7\u00e3o de ativa\u00e7\u00e3o \u00e9 a sua n\u00e3o-linearidade. \n\nCaso a fun\u00e7\u00e3o de ativa\u00e7\u00e3o seja linear, o poder computacional da rede ser\u00e1 equivalente a \n\numa rede de uma camada [25]. \n\n \n\nAprendizado  \n\nTodo o \u201cconhecimento\u201d de uma rede neural \u00e9 guardado nas \u201csinapses\u201d, os pesos \n\ndas conex\u00f5es entre os neur \u00f4nios. Uma vez que o conhecimento est\u00e1 presente nos \n\ncorrespondentes pesos sin\u00e1pticos da rede, a apresenta\u00e7\u00e3o de um padr\u00e3o na entrada de \n\numa rede ter\u00e1 maior probabilidade de produzir a sa\u00edda correta. A rede adquire esse \n\nconhecimento atrav\u00e9s do treinamento. Associa\u00e7\u00f5es de padr\u00f5es s\u00e3o apresentados \u00e0 rede \n\nem seq\u00fc\u00eancia e os pesos s\u00e3o ajustados para capturar esse conhecimento. O esquema de \n\najuste dos pesos \u00e9 conhecido como algoritmo de aprendizado. \n\nUm dos primeiros m\u00e9todos de aprendizado foi formulado por Donald Hebb. O \n\naprendizado hebbiano \u00e9 descrito pelo ajuste dos pesos das conex\u00f5es baseadas nos \n\nvalores de ativa\u00e7\u00e3o dos neur\u00f4nios conectados ao neur\u00f4nio: \n\n?wij = ? * ai * aj                                                                                                    (3a) \n\nonde ? \u00e9 a taxa de aprendizado, ai \u00e9 a ativa\u00e7\u00e3o do i-\u00e9simo neur\u00f4nio em uma \n\ncamada de neur\u00f4nios, aj \u00e9 a ativa\u00e7\u00e3o do j-\u00e9simo neur\u00f4nio da outra camada, e wij \u00e9 o \n\npeso da conex\u00e3o entre os dois neur\u00f4nios. Uma variante dessa regra de aprendizado \u00e9 a \n\nlei do sinal Hebbiano: \n\n?wij = -wij  +  f(ai)* f(aj)                                                                                       (3b) \n\nxe\nxf ?+\n\n=\n1\n\n1\n)(\n\n)tanh()( xxf =\n\n\n\n18 \n\n \n\nonde f \u00e9 uma fun\u00e7\u00e3o sigm\u00f3ide. \n\n \n\nA Rede Perceptron de M\u00faltiplas Camadas  \n\nAs primeiras redes neurais utilizavam o modelo do neur\u00f4nio perceptron. Como \n\nclassificadores de padr\u00f5es, esses neur\u00f4nios conseguiam somente resolver problemas de \n\nsepara\u00e7\u00f5es lineares. Surgiram, portanto, as redes perceptron multicamada. Segundo [8] \n\nessas redes neurais utilizavam um algoritmo de aprendizado muito popular \u2013 o \n\nalgoritmo backpropagation. Trata-se de um algoritmo de aprendizado supervisionado \n\npor corre\u00e7\u00e3o de erro, com efici\u00eancia computacional satisfat\u00f3ria. S\u00e3o feitos dois passos \n\npara este tipo de aprendizado: um passo para frente, em que as sa\u00eddas da rede neural s\u00e3o \n\ngeradas dadas amostras de entrada; e um passo para tr\u00e1s, onde os pesos sin\u00e1pticos de \n\ntodos os neur\u00f4nios da rede neural s\u00e3o atualizados de acordo com a regra de corre\u00e7\u00e3o de \n\nerro. A atualiza\u00e7\u00e3o surge da compara\u00e7\u00e3o das sa\u00eddas geradas com as respostas desejadas \n\nda rede dada a amostra de entrada apresentada \u00e0 rede no passo anterior. O uso da rede \n\nneural necessita somente do passo para frente. \n\nUm perceptron de m\u00faltiplas camadas tem tr\u00eas caracter\u00edsticas principais: \n\n\u2022 O modelo de cada neur\u00f4nio inclui uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o n\u00e3o linear; \n\n\u2022 A rede cont\u00e9m uma ou mais camadas de neur\u00f4nios ocultos, que n\u00e3o s\u00e3o parte da \n\ncamada de entrada ou sa\u00edda da rede neural; \n\n\u2022 A rede tem um alto grau de conectividade, determinado pelas sinapses da rede. \n\nEssas caracter\u00edsticas s\u00e3o respons\u00e1veis pela capacidade do perceptron de \n\nm\u00faltiplas camadas de aprender com a experi\u00eancia atrav\u00e9s de treinamento. Por\u00e9m, essas \n\nmesmas caracter\u00edsticas s\u00e3o respons\u00e1veis tamb\u00e9m pela defici\u00eancia do conhecimento do \n\nprojetista sobre o comportamento da rede. Isto porque o fato da fun\u00e7\u00e3o de ativa\u00e7\u00e3o do \n\nneur\u00f4nio ser n\u00e3o linear e a rede ter alta conectividade, faz com que a an\u00e1lise te\u00f3rica da \n\nrede neural seja dificultada. Como a rede tem camadas de neur\u00f4nios ocultas, acaba por \n\ndificultar a visualiza\u00e7\u00e3o do processo de aprendizado. Em [8], o algoritmo de \n\nretropropaga\u00e7\u00e3o \u00e9 mostrado matematicamente em detalhes. Neste trabalho iremos nos \n\nconcentrar nas propriedades de generaliza\u00e7\u00e3o da rede neural a ser usada.  \n\nA generaliza\u00e7\u00e3o \u00e9 a capacidade da rede neural produzir respostas adequadas para \n\nentradas fora do universo das amostras de treino da rede neural. A capacidade de \n\ngeneraliza\u00e7\u00e3o depende da etapa de treinamento da rede neural. A falta de treinamento \n\n\n\n19 \n\n \n\ncausa a m\u00e1 generaliza\u00e7\u00e3o da rede neural. Por\u00e9m o excesso de treinamento tamb\u00e9m \n\npoder\u00e1 prejudicar a generaliza\u00e7\u00e3o da rede neural. Isto porque as amostras utilizadas para \n\no treino da rede neural podem n\u00e3o ser representativos do problema em quest\u00e3o a ser \n\nresolvido. A rede neural pode \u201cmemorizar\u201d os dados de treino, detectando \n\ncaracter\u00edsticas intr\u00ednsecas nos mesmos (devido a ru\u00eddos, por exemplo) que os detalham \n\nbem, por\u00e9m n\u00e3o detalham corretamente o comportamento do sistema a ser modelado. \n\nDeste modo, com o treinamento em excesso da rede neur al, apesar de se obter erros \n\npequenos da rede com rela\u00e7\u00e3o \u00e0s amostras de treino, a mesma poder\u00e1 ter erros \n\nsignificativos no processamento das amostras de teste e valida\u00e7\u00e3o. \n\nPor isso \u00e9 importante a escolha cuidadosa das amostras que ser\u00e3o usadas para o \n\ntreinamento da rede neural. A escolha de amostras \u201cruins\u201d acarretar\u00e1 o aprendizado \n\nincorreto da rede neural, fazendo com que a generaliza\u00e7\u00e3o da mesma seja pobre. \n\n \n\nA rede LVQ (learning vector quantization): \n\nDe acordo com [8;26], Redes Neurais do tipo Mapa Auto-Organiz\u00e1veis (SOM - \n\nself organizing maps) s\u00e3o redes que podem aprender a detectar regularidades e \n\ncorrela\u00e7\u00f5es nas suas entradas e adaptar as suas futuras respostas para aquela  \n\ndeterminada entrada. Os neur\u00f4nios das redes competitivas aprendem a reconhecer \n\ngrupos de vetores de entradas similares. Mapas auto-organiz\u00e1veis aprendem a \n\nreconhecer grupos de vetores de entrada similares de um modo tal que os neur\u00f4nios \n\nfisicamente pr\u00f3ximos entre si respondem a vetores de entrada similares. \n\nQuantiza\u00e7\u00e3o Vetorial com Aprendizado (LVQ - learning vector quantization) \u00e9 \n\num m\u00e9todo para o treinamento de camadas competitivas de uma maneira supervisada. A \n\ncamada competitiva automaticamente agrupa os vetores de entrada. Por\u00e9m, as classes \n\nque essa camada competitiva encontra s\u00e3o dependentes somente das dist\u00e2ncias entre os \n\nvetores de entrada. N\u00e3o h\u00e1 nenhum mecanismo em um projeto de uma camada \n\nestritamente competitiva que diz se dois vetores de entrada s\u00e3o ou n\u00e3o da mesma classe \n\nou de diferentes classes. \n\nRedes LVQ, por outro lado, aprendem a classificar vetores de entrada em \n\nclasses-alvo escolhidos pelo usu\u00e1rio. A regra de aprendizado utilizado foi a Kohonen. \n\nEm [36], o LVQ b\u00e1sico do aplicativo Matlab tem alguns atalhos que s\u00e3o \n\nendere\u00e7ados pelas extens\u00f5es LVQ1 e LVQ2. O LVQ1 \u00e9 a primeira etapa de \n\n\n\n20 \n\n \n\naprendizado, que utiliza um fator de \"consci\u00eancia\" que encoraja todos os neur\u00f4nios a \n\ntomar uma parte ativa do aprendizado. O LVQ2 \u00e9 a segunda etapa de aprendizado, que \n\nse trata de  um mecanismo de sintonia fina que refina os limites da classe. \n\nOs neur\u00f4nios da camada escondida de uma rede LVQ competem entre si para \n\naprender. Somente o vencedor (e opcionalmente os seus vizinhos) aprende a partir de \n\num dado vetor de treinamento.  \n\nDe acordo com [35], no aplicativo NeuralWorks o n\u00famero de neur\u00f4nios da \n\ncamada escondida pode ser definido como uma porcentagem do conjunto de dados de \n\ntreino. Por\u00e9m a rede LVQ tem o paradigma do n\u00famero de neur\u00f4nios da camada \n\nKohonen ser um m\u00faltiplo do n\u00famero de neur\u00f4nios de sa\u00edda. \n\n \n\nPr\u00e9-processamento de dados e An\u00e1lise de Componentes Principais \n\nA parte mais complicada no desenvolvimento de uma rede neural \u00e9 o pr\u00e9-\n\nprocessamento dos dados. Quando os dados s\u00e3o processados corretamente, muitos \n\nproblemas podem ser resolvidos por somente uma rede neural. Por outro lado, um pr\u00e9-\n\nprocessamento indevido pode fazer com que um problema fique muitas vezes insol\u00favel. \n\nA An\u00e1lise de Componentes Principais faz com que o espa\u00e7o dos dados de \n\nentrada (dados de treino) seja transformado em um espa\u00e7o reduzido de caracter\u00edsticas \n\nefetivas [8], e ainda reter o m\u00e1ximo das caracter\u00edsticas \u201cintr\u00ednsecas\u201d dos dados de \n\nentrada. Ou seja, os dados de entrada ser\u00e3o submetidos a uma redu\u00e7\u00e3o de \n\ndimensionalidade. \n\nBuscam-se tr\u00eas efeitos com a an\u00e1lise de componentes principais [8, 25, 26]: \n\n\u2022 Ortogonalizar os componentes dos vetores de entrada, para que eles n\u00e3o estejam \n\ncorrelacionados entre si;  \n\n\u2022 Ordenar os componentes ortogonais resultantes \u2013 os componentes principais \u2013 para \n\nque os componentes com maior varia\u00e7\u00e3o sejam inseridos nas primeiras posi\u00e7\u00f5es da \n\nordena\u00e7\u00e3o; \n\n\u2022 Eliminar aqueles componentes que contribuem com valores menores que um \n\nm\u00ednimo de varia\u00e7\u00e3o no conjunto de dados. \n\n\n\n21 \n\n \n\n3.1 As M\u00e1quinas de Comit\u00ea \n\nPara resolver problemas complexos de reconhecimento de padr\u00f5es, podem ser \n\nutilizadas t\u00e9cnicas mais eficazes de reconhecimento, que levam em conta n\u00e3o somente \n\num sistema reconhecedor, mas um conjunto deles. Essas t\u00e9cnicas se baseiam, segundo \n\n[8], no princ\u00edpio de dividir e conquistar muito usado em engenharia. Este princ\u00edpio \n\ndescreve a resolu\u00e7\u00e3o de problemas complexos atrav\u00e9s da combina\u00e7\u00e3o de solu\u00e7\u00f5es de um \n\ndeterminado n\u00famero de tarefas computacionais simples. \n\nAs m\u00e1quinas de comit\u00ea podem ser definidas, segundo [8], como uma \n\ncombina\u00e7\u00e3o de sistemas especialistas, que no caso de aprendizado supervisionado \n\ndividem entre si a tarefa de aprendizagem computacional. Essas m\u00e1quinas podem ser \n\ndivididas em duas grandes categorias: estruturas est\u00e1ticas ou estruturas din\u00e2micas. \n\nNas estruturas est\u00e1ticas, as respostas dos v\u00e1rios especialistas s\u00e3o combinadas por \n\nmeio de um mecanismo que n\u00e3o depende dos sinais de entrada. Nesta categoria s\u00e3o \n\ninclu\u00eddos os m\u00e9todos de M\u00e9dia de Ensemble, onde as sa\u00eddas dos diferentes especialistas \n\ns\u00e3o combinadas linearmente para produzir a sa\u00edda global; e o m\u00e9todo de Boosting, em \n\nque um algoritmo fraco de aprendizagem \u00e9 convertido em outro com precis\u00e3o arbitr\u00e1ria \n\nalta. \n\nAs estruturas din\u00e2micas envolvem as entradas do sistema no mecanismo de \n\ncombina\u00e7\u00e3o dos v\u00e1rios sistemas especialistas. Em [8], s\u00e3o citadas duas estruturas: a \n\nMistura de Especialistas, em que as respostas individuais de cada especialista s\u00e3o \n\ncombinadas n\u00e3o-linearmente por meio de uma \u00fanica rede de passagem; e a Mistura \n\nHier\u00e1rquica de Especialistas, em que a combina\u00e7\u00e3o dos especialistas \u00e9 feita com v\u00e1rias \n\nredes de passagem arranjadas hierarquicamente.  \n\nO m\u00e9todo de Ensemble foi usado em [27] para aproximar uma fun\u00e7\u00e3o ruidosa \n\nsenoidal. Em [28] foram reconhecidos 14 tipos de \u00f3leos usando um nariz eletr\u00f4nico \n\ncomercial e duas abordagens de m\u00e1quinas de comit\u00ea: a primeira abordagem utilizava \n\num Perceptron Multi-Camadas e SIMCA (modelamento de software independente de \n\nanalogia de classe) para uma classifica\u00e7\u00e3o hier\u00e1rquica dos \u00f3leos. A segunda abordagem \n\nutilizou uma m\u00e1quina de aprendizado chamada PND (dicotomizadores paralelos n\u00e3o-\n\nlineares), baseada na decomposi\u00e7\u00e3o de um problema de classifica\u00e7\u00e3o de K classes em \n\n\n\n22 \n\n \n\num conjunto de reconhecedores (chamados dicotomizadores) de duas classes. Neste \n\ntrabalho foram implementadas somente as m\u00e1quinas de comit\u00ea est\u00e1ticas. \n\n \n\nM\u00e9dia de Ensemble  \n\nO m\u00e9todo da m\u00e9dia de ensemble combina um n\u00famero de especialistas (redes \n\nneurais) treinadas diferentemente (por exemplo, iniciadas com pesos diferentes), que \n\ncompartilham um conjunto de dados de entrada comum, e s\u00e3o combinadas linearmente \n\npara produzir uma sa\u00edda global. Segundo [8,10], a motiva\u00e7\u00e3o do seu uso tem v\u00e1rios \n\naspectos: \n\n\u2022 Para resolver um problema complexo de reconhecimento de padr\u00f5es utilizando \n\nsomente uma rede neural, esta teria de ter uma complexidade compat\u00edvel (muitos \n\nneur\u00f4nios em uma ou mais camadas escondida s) para resolver o problema, \n\nresultando em um n\u00famero grande de par\u00e2metros a serem ajustados. O tempo de \n\ntreinamento \u00e9 provavelmente maior do que o caso em que sistemas especialistas \n\nmais simples fossem treinados em paralelo.  \n\n\u2022 O risco de ajuste em excesso (problema de \u201coverfitting\u201d) aumenta quando o n\u00famero \n\nde par\u00e2metros a serem ajustados \u00e9 grande comparado com o tamanho do conjunto \n\ndas amostras de treino. \n\n\u2022 Os dados de treinamento podem n\u00e3o prover informa\u00e7\u00f5es suficientes para escolher \n\num melhor classificador. Muitos algoritmos de classifica\u00e7\u00e3o consideram um grande \n\nespa\u00e7o de hip\u00f3teses. Mesmo eliminando hip\u00f3teses err\u00f4neas, h\u00e1 ainda muitas \n\nhip\u00f3teses restantes. A m\u00e1quina ensemble formada a partir desta cole\u00e7\u00e3o de hip\u00f3teses \n\nrestantes ser\u00e1 provavelmente mais robusta. \n\n\u2022 Para encontrar os pesos corretos da rede neural a partir de um conjunto de \n\ntreinamento, os algoritmos de redes neurais usam inicializa\u00e7\u00e3o rand\u00f4mica e m\u00e9todos \n\nde procura locais. Inicializa\u00e7\u00f5es diferentes levam a diferentes conjuntos \u00f3timos de \n\npesos. O m\u00e9todo Ensemble pode ser visto como uma maneira de compensar esses \n\nalgoritmos de procura imperfeitos. \n\n \n\nM\u00e9todo de Boosting  \n\n\u00c9 um m\u00e9todo geral que tenta refor\u00e7ar o acerto de um dado algoritmo de \n\naprendizagem fraca [8,10]. Um algoritmo de aprendizagem fraca \u00e9 aquele que encontra \n\n\n\n23 \n\n \n\numa hip\u00f3tese de reconhecimento com uma taxa de erro um pouco menor que 1/2. Deste \n\nmodo, o algoritmo de aprendizagem fraca ter\u00e1 quase a mesma taxa de acertos do que \n\numa estimativa aleat\u00f3ria. \n\nAo contr\u00e1rio do m\u00e9todo de Ensemble em que os especialistas utilizam um \n\nmesmo conjunto de dados de entrada, no m\u00e9todo de boosting os especialistas s\u00e3o \n\ntreinados com conjuntos de dados com distribui\u00e7\u00f5es totalmente diferentes. O m\u00e9todo de \n\nboosting pode ser implementado de tr\u00eas modos diferentes [8]: \n\n\u2022 Refor\u00e7o por filtragem, usada com conjuntos grandes de treinamento. Os dados s\u00e3o \n\ndescartados ou mantidos durante a fase de treinamento. \n\n\u2022 Refor\u00e7o por subamostragem, usado com um conjunto de treinamento de tamanho \n\nfixo. \n\n\u2022 Refor\u00e7o por pondera\u00e7\u00e3o, similar ao anterior, por\u00e9m o algoritmo de aprendizagem \n\nfraca pode receber exemplos \u201cponderados\u201d. \n\n3.2 A L\u00f3gica Fuzzy \n\nA L\u00f3gica Fuzzy \u00e9 uma ferramenta que formaliza o pensamento humano e a \n\nlinguagem natural, capturando a natureza inexata do mundo real [13]. A l\u00f3gica Fuzzy \u00e9 \n\nusada principalmente em Controladores L\u00f3gicos Fuzzy (FLC \u2013  Fuzzy Logic Controller) \n\npara modelar processos que seriam complexos demais para serem equacionados com \n\nm\u00e9todos tradicionais. \n\nDe acordo com [29], um conjunto ou classe fuzzy \u00e9 caracterizado por uma \n\nfun\u00e7\u00e3o de pertin\u00eancia que associa cada ponto de entrada a um intervalo fechado de \n\nn\u00fameros reais [0;1]. Os conjuntos crisp (exatos ou precisos), associam cada ponto de \n\nentrada a dois poss\u00edveis valores num\u00e9ricos discretos: 0 ou 1. A opera\u00e7\u00e3o de uni\u00e3o entre \n\nconjuntos Fuzzy \u00e9 definida como o m\u00e1ximo das fun\u00e7\u00f5es de pertin\u00eancias dos conjuntos \n\nfuzzy de entrada, e a intersec\u00e7\u00e3o \u00e9 definida como o m\u00ednimo.  \n\nS\u00e3o estipuladas regras fuzzy para o FLC, al\u00e9m das implementa\u00e7\u00f5es das etapas de \n\nfuzzifica\u00e7\u00e3o das entradas e defuzifica\u00e7\u00e3o das sa\u00eddas. E esta caixa preta \u00e9 aplicada \n\ndiretamente ao sistema o qual se quer controlar. As regras s\u00e3o divididas em regras \n\nprecedentes, e regras conseq\u00fcentes. Essas regras s\u00e3o combina\u00e7\u00f5es de atributos de \n\nlinguagem natural (adjetivos tais como alto, baixos, m\u00e9dios, pequenos, grandes, r\u00e1pidos, \n\nlentos, etc.). Em [25,26] h\u00e1 mais detalhes da l\u00f3gica fuzzy aplicada em um FLC. \n\n\n\n24 \n\n \n\nAssim como os sensores que utilizam redes neurais artificiais, a l\u00f3gica fuzzy se \n\naplicar\u00e1 nos sensores inteligentes nas etapas de modifica\u00e7\u00e3o e condicionamento de \n\nsinais. \n\nSegundo [30], um FLC \u00e9 um conjunto de regras que relaciona os conceitos duais \n\nde implica\u00e7\u00e3o fuzzy e a regra composicional de infer\u00eancia. O FLC promove um \n\nalgoritmo que converte a estrat\u00e9gia de controle ling\u00fc\u00edstico baseado no conhecimento \n\nespecialista em uma estrat\u00e9gia de controle autom\u00e1tico. O FLC \u00e9 dividido em quatro \n\npartes principais: \n\n\u2022 Fuzzifica\u00e7\u00e3o: etapa inicial de um FLC em que s\u00e3o feitas as transforma\u00e7\u00f5es \n\nnecess\u00e1rias nos dados para o seu uso no FLC; \n\n\u2022 Base de Dados: \u00e9 dividido em dois componentes: uma base de dados contendo as \n\ndefini\u00e7\u00f5es para o uso de regras de controle ling\u00fc\u00edstico e manipula\u00e7\u00e3o de dados \n\nfuzzy; e uma base de regras que representa o conhecimento do especialista \n\ntransformado em regras can\u00f4nicas. \n\n\u2022 L\u00f3gica de tomadas de decis\u00f5es: simula a tomada de decis\u00f5es feita pelos seres \n\nhumanos, de acordo com os v\u00e1rios resultados poss\u00edveis e seus diferentes graus de \n\npertin\u00eancias. \n\n\u2022 Defuzzifica\u00e7\u00e3o: possibilita a atua\u00e7\u00e3o \u201ccrisp\u201d (precisas) feita pelo FLC, \n\ntransformando a resposta do FLC no dom\u00ednio fuzzy em uma sa\u00edda de resposta \n\n\u201ccrisp\u201d, trat\u00e1vel pelo mundo real. \n\nA estrat\u00e9gia de fuzzifica\u00e7\u00e3o para entradas de valores precisos \u00e9 representada \n\ncomo um fuzzy singleton, ou seja, um pulso com amplitude unit\u00e1ria em torno do valor \n\nda entrada. Para dados com ru\u00eddos rand\u00f4micos no operador fuzzy (um tri\u00e2ngulo \n\nis\u00f3sceles, por exemplo) os dados probabil\u00edsticos podem ser convertidos em um n\u00famero \n\nfuzzy. Nos casos em que h\u00e1 dados precisos e imprecisos simultaneamente, \u00e9 usado o \n\nconceito de n\u00fameros h\u00edbridos. \n\nA base de conhecimento de um FLC \u00e9 dividida em duas partes: a base de dados  \n\ne a base de regras  do controlador fuzzy.  \n\nA base de dados \u00e9 baseada na opini\u00e3o subjetiva do especialista. Para a sua \n\nconstru\u00e7\u00e3o, devem ser considerados os aspectos de discretiza\u00e7\u00e3o/normaliza\u00e7\u00e3o dos \n\nuniversos de discurso, em que o mapeamento linear ou n\u00e3o linear dos dados de entrada \u00e9 \n\n\n\n25 \n\n \n\nfeito de acordo com um conhecimento a priori do especialista dentro do intervalo real \n\nfechado entre 0 e 1; al\u00e9m do aspecto de parti\u00e7\u00e3o fuzzy dos espa\u00e7os de entrada  e sa\u00edda, \n\nem que as vari\u00e1veis fuzzy s\u00e3o divididas em parti\u00e7\u00f5es, que representam os seus poss\u00edveis \n\nvalores com significados caracter\u00edsticos relacionados ao sistema, tais como: muito \n\npouco, pouco, muito, exagerado, etc. \n\nA base de regras \u00e9 composta por regras can\u00f4nicas que relacionam os atributos da \n\nentrada e os seus respectivos valores fuzzy em rela\u00e7\u00f5es do tipo: SE&lt;condi\u00e7\u00e3o \n\nsatisfeita> ENT\u00c3O&lt;conjunto de conseq\u00fc\u00eancias>. \n\nO algoritmo fuzzy dever\u00e1 prever uma a\u00e7\u00e3o resultante para todos os estados \n\nposs\u00edveis do sistema (completeza). E para assegurar esta propriedade, s\u00e3o definidas duas \n\nestrat\u00e9gias: A estrat\u00e9gia da base de dados, em que a uni\u00e3o dos suportes nos quais os \n\nconjuntos fuzzy est\u00e3o definidos dever\u00e3o cobrir o universo de discurso em rela\u00e7\u00e3o a \n\nalgum n\u00edvel setado ? . Escolhendo-se um n\u00edvel no ponto de crossover, sempre existir\u00e1 \n\numa regra dominante. O caso extremo ser\u00e1 a escolha de duas regras dominantes com \n\nn\u00edveis iguais a 0,5. A estrat\u00e9gia da base de regras  \u00e9 baseada na experi\u00eancia do \n\nespecialista do sistema. Novas regras s\u00e3o inclu\u00eddas no sistema quando uma condi\u00e7\u00e3o \n\nfuzzy n\u00e3o est\u00e1 inclu\u00edda na base de regras, ou quando as condi\u00e7\u00f5es pr\u00e9-definidas fuzzy \n\nt\u00eam cren\u00e7as menores que um determinado valor, por exemplo, 0,5. \n\nH\u00e1 duas formas de definir as fun\u00e7\u00f5es de pertin\u00eancia de um conjunto fuzzy: A \n\ndefini\u00e7\u00e3o num\u00e9rica, em que o grau de pertin\u00eancia \u00e9 representado como um vetor de \n\ndimens\u00e3o dependente do grau de discretiza\u00e7\u00e3o, \u00e9 usada quando o universo de discurso \u00e9 \n\ndiscreto. Na defini\u00e7\u00e3o funcional, as fun\u00e7\u00f5es de pertin\u00eancia s\u00e3o definidas como fun\u00e7\u00f5es, \n\ne \u00e9 usado quando o universo de discurso \u00e9 cont\u00ednuo.  \n\nPara a defini\u00e7\u00e3o da base de regras, deve-se escolher as vari\u00e1veis de estado do \n\nprocesso, e as vari\u00e1veis de controle das regras do controlador fuzzy. As regras podem \n\nser vindas do conhecimento de engenharia de controle e do especialista, baseados em \n\na\u00e7\u00f5es de controle dos operadores, no modelo fuzzy de um processo, ou baseados no \n\naprendizado. \n\nSegundo [31], uma regra de controle fuzzy \u00e9 uma rela\u00e7\u00e3o fuzzy expressa como \n\numa implica\u00e7\u00e3o fuzzy. Os crit\u00e9rios b\u00e1sicos de uma fun\u00e7\u00e3o de implica\u00e7\u00e3o s\u00e3o: \n\npropriedade fundamental, suavidade, interfer\u00eancia irrestrita, simetria de modus \n\nponens/tollens generalizado (GMP/GMT), e medida de propaga\u00e7\u00e3o de vaguidade. \n\n\n\n26 \n\n \n\nO mecanismo de infer\u00eancia empregado em um FLC \u00e9 bem mais simples que o \n\nusado em sistemas especialistas, j\u00e1 que o conseq\u00fcente de uma regra n\u00e3o se torna o \n\nantecedente de outra. Pode -se mostrar que o operador sup-m\u00ednimo \u00e9 comutativo. Isso \n\nfaz com que a a\u00e7\u00e3o inferida pelo sistema completo \u00e9 equivalente ao resultado agregado \n\nde cada regra individualmente. \n\nH\u00e1 quatro tipos de racioc\u00ednio fuzzy empregados em aplica\u00e7\u00f5es FLC: \n\n\u2022 Opera\u00e7\u00e3o de M\u00ednimo (Mandani), \u00e9 o m\u00e9todo mais simples, pois utiliza o operador \n\nm\u00ednimo para o c\u00e1lculo do valor resultante de cada regra. Necessita do uso da etapa \n\nde defuzzifica\u00e7\u00e3o. Produto de Larsen: em que o resultado da regra \u00e9 o produto de \n\ndois fatores: o valor escalar m\u00e1ximo da intersec\u00e7\u00e3o entre o valor medido e os \n\npredicados ling\u00fc\u00edsticos da regra antecedente; e o predicado ling\u00fc\u00edstico da regra \n\nconseq\u00fcente.Uso de fun\u00e7\u00f5es de pertin\u00eancia monot\u00f4nicas (Tsukamoto): simplifica \n\nos c\u00e1lculos, pois as fun\u00e7\u00f5es de pertin\u00eancia s\u00e3o monot\u00f4nicas. Com isso, n\u00e3o \u00e9 \n\nnecess\u00e1ria a etapa de defuzzifica\u00e7\u00e3o, e a atua\u00e7\u00e3o ser\u00e1 calculada como a m\u00e9dia \n\nponderada das regras envolvidas.A conseq\u00fc\u00eancia de uma regra \u00e9 fun\u00e7\u00e3o das \n\nvari\u00e1veis ling\u00fc\u00edsticas de entrada (Sugeno): as regras s\u00e3o expressas na forma: SE \n\n<x \u00e9 Ai, ..., y \u00e9 Bi> ENT\u00c3O&lt;z = fi(x ... y)>. Assim, o valor crisp de atua\u00e7\u00e3o \u00e9 \n\nobtido com uma m\u00e9dia ponderada das entradas relacionadas com seus graus de \n\npertin\u00eancia ?. \n\nA defuzzifica\u00e7\u00e3o \u00e9 a etapa em que as a\u00e7\u00f5es de controle fuzzy s\u00e3o transformadas \n\nem um universo de discurso \u201ccrisp\u201d para as atua\u00e7\u00f5es de controle. As estr at\u00e9gias \n\nposs\u00edveis s\u00e3o o crit\u00e9rio de m\u00e1ximo, m\u00e9dia do m\u00e1ximo ou centro de \u00e1rea. \n\nUm dos problemas do FLC \u00e9 de que o processo de transfer\u00eancia do \n\nconhecimento do especialista para o FLC demanda tempo e n\u00e3o \u00e9 trivial. Ser\u00e3o \n\nnecess\u00e1rios procedimentos com fundamentos de projeto de sistemas de controle \n\ncl\u00e1ssico.  \n\n \n\n\n\n27 \n\n \n\n4 Metodologia Experimental \n\nO texto deste trabalho est\u00e1 organizado segundo mostra a Tabela 4a abaixo. \n\nForam estudados dois problemas de reconhecimento de padr\u00f5es: amostras de vapor de \n\n\u00e1lcool combust\u00edvel e amostras de g\u00e1s GLP. Para resolver esses problemas foram \n\naplicadas tr\u00eas abordagens: Redes Neurais Artificiais, L\u00f3gica Fuzzy e as M\u00e1quinas de \n\nComit\u00ea Est\u00e1ticas. \n\nTabela 4a: Detalhamento dos experimentos realizados \n\n Redes Neurais L\u00f3gica Fuzzy M\u00e1quinas de Comit\u00ea \n\nCom Pr\u00e9-Processamento \ne An\u00e1lise de \nComponentes Principais \n\n\u2022 \u00c1lcool e GLP \n\n\u2022 Dados Sint\u00e9ticos e \nExperimentais \n\n\u2022 GLP \n\n\u2022 Dados Sint\u00e9ticos \n\n\u2022 GLP \n\n\u2022 Dados Sint\u00e9ticos e \nExperimentais \n\nSem Pr\u00e9-Processamento e \nAn\u00e1lise de Componentes \nPrincipais \n\n\u2022 \u00c1lcool e GLP \n\n\u2022 Dados Sint\u00e9ticos e \nExperimentais \n\n\u2022 Implementa\u00e7\u00e3o em \nHardware (DSP) \n\n\u2022 GLP \n\n\u2022 Dados Sint\u00e9ticos \n\n\u2022 GLP \n\n\u2022 Dados Sint\u00e9ticos e \nExperimentais \n\n \n\nForam observadas as influ\u00eancias das ferramentas de pr\u00e9-processamento dos \n\ndados e an\u00e1lise de componentes principais na velocidade de treinamento das redes \n\nneurais e na capacidade de generaliza\u00e7\u00e3o das mesmas. \n\nNeste item s\u00e3o apresentadas duas metodologias adotadas para a extra\u00e7\u00e3o dos \n\ndados do vapor de \u00e1lcool combust\u00edvel, e do g\u00e1s GLP. Foram feitas v\u00e1rias medidas das \n\nresist\u00eancias dos sensores antes e depois as inje\u00e7\u00f5es das amostras. \n\nAs medidas dos vapores de \u00e1lcool foram feitas pontualmente, isto \u00e9, somente os \n\nvalores iniciais e finais das tens\u00f5es dos sensores, com os sinais em regime permanente \n\n(varia\u00e7\u00e3o da resist\u00eancia menor do que 5%) foram extra\u00eddos. As medidas do GLP foram \n\nfeitas no dom\u00ednio do tempo. Mas neste trabalho, os valores das medidas usadas do GLP \n\nforam os valores iniciais e finais das tens\u00f5es dos sensores em regime permanente. \n\nOs sensores utilizados nas c\u00e2meras de sensores nos experimentos descritos nos \n\nitens seguintes foram os de \u00f3xido de estanho do fabricante Taguchi. Os sensores e os \n\ncompostos que eles detectam est\u00e3o listados na Tabela 4a [12]: \n\n\n\n28 \n\n \n\nTabela 4a: Compostos Detectados para cada sensor  \n\nSensores Taguchi Compostos Detectados \n\nTGS-2442 Mon\u00f3xido de Carbono \n\nTGS-2600 Etanol, Metanol, Butano e Propano \n\nTGS-2602 Etanol, Tolueno e Am\u00f4nia  \n\nTGS-2610 Propano e Butano \n\nTGS-2611 G\u00e1s Natural / Metano  \n\nTGS-2622 Etanol, Tolueno, Xileno e Solventes Org\u00e2nicos \n\n \n\nEstes seis sensores foram escolhidos de modo que eles n\u00e3o ficassem \n\n\u201csintonizados\u201d para os compostos detectados. Para tanto, eles dever\u00e3o responder a um \n\ngrande n\u00famero de compostos que comp\u00f5e a subst\u00e2ncia a ser medida [32], e com isso, a \n\nseletividade dos compostos ser\u00e1 feita pela etapa de reconhecime nto de padr\u00f5es. Ou seja, \n\nos crit\u00e9rios da escolha do n\u00famero de sensores e do tipo de cada um foram baseados na \n\ndiversidade dos componentes a serem analisados e na diversidade nas respostas dos \n\nsensores. \n\nA obten\u00e7\u00e3o dos dados experimentais est\u00e1 descrita nos cap\u00edtulos seguintes. A \n\nextra\u00e7\u00e3o dos atributos obtidos dos dados foi feita de acordo com a Varia\u00e7\u00e3o Fracional \n\nda Resist\u00eancia (adaptada a partir da Varia\u00e7\u00e3o Fracional da Condut\u00e2ncia em [32]). A \n\nVaria\u00e7\u00e3o Fracional da Resist\u00eancia foi definida pela equa\u00e7\u00e3o (4a). \n\n \n\nRNORM = (RINICIAL \u2013 RFINAL) / RINICIAL                                                                       (4a) \n\n \n\nA raz\u00e3o da escolha da equa\u00e7\u00e3o da Varia\u00e7\u00e3o Fracional da Condut\u00e2ncia como \n\nequa\u00e7\u00e3o base na extra\u00e7\u00e3o dos atributos dos dados experimentais foi a de que o tempo de \n\naprendizado da rede neural \u00e9 menor com rela\u00e7\u00e3o a outras equa\u00e7\u00f5es de extra\u00e7\u00e3o de \n\natributos, tais como a Condut\u00e2ncia Relativa, Logaritmo da Condut\u00e2ncia Relativa, etc. E \n\nal\u00e9m disso, o desempenho (e portanto, a capacidade de generaliza\u00e7\u00e3o) do sistema \n\nreconhecedor \u00e9 aproximadamente o mesmo se tomados os algoritmos de extra\u00e7\u00e3o de \n\natributos relacionados em [ 32]. \n\n\n\n29 \n\n \n\n \n\n4.1 Procedimento para coleta da amostra de \u00e1lcool combust\u00edvel \n\nAs amostras de vapor de \u00e1lcool combust\u00edvel foram extra\u00eddas com o aux\u00edlio da \n\nc\u00e2mara ilustrada na Fig. 4.1a: \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig 4.1a: C\u00e2mara de medidas. \n\n \n\nO Ventilador serviu para efetuar a purga ou limpeza da c\u00e2mara com os sensores. \n\nA c\u00e2mara com o composto S\u00edlica-Gel foi usada para absorver a umidade e os gases da \n\nc\u00e2mara com os sensor es. As v\u00e1lvulas 1 e 2 controlavam o fluxo de g\u00e1s no interior do \n\nsistema. Na figura, as v\u00e1lvulas est\u00e3o fechadas, pois suas \u201calavancas\u201d est\u00e3o posicionadas \n\nna vertical. A abertura das v\u00e1lvulas \u00e9 feita girando as \u201calavancas\u201d at\u00e9 elas ficarem na \n\nhorizontal. As dire\u00e7\u00f5es de abertura/fechamento est\u00e3o descritas no corpo das v\u00e1lvulas. O \n\nc\u00edrculo central escuro no centro da C\u00e2mara com os Sensores representa um material de \n\nborracha, que serviu como uma v\u00e1lvula para que a amostra de vapor de \u00e1lcool pudesse \n\nser injetada na c\u00e2mara por meio de uma seringa. \n\nOs sensores foram alimentados com 12Vcc. A tens\u00e3o dos aquecedores dos \n\nsensores foi ajustada com 5Vcc. Os fios de alimenta\u00e7\u00e3o da placa com o sensor de \n\numidade foram alimentados com 5Vcc. O conector DB9 foi ligado a uma placa de \n\naquisi\u00e7\u00e3o de sinais, que por sua vez foi ligado ao equipamento PXI. \n\nO sistema de coleta do vapor de \u00e1lcool foi feito por meio de uma seringa. O \n\nprocedimento operacional empregado para efetuar as medidas foi: \n\n+5 Vcc (placa do sensor de \n\n C\u00e2mara com S\u00edlica Gel \n\nC\u00e2mara com os Sensores \n\nV\u00e1lvula 1 V\u00e1lvula 2 \n\nVentilador \nVcc   = 5V \n\nConector \nDB9  \n\n\n\n30 \n\n \n\n1)  Ligar as fontes de alimenta\u00e7\u00e3o dos sensores e dos aquecedores dos mesmos, al\u00e9m da \n\nfonte dos sensores de umidade e temperatura. \n\n2)  Verificar a correta interliga\u00e7\u00e3o do sistema com o equipamento PXI e o conversor \n\nanal\u00f3gico/digital. \n\n3)  Executar o programa cliente de extra\u00e7\u00e3o de medidas instalado no equipamento PXI. \n\n4)  Verificar a C\u00e2mara com S\u00edlica Gel, e observar se o mesmo est\u00e1 em condi\u00e7\u00f5es de \n\nuso. Se a S\u00edlica Gel estiver vermelha, ela dever\u00e1 ser aquecida com o aux\u00edlio de um \n\n\u201chot-plate\u201d para eliminar a umidade do mesmo. \n\n5)  Abrir as v\u00e1lvulas 1 e 2, e ligar o ventilador para efetuar a purga ou limpeza da \n\nc\u00e2mara com os sensores. Observar a resposta dos sensores no PXI, e esperar a \n\nestabiliza\u00e7\u00e3o de todos os sensores. \n\n6)  Desligar o ventilador e fechar as v\u00e1lvulas 1 e 2. Espere novamente a estabiliza\u00e7\u00e3o \n\ndos sensores. \n\n7)  Inicie  uma medida com o equipamento PXI clicando em \u201cGravar\u201d (grava todas os \n\npontos da amostragem de todos os sensores em um per\u00edodo de amostragem definida \n\nno programa em um arquivo texto) ou \u201cSnapshot Inicial\u201d (grava somente os pontos \n\niniciais e finais da medida). Neste experimento quase todas as medidas foram \n\nrealizadas com o uso do \u201cSnapshot\u201d. \n\n8)  Preparar a seringa sugando o vapor de uma amostra de \u00e1lcool combust\u00edvel. Espetar a \n\nseringa na C\u00e2mara com os Sensores, e injetar a amostra lentamente. \n\n9)  Ao estabilizar as medidas dos sensores, finalizar a grava\u00e7\u00e3o do arquivo texto \n\niniciado no passo 7 no equipamento PXI. \n\n10)  Para realizar uma nova medida, repetir os passos 5 a 9. \n\n11)  Para finalizar as medidas, fa\u00e7a a limpeza da c\u00e2mara segundo o passo 5, feche as \n\nv\u00e1lvulas 1 e 2, e desligue a alimenta\u00e7\u00e3o do ventilador. N\u00e3o desligue a alimenta\u00e7\u00e3o \n\ndos sensores. \n\n \n\nOs dados obtidos por este m\u00e9todo foram usados no cap\u00edtulo 5.1 deste trabalho. \n\n\n\n31 \n\n \n\n \n\n4.2 Primeiro m\u00e9todo de extra\u00e7\u00e3o de caracter\u00edsticas do g\u00e1s GLP \n\nA primeira abordagem para a extra\u00e7\u00e3o de caracter\u00edsticas do poder calor\u00edfico do \n\ng\u00e1s GLP foi baseado em [7]. O sistema consiste de uma c\u00e2mara com seis sensores \n\nTaguchi, que detectam concentra\u00e7\u00f5es de diferentes tipos de gases (mon\u00f3xido de \n\ncarbono, etanol, metano, butano, propano, am\u00f4nia, etc.). O combust\u00edvel gasoso foi \n\ninjetado nesta c\u00e2mara junto com o g\u00e1s nitrog\u00eanio. As propor\u00e7\u00f5es das concentra\u00e7\u00f5es de \n\ncada componente desta mistura foram reguladas atrav\u00e9s de rot\u00e2metros. \n\nA primeira metodologia de extra\u00e7\u00e3o de amostras dos gases foi a seguinte: Fixada \n\numa vaz\u00e3o do g\u00e1s combust\u00edvel GLP (5mm do rot\u00e2metro, que corresponde a uma vaz\u00e3o \n\nde 3ml/min), fixou-se a vaz\u00e3o do g\u00e1s nitrog\u00eanio ajustada atrav\u00e9s de outros dois \n\nrot\u00e2metros. Essa mistura dos gases foi aplicada na c\u00e2mara dos sensores atrav\u00e9s do ajuste \n\ndo Rot\u00e2metro R7 em 8mm. A varia\u00e7\u00e3o dos rot\u00e2metros que controlam a vaz\u00e3o do g\u00e1s \n\nnitrog\u00eanio simulou a presen\u00e7a dos dois gases combust\u00edveis de poderes calor\u00edficos \n\ndiferentes, atrav\u00e9s da maior ou menor concentra\u00e7\u00e3o de GLP na mistura. \n\nTr\u00eas amostras do padr\u00e3o de GLP puro com alto poder calor\u00edfico foram obtidas \n\ncom os valores em regime permanente das resist\u00eancias dos sensores antes e ap\u00f3s cada \n\ninje\u00e7\u00e3o de combust\u00edvel. E com esses dados, foi montado um banco de dados com seis \n\natributos, correspondentes aos valores dos seis sensores. Esse banco de dados foi \n\nsubdividido em um conjunto de dados de treino e outro conjunto de dados de teste. \n\nPara aumentar o banco de dados de amostras dos diversos padr\u00f5es de gases GLP \n\nforam gerados dados sint\u00e9ticos baseados em alguns dados medidos dispon\u00edveis de um \n\npadr\u00e3o de g\u00e1s GLP. Foram criados 100 pontos para cada padr\u00e3o de combust\u00edvel. Como \n\nforam simulados tr\u00eas padr\u00f5es, o total de pontos foi de 300. Os comportamentos t\u00edpicos \n\ndos valores de resist\u00eancias dos sensores est\u00e3o mostrados na Fig. 5.4a abaixo. Os \n\ngr\u00e1ficos dos dados sint\u00e9ticos dos seis sensores para um dado valor de umidade e \n\ntemperatura est\u00e3o mostrados na Fig.  5.4b. Os valores do desvio padr\u00e3o e m\u00e9dia de \n\nvaria\u00e7\u00e3o da resist\u00eancia normalizada para a primeira classe foram obtidos a partir das \n\nmedidas existentes. Como uma primeira abordagem de compara\u00e7\u00e3o entre as solu\u00e7\u00f5es de \n\netapa de reconhecimento por RNA ou FIS, os diferentes padr\u00f5es foram obtidos com os \n\nmesmos desvios padr\u00f5es da primeira classe, por\u00e9m a m\u00e9dia do segundo padr\u00e3o \n\ncorresponde a uma raz\u00e3o de 1/3 do primeiro padr\u00e3o, e a m\u00e9dia do terceiro padr\u00e3o \n\n\n\n32 \n\n \n\ncorresponde a uma raz\u00e3o de 0,5. A implementa\u00e7\u00e3o deste item foi feita atrav\u00e9s de um \n\nprograma escrito em Matlab [36] denominado \u201cAmSint1.m\u201d. \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.4a: Comportamento t\u00edpico dos sensores na inje\u00e7\u00e3o da amostra. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.4b: Dados Sint\u00e9ticos usados para a cria\u00e7\u00e3o da Rede Neural e o FIS \n\nOs dados de cor vermelha foram relativos ao padr\u00e3o de combust\u00edvel com maior \n\npoder calor\u00edfico. O padr\u00e3o2 em azul representa o padr\u00e3o de combust\u00edvel com o menor \n\nTempo (s) \n\nResist\u00eancias \n\n(kOhms) \n\nRi (Kohms) \n\n\n\n33 \n\n \n\npoder calor\u00edfico. O padr\u00e3o3 em verde representa o padr\u00e3o de combust\u00edvel com um poder \n\ncalor\u00edfico intermedi\u00e1rio com rela\u00e7\u00e3o aos anteriores.  \n\nOs dados obtidos por este m\u00e9todo foram usados no cap\u00edtulo 5.3 deste trabalho. \n\nComo a obten\u00e7\u00e3o das amostras experimentais deste m\u00e9todo era dificultada pelo \n\nmanuseio dos rot\u00e2metros, optou-se por adotar uma nova metodologia experimental para \n\nextrair e tratar os dados do g\u00e1s GLP, que est\u00e1 mostrado no item 4.3 a seguir. \n\n \n\n4.3 Procedimento para extra\u00e7\u00e3o e tratamento dos dados do GLP \n\nPara a extra\u00e7\u00e3o de caracter\u00edsticas das amostras de g\u00e1s GLP, foram usados dois \n\nprocedimentos distintos. O primeiro procedimento foi o de coleta do g\u00e1s GLP e o \n\nconfinamento do mesmo em uma seringa. Este procedimento foi diferente do usado na \n\nextra\u00e7\u00e3o do vapor de \u00e1lcool, pois n\u00e3o pod\u00edamos extrair a amostra de g\u00e1s GLP \n\ndiretamente do botij\u00e3o de g\u00e1s dispon\u00edvel do laborat\u00f3rio, e com isso, foi necess\u00e1rio o uso \n\ndo artif\u00edcio da bexiga. O segundo procedimento foi a obten\u00e7\u00e3o das medidas do g\u00e1s GLP \n\ncom a inje\u00e7\u00e3o da amostra da seringa na c\u00e2mara de sensores. Esses dois procedimentos \n\nest\u00e3o descritos nos itens 4.3.1 e 4.3.2 a seguir. \n\n \n\n4.3.1 Procedimento para coleta da amostra de g\u00e1s GLP \n\nO procedimento experimental adotado para extrair os dados de uma amostra de \n\ng\u00e1s combust\u00edvel do sistema de reconhecimento foi baseado em uma c\u00e2mara mostrada na \n\nFig. 4.3.1a: \n\n \n\n \n\n \n\n \n\n \n\nFig. 4.3.1a: C\u00e2mara de medidas. \n\nA purga ou limpeza da c\u00e2mara dos sensores foi feita atrav\u00e9s de uma bomba de \n\nsuc\u00e7\u00e3o de ar. As v\u00e1lvulas 1 e 2 foram mantidas fechadas (com as suas \u201calavancas\u201d \n\nposicionadas na vertical), para isolar a c\u00e2mara dos sensores do meio externo. A c\u00e2mara \n\nfoi tampada com uma tampa de borracha por onde passa o tubo de suc\u00e7\u00e3o do ar feito \n\n C\u00e2mara com os Sensores\n\nV\u00e1lvula 1 V\u00e1lvula 2\n\nConector\nDB9Bomba de\n\nsuc\u00e7\u00e3o de ar\n\nInje\u00e7\u00e3o da amostra de g\u00e1s\n\n+5 Vcc (placa do sensor de \n\n\n\n34 \n\n \n\npela bomba. A inser\u00e7\u00e3o da amostra de g\u00e1s foi feita injetando a amostra de g\u00e1s a ser \n\nanalisado por meio de uma seringa, espetada nesta tampa de borracha.  \n\nOs seis sensores utilizados foram os mesmos do item 4.2 anterior. Eles foram \n\nalimentados com 12Vcc. A tens\u00e3o dos aquecedores dos sensores foi ajustada com 5Vcc. \n\nOs fios de alimenta\u00e7\u00e3o da placa com o sensor de umidade foram alimentados com 5Vcc. \n\nO conector DB9 foi ligado a uma placa de aquisi\u00e7\u00e3o de sinais, que por sua vez foi \n\nligado ao equipamento PXI. \n\nO sistema de coleta do g\u00e1s foi feito por meio de uma seringa e uma bexiga. O \n\nprocedimento de coleta est\u00e1 ilustrado na Fig. 4.3.1b abaixo: \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 4.3.1b: Sistema de coleta do g\u00e1s \n\nA coleta da amostra dos gases foi feita seguindo o seguinte procedimento \n\noperacional: \n\n1)  Ligar a fonte de tens\u00e3o para alimentar o controlador de fluxo (MKS186A) de g\u00e1s, e \n\nespere 5 minutos para o controlador aquecer at\u00e9 atingir a precis\u00e3o necess\u00e1ria.  \n\n2)  Desconectar o tubo de borracha com a bexiga do tubo de pl\u00e1stico, para assegurar \n\nque res\u00edduos de g\u00e1s n\u00e3o fiquem nos tubos de acesso \u00e0 bexiga. \n\n3)  Abrir a VI \u201cmedidas\u201d (Fig 4.2c). Se este programa come\u00e7ar enviando uma \n\nmensagem de erro, seguir o procedimento sugerido na mesma mensagem.   \n\n4)  Ligar a v\u00e1lvula V3, abrindo o fluxo do g\u00e1s nitrog\u00eanio. Ligar a v\u00e1lvula V4 at\u00e9 que o \n\nman\u00f4metro ligado no botij\u00e3o do g\u00e1s GLP atinja valor est\u00e1vel. Fechar a v\u00e1lvula V4, \n\npara que seja usado somente o g\u00e1s GLP contido na mangueira, e tamb\u00e9m por \n\nquest\u00f5es de seguran\u00e7a. Ap\u00f3s esvaziar completamente a bexiga, re-encaixar o tubo \n\nde borracha no tubo de pl\u00e1stico. \n\nFonte de\nTens\u00e3o\n\n+15 V\n\nGND\n-15V\n\nbal\u00e3o com\ng\u00e1s\n\nTubo de borracha\n\nG\nL\nP\n\nN\n2\n\nControladores de\nfluxo de g\u00e1s\n\nN\n2 GLP\n\nV3\n\nV4\n\nPXI\n\nManipulador dos\nControladores de\n\nFluxo\n\nSeringa\n\nTubos de pl\u00e1stico\n\n\n\n35 \n\n \n\n5)  Na VI \u201cmedidas\u201d coloque o fluxo, em sccm (scc=standart cubic centimeter per \n\nminute; 1000sccm = 1litro/min) de g\u00e1s, e o tempo durante o qual este fluxo deve \n\nestar entrando na bexiga. Seguir o mesmo procedimento com o Nitrog\u00eanio. \n\nRecomenda -se utilizar um mesmo intervalo de tempo tanto para o g\u00e1s qua nto para o \n\nNitrog\u00eanio. Os valores que se mostram na Fig. 4.3.1c correspondem a uma \n\npropor\u00e7\u00e3o de 5:1. Recomenda-se 1000sccm de nitrog\u00eanio e a faixa de fluxo de 10 a \n\n500sccm de GLP. Os tempos recomendados de inje\u00e7\u00e3o para cada um dos gases s\u00e3o \n\nde 10 segundos. Com a op\u00e7\u00e3o \u201cPrepara\u00e7\u00e3o de Amostra\u201d selecionada, pressione o \n\nbot\u00e3o \u201cOK\u201d. Uma vez que a bexiga tenha enchido um pouco, a amostra de g\u00e1s pode \n\nser extra\u00edda com a seringa, espetando-o no tubo de borracha. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 4.3.1c: VI que controla a mistura de g\u00e1s com nitrog\u00eanio. \n\nCada extra\u00e7\u00e3o de g\u00e1s poder\u00e1 fornecer de duas a tr\u00eas medidas seq\u00fcenciais. Ap\u00f3s \n\ntr\u00eas medidas no m\u00e1ximo, esvaziar a bexiga e coletar novamente a mistura de g\u00e1s \n\nexecutando o aplicativo VI \u201cmedidas\u201d. Caso se queira medir a resposta das resist\u00eancias \n\ndos sensores ao g\u00e1s puro, \u00e9 necess\u00e1rio somente colocar 0sccm no campo nitrog\u00eanio. \n\nPara medir as respostas das resist\u00eancias dos sensores quando expostos ao g\u00e1s nitrog\u00eanio \n\npuro, coloque 0sccm no campo G\u00e1s. \n\n \n\n4.3.2 Procedimento para efetuar as medi\u00e7\u00f5es de g\u00e1s GLP \n\nO procedimento aplicado para as medi\u00e7\u00f5es foi o seguinte: \n\n1)  Ligar o equipamento PXI, executar o programa NarizEletr\u00f4nico Cliente, clicar no \n\nbot\u00e3o Conectar, e inserir o login e password.  \n\n\n\n36 \n\n \n\n2)  Efetuar a limpeza da c\u00e2mara. Para isso, ligue a bomba de suc\u00e7\u00e3o de ar que ir\u00e1 extrair \n\no ar dentro da c\u00e2mara. Observe no PXI a resposta dos sensores. Espere os mesmos \n\nse estabilizarem (considera-se que os sensores est\u00e3o estabilizados, quando a \n\nvaria\u00e7\u00e3o das resist\u00eancias dos mesmos n\u00e3o ultrapasse a porcentagem de 0.05% da \n\nvaria\u00e7\u00e3o medida entre os 15 pontos sucessivos (para uma amostragem de 0.5s, \n\nequivale a um tempo de 7.5s) para iniciar as medidas). \n\n3)  Desligue a bomba de v\u00e1cuo. Espere os sensores se estabilizarem novamente, \n\nobservando suas respostas no PXI. \n\n4)  Extrair uma amostra de g\u00e1s combust\u00edvel, seguindo o \u201cProcedimento para Coleta de \n\namostra de g\u00e1s\u201d, descrito anteriormente. Espetar a seringa na mangueira de \n\nborracha, e extrair a amostra do g\u00e1s (recomenda -se retirar um volume de 1ml). Pare \n\na execu\u00e7\u00e3o na VI \u201cmedidas\u201d, clicando em \u201cSTOP\u201d. \n\n5)  No PXI escolha um nome de arquivo em formato texto no padr\u00e3o:  \n\n<data>_<fluxo>sccmN2_<fluxo>sccmGAS<tempo>s_<volume>ml-\n\ninjetado_T<temperatura>UR<umidade relativa>_med<n\u00famero da medida>.txt \n\nPor exemplo, para uma medi\u00e7\u00e3o de uma amostra feita no dia 08 de agosto de \n\n2003, segundo 200sccm de g\u00e1s, 1000sccm de nitrog\u00eanio por um tempo de 10s, volume \n\nde inje\u00e7\u00e3o de 1ml, temperatura de 20\u00b0C, Umidade Relativa de 14%, e n\u00fame ro da \n\nmedida 15, o nome do arquivo adotado para a pr\u00f3xima medi\u00e7\u00e3o foi: \n\n08-08-03_200sccmGAS_1000sccmN2_10s_1ml-injetado_T20_UR14_med16.txt.  \n\n6)  Clique em gravar, e insira o g\u00e1s da seringa na c\u00e2mara com os sensores. Espere \n\nestabilizar a resposta dos mesmos observando o PXI, e clique em Parar.  \n\n7)  Para efetuar uma nova leitura, repita os passos de 2 a 6. \n\n8)  Ap\u00f3s terminar as medi\u00e7\u00f5es, assegure-se de desligar os registros do g\u00e1s e do \n\nNitrog\u00eanio. Al\u00e9m disso, desligue a fonte de alimenta\u00e7\u00e3o do controlador de fluxo \n\n(MKS186A). Por\u00e9m n\u00e3o desligue a fonte que alimenta os sensores de g\u00e1s. \n\n \n\n\n\n37 \n\n \n\n5 Pr\u00e9-Processamento dos Dados \n\nO aplicativo Data Sculptor foi usado neste trabalho para um pr\u00e9vio estudo do \n\nefeito do pr\u00e9-processamento dos dados na etapa de treinamento e na caracter\u00edstica de \n\ngeneraliza\u00e7\u00e3o da rede neural. Para realizar este estudo, foi montado o esquema da Fig. \n\n5a utilizando os objetos do Data Sculptor. As caixas s\u00e3o os objetos do Data Sculptor \n\nque t\u00eam fun\u00e7\u00f5es distintas. As flechas que as interligam representam o fluxo dos dados \n\nque trafegam no esquema montado.  \n\n \n\nFig. 5a: O aplicativo Data Sculptor \n\nOs dados utilizados como entrada foram amostras sint\u00e9ticas baseadas em \n\nexperi\u00eancias feitas com sensores de \u00e1lcool et\u00edlico. Foram estipuladas faixas de resposta \n\npara cada sensor submetido a amostras de \u00e1lcool com qualidade boa (\u00e1lcool de \n\nconcentra\u00e7\u00f5es entre 94.3 e 98.4%), e ruim (\u00e1lcool com concentra\u00e7\u00f5es fora desta faixa). \n\nEsses dados de entrada s\u00e3o inseridos no sistema atrav\u00e9s do objeto \u201cIN\u201d nomeado como \n\n\u201cDados Crus\u201d na Fig. 5a. Os dados foram colocados em gr\u00e1ficos listados nos objetos \n\n\u201cGRAPH\u201ds nomeados como \u201cPAR1 x SA\u00cdDAS\u201d; \u201cPAR2 x SAIDAS\u201d, etc. \n\n \n\n\n\n38 \n\n \n\nO banco de dados resultante foi composto de 50 amostras (\"medidas\" dos oito \n\nsensores considerados, obtendo-se oito atributos) para treino e mais 50 amostras para \n\nteste. Cada atributo foi obtido dividindo-se os valores de tens\u00e3o dos sensores depois da \n\nexposi\u00e7\u00e3o dos mesmos ao \u00e1lcool (em regime permanente) pelos valores de tens\u00e3o antes \n\nda exposi\u00e7\u00e3o dos sensores ao \u00e1lcool. Para o \u00e1lcool de boa qualidade foram gerados 50 \n\namostras sint\u00e9ticas, e o mesmo n\u00famero foi gerada para o \u00e1lcool de qualidade ruim. Mas \n\npara submeter esses dados ao aplicativo Data Sculptor, eles foram ajuntados em um \n\narquivo avulso contendo 100 amostras sint\u00e9ticas. Para cada amostra foram designados \n\ndois campos (colunas na Tabela 5a do banco de dados) destinados a sa\u00edda desejada, \n\nsendo um dado enumerado: \n\nTabela 5a: Valores Desejados adotados para as sa\u00eddas \n\n Saida Desejada 1 Saida Desejada 2 \n\u00c1lcool Bom 1 0 \n\u00c1lcool Ruim 0 1 \n\n \n\nComo os gr\u00e1ficos obtidos para os oito atributos versus a sa\u00edda ficaram parecidos, \n\npor conveni\u00eancia, abaixo na Fig. 5b \u00e9 mostrado um exemplo de gr\u00e1fico com os dados \n\n\u201ccrus\u201d para um dos sensores, obtido pelo objeto GRAPH (PAR1 X SAIDAS): \n\n \n\nFig. 5b: Gr\u00e1fico obtido com o Data Sculptor antes do pr\u00e9-processamento \n\nPara testar o car\u00e1ter seletivo do aplicativo Data Sculptor, foram introduzidos \n\nalgumas amostras redundantes e inconsistentes ao banco de dados. Duas amostras s\u00e3o \n\n\n\n39 \n\n \n\nconsideradas redundantes quando os seus atributos e sa\u00eddas desejadas s\u00e3o exatamente os \n\nmesmos, ou seja, as duas amostras s\u00e3o id\u00eanticas. \u00c9 necess\u00e1rio remover essas amostras \n\ndo banco de dados, pois elas atrapalham o treinamento da rede neural. Duas amostras \n\ns\u00e3o inconsistentes quando seus atributos s\u00e3o id\u00eanticos, por\u00e9m suas sa\u00eddas desejadas n\u00e3o \n\no s\u00e3o. Para remover essas amostras foram utilizados dois objetos, o objeto Sort \n\n(ORDEM_ASCEND) e o Sieve (INCONS_REDUND). \n\nPara o objeto Sort foi selecionado o atributo 1 do banco de dados (primeira \n\ncoluna), e todas as amostras foram ordenadas baseadas no valor crescente desse \n\natributo. Depois esses dados passaram pelo objeto Sieve, que fez uma filtragem nesse \n\nbanco de dados. Foi utilizada a seguinte express\u00e3o para a filtragem: \n\n!REC_COMP(PARAMETRO1, 109, 1)  \n\nSendo que o s\u00edmbolo \u201c!\u201d representa uma nega\u00e7\u00e3o, ou seja, as amostras diferentes \n\ns\u00e3o as que continuar\u00e3o no fluxo de dados, e as amostras id\u00eanticas ou inconsistentes \n\nser\u00e3o filtradas. Depois o banco de dados foi submetido a um objeto Detail (ADP). Nesse \n\nobjeto foi utilizada a ferramenta ADP (Automatic Data Preprocessing). Essa ferramenta \n\nrealiza automaticamente a escolha das fun\u00e7\u00f5es matem\u00e1ticas que s\u00e3o usadas para \n\ntransformar as amostras que est\u00e3o sendo injetados nele. O ADP possui tr\u00eas modos de \n\ntransforma\u00e7\u00e3o de acordo com o n\u00edvel crescente de complexidade das fun\u00e7\u00f5es \n\nmatem\u00e1ticas obtidas pelo mesmo: o BRIEF, o MODERATE, o THROUGHOUT e o \n\nEXPERT. O usu\u00e1rio dever\u00e1 escolher o modo de transforma\u00e7\u00e3o de acordo com a sua \n\npercep\u00e7\u00e3o da complexidade do problema a ser resolvido.  \n\nPara o modo BRIEF, foram obtidas as seguintes fun\u00e7\u00f5es para cada atributo \n\n(PARAMETRO's): \n\nPARAMETRO10 = TRNSCALE(PARAMETRO1,0,1,0.838042,1.21017)  \n\nPARAMETRO20 = TRNSCALE(PARAMETRO2,0,1,0.213979,2.42972)  \n\n(...) \n\nPARAMETRO80 = TRNSCALE(PARAMETRO8 ...) \n\nSAIDA10 = TRNSCALE(SAIDA1,0,1,0,1) \n\nSAIDA20 = TRNSCALE(SAIDA2,0,1,0,1) \n\nPara o modo MODERATE, foram obtidas as seguintes fun\u00e7\u00f5es: \n\nPARAMETRO1000 = TRNSQR(PARAMETRO1,0,1,0.814189,1.21017) \n\nPARAMETRO1001 = TRNSCALE(PARAMETRO1,0,1,0.84485,1.21017) \n\n(...) \n\n\n\n40 \n\n \n\nSAIDA10 = TRNSCALE(SAIDA1,0,1,0,1) \n\nSAIDA20 = TRNSCALE(SAIDA2,0,1,0,1) \n\nPara o modo THROUGHOUT, foram obtidas as fun\u00e7\u00f5es: \n\nPARAMETRO10000 = TRNSQR(PARAMETRO1,0,1,0.814189,1.21017) \n\nPARAMETRO10001 = TRNFUZZYLEFT(PARAMETRO10000,0,0.142857) \n\nPARAMETRO10002 = TRNFUZZYCENTER(PARAMETRO10000,0,0.142857,0.428571) \n\nPARAMETRO10003 = TRNFUZZYCENTER(PARAMETRO10000,0,0.285714,0.571429) \n\n(...) \n\nPARAMETRO10007 = TRNFUZZYCENTER(PARAMETRO10000,0,0.714286,1) \n\nPARAMETRO10008 = TRNFUZZYRIGHT(PARAMETRO10000,0,0.857143,1) \n\nSAIDA10 = TRNSCALE(SAIDA1,0,1,0,1) \n\nSAIDA20 = TRNSCALE(SAIDA2,0,1,0,1) \n\nE, finalmente para o modo EXPERT, as fun\u00e7\u00f5es obtidas foram: \n\nPARAMETRO10000 = TRNSQR(PARAMETRO1,0,1,0.814189,1.21017) \n\nPARAMETRO10001 = TRNFUZZYLEFT(PARAMETRO10000,0,0.142857) \n\nPARAMETRO10002 = TRNFUZZYCENTER(PARAMETRO10000,0,0.142857,0.428571) \n\nPARAMETRO10003 = TRNFUZZYCENTER(PARAMETRO10000,0,0.285714,0.571429) \n\n(...) \n\nPARAMETRO10007 = TRNFUZZYCENTER(PARAMETRO10000,0,0.714286,1) \n\nPARAMETRO10008 = TRNFUZZYRIGHT(PARAMETRO10000,0,0.857143,1) \n\nPARAMETRO10009 = TRNSCALE(PARAMETRO1,0,1,0,0.844815) \n\nSAIDA10 = TRNSCALE(SAIDA1,0,1,0,1) \n\nSAIDA20 = TRNSCALE(SAIDA2,0,1,0,1) \n\n \n\nNeste trabalho foi escolhido o modo \u201cbrief\u201d, pois os dados sint\u00e9ticos dispon\u00edveis \n\neram linearmente separ\u00e1veis. Para o modo BRIEF, ap\u00f3s a transforma\u00e7\u00e3o dos dados feita \n\npelo ADP, o gr\u00e1fico resultante para o objeto GRAPH (PAR10 X SAIDAS) \u00e9 mostrado \n\nna Fig. 5c. Vemos por esse gr\u00e1fico que o Data Sculptor ajustou uma escala para as \n\namostras rand\u00f4micas, fazendo com que todos os registros se enquadrassem no intervalo \n\nentre 0 a 1. Isso porque a inten\u00e7\u00e3o foi usar os dados para treinar uma rede segundo a \n\nfun\u00e7\u00e3o de ativa\u00e7\u00e3o de sigm\u00f3ide, cujo intervalo vai de 0 a 1. Caso a rede a ser treinada \n\nfosse usar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o de tangente hiperb\u00f3lica, o intervalo definido na caixa de \n\ndi\u00e1logo do ADP seria de -0.8 a +0.8, e conseq\u00fcentemente, a escala do gr\u00e1fico resultante \n\nse ajustaria ao mesmo intervalo de -0.8 a +0.8. O objetivo desse ajuste \u00e9 melhorar a \n\n\n\n41 \n\n \n\nperformance do treinamento da rede neural, que ir\u00e1 enxergar dados mais compat\u00edveis, e \n\nvisivelmente mais separados, acelerando o processo de aprendizado da rede. \n\n \n\nFig. 5c: Gr\u00e1fico obtido com o Data Sculptor ap\u00f3s o pr\u00e9-processamento \n\nA pr\u00f3xima etapa foi ordenar randomicamente as amostras no banco de dados. \n\nPara isso foi utilizado um objeto SORT (RANDOMIZE) ordenando aleatoriamente as \n\namostras baseando-se no PARAMETRO1. Depois foi utilizado um objeto DETAIL \n\n(SEP_CONJ) para separar as amostras em tr\u00eas conjuntos de amostras, um conjunto de \n\ndados de valida\u00e7\u00e3o, outro de treino, e por \u00faltimo um conjunto para teste. A express\u00e3o \n\nutilizada para separar os tr\u00eas conjuntos no objeto DETAIL (modo BRIEF) foi: \n\nSEP_CONJ = DATASET (PARAMETRO10,8,92,50) \n\nEsse comando criou um novo atributo chamado SEP_CONJ, que cont\u00eam o \n\nresultado da sele\u00e7\u00e3o feita por esse objeto. Isto \u00e9, as atribui\u00e7\u00f5es a essa vari\u00e1vel s\u00e3o feitas \n\nsegundo a regra mostrada na Tabela 5b: \n\nTabela 5b: Separa\u00e7\u00e3o dos dados feita pelo aplicativo Data Sculptor  \n\nDados destinados a: Valor atribu\u00eddo a vari\u00e1vel SEP_CONJ \nNenhum banco de dados 0 \n\nBanco de dados de valida\u00e7\u00e3o 1 \nBanco de dados de treino 2 \nBanco de dados de teste 3 \n\n \n\n\n\n42 \n\n \n\nNesse exemplo, a ordena\u00e7\u00e3o aleat\u00f3ria das amostras foi baseada nos valores do \n\nprimeiro atributo, ou seja, PARAMETRO10. Os par\u00e2metros que se seguem desse \n\ncomando representam respectivamente, a porcentagem do total do banco de dados que \n\nser\u00e1 destinado ao conjunto de valida\u00e7\u00e3o (8%), a porcentagem da soma dos conjuntos de \n\ntreino e teste com rela\u00e7\u00e3o ao n\u00famero total do banco de dados (92%, ou seja 0% das \n\namostras n\u00e3o pertencer\u00e1 a nenhum conjunto), e finalmente o \u00faltimo par\u00e2metro \n\nrepresenta a raz\u00e3o entre o n\u00famero de dados de treino versus teste. Nesse caso 50%, ou \n\nseja, metade dos 92% do banco de dados para cada conjunto. Por\u00e9m para o treinamento \n\nda rede com os dados pr\u00e9-processados, foi utilizado somente o conjunto de treino. O \n\nconjunto de teste foi usado para testar a rede j\u00e1 treinada. \n\nO objeto SIEVE para cada conjunto serve para separar efetivamente os tr\u00eas \n\nbancos de dados. As express\u00f5es utilizadas em cada bloco SIEVE para a filtragem de \n\ncada conjunto foram as seguintes: \n\nPara o conjunto de valida\u00e7\u00e3o:                   SEP_CONJ == 1 \n\nPara o conjunto de teste:                           SEP_CONJ == 2 \n\nPara o conjunto de treino:                         SEP_CONJ == 3 \n\n \n\nImplementa\u00e7\u00e3o de an\u00e1lise de componentes principais no Matlab \n\nA implementa\u00e7\u00e3o da an\u00e1lise de componentes principais dos dados de \n\ntreinamento no aplicativo Matlab \u00e9 feita com dois comandos do toolbox de Redes \n\nNeurais Artificiais: premnmx e prepca. Eles est\u00e3o detalhados a seguir. \n\n[pn,meanp,stdp] = premnmx(<Dados de Treino>); \n\nOs dados de treino necessitam ser inicialmente normalizados pelo comando \n\n\u201cpremnmx\u201d para que eles tenham m\u00e9dia 0 (zero) e desvio padr\u00e3o igual a 1. Em pn estar\u00e1 \n\ncontido o vetor normalizado dos dados de treino. A vari\u00e1vel meanp conter\u00e1 a m\u00e9dia das \n\namostras de treino e stdp conter\u00e1 o desvio padr\u00e3o dos mesmos. E, na linha: \n\n[ptrans,transMat] = prepca(pn,0.5); \n\nO segundo argumento passado para o comando \u201cprepca\u201d determina que aqueles \n\ncomponentes principais de \u201cpn\u201d que contribuirem com menos de 0,5 % da varia\u00e7\u00e3o total \n\nno conjunto de dados de treinamento ser\u00e3o eliminados. \n\nA matriz \u201cptrans\u201d cont\u00e9m os vetores de entrada transformados, e a matriz \n\n\u201ctransMat\u201d cont\u00e9m a matriz que foi obtida da an\u00e1lise dos componentes principais das \n\n\n\n43 \n\n \n\namostras de treino da rede neural artificial. Portanto, multiplicando-se os dados de \n\nentrada com a matriz transMat, obt\u00e9m-se a matriz ptrans. \n\nNa implementa\u00e7\u00e3o de uma rede neural do tipo feedforward, necessitamos \n\nfornecer a matriz \u201cPR\u201d, que nada mais \u00e9 do que uma matriz R x 2, onde R \u00e9 o n\u00famero \n\nde entradas da rede neural (no nosso caso, 8 entradas). \n\nFoi usado o comando premnmx ao inv\u00e9s do comando prepca, pois ao contr\u00e1rio \n\ndeste \u00faltimo que normaliza os dados de treinamento em um intervalo entre 0 e 1, o \n\ncomando premnmx normaliza os dados no intervalo entre \u2013 1 e 1. Com isso, foi poss\u00edvel \n\no uso das fun\u00e7\u00f5es de ativa\u00e7\u00e3o de tangente hiperb\u00f3lica, ao inv\u00e9s da sigm\u00f3ide. \n\nOs dados de teste necessitaram de transforma\u00e7\u00f5es para serem usados nas redes \n\nneurais treinadas com os dados pr\u00e9-processados e com a an\u00e1lise de compone ntes \n\nprincipais. A normaliza\u00e7\u00e3o dos dados de teste foi feita com o comando tramnmx. Foram \n\nutilizados os mesmos valores de m\u00e1ximos e m\u00ednimos dos dados de treino da RNA. A \n\nmatriz com os dados de teste normalizados foi transformada com o comando trapca, \n\nutilizando a matriz de transforma\u00e7\u00e3o transMat obtida dos dados de treinamento.  \n\nAp\u00f3s a simula\u00e7\u00e3o da rede neural com os dados de teste transformados por esses \n\ndois comandos tramnmx e trapca, o resultado da rede neural ainda se encontrava no \n\nintevalo entre \u20131 e 1. Foi necess\u00e1rio utilizar o comando postmnmx para que os dados \n\nresultantes ficassem no intervalo entre 0 e 1; e pudessem ser comparados com os dados \n\nde teste com as sa\u00eddas desejadas. \n\n5.1 Reconhecimento da qualidade de \u00e1lcool com Redes Neurais \n\nPara o reconhecimento da qualidade do \u00e1lcool combust\u00edvel, foi usado o mesmo \n\nsistema de sensores Taguchi. Foram considerados dois padr\u00f5es de \u00e1lcool: de boa \n\nqualidade (com concentra\u00e7\u00f5es de 92.6% a 94.3%) e o de m\u00e1 qualidade (com outras \n\nconcentra\u00e7\u00f5es). O \u00e1lcool combust\u00edvel foi injetado na c\u00e2mara de sensores por meio de \n\numa seringa. O sistema usado por rot\u00e2metros foi substitu\u00eddo por um sistema de tubos. A \n\nlimpeza da c\u00e2mara foi feita com um ventilador e um filtro de ar contendo s\u00edlica gel, para \n\nal\u00e9m de retirar res\u00edduos de \u00e1lcool, retirar tamb\u00e9m a umidade. O sistema de aquisi\u00e7\u00e3o de \n\ndados foi o mesmo.  \n\nForam testados dois algoritmos de aprendizagem de redes neurais \u2013 \n\nbackpropagation e LVQ \u2013  para reconhecer os padr\u00f5es dos combust\u00edveis gasosos, seja \n\n\n\n44 \n\n \n\ncom ou sem a an\u00e1lise de componentes principais. Sendo que neste \u00faltimo caso, a rede \n\nneural teve um n\u00famero menor de entradas, em conseq\u00fc\u00eancia da redu\u00e7\u00e3o de \n\ndimensionalidade. \n\nAs redes neurais sem ACP tiveram 6 neur\u00f4nios de entrada, 6 neur\u00f4nios na \n\ncamada escondida, e dois neur\u00f4nios na camada de  sa\u00edda. J\u00e1 as redes neurais com ACP \n\ntiveram 4 entradas, 4 neur\u00f4nios na camada escondida e dois neur\u00f4nios na camada de \n\nsa\u00edda. O n\u00famero de neur\u00f4nios usados nas camadas escondidas foi obtido a partir de \n\ntreinamentos pr\u00e9vios das redes neurais, e an\u00e1lises das capacidades de generaliza\u00e7\u00e3o para \n\ncada um dos casos, obtendo-se um valor \u00f3timo emp\u00edrico de n\u00famero de neur\u00f4nios \n\nescondidos. \n\nV\u00e1rios treinamentos foram realizados. O par\u00e2metro para a interrup\u00e7\u00e3o do \n\ntreinamento foi o erro RMS com valor de 0.01, ou o gradiente do erro de 0.001. Os \n\nresultados obtidos est\u00e3o demonstrados na Tabela 5.1a. \n\nA Fig. 5.1a mostra um gr\u00e1fico obtido de um dos sensores com os dados \n\nexperimentais do \u00e1lcool combust\u00edvel. Os pontos azuis escuros representam o padr\u00e3o de \n\n\u00e1lcool bom, enquanto os pontos em rosa representam os dados do \u00e1lcool ruim. Pode -se \n\nnotar que h\u00e1 sobreposi\u00e7\u00e3o de alguns dados do \u00e1lcool bom nos dados do \u00e1lcool ruim, o \n\nque pode explicar os erros obtidos na generaliza\u00e7\u00e3o da rede neural.  \n\nUm estudo do efeito do pr\u00e9-processamento nos dados de entrada com o \n\naplicativo DataSculptor foi feito com dados sint\u00e9ticos simulando a inje\u00e7\u00e3o de \u00e1lcool na \n\nc\u00e2mara de ensaios. O crit\u00e9rio de converg\u00eancia da rede foi o erro m\u00e9dio quadr\u00e1tico a \n\n0.01. O m\u00e9todo de pr\u00e9-processamento escolhido foi o \u201cbrief\u201d, pois n\u00e3o havia \n\nsobreposi\u00e7\u00e3o dos dados sint\u00e9ticos dos dois padr\u00f5es, e al\u00e9m disso os dados dos dois \n\npadr\u00f5es eram linearmente separ\u00e1veis. O DataSculptor aplicou uma escala nos dados e \n\nseparou-os randomicamente em conjuntos de treino e teste. A rede foi treinada dez \n\nvezes com os dados crus, e depois mais dez vezes com os dados pr\u00e9-processados. Foi \n\nobservado que a rede com os dados pr\u00e9-processados foi treinada mais rapidamente, \n\ntendo uma raz\u00e3o m\u00e9dia de \u00e9pocas necess\u00e1rias para o treinamento de 0.15 com rela\u00e7\u00e3o a \n\nrede treinada com os dados sem o pr\u00e9-processamento. \n\n\n\n45 \n\n \n\n \n\nTabela 5.1a: Resultados obtidos das RNA\u2019s no reconhecimento de \u00e1lcool \n\n Com Pr\u00e9-Processamento Sem Pr\u00e9 -Processamento \n\nTraingdx  Trainlm Learnlvq  Traingdx  Trainlm Learnlvq  \n \n\n\u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \n\nM\u00e1ximo 774 25 961 34 200 31 6395 28 119 25 200 31 \n\nM\u00ednimo 170 15 2 15 200 31 1309 18 1 18 200 31 \n\nM\u00e9dia 369.95 20.19 221.33 25.38 200 31 2501.63 20.18 32.91 21.13 200 31 \n\nDesvio \nPadr\u00e3o \n\n159.04 3.39 218.19 5.97 0 0 1396.59 2.99 31.45 2.86  0 0 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.1a: Varia\u00e7\u00e3o da resist\u00eancia versus resist\u00eancia inicial (sensor 6) \n\n \n\n5.2 Implementa\u00e7\u00e3o da RNA em DSP (Digital Signal Processor)  \n\nFoi utilizado o software Neural Works Professional II Plus para construir o \n\nc\u00f3digo em C da rede neural a ser utilizada. O procedimento da cria\u00e7\u00e3o da rede neural \n\nartificial foi a seguinte: \n\nCom o comando InstaNet [33], foi escolhido a rede neural do tipo Back \n\nPropagation. A seguir, foi escolhido o n\u00famero de neur\u00f4nios utilizados: oito entradas, \n\nduas sa\u00eddas e tr\u00eas camadas escondidas (camadas 3, 4 e 5) com 8 neur\u00f4nios em cada \n\numa. Como regra de aprendizado, foi utilizado a backpropagation. Como fun\u00e7\u00e3o de \n\ntransfer\u00eancia foi utilizada a fun\u00e7\u00e3o sigm\u00f3ide (cuja express\u00e3o \u00e9 f(x) = 1/(1+exp(-x))). \n\nPara os bancos de dados de treino e de teste foram utilizados respectivamente os \n\n\n\n46 \n\n \n\narquivos treino.nna e teste.nna (para os dados crus, sem serem submetidos ao \n\nData Sculptor), e depois os arquivos cnj_trn.nna e cnj_tst.nna. \n\nO crit\u00e9rio de converg\u00eancia da rede foi escolhido como RMS Error &lt;0,01. \n\nA tabela de aprendizado/teste de par\u00e2metros da rede neural utilizada est\u00e1 \n\nmostrada na Tabela 5.2a: \n\nTabela 5.2a: Coeficientes utilizados no aplicativo Neural Works  \n\nTabela de aprendizado/teste \nLearning Count 50000 \n\nTemperature 0,000 \nLearning Rate 0,9 \nMomentum 0,6 \n\nError Tolerance 0 \nWeight Decay 0 \nCoefficient5 0,002 \nCoefficient6 0 \nCoefficient7 0 \nCoefficient8 0 \nCoefficient9 0 \n\nA tabela das v\u00e1rias tentativas e itera\u00e7\u00f5es de treinamento da rede utilizando-se as \n\namostras pr\u00e9-processadas e as amostras cruas \u00e9 mostrada na Tabela 5.2b: \n\nTabela 5.2b: Itera\u00e7\u00f5es necess\u00e1rias para o treinamento da rede neural \n\nItera\u00e7\u00f5es necess\u00e1rias para treinar a rede backpropagation: \nCom Pr\u00e9 Processamento Sem Pr\u00e9 Processamento \n\n4162 9429 \n2158 28658 \n3700 26700 \n2658 25200 \n3658 9666 \n5158 29200 \n2656 31300 \n1700 23204 \n2429 9704 \n\nPode-se notar que as amostras Pr\u00e9-Processadas possibilitaram que a rede fosse \n\ntreinada com um Erro RMS menor que 0,01, em um n\u00famero de itera\u00e7\u00f5es menor do que \n\nse a rede fosse treinada com as amostras cruas. Os valores de itera\u00e7\u00f5es em negrito \n\ncorrespondem \u00e0 rede resultante que foi utilizada para realizar o c\u00f3digo em C da rede \n\nneural a ser implementada no kit de DSP. \n\nFoi utilizado um kit de DSP da Analog Devices com o processador ADSP-21061 \n\npara implementar a rede neural j\u00e1 treinada, e gravados os dados de teste na mem\u00f3ria \n\nEPROM do kit para posteriormente observar as suas sa\u00eddas resultantes. Para isso, o \n\nbanco de dados com as amostras sint\u00e9ticas de teste foi utilizado como um arquivo *.h de \n\nbiblioteca. Foi utilizado tamb\u00e9m um sinal de interrup\u00e7\u00e3o externa (IRQ1) ligado a um \n\nbot\u00e3o de pressionamento para que fosse poss\u00edvel o monitoramento da entrada dos sinais \n\ne das sa\u00eddas resultantes.  \n\n\n\n47 \n\n \n\nO arquivo utilizado foi o \u201ctestev.h\u201d. O c\u00f3digo em C da rede neural foi obtida \n\nautomaticamente pelo programa Neural Works Professional II Plus. Este c\u00f3digo em C \n\nda rede neural gerada pelo Neural Works foi adaptado ao algoritmo e implementado no \n\nDSP. \n\n \n\n5.2.1 O programa implementado no DSP \n\nPrimeiramente foram feitas as defini\u00e7\u00f5es das v\u00e1rias vari\u00e1veis do programa, bem \n\ncomo da matriz cnj_teste que cont\u00e9m o banco de dados testev.h com amostras \n\ncruas. A declara\u00e7\u00e3o pm na linha \u201cfloat pm \n\ncnj_teste[NUM_REGISTROS][NUM_ATRIBUTOS]\u201d, significa que a matriz ser\u00e1 colocada \n\nna mem\u00f3ria de programa do DSP. As declara\u00e7\u00f5es \u201cvolatile\u201d significam que essas \n\nvari\u00e1veis s\u00e3o globais e que podem mudar a qualquer instante. \n\nDepois foi feito o prot\u00f3tipo da fun\u00e7\u00e3o NN_Recall e foram implementadas as \n\nfun\u00e7\u00f5es que ser\u00e3o utilizadas no programa main. Trata-se de duas rotinas de interrup\u00e7\u00e3o, \n\nem que a primeira, \u201cvoid timer_handler(int signal)\u201d trata de uma interrup\u00e7\u00e3o por \n\ntime-out, isto \u00e9, ap\u00f3s um determinado tempo (definido pela contagem do n\u00famero de \n\nciclos da fun\u00e7\u00e3o timer_set((unsigned int)10000, (unsigned int)10000);isto \u00e9, \n\n10000 ciclos), a interru\u00e7\u00e3o \u00e9 gerada. A segunda rotina,  void irq1_handler(int \n\nsignal) trata-se de uma interrup\u00e7\u00e3o externa causada por um bot\u00e3o de pressionamento \n\nexterno soldado no kit. \n\nO c\u00f3digo gerado pelo Neural Works j\u00e1 inclui os pesos resultantes da rede \n\ntreinada. Essa fun\u00e7\u00e3o NN_Recall, n\u00e3o possui retropropaga\u00e7\u00e3o, somente o passo de  feed-\n\nforward. Ela tem como par\u00e2metros de entrada as vari\u00e1veis yin[NUM_ATRIBUTOS \n\n(=8)], e yout[NUM_SAIDAS (=2)]. Essa fun\u00e7\u00e3o retorna zero caso sua execu\u00e7\u00e3o for \n\nbem sucedida. Como o par\u00e2metro yout \u00e9 um arranjo (ou vetor) de dados do tipo float, a \n\nfun\u00e7\u00e3o ao modificar esse vetor internamente, automaticamente estar\u00e1 mudando os seus \n\nvalores resultantes, que ser\u00e3o apresentados no t\u00e9rmino da execu\u00e7\u00e3o dessa fun\u00e7\u00e3o.  \n\nJ\u00e1 a rotina principal main realiza o seguinte algoritmo: \n\n1. Seta as vari\u00e1veis de interrup\u00e7\u00f5es (time-out e o bot\u00e3o de \n\npressionamento) para zero; \n\n2. Configura as rotinas de servi\u00e7o de interrup\u00e7\u00f5es (ISR's) para \n\nresponder as interrup\u00e7\u00f5es; \n\n3. Configura o timer para um time-out de 1000 ciclos de rel\u00f3gio; \n\n\n\n48 \n\n \n\n4. Inicializa os LEDs e entra em um la\u00e7o infinito sendo apresentadas \n\ntodas as amostras seq\u00fcencialmente: \n\n5. Espera ocorrer uma interrup\u00e7\u00e3o (instru\u00e7\u00e3o idle()); \n\n6. Se ocorrer a interrup\u00e7\u00e3o do pressionamento do bot\u00e3o segue os \n\nseguintes passos: \n\n7. Desliga o flag da interrup\u00e7\u00e3o IRQ1; \n\n8. Sinaliza que a interrup\u00e7\u00e3o ocorreu pela mudan\u00e7a do FLAG3 (LED6); \n\n9. Atribui uma amostra do banco de dados a vari\u00e1vel yin; \n\n10. Insere o yin na fun\u00e7\u00e3o NN_Recall e \u00e9 extra\u00eddo o valor de yout; \n\n11. Compara os valores de yout[0] e yout[1]. De acordo com a \n\ndiferen\u00e7a entre esses valores, o \u00e1lcool ser\u00e1 classificado como bom \n\nou ruim. A diferencia\u00e7\u00e3o \u00e9 feita atrav\u00e9s dos leds D3 e D4. \n\n12. Volta para o passo 5 \n\n \n\n5.2.2 Compila\u00e7\u00e3o e Linkagem \n\nPara compilar e linkar o programa do DSP, foi utilizado o comando: \n\ng21k -o rna4.21k rna4.c \n\nOnde o g21k \u00e9 o compilador e linkador propriamente dito. A op\u00e7\u00e3o -o significa \n\ncolocar a sa\u00edda (arquivo compilado) em um nome especificado. No caso foi o rna4.21k.  \n\nEsse comando gerou o arquivo rna4.21k, que mais tarde foi submetido a um \n\nprograma de interface do computador com o kit de DSP. Esse programa carrega o \n\narquivo rna4.21k na placa do kit de DSP. E com isso, a placa come\u00e7ou a executar o \n\nprograma sozinho. \n\nOutro comando utilizado foi o seguinte: \n\ng21k -g rna4.c -o rna4.exe \n\nEsse comando \u00e9 similar ao anterior, por\u00e9m ele gera um arquivo chamado \n\nrna4.exe; e al\u00e9m disso a op\u00e7 \u00e3o -g produz um c\u00f3digo debug\u00e1vel para ser usado com o \n\nCBUG. O CBUG \u00e9 um simulador que pode ser usado para testar o programa do \n\nprojetista antes mesmo de ele ser carregado no kit. \n\n \n\n5.2.3 Implementa\u00e7\u00e3o da Rede Neural em Hardware \n\nA rede neural artificial treinada com o programa NeuralWorks foi implementada \n\nem um kit de DSP da Analog Devices. Este programa extraiu os pesos de cada sinapse \n\nda rede neural, e eles foram transformados em um programa em linguagem C. Foi \n\n\n\n49 \n\n \n\nconsiderado que o hardware n\u00e3o iria aprender por si pr\u00f3prio. Com isso, a fun\u00e7\u00e3o que \n\nimplementa o passo forward da rede foi composta basicamente de somas e \n\nmultiplica\u00e7\u00f5es. A maior complexidade computacional encontrada foi o c\u00e1lculo da \n\nresposta da fun\u00e7\u00e3o de ativa\u00e7\u00e3o dos neur\u00f4nios, que tem car\u00e1ter n\u00e3o-linear. \n\nO sistema implementado inicialmente foi o de reconhecimento da qualidade do \n\n\u00e1lcool combust\u00edvel. Como uma primeira abordagem, o conjunto de dados de teste foi \n\ninclu\u00eddo no programa do kit do DSP. Com um bot\u00e3o do pr\u00f3prio kit, foi poss\u00edvel varrer \n\ntodos os dados de teste, e observar o resultado da rede neural atrav\u00e9s de dois led\u2019s do \n\npr\u00f3prio kit. Houve consist\u00eancia dos resultados simulados pelo programa NeuralWorks, \n\ncom os resultados observados nas respostas dos led\u2019s do kit. \n\nUm hardware dedicado ao reconhecimento da qualidade de \u00e1lcool composto por \n\num microcontrolador foi implementado em nosso laborat\u00f3rio. O programa da rede \n\nneural foi armazenado na mem\u00f3ria EEPROM, e os pesos sin\u00e1pticos foram armazenados \n\nna mem\u00f3ria FLASH. Como a mem\u00f3ria dispon\u00edvel era pequena, o programa da rede \n\nneural gerado pelo NeuralWorks teve de ser otimizado. O sistema somente trata os \n\nvalores iniciais e finais da resposta do sensor em regime permanente. Com esses \n\nvalores, ele realizou os c\u00e1lculos e apresentou a resposta da rede neural atrav\u00e9s dos led\u2019s \n\nsoldados no cart\u00e3o.  \n\n \n\n5.2.4 Resultados do programa implementado no DSP \n\nOs resultados obtidos est\u00e3o mostrados na Tabela 5.2.4a. \n\nComo se pode observar, houve um erro no reconhecimento da amostra. Vemos \n\ntamb\u00e9m que se tratou do \u00fanico resultado em que os valores de yout[0] e yout[1] ficaram \n\nbem parecidos. Ao inv\u00e9s de retornar um valor errado na sa\u00edda, pode-se atribuir um valor \n\nde d\u00favida, para que erros sejam evitados no reconhecimento do \u00e1lcool em quest\u00e3o. Isso \n\nmostra que a rede backpropagation \u00e9 efetiva, por\u00e9m n\u00e3o \u00e9 perfeita. H\u00e1 alguns erros que \n\nessa topologia deixa passar. Uma outra possibilidade para melhorar o reconhecimento \u00e9 \n\na escolha de um outro tipo de rede, como a Kohonen, LVQ, ADALINE, etc. \n\n\n\n50 \n\n \n\n \n\nTabela 5.2.4a: Resultados obtidos nas simula\u00e7\u00f5es das amostras de \u00e1lcool \n\n Arquivo testev.h Simula\u00e7\u00f5es  Experimento - kit \n# Reg. Yout[0] yout[1] Yout[0] yout[1] yout[0] yout[1] \n\n0 1 0 1,02076 -0,0176 1 0 \n1 0 1 -0,0001 1,00072 0 1 \n2 1 0 1,01205 -0,0115 1 0 \n3 0 1 0,04834 0,95376 0 1 \n4 1 0 1,02130 -0,0178 1 0 \n5 0 1 -0,0014 1,0020 0 1 \n6 1 0 0,95314 0,03707 1 0 \n7 0 1 -0,0072 1,0075 0 1 \n8 0 1 -0,0071 1,0074 0 1 \n9 0 1 0,01681 0,98457 0 1 \n10 0 1 0,13379 0,86803 0 1 \n11 0 1 -0,0076 1,00792 0 1 \n12 0 1 -0,0065 1,00689 0 1 \n13 0 1 0,00455 0,99635 0 1 \n14 0 1 -0,0065 1,00681 0 1 \n15 0 1 0,00238 0,99842 0 1 \n16 1 0 0,99615 0,00079 1 0 \n17 1 0 1,01458 -0,0134 1 0 \n18 1 0 0,99666 0,00039 1 0 \n19 0 1 0,05668 0,94552 0 1 \n20 0 1 -0,0033 1,00382 0 1 \n21 1 0 1,01041 -0,0128 1 0 \n22 0 1 -0,0068 1,00717 0 1 \n23 0 1 0,02828 0,97343 0 1 \n24 1 0 0,96694 0,02505 1 0 \n25 0 1 -0,0017 1,00235 0 1 \n26 0 1 0,04369 0,95834 0 1 \n27 0 1 0,51529 0,47285 1 0 \n28 0 1 -0,0011 1,00175 0 1 \n29 0 1 0,00898 0,99211 0 1 \n30 0 1 -0,0059 1,00629 0 1 \n31 0 1 -0,0081 1,00832 0 1 \n32 0 1 0,07265 0,92965 0 1 \n33 0 1 -0,0074 1,00773 0 1 \n34 1 0 1,02107 -0,0177 1 0 \n35 0 1 -0,0082 1,00844 0 1 \n36 1 0 1,01699 -0,0151 1 0 \n37 1 0 1,01859 -0,0162 1 0 \n38 0 1 -0,0074 1,00768 0 1 \n39 0 1 -0,0038 1,00432 0 1 \n40 1 0 1,02141 -0,0178 1 0 \n41 1 0 1,02174 -0,0179 1 0 \n42 0 1 -0,0076 1,00787 0 1 \n43 1 0 1,02065 -0,0175 1 0 \n44 1 0 1,01292 -0,0121 1 0 \n45 0 1 0,00223 0,99856 0 1 \n46 1 0 1,01541 -0,0139 1 0 \n47 0 1 0,27583 0,72194 0 1 \n48 1 0 1,02168 -0,0179 1 0 \n49 0 1 -0,0082 1,00847 0 1 \n\n \n\n5.3 Implementa\u00e7\u00e3o do Sistema de Infer\u00eancia Fuzzy \n\nForam implementados dois sistemas FIS resolver o problema de classifica\u00e7\u00e3o do \n\npoder calor\u00edfico do g\u00e1s combust\u00edvel, sendo o primeiro utilizando dados sem Pr\u00e9-\n\nProcessamento de dados e An\u00e1lise de Componentes Principais, e o segundo utilizando \n\nessas duas ferramentas.  \n\n\n\n51 \n\n \n\nA abordagem utilizada para implementar o sistema FIS foi o uso da \n\nclusteriza\u00e7\u00e3o subtrativa, m\u00e9todo ilustrado na Fig. 5.3a abaixo. Essa figura mostra os \n\ndados de entrada e as fun\u00e7\u00f5es de pertin\u00eancia ajustadas para as duas dimens\u00f5es dos \n\ndados de entrada. A partir do conhecimento pr\u00e9vio de qual padr\u00e3o pertence cada cluster \n\nde dados e tamb\u00e9m das fun\u00e7\u00f5es de pertin\u00eancia, \u00e9 gerada a base de conhecimento do \n\nsistema FIS. Maiores detalhes da implementa\u00e7\u00e3o desta abordagem est\u00e3o descritos nas \n\nse\u00e7\u00f5es seguintes. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3a: Clusteriza\u00e7\u00e3o Subtrativa utilizada no FIS \n\n5.3.1 FIS com os dados sem pr\u00e9-processamento e PCA \n\nAs bases de conhecimento dos FIS foram extra\u00eddas a partir dos pr\u00f3prios dados \n\nsint\u00e9ticos. Segundo [34], as regras fuzzy s\u00e3o extra\u00eddas das estimativas de clusters nos \n\ndados; sendo que cada cluster representa uma regra que relaciona uma regi\u00e3o do espa\u00e7o \n\nde entrada a uma classe de sa\u00edda. A extra\u00e7\u00e3o das regras fuzzy foi feita atrav\u00e9s da \n\nclusteriza\u00e7\u00e3o subtrativa, e com isso foram obtidos centros de cluster para cada padr\u00e3o e \n\nsensor. Os atributos analisados foram a resist\u00eancia inicial dos sensores e a varia\u00e7\u00e3o \n\nnormalizada dos mesmos. A Fig. 5.3.1a mostra os dados sint\u00e9ticos usados como \n\nentradas do sistema FIS. Os centros dos clusters est\u00e3o assinalados com s\u00edmbolos negros. \n\nOs pontos coloridos s\u00e3o os padr\u00f5es de combust\u00edvel simulados, sendo vermelho o padr\u00e3o \n\n\n\n52 \n\n \n\ncom maior poder calor\u00edfico, azul o padr\u00e3o com menor poder calor\u00edfico, e o verde o \n\npadr\u00e3o de combust\u00edvel com poder calor\u00edfico interme di\u00e1rio. \n\nPara a extra\u00e7\u00e3o destes clusters foram usadas as mesmas amostras de treino das \n\nRedes Neurais Artificiais. Ap\u00f3s a constru\u00e7\u00e3o e ajuste da base de conhecimento, as \n\namostras de teste foram usadas no FIS para testar sua capacidade de generaliza\u00e7\u00e3o. Os \n\nresultados e metodologias est\u00e3o detalhados nas se\u00e7\u00f5es posteriores. O sistema Fuzzy \n\nimplementado est\u00e1 mostrado na Fig. 5.3.1b. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1a: Dados sint\u00e9ticos de cada sensor usados no FIS. \n\n1 \n\n2 \n\n3 \n\n4 \n\n5 \n\n6 \n\n\n\n53 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1b: Sistema Fuzzy com os dados sem pr\u00e9-processamento e PCA \n\n \n\n5.3.1.1 Modelamento das Vari\u00e1veis Ling\u00fc\u00edsticas \n\nAs vari\u00e1veis ling\u00fc\u00edsticas usadas para cada sensor foram a Resist\u00eancia Inicial de \n\ncada sensor, e a sua resposta \u00e0 inje\u00e7\u00e3o do g\u00e1s representado pela Resist\u00eancia \n\nNormalizada. As fun\u00e7\u00f5es de pertin\u00eancia foram ajustadas manualmente at\u00e9 obter um \n\ndesempenho satisfat\u00f3rio.  \n\nEntrada \u2013 Resist\u00eancia Inicial \n\nA Fig. 5.3.1.1a mostra as fun\u00e7\u00f5es de pertin\u00eancia relacionada com a vari\u00e1vel \n\nling\u00fc\u00edstica Resist\u00eancia Inicial. Esta vari\u00e1vel representa a Resist\u00eancia Inicial do Sensor \n\nno instante anterior \u00e0 inje\u00e7\u00e3o do g\u00e1s combust\u00edvel na c\u00e2mara com os sensores de g\u00e1s. \n\nEsta fun\u00e7\u00e3o foi usada para todos os sensores utilizados. \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: Resist\u00eancia Inicial \n\n\u2022 Universo de Discurso: 0 a 50 Ohms (valores normalizados para o intervalo \n\n[0;1]) \n\n\u2022 Valores Ling\u00fc\u00edsticos: baixo, m\u00e9dio e alto \n\n\n\n54 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1.1a: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Resist\u00eancia Inicial \n\n \n\nEntrada \u2013 Resist\u00eancia Normalizada para o Sensor 1 a 6 \n\nA Fig. 5.3.1.1b mostra as fun\u00e7\u00f5es de pertin\u00eancia relacionadas com as vari\u00e1veis \n\nling\u00fc\u00edsticas Resist\u00eancia Normalizada para os sensores 1 a 6. Esta vari\u00e1vel representa o \n\nresultado do c\u00e1lculo relacionado com as resist\u00eancias final e inicial dos sensores em \n\nregime permanente. O c\u00e1lculo \u00e9 feito atrav\u00e9s da equa\u00e7\u00e3o (4a). Estas fun\u00e7\u00f5es de \n\npertin\u00eancia foram ajustadas manualmente. \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: Resist\u00eancia Normalizada  \n\n\u2022 Universo de Discurso: 0 a 1 \n\n\u2022 Valores Ling\u00fc\u00edsticos: baixo, m\u00e9dio e alto \n\n \n\nSa\u00eddas  \n\nA Fig. 5.3.1.1c mostra as fun\u00e7\u00f5es de pertin\u00eancia relacionadas com a vari\u00e1vel de \n\nsa\u00edda de cada sistema para cada sensor.  \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: Sa\u00edda  \n\n\u2022 Universo de Discurso: 0 a 1. \n\n\u2022 Valores Ling\u00fc\u00edsticos: Classe1, Classe2 e Classe3 \n\n\n\n55 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1.1b: Fun\u00e7\u00f5es de Pertin\u00eancia ajustadas para cada sensor. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1.1c: Fun\u00e7\u00f5es de pertin\u00eancia da sa\u00edda do FIS. \n\n \n\n5.3.1.2 Base de Conhecimento do FIS \n\nAs regras utilizadas para o reconhecimento dos padr\u00f5es foram baseadas nos \n\ndados de treino, e nos clusters obtidos. Para todos os sensores as regras foram as \n\nmesmas. Por exemplo, para o sensor n\u00famero 5, as regras foram: \n\nensor 1 \n\nensor 2 \n\nensor 3 \n\nensor 4 \n\nensor 5 \n\nensor 6 \n\n\n\n56 \n\n \n\n1.  SE (Ri5 \u00e9 baixo) E (Atr5 \u00e9 baixo) ENT\u00c3O (output5 \u00e9 classe2)  \n\n2.  SE (Ri5 \u00e9 m\u00e9dio) E (Atr5 \u00e9 baixo) ENT\u00c3O (output5 \u00e9 classe2) \n\n3.  SE (Ri5 \u00e9 alto) E (Atr5 \u00e9 baixo) ENT\u00c3O (output5 \u00e9 classe2) \n\n4.  SE (Ri5 \u00e9 baixo) E (Atr5 \u00e9 m\u00e9dio) ENT\u00c3O (output5 \u00e9 classe3) \n\n5.  SE (Ri5 \u00e9 m\u00e9dio) E (Atr5 \u00e9 m\u00e9dio) ENT\u00c3O (output5 \u00e9 classe3) \n\n6.  SE (Ri5 \u00e9 alto) E (Atr5 \u00e9 m\u00e9dio) ENT\u00c3O (output5 \u00e9 classe3) \n\n7.  SE (Ri5 \u00e9 baixo) E (Atr5 \u00e9 alto) ENT\u00c3O (output5 \u00e9 classe1) \n\n8.  SE (Ri5 \u00e9 m\u00e9dio) E (Atr5 \u00e9 alto) ENT\u00c3O (output5 \u00e9 classe1) \n\n9.  SE (Ri5 \u00e9 alto) E (Atr5 \u00e9 alto) ENT\u00c3O (output5 \u00e9 classe1) \n\n \n\n5.3.1.3 Metodologia utilizada para realizar os tratamentos dos dados  \n\nPara criar o ambiente necess\u00e1rio para realizar os tratamentos de dados no \n\nSistema de Infer\u00eancia Fuzzy foi feito o seguinte procedimento: \n\n1) Criar as amostras de treino e de teste atrav\u00e9s da execu\u00e7\u00e3o arquivo \u201cAmSint1.m\u201d. \n\nEste arquivo criar\u00e1 amostras sint\u00e9ticas relacionadas com o g\u00e1s combust\u00edvel. Ele \n\ngerar\u00e1 os arquivos com as 300 amostras sint\u00e9ticas que ser\u00e3o usados tanto na Rede \n\nNeural Artificial quanto no Sistema de Infer\u00eancia Fuzzy (FIS).  \n\n2) Para modela r o FIS \n\nO arquivo utilizado do FIS foi fzrulext__1.mdl, feito atrav\u00e9s do Simulink (ver \n\nFig. 5.3.1b). Neste arquivo foram modeladas todas as fun\u00e7\u00f5es de pertin\u00eancia de entrada \n\ne sa\u00edda referentes a todos os componentes principais, al\u00e9m das regras fuzzy usados no \n\nFIS, descritos na se\u00e7\u00e3o anterior.  \n\n3) Executar o arquivo \u201cfre.m\u201d (extra\u00e7\u00e3o de regras fuzzy)  \n\nEste arquivo criar\u00e1 todas as vari\u00e1veis de entrada do FIS, usando os arquivos de \n\ntreino e teste obtidos anteriormente. Ele tamb\u00e9m inserir\u00e1 os FIS dos 6 sensores na \u00e1rea \n\nde trabalho do Matlab. Atribuir valor zero \u00e0 vari\u00e1vel PreProcessamento. \n\n4) Executar a simula\u00e7\u00e3o do arquivo do Simulink fzrulext__1.mdl. \n\nA execu\u00e7\u00e3o do FIS no Simulink criar\u00e1 os arquivos 'resultadosensor(i)' sendo i, a \n\ndenomina\u00e7\u00e3o dos sensores de 1 a 6.  \n\n5) Executar o arquivo res_g_fis (resultado global do fis) \n\n\n\n57 \n\n \n\nEste arquivo criar\u00e1 os resultados globais obtidos dos 6 FIS criados \n\nseparadamente. Os pontos resultantes foram colocados no gr\u00e1fico da Fig. 5.3.1.4b. \n\n \n\n5.3.1.4 Resultados Obtidos com o Sistema de Infer\u00eancia Fuzzy \n\nOs resultados do FIS sem otimiza\u00e7\u00f5es nas fun\u00e7\u00f5es de pertin\u00eancia n\u00e3o foram \n\nmuito satisfat\u00f3rios. As 150 amostras foram submetidas ao FIS seq\u00fcencialmente, um a \n\ncada segundo, sendo as primeiras 50 primeiras correspondentes ao primeiro padr\u00e3o, as \n\n50 subseq\u00fcentes pertencentes ao padr\u00e3o 3, e as \u00faltimas 50 correspondentes ao padr\u00e3o 2. \n\nForam estabelecidos limites para as sa\u00eddas do FIS: valores de 0 a 0,25 reconhece o \n\npadr\u00e3o 1; valores de 0,25 a 0,75 reconhece o padr\u00e3o 3, e valores de 0,75 a 1,0 reconhece \n\no padr\u00e3o 2.  \n\nAs curvas de resposta de cada subsistema FIS para cada sensor est\u00e3o mostradas \n\nna Fig. 5.3.1.4a. Pode-se observar que somente o sensor 2 teve um resultado \n\nsatisfat\u00f3rio. Isso porque os padr\u00f5es estavam bem definidos e sem sobreposi\u00e7\u00f5es. Pode -\n\nse notar que houve muitos erros no reconhecimento do padr\u00e3o 3, mesmo com o ajuste \n\nmanual das fun\u00e7\u00f5es de pertin\u00eancia. Isto porque os  padr\u00f5es observados nos outros \n\nsensores ficaram sobrepostos, como se pode observar na Fig. 5.3.1a. \n\nNa Fig. 5.3.1.4b pode ser observada a resposta global do sistema fuzzy, isto \u00e9, o \n\nresultado da vota\u00e7\u00e3o de todos os sistemas fuzzy implementados. Os valores de sa\u00edda \n\ncorrespondem \u00e0 classe reconhecida, por exemplo, o valor 1 corresponde \u00e0 classe 1, 2 \n\ncorresponde \u00e0 classe 2 e o valor 3 corresponde \u00e0 classe 3. O valor 4 \u00e9 obtid o quando a \n\nvota\u00e7\u00e3o das tr\u00eas classes obtiveram o mesmo n\u00famero de votos. S\u00e3o, portanto, os casos \n\nem que houve as maiores d\u00favidas. Valores intermedi\u00e1rios de 1,5 e 2,5 representam que \n\no sistema ficou com resposta duvidosa entre os padr\u00f5es 1 e 2, e os padr\u00f5es 2 e 3 \n\nrespectivamente. \n\n\n\n58 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1.4a: Resultados Obtidos dos FIS para cada sensor. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.1.4b: Resultado final da vota\u00e7\u00e3o obtida para a resposta global do FIS. \n\n \n\nResposta Sensor 1 \n\nResposta Sensor 2 \n\nResposta Sensor 3 \n\nResposta Sensor 4 \n\nResposta Sensor 5 \n\nResposta Sensor 6 \n\n\n\n59 \n\n \n\nHouve 74% de acertos de reconhecimento dentre as amostras de teste. Pode ser \n\nobservado que o FIS n\u00e3o tem uma boa performance de reconhecimento quando os \n\npadr\u00f5es a serem reconhecidos est\u00e3o sobrepostos. A implementa\u00e7\u00e3o do FIS tamb\u00e9m \u00e9 \n\ntrabalhosa, pois \u00e9 necess\u00e1rio realizar v\u00e1rios ajustes nas fun\u00e7\u00f5es de pertin\u00eancia para as \n\nentradas e sa\u00eddas at\u00e9 que se pudesse obter um resultado satisfat\u00f3rio. \n\nUma etapa de pr\u00e9-processamento de dados e an\u00e1lise de componentes principais \n\nnos dados de entrada de treino e de teste pode ser usada para melhorar a taxa de acertos \n\ndo sistema de infer\u00eancia fuzzy utilizado. E esta abordagem foi implementada na se\u00e7\u00e3o \n\nseguinte. \n\n \n\n5.3.2 FIS utilizando dados com pr\u00e9-processamento e PCA \n\nOs dados sint\u00e9ticos foram os mesmos usados na se\u00e7\u00e3o anterior. Por\u00e9m eles \n\nforam submetidos a um pr\u00e9-processamento e an\u00e1lise de componentes principais. Para \n\nisto, os dados de treino foram normalizados de tal modo que eles ficassem no intervalo \n\n[-1;1] atrav\u00e9s da fun\u00e7\u00e3o do toolbox do programa Matlab premnmx. \n\nEm seguida, foi usado o comando prepca para que somente os componentes dos \n\ndados com a fra\u00e7\u00e3o m\u00ednima de vari\u00e2ncia de valor 0,07 com rela\u00e7\u00e3o a varia\u00e7\u00e3o total do \n\nconjunto de dados, fosse inclu\u00eddo no conjunto de dados transformado. Em seguida, foi \n\nutilizado o comando subclust para extrair os clusters subtrativos dos dados j\u00e1 \n\ntransformados. O conjunto de dados de treino foi reduzido de 6 atributos para somente \n\ndois atributos, que foram colocados num gr\u00e1fico atributo1 \u201cversus\u201d atributo2 mostrado \n\nna Fig. 5.3.2a. Os dados em preto representam o primeiro padr\u00e3o de g\u00e1s GLP com o \n\nmaior poder calor\u00edfico. Os dados em vermelho representam o terceiro padr\u00e3o de GLP \n\ncom o menor poder calor\u00edfico, e os dados em azul representam o segundo padr\u00e3o de \n\nGLP com um valor intermedi\u00e1rio de poder calor\u00edfico. Os dados de cor verde \n\nrepresentam os clusters obtidos pelo processo de clusteriza\u00e7\u00e3o subtrativa. Pode-se notar \n\nque as classes 2 e 3 foram invertidas com rela\u00e7\u00e3o ao experimento anterior, mas isso n\u00e3o \n\nmudou a metodologia, e o tratamento dos dados e resultados experimentais n\u00e3o ficaram \n\ncomprometidos.  \n\nAp\u00f3s a constru\u00e7\u00e3o e ajuste da base de conhecimento, as amostras de teste foram \n\ntransformadas utilizando as mesmas transforma\u00e7\u00f5es sofridas pelo conjunto de treino. E \n\nos dados resultantes de treino foram usados no FIS para testar sua capacidade de \n\ngeneraliza\u00e7\u00e3o. Os dados de teste resultantes est\u00e3o mostrados na Fig. 5.3.2b. Os \n\n\n\n60 \n\n \n\nresultados e metodologias est\u00e3o detalhados nas se\u00e7\u00f5es posteriores. O sistema Fuzzy est\u00e1 \n\nmostrado na Fig. 5.3.2c. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2a: Gr\u00e1fico dos dois Componentes Principais com os Clusters dos dados de \n\ntreino \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2b: Dados sint\u00e9ticos de teste usados no FIS. \n\n\n\n61 \n\n \n\n \n\nFig. 5.3.2c: Sistema FIS utilizado com os dados de treino submetidos ao pr\u00e9-\n\nprocessados e an\u00e1lise de componentes principais \n\n \n\n5.3.2.1 Modelamento das Vari\u00e1veis Ling\u00fc\u00edsticas \n\nAs vari\u00e1veis ling\u00fc\u00edsticas usadas cada sensor foram os dois componentes \n\nprincipais dos dados de treino, denominados PrinComp1 e PrinComp2 \n\nrespectivamente. As fun\u00e7\u00f5es de pertin\u00eancia foram ajustadas manualmente at\u00e9 obter um \n\ndesempenho satisfat\u00f3rio.  \n\nEntrada \u2013 Componente Principal 1 \n\nA Fig. 5.3.2.1a mostra as fun\u00e7\u00f5es de pertin\u00eancia relacionada com a vari\u00e1vel \n\nling\u00fc\u00edstica PrinComp1. Esta vari\u00e1vel representa o primeiro componente principal da \n\nan\u00e1lise feita nos dados de treino.  \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: PrinComp1 \n\n\u2022 Universo de Discurso: -2 a 2 \n\n\u2022 Valores Ling\u00fc\u00edsticos: baixo1, m\u00e9dio1 e alto1 \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2.1a: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Ling\u00fc\u00edstica PrinComp1 \n\n \n\n\n\n62 \n\n \n\nEntrada \u2013 Componente Principal 2 \n\nA Fig. 5.3.2.1b mostra a fun\u00e7\u00e3o de pertin\u00eancia relacionadas com a vari\u00e1vel \n\nling\u00fc\u00edstica PrinComp2 para o segundo componente principal dos dados de treino. Esta \n\nvari\u00e1vel representa o resultado do c\u00e1lculo relacionado com as resist\u00eancias inicial e final \n\ndos sensores em regime permanente. O c\u00e1lculo \u00e9 feito atrav\u00e9s da equa\u00e7\u00e3o (4a). Estas \n\nfun\u00e7\u00f5es de pertin\u00eancia foram ajustadas manualmente.  \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: Resist\u00eancia Normalizada  \n\n\u2022 Universo de Discurso: 0 a 1 \n\n\u2022 Valores Ling\u00fc\u00edsticos: baixo2, m\u00e9dio2 e alto2 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2.1b: Fun\u00e7\u00e3o de Pertin\u00eancia da Vari\u00e1vel Ling\u00fc\u00edstica PrinComp2 \n\n \n\nSa\u00edda \u2013  Separa\u00e7\u00e3o das Classes \n\nA Fig. 5.3.2.1c mostra as fun\u00e7\u00f5es de pertin\u00eancia relacionadas com a vari\u00e1vel de \n\nsa\u00edda de cada sistema para cada sensor.  \n\n\u2022 Vari\u00e1vel Ling\u00fc\u00edstica: Sa\u00edda  \n\n\u2022 Universo de Discurso: 0 a 1. \n\n\u2022 Valores Ling\u00fc\u00edsticos: Classe1, Classe2 e Classe3 \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2.1c: Fun\u00e7\u00f5es de pertin\u00eancia da sa\u00edda do FIS. \n\n \n\n\n\n63 \n\n \n\nBase de Conhecimento do FIS \n\nAs regras utilizadas para o reconhecimento dos padr\u00f5es foram baseadas nos \n\ndados de treino, e nos clusters obtidos. As regras utilizadas foram: \n\n \n\n1.  SE (PrincComp1 \u00e9 baixo1) E (PrinComp2 \u00e9 baixo2) ENT\u00c3O (output1 \u00e9 classe1) \n\n2.  SE (PrincComp1 \u00e9 baixo1) E (PrinComp2 \u00e9 m\u00e9dio2) ENT\u00c3O (output1 \u00e9 classe1) \n\n3.  SE (PrincComp1 \u00e9 baixo1) E (PrinComp2 \u00e9 alto2) ENT\u00c3O (output1 \u00e9 classe1)  \n\n4.  SE (PrincComp1 \u00e9 m\u00e9dio1) E (PrinComp2 \u00e9 baixo2) ENT\u00c3O (output1 \u00e9 classe2)  \n\n5.  SE (PrincComp1 \u00e9 m\u00e9dio1) E (PrinComp2 \u00e9 m\u00e9dio2) ENT\u00c3O (output1 \u00e9 classe2)  \n\n6.  SE (PrincComp1 \u00e9 me dio1) E (PrinComp2 \u00e9 alto2) ENT\u00c3O (output1 \u00e9 classe2)  \n\n7.  SE (PrincComp1 \u00e9 alto1) E (PrinComp2 \u00e9 baixo2) ENT\u00c3O (output1 \u00e9 classe3)  \n\n8.  SE (PrincComp1 \u00e9 alto1) E (PrinComp2 \u00e9 m\u00e9dio2) ENT\u00c3O (output1 \u00e9 classe3)  \n\n9.  SE (PrincComp1 \u00e9 alto1) E (PrinComp2 \u00e9 alto2) ENT\u00c3O (output1 \u00e9 classe3) \n\n \n\n5.3.2.2 Metodologia utilizada para realizar os tratamentos dos dados \n\nPara criar o ambiente necess\u00e1rio para realizar os tratamentos de dados no \n\nSistema de Infer\u00eancia Fuzzy foi feito o seguinte procedimento: \n\n1) Criar as amostras de treino e de teste atrav\u00e9s do arquivo \u201cAmSint1.m\u201d. \n\nEste arquivo criar\u00e1 amostras sint\u00e9ticas relacionadas com o g\u00e1s combust\u00edvel. Ele \n\ngerar\u00e1 os arquivos \u201ctreino.txt\u201d e \u201cteste.txt\u201d que ser\u00e3o usados tanto na Rede Neural \n\nArtificial quanto no Sistema de Infer\u00eancia Fuzzy (FIS). \n\n2) Para modelar o FIS \n\nO arquivo utilizado do FIS foi FreFis.mdl, feito atrav\u00e9s do Simulink (ver Fig. \n\n5.3.2c). Neste arquivo foram modeladas todas as fun\u00e7\u00f5es de pertin\u00eancia de entrada e \n\nsa\u00edda referentes a todos os sensores, al\u00e9m das regras fuzzy usados no FIS, descritos na \n\nse\u00e7\u00e3o anterior. \n\n3) Executar o arquivo fre.m (extra\u00e7\u00e3o de regras fuzzy vers\u00e3o 1.0)  \n\nEste arquivo criar\u00e1 todas as vari\u00e1veis de entrada do FIS, usando os arquivos de \n\ntreino e teste obtidos anteriormente. Ele tamb\u00e9m inserir\u00e1 o FIS correspondente aos \n\ndados com pr\u00e9-processamento e an\u00e1lise de componentes principais na \u00e1rea de trabalho \n\ndo Matlab. Atribuir valor um \u00e0 vari\u00e1vel PreProcessamento. \n\n\n\n64 \n\n \n\n4) Executar o modelo do Simulink FreFis.mdl \n\nA execu\u00e7\u00e3o do FIS no Simulink criar\u00e1 a vari\u00e1vel ResultadoFrePCA na \u00e1rea de \n\ntrabalho no Matlab.  \n\n5) Analisar o resultado obtido no passo anterior  \n\nForam contadas as amostras em que o reconhecimento foi err\u00f4neo, atrav\u00e9s da \n\nan\u00e1lise da Fig. 5.3.2.3a da se\u00e7\u00e3o seguinte. \n\n \n\n5.3.2.3 Resultados Obtidos com o Sistema de Infer\u00eancia Fuzzy \n\nComo foi obtido somente um controlador Fuzzy, n\u00e3o foram necess\u00e1rias as etapas \n\nde combina\u00e7\u00e3o e vota\u00e7\u00e3o dos v\u00e1rios sistemas FIS, e com isso, foi obtido somente uma \n\ncurva de resposta do FIS que est\u00e1 mostrada na Fig. 5.3.2.3a. Para obter esta curva de \n\nresposta, foram utilizadas as mesmas amostras de teste da se\u00e7\u00e3o anterior, para \n\ncompara\u00e7\u00e3o, e tamb\u00e9m para avaliar a capacidade de generaliza\u00e7\u00e3o do sistema FIS. \n\nPode-se observar que houve uma melhora de reconhecimento com rela\u00e7\u00e3o ao sistema \n\nFIS implementado sem o pr\u00e9-processamento e an\u00e1lise de componentes principais, \n\nobtidas na se\u00e7\u00e3o 5.3.1 desta Disserta\u00e7\u00e3o.  \n\nOs dados foram agrupados do seguinte modo: as amostras de n\u00famero 1 a 50 \n\npertencem ao padr\u00e3o 1, as amostras de n\u00famero 51 a 100 pertencem ao padr\u00e3o 3, e \n\nfinalmente as amostras de n\u00famero 101 a 150 pertencem ao padr\u00e3o 2. Por causa das \n\ncondi\u00e7\u00f5es inicia is impostas pelo aplicativo Simulink do programa Matlab, a amostra de \n\nn\u00famero 0 n\u00e3o foi considerado no c\u00e1lculo das taxas de erros e acertos do \n\nreconhecimento. Cada amostra foi inserida no sistema a uma taxa de amostragem de 1s. \n\nA simula\u00e7\u00e3o do programa Matla b foi feita entre os tempos 0 e 155s, portanto, tempos \n\nsuperiores a 151s tamb\u00e9m n\u00e3o foram considerados no c\u00e1lculo. \n\n\n\n65 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.3.2.3a: Resultado do FIS para os dados com pr\u00e9-processamento e an\u00e1lise de \n\ncomponentes principais. \n\n \n\nConsiderando os limiares de 0,3 para o reconhecimento entre os padr\u00f5es 1 e 3; e \n\n0,7 para os padr\u00f5es 2 e 3, pode -se observar que houveram 12 erros de reconhecimento \n\nem um universo de 150 amostras. Portanto, foi obtida uma taxa de acer tos de 92,0% no \n\nreconhecimento das amostras de teste. Pode ser observado que o pr\u00e9-processamento dos \n\ndados e a an\u00e1lise de componentes principais nos dados de entrada fizeram com que o \n\nn\u00famero de amostras dos padr\u00f5es sobrepostas fosse diminu\u00eddo. Isso fez com que o \n\ndesempenho do FIS para a etapa de reconhecimento melhorasse. A implementa\u00e7\u00e3o do \n\nFIS deste item foi menos trabalhosa que a da se\u00e7\u00e3o anterior, pois foram necess\u00e1rios \n\najustes nas fun\u00e7\u00f5es de pertin\u00eancia das entradas e da sa\u00edda de somente um sistema FIS, \n\nat\u00e9 que se pudesse obter um resultado satisfat\u00f3rio. O fato dos dados de entrada estarem \n\nbem separados e limitados tamb\u00e9m facilitou a cria\u00e7\u00e3o das fun\u00e7\u00f5es de pertin\u00eancia.  \n\n \n\n5.4 Reconhecimento do poder calor\u00edfico do GLP em Redes Neurais \n\nForam testadas dois algoritmos de aprendizado de redes neurais \u2013 a \n\nbackpropagation e LVQ \u2013 para reconhecer os padr\u00f5es dos combust\u00edveis gasosos. Para a \n\nrede backpropagation, foram usadas duas fun\u00e7\u00f5es de treinamento, a backpropagation \n\ncom o gradiente decrescente com momentum adaptativo (fun\u00e7\u00e3o traingdx do Matlab) e \n\n\n\n66 \n\n \n\na fun\u00e7\u00e3o backpropagation Levenberg-Marquardt (trainlm do Matlab), seja com ou sem a \n\nan\u00e1lise de componentes principais. Sendo que neste \u00faltimo caso, a rede neural teve um \n\nn\u00famero menor de entradas, em conseq\u00fc\u00eancia da redu\u00e7\u00e3o de dimensionalidade. \n\nOs dados de entrada utilizados para o treino e teste foram os dados sint\u00e9ticos \n\nobtidos segundo o procedimento descrito no item 4.2. Os 300 dados obtidos foram \n\ndivididos em dois conjuntos de 150 amostras de treino e 150 amostras teste. Cada \n\nconjunto tinha 50 amostras para cada um dos tr\u00eas padr\u00f5es de g\u00e1s GLP. \n\nAs redes neurais backpropagation sem ACP tiveram 6 neur\u00f4nios na camada de \n\nentrada, 6 neur\u00f4nios na camada escondida, e tr\u00eas neur\u00f4nios na camada de sa\u00edda. J\u00e1 as \n\nredes neurais com ACP tiveram 4 entradas, 4 neur\u00f4nios na camada escondida e tr\u00eas \n\nneur\u00f4nios na camada de sa\u00edda. \n\nPara cada rede, foram feitos 10 treinamentos. Os par\u00e2metros para a parada do \n\ntreinamento foi o erro RMS de 0.01, e o gradiente do erro de 0.001. Os resultados \n\nobtidos com as m\u00e9dias e vari\u00e2ncias das redes est\u00e3o demonstrados na Tabela 5.4a a \n\nseguir. \n\nO n\u00famero de acertos foi obtido com testes das redes neurais usando as amostras \n\nde teste. Pode-se notar que o desempenho da rede neural backpropagation usando a \n\nfun\u00e7\u00e3o \u2018traingdx\u2019 teve um maior \u00edndice de acertos, com menor desvio padr\u00e3o. A rede \n\nbackpropagation \u2018trainlm\u2019, apesar de ter o maior \u00edndice de acertos com muitas poucas \n\n\u00e9pocas de treina mento, houve muitos casos com poucos acertos que n\u00e3o foram \n\ncomputados nesta tabela. O treinamento da rede LVQ foi limitado a 200 \u00e9pocas, porque \n\nfoi observado que o erro RMS oscilava ap\u00f3s a \u00e9poca 75. Portanto a rede neural treinada \n\nno Matlab eleita foi a ba ckpropagation com a fun\u00e7\u00e3o de treinamento \u2018traingdx\u2019 sem pr\u00e9-\n\nprocessamento. \n\nTabela 5.4a: Treinamentos das RNA\u2019s com Matlab \n\n Com An\u00e1lise de Componenetes Principais  Sem An\u00e1lise de Componentes Principais \n\nTraingdx Trainlm Learnlvq Traingdx  Trainlm Learnlvq \n \n\n\u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \u00c9pocas Acertos \n\nM\u00e1x. 633 140 1028 142 200 134 915 144 139 148 200 135 \n\nM\u00edn. 262 135 25 129 200 134 285 128 14 138 200 134 \n\nM\u00e9dia 389.45 137.94 117.34 137.28 200 134 444.49 139.97 45.31 143.17 200 134 \n\nDesvio \nPadr\u00e3o \n\n87.64 1.25 148.96 2.90 0 0 116.52 2.85 31.64 2.35  0 134.7 \n\n \n\n\n\n67 \n\n \n\nOutro aplicativo utilizado para o treinamento da rede neural foi o NeuralWorks. \n\nOs mesmos conjuntos de treino e de teste da rede foram submetidos ao programa. A \n\nrede neural teve seis neur\u00f4nios na camada de entrada e na camada escondida, e mais tr\u00eas \n\nneur\u00f4nios na camada de sa\u00edda. Foram feitos dez treinamentos para a rede neural MLP \n\ncom o algoritmo de aprendizado backpropagation sem a tabela \u201cMinMax\u201d, mais dez \n\ntreinamentos com essa tabela. Segundo [20], a tabela \u201cMinMax\u201d armazena m\u00e1ximos e \n\nm\u00ednimos de cada dado de entrada. Ela \u00e9 usada para realizar um pr\u00e9-processamento que \n\naplica uma escala nos dados de entrada. Assim, ela previne a satura\u00e7\u00e3o da fun\u00e7\u00e3o de \n\nativa\u00e7\u00e3o que faz com que o neur\u00f4nio pare de aprender. Os crit\u00e9rios de parada do \n\ntreinamento foram a m\u00e1xima itera\u00e7\u00e3o de 50000 \u00e9pocas, ou o erro RMS menor que \n\n0.001. O m\u00e1ximo n\u00famero de itera\u00e7\u00f5es foi fixado para que a rede n\u00e3o sofresse \n\n\u201coverfitting\u201d.  \n\nNeste mesmo aplicativo foi treinada uma rede LVQ. A regra de treinamento para \n\nas 4500 primeiras itera\u00e7\u00f5es usou um fator de consci\u00eancia de 1.0, o qual encorajou todos \n\nos neur\u00f4nios no aprendizado. Para outras 2250 itera\u00e7\u00f5es, a regra de treinamento fez um \n\nrefinamento de limites entre as classes. Os resultados dos treinamentos da rede neural \n\ncom o aplicativo NeuralWorks est\u00e3o demonstrados na Tabela 5.4b. A an\u00e1lise de \n\ncomponentes principais n\u00e3o foi utilizado neste programa. \n\nOs resultados de generaliza\u00e7\u00e3o da rede neural treinada pelo NeuralWorks foram \n\nmelhores. Portanto, a rede escolhida para implementar a rede neural em hardware foi a \n\ncriada pelo programa NeuralWorks com a tabela MinMax. Uma  poss\u00edvel \n\nimplementa\u00e7\u00e3o da Rede Neural para o reconhecimento de g\u00e1s GLP no hardware do DSP \n\nseria muito parecida com o apresentado no item 5.2. \n\nTabela 5.4b: Treinamento das RNA\u2019s com Neural Works \n\nRede Backpropagation Sem Tabela \n\u201cMinMax\u201d \n\nRede Backpropagation Com Tabela \n\u201cMinMax\u201d \n\nRede LVQ \n \n\n\u00c9pocas Acertos  \u00c9pocas Acertos \u00c9pocas Acertos \n\nM\u00e1ximo 50000 142 50000 147 6750 134 \n\nM\u00ednimo 50000 141 50000 145 2545 135 \n\nM\u00e9dia 50000 141 .6 50000 146.1 5711.6 134 \n\nDesvio Padr\u00e3o 0 0.52 0 0.74 1565.5 134.7 \n\n \n\nPor\u00e9m, neste trabalho foram utilizadas amostras de gases em dois padr\u00f5es. O \n\nprimeiro padr\u00e3o de g\u00e1s GLP foi a inje\u00e7\u00e3o de 200ml de g\u00e1s na c\u00e2mara de sensores. J\u00e1 o \n\nsegundo padr\u00e3o de g\u00e1s GLP  foi injetado 1000ml de uma mistura de GLP com g\u00e1s \n\n\n\n68 \n\n \n\nNitrog\u00eanio, numa propor\u00e7\u00e3o de 1:5. Essa mistura foi obtida por meio do ajuste dos \n\nfluxos dos gases GLP em 200sccm e Nitrog\u00eanio em 1000sccm. A metodologia adotada \n\npara a extra\u00e7\u00e3o dessas amostras de gases j\u00e1  foram discutidas nos itens 4.2 e 4.3.2 deste \n\ntrabalho.  \n\nForam obtidas 36 amostras da inje\u00e7\u00e3o de 200ml do g\u00e1s GLP e 43 amostras da \n\ninje\u00e7\u00e3o da mistura de 200sccm de GLP com 1000sccm de g\u00e1s Nitrog\u00eanio. O tratamento \n\ndesses dados obtido experimentalmente est\u00e1 descrito nos itens posteriores. Os dados \n\nmedidos para cada sensor est\u00e3o mostrados na Fig. 5.4b. \n\n \n\n5.4.1 Tratamento dos dados experimentais do GLP \n\nO tratamento dos dados experimentais foi feito no programa MATLAB. Foram \n\nfeitas medidas com o g\u00e1s GLP puro e a mistura de g\u00e1s GLP e Nitrog\u00eanio. Sendo que \n\nesta \u00faltima estava simulando um tipo de combust\u00edvel com menor poder calor\u00edfico que o \n\nprimeiro. O volume injetado do g\u00e1s GLP puro foi de 0.2ml, enquanto que o volume \n\ninjetado da mistura foi de 1ml. Essa mistura foi obtida com a inje\u00e7\u00e3o de um fluxo de \n\n200sccm de g\u00e1s GLP para 1000sccm de Nitrog\u00eanio. Ou seja, a propor\u00e7\u00e3o da mistura foi \n\nde  1 parte de GLP para 5 partes de g\u00e1s Nitrog\u00eanio. \n\nOs valores iniciais e finais de cada medida foram submetidos a um programa \n\nfeito em Matlab (arquivo \u201camostras_1.m\u201d), destinado a tratar os dados e realizar o \n\nreconhecimento do poder calor\u00edfico de cada padr\u00e3o de g\u00e1s combust\u00edvel.  \n\n\n\n69 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.4b: Amostras do G\u00e1s GLP e da mistura GLP com Nitrog\u00eanio.  \n\nOs atributos correspondentes \u00e0 resist\u00eancia normalizada dos sensores foram \n\ncalculados segundo a equa\u00e7\u00e3o (4a). O tratamento dos dados foi feito segundo a \n\nexecu\u00e7\u00e3o do arquivo em Matlab \u201cExtAtributos1.m\u201d. As amostras de cada padr\u00e3o foram \n\ndivididas em dois conjuntos aproximadamente iguais, sendo um destinado ao treino do \n\nsistema reconhecedor, e o outro foi destinado ao teste. Como o n\u00famero de amostras era \n\nreduzido, foi optado n\u00e3o criar um novo conjunto de valida\u00e7\u00e3o, para que o treinamento \n\n\n\n70 \n\n \n\nn\u00e3o fosse comprometido com poucos exemplos a serem aprendidos. Isto \u00e9, os dados \n\nforam divididos em somente dois subconjuntos, um de treino e outro de teste. \n\nOs dados extra\u00eddos foram submetidos a sistemas de reconhecimento baseados \n\nem redes neurais artificiais e em sistemas de infer\u00eancia fuzzy, sendo este \u00faltimo \n\ndetalhado no cap\u00edtulo 5.3. A primeira abordagem foi o treinamento da rede neural e o \n\najuste de um sistema de infer\u00eancia fuzzy com os dados obtidos com todos os sensores \n\nfuncionando corretamente.  \n\nA segunda abordagem adotada foi a simula\u00e7\u00e3o de sensores com falha nos dados \n\nmedidos. Foi observado empiricamente que um sensor com falha n\u00e3o tem sensibilidade \n\nalguma quando exposto ao g\u00e1s a ser medido. Como os valores iniciais e finais n\u00e3o \n\nmudam, o valor submetido ao sistema reconhecedor ser\u00e1 zero, por causa da \n\nnormaliza\u00e7\u00e3o adotada para a obten\u00e7\u00e3o dos dados medidos. \n\n \n\n5.4.2 Primeiro Experimento: Rede Neural Simples \n\nUma arquitetura de rede neural artificial foi treinada 100 vezes at\u00e9 obter um \n\nmelhor resultado de generaliza\u00e7\u00e3o. Deste modo, foi escolhida uma rede tal que o erro de \n\nreconhecimento das amostras de teste foi o menor de todos. Os dados utilizados para o \n\ntreinamento dessas redes foram os pr\u00f3prios dados medidos experimentalmente, \n\nconsiderando que todos os sensores estavam funcionando corretamente. A rede neural \n\nest\u00e1 mostrada na Fig. 5.4.2a. Um exemplo de curva de erro RMS obtida versus n\u00famero \n\nde itera\u00e7\u00f5es est\u00e1 mostrado na Fig 5.4.2b. \n\nE para isso, foi executada a fun\u00e7\u00e3o RNAMelhorGeneralizacao.m 10 vezes, \n\ncom o n\u00famero de 10 experimentos at\u00e9 encontrar a melhor rede neural com melhor \n\ngeneraliza\u00e7\u00e3o. A sele\u00e7\u00e3o do tipo de tratamento de dados de entrada utilizados (com ou \n\nsem pr\u00e9-processamento e an\u00e1lise de componentes principais) foi selecionado atrav\u00e9s de \n\num par\u00e2metro de entrada na execu\u00e7\u00e3o da fun\u00e7\u00e3o RNAMelhorGeneralizacao.m. \n\nAs caracter\u00edsticas da Rede Neural usadas nesse experimento est\u00e3o descritas na \n\nTabela 5.4.2a: \n\n \n\n\n\n71 \n\n \n\nSem Pr\u00e9-Processamento e An\u00e1lise de Componentes Principais: \n\nTabela 5.4.2a: Propriedades da RNA sem PCA \n\nepochs Goal \u00b5\u00b5  \u00b5\u00b5  dec \u00b5\u00b5  inc max_fail Max \nperf inc \n\nMc min_grad show time \n\n10000 0 0.01 0.7 1.05 5 1.04 0.9 1e-7 25 inf \n\n \nTipo de Rede Fun\u00e7\u00e3o de \n\nTreinamento \nFun\u00e7\u00e3o de \nAdapta\u00e7\u00e3o \n\nFun\u00e7\u00e3o de \nPerformance (erro) \n\nN\u00famero de Camadas \n\nBackpropagation traingdx LEARNGDM Erro m\u00e9dio quadr\u00e1tico 2 \n\n \nPropriedades da camada: N\u00famero de Neur\u00f4nios Transfer Function \n\n1 (entrada) 8 - \n2 (escondido) 10 sigm\u00f3ide logar\u00edtmica \n\n3 (sa\u00edda) 2 Sigm\u00f3ide logar\u00edtmica \n\n \n\n \n\nFig. 5.4.2a: RNA usada no Reconhecimento dos dados experimentais  \n\n \n\nFig. 5.4.2b: Exemplo de curva de erro obtido dos dados experimentais \n\n \n\nPara os dados dos sensores sem pr\u00e9-processamento, os resultados de \n\ngeneraliza\u00e7\u00e3o da melhor rede neural foram de 100% acertos para um determinado \n\nconjunto de amostras de teste. Por\u00e9m, o tempo de treinamento foi elevado, com 10 mil \n\nitera\u00e7\u00f5es para cada rede neural treinada, e o erro foi de pouco mais de 0,01.  \n\n \n\n\n\n72 \n\n \n\nCom Pr\u00e9-Processamento e An\u00e1lise de Componentes Principais: \n\nFoi utilizada a mesma metodologia da se\u00e7\u00e3o anterior, por\u00e9m os dados de treino e \n\nteste foram submetidos a uma an\u00e1lise de componentes principais. A arquitetura de rede \n\nneural artificial foi treinada 10 vezes at\u00e9 obter um melhor resultado de generaliza\u00e7\u00e3o. \n\nOs dados utilizados para o treinamento dessas redes foram os dados medidos \n\nexperimentalmente, considerando que todos os sensores estavam funcionando \n\ncorretamente. A rede neural est\u00e1 mostrada na Fig. 5.4.2c. Um exemplo de curva de erro \n\nRMS obtida versus n\u00famero de itera\u00e7\u00f5es est\u00e1 mostrado na Fig 5.4.2d. \n\nOs par\u00e2metros da rede est\u00e3o mostrados na Tabela 5.4.2b: \n\nTabela 5.4.2b: Par\u00e2metros utilizados na RNA com PCA \n\nepochs goal \u00b5\u00b5  \u00b5\u00b5  dec \u00b5\u00b5  inc max_fail Max \nperf inc \n\nMc min_gra\nd \n\nshow time \n\n10000 0 0.01 0.7 1.05 5 1.04 0.9 1e-20 25 inf \n\n \nTipo de Rede Fun\u00e7\u00e3o de \n\nTreinamento \nFun\u00e7\u00e3o de \nAdapta\u00e7\u00e3o \n\nFun\u00e7\u00e3o de \nPerformance (erro) \n\nN\u00famero de Camadas \n\nbackpropagation traingdx LEARNGDM Erro m\u00e9dio quadr\u00e1tico 2 \n\n \nPropriedades da camada: N\u00famero de Neur\u00f4nios Transfer Function \n\n1 (entrada) 6 - \n2 (escondido) 3 Sigm\u00f3ide logar\u00edtmica \n\n3 (sa\u00edda) 2 Sigm\u00f3ide logar\u00edtmica \n\n \n\n \n\nFig. 5.4.2c: Rede Neural com PCA treinada com os dados experimentais \n\n \n\nFig. 5.4.2d: Exemplo de curva de erro da RNA com dados experimentais \n\n\n\n73 \n\n \n\nSubmetendo os dados a um pr\u00e9-processamento e uma an\u00e1lise de componentes \n\nprincipais, foi observado que a rede tamb\u00e9m reconheceu 100% das amostras de teste. \n\nAl\u00e9m disso, foi observado um tempo de treinamento muito menor do que o caso \n\nanterior: com aproximadamente 1000 itera\u00e7\u00f5es, a rede neural convergia para erros RMS \n\nmenores que 10-20. \n\n \n\n5.4.3 Tratamento dos dados que simulam um sensor com falha \n\nA partir dos dados obtidos experimentalmente, foram gerados dados que \n\nsimulavam a falha de cada um dos sensores individualmente. Deste modo, para cada \n\nsensor com falha, os seus valores para todas as amostras de treino foram zerados. Isto \n\nporque foi observado experimentalmente que a falha do sensor faz com que o sensor \n\npare de responder a est\u00edmulos dos vapores de gases. Assim, o valor de resist\u00eancia \n\nextra\u00eddo do sensor ter\u00e1 uma varia\u00e7\u00e3o normalizada nula. \n\nO conjunto de dados de treinamento foi multiplicado pelo n\u00famero de sensores \n\nmais um. Como os sensores de temperatura e umidade n\u00e3o foram considerados como \n\nposs\u00edveis de ter falhas, o novo conjunto de dados de treino teve um tamanho de 6+1=7 \n\nvezes do tamanho original. O conjunto dos dados de teste tamb\u00e9m foi submetido a essa \n\ngera\u00e7\u00e3o de dados de sensores com falha e, portanto, teve o seu tamanho tamb\u00e9m \n\nmultiplicado por esse fator.  \n\n \n\n5.4.4 Segundo Experimento: RNA de sistema com um sensor em falha \n\nNesta abordagem, uma rede neural com as mesmas caracter\u00edsticas da rede \n\nanterior foi treinada com todos os dados originais ou com os dados que simulavam a \n\nfalha de um sensor. No caso dos dados que simulavam a falha de um sensor, sem pr\u00e9-\n\nprocessamento e an\u00e1lise de componentes principais, houve um caso \u00f3timo em que \n\nforam reconhecidos 260 das 273 amostras dos dados de teste. Todos os treinamentos \n\nacabaram com 10mil \u00e9pocas, que foi o crit\u00e9rio m\u00e1ximo adotado. Com o pr\u00e9-\n\nprocessamento e an\u00e1lise de componentes principais de dados houve um resultado \u00f3timo \n\nem que foram reconhecidos 266 das 273 amostras, utilizando uma m\u00e9dia de 1000 \n\n\u00e9pocas de treinamento. Os resultados est\u00e3o detalhados na Tabela 5.4.6a na coluna \n\ndenominada \u201cRNNA Simples\u201d. \n\n\n\n74 \n\n \n\nA gera\u00e7\u00e3o dos dados sint\u00e9ticos que simulavam a falha do sensor foi feita por \n\nmeio da execu\u00e7\u00e3o do arquivo \u201cCriaDadosRuidosos.m\u201d. A cria\u00e7\u00e3o e o uso da rede neural \n\nque utiliza esses dados sint\u00e9ticos foram feitos por meio da execu\u00e7\u00e3o da rotina \n\n\u201cExperimento1.m\u201d. \n\nPara tentar melhorar a generaliza\u00e7\u00e3o do sistema reconhecedor, foram utilizadas \n\nas m\u00e1quinas de comit\u00ea, detalhadas nas se\u00e7\u00f5es seguintes.  \n\n \n\n5.4.5 Terceiro Experimento: Implementa\u00e7\u00e3o das M\u00e1quinas de Comit\u00ea \n\nNa segunda abordagem, foi proposta a m\u00e1quina de comit\u00ea mostrado na Fig. \n\n5.4.5a abaixo. Esta m\u00e1quina teve sete redes neurais, sendo que seis redes foram \n\ntreinadas, levando em conta os sensores de gases com falha. Os sensores de temperatura \n\ne umidade relativa n\u00e3o foram considerados nessas simula\u00e7\u00f5es. A primeira rede neural \n\nfoi treinada levando em conta que o sensor 1 estava com falha, e desta maneira, ela \n\nsimplesmente ignorou os dados do sensor 1 para o seu treinamento. A segunda rede \n\nneural ignorava os dados do sensor 2, e assim por diante at\u00e9 a sexta rede neural. A \n\ns\u00e9tima rede neural foi treinada levando-se em conta que todos os sensores estavam \n\nfuncionando normalmente. \n\nO arquivo correspondente \u00e0 implementa\u00e7\u00e3o desta abordagem de sistema \n\nreconhecedor foi denominado de \u201cTeste1.m\u201d. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.4.5a: Primeira proposta de m\u00e1quina de comit\u00ea \n\nRNA 1\n\nRNA 2\n\nRNA 3\n\nRNA 4\n\nRNA 5\n\nRNA 6\n\nRNA 7\n\nAmostras \nde Treino \ne Teste\n\nSeletor\n\nSa\u00edda\nGlobal\n\nRNA 1\n\nRNA 2\n\nRNA 3\n\nRNA 4\n\nRNA 5\n\nRNA 6\n\nRNA 7\n\nAmostras \nde Treino \ne Teste\n\nSeletor\n\nSa\u00edda\nGlobal\n\n\n\n75 \n\n \n\nAs redes neurais foram treinadas individualmente para depois serem juntadas \n\nneste sistema. Todas foram submetidas a 10 treinamentos com valores iniciais aleat\u00f3rias \n\ndos pesos. As redes neurais com melhores generaliza\u00e7\u00f5es foram escolhidas para \n\nfazerem parte do sistema. \n\nA etapa de Sele\u00e7\u00e3o faz o direcionamento do dado inserido no sistema para a rede \n\nneural incumbida de fazer o seu reconhecimento. A escolha foi feita da seguinte \n\nmaneira: se os dados de todos os sensores fossem diferentes de zero, foi considerado \n\nque todos os sensores estavam em bom estado e, portanto, esta amostra foi submetida \u00e0 \n\ns\u00e9tima rede neural. Caso a resposta do sensor 1 for nulo, ela foi submetida a rede neural \n\n1, em que foi considerado o sensor 1 com falha. E assim por diante para os demais \n\nsensores. \n\nNo caso da sele\u00e7\u00e3o n\u00e3o cega do sensor com falha, isto \u00e9, quando a informa\u00e7\u00e3o da \n\nfalha de um determinado sensor \u00e9 passada previamente para o sistema, o mesmo obteve \n\num resultado \u00f3timo de 271 acertos dentro do universo de 273 amostras, no caso das \n\nredes neurais utilizarem os dados crus. Isto \u00e9, houve uma taxa de acertos de 99,3%. J\u00e1 o \n\nsistema em que os dados foram submetidos a um pr\u00e9-processame nto e an\u00e1lise de \n\ncomponentes principais, o resultado da generaliza\u00e7\u00e3o do sistema foi de 270 acertos \n\ndentro do universo de 273 amostras de teste. Isso resultou em uma taxa de acerto de \n\n98,9%. \n\nNo caso da sele\u00e7\u00e3o cega, isto \u00e9, quando a informa\u00e7\u00e3o da falha de um \n\ndeterminado sensor n\u00e3o era passada para o sistema reconhecedor e o sistema inferia por \n\nsi s\u00f3 a falha de um dos sensores analisando os dados de entrada, houve um caso \u00f3timo \n\nde 262 acertos sem pr\u00e9-processamento e an\u00e1lise de componentes principais, e 263 \n\nacertos no caso em que foram usados o pr\u00e9-processamento e an\u00e1lise de componentes \n\nprincipais. \n\n \n\n5.4.6 Quarto Experimento: Redes Ensemble  \n\nFoi implementado um sistema de dez redes neurais em paralelo, de modo que \n\nsuas sa\u00eddas foram combinadas linearmente para produzir a resposta com maior \n\nprobabilidade de reconhecimento do sistema. O sistema constru\u00eddo est\u00e1 demonstrado na \n\nFig. 5.4.6a a seguir. O arquivo correspondente a implementa\u00e7\u00e3o das redes ensemble em \n\nMatlab foi denominado \u201cEnsemble.m\u201d. \n\n\n\n76 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFig. 5.4.6a: M\u00e1quina de Comit\u00ea Ensemble \n\nTodas as redes foram treinadas com condi\u00e7\u00f5es iniciais aleat\u00f3rias e diferentes \n\nentre si. Elas foram combinadas simplesmente somando as suas respostas, dividindo por \n\ndez, e comparando com as respostas desejadas. \n\nAs redes ensemble que utilizavam os dados sem o pr\u00e9-processamento e an\u00e1lise \n\nde componentes principais tiveram um caso \u00f3timo em que houveram 250 acertos dentro \n\ndo universo de 273 amostras. J\u00e1 o resultado \u00f3timo de generaliza\u00e7\u00e3o do sistema de redes \n\nneurais com pr\u00e9-processamento de dados e an\u00e1lise de componentes principais foi de 264 \n\nacertos dentro do universo de 273 amostras. \n\nTabela 5.4.6a: Resultados de  RNA simples ou m\u00e1quinas de comit\u00ea, que \n\nsimulavam a falha de um dos sensores. \n\n \n\n \n\n \n\n \n \n\nRNA 1\n\nRNA 2\n\nRNA 3\n\nRNA 4\n\nRNA 5\n\nRNA 6\n\nRNA 7\n\nRNA 8\n\nRNA 9\n\nRNA 10\n\nCombinador\nSa\u00edda Global\n\nAmostras \nde Treino \ne Teste\n\nRNA 1\n\nRNA 2\n\nRNA 3\n\nRNA 4\n\nRNA 5\n\nRNA 6\n\nRNA 7\n\nRNA 8\n\nRNA 9\n\nRNA 10\n\nRNA 1\n\nRNA 2\n\nRNA 3\n\nRNA 4\n\nRNA 5\n\nRNA 6\n\nRNA 7\n\nRNA 8\n\nRNA 9\n\nRNA 10\n\nCombinador\nSa\u00edda Global\n\nCombinador\nSa\u00edda Global\n\nAmostras \nde Treino \ne Teste\n\nSem PP e PCA Com PP e PCA Sem PP e PCA Com PP e PCA Sem PP e PCA Com PP e PCA Sem PP e PCA Com PP e PCA\nM\u00e9dia 235,91 257,73 260,00 262,50 267,33 266,67 236,00 262,00\nM\u00e1ximo 260,00 266,00 262,00 263,00 271,00 270,00 250,00 264,00\nM\u00ednimo 223,00 246,00 256,00 262,00 263,00 264,00 231,00 251,00\nDesvio Padr\u00e3o 9,48 6,72 3,46 0,71 4,04 3,06 5,92 3,74\n\nRNNA Simples\nM\u00e1quina de Comit\u00ea Substitutiva \n\n(Sele\u00e7\u00e3o N\u00e3o Cega) M\u00e1quina de Comit\u00ea Ensemble\nM\u00e1quina de Comit\u00ea Substitutiva \n\n(Sele\u00e7\u00e3o Cega)\n\n\n\n77 \n\n \n\n6 Conclus\u00f5es e Perspectivas Futuras \n\nFoi demonstrado que \u00e9 poss\u00edvel implementar um sistema embarcado para o \n\nreconhecimento do poder calor\u00edfico de um g\u00e1s combust\u00edvel ou da qualidade do \u00e1lcool \n\ncombust\u00edvel, tanto com redes neurais quanto com um FIS. A rede neural teve uma alta \n\ntaxa de acertos, comprovando a sua capacidade de generaliza\u00e7\u00e3o.  \n\nA an\u00e1lise de componentes principais \u00e9 uma ferramenta importante na redu\u00e7\u00e3o da \n\ndimensionalidade do espa\u00e7o de entrada do sistema. Mas vimos que ela pode prejudicar a \n\ncapacidade de generaliza\u00e7\u00e3o da rede, pois sensores que aparentemente n\u00e3o influem nos \n\ndados podem ter participa\u00e7\u00e3o no reconhecimento. Tamb\u00e9m foi observado que o pr\u00e9-\n\nprocessamento dos da dos ajuda na velocidade de converg\u00eancia da rede neural. \n\nUm sistema FIS pode ser uma alternativa para a solu\u00e7\u00e3o do problema proposto. \n\nA sua vantagem \u00e9 que a modelagem do sistema \u00e9 acess\u00edvel. O modelo da rede neural \n\ntem propriedades te\u00f3ricas poderosas [8], mas h\u00e1 problemas graves no ajuste da \n\ncomplexidade do modelo da rede. Isto porque o pr\u00f3prio n\u00famero de par\u00e2metros livres da \n\nrede n\u00e3o \u00e9 obtido diretamente, dependendo das caracter\u00edsticas de generaliza\u00e7\u00e3o \n\ndesejadas da aproxima\u00e7\u00e3o do modelo. A desvantagem do FIS \u00e9 que ele n\u00e3o consegue \n\nseparar padr\u00f5es sobrepostos.  \n\nPara melhorar a generaliza\u00e7\u00e3o do FIS, foi utilizada uma etapa de pr\u00e9-\n\nprocessamento e an\u00e1lise de componentes principais nos dados de treino submetidos ao \n\nFIS. Isso fez com que o n\u00famero de sensores que tinham uma vari\u00e2ncia maior fossem \n\nconsiderados, e com isso, o n\u00famero de atributos passou de 6 para 2. O resultado do \n\nsistema foi melhorado significativamente com esse tratamento. E, portanto, podemos \n\nconsiderar o sistema FIS como uma \u00f3tima alternativa de reconhecimento, sob \n\ndeterminadas condi\u00e7\u00f5es dos dados de entrada. \n\nFoi observado que tamb\u00e9m \u00e9 poss\u00edvel implementar um sistema reconhecedor de \n\nmodo a aumentar a sua robustez \u00e0 perda de sensores. E para este fim, pode -se usar as \n\nm\u00e1quinas de comit\u00ea est\u00e1ticas do tipo Ensemble ou um sistema formado de redes neurais \n\ntreinadas com subconjuntos dos dados. Dentre as abordagens discutidas, a sistema que \n\nobteve a melhor taxa de acertos das amostras de teste (generaliza\u00e7\u00e3o) foi a obtida no \n\nTerceiro Experimento do item 5.4.5, em que foram usadas sete redes neurais treinadas \n\ncom conjuntos diferentes dos dados, de acordo com o sensor com falha que era \n\n\n\n78 \n\n \n\nconsiderado. Mas mesmo esta abordagem pode te r duas possibilidades: se o sistema \n\nreconhecedor tiver um meio de detectar a falha do sensor, ela ter\u00e1 uma taxa de \n\nreconhecimento melhor se comparado com um sistema em que o mesmo tem de inferir a \n\nfalha do sensor por si s\u00f3. \n\nNeste trabalho foi estimada uma maior facilidade na implementa\u00e7\u00e3o em \n\nhardware da rede neural do que o sistema de infer\u00eancia fuzzy para o problema de \n\nreconhecimento de padr\u00f5es. Isto porque a pr\u00f3pria arquitetura da rede neural MLP \u00e9 mais \n\nsimples que a arquitetura necess\u00e1ria para implementar um FIS. A fun\u00e7\u00e3o matem\u00e1tica \n\nmais complicada na implementa\u00e7\u00e3o da rede neural em hardware \u00e9 a fun\u00e7\u00e3o de ativa\u00e7\u00e3o, \n\nque normalmente \u00e9 n\u00e3o linear. J\u00e1 o sistema fuzzy necessita de v\u00e1rios c\u00e1lculos nas etapas \n\nde fuzzifica\u00e7\u00e3o, infer\u00eancia, e defuzzifica\u00e7\u00e3o.  \n\nAs abordagens de sistemas de aumento de robustez do sistema ainda poder\u00e3o ser \n\naplicadas recursivamente. Ou seja, dada a determina\u00e7\u00e3o do sensor com falha e a escolha \n\ndo sistema adequado para tratar a perda deste sensor, a metodologia adotada neste \n\ntrabalho ainda poder\u00e1 ser usado novamente para o caso em que se detectar a falha de um \n\noutro sensor, diferente do primeiro.  \n\nA metodologia adotada para aumentar a robustez do sistema ainda poder\u00e1 \n\nsubmeter os dados a outros sistemas est\u00e1ticos de m\u00e1quinas de comit\u00ea, a M\u00e1quina \n\nRefor\u00e7o ou a M\u00e1quina AdaBoost [8], em que o treinamento do sistema prev\u00ea a pouca \n\ndisponibilidade de amostras de treino, e com isso, \u00e9 previsto que se possa obter \n\nresultados melhores com um conjunto reduzido de amostras. M\u00e1quinas de Comit\u00ea \n\nDin\u00e2micas tamb\u00e9m poder\u00e3o ser uma outra alternativa para resolver o problema. \n\nA implementa\u00e7\u00e3o do hardware da melhor solu\u00e7\u00e3o encontrada ainda necessitar\u00e1 \n\nser estudada e discutida. Como a implementa\u00e7\u00e3o do problema de reconhecimento no Kit \n\ndo DSP da Analog Devices \u00e9 muito limitada, por esta ter somente uma entrada \n\nanal\u00f3gica est\u00e9reo, ser\u00e1 necess\u00e1ria a discuss\u00e3o da implementa\u00e7\u00e3o em outro sistema \n\nbaseado em microprocessador, j\u00e1 desenvolvida no laborat\u00f3rio do LME. \n\n \n\n\n\n79 \n\n \n\n7 Bibliografia: \n \n\n[1] PAULSSON, N.; LARSSON, E.; WINQUIST, F.; Extraction and selection of \n\nparameters for evaluation of breath alcohol measurement with an electronic nose, \n\nSensors and Actuators 84 (2000) 187-197. \n\n[2] DRAKE, M. A. et. al, Application of an electronic nose to correlat e with \n\ndescriptive sensory analysis of aged Cheddar cheese, Lebensm.-Wiss. Technologie \n\n36(1):13-20, 2003; IFT Annual Meeting Technical Program Abstracts.  15C-37 p 28, \n\n2001. \n\n[3] KELLER, P. E. et al., Electronic noses and their applications , IEEE Technical \n\nApplications Conference and Workshop: Conference Record, Institute of Electrical and \n\nElectronic Engineers \u2013 October (1995) 116-119. \n\n[4] YANG, Y.; YANG, P.; WANG, X.; Electronic nose based on SAWS array and \n\nits odor identification capability, Sensors and Actuators B 66 (2000), 167-170. \n\n[5] GARDNER, J., SHIN H., HINES E., An electronic nose to diagnose illness, \n\nSensors and Actuators B 70 (2000), 19-24. \n\n[6] GARDNER, J. et al., An electronic nose system for monitoring the quality of \n\npotable water, Sensors and Actuators B 69 (2000), 336-341. \n\n[7] HIRAYAMA, V., RAMIREZ-FERNANDES, F. J., Embedded System to \n\nrecognize the heat power of a fuel gas and to classificate the quality of alcohol fuel, \n\n2003 IEEE International Symposium on Industrial Electronics, Rio de Janeiro Brazil. \n\n[8] HAYKIN, S. Redes Neurais \u2013 Princ\u00edpios e Pr\u00e1tica, Segunda Edi\u00e7\u00e3o, Porto Alegre, \n\nEditora Bookman, 1999, 900p. \n\n[9] ULBIG, R.; HOBURG D.; Determination of the calorific value of natural gas by \n\ndifferent methods , Thermochimica Acta 382 (2000), 27-35. \n\n[10] PARDO, M., SBERVEGLIERI, G., Learning From Data: A Tutorial With \n\nEmphasis on Modern Pattern Recognition Methods , IEEE SENSORS JOURNAL, \n\nVOL. 2, NO. 3, JUNE 2002, 203-217. \n\n[11] KARTALOPOULOS, S.V., Understanding Neural Networks and Fuzzy Logic, \n\nWiley-IEEE Press, 1995, 232 p \n\n\n\n80 \n\n \n\n[12] FIGARO; General Information for TGS Sensors , site da internet \n\nwww.figarosensors.com.  \n\n[13] VEGA, M.L.B.P., Sistema Inteligente para Identificar Gases, Tese de Mestrado, \n\nUniversidade de S\u00e3o Paulo, Brasil. 1998, 109p. \n\n[14] RAKOW , N.A.; SUSLICK, K. S.; A colorimetric sensor array for odour \n\nvisualization, Department of Chemistry, University of Illinois \u2013  USA. Revista \n\n\u201cNature\u201d, Vol.406, 17 de agosto de 2000, p\u00e1ginas 710 a 713.  \n\n[15] LUNDSTR\u00d6M, I.; Picture The Smell; Revista \u201cNature\u201d, Vol. 406, 17 de agosto \n\nde 2000, p\u00e1ginas 682 a 683.  \n\n[16] ORTH, A.; STEMMER, M. R.; Sistema de vis\u00e3o aplicado ao Monitoramento do \n\ndesgaste de ferramentas de corte: Uma estrat\u00e9gia para medir o desgaste de flanco \n\n(Vb), UFSC \u2013 Universidade Federal de Santa Catarina \u2013 SC \u2013 Brasil. 2001.  \n\n[17] CASTILHO, D.; Analisador de Gases, Tese de Mestrado \u2013  USP \u2013 Universidade \n\nde S\u00e3o Paulo \u2013  SP \u2013  Brasil. 1996, 155p. \n\n[18] KELLER, P.E. et al; Transmission of Olfactory Information for Telemedicine , \n\nPacific Northwest Laboratory, USA. IOS Press, Vol. 18, January 1995.  \n\n[19] KELLER, P. E.; KOUZES, R. T., KANGAS, L. J.; Three Neural Network Based \n\nSensor Systems for Environment Monitoring, Northwest College and University \n\nAssociation for Science (Washington State University), USA. Miller Freeman, Inc., \n\nMay 1994, 377-382. \n\n[20] WU, H.; SIEGEL, M.; Odor-Based Incontinence Sensor, Robotics Institute, \n\nSchool of Computer Science, Carnelie Mellon University, Pittsburgh. Proceedings of \n\nthe 17th IEEE Instrumentation and Measurement Technology Conference, Vol. 1, May \n\n2000, pp.63-68. \n\n[21] LU, Y.; BIAN L.; YANG, P.; Quantitative Artificial Neural Network For \n\nElectronic Noses, Department of Chemistry, Fudan University, Shangai, China. \n\nAnalytica Chimica Acta. 417:101-110, 2000 \n\n[22] FLEXER, A.; Statistical Evaluation of Neural Network Experiments: \n\nMinimum Requirements and Current Practice , Austrian Research Institute for \n\nArtificial Intelligence; Vienna \u2013  Austria. 1995 \n\n\n\n81 \n\n \n\n[23] LAZZERINI, B.; MAGGIORE, A; MARCELLONI, F.; FROS: a fuzzy logic -\n\nbased recognizer of olfactory signals, University of Pisa, Pisa, Italy. Pattern \n\nRecognition 34(11): 2215-2226 (2001) \n\n[24] JUNIOR, E. F. C., e Grupo Comunica\u00e7\u00f5es Homem - M\u00e1quina, Redes Neurais \n\nArtificiais, um curso te\u00f3rico e pr\u00e1tico para engenheiros e cientistas , Escola \n\nPolit\u00e9cnica da USP (Universidade de S\u00e3o Paulo), Departamento de Engenharia \n\nEletr\u00f4nica e Laborat\u00f3rio de Comunica\u00e7\u00f5es de Sinais (PCS). EDUSP, 1999. \n\n[25] STORK, D. G.; DUDA, R. O.; HART, P. E., Pattern Recognition, Wiley-\n\nInterscience; 2 edition (October 2000) , 654p. \n\n[26] MATHWORKS, The Inc. , Documenta\u00e7\u00e3o do Software MATLAB Vers\u00e3o 6.0. \n\n[27] ROSEN, B.E., Ensemble learning using decorrelated neural networks, \n\nConnection Science, 8, 3-4, pp. 373-384, 1996. \n\n[28] PARDO, M. et al., Decompositive classification models for electronic noses, \n\nAnal. Chimica Acta, 2001. pp. 223-232. \n\n[29] ZADEH, L. A., Fuzzy Sets, Information and Control 8, pg. 338-353 (1965). \n\n[30] LEE, C. C., Fuzzy Logic in Control Systems: Fuzzy Logic Controller \u2013 Part I, \n\nIEEE Transations on Systems, man, and cybernetics, vol.10, number 2, 1990, 404-418. \n\n[31] LEE, C. C., Fuzzy Logic in Control Systems: Fuzzy Logic Controller \u2013 Part II, \n\nIEEE Transations on Systems, man, and cybernetics, vol.20, number 2, 1990, 419-435. \n\n[32] BARATTO, G.; Classificador de Aromas Com Redes Neurais Artificiais para \n\num Nariz Eletr\u00f4nico, Tese de Doutorado \u2013  USP \u2013 Universidade de S\u00e3o Paulo \u2013  SP \u2013 \n\nBrasil. 1997. \n\n[33] NEURALWARE INC., Using  NeuralWorks - NeuralWorks Professional \n\nII/PLUS and NeuralWorks Explorer, NeuralWare, Inc., 1995; \n\n[34] DUBOIS, D.; PRADE H.; YAGER R., Fuzzy Information Engineering: A \n\nGuided Tour of Applications; Cap\u00edtulo 9: Extracting Fuzzy Rules from data for \n\nfunction approximation and pattern recognition (CHIU, S. L.,). Wiley; (November \n\n1996), 149-162. \n\n[35] NEURALWARE INC., Pre -processing Data with Data Sculptor, NeuralWare, \n\nInc., 1995;  \n\n\n\n82 \n\n \n\n[36] Mathworks, The Inc. , Software MATLAB."}]}}}