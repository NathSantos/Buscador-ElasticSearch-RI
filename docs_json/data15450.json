{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.19793"}, {"@name": "filename", "#text": "3213_Sanchetta_AlexandreCruz_M.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "0 \n\n \n\n \n\n \n\n \n\nUNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA MEC\u00c2NICA E \n\nINSTITUTO DE GEOCI\u00caNCIAS \n\nCOMISS\u00c3O DE PROGRAMA MULTIDISCIPLINAR DE P\u00d3S-\n\nGRADUA\u00c7\u00c3O EM CI\u00caNCIAS E ENGENHARIA DE PETR\u00d3LEO \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n Reconhecimento E Classifica\u00e7\u00e3o De F\u00e1cies \n\nGeol\u00f3gicas Atrav\u00e9s Da An\u00e1lise De \n\nComponentes Independentes \n \n\n \n\n \n\n \n\n \n\n \n\nAutor: Alexandre Cruz Sanchetta \n\nOrientador: Rodrigo de Souza Portugal \n\nCo-orientador: \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n i \n\n \n\n \n\n \n\n \n\nUNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA MEC\u00c2NICA E \n\nINSTITUTO DE GEOCI\u00caNCIAS \n\nCOMISS\u00c3O DE PROGRAMA MULTIDISCIPLINAR DE P\u00d3S-\n\nGRADUA\u00c7\u00c3O EM CI\u00caNCIAS E ENGENHARIA DE PETR\u00d3LEO \n \n\n \n\n \n\n \n\n  \n\nReconhecimento E Classifica\u00e7\u00e3o De F\u00e1cies \n\nGeol\u00f3gicas Atrav\u00e9s Da An\u00e1lise De \n\nComponentes Independentes  \n \n\n \n\nAutor: Alexandre Cruz Sanchetta \n\nOrientador: Rodrigo de Souza Portugal \n\nCo-orientador: \n\n \n\n \n\n \n\n \n\n \n\nPrograma: Ci\u00eancias e Engenharia de Petr\u00f3leo \n\n\u00c1rea de Concentra\u00e7\u00e3o: Caracteriza\u00e7\u00e3o de Reservat\u00f3rios  \n\n \n\n \n\nDisserta\u00e7\u00e3o de mestrado acad\u00eamico apresentada \u00e0 Comiss\u00e3o de P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancias \n\ne Engenharia de Petr\u00f3leo da Faculdade de Engenharia Mec\u00e2nica e Instituto de Geoci\u00eancias, como \n\nrequisito para a obten\u00e7\u00e3o do t\u00edtulo de Mestre em Ci\u00eancias e Engenharia de Petr\u00f3leo. \n\n \n\n \n\n \n\nCampinas, 2010 \n\nSP \u2013 Brasil. \n\n\n\n ii \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFICHA  CATALOGR\u00c1FICA  ELABORADA  PELA  \n\n  BIBLIOTECA  DA  \u00c1REA  DE  ENGENHARIA  E  ARQUITETURA  -  BAE  -  UNICAMP \n\n \n\n \n\n \n\n \n\n    C889r \n\n \n\nCruz Sanchetta, Alexandre \n\n     Reconhecimento e classifica\u00e7\u00e3o de f\u00e1cies geol\u00f3gicas \n\natrav\u00e9s da an\u00e1lise de componentes independentes / \n\nAlexandre Cruz Sanchetta. --Campinas, SP: [s.n.], 2010. \n\n \n\n     Orientador: Rodrigo de Souza Portugal. \n\n     Disserta\u00e7\u00e3o de Mestrado - Universidade Estadual de \n\nCampinas, Faculdade de Engenharia Mec\u00e2nica e \n\nInstituto de Geoci\u00eancias. \n\n \n\n     1. An\u00e1lise multivariada.  2. F\u00e1cies (Geologia).  3. \n\nReconhecimento de padr\u00f5es.  4. Classifica\u00e7\u00e3o.  I. \n\nPortugal, Rodrigo de Souza.  II. Universidade Estadual \n\nde Campinas. Faculdade de Engenharia Mec\u00e2nica e \n\nInstituto de Geoci\u00eancias.  III. T\u00edtulo. \n\n \n\n \n\nT\u00edtulo em Ingl\u00eas: Recognition and classification of geological facies based on \n\nindependent component analysis \n\nPalavras-chave em Ingl\u00eas: Multivariate analysis, Facies (Geology), Recognition \n\nof patterns, Classification \n\n\u00c1rea de concentra\u00e7\u00e3o: Reservat\u00f3rios e Gest\u00e3o \n\nTitula\u00e7\u00e3o: Mestre em Ci\u00eancias e Engenharia de Petr\u00f3leo \n\nBanca examinadora: Emilson Pereira Leite, Liliana Alcazar Diogo \n\nData da defesa: 02/12/2010 \n\nPrograma de P\u00f3s Gradua\u00e7\u00e3o: Ci\u00eancias e Engenharia de Petr\u00f3leo \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\niii \n\n \n\n \n\nUNIVERSIDADE ESTADUAL DE CAMPINAS \n\nFACULDADE DE ENGENHARIA MEC\u00c2NICA E \n\nINSTITUTO DE GEOCI\u00caNCIAS \n\nCOMISS\u00c3O DE PROGRAMA MULTIDISCIPLINAR DE P\u00d3S-\n\nGRADUA\u00c7\u00c3O EM CI\u00caNCIAS E ENGENHARIA DE PETR\u00d3LEO \n \n\n DISSERTA\u00c7\u00c3O DE MESTRADO ACAD\u00caMICO \n \n\n \n\n Reconhecimento E Classifica\u00e7\u00e3o De F\u00e1cies \n\nGeol\u00f3gicas Atrav\u00e9s Da An\u00e1lise De \n\nComponentes Independentes  \n\n \n \n\nAutor: Alexandre Cruz Sanchetta \n\nOrientador: Rodrigo de Souza Portugal \n\nCo-orientador: \n\n \n\nA Banca Examinadora composta pelos membros abaixo aprovou esta Disserta\u00e7\u00e3o: \n\n \n\n \n\n____________________________________________________ \n\nProf. Dr. Rodrigo de Souza Portugal, Presidente \n\nSHUMBLERGER \n\n \n\n \n\n____________________________________________________ \n\nProf. Dr. Emilson Pereira Leite \n\nDGRN/IG/UNICAMP \n\n \n\n \n\n____________________________________________________ \n\nProf\u00aa. Dr\u00aa. Liliana Alcazar Diogo \n\nIAG/USP \n\n \n\nCampinas, 02 de dezembro de 2010\n\n\n\n \n\n \n\n \n\nv \n\n \n\n \n\n \n\n \n\n \n\n \n\nDedicat\u00f3ria \n\nDedico este trabalho a minha fam\u00edlia, por sempre permitir que eu continuasse com meus \n\nobjetivos. \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n \n\nvii \n\n \n\n \n\n \n\n \n\n \n\n \n\nAgradecimentos \n\nAgrade\u00e7o imensamente... \n\n\u00c0 minha fam\u00edlia, como dedicado previamente, esse trabalho \u00e9 de voc\u00eas. \n\nAos meus pais, Pedro e Wanda, pelo incentivo, apoio e carinho durante todo esse caminho. \n\nAo meu orientador, Professor Rodrigo de Souza Portugal, pela orienta\u00e7\u00e3o (praticamente) \u00e0 \n\ndist\u00e2ncia e pela paci\u00eancia durante os anos. \n\nAo Professor Alexandre Campane Vidal, pelo acolhimento e confian\u00e7a depositada. \n\nAos Professores Emilson Pereira Lima e Rodrigo Duarte Drummond, pelas corre\u00e7\u00f5es, \n\ndicas, e conversas. \n\nAos amigos de of\u00edcio: Juliana, Michele, Ancila, e Bruno (Champz), pelo companheirismo, \n\namizade e pela ajuda prestada. \n\nA meus companheiros da Rep\u00fablica Viracopos, pelo entretenimento e suporte nas horas em \n\nque a mente pedia um descanso. \n\n \n\n \n\n \n\n \n\n\n\n \n\nix \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\u201cA m\u00fasica \u00e9 um exerc\u00edcio inconsciente \n\nde c\u00e1lculos\u201d.  \n\n \n\nLeibniz \n\n \n\n \n\n\n\n \n\n \n\n  \n\nxi \n\n \n\n \n\n \n\n \n\n \n\nResumo \n\nSANCHETTA, Alexandre Cruz, Reconhecimento E Classifica\u00e7\u00e3o De F\u00e1cies Geol\u00f3gicas Atrav\u00e9s \n\nDa An\u00e1lise De Componentes Independentes, Campinas, Faculdade de Engenharia \n\nMec\u00e2nica, Universidade Estadual de Campinas, 2010. 94 p. Disserta\u00e7\u00e3o de Mestrado. \n\nO uso m\u00e9todo de an\u00e1lise multivariada ICA (An\u00e1lise de Componentes Independentes), \n\nmais o m\u00e9todo K-NN (K-vizinhos mais Pr\u00f3ximos) aplicados em dados de po\u00e7os e em dados \n\ns\u00edsmicos buscando classificar f\u00e1cies geol\u00f3gicas e suas caracter\u00edsticas. Esses dois m\u00e9todos foram \n\naplicados em dados retirados do Campo de Namorado, na Bacia de Campos, Brasil. A ICA \n\nencontra as componentes independentes dos dados, que quando treinadas pelo m\u00e9todo K-NN para \n\nreconhecer padr\u00f5es nos dados, predizem f\u00e1cies geol\u00f3gicas e outras informa\u00e7\u00f5es sobre as rochas, \n\ncomo as caracter\u00edsticas de reservat\u00f3rio. Essas componentes independentes configuram uma nova \n\nop\u00e7\u00e3o de interpreta\u00e7\u00e3o das informa\u00e7\u00f5es dispon\u00edveis, pois nessas novas vari\u00e1veis, o espa\u00e7o de \n\nan\u00e1lise n\u00e3o apresenta dimens\u00f5es dependentes e exclui informa\u00e7\u00f5es repetidas ou d\u00fabias da \n\ninterpreta\u00e7\u00e3o dos resultados. Al\u00e9m disso, a maior parte da informa\u00e7\u00e3o \u00e9 resumida em poucas \n\ndimens\u00f5es, resultando em uma poss\u00edvel redu\u00e7\u00e3o de vari\u00e1veis referentes ao problema.  Um \n\nabundante n\u00famero de testes foi feito procurando a taxa de sucesso desse m\u00e9todo. Como taxa de \n\nsucesso, \u00e9 compreendida a divis\u00e3o do n\u00famero de predi\u00e7\u00f5es corretas dividido pelo n\u00famero total de \n\ntentativas.  O que se observa \u00e9 uma taxa de sucesso alta, em torno de 85% de acerto em algumas \n\nsitua\u00e7\u00f5es, ressaltando-se que as componentes t\u00eam distribui\u00e7\u00e3o gaussiana, sendo que o m\u00e9todo \n\nfunciona melhor em encontrar componentes n\u00e3o-gaussianas. Mesmo nessa situa\u00e7\u00e3o adversa o \n\nm\u00e9todo se mostrou robusto.  A solidez do m\u00e9todo mostra-se uma alternativa para novas formas \n\nde interpreta\u00e7\u00e3o geol\u00f3gicas e petrof\u00edsicas. Um dos trunfos desse m\u00e9todo \u00e9 que a base da sua \n\naplica\u00e7\u00e3o pode ser estendida para outros tipos de dados, inclusive de naturezas f\u00edsicas diferentes. \n\n \n\n \n\nPalavras Chave \n\nAn\u00e1lise de Multivariada; F\u00e1cies (Geologia); Reconhecimento de Padr\u00f5es; Classifica\u00e7\u00e3o \n\n\n\n \n\n \n\n  \n\nxiii \n\n \n\n \n\n \n\nAbstract \n\nSANCHETTA, Alexandre Cruz, Recognition and Classification of Geological Facies Based on \n\nIndependent Component Analysis, Campinas, Faculdade de Engenharia Mec\u00e2nica, \n\nUniversidade Estadual de Campinas, 2010. 94 p. Disserta\u00e7\u00e3o de Mestrado. \n\nThe use of multivariate analysis method ICA (Independent Component Analysis), plus the \n\nK-NN method (K-nearest Neighbor) applied on well log data and seismic data to predict the \n\nclassification of geological facies and their characteristics. These two methods were applied to \n\ndata from the Campo de Namorado, in the Campos Basin, Brasil. The ICA  finds the independent \n\ncomponents of the data that can be trained by K-NN method to recognize patterns in the data  and \n\npredict the geological facies or other information about the rocks, as the characteristics of the \n\nreservoir. These independent components make up a new option for interpretation of available \n\ninformation, because with these new variables, the space has no dependent dimensions and the \n\nduplicate information or dubious interpretation of results are excluded. Moreover, most of the \n\ninformation is summarized in a few dimensions, resulting in a possible reduction of variables \n\nrelated to the problem. An abundant number of tests were done looking for the success rate of \n\nthis method. As success rate, it is understood by the division of the number of correct predictions \n\ndivided by total attempts. What is observed is a high success rate, around 85% accuracy in some \n\nsituations, pointing out that the components have a Gaussian distribution and the method works \n\nbest in finding non-Gaussian components. Even in this adverse situation the method was robust. \n\nThe robustness of the method proves that ICA can be an alternative to new forms of geological \n\nand petrophysical interpretation. One of the advantages of this method is that the basis of their \n\napplication can be extended to other types of data, including datas with different physical natures. \n\n \n\n \n\n \n\nKey Words \n\nMultivariate Anaysis, Facies (Geology), Pattern Recognition; Classification  \n\n\n\n \n\n \n\n  \n\nxv \n\n \n\n \n\n \n\n\u00cdndice \n\n \n\nLista de Figuras................................................................................................ xvii \n\nLista de Tabelas ................................................................................................ xix \n\nSiglas ................................................................................................................ xxi \n\nCap\u00edtulo 1 ............................................................................................................ 1 \n\n   Introdu\u00e7\u00e3o ...........................................................................................................  \n\nCap\u00edtulo 2 ............................................................................................................ 3 \n\n   Separa\u00e7\u00e3o Cega de Sinais ....................................................................................  \n\nCap\u00edtulo 3 ............................................................................................................ 7 \n\n   An\u00e1lise de Componentes Principais ....................................................................  \n\nCap\u00edtulo 4 .......................................................................................................... 11 \n\n   An\u00e1lise De Componentes Independentes.............................................................  \n\nCap\u00edtulo 5 .......................................................................................................... 33 \n\n   Reconhecimento de Padr\u00f5es ................................................................................  \n\nCap\u00edtulo 6 .......................................................................................................... 47 \n\n   Metodologia ........................................................................................................  \n\nCap\u00edtulo 7 .......................................................................................................... 57 \n\n   Resultados e Discuss\u00f5es ......................................................................................  \n\nCap\u00edtulo 8 .......................................................................................................... 77 \n\n   Conclus\u00f5es ..........................................................................................................  \n\nBibliografia ........................................................................................................ 81 \n\nAp\u00eandices .......................................................................................................... 87 \n\n \n\n\n\n \n\n \n\n  \n\nxvii \n\n \n\n \n\n \n\n \n\n \n\n \n\nLista de Figuras \n\n \n\nFigura 2.1          Cocktail Party Problem................................................................................... 4 \n\nFigura 5.1          Exemplos de Transla\u00e7\u00e3o e Rota\u00e7\u00e3o de um Objeto-Padr\u00e3o Inicial.................35 \n\nFigura 5.2          Vizinhos Utilizados para Diferentes n\u00fameros de K.........................................43 \n\nFigura 6.1          Fluxograma dos m\u00e9todos............................................................................... 54 \n\nFigura 6.2          Tabela de classifica\u00e7\u00e3o Reservat\u00f3rio/N\u00e3o Reservat\u00f3rio ............................... 56 \n\nFigura 7.1          Primeira Bateria de Resultados...................................................................... 60 \n\nFigura 7.2          Segunda Bateria de Resultados...................................................................... 62 \n\nFigura 7.3          Terceira Bateria de Resultados  - Classifica\u00e7\u00e3o F\u00e1cies.................................. 64 \n\nFigura 7.4          Terceira Bateria de  Resultados - Classifica\u00e7\u00e3o de Reservat\u00f3rios................. 66 \n\nFigura 7.5           Quarta Bateria de Resultados......................................................................... 68 \n\nFigura 7.6          Teste entre ICA e FastICA \u2013 Classifica\u00e7\u00e3o de F\u00e1cies.................................... 69 \n\nFigura 7.7          Teste entre ICA e FastICA \u2013 Classifica\u00e7\u00e3o de Reservat\u00f3rio.......................... 69 \n\nFigura 7.8       Compara\u00e7\u00e3o entre velocidades de ICA e FastICA......................................... 70 \n\nFigura 7.9           Compara\u00e7\u00e3o ICA e PCA \u2013 Classifica\u00e7\u00e3o F\u00e1cies............................................ 71 \n\nFigura 7.10           Compara\u00e7\u00e3o ICA e PCA \u2013 Classifica\u00e7\u00e3o F\u00e1cies............................................ 72 \n\nFigura 7.11  Predi\u00e7\u00e3o de Po\u00e7o NA01.................................................................................. 74 \n\nFigura 7.12           Teste Par/\u00cdmpar \u2013 Classifica\u00e7\u00e3o de F\u00e1cies .................................................... 75 \n\nFigura 7.13          Teste Menos-Um \u2013 Classifica\u00e7\u00e3o de Reservat\u00f3rios........................................ 76 \n\n\n\n \n\n \n\n  \n\nxix \n\n \n\n \n\n \n\n \n\n \n\n \n\nLista de Tabelas \n\n \n\nTabela 4.1 Rotina do algoritmo de gradiente atrav\u00e9s da negentropia - Adaptado de \n\n(Hyv\u00e4rinen,2001).......................................................................................................................... 19 \n\n \n\nTabela 4.2 Rotina do FastICA atrav\u00e9s da negentropia - Adaptado de \n\n(Hyv\u00e4rinen,2001).......................................................................................................................... 20 \n\n \n\nTabela 4.3 Rotina do FastICA atrav\u00e9s da Ortogonaliza\u00e7\u00e3o Deflacion\u00e1ria- Adaptado de \n\n(Hyv\u00e4rinen,2001).......................................................................................................................... 21 \n\n \n\nTabela 4.4 Rotina do FastICA atrav\u00e9s da Ortogonaliza\u00e7\u00e3o Sim\u00e9trica - Adaptado de \n\n(Hyv\u00e4rinen,2001)......................................................................................................................... 22 \n\n \n\nTabela 4.5 Rotina do FastICA atrav\u00e9s da Estimativa de M\u00e1xima Probabilidade - Adaptado de \n\n(Hyv\u00e4rinen,2001).......................................................................................................................... 28 \n\n \n\nTabela 5.1 Aplica\u00e7\u00f5es do Reconhecimento de Padr\u00f5es.............................................................. 32 \n\n \n\nTabela 6.1 F\u00e1cies Litol\u00f3gicas...................................................................................................... 48 \n\n \n\nTabela 6.2 Perfis Geol\u00f3gicos...................................................................................................... 49 \n\n \n\nTabela 6.3 Exemplo de Amostra de Dado de Perfil....................................................................50  \n\n \n\nTabela 6.4 Dados S\u00edsmicos......................................................................................................... 51 \n\n \n\nTabela 6.5 Exemplo de Amostra de Dado S\u00edsmico..................................................................... 52 \n\n \n\nTabela 6.6 Separa\u00e7\u00e3o de Testemunho nos Dados S\u00edsmicos com Predomin\u00e2ncia...................... 53 \n\n \n\nTabela 6.7  Separa\u00e7\u00e3o de Testemunho nos Dados S\u00edsmicos sem Predomin\u00e2ncia...................... 53 \n\n \n\nTabela 7.1 Fun\u00e7\u00f5es-Objetivo......................................................................................................67 \n\n \n\nTabela 7.2 Sequ\u00eancia Quarta Bateria de Resultados..................................................................67 \n\n \n\nTabela 7.3 Predi\u00e7\u00e3o individual da parte testemunhada..............................................................73 \n\n \n\n \n\n\n\n \n\n \n\n  \n\nxxi \n\n \n\n \n\n \n\n \n\n \n\nSiglas \n\n \n\nBSS \u2013 Separa\u00e7\u00e3o Cega de Sinais \n\n \n\nPCA \u2013 An\u00e1lise de Componentes Principais \n\n \n\nICA \u2013 An\u00e1lise de Componentes Independentes \n\n \n\nK-NN \u2013 K-Vizinhos mais Pr\u00f3ximos \n\n \n\nNMV \u2013 N\u00famero M\u00ednimo de Vizinhos \n\n \n\nVMA \u2013 Valor de M\u00e1ximo Acerto \n\n \n\n \n\n\n\n \n\n \n\n  \n\n1 \n\n \n\n \n\n \n\n \n\n \n\nCap\u00edtulo 1 \n\n \n\nIntrodu\u00e7\u00e3o \n\n \n\nNa esfera da an\u00e1lise multivariada, uma das ferramentas que podem ser utilizadas em v\u00e1rios \n\ntipos de processamento de dados \u00e9 a An\u00e1lise de Componentes Independentes (Stone, 2005). Este \n\nartigo tem como objetivo aprofundar o conhecimento nesse m\u00e9todo, al\u00e9m de alternativas para seu \n\nfuncionamento, seja essas alternativas computacionais, ou conceituais. \n\nComo o ICA n\u00e3o carece de nenhuma informa\u00e7\u00e3o sobre os dados, como qualquer Separa\u00e7\u00e3o \n\nCega de Sinais (Murata ET AL, 2001), a exist\u00eancia de sinais independentes nos dados \u00e9 \n\nassumida, sem perda de aplicabilidade do m\u00e9todo, visto que a independ\u00eancia \u00e9 praticamente um \n\npreceito f\u00edsico (Casey, 2001). Diferente dos outros m\u00e9todos procura-se encontrar estimativas dos \n\nsinais independentes n\u00e3o-gaussianos, ou seja, que sua distribui\u00e7\u00e3o se afaste ao m\u00e1ximo da \n\ndistribui\u00e7\u00e3o gaussiana (Comom, 1994). Logo, a busca por essa n\u00e3o-gaussianidade tem alto n\u00edvel \n\nde import\u00e2ncia no m\u00e9todo. Existem v\u00e1rios m\u00e9todos para encontrar tal atributo estat\u00edstico, por\u00e9m \u00e9 \n\ninteressante balancear-se poss\u00edveis perdas e ganhos nessa procura. M\u00e9todos de robustez elevada \n\npodem levar a um pre\u00e7o computacional proporcionalmente elevado. M\u00e9todos r\u00e1pidos podem ter \n\ngrandes estimativas err\u00f4neas ou apresentar desvios perigosamente elevados. Essas preocupa\u00e7\u00f5es, \n\nsomadas \u00e0 aplicabilidade do m\u00e9todo aos dados podem traduzir um avan\u00e7o na an\u00e1lise dos dados, \n\nassim como traduz a motiva\u00e7\u00e3o dessa pesquisa em si.  \n\nMedir a n\u00e3o-gaussianidade de um conjunto de dados, pode ser descrito como maximizar um \n\nconjunto de fun\u00e7\u00f5es-objetivo procurando as estimativas das componentes independentes.  \n\nEnquanto o ICA, como proposto em seu princ\u00edpio (Hyv\u00e4rinen, 2001), apesar de eficiente, tinha \n\nalto custo computacional. Por esse motivo, diversos autores procuraram novas alternativas para \n\nmelhorar, acelerar ou otimizar tal processo [(Hyv\u00e4rinen, 1999), (Marchini ET AL, 2009), \n\n(Cardoso ET AL, 2002)].  \n\n\n\n \n\n \n\n  \n\n2 \n\n \n\nDentre esses autores, um dos que se destacam \u00e9 Aapo Hyv\u00e4rinen da Universidade de \n\nTecnologia de Helsinki, que criou o FastICA, um algoritmo baseado no esquema de itera\u00e7\u00e3o de \n\nponto - fixo que maximiza a n\u00e3o-gaussianidade como uma medida de independ\u00eancia estat\u00edstica.  \n\nEntre as vantagens do FastICA, podemos destacar que o m\u00e9todo \u00e9 muito mais eficaz no que \n\nse diz respeito a custo computacional. A busca r\u00e1pida dentro da fun\u00e7\u00e3o objetivo faz com que o \n\nFastICA seja mais r\u00e1pido, como um todo, que o m\u00e9todo convencional baseado em decl\u00ednio de \n\ngradiente. Estima-se que a converg\u00eancia paire na casa de 10 a 100 vezes mais r\u00e1pida, de acordo \n\ncom o pr\u00f3prio Hyv\u00e4rinen. Al\u00e9m disso, esse m\u00e9todo n\u00e3o requer que o usu\u00e1rio defina par\u00e2metros \n\npara o funcionamento do mesmo, ao contr\u00e1rio do m\u00e9todo convencional que necessita de algumas \n\nescolhas, como a taxa de aprendizado do gradiente para atualiza\u00e7\u00e3o da matriz.  \n\n Existem v\u00e1rios trabalhos que relacionam esta poderosa ferramenta com dados de perfil de \n\npo\u00e7o [(Sancevero ET AL, 2008), (Landim, 1998), (Rosa ET AL, 2008)] e de forma robusta \n\ncomp\u00f5e um quadro j\u00e1 estabelecido desta utiliza\u00e7\u00e3o. \n\n Este presente trabalho tem como objetivo estudar mais a fundo a t\u00e9cnica de FastICA, al\u00e9m \n\nde desenvolver v\u00e1rios testes que demonstrem a efici\u00eancia do m\u00e9todo em cima de dados do \n\nCampo de Namorado, na Bacia de Campos. Com esse objetivo, foram buscados algoritmos para a \n\nutiliza\u00e7\u00e3o da An\u00e1lise de Componentes Independentes. Diversos testes foram simulados e \n\nvalidados com mudan\u00e7as de par\u00e2metros dos m\u00e9todos envolvidos, com a finalidade de observar o \n\nimpacto dessas mudan\u00e7as nos resultados observados. \n\nA forma de an\u00e1lise \u00e9 apresenta de forma quantitativa, atrav\u00e9s de gr\u00e1ficos com porcentagem \n\nde acertos dentro do testemunho dos po\u00e7os, podendo exprimir resultados que possam ser \n\ncomparados e analisados, n\u00e3o s\u00f3 pelo ponto de vista te\u00f3rico, mas tamb\u00e9m em simula\u00e7\u00f5es com \n\ndados reais. \n\n \n\n\n\n \n\n \n\n  \n\n3 \n\n \n\n \n\n \n\n \n\n \n\nCap\u00edtulo 2 \n\n  \n\nSepara\u00e7\u00e3o Cega de Sinais \n\n \n\nA Separa\u00e7\u00e3o Cega de Sinais (BSS, tradu\u00e7\u00e3o de Blind Source/Signal Separation) \u00e9 um \n\nm\u00e9todo de processamento digital de sinais que consiste em encontrar, reconhecer ou separar \n\nfontes desconhecidas de um sinal captado por algum tipo de receptor (Jutten ET AL, 1991). A \n\nqualidade ?cega? que esse m\u00e9todo carrega em seu nome, prov\u00e9m da desnecessidade de \n\nconhecimentos sobre essas fontes que se deseja encontrar. O que \u00e9 um artif\u00edcio t\u00e3o simples para a \n\ncompreens\u00e3o humana, uma vez que o c\u00e9rebro faz esse tipo de separa\u00e7\u00e3o a todo o momento, a \n\nBSS encontra v\u00e1rios desafios na \u00e1rea computacional, desde o processamento at\u00e9 a constru\u00e7\u00e3o do \n\nm\u00e9todo, o que motiva muitos estudos sobre o caso.  \n\nA separa\u00e7\u00e3o cega de sinais teve in\u00edcio nos anos 1980 com o trabalho de Christian Jutten e \n\nconsiste na separa\u00e7\u00e3o de uma mistura de sinais, em sub-sinais que comp\u00f5em a mistura inicial \n\ncaptada, com pouca, ou nenhuma informa\u00e7\u00e3o adicional sobre as fontes envolvidas ou o processo \n\nque levou a mistura desses sinais. Logo, a BSS faz parte do grupo de m\u00e9todos chamados n\u00e3o-\n\nsupervisionados, ou seja, m\u00e9todos que n\u00e3o requerem treinos e classifica\u00e7\u00f5es para serem \n\npropostos em um trabalho. \n\nUm exemplo largamente discutido e reproduzido \u00e9 a festa de coquetel (?cocktail party\u2018) \n\nilustrado na Figura 1: Em uma festa, num determinado ambiente, existem alguns microfones \n\nespalhados que captam os sons de pessoas conversando. O sinal recebido em um microfone, nada \n\nmais \u00e9 do que todo ru\u00eddo da festa mais a mistura das vozes de cada uma das pessoas, de modo \n\nque essas misturas s\u00e3o captadas de formas diferentes, pois os microfones s\u00e3o afetados de \n\nmaneiras diferentes pelos sinais originais de voz. A explica\u00e7\u00e3o para que cada microfone registre \n\numa mistura diferente est\u00e1 baseada no fato do som, como onda mec\u00e2nica, interagir com os \n\nobjetos na festa, recebendo a a\u00e7\u00e3o de reverbera\u00e7\u00e3o e/ou obst\u00e1culos, al\u00e9m da dist\u00e2ncia entre as \n\npessoas e os microfones, causando uma \u00f3bvia diferen\u00e7a entre cada receptor, dependendo da \n\nposi\u00e7\u00e3o da pessoa na festa. Dentro desse sistema apresentado, o problema passa a ser a an\u00e1lise de \n\ntodos os sinais gravados nos microfones e dessas misturas, extra\u00edrem-se os sinais originais de \n\n\n\n \n\n \n\n  \n\n4 \n\n \n\ncada interlocutor, sem ter informa\u00e7\u00f5es sobre os dados objetos da festa, ou ainda sobre a posi\u00e7\u00e3o \n\nde cada pessoa (fontes).  \n\nNesse exemplo, qualquer outro tipo de som, ou onda mec\u00e2nica, capturado pelos receptores \n\n(m\u00fasica telefones tocando,...) \u00e9 considerado ru\u00eddo.  \n\n \n\n \n\nFigura 2.1 Cocktail Party Problem \n\n \n\nDado o exemplo, podemos extrapolar essa id\u00e9ia de separa\u00e7\u00e3o de sinais para v\u00e1rias outras \n\n\u00e1reas do conhecimento, como processamento de conjunto de dados; comunica\u00e7\u00f5es multiusu\u00e1rios; \n\nreconhecimento de voz e imagem; processamento de sinais biom\u00e9dicos (Leite, 2004) e tamb\u00e9m \n\npara a Geoestat\u00edstica (Hyv\u00e4rinen, Karhunen &amp; Oja, 2001). \n\nComo o m\u00e9todo tenta separar fontes sem qualquer informa\u00e7\u00e3o pr\u00e9via, tal separa\u00e7\u00e3o pode \n\nser feita de v\u00e1rias formas, alterando-se v\u00e1rios par\u00e2metros, como por exemplo, pode-se entender \n\nque os sinais observados podem ser reais ou complexos, cont\u00ednuos ou discretos, de diferentes \n\ndistribui\u00e7\u00f5es de probabilidade, entre outras caracter\u00edsticas que influenciam nos c\u00e1lculos de BSS \n\n(Cavalcante, 2004). Pela grande abrang\u00eancia de caracter\u00edsticas que um sinal pode ter, somada a \n\ncomplexidade do problema, a Separa\u00e7\u00e3o Cega de Sinais necessita de estruturas matem\u00e1ticas n\u00e3o-\n\nlineares para que sua solu\u00e7\u00e3o contemple a perfeita resposta dos sinais-fonte. Como tais estruturas \n\nfrequentemente n\u00e3o t\u00eam solu\u00e7\u00e3o conhecida, ou necessitam de ferramentas matem\u00e1ticas \n\ndemasiadas robustas para a obten\u00e7\u00e3o da sua solu\u00e7\u00e3o, opta-se, sem perda de generalidade, por \n\nconstru\u00e7\u00f5es lineares que simplificam o sistema de equa\u00e7\u00f5es e inc\u00f3gnitas. Al\u00e9m dessa lineariza\u00e7\u00e3o \n\ndo problema, algumas outras restri\u00e7\u00f5es s\u00e3o impostas \u00e0 solu\u00e7\u00e3o, dependendo do tipo de m\u00e9todo de \n\n\n\n \n\n \n\n  \n\n5 \n\n \n\nBSS utilizado, a saber: fontes discretas, mesma distribui\u00e7\u00e3o de probabilidade, sub-gaussianas, \n\nn\u00e3o-gaussianas, independentes e estatisticamente independentes. (Cavalcante, 2004) \n\n Matematicamente, a proposta do BSS pode ser descrita como um conjunto de sinais \n\nobservados   , que na verdade s\u00e3o uma combina\u00e7\u00e3o de sinais-fontes, denotados por   e esta \n\ncombina\u00e7\u00e3o esta relacionada atrav\u00e9s de uma matriz de mistura  , podendo o sistema ter a adi\u00e7\u00e3o \n\nde algum tipo de ru\u00eddo  , isto \u00e9 \n\n       \n\nA matriz de mistura  , no caso do cocktail party, seria o conjunto de fatores que constroem \n\no sinal captado em um dado receptor. Nota-se que essa suposi\u00e7\u00e3o linear da mistura \u00e9 usada pela \n\nsimplicidade, dita acima, sem que o m\u00e9todo perca sua aplicabilidade. (Hyv\u00e4rinen, Karhunen &amp; \n\nOja, 2001). \n\nCaso a matriz de mistura A fosse conhecida, o problema seria, simplesmente, resolver o \n\nsistema das equa\u00e7\u00f5es envolvidas, n\u00e3o sendo necess\u00e1rio nenhum tipo de m\u00e9todo de resolu\u00e7\u00e3o. \n\nContudo, como n\u00e3o existem informa\u00e7\u00f5es sobre essa matriz A, o BSS dever\u00e1 utilizar-se do \n\nconjunto de sinais observados para descobrir quais s\u00e3o os sinais-fonte. \n\nO objetivo desse m\u00e9todo \u00e9 encontrar os sinais S, que est\u00e3o relacionados com os sinais \n\ncaptados  , atrav\u00e9s da matriz        , de forma que       . Essa matriz   \u00e9 chamada de \n\nMatriz de separa\u00e7\u00e3o e \u00e9 uma aproxima\u00e7\u00e3o da matriz pseudo-inversa    , pois como   pode ter \n\nqualquer tipo de dimens\u00e3o, n\u00e3o necessariamente quadrada, n\u00e3o tendo, portando, uma verdadeira \n\nmatriz inversa. \n\nA invers\u00e3o da matriz A \u00e9 sempre poss\u00edvel quando alguma caracter\u00edstica estat\u00edstica \u00e9 \n\naplicada. Por exemplo, no caso da An\u00e1lise de Componentes Independentes (ICA), como o \n\npr\u00f3prio nome denota, os sinais s\u00e3o estaticamente independentes, e se A \u00e9 uma matriz quadrada, \n\nseu determinante \u00e9 diferente de zero, logo A \u00e9 invert\u00edvel. Se A n\u00e3o \u00e9 uma matriz quadrada, o \n\nc\u00e1lculo \u00e9 feito atrav\u00e9s da matriz pseudo-inversa. Quanto \u00e0 parcela que cabe ao ru\u00eddo, ela esta \n\nembutida nessa proposta, caso este seja tratado como uma fonte. Caso n\u00e3o seja, quando o vetor \n\nru\u00eddo r \u00e9 considerado de forma aditiva no sistema de equa\u00e7\u00f5es,este n\u00e3o fornece dificuldades para \n\na invers\u00e3o da matriz A, pois o ru\u00eddo teria dimensionalidade compat\u00edvel com a do sistema, e a \n\nsoma e subtra\u00e7\u00e3o de matrizes \u00e9 uma opera\u00e7\u00e3o simples e que n\u00e3o muda essa tal dimensionalidade. \n\n A dimens\u00e3o do sistema influencia no tipo de resolu\u00e7\u00e3o que o m\u00e9todo tem como resposta, \n\npois a constru\u00e7\u00e3o do sistema \u00e9 dependente do n\u00famero de captadores e do n\u00famero de sinais-fonte \n\n\n\n \n\n \n\n  \n\n6 \n\n \n\nque est\u00e3o colaborando para os sinais registrados. Sem uma an\u00e1lise profunda da matriz de mistura \n\nA, e levando em conta a matem\u00e1tica dessas fun\u00e7\u00f5es e o n\u00famero de fontes e de receptores, podem-\n\nse dividir as situa\u00e7\u00f5es em tr\u00eas possibilidades plaus\u00edveis: \n\n \n\nA) N\u00famero de Receptores menor que o N\u00famero de Fontes \n\nTamb\u00e9m conhecido como caso sub-determinando. Quando o n\u00famero de sinais originais \u00e9 \n\nmaior do que o n\u00famero de receptores, na constru\u00e7\u00e3o de equa\u00e7\u00f5es, haveria menos \n\nequa\u00e7\u00f5es ( ) que inc\u00f3gnitas ( ), logo nosso sistema \u00e9 considerado incompleto. \n\nMatematicamente ainda \u00e9 poss\u00edvel em alguns casos, construir uma no\u00e7\u00e3o de recorr\u00eancia \n\npara determinar as fontes faltantes, mas talvez sem um significado f\u00edsico. Um m\u00e9todo \n\npara estimar essas fontes seria atrav\u00e9s da pseudo-inversa da matriz de misturas, ou outras \n\nt\u00e9cnicas mais sofisticadas (Hyv\u00e4rinen, Karhunen &amp; Oja, 2001). \n\n  \n\nB) N\u00famero de Receptores maior que o N\u00famero de Fontes \n\nConhecido como caso sobre-determinado. Quando o n\u00famero de sinais \u00e9 menor que o \n\nn\u00famero de receptores, a constru\u00e7\u00e3o de equa\u00e7\u00f5es fica com um n\u00famero maior de equa\u00e7\u00f5es \n\n( ) que inc\u00f3gnitas ( ). De fato, se as equa\u00e7\u00f5es n\u00e3o forem dependentes ou n\u00e3o se tratar de \n\num sistema imposs\u00edvel, a resolu\u00e7\u00e3o \u00e9 a trivial do m\u00e9todo, j\u00e1 que existe um n\u00famero de \n\ndados maiores que o necess\u00e1rio para a constru\u00e7\u00e3o do sistema de equa\u00e7\u00f5es. \n\n  \n\nC) N\u00famero de Receptores igual ao n\u00famero de fontes \n\n\u00c9 o caso cl\u00e1ssico da resolu\u00e7\u00e3o de BSS. Com o mesmo n\u00famero de receptores e de fontes, o \n\nsistema de equa\u00e7\u00f5es \u00e9 um sistema de equa\u00e7\u00f5es tamb\u00e9m com a mesma dimens\u00e3o, logo   \u00e9 \n\nquadrada e   tamb\u00e9m, deixando o problema envolvido       , mais simples e com \n\numa solu\u00e7\u00e3o bem definida, desde que as fontes n\u00e3o permitam graus de liberdade e \n\ndepend\u00eancia. \n\n  \n\n V\u00e1rios m\u00e9todos s\u00e3o usados para a Separa\u00e7\u00e3o Cega de Sinais, entre os mais conhecidos \n\nest\u00e3o a An\u00e1lise de Componentes Principais (PCA), An\u00e1lise de Componentes Independentes \n\n(ICA). Existem ainda as Redes Neurais, como os Mapas Auto-organiz\u00e1veis(SOM, tradu\u00e7\u00e3o de \n\nSelf Organizing Maps) (Kohonen, 1997). \n\n\n\n \n\n                                                  \n\n  \n\n7 \n\n \n\n \n\n \n\n \n\n \n\nCap\u00edtulo 3 \n\n \n\nAn\u00e1lise de Componentes Principais \n\n \n\nA An\u00e1lise de Componentes Principais (PCA, tradu\u00e7\u00e3o de Principal Component Analysis) \u00e9 \n\num dos m\u00e9todos mais conhecidos para o problema da Separa\u00e7\u00e3o Cega de Sinais e \u00e9 muito \n\nutilizada na \u00e1rea de processamento de dados que necessitam de extra\u00e7\u00e3o de redund\u00e2ncias \n\n(dimens\u00f5es dependentes) de algum conjunto de informa\u00e7\u00f5es (Hyv\u00e4rinen ET AL, 2001). Essas \n\nextra\u00e7\u00f5es podem ser entendidas como uma redu\u00e7\u00e3o de dimensionalidade dos dados, caso seja \n\nnecess\u00e1rio diminuir a carga de informa\u00e7\u00e3o dispon\u00edvel. \n\nSeja  , um vetor aleat\u00f3rio com   elementos, de forma que sejam conhecidos   elementos \n\ndesse vetor, por exemplo,            A primeira hip\u00f3tese que deve ser considerada sobre o vetor \n\n , para a utiliza\u00e7\u00e3o do PCA, \u00e9 de que esses vetores, necessariamente, devam apresentar alguma \n\nredund\u00e2ncia, ou seja, devam ser mutuamente correlacionados. No caso de elementos \n\nindependentes (a hip\u00f3tese do ICA), o PCA n\u00e3o consegue resultados. \n\nPara o in\u00edcio do m\u00e9todo, os dados s\u00e3o centralizados e \u00e9 aplicado um pr\u00e9-processamento \n\nchamado Branqueamento (Ap\u00eandice A). Depois desse pr\u00e9-processamento, a An\u00e1lise de \n\nComponentes Principais aplica uma mudan\u00e7a de base que mantenha as caracter\u00edsticas espaciais \n\ndo vetor aleat\u00f3rio  , agora padronizado e sem redund\u00e2ncias. Nessa O vetor   apresenta    \n\nelementos, onde    , pois o vetor   n\u00e3o apresenta redund\u00e2ncia (correla\u00e7\u00e3o) entre seus termos \n\ne tem dimens\u00e3o menor que de  . \n\nEssa mudan\u00e7a de base se d\u00e1 pela busca de um novo conjunto de coordenadas, onde   \n\nmantenha as caracter\u00edsticas espaciais e estruturais de  , entretanto em um espa\u00e7o de menor \n\ndimens\u00e3o. \n\nSe   \u00e9 uma poss\u00edvel representa\u00e7\u00e3o sem redund\u00e2ncias, seus elementos s\u00e3o n\u00e3o-\n\ncorrelacionados, ou seja, as proje\u00e7\u00f5es dos elementos de   nesse novo espa\u00e7o s\u00e3o n\u00e3o-\n\ncorrelacionados e ortogonais. Al\u00e9m disso, as vari\u00e2ncias desses elementos s\u00e3o maximizadas uma a \n\numa, de forma que a primeira componente principal tenha a m\u00e1xima vari\u00e2ncia, a segunda \n\n\n\n \n\n                                                  \n\n  \n\n8 \n\n \n\ncomponente principal tenha a segunda maior vari\u00e2ncia, assim por diante, at\u00e9 que a m-\u00e9sima \n\ncomponente principal tenha a menor vari\u00e2ncia. \n\nMatematicamente, existem in\u00fameras formas de se encontrar esse novo sistema de \n\ncoordenadas com os mais variados m\u00e9todos estat\u00edsticos, entre eles, algoritmos diretos \n\n(Maximiza\u00e7\u00e3o de Vari\u00e2ncia, Compress\u00e3o de erro de quadrados m\u00e9dios m\u00ednimos) (Jolliffe, 2002) \n\ne algoritmos de aprendizado (Gradiente Ascendente Estoc\u00e1stico) (Gausch, 1982).  \n\n \n\n3.1 PCA atrav\u00e9s da Maximiza\u00e7\u00e3o da Vari\u00e2ncia \n\n Utilizando a nota\u00e7\u00e3o anterior, uma poss\u00edvel combina\u00e7\u00e3o linear dos elementos de        \u00b7, \u00e9   \n\n            \n  \n\n \n\n   \n\n \n\nonde           s\u00e3o coeficientes ou pesos do vetor    com dimens\u00e3o de tamanho n. Como a \n\nPCA \u00e9 um m\u00e9todo iterativo, o vetor    \u00e9 um chute inicial e aleat\u00f3rio, a ser atualizado a cada \n\nitera\u00e7\u00e3o buscando sua converg\u00eancia final, da onde sair\u00e3o as coordenadas da nova base. \n\n Se a vari\u00e2ncia de   \u00e9 m\u00e1xima, ent\u00e3o    \u00e9 a primeira componente principal de x. Como a \n\nvari\u00e2ncia depende da norma e orienta\u00e7\u00e3o do vetor de pesos      isto \u00e9, a vari\u00e2ncia cresceria sem \n\nlimites, conforme a norma fosse aumentando, \u00e9 imposto, sem perda de generalidade, que a norma \n\nde    seja constante e igual a 1. Com o vetor   maximizando o crit\u00e9rio de PCA, \n\n  \n             \n\n        \n        \n\n             \n      \n\nOnde   \u00e9 a Negentropia envolvida, um termo que ser\u00e1 mais bem desenvolvido no Cap\u00edtulo \n\n4, E[x] \u00e9 a esperan\u00e7a sobre a fun\u00e7\u00e3o de densidade de probabilidade do vetor x, a norma de    \u00e9 a \n\nnorma euclidiana usual  e    \u00e9 a matriz       de covari\u00e2ncias do vetor x. \n\nNesse novo espa\u00e7o, os vetores s\u00e3o ortonormais, pois s\u00e3o ortogonais com norma igual a 1, \n\nent\u00e3o os autovalores         satisfazem a ordem crescente              e a solu\u00e7\u00e3o \n\nmaximizada do PCA se d\u00e1 por \n\n       \n\n onde    \u00e9 o vetor can\u00f4nico da base do sistema de coordenadas sem redund\u00e2ncia. \n\nConsequentemente,    \u00e9 ortonormal a todos os outros vetores e o autovalor    \u00e9 m\u00e1ximo \n\nentre os autovalores que restaram, portanto \n\n       \n\n\n\n \n\n                                                  \n\n  \n\n9 \n\n \n\nRecursivamente, segue que \n\n       \n\ne a k-\u00e9sima Componente Principal \u00e9 dada por \n\n      \n   \n\n \n\n3.2 PCA atrav\u00e9s da compress\u00e3o do erro das m\u00e9dias quadr\u00e1ticas m\u00ednimas \n\nAssim como no exemplo anterior, nesse m\u00e9todo de PCA, as componentes principais s\u00e3o \n\ndefinidas como somas com pesos onde a vari\u00e2ncia \u00e9 m\u00e1xima. Dessa forma, essas componentes, \n\nse normalizadas, formam uma base de m vetores ortonormais do subespa\u00e7o de dimens\u00e3o m. \n\nDenotando os vetores bases, mais uma vez como        , a proje\u00e7\u00e3o de x no subespa\u00e7o \n\nque o abrange \u00e9     \n  \n\n       . O crit\u00e9rio do erro das m\u00e9dias quadr\u00e1ticas (MSE, sigla do ingl\u00eas \n\nMean-square Error), para ser minimizado deve atender a \n\n    \n              \n\n     \n \n\n   \n\n    \n\nComo os vetores s\u00e3o ortonormais, eles tamb\u00e9m s\u00e3o n\u00e3o correlacionados, e dessa forma, \n\npodemos reescrever o crit\u00e9rio MSE como \n\n    \n                   \n\n   \n \n\n   \n\n    \n\n                \n     \n\n \n    \n\nDe acordo com (Diamantaras, 1996), de fato a equa\u00e7\u00e3o anterior tem como resultados os m \n\nvetores can\u00f4nicos da base que procuramos. \n\n \n\n3.3 PCA atrav\u00e9s do algoritmo de gradiente ascendente estoc\u00e1stico \n\nDiferentemente dos dois m\u00e9todos anteriores para encontrar as componentes principais, esse \n\nm\u00e9todo apresenta uma regra de aprendizado, muito comum em processos que necessitam de \n\nalgum tipo de atualiza\u00e7\u00e3o para cada passo que o m\u00e9todo procura as componentes. \n\nSe a primeira componente principal    atende a hip\u00f3tese de que         , a regra de \n\naprendizado do m\u00e9todo \u00e9 \n\n                                    \n          \n\nonde      \u00e9 a taxa de aprendizado que controla a velocidade com que o gradiente converge. \n\n\n\n \n\n                                                  \n\n  \n\n10 \n\n \n\nTanto a taxa     \u00b7, quanto \u00e0 pr\u00f3pria converg\u00eancia do m\u00e9todo, tem seus detalhes e \n\ncaracter\u00edsticas garantidas por (Oja, 1995), e ap\u00f3s alguma algebriza\u00e7\u00e3o, pode-se reescrever a regra \n\nde aprendizado como \n\n              \n     \n\nA nomenclatura do m\u00e9todo Gradiente Ascendente Estoc\u00e1stico (SGA, do ingl\u00eas stochastic \n\ngradient ascent) \u00e9 justificada, pois o gradiente n\u00e3o diz respeito \u00e0 vari\u00e2ncia     \n  , mas sim ao \n\nvalor rand\u00f4mico instant\u00e2neo   \n . Matematicamente, isso corresponde a uma aproxima\u00e7\u00e3o \n\nestoc\u00e1stica (Kushner, 1978).  \n\nAtrav\u00e9s da aproxima\u00e7\u00e3o estoc\u00e1stica com respeito ao vetor   , o gradiente   \n  \u00e9 aproximado \n\ne o m\u00e9todo de algoritmo de gradiente ascendente estoc\u00e1stico, termina como \n\n                       \n   \n\n  \n\nOnde do lado direito da express\u00e3o o termo    , \u00e9 tamb\u00e9m conhecido como termo Hebbiano. \n\nO termo    , assim como o termo Hebbiano tem a converg\u00eancia garantida por (Oja, 1997). Os \n\noutros termos da equa\u00e7\u00e3o s\u00e3o restritos devidos as condi\u00e7\u00f5es da ortonormalidade, logo, o m\u00e9todo \n\ncomo um todo, apresentada converg\u00eancia demonstrada por (Oja, 1982). Quando    , o m\u00e9todo \n\ntem regra de aprendizado unidimensional, quando    , a regra tem bidimensionalidade, assim \n\npor diante, at\u00e9 que se    , a regra de aprendizado \u00e9 n-dimensional. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n                                                  \n\n  \n\n11 \n\n \n\n \n\n \n\n \n\nCap\u00edtulo 4 \n\n \n\nAn\u00e1lise de Componentes Independentes \n\n \n\nA an\u00e1lise de componentes independentes (ICA, do ingl\u00eas Independent Component \n\nAnalysis) \u00e9 um m\u00e9todo estat\u00edstico computacional, que \u00e9 usado para a separa\u00e7\u00e3o cega de sinais, \n\nassim como a an\u00e1lise de componentes principais (PCA, do ingl\u00eas Principal Component Analysis). \n\nDe fato, seu m\u00e9todo \u00e9 muito parecido com o m\u00e9todo PCA, onde alguns autores (Hyv\u00e4rinen, \n\nFastICA 2.5) aplicam PCA nos dados antes de aplicar o ICA, como um pr\u00e9-processamento. \n\nApesar dos dois m\u00e9todos basearem-se em uma estrutura semelhante, sua diferen\u00e7a tem origem na \n\nnatureza dos dados: a correla\u00e7\u00e3o e independ\u00eancia das informa\u00e7\u00f5es. No caso da aplica\u00e7\u00e3o da \n\nan\u00e1lise de componentes principais, \u00e9 necess\u00e1rio que os dados apresentem algum tipo de \n\ncorrela\u00e7\u00e3o (redund\u00e2ncia), ou seja, dentro dos vetores que comp\u00f5em a base dos dados, pelo menos \n\num deles \u00e9 uma combina\u00e7\u00e3o de outros vetores dessa base.  \n\nJ\u00e1 no caso da an\u00e1lise de componentes independentes, a hip\u00f3tese levantada \u00e9 justamente a \n\noposta: os dados devem ser mutuamente independentes.  \n\nApenas com essa mudan\u00e7a de hip\u00f3tese, caem algumas restri\u00e7\u00f5es da An\u00e1lise de \n\nComponentes Principais, como a necessidade de depend\u00eancia inicial dos dados. De fato, como o \n\nICA tem menos restri\u00e7\u00f5es, um maior n\u00famero de dados pode receber sua aplica\u00e7\u00e3o. A hip\u00f3tese \n\nonde as componentes buscadas s\u00e3o independentes abre um grande leque de propriedades \n\nestat\u00edsticas que contribuem para o c\u00e1lculo das componentes, como por exemplo, a possibilidade \n\nde se trabalhar com estat\u00edsticas de ordem superior a estat\u00edsticas de segunda ordem. (Comon, \n\n1994) \n\nO largo uso das separa\u00e7\u00f5es cegas de sinais justifica a quantidade grande de m\u00e9todos que \n\ngarantem o sucesso, e a escolha para esse m\u00e9todo pode associar-se a uma escolha emp\u00edrica com \n\nbase na observa\u00e7\u00e3o dos dados. Como, por exemplo, no caso do ?Cocktail Party Problem?, em que \n\nduas pessoas conversando geram sinais independentes, afinal, uma pessoa, mesmo que esteja \n\nfalando diretamente com outra pessoa, n\u00e3o produz informa\u00e7\u00f5es sobre como ser\u00e1 o sinal emitido \n\n\n\n \n\n                                                  \n\n  \n\n12 \n\n \n\npor esta \u00faltima. Empiricamente ainda, \u00e9 comum considerar que sinais f\u00edsicos tenham natureza \n\nindependente, usando a mesma hip\u00f3tese do exemplo supracitado. \n\n \n\n4.1 Conceito Matem\u00e1tico da An\u00e1lise de Componentes independentes \n \n\nAssim como a An\u00e1lise de Componentes Principais (PCA), a an\u00e1lise de componentes \n\nindependentes (ICA), busca o conjunto de   sinais emitidos por fontes independentes   \n\n            que s\u00e3o capturados por   receptadores              , onde todos os elementos \n\nde   \u00e9 uma mistura dos   elementos de  . Sem perda de generalidade, assim como feito na \n\nsuposi\u00e7\u00e3o inicial da separa\u00e7\u00e3o cega de sinais, caso essa mistura seja linear, os elementos de s e x \n\nest\u00e3o interagindo atrav\u00e9s dos elementos           , de forma que  \n\n     \n\nO objetivo do problema \u00e9 encontrar os elementos de  , ou seja, encontrar os elementos \n\n             \n   que concebem as componentes independentes em \n\n     \n\n  Quest\u00f5es de dimensionalidade foram previamente discutidas dentro do Cap\u00edtulo 1. Apenas \n\npor simplicidade, nos c\u00e1lculos para a solu\u00e7\u00e3o da an\u00e1lise de componentes independentes, ser\u00e1 \n\nconsiderado o caso em que o n\u00famero de fontes \u00e9 o mesmo de receptores, ou seja,     \n\ndeixando id\u00eanticas as dimens\u00f5es das matrizes   e  . Como j\u00e1 explanado, apenas nesse caso,   \u00e9 \n\na inversa, propriamente dita, da matriz de mistura  .  \n\nEm uma visualiza\u00e7\u00e3o matricial, se o problema inicial est\u00e1 parametrizado por uma vari\u00e1vel \n\nde tempo  , tem-se \n\n \n     \n\n \n     \n\n   \n\n       \n   \n\n       \n   \n\n     \n \n\n     \n  \n\n \n\ncuja respectiva solu\u00e7\u00e3o \u00e9   \n\n \n     \n\n \n     \n\n   \n\n       \n   \n\n       \n   \n\n     \n \n\n      \n  \n\n \n\nDessa configura\u00e7\u00e3o, percebe-se que o problema est\u00e1 baseado na resolu\u00e7\u00e3o do sistema \n\nlinear \n\n\n\n \n\n                                                  \n\n  \n\n13 \n\n \n\n                                     \n\n                                     \n\n  \n\n                                     \n\nSem informa\u00e7\u00f5es pr\u00e9vias sobre a natureza dos dados, traduzidos no formato da matriz de \n\nmistura, muitas vezes, a \u00fanica solu\u00e7\u00e3o plaus\u00edvel para esse tipo de problema, seria uma infinita \n\nbateria de testes com as mais variadas configura\u00e7\u00f5es buscando a solu\u00e7\u00e3o do sistema de equa\u00e7\u00f5es. \n\nDefinitivamente, esse n\u00e3o \u00e9 um processamento de dados interessante, e uma solu\u00e7\u00e3o poss\u00edvel \n\npara esse problema, pode ser calculada atrav\u00e9s da an\u00e1lise de componentes independentes se \n\nconsiderarmos que os sinais    s\u00e3o n\u00e3o-gaussianos. Isso basta para encontrar os elementos     \n\nda matriz   e a solu\u00e7\u00e3o do sistema linear, conhecendo ent\u00e3o as solu\u00e7\u00f5es              . Em \n\nalguns casos, faz-se necess\u00e1rio a multiplica\u00e7\u00e3o de    por um escalar       para encontrar o \n\nvalor exato da fonte original, j\u00e1 que na solu\u00e7\u00e3o levantada pelo ICA, n\u00e3o s\u00e3o feitas suposi\u00e7\u00f5es \n\nquanto \u00e0 amplitude do sinal.  \n\nCaso n\u00e3o fosse feita a suposi\u00e7\u00e3o das componentes serem n\u00e3o-gaussianas, surgem \n\nproblemas espaciais e de distribui\u00e7\u00e3o (Hyv\u00e4rinen, 2001). Por exemplo, caso duas componentes \n\nindependentes    e    tenham uma fun\u00e7\u00e3o densidade de probabilidade (fdp) conjunta gaussiana.  \n\nAs fun\u00e7\u00f5es densidade de probabilidade marginais de    e de    s\u00e3o denotadas por \n\n                     \n\n                     \n\ncomo    e    s\u00e3o independentes por hip\u00f3tese, a fdp conjunta dessas duas componentes \u00e9 dada por \n\n                       \n\nLogo, se a fdp conjunta de    e   \u00e9 gaussiana,  \n\n         \n \n\n  \n      \n\n  \n     \n\n \n\n \n   \n\n \n\n  \n      \n\n     \n\n \n  \n\ncomo a matriz   \u00e9 ortogonal, devido ao branqueamento dos dados (pr\u00e9-processamento), ela \n\natende a seguinte propriedade \n\n       \n\ne a fdp conjunta das misturas    e    \u00e9 dada por (Papoulis, 1991) \n\n\n\n \n\n                                                  \n\n  \n\n14 \n\n \n\n          \n \n\n  \n      \n\n       \n\n \n         \n\nMas como   \u00e9 ortogonal              e                  , a fdp conjunta das \n\nmisturas pode ser resumida em  \n\n          \n \n\n  \n      \n\n     \n\n \n  \n\nFica claro ent\u00e3o que as misturas    e    e as componentes independentes    e    t\u00eam a \n\nmesma estrutura espacial, chegando \u00e0 conclus\u00e3o que a matriz de mistura   n\u00e3o tem efeito sobre \n\nos dados, n\u00e3o podendo ser poss\u00edvel encontrar componentes independentes que componham uma \n\nnova base de vetores com dimens\u00e3o reduzida. \n\nAs condi\u00e7\u00f5es abordadas at\u00e9 agora s\u00e3o prim\u00e1rias para o funcionamento do m\u00e9todo da \n\nan\u00e1lise de componentes independentes. Com tais condi\u00e7\u00f5es consolidadas, o desafio do m\u00e9todo \n\nest\u00e1 em encontrar as componentes independentes menos-gaussianas poss\u00edveis.  J\u00e1 foram \n\nestudados v\u00e1rios processos matem\u00e1ticos e estat\u00edsticos para o c\u00e1lculo dessas Componentes \n\nIndependentes: atrav\u00e9s da curtose e negentropia; atrav\u00e9s do Estimador de M\u00e1xima Probabilidade \n\ne atrav\u00e9s da Minimiza\u00e7\u00e3o de Informa\u00e7\u00e3o M\u00fatua (Hyv\u00e4rinen, 2001). Nas pr\u00f3ximas se\u00e7\u00f5es \n\nabordaremos tais m\u00e9todos, entretanto para os testes ser\u00e3o utilizados os m\u00e9todos de FastICA \n\natrav\u00e9s de negentropia. \n\n \n\n4.2 Converg\u00eancia atrav\u00e9s de curtose e negentropia \n \n\nQuando a converg\u00eancia \u00e9 feita atrav\u00e9s de curtose ou negentropia, podem-se dividir os \n\nm\u00e9todos em dois grandes grupos: os que calculam uma Componente Independente por vez e \n\naqueles que calculam v\u00e1rias componentes independentes simultaneamente.  \n\nPor sua vez, para o c\u00e1lculo de uma componente por vez, existem tr\u00eas alternativas mais \n\ncorriqueiras: C\u00e1lculo de N\u00e3o-Gaussianidade, Algoritmo de Gradiente e o Algoritmo R\u00e1pido de \n\nPonto - fixo. Cada um destes tr\u00eas m\u00e9todos pode ter sua converg\u00eancia calculada atrav\u00e9s de duas \n\nmedidas estat\u00edsticas: via curtose ou via negentropia. \n\nPor outro lado, quando o c\u00e1lculo baseia-se na escolha de v\u00e1rias componentes \n\nindependentes simultaneamente, os m\u00e9todos mais conhecidos s\u00e3o a Ortogonaliza\u00e7\u00e3o \n\ndeflacion\u00e1ria e Ortogonaliza\u00e7\u00e3o sim\u00e9trica (Hyv\u00e4rinen, 1997). \n\n\n\n \n\n                                                  \n\n  \n\n15 \n\n \n\n4.2.1 Converg\u00eancia do m\u00e9todo de uma componente por vez \n \n\nQuando o processo estat\u00edstico ou matem\u00e1tico encontra uma componente independente \n\npara cada itera\u00e7\u00e3o, a converg\u00eancia \u00e9 chamada de unit\u00e1ria ou uma-a-uma, como \u00e9 informalmente \n\nconhecida. \n\nSeu funcionamento est\u00e1 baseado na maximiza\u00e7\u00e3o da n\u00e3o-gaussianidade, e a converg\u00eancia \n\ndesse processo gera a Primeira Componente Independente. Essa componente \u00e9 a mais n\u00e3o-\n\ngaussiana poss\u00edvel dentro dos dados.  Depois de encontrada a primeira componente n\u00e3o \n\nindependente, o m\u00e9todo procura outra componente, ortogonal a primeira e com a m\u00e1xima n\u00e3o-\n\ngaussianidade poss\u00edvel, quando encontrada, essa corresponde a Segunda Componente \n\nIndependente. Similarmente, a  -\u00e9sima componente encontrada durante o m\u00e9todo \u00e9 ortogonal as \n\n    componentes anteriores e suas gaussianidades s\u00e3o decrescentes. \n\n4.2.1.1 N\u00e3o-gaussianidade atrav\u00e9s de curtose \n \n\nA curtose (Ap\u00eandice A) ou seu valor absoluto s\u00e3o usados em larga escala como medidas \n\nde n\u00e3o-gaussianidade na an\u00e1lise de componentes independentes. De fato, como visto acima, uma \n\nsimples an\u00e1lise pode ajudar a descartar poss\u00edveis vari\u00e1veis, j\u00e1 que a curtose nula \u00e9 geralmente \n\nassociada a distribui\u00e7\u00f5es gaussianas. \n\nAl\u00e9m dessa an\u00e1lise superficial, a curtose pode agir na escolha das componentes. Se o \n\nmodelo de ICA     , admite   componentes independentes   , com vari\u00e2ncia unit\u00e1ria, a \n\nprimeira componente   deve ter uma resposta que seja compat\u00edvel com a estrutura      , pois \n\nest\u00e1 sofrer\u00e1 uma transforma\u00e7\u00e3o linear. \n\nUma mudan\u00e7a de base plaus\u00edvel pode ser descrita como      , logo, substituindo as \n\nequa\u00e7\u00f5es, o problema pode ser proposto como       . \n\nDevido \u00e0 linearidade da curtose (Nandi, 1999), tem-se \n\n                                   \n          \n\n \n\n   \n\n \n\n \n\n   \n\n \n\ne, por simplicidade, tomando-se kurt      , \n\n           \n  \n\n \n\n   \n\n \n\n\n\n \n\n                                                  \n\n  \n\n16 \n\n \n\ne         passa a ser um problema de otimiza\u00e7\u00e3o (Hill ET AL.2007), pois as curvas geradas nas \n\n  dimens\u00f5es ter\u00e3o a n\u00e3o-gaussianidade maximizada, quando suas proje\u00e7\u00f5es forem vetores \n\nformadores dessa nova base.  \n\n \n\n \n\n \n\n4.2.1.2 Algoritmo de Gradiente atrav\u00e9s de curtose \n \n\nDe uma forma geral, para maximizar a curtose, dentro de uma amostra            ,deve \n\nser computado o valor da curtose de        em que esta tem maior taxa de crescimento,onde  \n\n  \u00e9 um vetor de mistura.  Esse \u00e9 um problema correlato de m\u00e9todos de Gradiente (Stewart, 2005), \n\ne pode ser descrito como (Hyv\u00e4rinen, 2001) \n\n              \n\n  \n                                       \n\nO \u00faltimo termo dentro dos colchetes tem efeito apenas na norma, e n\u00e3o afeta o algoritmo \n\nde gradiente, logo pode ser desconsiderado. Al\u00e9m disso, como os dados s\u00e3o branqueados, \n\n               . Ent\u00e3o podemos reescrever o algoritmo do gradiente como \n\n                            \n\n         \n\nagindo como uma vers\u00e3o iterativa e adaptativa.  \n\n4.2.1.3 Algoritmo R\u00e1pido de Ponto - Fixo atrav\u00e9s de curtose \n \n\nO algoritmo via Gradiente com seu modelo final de aprendizado, gera uma r\u00e1pida \n\nadapta\u00e7\u00e3o nos ambientes n\u00e3o-estacion\u00e1rios. Entretanto, sua converg\u00eancia \u00e9 lenta e depende de \n\numa boa escolha da sequ\u00eancia de taxa de aprendizado. \u00c9 comum que uma escolha ruim da taxa \n\nde aprendizado, impossibilite o m\u00e9todo de convergir em componentes independentes.  Uma \n\nitera\u00e7\u00e3o com ponto fixo pode ajudar com esses problemas. Nesse caso, como o gradiente deve \n\nestar apontando para  , a reposta esperada deve ser igual a um escalar que multiplique por  . De \n\nacordo com a t\u00e9cnica de Multiplicadores de Lagrange (Stewart, 2005), a converg\u00eancia \u00e9 \n\ngarantida. Derivando a equa\u00e7\u00e3o de    , temos \n\n                       \n\n\n\n \n\n                                                  \n\n  \n\n17 \n\n \n\nSugerindo um algoritmo de ponto - fixo, onde o lado direito da equa\u00e7\u00e3o \u00e9 calculado e \n\npassa a corresponder ao novo valor de  . Como         \n\n                 \n\nPodemos notar que o valor novo e o valor velho de   apontam para a mesma dire\u00e7\u00e3o, \n\nlogo n\u00e3o necessariamente ele converge para um \u00fanico ponto, j\u00e1 que   e \u2013   definem a mesma \n\ndire\u00e7\u00e3o. Isso corrobora a afirma\u00e7\u00e3o anterior de que os m\u00e9todos de ICA podem necessitar da \n\nmultiplica\u00e7\u00e3o de um escalar para resgatar sinais originais id\u00eanticos. \n\nEsse m\u00e9todo \u00e9 t\u00e3o eficiente que \u00e9 chamado de FastICA( Hyv\u00e4rinen,1997). Suas \n\npropriedades o tornam claramente superior aos m\u00e9todos anteriores, pois al\u00e9m de sua \n\nconverg\u00eancia ser C\u00fabica (converge muito mais r\u00e1pido), a desnecessidade de par\u00e2metros para as \n\nitera\u00e7\u00f5es, elimina qualquer erro de converg\u00eancia, caso fosse necess\u00e1rio escolher uma taxa inicial \n\npara o m\u00e9todo. \n\nNa verdade, esse m\u00e9todo \u00e9 t\u00e3o efetivo que \u00e9 utilizado nas maiorias dos pacotes de an\u00e1lise \n\nde componentes independentes, devido a sua efici\u00eancia e rapidez. Os resultados deste presente \n\ntrabalho tamb\u00e9m foram computados utilizando-se desse m\u00e9todo.  \n\nA negentropia \u00e9 o m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-gaussianidade mais completo e confi\u00e1vel, \n\nsendo definido por (Hyv\u00e4rinen, 2001) como o estimador \u00f3timo para medi\u00e7\u00e3o de n\u00e3o-\n\ngaussianidade. Entretanto, seu custo computacional \u00e9 extremamente elevado e necessita de \n\nestimadores n\u00e3o-param\u00e9tricos para sua utiliza\u00e7\u00e3o. Simplifica\u00e7\u00f5es do m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-\n\ngaussianidade atrav\u00e9s de negentropia excluem as maiores dificuldades do m\u00e9todo e s\u00e3o \n\namplamente utilizadas.  \n\n4.2. 1.4 N\u00e3o-gaussianidade atrav\u00e9s de negentropia \n \n\nUma primeira aproxima\u00e7\u00e3o para a negentropia \u00e9 a utiliza\u00e7\u00e3o de cumulantes de ordem \n\nsuperior, de mesmo aspecto da resolu\u00e7\u00e3o do m\u00e9todo via curtose, \n\n      \n \n\n  \n         \n\n \n\n  \n          \n\nEntretanto, se   tem uma distribui\u00e7\u00e3o aproximadamente sim\u00e9trica, ent\u00e3o       ser\u00e1 \n\naproximadamente zero e a aproxima\u00e7\u00e3o de negentropia fica dependente apenas da curtose. \n\nOutra aproxima\u00e7\u00e3o mais completa do ponto de vista da negentropia \u00e9 necess\u00e1ria. Outra \n\naproxima\u00e7\u00e3o \u00e9 substituir termos de ordem superior         por fun\u00e7\u00f5es n\u00e3o quadr\u00e1ticas   , e \n\naproximar a negentropia atrav\u00e9s de suas esperan\u00e7as. Sem perda de generalidade, podem-se \n\n\n\n \n\n                                                  \n\n  \n\n18 \n\n \n\nescolher duas fun\u00e7\u00f5es n\u00e3o-quadr\u00e1ticas    e    de forma que a primeira \u00e9 \u00edmpar e a segunda par. \n\nA aproxima\u00e7\u00e3o da negentropia \u00e9 dada por \n\n                 \n                         \n\n  \n\nonde    e    s\u00e3o constantes positivas e   \u00e9 uma vari\u00e1vel gaussiana normalizada. \n\nPode-se utilizar ainda a aproxima\u00e7\u00e3o com apenas uma fun\u00e7\u00e3o n\u00e3o-quadr\u00e1tica   \n\n                         \n\ndesde que a fun\u00e7\u00e3o   n\u00e3o tenha distribui\u00e7\u00e3o sim\u00e9trica, ou a aproxima\u00e7\u00e3o      depender\u00e1 mais \n\numa vez apenas da curtose. \n\nComputacionalmente, fun\u00e7\u00f5es n\u00e3o-quadr\u00e1ticas com crescimento n\u00e3o muito r\u00e1pido, \n\ncomp\u00f5em estimadores mais robustos e com boa taxa de converg\u00eancia, como por exemplo  \n\n       \n \n\n  \n           \n\n              \n     \n\nonde        , geralmente escolhida igual a um. \n\n4.2. 1.5 Algoritmo de Gradiente atrav\u00e9s de negentropia \n \n\nAssim como o Algoritmo de Gradiente atrav\u00e9s de curtose, uma aproxima\u00e7\u00e3o do modelo \n\nde gradiente para a negentropia                         , com respeito \u00e0   e levando em \n\nconsidera\u00e7\u00e3o a normaliza\u00e7\u00e3o                 , pode ser descrita como \n\n                 \n\n         \n\ncom                      ,   uma vari\u00e1vel aleat\u00f3ria gaussiana padronizada e     . \n\nA constante  , considerado o algoritmo de auto-adapta\u00e7\u00e3o do m\u00e9todo, pode ser estimado \n\nem cada itera\u00e7\u00e3o como \n\n                      \n\nAssim como a medida de n\u00e3o-gaussianidade atrav\u00e9s da negentropia, diferentes escolhas \n\nde fun\u00e7\u00f5es afetam os resultados e principalmente sua converg\u00eancia. Na aproxima\u00e7\u00e3o de n\u00e3o-\n\ngaussianidade atrav\u00e9s do algoritmo de gradiente e negentropia, as fun\u00e7\u00f5es mais indicadas s\u00e3o \n\n                \n\n             \n  \n\n \n  \n\n\n\n \n\n                                                  \n\n  \n\n19 \n\n \n\n       \n  \n\nonde        , geralmente escolhida igual a um \n\nA rotina que descreve o m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-gaussianidade com algoritmo de \n\ngradiente atrav\u00e9s da negentropia pode ser resumido como \n\nTabela 4.1 Rotina do algoritmo de gradiente atrav\u00e9s da negentropia - Adaptado de \n\n(Hyv\u00e4rinen,2001) \n\n1. Centralizar os dados. \n\n2. Aplicar o Branqueamento para obter o vetor  . \n\n3. Escolher aleatoriamente um vetor   de norma unit\u00e1ria e um valor para  . \n\n4. Atualizar                  com a escolha de   definida. \n\n5. Normalizar         . \n\n6. Caso o sinal de   n\u00e3o seja conhecido, atualizar                    \n\n  . \n\n7. Se o modelo n\u00e3o apresentar converg\u00eancia, voltar ao passo 4. \n\n \n\n4.2. 1.6 Algoritmo R\u00e1pido de Ponto - Fixo atrav\u00e9s de negentropia \n \n\nAssim como na converg\u00eancia atrav\u00e9s da curtose, o algoritmo r\u00e1pido de ponto - fixo \n\natrav\u00e9s da negentropia \u00e9 tamb\u00e9m um m\u00e9todo conhecido de FastICA.  \n\nO modelo de gradiente aponta para uma itera\u00e7\u00e3o de ponto fixo onde \n\n                \n\nseguida da normaliza\u00e7\u00e3o de  . O coeficiente   \u00e9 eliminado pela normaliza\u00e7\u00e3o, portanto \u00e9 omitido \n\nda itera\u00e7\u00e3o. Entretanto, o sucesso da converg\u00eancia n\u00e3o est\u00e1 t\u00e3o garantido, devido \u00e0s dificuldades \n\nalg\u00e9bricas da itera\u00e7\u00e3o. \n\nUma possibilidade baseia-se em adicionar   multiplicado por uma constante   dos dois \n\nlados da aproxima\u00e7\u00e3o anterior, gerando  \n\n \n\n                          \n\n \n\n\n\n \n\n                                                  \n\n  \n\n20 \n\n \n\ne como essa soma n\u00e3o influencia na dire\u00e7\u00e3o da nossa aproxima\u00e7\u00e3o, essa soma n\u00e3o interfere nos \n\npontos-fixos, sendo estes iguais aos pontos fixos da aproxima\u00e7\u00e3o                . \n\nA constante   adequada determina a converg\u00eancia e o velocidade desse algoritmo \n\nFastICA, portanto sua escolha deve ser otimizada para evitar itera\u00e7\u00f5es desnecess\u00e1rias.  Um dos \n\nm\u00e9todos mais robustos de aproxima\u00e7\u00e3o \u00e9 o m\u00e9todo de Newton-Raphson, definido por, \n\n          \n     \n\n      \n \n\nApesar de o m\u00e9todo atingir a converg\u00eancia em poucas itera\u00e7\u00f5es, geralmente, para cada \n\nitera\u00e7\u00e3o \u00e9 necess\u00e1ria computar uma matriz inversa, diminuindo a velocidade do m\u00e9todo.  \n\nComo esse algoritmo \u00e9 computado com o intuito de fornecer os resultados o mais r\u00e1pido \n\nposs\u00edvel, algumas variantes do m\u00e9todo devem ser propostas e aplicadas. \n\nO primeiro passo para essa variante, \u00e9 notar que o m\u00e1ximo da aproxima\u00e7\u00e3o via \n\nnegentropia de     \u00e9 obtida com a otimiza\u00e7\u00e3o de          . De acordo com as condi\u00e7\u00f5es de \n\nLagrange (Stewart, 2005), tal otimiza\u00e7\u00e3o \u00e9 obtida quando \n\n                  \n\nDenotando a fun\u00e7\u00e3o do lado esquerdo da equa\u00e7\u00e3o de F, e aplicando o gradiente (segunda \n\nvariante de Lagrange),  \n\n  \n\n  \n                   \n\nUma aproxima\u00e7\u00e3o razo\u00e1vel \u00e9 dada por                                   \n\n             . O gradiente torna-se diagonal e pode ser facilmente invertido, e sua aproxima\u00e7\u00e3o \n\nno m\u00e9todo de Newton, pode ser descrita como \n\n                                      \n\nque pode ser mais simplificada ainda caso seja multiplicada por               e simplificada \n\nde forma que \n\n                               \n\nA rotina que descreve o m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-gaussianidade com algoritmo r\u00e1pido \n\nde ponto - fixo (FastICA) pode ser resumido como \n\nTabela 4.2 Rotina do FastICA atrav\u00e9s da negentropia - Adaptado de (Hyv\u00e4rinen,2001) \n\n1. Centralizar os dados. \n\n2. Aplicar o Branqueamento para obter o vetor  . \n\n\n\n \n\n                                                  \n\n  \n\n21 \n\n \n\n3. Escolher aleatoriamente um vetor   de norma unit\u00e1ria. \n\n4. Aproximar                                \n\n5. Normalizar         . \n\n6. Se o modelo n\u00e3o apresentar converg\u00eancia, voltar ao passo 4. \n\n \n\n4.2.2 Converg\u00eancia de M\u00faltiplas Componentes Independentes \n \n\nOs m\u00e9todos anteriores para a converg\u00eancia estimavam uma componente independente, \n\npor isso elas s\u00e3o chamadas de converg\u00eancias unit\u00e1rias. Entretanto, \u00e9 poss\u00edvel encontrar mais de \n\numa componente independente de uma vez ainda mantendo a n\u00e3o-gaussianidade m\u00e1xima. Uma \n\nop\u00e7\u00e3o para essa estimativa pode partir do fato que as componentes independentes devem ser n\u00e3o -\n\ncorrelacionadas no espa\u00e7o branqueado, logo      \n      \n\n       \n   , onde essa n\u00e3o correla\u00e7\u00e3o \n\nindica ortogonalidade. Logo, para a converg\u00eancia de v\u00e1rias Componentes Independentes \u00e9 \n\nnecess\u00e1rio rodar os m\u00e9todos demonstrados anteriormente, para v\u00e1rios valores de vetores    e \n\nortogonalizar os resultados ap\u00f3s cada itera\u00e7\u00e3o evitando que duas componentes independentes \n\ntenham converg\u00eancia em um mesmo vetor. \n\nDois m\u00e9todos comuns utilizados para a converg\u00eancia de v\u00e1rias componentes \n\nindependentes s\u00e3o os m\u00e9todos de Ortogonaliza\u00e7\u00e3o Deflacion\u00e1ria e Ortogonaliza\u00e7\u00e3o Sim\u00e9trica. \n\n4.2.2.1 Ortogonaliza\u00e7\u00e3o Deflacion\u00e1ria \n \n\nA Ortogonaliza\u00e7\u00e3o Deflacion\u00e1ria \u00e9 derivada do m\u00e9todo de Gram-Shmidt (Farebrother, \n\n1974) que consiste em calcular      componentes independentes unit\u00e1rias de interesse, e depois \n\nde cada itera\u00e7\u00e3o \u00e9 subtra\u00edda da componente independente     , as proje\u00e7\u00f5es de todos as \n\ncomponentes      \n      ,         anteriores. \n\nA rotina que descreve o m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-gaussianidade com Ortogonaliza\u00e7\u00e3o \n\nDeflacion\u00e1ria (FastICA) pode ser resumida como \n\nTabela 4.3 Rotina do FastICA atrav\u00e9s da Ortogonaliza\u00e7\u00e3o Deflacion\u00e1ria- Adaptado de \n\n(Hyv\u00e4rinen,2001) \n\n1. Centralizar os dados. \n\n2. Aplicar o Branqueamento para obter o vetor  . \n\n\n\n \n\n                                                  \n\n  \n\n22 \n\n \n\n3. Escolher o n\u00famero    de Componentes Independentes. Iniciar contador   em \n\n1. \n\n4. Escolher aleatoriamente um vetor    de forma rand\u00f4mica. \n\n5. Aproximar                                \n\n6. Ortogonalizar o resultado com  \n\n              \n      \n\n   \n\n   \n\n \n\n7. Normalizar            . \n\n8. Se    n\u00e3o apresentar converg\u00eancia, voltar ao passo 5. \n\n9. Com a converg\u00eancia de   ,         Se     , voltar ao passo quatro. \n\n \n\n4.2.2.2 Ortogonaliza\u00e7\u00e3o Sim\u00e9trica \n \n\nEnquanto a ortogonaliza\u00e7\u00e3o deflacion\u00e1ria encontra uma Componente Independente por \n\nvez, e confere se cada itera\u00e7\u00e3o apresenta vetores n\u00e3o-correlacionados, a ortogonaliza\u00e7\u00e3o sim\u00e9trica \n\nencontra as componentes independentes em paralelo, todas de uma vez. Uma motiva\u00e7\u00e3o para a \n\nescolha desse m\u00e9todo, \u00e9 que os erros de estimativa nos primeiros vetores do m\u00e9todo deflacion\u00e1rio \n\nacumulam-se nos vetores seguintes \u00e0 ortogonaliza\u00e7\u00e3o.   \n\nA ortogonaliza\u00e7\u00e3o sim\u00e9trica \u00e9 feita fazendo a converg\u00eancia unit\u00e1ria para   vetores    em \n\nparalelo, e depois ortogonalizar todos os vetores    atrav\u00e9s dos m\u00e9todos sim\u00e9tricos (Chen, 2009)  \n\n        \n \n\n \n   \n\nOnde a raiz quadrada inversa      \n \n\n \n\n  \u00e9 obtida pela decomposi\u00e7\u00e3o de autovalores       \n\n                \n , de forma que \n\n                   \n    \n\n           \n  \n\nA rotina que descreve o m\u00e9todo de medi\u00e7\u00e3o de n\u00e3o-gaussianidade com Ortogonaliza\u00e7\u00e3o \n\nSim\u00e9trica (FastICA) pode ser resumida como \n\nTabela 4.4 Rotina do FastICA atrav\u00e9s da Ortogonaliza\u00e7\u00e3o Sim\u00e9trica - Adaptado de \n\n(Hyv\u00e4rinen,2001) \n\n1. Centralizar os dados. \n\n\n\n \n\n                                                  \n\n  \n\n23 \n\n \n\n2. Aplicar o Branqueamento para obter o vetor  . \n\n3. Escolher o n\u00famero    de Componentes Independentes. \n\n4. Escolher os valores iniciais para os vetores   .  \n\n5. Ortogonalize a matriz  , de acordo com o passo 6. \n\n6. Para todo         aproximar                                \n\n7. Ortogonalizar simetricamente a matiz            \n  calculando  \n\n        \n \n\n \n   \n\n8. Se   n\u00e3o apresentar converg\u00eancia, voltar ao passo 5. \n\n \n\n4.3 Converg\u00eancia atrav\u00e9s do Estimador de M\u00e1xima Probabilidade \n \n\nO m\u00e9todo de m\u00e1xima probabilidade \u00e9 um m\u00e9todo baseado no ajuste de uma distribui\u00e7\u00e3o \n\nou de modelo estat\u00edstico para os dados, provendo estimativas para os par\u00e2metros deste modelo ou \n\ndistribui\u00e7\u00e3o (Lucien, 1990). O m\u00e9todo de Probabilidade M\u00e1xima, tradu\u00e7\u00e3o do Ingl\u00eas ?Maximum \n\nLikelihood (ML)?, \u00e9 um aperfei\u00e7oamento das Estat\u00edsticas Bayesianas, que descrevem as \n\nincertezas sobre quantidades invis\u00edveis nos dados de forma probabil\u00edstica (Howson, 2005). \n\nSeja   um vetor, a Estimativa de m\u00e1xima Probabilidade,      maximiza a fun\u00e7\u00e3o de \n\nprobabilidade de distribui\u00e7\u00e3o conjunta \n\n                               \n\nConvenientemente, s\u00e3o escolhidos os c\u00e1lculos sobre o logaritmo da fun\u00e7\u00e3o de \n\nprobabilidade, pois muitas densidades de probabilidades cont\u00eam termos exponenciais. Em todo o \n\ncaso, o resultado n\u00e3o \u00e9 afetado, pois o estimador      que m\u00e1xima a fun\u00e7\u00e3o        , tamb\u00e9m \n\nmaximiza a fun\u00e7\u00e3o              (Hyv\u00e4rinen, 2001). O Estimador de m\u00e1xima probabilidade \u00e9 \n\ngeralmente encontrado atrav\u00e9s das solu\u00e7\u00f5es da equa\u00e7\u00e3o de probabilidade \n\n \n \n\n  \n               \n\n       \n   \n\nPor hip\u00f3tese, na An\u00e1lise de componentes independentes, os vetores s\u00e3o independentes, \n\nent\u00e3o a fun\u00e7\u00e3o de probabilidade de distribui\u00e7\u00e3o conjunta pode ser reescrita como \n\n                  \n\n \n\n   \n\n \n\n\n\n \n\n                                                  \n\n  \n\n24 \n\n \n\nUm artif\u00edcio muito utilizado para o c\u00e1lculo desse estimador est\u00e1 associado \u00e0 rela\u00e7\u00e3o de \n\nmapeamento de dois ou mais vetores no espa\u00e7o. Se dois vetores aleat\u00f3rios   e  , est\u00e3o \n\nrelacionados atrav\u00e9s de um mapeamento \u00fanico \n\n       \n\n         \n\nEnt\u00e3o, a densidade de distribui\u00e7\u00e3o      pode ser obtida atrav\u00e9s da densidade      (Dantas, \n\n2004), de forma que \n\n       \n \n\n        \n       \n\n    \n       \n\nonde    \u00e9 a matriz Jacobiana \n\n      \n\n \n \n \n \n \n \n \n \n      \n\n   \n\n      \n\n   \n  \n\n      \n\n   \n      \n\n   \n\n      \n\n   \n \n\n      \n\n   \n    \n\n      \n\n   \n\n      \n\n   \n \n\n      \n\n    \n \n \n \n \n \n \n \n\n \n\n onde       \u00e9 a i-\u00e9sima componente do vetor fun\u00e7\u00e3o     .  \n\nAtrav\u00e9s desse artif\u00edcio, o vetor densidade    do vetor de mistura      pode ser dado \n\ncalculado atrav\u00e9s de \n\n                                 \n\n \n\n \n\nonde       . Essa identidade pode ser expressa atrav\u00e9s da fun\u00e7\u00e3o             \n  \n\n                   \n   \n\n \n\n \n\nUtilizando-se da propriedade de calcular o Estimador de M\u00e1xima Probabilidade com \n\nvetores independentes em formato de produt\u00f3rio, o Estimador  , em rela\u00e7\u00e3o \u00e0   \u00e9 \n\n             \n             \n\n \n\n   \n\n \n\n   \n\n \n\nComo dito acima, \u00e9 comum utilizar-se do logaritmo do Estimador para o c\u00e1lculo, sendo \n\nassim \n\n\n\n \n\n                                                  \n\n  \n\n25 \n\n \n\n                   \n                  \n\n \n\n   \n\n \n\n   \n\n \n\npor simplicidade de nota\u00e7\u00e3o, substitui-se a somat\u00f3ria sobre o \u00edndice t pela esperan\u00e7a do operador, \n\ne dividi-se o estimador por T, obtendo \n\n \n\n \n                    \n\n                  \n\n \n\n   \n\n \n\nNa pr\u00e1tica, s\u00e3o necess\u00e1rios algoritmos que computem a Estimativa de M\u00e1xima \n\nProbabilidade, entre eles: Algoritmos de Gradiente, Algoritmo r\u00e1pido de ponto - fixo e o \n\nprinc\u00edpio da m\u00e1xima informa\u00e7\u00e3o m\u00fatua, Infomax (Linsker, 1988). \n\n \n\n4.3.1 Algoritmos de Gradiente para a Estimativa da M\u00e1xima Probabilidade \n \n\nAssim como as converg\u00eancias dos Componentes Independentes da ICA, utilizando-se da \n\nmaximiza\u00e7\u00e3o de n\u00e3o-gaussianidade, os m\u00e9todos mais simples para o c\u00e1lculo da M\u00e1xima \n\nProbabilidade s\u00e3o os algoritmos de gradiente. De fato, eles t\u00eam uma sequ\u00eancia de computa\u00e7\u00f5es \n\nbem similar. Dois m\u00e9todos conhecidos para maximizar a probabilidade s\u00e3o o algoritmo de Bell-\n\nSejnowsi e o algoritmo de gradiente natural (Hyv\u00e4rinen, 2001). \n\n \n\n4.3.1.1 Algoritmo de Bell-Sejnowski \n \n\nO algoritmo de Bell-Sejnowski (Bell, 1995) \u00e9 descrito como uma vers\u00e3o estoc\u00e1stica do \n\ngradiente do algoritmo de M\u00e1xima probabilidade. \n\nO primeiro passo desse m\u00e9todo consiste em calcular o gradiente do logaritmo do Estimador \n\nde M\u00e1xima Probabilidade  \n\n \n\n \n\n     \n\n  \n                     \n\nonde                         \u00e9 um vetor de fun\u00e7\u00f5es onde os elementos s\u00e3o chamados de \n\nfun\u00e7\u00f5es escore e s\u00e3o definidas como \n\n          \n  \n\n  \n \n\n  \n \n\n\n\n \n\n                                                  \n\n  \n\n26 \n\n \n\nA estimativa de M\u00e1xima Probabilidade \u00e9 proporcional ao gradiente da fun\u00e7\u00e3o de \n\nprobabilidade \n\n                         \n\nE uma vers\u00e3o estoc\u00e1stica dessa proporcionalidade \u00e9 dada pela omiss\u00e3o da esperan\u00e7a \n\n                     \n\nApesar de convergir, a invers\u00e3o da matriz   deixa o processo extremamente lento. A \n\nvelocidade do m\u00e9todo pode ser aumentada, tratando os dados com pr\u00e9-processamento \n\n(branqueamento), ou utilizando-se do Gradiente Natural. \n\n \n\n4.3.1.2 Algoritmo de Gradiente Natural \n \n\nO gradiente natural \u00e9 baseado da estrutura geom\u00e9trica do espa\u00e7o param\u00e9trico onde est\u00e3o os \n\ndados e est\u00e1 relacionado com os Grupos de Lie e sua estrutura (San Martin, 1999). \n\nO in\u00edcio do m\u00e9todo de gradiente natural est\u00e1 baseado em multiplicar a estimativa da \n\nm\u00e1xima probabilidade anterior, em sua forma n\u00e3o-estoc\u00e1stica, por    , obtendo \n\n                    \n\nEsse algoritmo pode ser interpretado como uma decorrela\u00e7\u00e3o n\u00e3o-linear (Li ET AL, 2010). \n\nA converg\u00eancia acontecer\u00e1 quando            , o que significa que    e        s\u00e3o n\u00e3o \n\ncorrelacionados para todo \u00edndices diferentes    . \n\nAs tr\u00eas fun\u00e7\u00f5es n\u00e3o-lineares mais utilizadas como     , s\u00e3o \n\n                 \n\n                 \n\n          \n\nOs \u00edndices   e \u2013 nas duas primeiras fun\u00e7\u00f5es n\u00e3o-lineares est\u00e3o relacionadas com uma estimativa \n\nde qual fun\u00e7\u00e3o \u00e9 mais adequada aos dados. \n\nEssa escolha pode ser feita computando o momento n\u00e3o-polinomial (Leite, 2005), onde \n\n                           \n   \n\nonde    s\u00e3o algumas estimativas de componentes independentes. Caso o momento n\u00e3o-polinomial \n\ntenha resposta positiva, a fun\u00e7\u00e3o de n\u00e3o-linearidade a ser usada deve ser      , caso o momento \n\nn\u00e3o-polinomial tenha resposta negativa, a fun\u00e7\u00e3o escolhida deve ser a      , justificando a \n\nrela\u00e7\u00e3o de seus \u00edndices. \n\n\n\n \n\n                                                  \n\n  \n\n27 \n\n \n\n4.3.2 Algoritmo R\u00e1pido de Ponto - Fixo \n \n\nO algoritmo r\u00e1pido de ponto - fixo (FastICA) para a estimativa de m\u00e1xima probabilidade \n\n\u00e9 quase id\u00eantico ao algoritmo empregado para a maximiza\u00e7\u00e3o da n\u00e3o-gaussianidade, se as \n\nestimativas das componentes independentes compuserem um espa\u00e7o branqueado. \n\n Considerando a linearidade da esperan\u00e7a no logaritmo da probabilidade, com uma \n\ndensidade assumida de    , seu c\u00e1lculo \u00e9 \n\n \n\n \n           \n\n \n\n   \n\n         \n                    \n\n O primeiro termo do lado direito da igualdade \u00e9 a soma de termos       \n    , \n\nmaximizado quando      geram componentes independentes, de acordo com o Teorema 8.1 \n\nde (Hyv\u00e4rinen, 2001). A n\u00e3o-correla\u00e7\u00e3o e a vari\u00e2ncia unit\u00e1ria de    significam que     \n   \n\n           , que pode ser reescrito como \n\n                                              \n\nlogo      deve ser constante. \n\nEm particular, o termo            deve ser constante e a probabilidade basicamente \n\nconsiste na soma de   termos da forma otimizada do FastICA, deixando o m\u00e9todo de algoritmo \n\nr\u00e1pido de ponto-fixo para a estimativa de m\u00e1xima probabilidade, quase id\u00eantico ao algoritmo \n\nusado para a maximiza\u00e7\u00e3o da n\u00e3o-gaussianidade. \n\nO algoritmo para a n\u00e3o-gaussianidade \u00e9 calculado atrav\u00e9s de  \n\n                                      \n\nonde               . Reescrevendo esse algoritmo na forma de matriz, \u00e9 obtido \n\n                               \n        \n\nOnde            \n            e     . \n\nPara expressar a converg\u00eancia usando informa\u00e7\u00e3o n\u00e3o branqueada, basta multiplicar os dois \n\nlados pela matriz de branqueamento. Em suma, essa multiplica\u00e7\u00e3o apenas substitui   por  : \n\n                               \n        \n\nonde                       e            \n           . Essa \u00faltima \u00e9 a itera\u00e7\u00e3o b\u00e1sica \n\ndo FastICA com estimativa de m\u00e1xima probabilidade e gradiente natural. \n\nA cada passo do FastICA, a matriz   deve ser projetada sobre as matrizes branqueadas, e \n\ntal proje\u00e7\u00e3o pode ser feita atrav\u00e9s do m\u00e9todo cl\u00e1ssico das ra\u00edzes quadradas de matrizes \n\n\n\n \n\n                                                  \n\n  \n\n28 \n\n \n\n         \n \n\n \n   \n\nonde C \u00e9 a matriz de correla\u00e7\u00e3o dos dados. \n\nA rotina que descreve o m\u00e9todo de estimativa de m\u00e1xima probabilidade com Algoritmo \n\nR\u00e1pido de Ponto - Fixo (FastICA) pode ser resumida como \n\n \n\nTabela 4.5 Rotina do FastICA atrav\u00e9s da Estimativa de M\u00e1xima Probabilidade - Adaptado de \n\n(Hyv\u00e4rinen, 2001) \n\n1. Centralizar os dados. \n\n2. Computar a matriz de correla\u00e7\u00e3o           \n\n3. Escolher a matriz inicial de separa\u00e7\u00e3o   \n\n4. Computar  \n\n     \n\n                         \n\n     \n \n\n             \n         \n\n5. Atualizar a matriz de separa\u00e7\u00e3o de acordo com \n\n                               \n        \n\n6. Descorrelacionar e normalizar  , atrav\u00e9s de  \n\n         \n \n\n \n   \n\n \n\n7. Se   n\u00e3o apresentar converg\u00eancia, voltar ao passo 3. \n\n \n\n4.3.3 Princ\u00edpio da Maximiza\u00e7\u00e3o da Informa\u00e7\u00e3o M\u00fatua (Infomax) \n \n\nO princ\u00edpio da Infomax \u00e9 baseado na maximiza\u00e7\u00e3o da entropia como um vetor-sa\u00edda, ou seja, do \n\nfluxo de informa\u00e7\u00e3o dentro de redes-neurais (Wasserman, 1989) \n\nSe   \u00e9 um vetor-entrada, onde os vetores-sa\u00eddas, ap\u00f3s um processo de redes-neurais, s\u00e3o da \n\nforma \n\n          \n        \n\n\n\n \n\n                                                  \n\n  \n\n29 \n\n \n\nonde    \u00e9 alguma fun\u00e7\u00e3o n\u00e3o-linear e    \u00e9 o vetor-peso dos neur\u00f4nios(Anderson, 1995),   \u00e9 um \n\nvetor gaussiano com componentes de ru\u00eddo. A entropia dos vetores-sa\u00edda (Gokhale, 1989) \u00e9 \n\n             \n           \n\n     \n\n   Utilizando-se da f\u00f3rmula de entropia para uma transforma\u00e7\u00e3o (Cover, 1991) \n\n       \n           \n\n                     \n  \n\n  \n      \n\nonde         \n           \n\n   , \u00e9 a fun\u00e7\u00e3o definida pela rede neural. A derivada do segundo \n\ntermo do lado direito da igualdade da entropia resulta em \n\n         \n  \n\n  \n               \n\n    \n              \n\n \n\n \n\nO vetor-sa\u00edda da entropia tem o mesmo formato do c\u00e1lculo da maximiza\u00e7\u00e3o de m\u00e1xima \n\nprobabilidade, com a diferen\u00e7a de que as fun\u00e7\u00f5es densidades   utilizadas anteriormente s\u00e3o \n\nsubstitu\u00eddas pela utiliza\u00e7\u00e3o das derivadas   \n  de fun\u00e7\u00f5es n\u00e3o-lineares. A escolha das fun\u00e7\u00f5es n\u00e3o-\n\nlineares    recai sobre as op\u00e7\u00f5es anteriores dos outros algoritmos, ou seja,              \n\npode ser denotada por k. \n\n4.4 Converg\u00eancia atrav\u00e9s da Minimiza\u00e7\u00e3o de Informa\u00e7\u00e3o M\u00fatua \n \n\nNo t\u00f3pico 2.3.3 foi apresentado o conceito de maximiza\u00e7\u00e3o da informa\u00e7\u00e3o m\u00fatua, tamb\u00e9m \n\nconhecido em sua literatura como Infomax. Mas assim como seus algoritmos antecedentes, os \n\ndados envolvidos foram considerados como seguindo um modelo compat\u00edvel com o da Analise \n\nde Componentes Independentes.  Caso a hip\u00f3tese do modelo n\u00e3o seja levada em considera\u00e7\u00e3o, \n\num novo tipo de medida para a converg\u00eancia das Componentes Independentes deve ser usada. \n\nEm todo o caso, o m\u00e9todo ICA dever\u00e1 ser composto por componentes independentes, logo \n\numa medida que calcula a depend\u00eancia de duas poss\u00edveis componentes pode ser utilizado para o \n\nc\u00e1lculo da converg\u00eancia. Dessa forma, a an\u00e1lise de componentes independentes pode ser vista \n\ncomo uma decomposi\u00e7\u00e3o linear que minimiza a depend\u00eancia (Hyv\u00e4rinen, 2001) de vetores dentro \n\nde um espa\u00e7o. \n\nUma aproxima\u00e7\u00e3o interessante para essa tentativa de converg\u00eancia \u00e9 utilizando-se da \n\nMinimiza\u00e7\u00e3o da Informa\u00e7\u00e3o M\u00fatua. \n\nA Informa\u00e7\u00e3o M\u00fatua   entre    vari\u00e1veis aleat\u00f3rias            \u00e9 definida como \n\n\n\n \n\n                                                  \n\n  \n\n30 \n\n \n\n                       \n\n \n\n   \n\n     \n\nonde      \u00e9 o diferencial de entropia, previamente apresentado. \n\n \n\n4.4.1 A n\u00e3o-gaussianidade dentro da Minimiza\u00e7\u00e3o da informa\u00e7\u00e3o m\u00fatua  \n \n\nA informa\u00e7\u00e3o m\u00fatua pode ser obtida atrav\u00e9s do diferencial de entropia, assim como feito no \n\nt\u00f3pico 2.3.3. Para uma transforma\u00e7\u00e3o linear do tipo      \n\n                       \n\n \n\n   \n\n                \n\nConsiderando os vetores   , por hip\u00f3tese, n\u00e3o-correlacionados e vari\u00e2ncia unit\u00e1ria, como \n\nvisto no algoritmo FastICA para maximiza\u00e7\u00e3o de m\u00e1xima probabilidade, podemos concluir que  \n\n                                              \n\nportanto ,   deve ser constante e           tamb\u00e9m deve ser constante.  \n\nA negentropia     , assim como descrita anteriormente  \n\n                      \n\nDevido \u00e0 vari\u00e2ncia unit\u00e1ria dos vetores   , pode ser considerada como  \n\n                  \n\nDessas duas igualdades, ap\u00f3s alguma algebriza\u00e7\u00e3o, a medida de informa\u00e7\u00e3o m\u00fatua pode ser \n\nescrita como \n\n                             \n\n \n\n   \n\n \n\nNessa configura\u00e7\u00e3o, a minimiza\u00e7\u00e3o da informa\u00e7\u00e3o m\u00fatua \u00e9 equivalente a encontrar as \n\ndire\u00e7\u00f5es onde a negentropia \u00e9 maximizada. Como visto anteriormente, a negentropia pode ser \n\nutilizada como uma medida de n\u00e3o-gaussianidade. Entretanto, ao contr\u00e1rio da converg\u00eancia via \n\nnegentropia, onde era poss\u00edvel encontrar v\u00e1rias componentes, atrav\u00e9s da ortogonaliza\u00e7\u00e3o \n\ndeflacion\u00e1ria, com o m\u00e9todo de minimiza\u00e7\u00e3o de informa\u00e7\u00e3o m\u00fatua, esse tipo de converg\u00eancia \n\nunit\u00e1ria para v\u00e1rias componentes independentes n\u00e3o \u00e9 poss\u00edvel. \n\nCaso seja necess\u00e1rio, pode-se excluir a hip\u00f3tese de n\u00e3o-correla\u00e7\u00e3o dos vetores   , e \n\ntrabalhar-se apenas com a aproxima\u00e7\u00e3o anterior. Tal resultado, apesar de n\u00e3o \u00f3timo, aumenta o \n\nalcance do m\u00e9todo, e demonstrar robustez na t\u00e9cnica. \n\n\n\n \n\n                                                  \n\n  \n\n31 \n\n \n\n \n\n4.4.2 M\u00e1xima Probabilidade na minimiza\u00e7\u00e3o de informa\u00e7\u00e3o m\u00fatua  \n \n\nA rela\u00e7\u00e3o desses dois algoritmos j\u00e1 foi percebida na introdu\u00e7\u00e3o do principio de \n\nMaximiza\u00e7\u00e3o de Informa\u00e7\u00e3o M\u00fatua (Infomax) como uma alternativa para a estimativa de \n\nm\u00e1xima probabilidade. \n\nRelembrando que a expectativa do logaritmo da m\u00e1xima probabilidade,   \n\n \n\n \n           \n\n \n\n   \n\n         \n                    \n\nComo visto na estimativa de m\u00e1xima probabilidade, as densidades    s\u00e3o iguais as \n\ndensidades de   \n  . Denotando as fdp de                    , a informa\u00e7\u00e3o m\u00fatua pode ser \n\naproximada para \n\n                  \n\n   \n\n                        \n\nEssa configura\u00e7\u00e3o de Informa\u00e7\u00e3o m\u00fatua \u00e9 uma aproxima\u00e7\u00e3o da estimativa de m\u00e1xima \n\nprobabilidade, a n\u00e3o ser pelo sinal e pela constante     . \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n                                                  \n\n  \n\n33 \n\n \n\n \n\nCap\u00edtulo 5 \n\n \n\nReconhecimento de Padr\u00f5es \n\n \n\nReconhecer padr\u00f5es \u00e9 uma das vertentes da Aprendizagem de M\u00e1quina (Duda, 1973), \n\ncujo objetivo \u00e9 classificar informa\u00e7\u00f5es, baseado em algum conhecimento pr\u00e9vio da natureza \n\ndessas informa\u00e7\u00f5es ou em informa\u00e7\u00f5es estat\u00edsticas extra\u00eddas dos padr\u00f5es existentes nos dados. \n\nPor sua vez, a aprendizagem da m\u00e1quina \u00e9 um campo da Intelig\u00eancia Artificial e seu estudo \u00e9 \n\nvoltado ao desenvolvimento e descoberta de algoritmos que auxiliam a m\u00e1quina em uma \n\ndeterminada tarefa atrav\u00e9s de aperfei\u00e7oamentos em cada itera\u00e7\u00e3o de seu programa (Michie ET \n\nAL 1994). Em suma, um algoritmo pode aprender ou aperfei\u00e7oar a realiza\u00e7\u00e3o de alguma tarefa \n\nespec\u00edfica atrav\u00e9s de um banco de dados ou atrav\u00e9s de uma an\u00e1lise instant\u00e2nea.  \n\nOs estudos de Intelig\u00eancia Artificial (IA) come\u00e7aram logo ap\u00f3s a Segunda Guerra \n\nMundial (Turing, 1950) e contemplam pesquisas sobre dispositivos computacionais que simulam \n\numa capacidade racional na resolu\u00e7\u00e3o de um problema ou na execu\u00e7\u00e3o de uma tarefa.  \n\nApesar da \u00e1rea de pesquisa em Intelig\u00eancia Artificial ser t\u00e3o grande e abranger in\u00fameros \n\nm\u00e9todos, o Reconhecimento de padr\u00f5es \u00e9 uma t\u00e9cnica extremamente utilizada (van der Walt ET \n\nAL, 2006) e geralmente associada a redes neurais. \n\nO termo padr\u00e3o pode ter diversas acep\u00e7\u00f5es, de acordo com a \u00e1rea em que \u00e9 utilizado. No \n\npresente caso, pode-se associar-se padr\u00e3o a um comportamento de um perfil para uma \n\ndeterminada f\u00e1cie. Se esses elementos repetem-se de maneira previs\u00edvel, diz-se que existe um \n\npadr\u00e3o desses elementos. Um modelo que pode ser usado para gerar algum tipo de informa\u00e7\u00e3o, a \n\npriori, n\u00e3o dispon\u00edvel atrav\u00e9s da an\u00e1lise do comportamento do modelo, tamb\u00e9m \u00e9 um \n\nreconhecimento de padr\u00e3o desse modelo (Fuller, 1975). \n\n \n\n 5.1 Sistema de Reconhecimento de padr\u00f5es \n\nEm geral, pode-se dividir o Reconhecimento de Padr\u00f5es, em etapas subseq\u00fcentes que \n\ncontemplam a aplica\u00e7\u00e3o desse m\u00e9todo. S\u00e3o elas: aquisi\u00e7\u00e3o de dados, localiza\u00e7\u00e3o de segmenta\u00e7\u00e3o \n\nde padr\u00f5es, extra\u00e7\u00e3o de caracter\u00edsticas, classifica\u00e7\u00e3o e p\u00f3s-processamento, descritos a seguir. \n\n5.1.1 Aquisi\u00e7\u00e3o dos Dados \n\n\n\n \n\n                                                  \n\n  \n\n34 \n\n \n\n \n\nNo subt\u00edtulo, o termo ?Aquisi\u00e7\u00e3o?, geralmente associado \u00e0 a\u00e7\u00e3o de captar dados, tem uma \n\nmaior abrang\u00eancia do que esta costumeira. Nessa etapa, n\u00e3o \u00e9 necess\u00e1rio que os dados coletados \n\nsejam amostras recolhidas pelo pr\u00f3prio elemento que aplicar\u00e1 esse m\u00e9todo. O fato de se emular \n\nou simular dados pode ser considerado uma forma de aquisi\u00e7\u00e3o de informa\u00e7\u00f5es. Ou ainda, \n\npodem-se coletar informa\u00e7\u00f5es de um banco de dados conhecidos para o uso do m\u00e9todo. \n\nNessa etapa s\u00e3o escolhidas as caracter\u00edsticas que ser\u00e3o treinadas e testadas. A natureza \n\nf\u00edsica dos dados pode indicar qual tipo de caracter\u00edstica ter\u00e1 maior influ\u00eancia no treino e na \n\nclassifica\u00e7\u00e3o de um conjunto de informa\u00e7\u00f5es, como pode ser notado pelos exemplos de \n\naplica\u00e7\u00f5es mostrados na Tabela 3.1, baseada nas notas de aula do Prof. Jo\u00e3o Ascenso (Ascenso, \n\n2003) \n\n \n\n Tabela 5. 1 Aplica\u00e7\u00f5es do Reconhecimento de Padr\u00f5es \n\nAn\u00e1lise Entradas Sa\u00eddas \n\nIdentifica\u00e7\u00e3o de Recursos \n\nNaturais \n\nImagens Espectrais, Dados \n\nde amostras geol\u00f3gicas \n\nLitologia, presen\u00e7a de min\u00e9rios, formas \n\nde terrenos \n\nReconhecimento de Voz Sinais de Voz Palavras/ Identidade do locator \n\nTestes Destrutivos Ultra-sons/ Imagens Presen\u00e7a/Aus\u00eansia de anomalias \n\nDetec\u00e7\u00e3o de falhas  \n\n( Circuitos integrados, Texturas)  \nImagens Acieta\u00e7\u00e3o Rejei\u00e7\u00e3o \n\nRob\u00f3tica \nImagens 3D, Laser, Luz \n\nestruturada \n\nIdentifica\u00e7\u00e3o de Objetos, tarefas \n\nindustriais \n\nIdentifica\u00e7\u00e3o e contagem de \n\nc\u00e9lulas \n\nTecidos selecionados, \n\namostras de sangue \nTipo de c\u00e9lula \n\nDetec\u00e7\u00e3o/ Diagn\u00f3stico de \n\ndoen\u00e7as \nECG,EEG \n\nCondi\u00e7\u00f5es card\u00edacas, cerebrais. \n\nPatologias \n\n      \n\n \n\n5.1.2 Localiza\u00e7\u00e3o de segmenta\u00e7\u00e3o de padr\u00f5es \n\n \n\nA busca e reconhecimento dos padr\u00f5es existentes nos dados podem ser feitos por v\u00e1rios \n\nm\u00e9todos, onde os de maior uso podem ser divididos em algoritmos baseados em fatores \n\n\n\n \n\n                                                  \n\n  \n\n35 \n\n \n\nestat\u00edsticos (teoria da decis\u00e3o) ou algoritmos baseado em fatores sint\u00e1ticos (estrutural). Atrav\u00e9s \n\nde an\u00e1lises estat\u00edsticas, o reconhecimento de padr\u00f5es baseia-se na caracteriza\u00e7\u00e3o de padr\u00f5es \n\nestat\u00edsticos, os quais, por hip\u00f3tese, s\u00e3o gerados por distribui\u00e7\u00f5es probabil\u00edsticas. J\u00e1 as an\u00e1lises \n\nsint\u00e1ticas de reconhecimento de padr\u00f5es atuam baseadas na inter-rela\u00e7\u00e3o estrutural e espacial \n\n(geometria) de recursos. (Koutroumbas, 2008).  \n\n Uma grande variedade de algoritmos pode ser aplicada para reconhecimento de padr\u00f5es, \n\nentre os mais conhecidos est\u00e3o os classificadores mais simples do teorema de Bayes, os \n\nalgoritmos k-vizinhos, redes neurais, entre outros. \n\n \n\n5.1.3 Extra\u00e7\u00e3o de Caracter\u00edsticas \n\n \n\nOs padr\u00f5es encontrados em um conjunto de dados s\u00e3o identificados e diferenciado s \n\nformando, para cada qual, uma caracter\u00edstica associada (Kulikowski ET AL, 1991). Por exemplo, \n\ncaso seja utilizado um algoritmo Sint\u00e1tico, a estrutura dos dados \u00e9 o fator decisivo para separar e \n\nidentificar esses padr\u00f5es.  \n\nElementos dentro do conjunto de informa\u00e7\u00f5es que apresentam semelhan\u00e7as geom\u00e9tricas \n\nest\u00e3o associados ao mesmo padr\u00e3o, e assim, associados a uma mesma caracter\u00edstica estrutural. \n\nEssa caracter\u00edstica deve ser invariante a transforma\u00e7\u00f5es espaciais, ou a mesma n\u00e3o pode ser \n\nchamada de padr\u00e3o. Essas transforma\u00e7\u00f5es podem ser rota\u00e7\u00f5es, transla\u00e7\u00f5es, coordenadas polares, \n\nentre outras, como indica a Figura 5.1.  \n\n \n\nFigura 5. 1 Exemplos de Transla\u00e7\u00e3o e Rota\u00e7\u00e3o de um Objeto-Padr\u00e3o Inicial \n\n \n\n\n\n \n\n                                                  \n\n  \n\n36 \n\n \n\nQuando essas caracter\u00edsticas s\u00e3o extra\u00eddas intrinsecamente dos dados, o reconhecimento \n\nde padr\u00f5es \u00e9 chamado de aprendizado n\u00e3o-Supervisionado. Por outro lado, quando s\u00e3o inseridas \n\ninforma\u00e7\u00f5es sobre as caracter\u00edsticas, tais informa\u00e7\u00f5es cont\u00eam r\u00f3tulos (t\u00edtulos) que s\u00e3o utilizados \n\ncomo caracter\u00edsticas, esse reconhecimento de padr\u00f5es \u00e9 chamado de aprendizado supervisionado. \n\nEm suma, se um conjunto de padr\u00f5es que j\u00e1 foi classificado ou descrito, este conjunto de padr\u00f5es \n\n\u00e9 denominado o conjunto de treinamento e a estrat\u00e9gia de aprendizagem \u00e9 caracterizada como um \n\naprendizado supervisionado. Se o sistema de padr\u00f5es n\u00e3o \u00e9 fornecido, a priori, a estrat\u00e9gia de \n\naprendizagem estabelece classes de caracter\u00edsticas com base nas regularidades intr\u00ednsecas dos \n\npadr\u00f5es, e \u00e9 conhecida por aprendizado n\u00e3o-supervisonado. \n\n \n\n5.1.4 Classifica\u00e7\u00e3o \n\n \n\nComo visto no subt\u00f3pico 3.1.3, o aprendizado do reconhecimento de padr\u00f5es pode ser \n\nsupervisionado ou n\u00e3o-supervisionado, e a classifica\u00e7\u00e3o de informa\u00e7\u00f5es depende desse modelo \n\nde aprendizagem. \n\nO modelo supervisionado, como descrito, precisa de um conjunto de informa\u00e7\u00f5es pr\u00e9vias, \n\nanexadas de r\u00f3tulos que servir\u00e3o de base para as caracter\u00edsticas. A essas informa\u00e7\u00f5es pr\u00e9vias \u00e9 \n\ndado o nome de conjunto de treino (Schuermann ET AL, 1996). O conjunto de treino de uma \n\naprendizagem n\u00e3o supervisionada funciona como um banco de dados, guardando as informa\u00e7\u00f5es \n\nsobre determinadas estruturas e associando-as aos r\u00f3tulos anexados a cada caracter\u00edstica. Esse \n\nconjunto \u00e9 utilizado com a finalidade associar r\u00f3tulos a padr\u00f5es presentes em outro conjunto de \n\ndados, conhecido como conjunto de teste. \n\nBasicamente, o algoritmo encontra padr\u00f5es no conjunto de teste, associa-os a algum \n\npadr\u00e3o semelhante existente no conjunto de treino. Dada essa associa\u00e7\u00e3o, os padr\u00f5es encontrados \n\nno conjunto de teste t\u00eam anexados em si o r\u00f3tulo do padr\u00e3o de treino que lhe \u00e9 semelhante. \n\nSe o modelo de aprendizagem for do tipo n\u00e3o-supervisionada, ele n\u00e3o necessita do \n\nconjunto de treino. O algoritmo procura padr\u00f5es intr\u00ednsecos dentro das informa\u00e7\u00f5es e os separa \n\nem classes.  \n\nCada classe ser\u00e1 um subconjunto do conjunto total de informa\u00e7\u00f5es, e dentro de cada \n\nclasse, todos os elementos ser\u00e3o apenas informa\u00e7\u00f5es que tem o mesmo padr\u00e3o, ou seja, elementos \n\nque se assemelham estar\u00e3o juntos em alguma classe.  \n\n\n\n \n\n                                                  \n\n  \n\n37 \n\n \n\n \n\n5.1.5 P\u00f3s-Processamento \n\n \n\nEssa fase est\u00e1 associada a um aperfei\u00e7oamento da t\u00e9cnica como um passo iterativo de \n\nreconhecimento de padr\u00f5es. Averiguar quais caracter\u00edsticas comp\u00f5em melhores escolhas, ou \n\nest\u00e3o associadas mais fortemente aos padr\u00f5es dos dados. \n\nQuanto \u00e0 aprendizagem n\u00e3o-supervisionada, caso exista algum conhecimento sobre \n\nr\u00f3tulos, esses podem ser empregados nas classes que apresentam altas concentra\u00e7\u00f5es de \n\nelementos com esses r\u00f3tulos. De fato, esse tipo de rotula\u00e7\u00e3o n\u00e3o implica na mudan\u00e7a do termo da \n\naprendizagem para Supervisionada, j\u00e1 que toda separa\u00e7\u00e3o dessas classes foi feita sem essas \n\ninforma\u00e7\u00f5es. Como tais informa\u00e7\u00f5es s\u00e3o aferidas ap\u00f3s toda a separa\u00e7\u00e3o desses padr\u00f5es, esta \u00e9 \n\nconsiderada um p\u00f3s-processamento do algoritmo. \n\n \n\n5.2 Aprendizagem Supervisionada  \n\n \n\nNo caso da aprendizagem supervisionada, um conjunto de treino serve de guia para que o \n\nalgoritmo encontre os padr\u00f5es existentes nos dados. As altera\u00e7\u00f5es dos pesos desses padr\u00f5es s\u00e3o \n\ncalculadas de forma a que a resposta do algoritmo se assemelhe a alguma resposta que se \n\nencontra no conjunto de treino.  \n\nOs dados do conjunto de treino servem de exemplos de treinamento. Na aprendizagem \n\nsupervisionada, cada exemplo \u00e9 um par constitu\u00eddo por um objeto de entrada (normalmente um \n\nvetor) e um valor de sa\u00edda desejado (r\u00f3tulo). Um algoritmo de aprendizado supervisio nado \n\nanalisa os dados de treinamento e produz uma fun\u00e7\u00e3o que \u00e9 chamado de classificadora (se a sa\u00edda \n\n\u00e9 discreta) ou uma fun\u00e7\u00e3o de regress\u00e3o (se a sa\u00edda \u00e9 cont\u00ednua). A fun\u00e7\u00e3o inferida deve prever o \n\nvalor de sa\u00edda correta para qualquer objeto de entrada v\u00e1lida. Isto exige que o algoritmo de \n\naprendizagem generalize de forma satisfat\u00f3ria, a partir de situa\u00e7\u00f5es invis\u00edveis dos dados de \n\ntreinamento, os r\u00f3tulos desses valores de entrada.  \n\nA aprendizagem supervisionada pode ser tamb\u00e9m subdividida em m\u00e9todos param\u00e9tricos e \n\nm\u00e9todos n\u00e3o param\u00e9tricos. \n\n \n\n\n\n \n\n                                                  \n\n  \n\n38 \n\n \n\n5.2.1 M\u00e9todos Param\u00e9tricos \n\n \n\nM\u00e9todos Param\u00e9tricos s\u00e3o aplicados quando a distribui\u00e7\u00e3o que gerou o conjunto de dados \n\n\u00e9 conhecida, ou se pode avaliar qual a poss\u00edvel distribui\u00e7\u00e3o, dentre as v\u00e1rias distribui\u00e7\u00f5es \n\nplaus\u00edveis, que gerou as informa\u00e7\u00f5es (Cardoso, 2001). Admite-se ainda que a distribui\u00e7\u00e3o possa \n\nser expressa analiticamente, sendo o objetivo determinar os par\u00e2metros da mesma. \n\nMatematicamente, o modelo param\u00e9trico   \u00e9 uma cole\u00e7\u00e3o de distribui\u00e7\u00f5es de \n\nprobabilidade, onde cada membro dessa cole\u00e7\u00e3o    \u00e9 descrito por um par\u00e2metro de dimens\u00e3o \n\nfinita  . Se   \u00e9 a dimens\u00e3o total do modelo, para    , o conjunto de valores poss\u00edveis para os \n\npar\u00e2metros do m\u00e9todo s\u00e3o denotados por        e o modelo \u00e9 descrito como \n\n            \n\nSe o m\u00e9todo param\u00e9trico pode ser descrito apenas com fun\u00e7\u00f5es cont\u00ednuas, pode-se \n\ndescrev\u00ea-lo atrav\u00e9s de suas fun\u00e7\u00f5es de densidade de probabilidade \n\n            \n\nComo exemplos de m\u00e9todos param\u00e9tricos podem-se destacar: \n\n  A fam\u00edlia de Poisson (Good, 1986) \n\n          \n  \n\n  \n                       \n\nonde   \u00e9 a fun\u00e7\u00e3o densidade de probabilidade. Neste caso        e         \n\nA fam\u00edlia Normal (Marsaglia, 2004), parametrizada por        , \u00e9 dada por \n\n          \n \n\n     \n     \n\n      \n\n      \n \n             \n\n.  A Fam\u00edlia de Transla\u00e7\u00e3o de Weibull (Weibull, 1951), parametrizada por          , \u00e9 \n\ndada por \n\n          \n \n\n \n\n        \n\n      \n     \n\n      \n\n    \n                 \n\nApesar de notadamente conhecidos e de teoricamente bem estruturados, esses m\u00e9todos \n\nnecessitam de informa\u00e7\u00f5es geralmente n\u00e3o dispon\u00edveis pra sua utiliza\u00e7\u00e3o. O mais usual m\u00e9todo \n\nparam\u00e9trico para obter os par\u00e2metros da distribui\u00e7\u00e3o \u00e9 o m\u00e9todo de estimativa de m\u00e1xima \n\nverossimilhan\u00e7a, o que pode ser geralmente feito de modo anal\u00edtico. \n\n \n\n\n\n \n\n                                                  \n\n  \n\n39 \n\n \n\n5.2.1.1 Estimativa de M\u00e1xima Verossimilhan\u00e7a \n\n \n\nAssim como na An\u00e1lise de Componentes Independentes, a estimativa de m\u00e1xima \n\nverossimilhan\u00e7a para m\u00e9todos param\u00e9tricos de reconhecimento de padr\u00f5es \u00e9 um m\u00e9todo \n\nestat\u00edstico utilizado para ajustar um modelo estat\u00edstico para dados e fornecer estimativas para os \n\npar\u00e2metros do modelo. \n\nPara um conjunto fixo de dados e modelo de probabilidades subjacentes conhecidos, o \n\nm\u00e9todo de m\u00e1xima verossimilhan\u00e7a seleciona os valores dos par\u00e2metros do modelo que \n\nmaximizam a fun\u00e7\u00e3o de verossimilhan\u00e7a. A estimativa apresenta uma abordagem unificada para \n\navalia\u00e7\u00e3o, que \u00e9 bem definida, no caso da distribui\u00e7\u00e3o normal e em quase todas as outras \n\ndistribui\u00e7\u00f5es. \n\nSeja   um vetor de n-\u00e9sima dimens\u00e3o, onde seus elementos            s\u00e3o \n\nobserva\u00e7\u00f5es de uma distribui\u00e7\u00e3o desconhecida      . Essa distribui\u00e7\u00e3o desconhecida       \n\npertence a uma fam\u00edlia de distribui\u00e7\u00f5es definida por               . Ent\u00e3o      pode ser \n\ndefinida como \n\n                  \n\nO valor desconhecido   \u00e9 conhecido por valor verdadeiro do par\u00e2metro. O m\u00e9todo de \n\nestimativa m\u00e1xima de verossimilhan\u00e7a busca um estimador    que se aproxime do valor \n\nverdadeiro \n\nSem perda de generalidade, considerando que as amostras s\u00e3o independentes, a fun\u00e7\u00e3o \n\nde densidade conjunta de   \u00e9 definida por \n\n                                                \n\nEstendendo o dom\u00ednio da fdp, pode-se definir a densidade atrav\u00e9s do par\u00e2metro  . Para o \n\nmesmo vetor  , essa aproxima\u00e7\u00e3o \u00e9 conhecida por fun\u00e7\u00e3o de verossimilhan\u00e7a, e \u00e9 definida por \n\n                                          \n\n \n\n   \n\n \n\n\n\n \n\n                                                  \n\n  \n\n40 \n\n \n\nH\u00e1 de se notar que essa fun\u00e7\u00e3o de verossimilhan\u00e7a n\u00e3o \u00e9 uma densidade de probabilidade, \n\npois ela n\u00e3o precisa ser uma fun\u00e7\u00e3o aditiva, portanto n\u00e3o pode ser considerada uma medida de \n\nprobabilidade. \n\nComo visto no cap\u00edtulo 3, \u00e9 comum e mais conveniente trabalhar com o logaritmo da \n\nfun\u00e7\u00e3o de probabilidade. \u00c9 costumeiro, ainda, dividir a fun\u00e7\u00e3o de verossimilhan\u00e7a pela sua \n\nm\u00e9dia. Essa aproxima\u00e7\u00e3o pode ser definida por \n\n    \n \n\n \n                   \n\n \n\n \n        \n\n  \n\n   \n\n \n\nO m\u00e9todo de m\u00e1xima verossimilhan\u00e7a estima   , atrav\u00e9s da maximiza\u00e7\u00e3o da fun\u00e7\u00e3o \n\n        .Logo, o estimador de m\u00e1xima verossimilhan\u00e7a    \u00e9 definido por  \n\n      \n    \n\n                 \n\n \n\n5.2.2 M\u00e9todos N\u00e3o-Param\u00e9tricos \n\n \n\nEm um m\u00e9todo n\u00e3o-param\u00e9trico, n\u00e3o \u00e9 conhecida a distribui\u00e7\u00e3o que gerou os dados nem \n\nse admite que essa distribui\u00e7\u00e3o possa ser expressa analiticamente, sendo necess\u00e1rio exprimir a \n\ndensidade de probabilidade de forma num\u00e9rica. \n\nO primeiro passo para a aplica\u00e7\u00e3o desse m\u00e9todo \u00e9 calcular uma estimativa para a fun\u00e7\u00e3o \n\ndensidade de probabilidade a partir dos dados de treino. Em geral, se   \u00e9 a probabilidade de um \n\nvetor  , no conjunto de dados     pode ser expressa por \n\n             \n\n \n\n \n\nSe   \u00e9 um espa\u00e7o com   amostras, e dessas, est\u00e3o dispon\u00edveis   amostras nos dados, \n\natrav\u00e9s da probabilidade binomial, t\u00eam-se \n\n     \n \n \n            \n\ne sua esperan\u00e7a \u00e9        . \n\n\n\n \n\n                                                  \n\n  \n\n41 \n\n \n\nComo a distribui\u00e7\u00e3o binomial tem um pico muito alto na sua esperan\u00e7a, o n\u00famero de \n\namostras    deve ser aproximadamente igual ao valor esperado, ou seja,      , ou seja, para \n\num modelo discreto o suficiente,        . \n\nSe   \u00e9 uma regi\u00e3o pequena de forma que      n\u00e3o tenha grandes varia\u00e7\u00f5es, \u00e9 poss\u00edvel \n\naproximar-se a Probabilidade P por (Box ET AL, 1978): \n\n             \n\n \n\n         \n\n onde      e   \u00e9 o volume da regi\u00e3o  . \n\nComo              e       , a combina\u00e7\u00e3o dessas duas equa\u00e7\u00f5es resulta em \n\n       \n  \n  \n\n \n\nCom o intuito de estimar a densidade em   , define-se   subconjuntos de  , de forma que \n\n  esteja contido em     com         . Logo podemos definir as probabilidades marginais \n\ncomo \n\n       \n  \n   \n\n \n\ne            quando \n\n   \n   \n\n     \n\n   \n   \n\n     \n\n   \n   \n\n  \n \n\n   \n\nOs limites acima, para a converg\u00eancia        \n  \n\n   \n, n\u00e3o s\u00e3o numericamente computados, \n\npois dentro da possibilidade desses limites, \u00e9 mais interessante calcular a probabilidade p(x) e n\u00e3o \n\nsua aproxima\u00e7\u00e3o para essas   amostras. \n\nDada as condi\u00e7\u00f5es acima para m\u00e9todos n\u00e3o-param\u00e9tricos, \u00e9 comum o uso do algoritmo de \n\nJanelas de Parzen, quando    \n  \n\n  \n e do algoritmo dos K vizinhos mais pr\u00f3ximos, tamb\u00e9m \n\nconhecido por K-NN do ingl\u00eas ?K-nearest neighbour?, quando      . \n\n \n\n5.2.2.1 M\u00e9todo das Janelas de Parzen \n\n \n\n\n\n \n\n                                                  \n\n  \n\n42 \n\n \n\nO m\u00e9todo das janelas de Parzen'' (Parzen, 1962), funciona como um histograma cont\u00ednuo \n\nonde A regi\u00e3o    \u00e9 centralizada no ponto  , onde se \u00e9 desejada a estimativa da densidade \n\n(Bishop, 1995).  \n\nEspacialmente, assume-se que a regi\u00e3o    \u00e9 um cubo de   dimens\u00f5es e com aresta de \n\ntamanho   . O n\u00famero de amostras em cada regi\u00e3o \u00e9 obtido da fun\u00e7\u00e3o de janela \n\n                    \n \n\n \n                     \n\n  \n\nAs estimativas s\u00e3o dadas por  \n\n       \n    \n\n  \n \n\n \n\n   \n\n \n\n    \n \n\n \n \n\n \n\n  \n  \n\n    \n  \n\n \n\n \n\n   \n\n   \n\nO tamanho da janela tem grande influ\u00eancia na estimativa, pois se a janela \u00e9 muito grande, \n\nela acaba repetindo informa\u00e7\u00f5es j\u00e1 observadas em outras janelas. Se a janela \u00e9 pequena, uma \n\nquantidade de informa\u00e7\u00f5es ser\u00e1 descartada para os c\u00e1lculos da estimativa. \n\nA converg\u00eancia do m\u00e9todo \u00e9 aferida se \n\n   \n   \n\n                         \n   \n\n  \n       \n\nonde        \u00e9 a m\u00e9dia e   \n     \u00e9 a vari\u00e2ncia da vari\u00e1vel aleat\u00f3ria      . Pelas seguintes \n\npremissas, \n\n   \n   \n\n                  \n   \n\n      \n\na converg\u00eancia \u00e9 garantida. \n\nO m\u00e9todo de janela de Parzen apresenta certos problemas, como a escolha \u00f3tima do \n\ntamanho da janela, ou a sele\u00e7\u00e3o inicial do volume de  .   \n\n \n\n5.3.2.2 K Vizinhos mais Pr\u00f3ximos \n\n \n\nO m\u00e9todo dos k vizinhos mais pr\u00f3ximos, cuja sigla \u00e9 K-NN do ingl\u00eas K-nearest \n\nNeighbour, \u00e9 um algoritmo classificador baseado na dist\u00e2ncia estrutural de um conjunto de testes \n\ndo conjunto de treino (Cover, 1967). Dado um conjunto de testes com v\u00e1rias amostras, cada \n\n\n\n \n\n                                                  \n\n  \n\n43 \n\n \n\namostra ser\u00e1 analisada espacialmente e seu r\u00f3tulo (classifica\u00e7\u00e3o) ser\u00e1 igual a do maior n\u00famero de \n\nvizinhos na vizinhan\u00e7a escolhida. \n\nO m\u00e9todo K-NN \u00e9 considerado um tipo de aprendizado pregui\u00e7oso, pois dado um \n\nconjunto de treino, ele s\u00f3 atua localmente e seus c\u00e1lculos s\u00f3 s\u00e3o efetuados caso seja tamb\u00e9m \n\ninserido um conjunto de testes. Apesar disso, o K-NN est\u00e1 entre os algoritmos mais simples da \n\nAprendizagem da M\u00e1quina, o que torna sua utiliza\u00e7\u00e3o recomendada para casos onde \n\ncomputa\u00e7\u00f5es probabil\u00edsticas ou escolhas iniciais podem complicar a solu\u00e7\u00e3o. De fato, a \u00fanica \n\nentrada necess\u00e1ria, al\u00e9m dos conjuntos de Teste e Treino, \u00e9 o n\u00famero de vizinhos   da vizinhan\u00e7a \n\n(Toussaint, 2005) \n\n \n\n \n\n                                                                 Figura 5. 2  Vizinhos Utilizados para Diferentes N\u00fameros de K \n\n \n\nSeja              um conjunto de treino com   amostras rotuladas, ou seja, com uma \n\nidentifica\u00e7\u00e3o relacionada a cada amostrada e   um ponto que necessita ser classificado. \n\nO ponto mais pr\u00f3ximo       , matematicamente, \u00e9 o ponto onde \n\n\n\n \n\n                                                  \n\n  \n\n44 \n\n \n\n       \n     \n\n \n        \n\ncom         \u00e9 a dist\u00e2ncia entre os pontos        . \n\nSe     e        \u00e9 o ponto mais pr\u00f3ximo de  , o r\u00f3tulo atribu\u00eddo ao ponto  , ser\u00e1 o \n\nmesmo r\u00f3tulo do ponto     \n\nAnaliticamente, Se       e                      s\u00e3o os   pontos mais pr\u00f3ximos de  , \n\no r\u00f3tulo atribu\u00eddo ao ponto x, ser\u00e1 o r\u00f3tulo de maior freq\u00fc\u00eancia entre os elementos de  . \n\nCostuma-se escolher o n\u00famero de vizinhos   como sendo \u00edmpar, evitando poss\u00edveis \n\nempates entre os n\u00fameros de classes para uma classifica\u00e7\u00e3o. Na necessidade de um desempate, os \n\npontos de   que tem maior peso nos r\u00f3tulos, s\u00e3o exatamente os mais pr\u00f3ximos de    \n\nN\u00e3o existem regras que limitam o n\u00famero de vizinhos inferiormente ou superiormente, \n\nmas algumas observa\u00e7\u00f5es devem ser levadas em conta, considerando-se que existem   amostras \n\nde treino: \n\nSe   \u00e9 muito grande, dentro do espa\u00e7o de   amostras, o r\u00f3tulo atribu\u00eddo ao ponto de teste \n\n , pode conter um erro, devido ao grande n\u00famero de outras classifica\u00e7\u00f5es, que n\u00e3o as corretas \n\nespacialmente. \n\nSe   tem um determinado r\u00f3tulo, mas existem poucas amostras      \n  com o mesmo \n\nr\u00f3tulo, escolher um n\u00famero   de vizinhos muito grande, pode tamb\u00e9m induzir a classifica\u00e7\u00e3o \n\nerr\u00f4nea, pelo mesmo motivo da afirma\u00e7\u00e3o anterior. \n\n \n\n5.3 Aprendizagem N\u00e3o-Supervisionada \n\n \n\nOs algoritmos que se enquadram na categoria de aprendizagem n\u00e3o-supervisionada, \n\nassumem que os r\u00f3tulos de seus elementos n\u00e3o s\u00e3o conhecidos. Seu problema est\u00e1 relacionado \n\nem determinar como os dados est\u00e3o organizados, ou seja, separar os dados em classes, entretanto \n\nsem dar um r\u00f3tulo a essa classe. \n\nO m\u00e9todo consiste em encontrar padr\u00f5es nos dados de entrada e ent\u00e3o, arbitrariamente, \n\norganiza os padr\u00f5es em categorias. Se dois elementos t\u00eam padr\u00f5es semelhantes, ambos ter\u00e3o a \n\nmesma classe dentro da aprendizagem n\u00e3o-supervisionada. Se algum valor de entrada apresenta \n\num padr\u00e3o que n\u00e3o se assemelha a de nenhuma classe, o algoritmo cria uma nova classe para \n\nabrigar esse valor de entrada. \n\n\n\n \n\n                                                  \n\n  \n\n45 \n\n \n\nIterativamente, seja um conjunto de dados             e o primeiro valor de entrada \n\nseja             . O algoritmo encontra um padr\u00e3o para    e cria uma Classe A, tal que     . \n\nPara o segundo valor de entrada               , se o algoritmo encontra o mesmo padr\u00e3o \n\nencontrado para   , a Classe   incorpora tamb\u00e9m a entrada    , se n\u00e3o, \u00e9 criado uma Classe B tal \n\nque     . E assim sucessivamente para todos os elementos de  .  \n\nAs aprendizagens n\u00e3o-supervisionadas tamb\u00e9m podem ser divididas em dois grupos, \n\nm\u00e9todos param\u00e9tricos e m\u00e9todos n\u00e3o-param\u00e9tricos, assim como eram divididas as aprendizagens \n\nsupervisionadas. Entretanto, as aprendizagens n\u00e3o supervisionadas, por car\u00e1ter de aplica\u00e7\u00e3o est\u00e3o \n\nassociadas a problemas onde n\u00e3o existem informa\u00e7\u00f5es pr\u00e9vias sobre os dados. Dessa foram, os \n\nm\u00e9todos param\u00e9tricos s\u00e3o raramente usados, pois sua aplica\u00e7\u00e3o implicaria em uma contradi\u00e7\u00e3o \n\nno contexto do pr\u00f3prio algoritmo. Com essa premissa, ser\u00e3o apresentados apenas os m\u00e9todos \n\nn\u00e3o-param\u00e9tricos, que abrigam a grande maioria dos algoritmos utilizados \n\nComo exemplos de m\u00e9todos n\u00e3o-param\u00e9tricos de aprendizagem n\u00e3o supervisionada, \n\npodem-se citar: Mapas Auto Organiz\u00e1veis, An\u00e1lise de Agrupamento e a pr\u00f3pria An\u00e1lise de \n\nComponentes Independentes. \n\n \n\n5.3.1 Mapas Auto-Organiz\u00e1veis \n\n \n\nOs mapas auto-organiz\u00e1veis, com sigla SOM do ingl\u00eas Self-Organizing Maps, \u00e9 um tipo \n\nde rede neural artificial (Michie, 1994) que discretiza o espa\u00e7o de entrada das amostras de \n\ntreinamento produzindo uma representa\u00e7\u00e3o de baixa dimens\u00e3o, normalmente bidimensional, \n\nchamado de mapa. Dentro da categoria das redes neurais artificiais, o SOM tem um \n\ncomportamente \u00edmpar,  no sentido de que o seu algortimo contempla uma fun\u00e7\u00e3o de vizinhan\u00e7a \n\npara preservar as propriedades topol\u00f3gicas (Lima,1993) do espa\u00e7o de entrada. \n\nO modelo de SOM opera em dois modos: treinamento e mapeamento (Kohonen,1982).A \n\nparte do treinamento constr\u00f3i o mapa usando exemplos de entrada. Este processo \u00e9 do tipo \n\ncompetitivo, tamb\u00e9m chamado de quantiza\u00e7\u00e3o vetorial. Quanto ao mapeamento,este classifica \n\nautomaticamente um novo vetor de entrada. \n\nOs mapas auto-organiz\u00e1veis consistem em componentes estruturais chamadas de \n\nneur\u00f4nios. Associado a cada neur\u00f4nio,existe um vetor-peso de mesma dimens\u00e3o dos vetores dos \n\ndados de entrada e uma posi\u00e7\u00e3o espacial do mapa. O arranjo usual de neur\u00f4nios \u00e9 um espa\u00e7o \n\n\n\n \n\n                                                  \n\n  \n\n46 \n\n \n\nregular em uma grade hexagonal ou retangular, de forma que o mapeamento de um espa\u00e7o de \n\nentrada seja maior que o espa\u00e7o do mapa. Um elemento de entrada ser\u00e1 associado a um \n\ndeterminado neur\u00f4nio quando o vetor-peso relacionado a este neur\u00f4nio \u00e9 o mais pr\u00f3ximo poss\u00edvel \n\ndo elemento de entrada.  \n\nSeja    um vetor de entrada k-dimensional, tal que                \n  e           \n\nseja o n\u00famero de vetores de entrada do problema. Se   \u00e9 o espa\u00e7o de sa\u00eddas do SOM, cada \n\nneur\u00f4nio  , atrav\u00e9s de um vetor peso  , est\u00e1 associado \u00e0s entradas   , de forma que \n\n \n\n                     \n \n  \n\n \n\n5.3.2 An\u00e1lise de Agrupamento \n\n \n\nA an\u00e1lise de agrupamento,tradu\u00e7\u00e3o do ingl\u00eas clustering, \u00e9 a separa\u00e7\u00e3o de um conjunto de \n\nobserva\u00e7\u00f5es em subconjuntos (clusters) de modo que as observa\u00e7\u00f5es no mesmo cluster s\u00e3o \n\nsimilares em algum sentido estrutural (Aldenderfer, 1984). Os algoritmos de an\u00e1lise de \n\nagrupamento podem ser divididos em m\u00e9todos hier\u00e1rquicos ou particionais. Os algoritmos \n\nhier\u00e1rquicos encontram subgrupos sucessivos de neur\u00f4nios, usando clusters previamente \n\nestabelecidas, enquanto que os algoritmos particioanis determinam todos os clusters de uma vez.  \n\nH\u00e1 a possibilidade desses grupos reterem n\u00e3o apenas os elementos de cada cluster, mas \n\ncomo a caracter\u00edstica em comum entre estes elementos. Esse tipo de algoritmo \u00e9 conhecido como \n\nbi-clustering e sua resultante \u00e9 uma matriz de dados, onde as amostras e colunas s\u00e3o agrupadas \n\nsimultaneamente. \n\nA an\u00e1lise de agrupamentos, utiliza-se de conceitos m\u00e9tricos para determinar os clusters e \n\nos elementos que comp\u00f5e estes clusters (Romesburg,2004). Nesse aspecto, este m\u00e9todo \n\nassemelha-se ao m\u00e9todo K-NN, baseado nas mesmas m\u00e9tricas supracitadas (Euclidiana, \n\nMahalanobis, Manhattan). De fato, as escolhas estruturais dos algoritmos e os c\u00e1clculos que \n\ngovernam ambos os m\u00e9todos s\u00e3o similares, a n\u00e3o ser quanto a necessidade de um grupo de \n\ntreinamento, que o K-NN precisa, enquanto que a an\u00e1lise de agrupamento n\u00e3o. \n\n \n\n\n\n \n\n \n\n  \n\n47 \n\n \n\nCap\u00edtulo 6 \n\n \n\nM\u00e9todos \n\n \n\nObjetiva-se a constru\u00e7\u00e3o de um procedimento para o reconhecimento e classifica\u00e7\u00e3o de \n\nf\u00e1cies litol\u00f3gicas e de suas qualidades quanto \u00e0s caracter\u00edsticas \u00e0 possibilidade da rocha ser \n\nReservat\u00f3rio, ou seja, abrigar \u00f3leo ou g\u00e1s. Para esse novo procedimento, trabalhou-se com um \n\nm\u00e9todo de Separa\u00e7\u00e3o Cega de Sinais N\u00e3o-Supervisionado, a An\u00e1lise de Componentes \n\nIndependentes, e um m\u00e9todo de Reconhecimento de Padr\u00f5es Supervisionado, o K-Vizinhos mais \n\nPr\u00f3ximos. Uma aplica\u00e7\u00e3o correlata foi feita anteriormente (Sancevero, 2008) com \u00eaxito. \n\nQuanto \u00e0 abordagem, de acordo com (Gil, 1991), essa disserta\u00e7\u00e3o e sua metodologia, \n\npodem ser descritas como: \n\nPesquisa aplicada: conhecimentos pr\u00e1ticos s\u00e3o dirigidos \u00e0 solu\u00e7\u00e3o de problemas espec\u00edficos; \n\nAbordagem Quantitativa e Qualitativa: as informa\u00e7\u00f5es t\u00eam valores quantific\u00e1veis e din\u00e2micos \n\npara serem analisados e classificados.  \n\nPesquisa Explorat\u00f3ria: avalia o problema com maior familiaridade visando torn\u00e1-lo expl\u00edcito e \n\na construir hip\u00f3teses sobre o mesmo. \n\nPesquisa Explicativa: identifica os fatores que comp\u00f5e ou contribuem para a ocorr\u00eancia de \n\nobserva\u00e7\u00f5es f\u00edsicas. \n\nPara os testes foi utilizado como compilador o MATLAB R2008a, e as toolbox FastICA \n\n2.5(Hyv\u00e4rinen) e knnclassification, dispon\u00edvel no PUDN(Programmers United Develop Net). \n\n6.1 Dados \n \n\nNeste presente trabalho, dois conjuntos de dados foram escolhidos para servirem como \n\nbase para os m\u00e9todos: Dados de Perfil do Campo de Namorado e dados s\u00edsmicos obtidos de \n\nrela\u00e7\u00f5es n\u00e3o-lineares com esses dados, localizado na Bacia de Campos. A escolha do Campo de \n\nNamorado \u00e9 motivada pela abundante quantidade de informa\u00e7\u00f5es sobre esse campo, sendo \n\nconsiderado um campo escola (Barboza, 2005). \n\n \n\nA Bacia de Campos, localizada na por\u00e7\u00e3o sudeste do Brasil, ao longo da costa norte do \n\nEstado do Rio de Janeiro, possui uma \u00e1rea de 100 mil Km2, at\u00e9 a l\u00e2mina d\u2018\u00e1gua de 3.000 m \n\n(Sacco ET AL, 2007). O Campo de Namorado encontra-se na parte centro-norte da zona de \n\n\n\n \n\n \n\n  \n\n48 \n\n \n\nacumula\u00e7\u00f5es de hidrocarbonetos da Bacia de Campos, a 80 km da costa, em profundidade d'\u00e1gua \n\nentre 140 m e 250 m e foi descoberto em 1975(Vidal ET AL, 2007). Foram utilizados sete po\u00e7os \n\ndesse campo: NA01, NA02, NA04, NA07, NA011A, RJS234, RJS42.   \n\nTabela 6. 1 F\u00e1cies Litol\u00f3gicas \n\nF\u00e1cies   Descri\u00e7\u00e3o \n\n1 INLD Interlaminado Lamoso Deformado \n\n2 CBC Conglomerados e Brechas Carbon\u00e1ticas \n\n3 DAL Diamictito Arenoso Lamoso \n\n4 CR Conglomerados Residuais \n\n6 AGA Arenito Grosso, Amalgamado \n\n7 AMFL Arenito M\u00e9dio Fino Laminado \n\n8 AMGM Arenito M\u00e9dio Gradado ou Maci\u00e7o \n\n9 AMC Arenito M\u00e9dio Cimentado \n\n10 AFI Arenito/Folhelho Interestratificado \n\n11 AFFI Arenito/Folhelho Finamente Interestratificado \n\n12 SAE Siltito Argiloso Estratificado \n\n13 ISAM Interlaminado Siltito Argiloso e Marga \n\n14 FR Folhelho Radioativo \n\n15 IAB Interlaminado Arenoso Bioturbado \n\n16 ISFD Interlaminado de Siltito e Folhelho, Deformado, Bioturbado \n\n17 MB Marga Bioturbada \n\n18 R Ritmito \n\n19 AG Arenito Glaucon\u00edtico \n\n20 FSMN Folhelho Siltico com N\u00edveis de Marga Bioturbada \n\n21 ACFE Arenito Cimentado, com Fei\u00e7\u00f5es de Escorregamento \n\n22 SAAD Siltito Argiloso/Arenito Deformado \n\n23 AMFLC Arenito M\u00e9dio/Fino Laminado Cimentado \n\n24 ISFI Interestratificado Siltito/Folhelho Intensamente Bioturbados \n\n25 MBO Marga Bioturbada Outra \n\n26 FC Folhelho Carbonoso \n\n27 AMMF Arenito Maci\u00e7o Muito Fino \n\n28 SAA Siltito Areno-Argiloso \n\n29 ISF Interlaminado Siltito/Folhelho \n\n       \n\n \n\n \n\n6.1.1 Dados de Perfil \n \n\n\n\n \n\n \n\n  \n\n49 \n\n \n\nOs dados de Perfil utilizados neste trabalho s\u00e3o: Perfil S\u00f4nico (DT), Raio Gama (GR), \n\nResistividade (ILD), Densidade (RHOB), Porosidade Neutr\u00f4nica (NPHI). Ainda de Acordo com \n\n(Sacco ET AL, 2007), esses perfis s\u00e3o explicados na Tabela 6.2 \n\nTabela 6. 2 Perfis Geol\u00f3gicos \n\nDT Medi\u00e7\u00e3o do tempo que um pulso sonoro leva para atravessar determinado \n\nintervalo deforma\u00e7\u00e3o geol\u00f3gica. Relaciona inversamente o tempo de tr\u00e2nsito e a \n\nporosidade da forma\u00e7\u00e3o rochosa. Exemplo: Quanto maior o tempo de tr\u00e2nsito, \n\nmenor a densidade da forma\u00e7\u00e3o. \n\nGR Medi\u00e7\u00e3o da emiss\u00e3o radioativa natural de rochas que cont\u00e9m pot\u00e1ssio 40 ou \n\nelementos da s\u00e9rie ur\u00e2nio-t\u00f3rio. Usada para o c\u00e1lculo volum\u00e9trico de argila e \n\nc\u00e1lculo volum\u00e9trico de rocha no reservat\u00f3rio que cont\u00e9m argila \n\nILD Medi\u00e7\u00e3o da propriedade da forma\u00e7\u00e3o geol\u00f3gica em resistir \u00e0 passagem de uma \n\ncorrente el\u00e9trica, identificando os tipos de fluidos que preenchem os poros do \n\nreservat\u00f3rio, fornecendo informa\u00e7\u00f5es para o c\u00e1lculo de satura\u00e7\u00e3o de \u00e1gua. Caso \n\nseja conhecido o valor de NPHI \u00e9 poss\u00edvel estimar quantidade de \n\nhidrocarbonetos presentes nos poros \n\nRHOB Medi\u00e7\u00e3o da densidade m\u00e9dia de uma unidade litol\u00f3gica da forma\u00e7\u00e3o, baseando-\n\nse na emiss\u00e3o de raios-gama (provenientes do c\u00e9sio 137), que colidem com os \n\nel\u00e9trons presentes na rocha e ap\u00f3s essas colis\u00f5es, os raios gama que retornam s\u00e3o \n\ncontabilizados. Exemplo: Quanto menor a contagem de emiss\u00f5es gama que \n\nretornaram, maior a densidade da rocha. \n\nNPHI Medi\u00e7\u00e3o do \u00edndice de hidrog\u00eanio na forma\u00e7\u00e3o litol\u00f3gica atrav\u00e9s da emiss\u00e3o de \n\nn\u00eautrons. Seu princ\u00edpio baseia-se na inexist\u00eancia de carga el\u00e9trica do n\u00eautron que \n\ntem massa relativa ao hidrog\u00eanio. Os n\u00eautrons penetram a forma\u00e7\u00e3o geol\u00f3gica, e \n\ncolidem com os \u00e1tomos dos diferentes elementos. Essa colis\u00e3o desacelera os \n\nn\u00eautrons reduzindo-os at\u00e9 n\u00edveis termais quando retornam aos sensores \n\nA freq\u00fc\u00eancia de amostragem dos dados de perfil \u00e9 0.2m, ou seja, os perfis apresentam \n\nvalores a cada 20 cm de profundidade. Devido a seu alto custo, n\u00e3o existe testemunho probat\u00f3rio \n\npara todas as profundidades, sendo utilizados para aferi\u00e7\u00e3o do resultado apenas a parte \n\ntestemunhada dos po\u00e7os. Das 4732 amostras nos dados, existem 1950 amostras dispon\u00edveis com \n\ntestemunho. S\u00e3o essas amostras testemunhadas que s\u00e3o utilizadas nos testes desta pesquisa. \n\nA varia\u00e7\u00e3o de valores de cada perfil est\u00e1 atrelada \u00e0 natureza f\u00edsica que esta se prop\u00f5e a \n\nmedir. Dentro das amostras utilizadas essa varia\u00e7\u00e3o \u00e9 de: \n\n\n\n \n\n \n\n  \n\n50 \n\n \n\n \n\n52,0080 &lt;DT &lt;  120,9727 \n\n21,1875  &lt;GR &lt;   109,6797 \n\n-1,0420   &lt;  ILD &lt; 3229,000 \n\n0,578    &lt;NPHI  &lt;37,9408 \n\n1,7336&lt;RHOB&lt;2.7410 \n\nUm exemplo, retirado dos dados, de como as informa\u00e7\u00f5es est\u00e3o dispostas est\u00e1 na Tabela \n\n6.3 \n\n \n\nTabela 6. 3 exemplo de amostra de Dado de Perfil \n\nPO\u00c7O PROF DT GR ILD NPHI RHOB TEST \n\nNA01 3004.4 86.2667 58.9102 15.0469 22.0139 2.195 AMGM \n\n \n\nTal configura\u00e7\u00e3o \u00e9 entendida por uma amostra dos dados. \n\nNa primeira c\u00e9lula dessa tabela, encontra-se identificado o po\u00e7o de onde foi retirada essa \n\namostra de perfil; na segunda c\u00e9lula est\u00e1 a qual profundidade, em rela\u00e7\u00e3o \u00e0 superf\u00edcie, esta \n\namostra foi captada; na terceira, o perfil S\u00f4nico; na quarta, o Raio Gama; na quinta, a \n\nResistividade; na sexta, a Porosidade Neutr\u00f4nica; na s\u00e9tima, a Densidade e na oitava, o \n\ntestemunho relativo a essa amostra.  \n\n6.1.2 Dados S\u00edsmicos \n\n \n\nA base dos atributos s\u00edsmicos o tra\u00e7o s\u00edsmico complexo, que pode ser descrito como \n\n                 , onde       \u00e9 o tra\u00e7o s\u00edsmico, e      \u00e9 a transformada de Hilbert do tra\u00e7o \n\ns\u00edsmico. Em coordenadas polares, essa equa\u00e7\u00e3o \u00e9 escita como                      onde \n\n                    \u00e9 a amplitude inst\u00e2ntanea (Ampli) e                       a fase \n\ninstant\u00e2nea (Fase). \n\nDa derivada da fase instant\u00e2nea, consegue-se a frequencia instant\u00e2nea (Freq), ou seja, \n\n              . A derivada de uma tra\u00e7o s\u00edsmico(Deri), de acordo com (Russel, 2004), \u00e9 um \n\natributo recursivo e \u00e9 calculado atrav\u00e9s da aplica\u00e7\u00e3o de um operador ao longo do conjunto de \n\ntra\u00e7os. A aplica\u00e7\u00e3o de um operador diferen\u00e7a recursiva resulta na primeira derivada do tra\u00e7o \n\ns\u00edsmico. Isto \u00e9 feito tomando a diferen\u00e7a entre amostras adjacentes, onde            . \n\n\n\n \n\n \n\n  \n\n51 \n\n \n\nA imped\u00e2ncia ac\u00fastica Determin\u00edstica (DDI) \u00e9 baseada na minimiza\u00e7\u00e3o do erro entre a \n\nConvolu\u00e7\u00e3o Forward da refletividade do perfil da imped\u00e2ncia estimada e das amplitudes s\u00edsmicas \n\nde cada tra\u00e7o (Francis, 2005),            \n                     \n\n       , \n\nonde S = WR \u00e9 a convolu\u00e7\u00e3o escrita em sua forma multiplicativa e                     com \n\n   sendo a Imped\u00e2ncia Ac\u00fastica. A imped\u00e2ncia ac\u00fastica Estoc\u00e1stica (DSI) \u00e9 a m\u00e9dia de 50 \n\nrealiza\u00e7\u00f5es, isto \u00e9, 50 simula\u00e7\u00f5es estoc\u00e1sticas e ela gera um conjunto de representa\u00e7\u00f5es \n\nalternativas das imped\u00e2ncias heterog\u00eaneas de acordo com o volume de s\u00edsmica 3D. A tabela 6.4 \n\nresume as equa\u00e7\u00f5es e d\u00e1 um exemplo pata a impedancia ac\u00fastica retirada da invers\u00e3o estoc\u00e1stica \n\npois as equa\u00e7\u00f5es envolvidas  n\u00e3o est\u00e3o associadas diretadmente a esse atributo. \n\n \n\n    Tabela 6. 4 Dados S\u00edsmicos \n\nDDI            \n                     \n\n        \n\nDSI A imped\u00e2ncia Ac\u00fastica retirada da Invers\u00e3o S\u00edsmica Estoc\u00e1stica, ajuda na \n\nsolu\u00e7\u00e3o de quest\u00f5es interessantes (Dubrule ET AL, 1997), como constru\u00e7\u00e3o de \n\nrepresenta\u00e7\u00f5es geol\u00f3gico-realistas 3D e quantifica\u00e7\u00e3o da Incerteza sobre a gera\u00e7\u00e3o \n\nde modelos ou ?realiza\u00e7\u00f5es?. \n\n \n\nFase                       \n\nFreq                \n\nDeri             \n\nAmpli                     \n\n                                                         \n\n \n\n \n\nA varia\u00e7\u00e3o dos valores dos dados s\u00edsmicos \u00e9 de: \n\n \n\n5848,82 &lt;DDI &lt;  7875,3 \n\n5880,89  &lt;DSI &lt;   7786,4 \n\n-3,1406   &lt;  Fase &lt; 3,137,8 \n\n-2854,6    &lt;Freq  &lt;1694,91 \n\n-1683620<Deri &lt;1436740 \n\n\n\n \n\n \n\n  \n\n52 \n\n \n\n87,9947 &lt;Ampli &lt;28000,2 \n\n \n\nTabela 6. 5 Exemplo de Amostra de Dado S\u00edsmico \n\nPOCO DDI DSI Fase Freq Deri Ampli \n\nNA01A 6361.16 6344.68 -178.299 681.609 94755.7 7199.26 \n\n \n\nO intervalo de amostragem dos dados s\u00edsmicos gira em torno de 10 a 20 metros, ou seja, \n\ntem frequ\u00eancia muito maior que o intervalo de amostragem de dados de po\u00e7o. Como o \n\ntestemunho est\u00e1 relacionado aos dados de perfil, \u00e9 necess\u00e1ria uma mudan\u00e7a de escala para \n\nintegrar os dados s\u00edsmicos a um r\u00f3tulo de testemunho.  \n\nEsses dados s\u00edsmicos, em sua configura\u00e7\u00e3o original, representam informa\u00e7\u00f5es n\u00e3o de uma \n\namostra, mas de uma regi\u00e3o que abrange v\u00e1rias amostras, logo, para os dados de perfil foi \n\nnecess\u00e1ria, a constru\u00e7\u00e3o de m\u00e9dias que se referem \u00e0s amostras que tem participa\u00e7\u00e3o na resposta \n\ns\u00edsmica.   \n\nUma amostra utilizada nesse teste pode conter um n\u00famero vari\u00e1vel de amostras originais \n\nde dados de po\u00e7o, dependendo do n\u00famero de amostras que est\u00e3o participando da influ\u00eancia do \n\ndado s\u00edsmico, ou seja, para alguma das amostras utilizadas nesse teste, podem estar sendo usadas \n\nvinte amostras de dados de perfil, enquanto que para outra amostra, podem estar sendo usadas \n\napenas cinco. \n\nNessa configura\u00e7\u00e3o, t\u00eam-se v\u00e1rias respostas de uma mesma informa\u00e7\u00e3o para apenas uma \n\namostra e uma alternativa para a utiliza\u00e7\u00e3o dessas, sem trabalhar com dados err\u00f4neos, foi o \n\nc\u00e1lculo das m\u00e9dias dessas informa\u00e7\u00f5es para cada dado s\u00edsmico. Uma informa\u00e7\u00e3o s\u00edsmica tem sua \n\nresposta ligada a dez amostras dos dados de perfil, portanto, para essa informa\u00e7\u00e3o s\u00edsmica, \n\nexistem dez respostas para o perfil DT, dez respostas para o perfil GR, e dez repostas tamb\u00e9m \n\npara os perfis RHOB, NPHI e ILD. Como as t\u00e9cnicas Geoestat\u00edsticas funcionam com \n\ninforma\u00e7\u00f5es de uma amostra, essas m\u00faltiplas respostas devem ser aglutinadas em apenas uma \n\namostra, e para isso foi escolhida a m\u00e9dia entre elas. \n\nEssas m\u00e9dias s\u00e3o calculadas para os cinco tipos de dados de perfil, assim como s\u00e3o \n\ncalculados os desvios-padr\u00e3o dos dados, mas esses n\u00e3o t\u00eam uma utiliza\u00e7\u00e3o comprovada para os \n\ndados quando trabalhados com a An\u00e1lise de Componente Independente e, portanto n\u00e3o s\u00e3o \n\nutilizados como vari\u00e1veis para o ICA. \n\n\n\n \n\n \n\n  \n\n53 \n\n \n\nOs dados de testemunho tamb\u00e9m devem ser trabalhados para que representem apenas uma \n\namostra, pois assim como os dados de Perfil, v\u00e1rias respostas est\u00e3o ligadas \u00e0 apenas uma amostra \n\ns\u00edsmica. Mas, diferente das respostas nos dados de perfil, as repostas encontradas no testemunho \n\ns\u00e3o pouco vari\u00e1veis e, geralmente, tem predomin\u00e2ncia de uma resposta. As amostras, como \n\ntestemunho, podem ser classificadas como Indefinidas, N\u00e3o-Reservat\u00f3rio, Poss\u00edvel Reservat\u00f3rio \n\nou Reservat\u00f3rio. E a escolha para a classifica\u00e7\u00e3o da amostra \u00e9 em cima daquela que tem maior \n\nfreq\u00fc\u00eancia dentro das amostras originais dos dados. Se uma amostra de dado s\u00edsmico tem vinte e \n\nsete amostras de dados de po\u00e7o, de forma que elas estejam distribu\u00eddas como na identifica\u00e7\u00e3o 6.3, \n\na classifica\u00e7\u00e3o para essa amostra \u00e9 de Reservat\u00f3rio. \n\nTabela 6. 6 Separa\u00e7\u00e3o de Testemunho nos Dados S\u00edsmicos com predomin\u00e2ncia \n\nIndefinida Reser \n\nPossivel  \n\nReser N\u00e3o Reser \n\n0 20 1 6 \n\nPara esse caso, a escolha da classifica\u00e7\u00e3o n\u00e3o gera nenhuma d\u00favida, devido \u00e0 \n\npredomin\u00e2ncia de uma resposta em rela\u00e7\u00e3o \u00e0s outras. Entretanto, algumas amostras podem conter \n\nrespostas em que a escolha n\u00e3o \u00e9 un\u00e2nime, pois n\u00e3o existe uma larga predomin\u00e2ncia de uma \n\nresposta, como na Tabela 6.7.  \n\nTabela 6. 7  Separa\u00e7\u00e3o de Testemunho nos Dados S\u00edsmicos sem predomin\u00e2ncia  \n\nIndefinida Reser \n\nPossivel  \n\nReser N\u00e3o Reser \n\n0 13 0 13 \n\nA escolha para esse caso s\u00f3 n\u00e3o \u00e9 arbitr\u00e1ria dada \u00e0 natureza do programa criado para \n\nanalisar os dados, que escolher\u00e1 a classifica\u00e7\u00e3o de N\u00e3o Reservat\u00f3rio, pois ser\u00e1 o \u00faltimo dado ser \n\nanalisado.   \n\nEssa perda de informa\u00e7\u00e3o dos dados de reservat\u00f3rio \u00e9 relevada e aceitada a primeira \n\ninst\u00e2ncia, pois a grande maioria das amostras n\u00e3o tem dados relacionados dessa forma e \n\napresentam respostas com classifica\u00e7\u00f5es predominantes em rela\u00e7\u00e3o \u00e0s outras, sobrando poucas \n\namostras que possam encontrar problemas como observados na Identifica\u00e7\u00e3o 6.4. \n\n6.2 Aplica\u00e7\u00e3o dos m\u00e9todos \n \n\n\n\n \n\n \n\n  \n\n54 \n\n \n\nCom a base de dados consolidada, a pr\u00f3xima etapa da pesquisa \u00e9 a aplica\u00e7\u00e3o dos m\u00e9todos \n\nde An\u00e1lise de Componentes Independentes e K-vizinhos mais pr\u00f3ximos. Como as sa\u00eddas de \n\nm\u00e9todos supervisionados s\u00e3o os r\u00f3tulos para cada entrada, a sequ\u00eancia de aplica\u00e7\u00e3o dos m\u00e9todos \n\ncome\u00e7a com a An\u00e1lise de Componentes Independentes, que gera uma sa\u00edda com as caracter\u00edsticas \n\nsupracitada no cap\u00edtulo 2 (sem redund\u00e2ncia, redu\u00e7\u00e3o de dimensionalidade). \n\nEsse novo espa\u00e7o gerado pelo ICA ser\u00e1 divido em duas partes, onde uma dessas ser\u00e1 \n\nutilizada como treino e a outra ser\u00e1 utilizada como teste de classifica\u00e7\u00e3o do K-NN. Os \n\ntestemunhos ligados a parte de treino s\u00e3o os r\u00f3tulos conhecidos utilizados no K-NN. O \n\ntestemunho referente \u00e0 parte de classifica\u00e7\u00e3o n\u00e3o \u00e9 utilizada durante essa aplica\u00e7\u00e3o, afinal o K-\n\nNN devolver\u00e1 um r\u00f3tulo prov\u00e1vel para cada amostra de entrada (teste de classifica\u00e7\u00e3o). Esse \n\nr\u00f3tulo prov\u00e1vel \u00e9 comparado ao testemunho conhecido para averiguar o n\u00famero de predi\u00e7\u00f5es \n\ncorretas do m\u00e9todo. O n\u00famero de predi\u00e7\u00f5es corretas dividido pelo n\u00famero total de entradas \n\n(tentativas) \u00e9 a taxa de acerto do m\u00e9todo. O Fluxograma abaixo ilustra essas aplica\u00e7\u00f5es. \n\n \n\n \n\n                                                            Figura 6. 1 Fluxograma dos m\u00e9todos \n\n \n\nA taxa de acerto configura uma expectativa quantitativa do sucesso do m\u00e9todo entre todas \n\nas tentativas do teste, ou seja, se a taxa de acerto \u00e9 de 80%, isso significa que 80% dos poss\u00edveis \n\nr\u00f3tulos encontrados ao final dos m\u00e9todos eram id\u00eanticos ao testemunho comprovado do dado. \n\n6.3 Treinos e Testes \n \n\nCom a aplica\u00e7\u00e3o da An\u00e1lise de Componentes Independentes nos dados, obt\u00e9m-se um \n\nnovo conjunto de dados, por escolha da mesma dimens\u00e3o. Dessa forma, quanto aos dados de \n\nperfil, esses dados comp\u00f5em uma matriz 1950x5, ou seja, 1950 amostras com cinco colunas, \n\nPr\u00e9-\nProcessamento \n\ndos Dados\n\nAn\u00e1lise de \nComponentes \nIndependentes\n\nEscolha de \ncomponentes, \n\ntreino e \nclassifica\u00e7\u00e3o \n\nK-Vizinhos Mais \nPr\u00f3ximos\n\nC\u00e1lculo da Taxa \nde Acerto\n\n\n\n \n\n \n\n  \n\n55 \n\n \n\nonde essas colunas s\u00e3o os perfis dispon\u00edveis. Os dados s\u00edsmicos, similarmente, t\u00eam dimens\u00e3o \n\n1950x6, onde as colunas s\u00e3o os dados s\u00edsmicos dispon\u00edveis. \n\nCom essa nova base de dados, pode-se escolher com quantos componentes independentes \n\nser\u00e3o feitos os testes no KNN. No caso dos dados de perfil, se forem utilizadas todas as \n\ncomponentes independentes dispon\u00edveis, cada amostra de treino e teste ter\u00e1 cinco dimens\u00f5es. Se \n\nfor utilizada somente uma componente independente, cada amostra de treino e teste ter\u00e1 apenas \n\numa dimens\u00e3o. Assim como se forem escolhidas tr\u00eas componentes independentes, cada amostra \n\nde treino e classifica\u00e7\u00e3o ter\u00e1 tr\u00eas dimens\u00f5es. \n\nCom a base de dados escolhida e os m\u00e9todos j\u00e1 estabelecidos, o pr\u00f3ximo passo \u00e9 escolher \n\nposs\u00edveis conjuntos de Treino e Testes dos m\u00e9todos. A priori, foram testados dois arranjos de \n\nTreinos: Par/Impar e Todos/Um, assim como foram utilizados dois tipos de classifica\u00e7\u00e3o: \n\nLitof\u00e1cies e Reservat\u00f3rio/N\u00e3o Reservat\u00f3rio. \n\n6.3.1 Treinos  \n\n \n\nO treino Par/Impar \u00e9 identificado com esse nome, pois seu treino baseia-se nas amostras \n\npares do dado, enquanto as amostras \u00edmpares s\u00e3o classificadas e recebem o poss\u00edvel r\u00f3tulo do \n\nm\u00e9todo KNN. Dessa forma o conjunto de treino cont\u00e9m 975 amostras e o conjunto de \n\nclassifica\u00e7\u00e3o cont\u00e9m tamb\u00e9m 975 amostras.  \n\nO treino Todos/Um consiste em escolher uma das amostras dispon\u00edveis, considerar como \n\nconjunto de treino todas as outras amostras do dado e considerar como conjunto de teste, essa \n\namostra retirada. Esse processo \u00e9 feito para todas as amostras. Dessa forma, o conjunto de treino \n\n\u00e9 composto por 1949 amostras e o conjunto de teste por uma amostra, onde esse processo \u00e9 \n\nrepetido 1950 vezes, contabilizando um processo por amostra. \n\n6.3.2 Classifica\u00e7\u00e3o \nA classifica\u00e7\u00e3o de Litof\u00e1cies tem dispon\u00edvel para r\u00f3tulo os 29 tipos de rochas dispon\u00edveis \n\nna tabela 6.1, ou seja, cada amostra tem como r\u00f3tulo a rocha referente ao testemunho. Mas \n\napenas 21 destas rochas aparecem nos testemunhos. \n\nA classifica\u00e7\u00e3o Reservat\u00f3rio/N\u00e3o reservat\u00f3rio tem dispon\u00edvel para r\u00f3tulo a possibilidade \n\nde a amostra ser de uma rocha Reservat\u00f3rio, N\u00e3o Reservat\u00f3rio ou Poss\u00edvel Reservat\u00f3rio. Esta \n\nclassifica\u00e7\u00e3o est\u00e1 relacionada \u00e0s rochas, atrav\u00e9s da Figura 6.2. \n\n \n\n\n\n \n\n \n\n  \n\n56 \n\n \n\nF\u00e1cies   Descri\u00e7\u00e3o \n\n1 INLD Interlaminado Lamoso Deformado \n\n2 CBC Conglomerados e Brechas Carbon\u00e1ticas \n\n3 DAL Diamictito Arenoso Lamoso \n\n4 CR Conglomerados Residuais \n\n6 AGA Arenito Grosso, Amalgamado \n\n7 AMFL Arenito M\u00e9dio Fino Laminado \n\n8 AMGM Arenito M\u00e9dio Gradado ou Maci\u00e7o \n\n9 AMC Arenito M\u00e9dio Cimentado \n\n10 AFI Arenito/Folhelho Interestratificado \n\n11 AFFI Arenito/Folhelho Finamente Interestratificado \n\n12 SAE Siltito Argiloso Estratificado \n\n13 ISAM Interlaminado Siltito Argiloso e Marga \n\n14 FR Folhelho Radioativo \n\n15 IAB Interlaminado Arenoso Bioturbado \n\n16 ISFD Interlaminado de Siltito e Folhelho, Deformado, Bioturbado \n\n17 MB Marga Bioturbada \n\n18 R Ritmito \n\n19 AG Arenito Glaucon\u00edtico \n\n20 FSMN Folhelho Siltico com N\u00edveis de Marga Bioturbada \n\n21 ACFE Arenito Cimentado, com Fei\u00e7\u00f5es de Escorregamento \n\n22 SAAD Siltito Argiloso/Arenito Deformado \n\n23 AMFLC Arenito M\u00e9dio/Fino Laminado Cimentado \n\n24 ISFI Interestratificado Siltito/Folhelho Intensamente Bioturbados \n\n25 MBO Marga Bioturbada Outra \n\n26 FC Folhelho Carbonoso \n\n27 AMMF Arenito Maci\u00e7o Muito Fino \n\n28 SAA Siltito Areno-Argiloso \n\n29 ISF Interlaminado Siltito/Folhelho \n\n   \n\n \n\n   Reservat\u00f3rio \n\n   \n\n \n\n  Poss\u00edvel Reservat\u00f3rio \n\n   \n\n \n\n  N\u00e3o Reservat\u00f3rio \n\nFigura 6. 2 Classifica\u00e7\u00e3o Reservat\u00f3rio/N\u00e3o Reservat\u00f3rio \n\n\n\n \n\n \n\n  \n\n57 \n\n \n\nCap\u00edtulo 7 \n\n \n\nResultados e Discuss\u00f5es \n\n \n\n \n\n7.1 Treino Par/\u00cdmpar / Classifica\u00e7\u00e3o de Litof\u00e1cies \n \n\nEssa primeira bateria de resultados utiliza os par\u00e2metros padr\u00e3o do FastICA (Aproxima\u00e7\u00e3o \n\nfun\u00e7\u00e3o-objetiva c\u00fabica e Ortogonaliza\u00e7\u00e3o deflacion\u00e1ria). Quanto ao K-NN, foi utilizado um teste \n\nque consiste em treinar os dados \u00edmpares e classificar os dados pares (as matrizes de dados t\u00eam \n\n4732 amostras referentes a diferentes profundidades, retiradas de sete po\u00e7os) e a classifica\u00e7\u00e3o \n\nrefere-se a todas as vinte e uma litologias dispon\u00edveis no testemunho dos po\u00e7os.  \n\nNo primeiro teste desta bateria, foi computada apenas uma componente, que \u00e9 a primeira \n\ndas componentes independentes encontradas pelo m\u00e9todo FastICA, ou seja, a componente menos \n\ngaussiana poss\u00edvel. Como a ICA tem melhores resultados para as componentes menos gaussianas \n\nposs\u00edveis, foi levantada a hip\u00f3tese de que a resposta para esse teste seria melhor do que os testes \n\nque se aproximavam mais da distribui\u00e7\u00e3o normal. No geral, o primeiro teste apresenta um \n\ncomportamento esperado para esse tipo de busca de classifica\u00e7\u00e3o via KNN,uma vez que o gr\u00e1fico \n\napresenta um aumento na taxa de acerto conforme o n\u00famero de vizinhos cresce, estabilizando-se \n\na partir de um n\u00famero de vizinhos.  \n\n O n\u00famero m\u00ednimo de vizinhos (NMV) deste e de todas as baterias de resultados foi \n\nde tr\u00eas vizinhos. O teste inicia-se com uma taxa de acerto perto dos 23%, um n\u00famero pouco \n\nexpressivo, mesmo para as complexas rela\u00e7\u00f5es envolvidas com dados geol\u00f3gicos. Esse n\u00famero \n\ntem um comportamento crescente, at\u00e9 o teste com o vizinho com o m\u00e1ximo acerto (VMA), que \n\nneste caso \u00e9 o teste com 47 vizinhos, onde o teste encontra o auge do seu acerto, 35%. A \n\nmodifica\u00e7\u00e3o do n\u00famero de vizinhos para n\u00fameros maiores do que quinze n\u00e3o gera maiores taxas \n\nde acerto, mas a taxa de acerto encontrada j\u00e1 \u00e9 razo\u00e1vel. \n\nO segundo teste dessa bateria de resultados considera somente a segunda componente \n\nencontrada pelo m\u00e9todo FastICA. Nesse teste, com o NMV, obt\u00e9m-se uma taxa de acerto um \n\npouco maior que 23%, mas em compensa\u00e7\u00e3o, o VMA \u00e9 o teste com 41 vizinhos, onde este j\u00e1 \n\natinge seu auge e estabiliza-se, no mesmo molde da primeira tentativa desta bateria de testes.  \n\n\n\n \n\n \n\n  \n\n58 \n\n \n\nOutros testes envolvendo apenas uma componente, a exemplo dos testes anteriores, \n\napresentaram resultados similares a dos dois primeiros. \n\nO terceiro teste dessa bateria de resultados utilizou-se das duas primeiras \n\ncomponentes. Em geral a constru\u00e7\u00e3o da an\u00e1lise de dados via KNN, tem como car\u00e1ter comum \n\naumentar sua taxa de acerto, a partir do aumento de informa\u00e7\u00f5es espaciais sobre os dados, logo, \n\nos testes com mais componentes, como este, geram resultados com uma porcentagem de \n\nresultado maior. Entretanto resultado deste teste \u00e9 inferior aos dois primeiros, atingindo um \u00e1pice \n\nde sucesso em torno de 30% com quinze vizinhos, apesar de o seu NMV ter taxa superior ao dos \n\noutros dois testes, com 26% de acerto. Este teste foi marcante para a pesquisa, pois apresentou \n\ncerta inconsist\u00eancia entre seu resultado e o comportamento esperado pelo m\u00e9todo KNN. Essa \n\ninconsist\u00eancia \u00e9 notada quando s\u00e3o feitos testes com n\u00fameros de vizinhos superiores a quinze, \n\nque \u00e9 o VMA deste teste, com taxa de acerto de 30%. Por exemplo, o teste com vinte e um \n\nvizinhos, tem porcentagem de acerto de 28%. Essa pequena diminui\u00e7\u00e3o n\u00e3o gera uma \n\npreocupa\u00e7\u00e3o individual com esse teste, entretanto, uma hip\u00f3tese quanto \u00e0 inclus\u00e3o de m\u00faltiplas \n\ndimens\u00f5es na busca pela classifica\u00e7\u00e3o via KNN foi constru\u00edda para explicar esse comportamento \n\nan\u00f4malo do m\u00e9todo. Essa hip\u00f3tese \u00e9 esclarecida e debatida na parte de Conclus\u00f5es.   \n\nO quarto teste com 3 componentes apresentou uma melhora significativa na \n\nporcentagem de acerto . Seu NMV tem 35% de acerto, igualando-se ao VMA dos testes \n\nanteriores. Seu comportamento n\u00e3o \u00e9 t\u00e3o diferenciado quanto o teste feito com duas componentes \n\ne seu VMA \u00e9 atingido rapidamente com nove vizinhos, ao valor de 43% de acerto. \n\nAp\u00f3s analisar a inconsist\u00eancia no comportamento da classifica\u00e7\u00e3o com duas \n\ncomponentes e a est\u00e1vel classifica\u00e7\u00e3o com tr\u00eas componentes, o pr\u00f3ximo passo foi analisar se o \n\nemprego de todas as componentes teria algum impacto parecido com o visto no quarto teste dessa \n\nbateria. Sendo assim, o quinto teste \u00e9 feito com as cinco componentes independentes encontradas \n\npelo m\u00e9todo FastICA e o resultado \u00e9 mais an\u00f4malo do que o teste com tr\u00eas componentes, \n\nrefor\u00e7ando a hip\u00f3tese de que esse comportamento \u00e9 derivado da constru\u00e7\u00e3o espacial do problema. \n\nO NMV \u00e9 tamb\u00e9m o VMA com taxa de acerto superior a 65%. A taxa de acerto deste teste \u00e9 \n\ncaracterizada por um comportamento decrescente, isto \u00e9, quanto maior o n\u00famero de vizinhos, \n\nmenor a taxa de acerto. E essa taxa tem uma varia\u00e7\u00e3o substancial, pois com cinquenta e um \n\nvizinhos, sua taxa de acerto \u00e9 de 55%, sendo que come\u00e7ou com quase 66%. As principais raz\u00f5es \n\npara esse comportamento t\u00e3o dissonante, como dito acima, est\u00e3o na parte das conclus\u00f5es gerais, \n\n\n\n \n\n \n\n  \n\n59 \n\n \n\nentretanto, \u00e9 interessante verificar que mesmo com esse inesperado resultado, o m\u00e9todo atingiu \n\nsua maior taxa de acerto com um n\u00famero m\u00ednimo de vizinhos, ou seja, com uma computa\u00e7\u00e3o \n\nbarata, o teste tem uma taxa de acerto expressiva e consideravelmente superior ao \u00e1pice de acerto \n\ndos outros m\u00e9todos.    \n\n \n\n          (a)  Primeiro Teste                (b)  Segundo Teste \n\n\n\n \n\n \n\n  \n\n60 \n\n \n\n \n\n       (c)  Terceiro Teste                     (d)  Quarto Teste \n\n \n\n(e) Quinto Teste \n\n \n\nFigura 7.1 - Primeira Bateria de Resultados \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n61 \n\n \n\n7.2 Treino Par/\u00cdmpar / Classifica\u00e7\u00e3o de Reservat\u00f3rios \n \n\nApesar de uma alta porcentagem de acerto, a busca pela sua melhoria, apontou para a \n\nclassifica\u00e7\u00e3o via K-NN, uma vez que esta \u00e9 bem extensa e detalhada. Para essa bateria de testes, \n\nfoi escolhida uma classifica\u00e7\u00e3o mais simples, por\u00e9m n\u00e3o menos importante: a de cada amostra \n\ntestemunhada quanto a sua caracter\u00edstica como Reservat\u00f3rio, N\u00e3o-Reservat\u00f3rio, ou ainda, \n\nPoss\u00edvel Reservat\u00f3rio. Os outros par\u00e2metros continuam iguais ao do primeiro teste. \n\nNo primeiro teste desta bateria, com uma componente, j\u00e1 \u00e9 poss\u00edvel ver aumento na \n\nporcentagem de acerto, com NMV apresentando uma porcentagem de acerto maior que a 66%, \n\num n\u00famero superior a todos os testes da primeira bateria. O VMA foi com 47 vizinhos e chega a \n\nquase 72%.  \n\nO segundo teste dessa bateria envolve tr\u00eas componentes. Como esperado, o \n\ncomportamento do gr\u00e1fico difere-se do comum. O NMV aumenta a porcentagem de acerto para \n\npouco mais que 70%. O VMA ocorre no teste com nove vizinhos e tem aproximadamente 74% \n\nde acerto.  \n\nPela primeira vez observam-se duas partes bem distintas dentro desse \n\ncomportamento, do NMV at\u00e9 o VMA, o gr\u00e1fico \u00e9 apenas crescente, depois do VMA o gr\u00e1fico \n\ntorna-se decrescente. Tal efeito \u00e9 de certa forma, ben\u00e9fico \u00e0 interpreta\u00e7\u00e3o do gr\u00e1fico, pois \n\nrestringe poss\u00edveis testes posteriores, dentro dessa configura\u00e7\u00e3o de par\u00e2metros h\u00e1 um pequeno \n\nn\u00famero de vizinhos testados.  \n\nO terceiro teste desta bateria foi com quatro componentes, e seu NMV \u00e9 superior a \n\n78% e seu VMA \u00e9 de aproximadamente 79%, mas a varia\u00e7\u00e3o desse acerto para um n\u00famero maior \n\nde vizinhos \u00e9 menor que os outros testes, sendo que o teste com quarenta e cinco vizinhos tem \n\n76% de acerto. \n\n O quarto, e \u00faltimo teste, desta bateria s\u00e3o com as cinco componentes independentes \n\ndispon\u00edveis nos dados. Assim como no teste de cinco componentes da primeira bateria de testes, \n\no NMV coincide com o VMA e \u00e9 de quase 84% de acerto, entretanto seu comportamento \n\ndecrescente \u00e9 menos acentuado que o do teste com cinco componentes anterior, diminuindo a \n\ntaxa de acerto em 4%, resultando em 80% de acerto com o teste \u00e9 feito com cinquenta e um \n\nvizinhos. Assim como notado no primeiro teste, o comportamento quando o teste \u00e9 feito  com \n\n\n\n \n\n \n\n  \n\n62 \n\n \n\ncinco componentes \u00e9 o mais an\u00f4malo dentro da bateria de testes, entretanto, \u00e9 o que apresenta \n\nmaior acerto entre todos. \n\n  \n\n \n\n          (a)  Primeiro Teste             (b)  Segundo Teste \n\n \n\n           (c)  Terceiro Teste                 (d)  Quarto Teste \n\n \n\nFigura 7.2 - Segunda Bateria de Resultados \n\n \n\n\n\n \n\n \n\n  \n\n63 \n\n \n\n \n\n \n\n7.3 Treino Menos-um / Ambas Classifica\u00e7\u00f5es \n\n \n\nAp\u00f3s modificar o tipo de classifica\u00e7\u00e3o, e conseguir bons resultados quanto \u00e0 taxa de acerto, \n\noutra mudan\u00e7a quanto aos par\u00e2metros foi levada em considera\u00e7\u00e3o, o tipo de treino. Enquanto as \n\nduas primeiras baterias foram treinadas com metade das profundidades poss\u00edveis (\u00edmpares) e \n\ntentava-se classificar a outra metade (pares), foi constru\u00eddo um treino onde se treina com todas as \n\namostras menos uma delas e classifica-se essa amostra retirada. Por exemplo, exclu\u00eda-se a \n\nprimeira amostra do treino, treinava com todos os restantes, e classifica a amostra que foi \n\nretirada. Repetindo esse processo para cada amostra, tem-se a porcentagem de acerto desse \n\ntreino. \n\nEssa bateria de resultados foi dividia em duas partes, a primeira \u00e9 baseada na classifica\u00e7\u00e3o \n\nde todas as rochas (Figura 7.3), assim como a primeira bateria e a segunda parte \u00e9 baseada na \n\nclassifica\u00e7\u00e3o pelas caracter\u00edsticas quanto a reservat\u00f3rio, assim como a segunda bateria. \n\nNa primeira parte, o primeiro teste \u00e9 de uma componente independente e o segundo \u00e9 de \n\ntr\u00eas componentes. O primeiro n\u00e3o difere muito dos resultados da primeira bateria de testes e seu \n\ncomportamento \u00e9 similar, mas com um aumento na taxa de acerto. Seu NMV acerta pouco mais \n\nque 20%, enquanto seu VMA \u00e9 de 38% com cinq\u00fcenta e um vizinhos. O segundo apresenta uma \n\ncoer\u00eancia de classifica\u00e7\u00e3o muito grande, ficando em torno de 47% a maior parte do tempo. \n\nQuanto ao terceiro teste, com cinco componentes, seu NMV \u00e9 seu VMA e tem porcentagem de \n\nacerto de quase 69%, e o efeito an\u00f4malo \u00e9 amenizado, atingindo 60% de acerto quando o teste \u00e9 \n\nfeito com cinq\u00fcenta e um vizinhos. \n\n\n\n \n\n \n\n  \n\n64 \n\n \n\n \n\n        (a)  Primeiro Teste             (b)  Segundo Teste \n\n \n\n(c) Terceiro Teste \n\n \n\n Figura 7. 3 - Terceira Bateria de Resultados - Classifica\u00e7\u00e3o F\u00e1cies  \n\n \n\nNa segunda parte da terceira bateria de resultados (Figura 7.4), o primeiro teste feito com \n\numa componente, o segundo com tr\u00eas componentes e o terceiro com cinco componentes. O \n\nprimeiro teste tem um comportamento parecido com o do teste de uma componente da segunda \n\n\n\n \n\n \n\n  \n\n65 \n\n \n\nbateria, mas seu VMA sobe, atingindo 73% de acerto. Nos testes com tr\u00eas e cinco componentes, \n\no efeito an\u00f4malo \u00e9 amenizado, em rela\u00e7\u00e3o aos testes da segunda bateria e o VMA t\u00eam valores \n\naumentados. Para tr\u00eas componentes o VMA passa de 77% enquanto que para cinco componentes \n\nesse aumento chega a 85%. Essas informa\u00e7\u00f5es mostram um par\u00e2metro interessante, o efeito de \n\num treino na classifica\u00e7\u00e3o. Apesar dos valores superiores de acertos encontrados, o tempo \n\ncomputacional gasto \u00e9 muito superior ao tempo computacional gasto com o teste Par/\u00cdmpar.  \n\nA raz\u00e3o do tempo computacional do treino Menos-Um      e o tempo computacional do \n\ntreino Par/\u00cdmpar      \u00e9 aproximadamente \n    \n\n         \n  e devido a essa diferen\u00e7a, outros \n\ntestes nesse trabalho optaram pelo treino Par/\u00cdmpar, sem perda de aplicabilidade, pois um bom \n\ntreino \u00e9 suficiente e poupam tempo computacional do problema. \n\n\n\n \n\n \n\n  \n\n66 \n\n \n\n \n\n       (a)  Primeiro Teste                 (b)  Segundo Teste \n\n \n\n(c) Terceiro Teste \n\n \n\n \n\n                         Figura 7. 4 - Terceira Bateria de Resultados - Classifica\u00e7\u00e3o de Reservat\u00f3rios \n\n  \n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n67 \n\n \n\n \n\n7.4 Resultados Comparativos \n \n\nAp\u00f3s mudan\u00e7as em aspectos de treino e classifica\u00e7\u00e3o, mudan\u00e7as para com os diversos \n\npar\u00e2metros do FastICA fazem-se necess\u00e1rios. Para a quarta bateria de resultados, foi utilizado o \n\nmesmo treino e classifica\u00e7\u00e3o da Primeira Bateria de Testes e os testes foram feitos com cinco \n\npar\u00e2metros. Os par\u00e2metros modificados foram quanto \u00e0 fun\u00e7\u00e3o-objetivo inicial e quanto ao tipo \n\nde ortogonaliza\u00e7\u00e3o. As fun\u00e7\u00f5es-objetivo testadas foram as da Tabela 7.1, onde a1 e a2 s\u00e3o \n\nconstantes arbitr\u00e1rias. Quanto \u00e0 ortogonaliza\u00e7\u00e3o, ela foi testada na sua caracter\u00edstica \n\ndeflacion\u00e1ria, que procura uma componente de cada vez, e na sim\u00e9trica que procura todas as \n\ncomponentes de uma vez.  \n\nTabela 7. 1 \u2013 Fun\u00e7\u00f5es-Objetivo \n\nFun\u00e7\u00e3o Sigla F\u00f3rmula \n\nC\u00fabica Cubi g(u) = u\n3\n \n\nTangente \n\nHiperb\u00f3lica Tanh g(u)=tanh(a1.u) \n\nGaussiana Gauss g(u)=u.exp(-a2(u\n2\n/2))  \n\nQuadrada Skew g(u)=u\n2\n \n\n \n\nO teste foi rodado na sequ\u00eancia da Tabela 7.2 \n\n \n\nTabela7. 2 - Sequ\u00eancia Quarta Bateria de Resultados \n\nTeste Fun\u00e7\u00e3o  Ortogonaliza\u00e7\u00e3o \n\nPrimeiro Cubi Deflacion\u00e1ria \n\nSegundo Tanh Deflacion\u00e1ria \n\nTerceiro Gauss Deflacion\u00e1ria \n\nQuarto Skew Deflacion\u00e1ria \n\nQuinto Cubi Sim\u00e9trica \n\nSexto Tanh Sim\u00e9trica \n\nS\u00e9timo Gauss Sim\u00e9trica \n\nOitavo Skew Sim\u00e9trica \n\nTodos os testes apresentaram resultados id\u00eanticos, apenas com diferen\u00e7a no n\u00famero de \n\nitera\u00e7\u00f5es necess\u00e1rias para sua converg\u00eancia. Isso denota que a estrutura dos dados em litof\u00e1cies \n\nn\u00e3o \u00e9 afetada por mudan\u00e7as de par\u00e2metros de FastICA, devido a simplicidade espacial da sua \n\n\n\n \n\n \n\n  \n\n68 \n\n \n\nestrutura. Nas mudan\u00e7as efetuadas nos testes, o caso que necessitou de mais itera\u00e7\u00f5es foi o \n\nquarto. O quarto teste tem aproxima\u00e7\u00e3o quadr\u00e1tica de erro e por isso tem clara desvantagem \n\nperante os outros testes. Algumas tentativas de converg\u00eancia para o quarto testes n\u00e3o alcan\u00e7aram \n\nsucesso devido a esses problemas de converg\u00eancia. A resposta de todos est\u00e1 mostrada na Figura \n\n7.5 \n\n \n\n \n\n \n\n \n\nFigura7. 5 - Quarta Bateria de Resultados \n\n \n\n \n\n \n\n7.5 Comparativo ICA e FastICA \n\n \n\nDepois de testes com mudan\u00e7as de par\u00e2metros, esta quinta bateria de testes compara \n\nalgoritmo usado o FastICA 2.5, programado por Hyv\u00e4rinen, em 2001,com um t\u00edpico de ICA de \n\nProjection-Pursuit dispon\u00edvel em toolbox do MATLAB. \n\nEsse teste \u00e9 feito nos moldes da Segunda Bateria de Testes, com treino Par/\u00cdmpar e \n\nclassifica\u00e7\u00e3o de Litof\u00e1cies e pelo grupo da rocha quanto \u00e0 possibilidade desta ser reservat\u00f3rio. \n\n\n\n \n\n \n\n  \n\n69 \n\n \n\nPara poucas dimens\u00f5es, como o problema de perfis de po\u00e7o, ambas as classifica\u00e7\u00f5es de \n\nICA e de FastICA t\u00eam os mesmo resultados para uma aproxima\u00e7\u00e3o na casa de     . A Figura \n\n7.6, abaixo representa a classifica\u00e7\u00e3o id\u00eantica para todas as Litof\u00e1cies. \n\n \n\n \n\n \n  Figura 7. 6 -  Teste entre ICA e FastICA \u2013 Classifica\u00e7\u00e3o de F\u00e1cies \n\n \n\nA Figura 7.7 representa a classifica\u00e7\u00e3o id\u00eantica para a caracter\u00edstica de Reservat\u00f3rio \n\n \n\nFigura 7.7 - Teste entre ICA e FastICA \u2013 Classifica\u00e7\u00e3o de Reservat\u00f3rio \n\n\n\n \n\n \n\n  \n\n70 \n\n \n\n \n\nComo a resposta nos dois casos \u00e9 a mesma, passa-se para outro tipo de compara\u00e7\u00e3o entre \n\nos testes: o tempo que cada m\u00e9todo gasta para separar as componentes independentes. Foram \n\nconstru\u00eddas matrizes quadradas aleat\u00f3rias, com tamanho variando de duas a trinta dimens\u00f5es tal \n\nteste. Foi calculado o tempo computacional que cada programa utiliza para identificar as \n\ncomponentes independentes: \n\nO programa FastICA 2.5 \u00e9 extremamente mais r\u00e1pido que o programa convencional, \n\ncomo mostra a Figura 7.8 \u00e9 o resultado para esse teste, onde o eixo x denota as dimens\u00f5es das \n\nmatrizes e o eixo y o tempo gasto para cada uma dessas dimens\u00f5es.  \n\n \n\n \n\nFigura 7. 8 \u2013 Compara\u00e7\u00e3o entre velocidades de ICA e FastICA \n\n \n\n7.6 Comparativo ICA e PCA \n\n \n\nAs baterias de resultados anteriores mostram que a An\u00e1lise de Componentes \n\nIndependentes apresenta uma efic\u00e1cia satisfat\u00f3ria no reconhecimento e classifica\u00e7\u00e3o de f\u00e1cies \n\nlitol\u00f3gicas e da classifica\u00e7\u00e3o das rochas em grupos de reservat\u00f3rio, poss\u00edvel reservat\u00f3rio e n\u00e3o -\n\nreservat\u00f3rio. De fato, a metodologia assemelha-se muito \u00e0 classifica\u00e7\u00e3o atrav\u00e9s da An\u00e1lise de \n\nComponentes Principais, com vasta bibliografia referente ao tema (Sancevero, 2008), (Doveton, \n\n\n\n \n\n \n\n  \n\n71 \n\n \n\n1994), (Talaat, 1989). Dada essa bibliografia estruturada da PCA, uma an\u00e1lise interessante recai \n\nsobre a compara\u00e7\u00e3o das efici\u00eancias entre a PCA e a ICA.  \n\nEsta compara\u00e7\u00e3o ajuda a ilustrar a diferen\u00e7a final nas porcentagens de acerto, \n\nenquanto que uma compara\u00e7\u00e3o mais detalhada entre os m\u00e9todos j\u00e1 foi feita por (Wong, 2002), \n\nmostrando vantagem do m\u00e9todo ICA sobre o m\u00e9todo PCA. Para esta compara\u00e7\u00e3o, apenas \n\nilustrativa, primeiro escolheu-se o teste de K-NN Par/\u00cdmpar, a classifica\u00e7\u00e3o de todas as Litof\u00e1cies \n\npresentes no testemunho, e a utiliza\u00e7\u00e3o cinco componentes da ICA. Os resultados encontram-se \n\nna Figura 7.9 abaixo \n\nFigura 7. 9 \u2013 Compara\u00e7\u00e3o ICA e PCA \u2013 Classifica\u00e7\u00e3o F\u00e1cies \n\n \n\n A pr\u00f3xima etapa desta bateria de resultados consiste na utiliza\u00e7\u00e3o de K-NN com treino \n\nPar/\u00cdmpar, a classifica\u00e7\u00e3o de grupos de reservat\u00f3rio, e a utiliza\u00e7\u00e3o cinco componentes da ICA. Os \n\nresultados encontram-se na Figura 7.10  \n\n\n\n \n\n \n\n  \n\n72 \n\n \n\n \nFigura 7. 10 \u2013 Compara\u00e7\u00e3o ICA e PCA \u2013 Classifica\u00e7\u00e3o de Reservat\u00f3rios \n\n \n\n Em qualquer situa\u00e7\u00e3o a An\u00e1lise de Componentes Independentes mostrou-se mais eficiente \n\nque a An\u00e1lise de Componentes Principais. \n\n \n\n \n\n7.7 Resultados da predi\u00e7\u00e3o de um po\u00e7o \n\n \n\nO m\u00e9todo aplicado FastICA \u00e9 eficaz e mais eficiente que a PCA. Al\u00e9m disso, o m\u00e9todo tem \n\na mesma efici\u00eancia que a aplica\u00e7\u00e3o do m\u00e9todo ICA, com a vantagem de ter a velocidade \n\ncomputacional bem maior. Uma prova ao m\u00e9todo \u00e9 a tentativa de predi\u00e7\u00e3o de um po\u00e7o inteiro, \n\nutilizando-se apenas os testemunhos conhecidos de outros po\u00e7os, ou seja, classificar todas as \n\namostras dispon\u00edveis de um determinado po\u00e7o, sem usar nenhuma amostra com testemunho \n\nconhecido dele.  \n\nPara tal teste foi escolhida a classifica\u00e7\u00e3o do Po\u00e7o NA01 do Campo de Namorado. Este \n\npo\u00e7o tem 799 amostras de dados de perfil, as primeiras amostras do dado utilizado nos outros \n\ntestes. E como conjunto de treino utilizou-se todas as outras amostras que dispunham de \n\ntestemunho e n\u00e3o se encontravam no po\u00e7o NA01. As amostras foram classificadas em \n\nreservat\u00f3rio, poss\u00edvel reservat\u00f3rio e n\u00e3o-reservat\u00f3rio e a Figura 7.11 mostra a compara\u00e7\u00e3o entra a \n\npredi\u00e7\u00e3o e o testemunho conhecido. \n\nOutra perspectiva interessante deste teste \u00e9 identificar com quanto de porcentagem o \n\nm\u00e9todo foi capaz de classificar corretamente cada uma das poss\u00edveis classes, ou seja, identificar, \n\n\n\n \n\n \n\n  \n\n73 \n\n \n\nseparadamente, as taxas de acerto do m\u00e9todo. A Tabela 7.3 cont\u00e9m essas informa\u00e7\u00f5es baseadas \n\nno presente teste do Po\u00e7o NA01.  \n\n \n\nTabela7. 3 \u2013 Predi\u00e7\u00e3o individual da parte testemunhada \n\n      Predi\u00e7\u00e3o     \n\n   \n  \n\n   \n  T\n\ne\nst\n\ne\nm\n\nu\nn\n\nh\no\n\n   Reservat\u00f3rio \nPoss\u00edvel \nReservat\u00f3rio \n\nN\u00e3o \nReservat\u00f3rio   \n\nReservat\u00f3rio 97% 3% 0%   \n\nPoss\u00edvel \nReservat\u00f3rio \n\n8% 80% 12% \n  \n\nN\u00e3o Reservat\u00f3rio 21% 0% 79%   \n\n          \n \n\nDe acordo com os dados presentes na Tabela 7.3, quando a amostra em teste era \n\nreservat\u00f3rio, a predi\u00e7\u00e3o acertou 97% dos casos. Em 3% dos casos, a predi\u00e7\u00e3o apontou uma \n\namostra de poss\u00edvel reservat\u00f3rio, mas nunca classificou estas amostras como n\u00e3o-reservat\u00f3rio. \n\nEssas porcentagens demonstram a consist\u00eancia da classifica\u00e7\u00e3o quando a amostra trata-se de uma \n\namostra de reservat\u00f3rio, pois apesar de ocorrerem classifica\u00e7\u00f5es de poss\u00edveis reservat\u00f3rios, esta \u00e9 \n\numa escolha pr\u00f3xima da real condi\u00e7\u00e3o da amostra, em contrapartida, a classifica\u00e7\u00e3o de n\u00e3o \n\nreservat\u00f3rio, n\u00e3o apresenta a mesma qualidade de predi\u00e7\u00e3o. \n\nAs outras porcentagens mant\u00eam-se na m\u00e9dia esperada pela classifica\u00e7\u00e3o e s\u00e3o justificadas \n\npelos desvios e aproxima\u00e7\u00f5es da ICA. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n74 \n\n \n\n \n\nProfundidade (m)                      Testemunho                  Predi\u00e7\u00e3o via ICA \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n                Figura 7.11- Predi\u00e7\u00e3o de Po\u00e7o NA01 \n\n \n\n7.8 Testes Dados S\u00edsmicos \n\nOs testes t\u00eam o mesmo padr\u00e3o utilizado quando foram trabalhados os dados de po\u00e7o, \n\ninclusive com as mesmas plataformas. Os testes foram feitos usando apenas os dados de S\u00edsmica \n\n2.988,\n\n4 \n\n 3.148 \n\n\n\n \n\n \n\n  \n\n75 \n\n \n\n(DDI, DSI, Fase, Freq,Deri, Ampli), a An\u00e1lise de Componentes Independentes padr\u00e3o do, os \n\ntreinos foram o Par-\u00cdmpar ( que treina com as amostras \u00edmpares e classifica as pares) e o Menos-\n\nUm(que retira uma amostra dos dados, treina com todas as outras e tenta classificar a amostra \n\nretirada) e a classifica\u00e7\u00e3o \u00e9 Indefinida, N\u00e3o-Reservat\u00f3rio, poss\u00edvel reservat\u00f3rio e Reservat\u00f3rio, \n\ndentro do m\u00e9todo KNN.  \n\n \n\n 7.8.1 Teste Par- \u00edmpar \n\nForam feitos testes, utilizando uma componente (primeira e a segunda), duas, quatro e as \n\nseis componentes dispon\u00edveis. A porcentagem de acerto \u00e9 perfeita em quatro desses cinco treinos, \n\ne uma delas tem acerto de 99%. \n\n \n\nFigura 7.12 - Teste Par-\u00cdmpar \u2013 Classifica\u00e7\u00e3o de F\u00e1cies \n\n \n\n \n\n7.8.2 Teste Menos-Um \n\n \n\nForam feitos testes, utilizando uma componente, tr\u00eas e seis componentes dispon\u00edveis. A \n\nporcentagem de acerto do teste com apenas uma componente tem uma taxa de acerto para tr\u00eas \n\nvizinhos de 98,98% enquanto que para todos os outros \u00e9 de 99,01%. No segundo teste, com um \n\ncomportamento mais esperado temos o n\u00famero m\u00ednimo de vizinhos (NMV) com um acerto em \n\n\n\n \n\n \n\n  \n\n76 \n\n \n\ntorno de 98,96% e depois sobre para 99% de acerto, onde ocorre o valor de m\u00e1ximo acerto \n\n(VMA). J\u00e1 o teste com todas as componentes tem para o NMV, acerto de 98,90%, o VMA \u00e9 de \n\n99,05% e ocorre para quatro vizinhos, depois se estabiliza em pouco mais de 90% para os outros \n\nn\u00fameros de vizinhos. \n\n \n\n \n\nFigura 7.13 - Teste Menos-Um \u2013 Classifica\u00e7\u00e3o de Reservat\u00f3rios \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n  \n\n \n\n \n\n\n\n \n\n \n\n  \n\n77 \n\n \n\n \n\n \n\n \n\nCap\u00edtulo 8 \n\n \n\nConclus\u00f5es \n\n \n\n8.1 Conclus\u00f5es Dados de Po\u00e7o  \n\n \n\nQualquer an\u00e1lise mais aprofundada requer toda a aten\u00e7\u00e3o para diversos aspectos dial\u00e9ticos de \n\nobjetos de estudo que tem sua validade testada. E para tanto, situa\u00e7\u00f5es distintas levam a \n\ndiferentes maneiras de se observar um determinado resultado. Ao desconsiderarem-se os \n\nelementos parciais de uma observa\u00e7\u00e3o de qualquer resultado, o que sobra \u00e9 uma an\u00e1lise vazia e \n\nestagnada, meramente ilustrativa de uma situa\u00e7\u00e3o controlada. Objetivos devem ser alcan\u00e7ados e a \n\nmetodologia e os testes feitos est\u00e3o intimamente ligados \u00e0s escolhas unilaterais de como, quando \n\ne onde,tais testes s\u00e3o propostos e finalmente executados. \n\nOs testes rodados, apesar de n\u00e3o abrangerem todo e qualquer teste poss\u00edvel da An\u00e1lise de \n\nComponentes Independentes, conjecturam um padr\u00e3o de seu funcionamento claro e cria um \n\ncorpo s\u00f3lido de informa\u00e7\u00e3o, essa a qual \u00e9 a base para toda a discuss\u00e3o sobre o m\u00e9todo. N\u00e3o que o \n\nalgoritmo ICA precise de respaldo ou prova, as in\u00fameras aplica\u00e7\u00f5es, em \u00e1reas distintas do \n\nmesmo j\u00e1 prov\u00e9m o m\u00e9rito do seu uso.  Todo esse suporte e a bibliografia de crescimento \n\nexponencial tornam a amostra dos testes mais quantitativa que qualitativa, focando os testes para \n\num caminho mais experimental, juntamente com a an\u00e1lise de resultados. Essa investida \n\nexperimental pode, e deve ser prolongada, visto que apesar da efervesc\u00eancia na produ\u00e7\u00e3o de \n\nartigos sobre o ICA, ainda \u00e9 escasso o n\u00famero de papers na \u00e1rea de interesse desta pesquisa. \n\nO n\u00famero de dimens\u00f5es de uma amostra est\u00e1 diretamente ligado ao n\u00famero de componentes \n\nindependentes que podem ser encontradas e os resultados tamb\u00e9m conferem um padr\u00e3o \n\ninteressante sobre o uso das componentes dentro do treino do KNN. Um comportamento, \n\nesperado no caso de qualquer an\u00e1lise via KNN, seria um gr\u00e1fico crescente, que aos poucos tende \n\na estabilizar-se em um n\u00famero n\u00e3o muito grande de vizinhos. Esse comportamento \u00e9 verificado \n\nao utilizar-se somente uma, duas, ou at\u00e9 tr\u00eas componentes. Nos testes em que foram usadas \n\n\n\n \n\n \n\n  \n\n78 \n\n \n\nquatro e cinco componentes, nota-se um comportamento an\u00f4malo, onde o gr\u00e1fico tende a \n\ndecrescer conforme aumenta o n\u00famero de vizinhos. Esse tipo comportamento provavelmente \u00e9 \n\nderiva do de uma reposi\u00e7\u00e3o espacial dos pontos quando a dimens\u00e3o cresce. Ou seja, pontos que \n\nestavam agrupados em uma por\u00e7\u00e3o do espa\u00e7o, podem ser separados quando \u00e9 colocada uma nova \n\ndimens\u00e3o. Dessa forma, quanto menos vizinhos s\u00e3o considerados, melhor \u00e9 a taxa de acerto, j\u00e1 \n\nque ao escolher-se um n\u00famero grande de vizinhos, o m\u00e9todo pode acabar escolhendo vizinhos \n\nque n\u00e3o fazem parte da mesma classifica\u00e7\u00e3o, pois os pontos com mesma classifica\u00e7\u00e3o est \u00e3o \n\nespalhados no espa\u00e7o. De qualquer forma, as taxas de sucesso aumentam conforme se usam mais \n\ncomponentes, pois com um treino mais elaborado, as chances de acerto sobem, mesmo com esse \n\ninconveniente desvio de padr\u00e3o. A natureza dessa anomalia deve-se tamb\u00e9m ao fato de que as \n\ncomponentes independentes encontradas aproximam-se consideravelmente da Distribui\u00e7\u00e3o \n\nNormal, diferentemente do ideal que seria uma distribui\u00e7\u00e3o menos gaussiana poss\u00edvel.  \n\nPor diversas vezes os testes foram refeitos, e alguns resultados s\u00e3o m\u00e9dias dos resultados \n\nobtidos em todos os testes, pois como o m\u00e9todo \u00e9 iniciado a partir de um ponto aleat\u00f3rio, e seus \n\npassos seguintes dependem deste, os resultados podem ser diferentes, apresentando pequenos \n\ndesvios-padr\u00e3o, mas sempre correspondem a um mesmo padr\u00e3o. \n\nSe fosse o caso de apenas escolher um r\u00f3tulo de qualidade para a An\u00e1lise de Componentes \n\nindependentes via FastICA na classifica\u00e7\u00e3o de F\u00e1cies, de acordo com os resultados dos testes, o \n\ntermo seria eficiente. A efic\u00e1cia do m\u00e9todo \u00e9 \u00f3bvia, pois seus resultados chegam a taxas de \n\nsucesso superiores a 80%, em uma \u00e1rea de estudo que cont\u00e9m grandes erros e desvios, sem \n\nmencionar as incertezas geof\u00edsicas. Mas, mais do que isso, o teste \u00e9 extremamente eficiente, \n\npodendo resolver grandes blocos de informa\u00e7\u00e3o em poucos segundos. Sua efici\u00eancia permite que \n\nsejam rodados v\u00e1rios testes em cima das amostras, sem um comprometimento de prazo, j\u00e1 que o \n\ncusto computacional \u00e9 baixo.  \n\nComo j\u00e1 explanado, uma abrangente s\u00e9rie de testes, leva a m\u00faltiplos resultados, os quais \n\nindicam o melhor caminho a se tratar uma determinada situa\u00e7\u00e3o-padr\u00e3o. No caso dos dados de \n\npo\u00e7o, nota-se que mudan\u00e7as na fun\u00e7\u00e3o-objetivo ou de ortogonaliza\u00e7\u00e3o n\u00e3o afetam o resultado do \n\nteste, e como a natureza f\u00edsica desse tipo de dado \u00e9 sempre a mesma, o m\u00e9todo, ao ser empregado \n\nfuturamente, n\u00e3o precisa desse tipo de aporte. Por\u00e9m outros tipos de dados geram amostras \n\ndiferentes, onde a influ\u00eancia dessas mudan\u00e7as pode promover mudan\u00e7as estruturais nas \n\nComponentes Independentes e apresentar um resultado mais interessante. \n\n\n\n \n\n \n\n  \n\n79 \n\n \n\n \n\n8.2 Conclus\u00f5es Dados S\u00edsmicos \n\n \n\nOs dados s\u00edsmicos mostram-se muito eficientes para a classifica\u00e7\u00e3o das rochas quanto \n\nsuas caracter\u00edsticas de Reservat\u00f3rio. Mais eficientes que os dados de po\u00e7o, e isso prov\u00eam da \n\nestrutura dos dados. Os Dados s\u00edsmicos s\u00e3o gerados a partir de c\u00e1lculos n\u00e3o-lineares, ou seja, tem \n\nmenor correla\u00e7\u00e3o entre si, comparados aos dados de perfil. Essa menor correla\u00e7\u00e3o colabora para \n\ndistanciar os padr\u00f5es entre as amostras, deixando os padr\u00f5es mais robustos e diferenciados, \n\npermitindo ao m\u00e9todo da An\u00e1lise de Componentes Independentes itera\u00e7\u00f5es mais coerentes, \n\nchegando a um resultado mais correto para o M\u00e9todo K-NN gerar a classifica\u00e7\u00e3o dos dados. \n\nCom essa propriedade de acerto elevada, uma tentativa interessante de testes, \u00e9 dificultar o \n\ntipo de classifica\u00e7\u00e3o, como por exemplo, a classifica\u00e7\u00e3o completa de f\u00e1cies ou de caracter\u00edsticas \n\nmais dif\u00edceis de serem calculadas, como Porosidade. \n\nO comportamento dos dados, como levantado na An\u00e1lise de Componentes Independentes \n\nem cima de dados de po\u00e7o, n\u00e3o \u00e9 largamente debatido aqui, pois a taxa de acerto entre os \n\nvizinhos tem uma diferen\u00e7a m\u00ednima, mostrando que os dados est\u00e3o postados de forma que para \n\npoucos vizinhos, j\u00e1 se atinge uma porcentagem de acerto satisfat\u00f3ria. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n81 \n\n \n\nBibliografia \n \n\n \n\n \n\nANDERSON, J.A. An Introduction To Neural Networks- Bradford Book, 1995 \n\n \n\nASCENSO, J. Reconhecimento de Padr\u00f5es. Notas de Aula, 2003 \n\n  \n\nBALLANDA, K. P.; MACGILLIVRAY, H. L. Kurtosis: A Critical Review. The American \n\nStatistician (American Statistical Association) 42 (2): 111\u2013119, 1988 \n\n \n\nBARZOZA, E.G. An\u00e1lise Estratigr\u00e1fica do Campo de Namorado com base na interpreta\u00e7\u00e3o \n\nS\u00edsmica Tridimensional, Tese de Doutorado \u2013 UFRGS, 2005 \n\n \n\nBELL A.J.; SEJNOWSKI T.J. An information maximization approach to blind separation and \n\nblind deconvolution, Neural Computation, 7, 6, 1129-1159, 1995 \n\n \n\nBISHOP, C. M. Neural Networks for Pattern Recognition. Oxford University Press, 1995  \n\n \n\nBLACK, PAUL E. Manhattan distance,  ed., U.S. National Institute of Standards and \n\nTechnology.,2006 \n\n \n\nBOX, G.E.P.; HUNTER, J.S.; HUNTER, W.G. Statistics for Experimenters: An Introduction to \n\nDesign, Data Analysis, and Model Building. 1978 \n\n \n\nCARDOSO, J.-F. and Souloumiac, A., Blind beamforming for non Gaussian signals. IEE Proc. F. \n\nv140 i6. 362-370, 2002 \n\n \n\nCASEY.M.A. Method for extracting features from a mixture of signals, United States, Mitsubish \n\nElectric Research Laboratories, INC (Cambridge, MA), 2001  \n\n \n\nCHEN, X.; WANG, L.; XU,Y. A Symmetric Orthogonal FastICA Algorithm and Applications in \n\nEEG, vol. 2, pp.504-508, 2009 Fifth International Conference on Natural Computation, 2009 \n\n \n\nCOMON.P. Independent Component Analysis: a new concept?, Signal Processing, Elsevier, \n\n36(3):287\u2014314,1994 \n\n \n\nCOVER, T.M.; HART P.E. Nearest neighbor pattern classification. IEEE Transactions on \n\nInformation Theory 13 (1): 21\u201327, 1967 \n\n \n\nCOVER, T.M.; THOMAS, J.A. Elements of Information Theory. Wiley, 1991. \n\n \n\nDANTAS, C. A. B. Probabilidade: Um curso introdut\u00f3rio \u2013 2. ed.1 \u2013 S\u00e3o Paulo: Editora da \n\nUniversidade de S\u00e3o Paulo, 2004 \n\n \n\n\n\n \n\n \n\n  \n\n82 \n\n \n\nDE MAESSCHALCK, R.; JOUAN-RIMBAUD, D.; MASSART.D.L. The Mahalanobis distance. \n\nChemometrics and Intelligent Laboratory Systems 50:1\u201318. 2008 \n\n \n\nDIAMANTARAS, K.I.; KUNG, S.Y. Principal Component Neural Networks: Theory and \n\nApplications. Willey, 1996 \n\n \n\nDUBRULE, O.; THIBAU, M.; LAMY, P.; HAAS,A.  Geostatistical reservoir characterization \n\nconstrained by 3D seismic data,1997 \n\n \n\nFAREBROTHER, R. W. Algorithm AS 79: Gram-Schmidt Regression. Journal of the Royal \n\nStatistical Society. Series C (Applied Statistics) Vol. 23, No. 3 pp. 470-476, 1974 \n\n \n\nFRANCIS, A. Limitations of Deterministic and Advantages of Stochastic Seismic Inversion, \n\n2005 \n\n \n\nFULLER, R.B. ET AL. Synergetics: Explorations in the Geometry of Thinking, published by \n\nMacmillan , Vol. 1, 1975 \n\n \n\nGAUCH, H. G. , JR. Multivariate Analysis in Community Structure. Cambridge University \n\nPress, Cambridge, 1992 \n\n \n\nGIL, A.C. Como elaborar projetos de pesquisa. S\u00e3o Paulo: Atlas, 1991. \n\n \n\nGOKHALE, D.V.; AHMED, N.A.; RES, B.C.; PISCATAWAY, N.D. Entropy Expressions and \n\nTheir Estimators for Multivariate Distributions. Information Theory, IEEE Transactions on 35 \n\n(3): 688\u2013692, 1989 \n\nGOOD, I. J. Some statistical applications of Poisson's work. Statistical Science 1 (2): 157\u2013180, \n\n1986 \n\n \n\nHILL, T.; LEWICKI, P.  Statistics Methods and Applications. StatSoft, Tulsa, OK, 2007 \n\n \n\nHOWSON, C.; URBACH, P. Scientific Reasoning: the Bayesian Approach (3rd ed.). Open Court \n\nPublishing Company,2005 \n\n \n\nHYV\u00c4RINEN, A.; OJA, E. A fast fixed-point algorithm for independent component analysis. \n\nNeural Computation, 9(7):1483-1492, 1997 \n\n \n\n HYV\u00c4RINEN, A. Fast and Robust Fixed-Point Algorithms for Independent Component \n\nAnalysis. IEEE Transactions on Neural Networks, 1999 \n\n \n\nHYV\u00c4RINEN, A. New approximations of differential entropy for independent component \n\nanalysis and projection pursuit. In Advances in Neural Information Processing Systems, volume \n\n10, pages 273-279. MIT Press, 1998. \n\n \n\nHYV\u00c4RINEN, A.; KARHUNEN, J.; OJA,E. Independent Component Analysis, New York: \n\nWiley, 2001 \n\n\n\n \n\n \n\n  \n\n83 \n\n \n\n \n\nJOANES, D.N.; GILL, C.A. Comparing measures of sample skewness and kurtosis. Journal of \n\nthe Royal Statistical Society (Series D): The Statistician 47 (1), 1998 \n\n \n\nJOLLIFFE, I.T. Principal Component Analysis, Series: Springer Series in Statistics, 2nd ed., \n\nSpringer, NY, 2002, XXIX, 487 p. 28 illus.2002 \n\n \n\nJUTTEN, C.; HERAULT, J. Blind separation of sources, part I: An adaptive algorithm based on \n\nneuromimetic architecture. Signal Processing, 24:1-10, 1991. \n\n \n\nKOHONEN, T. Self-Organizing Maps, Springer Series in Information Science no. 30. Springer, \n\nBerlin Heidelberg, \n\n \n\nKOUTROUMBAS, K.; THEODORIDIS, S. Pattern Recognition (4th ed.). Boston: Academic \n\nPress, 2008 \n\n \n\nKULIKOWSKI, C. A.; WEISS, S. M. Computer Systems That Learn: Classification and \n\nPrediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems, 1991 \n\n \n\nKUSHNER, H.J.; CLARK. D.S. Stochastic approximation methods for constrained and \n\nunconstrained systems. Springer \u2013 Verlag, 1978. \n\n \n\nLE CAM, L. Maximum likelihood \u2014 an introduction. ISI Review 58 (2): 153\u2013171, 1990 \n\n \n\nLEITE, L. An\u00e1lise de Componentes Independentes Aplicada \u00e0 Identifica\u00e7\u00e3o de Regi\u00f5es \n\nLesionadas Em Mamogramas. COPPE/UFRJ,M.Sc.,Engenharia El\u00e9trica, 2005 \n\n \n\nLI, Z.; AN, J.; SUN, L.; YANG, M. A Blind Source Separation Algorithm Based on Whitening \n\nand Non-linear Decorrelation.  vol. 1, pp.443-447, 2010 Second International Conference on \n\nComputer Modeling and Simulation, 2010 \n\n \n\nLIMA, E. L. Espa\u00e7os M\u00e9tricos. Projeto Euclides, Rio de Janeiro, 1993. \n\n \n\nLINSKER, R. Self-organization in a perceptual network. Computer 21: 105-117, 1988 \n\n \n\nMARCHINI,J.L.; HEATON, C.; RIPLEY B. D. FastICA Algorithms to perform ICA and \n\nProjection Pursuit. R package version 1.1-11, 2009 \n\nMARSAGLIA, G. Evaluating the normal distribution. Journal of Statistical Software 11 (4), 2004  \n\n \n\nMICHIE, D.; SPIEGELHALTER, D.J.; TAYLOR, C.C. ML, neural and statistical classification, \n\nNew York. cap.1,2,11, 1994 \n\n \n\nMURATA, M.; IKEDA, S.; ZIEHE A. An approach to blind source separation based on temporal \n\nstructure of speech signals, in IEEE Trans. Signal Processing, 2001.. \n\n \n\nNANDI, A.  Blind Estimation Using Higher-Order Statistics. Kluwer, 1999 \n\n\n\n \n\n \n\n  \n\n84 \n\n \n\n \n\nOJA, E. Subspace Methods of Pattern Recognition. Research Studies Press, England, and Wiley, \n\nUSA, 1983 \n\n \n\nOJA, E.; KARHUNEN, J. Stochastic approximation of the eigenvectors and eigenvalues of the \n\nexpectation of a random matrix. Journal of Math. Analysis and Applications, 106:69-84, 1985 \n\n \n\nOJA , E.; OGAWA, H.; WANGVIWATTANA, J. Learning in nonlinear constrained Hebbian \n\nnetworks. In Proc. Int. Conf. on Artificial Neural Networks (ICANN\u201891), p\u00e1ginas 385-390, \n\nEspoo, Finl\u00e2ndia, 1991 \n\n \n\nPAPOULIS, A. Probability Random Variables, and Stochastic processes. McGraw \u2013 Hill, 3\nrd\n\n \n\nEdition, 1991 \n\n \n\nPARZEN, E. On estimation of a probability and mode. Ann. Math. States, 33:1065-1076, 1962 \n\n \n\nROSA, H., ET AL. Caracteriza\u00e7\u00e3o de eletrof\u00e1cies por meio de ferramentas estat\u00edsticas \n\nmultivariadas. Rem: Rev. Esc. Minas,  Ouro Preto,  v. 61,  n. 4, Dec.  2008 . \n\n \n\nRUSSEL, B. H. The application of multivariate statistics and neural networks to the prediction of \n\nreservoir parameters using seismic attributes, Tese de Doutorado defendida na Faculdade de \n\nCalgary, Alberta, 2004. \n\n \n\nSACCO, T. SUSLICK, S.B. VIDAL, A.C. Modelagem Geol\u00f3gica 3D do Campo de Namorado \n\nUtilizando Dados de Perfilagem de Po\u00e7os Verticais, 2007 \n\nSACEVERO, S.S, REMACRE A.Z , VIDAL, A.C , PORTUGAL R.S. Aplica\u00e7\u00e3o de t\u00e9cnicas de \n\nestat\u00edstica multivariada na defini\u00e7\u00e3o da litologia a partir de perfis geof\u00edsicos de po\u00e7os, RBGf \n\n38(1) 61-74,2008, 2008 \n\nSAN MARTIN, L. \u00c1lgebras de Lie, Editora UNICAMP, 1999 \n\n \n\nSANCEVERO, S., REMACRE, A., VIDAL, A., PORTUGAL, R.. Aplica\u00e7\u00e3o de t\u00e9cnicas de \n\nestat\u00edstica multivariada na defini\u00e7\u00e3o da litologia a partir de perfis geof\u00edsicos de po\u00e7os. Revista \n\nBrasileira de Geoci\u00eancias, Am\u00e9rica do Norte, 38, dez. 2008 \n\n \n\nSCHUERMAN, J. Pattern Classification: A Unified View of Statistical and Neural Approaches.  \n\nWiley&amp;Sons, 1996 \n\n \n\nSTEWART, J. Calculus .Pioneira Thomson Learning, 2\u00aa ed, 2005 \n\n \n\nSTONE, J. V. A Brief Introduction to Independent Component Analysis in Encyclopedia of \n\nStatistics in Behavioral Science, Volume 2, pp. 907\u2013912, Ed. Brian S. Everitt &amp; David C. \n\nHowell, John Wiley &amp; Sons, Ltd, Chichester, 2005 \n\n \n\n\n\n \n\n \n\n  \n\n85 \n\n \n\nTOUSSAINT, G.T. Geometric proximity graphs for improving nearest neighbor methods in \n\ninstance-based learning and data mining. International Journal of Computational Geometry and \n\nApplications 15 (2): 101\u2013150,  2005 \n\n \n\nVIDAL, A.C, SANCEVERO, S.S. REMACRE A.Z, COSTANZO, C.P., Modelagem \n\nGeoestat\u00edstica 3d Da Imped\u00e2ncia Ac\u00fastica Para a Caracteriza\u00e7\u00e3o Do Campo De Namorado, \n\nRBGf  25(3): 295-305, 2007 \n\n \n\nWASSERMAN, P. Neural Computing Theory and Practice. Van Nostrand Rheinhold, New York, \n\n1989. \n\n \n\nWEIBULL, W. A statistical distribution function of wide applicability, J. Appl. Mech.-Trans. \n\nASME 18 (3): 293\u2013297, 1951 \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n87 \n\n \n\n \n\n \n\n \n\nAp\u00eandice A \u2013 Termos Estat\u00edsticos \n\n \n\n- Centralizar um vetor em torno de Zero \n\nCentralizar um dado em torno de zero, consiste subtrair do vetor x sua esperan\u00e7a estat\u00edstica: \n\n          \n\nA esperan\u00e7a de uma determinada fun\u00e7\u00e3o, distribui\u00e7\u00e3o ou amostra      \u00e9 dada por \n\n               \n \n\n  \n\n \n\nEm alguns casos, \u00e9 comum dividir o vetor aleat\u00f3rio x centralizado pelo seu desvio padr\u00e3o \n\n , branqueando os seus dados. \n\nO desvio padr\u00e3o   \u00e9 dado por         x), e  \n\n                    \n\n Um dado branqueado \u00e9 quando a sua matriz de covari\u00e2ncia \u00e9 igual \u00e0 identidade. \n\n \n\n- Depend\u00eancia \n\nSeja T um subconjunto de um espa\u00e7o S, e            s\u00e3o os elementos de T. Diz-se que T \n\n\u00e9 linearmente dependente se existem escalares             , n\u00e3o todos nulos, tais que \n\n                      \n\n \n\n- Independ\u00eancia \n\nA defini\u00e7\u00e3o de uma base de vetores independentes \u00e9 que n\u00e3o existe combina\u00e7\u00e3o poss\u00edvel \n\nentre quaisquer vetores que sejam iguais a um vetor dessa base. Estatisticamente, a independ\u00eancia \n\n\u00e9 descrita quando a ocorr\u00eancia de um evento n\u00e3o interfere na ocorr\u00eancia de outro evento, e estes \n\ns\u00e3o chamados de independentes entre si. Algebricamente, para que dois eventos, X1 e Y1, sejam \n\nn\u00e3o-correlacionados, basta que \n\n              \n\nSe dois eventos    e    s\u00e3o independentes ent\u00e3o a densidade de probabilidade conjunta \u00e9 \n\nigual o produto das densidades marginais \n\n\n\n \n\n \n\n  \n\n88 \n\n \n\n \n\n                         \n\n \n\n- Curtose \n\nA curtose \u00e9 uma medida de dispers\u00e3o que se caracteriza por ser um cumulante de quarta \n\nordem de uma vari\u00e1vel aleat\u00f3ria (Ballanda, 1988). curtose \u00e9 a tradu\u00e7\u00e3o do ingl\u00eas ?Kurtosis\u2018, e por \n\nisso \u00e9 denotada de kurt ( ). Seu modelo cl\u00e1ssico tem rela\u00e7\u00e3o com o quarto momento padronizado \n\nde uma distribui\u00e7\u00e3o (   \n    (Joanes, 1998), entretanto, para os c\u00e1lculos envolvidos na an\u00e1lise de \n\ncomponentes independentes, \u00e9 comum utilizar-se da nota\u00e7\u00e3o com esperan\u00e7a estat\u00edstica, isto \u00e9, \n\n                          \n\nonde      \u00e9 a esperan\u00e7a. Como   \u00e9 assumida como normalizada, sua vari\u00e2ncia \u00e9 igual a um logo \n\n        e a fun\u00e7\u00e3o da curtose pode ser simplificada para \n\n                  \n\nPara vari\u00e1veis gaussianas a curtose \u00e9 zero, enquanto para a maioria das distribui\u00e7\u00f5es n\u00e3o -\n\ngaussianas ela \u00e9 n\u00e3o nula (Joanes, 1998), sugerindo que pode servir como medida de n\u00e3o-\n\ngaussianidade de uma vari\u00e1vel aleat\u00f3ria. \n\nA curtose ainda atende a propriedades de linearidade, isto \u00e9, dadas    e    vari\u00e1veis \n\naleat\u00f3rias independentes, s\u00e3o v\u00e1lidas as seguintes rela\u00e7\u00f5es \n\n                                \n\n           \n          \n\nonde   \u00e9 uma constante. \n\n \n\n \n\n \n\n- Negentropia \n\n \n\n\n\n \n\n \n\n  \n\n89 \n\n \n\nA negentropia, tamb\u00e9m conhecida como entropia negativa ou sintropia, \u00e9 baseada no \n\ndiferencial de entropia sobre uma quantidade de informa\u00e7\u00e3o. Para uma defini\u00e7\u00e3o mais \n\nquantitativa da negentropia \u00e9 necess\u00e1ria, por\u00e9m, uma defini\u00e7\u00e3o mais precisa da entropia. A \n\nentropia \u00e9 uma grandeza associada \u00e0 imprevisibilidade de uma vari\u00e1vel. Quanto mais \n\nimprevis\u00edvel for o resultado de uma a\u00e7\u00e3o  , maior ser\u00e1 a entropia associada \u00e0  . \n\nMatematicamente, a entropia   de um vetor aleat\u00f3rio com densidade       pode ser definida \n\ncomo \n\n                          \n\n  Uma vari\u00e1vel gaussiana maximiza um conjunto de vari\u00e1veis aleat\u00f3rias de mesma vari\u00e2ncia \n\n(Gokhale, 1989), pois sua distribui\u00e7\u00e3o \u00e9 a mais aleat\u00f3ria poss\u00edvel. A entropia tem valores \n\nmenores para distribui\u00e7\u00f5es que se encontram concentradas em certos valores, portanto, pode ser \n\nusada como medida de n\u00e3o-gaussianidade, uma vez que nesse caso a entropia \u00e9 m\u00e1xima.  \n\nUma maneira de se obter esse resultado \u00e9 utilizar-se de alguma medida que tenha limite \n\nigual a zero quando uma distribui\u00e7\u00e3o tenha uma distribui\u00e7\u00e3o que se distancia da distribui\u00e7\u00e3o \n\nnormal. A forma mais utilizada (Hyv\u00e4rinen, 1998) \u00e9 uma vers\u00e3o normalizada (diferencial de \n\nentropia), conhecida por negentropia   e definida como \n\n                      \n\nonde         \u00e9 uma vari\u00e1vel aleat\u00f3ria gaussiana com mesma correla\u00e7\u00e3o e covari\u00e2ncia de  .  \n\nDessa forma, a negentropia     , ser\u00e1 sempre n\u00e3o-negativa, pois           tem o maior \n\nvalor poss\u00edvel entre as vari\u00e1veis rand\u00f4micas de mesma vari\u00e2ncia de  , isto \u00e9                     \n\nPor outro lado, a negentropia      s\u00f3 ser\u00e1 zero quando       \u00e9 uma distribui\u00e7\u00e3o correlata \n\nda distribui\u00e7\u00e3o normal, logo,      \u00e9 uma medida de n\u00e3o gaussianidade. \n\n \n\n \n\n \n\n \n\n \n\n- Matriz de Covari\u00e2ncia \n\n \n\n\n\n \n\n \n\n  \n\n90 \n\n \n\nSe   \u00e9 um vetor tal que                  a matriz de covari\u00e2ncia   de   \u00e9 denotada por \n   e \u00e9 calculada atrav\u00e9s de \n\n \n\n     \n                     \n\n   \n                     \n\n  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\n  \n\n91 \n\n \n\n \n\n \n\nAp\u00eandice B \u2013 M\u00e9trica \n \n\nM\u00e9trica \u00e9 um conceito que generaliza a id\u00e9ia geom\u00e9trica de dist\u00e2ncia. Um conjunto em que \n\nh\u00e1 uma m\u00e9trica definida recebe o nome de espa\u00e7o m\u00e9trico (Lima, 1993). \n\nSe   \u00e9 um conjunto que admite uma m\u00e9trica  , ent\u00e3o a fun\u00e7\u00e3o dist\u00e2ncia            , \n\nassocia dois elementos de um conjunto a um n\u00famero real e deve obedecer aos seguintes axiomas: \n\n \n\n1)Ser sempre positiva                    \n\n2)Ser sim\u00e9trica                         \n\n3)Obedecer a desigualdade triangular                                   \n\n4)Ter resposta nula apenas para pontos coincidentes                \n\n \n\nO conceito empregado anteriormente para a defini\u00e7\u00e3o de ponto mais pr\u00f3ximo pode pairar \n\nsobre v\u00e1rios tipos de dist\u00e2ncia, tais como a dist\u00e2ncia euclidiana, dist\u00e2ncia de Manhattam, e a \n\ndist\u00e2ncia de Mahalanobis, descritos a seguir \n\nA dist\u00e2ncia usual entre dois pontos, que pode ser definida pela aplica\u00e7\u00e3o repetida do \n\nteorema de Pit\u00e1goras. \u00c9 o conceito de dist\u00e2ncia mais comumente utilizado. \n\nMais especificamente, se                               s\u00e3o pontos em algum espa\u00e7o de \n\n  dimen\u00f5es, ent\u00e3o a dist\u00e2ncia euclidiana    entre estes pontos \u00e9 definida como \n\n           \n           \n\n            \n              \n\n \n\n \n\n   \n\n \n\nA dist\u00e2ncia de Manhattan, considerada por Hermann Minkowski no s\u00e9culo XIX, \u00e9 uma \n\nforma de geometria em que a usual dist\u00e2ncia \u00e9 substitu\u00edda por uma nova m\u00e9trica, onde esta \u00e9 \n\ndada pela soma das diferen\u00e7as absolutas das coordenadas de dois pontos (Black, 2006). Tal \n\ndist\u00e2ncia tamb\u00e9m \u00e9 conhecida como Geometria do t\u00e1xi, ou dist\u00e2ncia   . Tal m\u00e9trica faz alus\u00e3o \u00e0 \n\ndist\u00e2ncia percorrida por t\u00e1xis nas ruas de Manhattan, que s\u00e3o dispostas em formato quadriculado. \n\nSe                               s\u00e3o pontos em algum espa\u00e7o de   dimens\u00f5es, ent\u00e3o a \n\ndist\u00e2ncia de Manhattan      entre estes pontos \u00e9 definida como \n\n\n\n \n\n \n\n  \n\n92 \n\n \n\n                                           \n\n \n\n   \n\n \n\nA t\u00edtulo de curiosidade, \u00e9 interessante notar que uma circunfer\u00eancia\ni\n\u00b7, na m\u00e9trica de \n\nManhattan, geometricamente \u00e9 dada por um quadrado cujos lados comp\u00f5em \u00e2ngulos de     com \n\nos eixos coordenados. \n\nA dist\u00e2ncia de Mahalanobis \u00e9 baseada nas correla\u00e7\u00f5es entre vari\u00e1veis com as quais \n\ndistintos padr\u00f5es podem ser identificados e analisados (Maesschalck, 2000). Essa m\u00e9trica foi \n\nintroduzida na d\u00e9cada de 1930 pelo matem\u00e1tico indiano Prasanta Chandra Mahalanobis. \u00c9 uma \n\nestat\u00edstica \u00fatil para determinar a similaridade entre uma amostra desconhecida e uma conhecida. \n\nDistingue-se da dist\u00e2ncia euclidiana j\u00e1 que leva em conta as correla\u00e7\u00f5es do conjunto de dados e \u00e9 \n\ninvariante \u00e0 escala, ou seja, n\u00e3o depende da escala das medi\u00e7\u00f5es. \n\nSe                \u00e9 um ponto em algum espa\u00e7o de   dimens\u00f5es,               , a \n\nm\u00e9dia de   e   a matriz de covari\u00e2ncia de  , ent\u00e3o a dist\u00e2ncia de Mahalanobis      desse ponto \n\n\u00e9 definida como \n\n              \n           \n\n \n\nSe                               s\u00e3o pontos de mesma distribui\u00e7\u00e3o em algum espa\u00e7o de \n\n  dimens\u00f5es e   \u00e9 a matriz de covari\u00e2ncia entre esses pontos, ent\u00e3o a dist\u00e2ncia de Mahalanobis \u00e9 \n\ndefinida como \n\n                \n           \n\nEm particular, se a matriz de covari\u00e2ncia \u00e9 a matriz de identidade, a dist\u00e2ncia de \n\nMahalanobis \u00e9 reduzida a dist\u00e2ncia euclidiana. Se a matriz de covari\u00e2ncia \u00e9 diagonal, a dist\u00e2ncia \n\nde Mahalanobis \u00e9 definida como uma dist\u00e2ncia euclidiana normalizada  \n\n      \n       \n\n \n\n  \n   \n\n       \n \n\n  \n     \n\n       \n \n\n  \n \n\n     \n       \n\n \n\n  \n \n\n \n\n   \n\n \n\nonde    \u00e9 o desvio padr\u00e3o entre       . \n\n \n\n \n\n                                                \ni\n Por defini\u00e7\u00e3o, circunfer\u00eancia \u00e9 o conjunto de pontos com dist\u00e2ncia fixa, chamada raio, at\u00e9 algum ponto chamado de \n\ncentro"}]}}}