{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.09310"}, {"@name": "filename", "#text": "14282_327126.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE SANTA CATARINA PROGRAMA DE POS-GRADUACAO EM CIENCIA DA COMPUTACAO\nMariana Dehon Costa e Lima\nmEtodo de discretizacao de variAveis para REDES BAYESIANAS UTILIZANDO ALGORITMOS GENETICOS\nmEtodo de discretizacao de variAveis para redes bayesianas utilizando algoritmos genEticos\nDisserta\u00e7\u00e3o submetida ao Programa de Pos Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7\u00e3o da Universidade Federal de Santa Catarina para a obten\u00e7ao do Grau de Mestre em Ci\u00eancia da Computa\u00e7\u00e3o.\nOrientadora: Silvia Modesto Nassar, Dra.\nmEtodo de discretizacao de variAveis para redes bayesianas utilizando algoritmos genEticos\nEsta Disserta\u00e7\u00e3o foi julgada aprovada para a obten\u00e7\u00e3o do T\u00edtulo de \u201cMestre em Ci\u00eancia da Computa\u00e7\u00e3o\u201d, e aprovada em sua forma final pelo Programa de P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia da Computa\u00e7ao da Universidade Federal de Santa Catarina.\nFlorian\u00f3polis, 27 de fevereiro 2014.\nRonaldo dos Santos Mello, Dr.\nCoordenador do Curso\nBanca Examinadora:\nMauro Roisenberg, Dr.\nDedico esse trabalho aos meus pais.\nAGRADECIMENTOS\nAgrade\u00e7o aos meus pais pelo suporte, dedica\u00e7\u00e3o e por terem uma f\u00e9 inabal\u00e1vel em mim e por serem meus maiores incentivadores. Esse trabalho n\u00e3o existiria sem todo o apoio que me deram e \u00e1 uma conquista t\u00e3o minha quanto suas.\nAgrade\u00e7o tambem \u00e0 toda minha fam\u00edlia, em especial a minha tia Marlene, meu avo Feliciano e minha av\u00e1 Nininha (in memoriam). Muito obrigada por terem acreditado em mim e por me dado todo o est\u00e1mulo que eu precisava.\nAgrade\u00e7o \u00e0 minha melhor amiga, Tatiane, que tem sido meu ponto de apoio ha v\u00e1rios anos e por sempre ter uma palavra de encorajamento quando eu me sentia desmotivada ou abalada.\nAgrade\u00e7o tambem \u00e0 Ana Luiza, que esteve presente durante todo o desenvolvimento desse trabalho e por ter me dado todo o apoio emocional que eu precisava durante esses anos. Muito obrigada por ter revisado essa dissertac\u00e7a\u00e3o quase tantas vezes quanto eu, por ter ouvido todas as minhas preocupa\u00e7\u00e3es (que n\u00e3o foram poucas) e por estar sempre dispon\u00e1vel pra mim.\nAgrade\u00e7o \u00e0 minha orientadora Silvia Modesto Nassar, que se tornou uma refer\u00eancia para mim tanto na vida pessoal quanto na profissional. Muito obrigada pela confian\u00e7a, paciencia e pelo conhecimento transmitido. E principalmente, muito obrigada por sempre ter me tratado como uma \u201cfilha acad\u00eamica\u201d, por acreditar em mim e por me dar todo o incentivo poss\u00e1vel para completar esse trabalho.\nAgrade\u00e7o \u00e0 Petrobras pelo suporte financeiro e pela oportunidade atraves do projeto que deu origem \u00e0 minha disserta\u00e7\u00e3o. Agrade\u00e7o tamb\u00e1em aos meus colegas de projeto e aos professores respons\u00e1aveis Paulo, Silvia, Mauro e Rivalino pela experi\u00eancia, sugest\u00f5es e contri-bui\u00e7c\u00e3oes durante a execu\u00e7c\u00e3ao desse trabalho.\nAgrade\u00e7co, enfim, aos meus colegas e amigos do laborat\u00e1orio PerformanceLab pela conviv\u00eaencia, conselhos e experi\u00eaencia adquirida. Muito obrigada, em especial, ao Altieres, Diego, Gabriel e Pedro.\nPara ser grande, s\u00ea inteiro: nada teu exagera ou exclui. S\u00ea todo em cada coisa. Poe quanto es no m\u00ednimo que fazes. Assim em cada lago a lua toda brilha, porque alta vive.\nRicardo Reis (Fernando Pessoa)\nRESUMO\nRede Bayesiana \u00e9 uma t\u00e9cnica de classifica\u00e7\u00e3o vastamente utilizada na area de Intelig\u00eancia Artificial. Sua estrutura e composta por um grafo ac\u00edclico direcionado usado para modelar a associac\u00e3o de variaveis categ\u00f3ricas (qualitativas). Entretanto, em casos onde existem variaveis numericas no dom\u00ednio, uma pr\u00e9 discretiza\u00e7\u00e3o e geralmente necessaria. Nesta disserta\u00e7\u00e3o, e apresentada uma discretiza\u00e7\u00e3o heur\u00edstica para Redes Bayesianas que procura padr\u00e3oes nos dados e os divide de acordo com os padro\u00e3es encontrados. Esses padr\u00e3oes s\u00e3ao identificados por dois eventos que sao otimizados por uma busca atraves do Algoritmo Genetico. Esses dois eventos mudam de acordo com a base de dados, tornando a discretiza\u00e7\u00e3o proposta mais flex\u00edvel para lidar com diferentes dom\u00ednios de aplica\u00e7ca\u00e3o.\nO metodo de discretizac\u00e3o proposto foi testado em duas situa\u00e7\u00e3es distintas: quando a variavel de sa\u00edda e qualitativa (classifica\u00e7\u00e3o) e tambem quando a variavel de sa\u00edda e quantitativa e e necessario estimar o seu valor medio e desvio-padrao.\nPara casos em que a sa\u00edda e qualitativa foram utilizados duas bases de dados: Iris Flower e Wine. Em ambas as bases de dados a acur\u00e9cia do metodo proposto foi superior quando comparada com outros dois metodos da literatura: um que discretiza as variaveis por frequ\u00eaencia e outro por tamanho de classes.\nPara representar os casos em que variavel de sa\u00edda e quantitativa, foi utilizada uma base de dados real com dados de perfura\u00e7c\u00e3ao de poc\u00e7os de petroleo com o objetivo de estimar a taxa media de perfurac\u00e3o de broca. Nesses casos, e feito a estima\u00e7\u00e3o do valor de sa\u00edda atraves da media da distribui\u00e7ca\u00e3o de probabilidade. O metodo proposto obteve um erro inferior na estima\u00e7\u00e3o quando comparado tanto com o metodo que discretiza por frequ\u00eaencia quanto com o metodo que discretiza por tamanho.\nCom os resultados, a conclusao e que o metodo pode discretizar as variaveis quantitativas atraves das identifica\u00e7c\u00e3oes dos eventos que desviam de um intervalo intermediario nos dados, seja para cima (pico) ou para baixo (vale). Tambem foi observado que o metodo esta ligado a um problema de otimiza\u00e7ca\u00e3o global quando todas as variaveis quantitativas s\u00e3ao discretizadas ao mesmo tempo.\nPalavras-chave: Redes Bayesianas, discretizac\u00e3o, otimiza\u00e7\u00e3o global, algoritmo genetico.\nABSTRACT\nBayesian Network (BN) is a classification technique widely used in Artificial Intelligence. Its structure is a DAG (direct acyclic graph) used to model the association of categorical variables. However, in cases where the variables are numerical, a previous discretization is usually necessary.\nIn this dissertation, we show a heuristic discretization for Bayesian Networks that search for data patterns and divide the data according to them. These patterns are identified by two events: peak and valley being optimized by a search through the Genetic Algorithm. These two events change according to the database, making the proposed method a flexible discretization to handle different application domains.\nThe Peak-Valley Discretization Method proposed was tested two different situations: only classification when the output variable is qualitative and also estimating the mean value and the standard deviation when the output variable is quantitative.\nConsidering the cases where the output is quantitative, two databases where used: Iris Flower and Wine. The accuracy in both of them was superior with the proposed method when compared with two other methods from the literature: one that discretizes the variable by frequency and one that does that by class' size.\nTo represent the cases where the output variable is quantitative, was used a real data of oil wells perforation with the objective of estimating the average perforation rate. In such cases, the estimation is done by the average of the output value distribution of probability. The proposed method achieved a lower error in the estimation when compared with the method of frequency discretization and with the method that discretizes by size.\nWith the results, the conclusion is that the method can properly discretize the quantitative variables by identifying events that deviate from expected results within the knowledge domain, whether up (peak) or down (valley). It was also observed that the method brings a problem of global optimization when discretizing all quantitative variables simultaneously. The problem of global optimization was treated by a Genetic Algorithm.\nKeywords: Bayesian Networks, discretization, global optimization, genetic algorithm.\nLISTA DE FIGURAS\nFigura 1 Aprendizado e previs\u00e3o dos algoritmos do tipo Aprendizado Supervisionado................................................ 36\nFigura 2 Topologia Naive Bayes..................................... 37\nFigura 3 Entradas e Sa\u00edda em uma Rede Bayesiana.................... 38\nFigura 4 Estrutura Geral de uma Rede Bayesiana..................... 39\nFigura 5 Fluxograma do Algoritmo Gen\u00e9tico.......................... 43\nFigura 6 Eventos de Pico e Vale.................................... 48\nFigura 7 Representa\u00e7\u00e3o de um indiv\u00edduo no DPV...................... 48\nFigura 8 Fluxograma do m\u00e9todo DPV.................................. 49\nFigura 9 RB treinada pelo DPV para o Problema Iris Flower.... 62 Figura 10 RB treinada pelo EFD para o Problema Iris Flower.... 62\nFigura 11 RB treinada pelo EWD para o Problema Iris Flower... 63\nFigura 12\tRB\ttreinada\tpelo DPV para o Problema Wine.............. 64\nFigura 13\tRB\ttreinada\tpelo EFD para o Problema Wine.............. 64\nFigura 14 RB treinada pelo EWD para o Problema Wine................ 65\nFigura 15\tRB\ttreinada\tpelo DPV para o Problema da ROP........ 69\nFigura 16\tRB\ttreinada\tpelo EFD para o Problema da ROP........ 70\nFigura 17 RB treinada pelo EWD para o Problema da ROP.......... 70\nFigura 18 Exemplo de entrada e estima\u00e7c\u00e3ao de valor para o problema ROP..............................................................   71\nFigura 19 Valores estimados de ROP no m\u00e9todo DPV (treinamento)............................................................. 72\nFigura 20 Valores estimados de ROP no m\u00e9etodo EFD (treinamento). 72\nFigura 21 Valores estimados de ROP no m\u00e9etodo EWD (treinamento)............................................................. 73\nFigura 22\tValores\testimados\tde\tROP\tno\tm\u00e9etodo\tDPV (teste).... 73\nFigura 23\tValores\testimados\tde\tROP\tno\tm\u00e9etodo\tEFD (teste).... 74\nFigura 24\tValores\testimados\tde\tROP\tno\tm\u00e9etodo\tEWD (teste).\t. . .\t74\nLISTA DE TABELAS\nTabela 1 Estado da Arte - Discretiza\u00e7\u00e3o Redes Bayesianas..... 33\nTabela 2 Exemplo de Tabela de Probabilidade Condicional (CPT). 38\nTabela 3 Matriz de classificac\u00e3o para o problema Iris Flower. ... 66\nTabela 4 Matriz de classifica\u00e7\u00e3o para o problema Wine........ 67\nTabela 5 Classes e Pontos M\u00e9dios para o problema ROP......... 71\nTabela 6 NRMSE obtido para o Problema da ROP................. 72\nRB\tRede Bayesiana........................................ 27\nDAG\tGrafo Ac\u00edclico Direcionado............................ 27\nFFD\tFixed Frequency Discretization........................ 29\nEWD\tEqual Width Discretization............................ 31\nEFD\tEqual Frequency Discretization........................ 31\nEMD\tEntropy Minimization Discretization................... 31\nLD\tLazy Discretization................................... 31\nPD\tProportional Discretization........................... 31\nFFD\tFixed Frequency Discretization........................ 31\nROC\tReceiver Operating Characteristic..................... 32\nAM\tAprendizado de M\u00e1quina................................ 35\nNN\tAlgoritmo do vizinho mais proximo..................... 35\nSVM\tM\u00e1quinas de vetores suporte........................... 35\nCPT\tTabela de Probabilidade Condicional................... 37\nRBH\tRedes Bayesianas H\u00edbridas............................. 39\nMTE\tMixtures of Truncated Exponentials.................... 41\nAG\tAlgoritmo Gen\u00e1tico.................................... 42\nDPV\tDiscretiza\u00e7\u00e3o Pico e Vale............................. 47\nNRMSE Normalized root mean square error......................... 57\nROP\tTaxa de penetra\u00e7\u00e3o.................................... 68\nRPM\tRevoluc\u00e3es por Minuto................................. 69\nPSB\tPeso sobre a Broca.................................... 69\nHSI\tPot\u00eancia Hidr\u00e1ulica por Polegada Quadrada............. 69\nLISTA DE ALGORITMOS\n1\tM\u00e9todo de\tDiscretiza\u00e7\u00e3o EWD...................... 40\n2\tM\u00e9todo de\tDiscretiza\u00e7ao EFD\t.................... 41\n3\tRelev\u00e2ncia\tdos cortes de pico\te\tvale e discretiza\u00e7\u00e3o .... 54\n4\tM\u00e9todo de\tDiscretiza\u00e7\u00e3o pico\te\tvale via\tAG....... 58\nSUMARIO\n1\tINTRODU\u00c7\u00c3O............................... 27\n1.1\tJUSTIFICATIVA E MOTIVA\u00c7\u00c3O .............. 27\n1.2\tPROBLEMATIZA\u00c7\u00c3O......................... 28\n1.3\tOBJETIVO GERAL.......................... 29\n1.4\tOBJETIVOS ESPEC\u00cdFICOS................... 29\n1.5\tESTRUTURA DA DISSERTA\u00c7Ao................ 30\n2\tESTADO DA ARTE........................... 31\n3\tFUNDAMENTA\u00c7\u00c3O TEORI\u00c7A.................... 35\n3.1\tAPRENDIZADO SUPERVISIONADO.............. 35\n3.2\tREDES BAYESIANAS........................ 35\n3.2.1\tModelagem Bayesiana................... 37\n3.3\tREDES BAYESIANAS HffiRIDAS ............. 39\n3.3.1\tDiscretiza\u00e7\u00e3o......................... 39\n3.3.2\t\u00c7ombina\u00e7\u00e3o de Exponenciais Truncadas . 41\n3.3.3\tAbordagem via \u00c7adeia de Markov - Monte \u00c7arlo . . 42\n3.4\tALGORITMOS GENETICOS.................... 42\n3.4.1\tRepresenta\u00e7\u00e3o de um indiv\u00edduo......... 44\n4\tPRO\u00c7EDIMENTOS METODOLOGI\u00c7OS.............. 47\n5\tMETODO PROPOSTO.......................... 51\n5.1\tPROPRIEDADES PICO E VALE ............. 51\n5.1.1\tExemplo de Aplica\u00e7\u00e3o.................. 54\n5.2\tO PROBLEMA DE OTIMIZACCAo............... 56\n6\tRESULTADOS E DIS\u00c7USsAo................... 61\n6.1\tSA\u00cdDA QUALITATIVA - BASE DE DADOS..... 61\n6.1.1\tO Problema Iris Flower ............... 61\n6.1.2\tO Problema Wine ...................... 63\n6.1.3\tResultados e \u00c7ompara\u00e7ao............... 65\n6.1.4\tDiscussao............................. 65\n6.2\tSAIDA QUANTITATIVA - BASE DE DADOS...... 68\n6.2.1\tProblema da Taxa de Penetrac\u00e3o da Broca (ROP) . 68\n6.2.2\tResultados e \u00c7ompara\u00e7\u00e3o............... 69\n6.2.3\tDiscussao............................. 71\n7\t\u00c7ONSIDERA\u00c7OES FINAIS .................... 77\nREFEREn\u00c7IAS................................. 79\n1\tINTRODU\u00c7\u00c3O\nUma Rede Bayesiana (RB) (PEARL, 1988) \u00e9 um modelo de representa\u00e7\u00e3o e racioc\u00ednio de incerteza que utiliza a probabilidade condicional entre as vari\u00e1veis categ\u00f3ricas (qualitativas) de um dom\u00ednio e as expressa via um grafo ac\u00edclico direcionado (Directed Acyclic Graph - DAG). Sua estrutura gr\u00e1fica consegue mapear as correla\u00e7\u00e3es entre as variaveis, sendo uma linguagem apropriada e com recursos eficientes para a representac\u00e3o da distribui\u00e7\u00e3o conjunta de probabilidades sobre um conjunto rand\u00f4mico de variaveis (FRIEDMAN; GEIGER; GOLDSZ-MIDT, 1997).\nEntretanto, a distribuic\u00e3o conjunta de probabilidades dentro da RB pode ser muito grande e o racioc\u00ednio Bayesiano (infer\u00f4ncia) n\u00e3o \u00e1 uma tarefa trivial. A utiliza\u00e7\u00e3o do componente de fatoriza\u00e7\u00e3o tende a diminuir a complexidade da infer\u00f4encia exata. Entre os algoritmos da \u00e1rea pode-se citar aqueles que sao exatos (SHENOY; SHAFER, 2008); (MADSEN; JENSEN, 1999) e os que s\u00e3ao aproximados para facilitar a infer\u00f4ncia em RBs complexas. Os algoritmos aproximados s\u00e3o divididos em dois tipos: estocasticos (FUNG; CHANG, 1990); (SALMERON; CANO; MORAL, 2000) ou determin\u00edsticos (JENSEN; LAURITZEN; OLESEN, 1990); (CANO; MORAL; SALMERON, 2000).\nA infer\u00f4encia Bayesiana cla\u00edssica \u00ede realizada em casos onde o dom\u00ednio de aplica\u00e7\u00e3o e exclusivamente qualitativo. Para que a t\u00edcnica possa ser aplicada quando o conjunto de vari\u00edveis \u00ed h\u00edbrido, ou seja, apresente vari\u00edaveis num\u00edericas (quantitativas) e vari\u00edaveis qualitativas, \u00ede necess\u00edario usar m\u00edetodos alternativos que possibilitem a infer\u00f4encia dentro da RB. M\u00edtodos de discretizac\u00e3o ou de simula\u00e7\u00e3o (LANGSETH et al., 2009) s\u00e3o geralmente empregados em Redes Bayesianas de dom\u00ednios h\u00edbridos e proporcionam uma infer\u00f4encia aproximada.\n1.1 JUSTIFICATIVA E MOTIVACAo\nEntre os m\u00edetodos de infer\u00f4encia aproximada, o mais comum para lidar com Rede Bayesianas H\u00edbridas e o de Discretizac\u00e3o. Esse metodo, muda o valor num\u00ederico da vari\u00edavel por um correspondente qualitativo, de acordo com alguma m\u00edtrica ou crit\u00e9rio espec\u00edfico. As abordagens de discretiza\u00e7ca\u00e3o sa\u00e3o usualmente feitas atrav\u00edes da distribui\u00e7c\u00e3ao de probabilidades ou usando par\u00f4ametros est\u00edaticos, como a frequ\u00f4encia de cada classe. Alguns fatores favorecem a discretizacao (categorizac\u00e3o) de vari\u00edveis\npara RBs, como:\n\u2022\tFalta de algoritmos eficientes para o aprendizado e a infer\u00eancia para dados cont\u00ednuos (FRIEDMAN; GOLDSZMIDT, 1996);\n\u2022\tFacilidade em compreender caracter\u00edsticas categorizadas em detrimento as cont\u00ednuas (LIU et al., 2002);\n\u2022\tClassificadores utilizando dados discretos (em intervalos) tendem\na ser menos complexos e mais precisos que utilizando dados cont\u00ednuos (FRANK; WITTEN, 1999);\n\u2022\tMenor complexidade computacional o que acarreta em uma maior rapidez no aprendizado e infer\u00eancia (FRIEDMAN; GOLDSZMIDT, 1996),(ROUSU, 2001), (YANG, 2003).\nA discretiza\u00e7ao tamb\u00e9m pode ser feita por especialistas da \u00e1rea de forma manual. Entretanto, essa pode ser uma tarefa complexa: h\u00e1 casos onde os dados nao seguem nenhum padr\u00e2o vis\u00edvel e quando seguem, esse padr\u00e2o pode mudar em diferentes ocasi\u00f5es. Portanto, \u00e1 necessario discretizar os dados com o conhecimento dos pr\u00e9prios dados, porque n\u00e3o h\u00e1 nenhum conhecimento pr\u00e9vio do seu comportamento.\n1.2\tproblematizacAo\nEsta pesquisa visa propor um m\u00e1etodo de discretizac\u00e7\u00e3ao de eventos, aqui chamados pico e vale, observ\u00e1veis em um vetor de dados. Este m\u00e1etodo ser\u00e1a implementado em algoritmo e testado em base de dados de forma que possam ser avaliados seus resultados. Portanto, trata-se de uma pesquisa de base tecnol\u00f3gica.\nEmbora haja varios de dados, A maioria dos algoritmos para discretiza\u00e7\u00e3o deles possui como objetivo principal a clusteriza\u00e7\u00e3o das vari\u00e1veis. Para realizar a discretiza\u00e7\u00e3o no dom\u00ednio da RB, acreditase que seja necess\u00e1ario considerar as distribui\u00e7c\u00e3oes condicionais de cada varia\u00e1vel no processo e como elas se distribuem globalmente na rede. Dentre as abordagens de discretizacao utilizadas em Redes Bayesianas, as mais comuns s\u00e3ao:\n\u2022\tDiscretiza\u00e7\u00e3o de igual largura (Equal Width Discretization - EWD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) - divide os valores de v em k intervalos (definidos por par\u00e2metro) de igual largura w = (xmax \u2014 xmin)/k;\n\u2022\tDiscretiza\u00e7ao de igual frequ\u00e2ncia (Equal Frequency Discretization -EFD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) - ordena os valores de v e os divide em m k intervalos (definidos por par\u00e2metro), sendo que cada intervalo contenha aproximadamente o mesmo nUmero de inst\u00e2ncias;\n\u2022\tDiscretiza\u00e7ao da Minimiza\u00e7ao da Entropia (Entropy Minimization Discretization - EMD) (FAYYAD; IRANI, 1993) - Ordena os valores de v e testa poss\u00edveis pontos de corte atrav\u00e9s do ponto medio de cada par x\u00bf, xi+i. Os dados sao ent\u00e3o discretizados em dois intervalos e a entropia \u00e9e calculada. Para avaliar o corte, a abordagem seleciona aquele com a menor entropia e ent\u00e3ao repete o processo recursivamente, sempre selecionando o melhor ponto de corte.\nOutras tecnicas tamb\u00e9m s\u00e3o aplicadas, como a \u201cDiscretiza\u00e7\u00e3o Pregui\u00e7osa\u201d (Lazy Discretization - LD) (HSU; HUANG; WONG, 2000); (HSU; HUANG; WONG, 2003), \u201cDiscretizacao Proporcional\u201d (Proportional Discretization - PD) e \u201cDiscretiza\u00e7\u00e3o de Frequ\u00eancia Fixa\u201d (Fixed Frequency Discretization - FFD) (YANG; WEBB, 2009).\nUm importante aspecto quanto as RBs esta\u00e9 na sua propriedade de infer\u00e2encia: a distribuic\u00e7a\u00e3o de probabilidades de uma vari\u00e9avel influencia diretamente a outra. Portanto, \u00e9e necess\u00e9ario realizar uma otimizac\u00e7\u00e3ao global para reduzir o erro na RB e, por consequ\u00e2encia, aumentar a sua acura\u00e9cia.\nE ent\u00e3o poss\u00e9vel encontrar um metodo de discretiza\u00e7ao que contribua para a descoberta do conhecimento e aumento da acuracia em Redes Bayesianas?\n1.3\tOBJETIVO GERAL\nPropor um metodo de discretiza\u00e7c\u00e3ao baseado em dados para Redes Bayesianas atraves da otimiza\u00e7\u00e3o global das variaveis do dom\u00e9nio de aplica\u00e7c\u00e3ao.\n1.4\tOBJETIVOS ESPECIFICOS\n\u2022\tIdentificar regi\u00e3oes com eventos de pico e vale nos dados;\n\u2022\tExplorar a viabilidade e as fun\u00e7c\u00e3oes objetivo do Algoritmo Genetico\ncomo m\u00e9todo de otimiza\u00e7\u00e3o global para vari\u00e9vel de sa\u00edda qualitativa e para variavel de sa\u00edda quantitativa;\n\u2022\tIntegrar as propriedades matem\u00e9ticas do m\u00e9todo proposto com o Algoritmo Gen\u00e9tico;\n\u2022\tAvaliar o m\u00e9etodo proposto.\n1.5\tESTRUTURA DA DISSERTACAo\nEsta dissertac\u00e3o est\u00e9 dividida em oito cap\u00edtulos. No Cap\u00e9tulo 1 e mostrada a introduc\u00e3o ao problema e dada uma vis\u00e3o geral da dis-serta\u00e7ao alem do objetivo geral e dos objetivos espec\u00edficos pretendidos.\nNo Cap\u00e9tulo 2 \u00e9 uma feita revisao bibliogr\u00e1fica dos principais metodos de discretiza\u00e7ca\u00e3o para Redes Bayesianas.\nNo Cap\u00e9tulo 3 \u00e9 feita a apresenta\u00e7ao dos fundamentos te\u00f3ricos utilizados: Redes Bayesianas, Redes Bayesianas H\u00edbridas e Algoritmos Geneticos.\nNo Cap\u00e9tulo 4 sao definidos os procedimentos metodologicos adotados na dissertac\u00e7\u00e3ao.\nNo Cap\u00edtulo 5 s\u00e3o mostradas as propriedades matematicas do metodo proposto e definidas as funcoes dos dois pontos de cortes: pico e vale. Tambem \u00e9 feita a mesclagem entre as propriedades de discre-tiza\u00e7ca\u00e3o propostas e o Algoritmo Genetico. S\u00e3ao propostas as fun\u00e7c\u00e3oes objetivo para duas situa\u00e7\u00f5es: quando a variavel de sa\u00e9da \u00e9 qualitativa e quando a variavel de sa\u00edda \u00e9 quantitativa.\nNo Cap\u00edtulo 6 s\u00e3o mostrados estudos de caso em que foram aplicados o metodo proposto e outros metodos da literatura e seus devidos desempenhos para diferentes bases de dados e esses resultados s\u00e3o discutidos e analisados.\nE finalmente, no Cap\u00e9tulo 7 o estudo \u00e9 conclu\u00eddo e s\u00e3o feitas as considera\u00e7c\u00e3oes finais alem da indica\u00e7ca\u00e3o de trabalhos futuros em complemento ao apresentado.\n2\tESTADO DA ARTE\nOs dois m\u00e9todos mais comuns para a discretiza\u00e7\u00e3o em Redes Bayesianas utilizam pontos de corte fixos para a defini\u00e7\u00e3o de intervalos. O primeiro, chamado EWD (Equal Width Discretization - EWD) (CATLETT, 1991); (KERBER, 1992); (DOUGHERTY; KOHAVI; SAHAMI, 1995) divide o conjunto de dados em tamanhos de igual largura e cada uma das divis\u00e3es equivale \u00e0 uma classe na RB, o segundo Discretiza\u00e7\u00e3o de igual frequencia (Equal Frequency Discretization -EFD) divide o conjunto de dados de forma que as classes possuam aproximadamente a mesma quantidade de dados. Ambos os m\u00e9todos nao levam em con-sidera\u00e7c\u00e3ao qualquer rela\u00e7c\u00e3ao entre as vari\u00e9aveis ou a melhoria da acur\u00e9acia da rede. Ambos os m\u00e9todos s\u00e3o utilizados em RBs devido \u00e0 sua simplicidade e boa performance (HSU; HUANG; WONG, 2003).\nFayyad e Irani (1993) propoe um metodo heur\u00edstico Discretiza\u00e7\u00e3o da Minimiza\u00e7\u00e3o da Entropia (Entropy Minimization Discretization -EMD), que ao contr\u00e9rio dos metodos EWD e EFD \u00e9 um m\u00e9todo de aprendizado supervisionado. Dougherty, Kohavi e Sahami (1995) aplicou esse metodo em v\u00e1rias bases de dados do reposit\u00f3rio da UCI utilizando Redes Bayesianas e obteve bons resultados.\nHsu, Huang e Wong (2000) prop\u00e3e o m\u00e9todo de \u201cDiscretiza\u00e7\u00e3o Preguicosa\u201d (Lazy Discretization - LD) que deriva diretamente das propriedades da Distribui\u00e7ao de Dirichlet. Nesse m\u00e9todo, a discretiza\u00e7\u00e3o \u00e9 adiada at\u00e9 o momento da classificac\u00e3o. Ele espera at\u00e9 que a inst\u00e2ncia de teste seja apresentada para ent\u00e3ao determinar os pontos de corte e estimar as probabilidades de cada classe. O m\u00e9etodo foi aplicado em bases de dados do reposito\u00e9rio da UCI e obteve bons resultados quando comparado com outros m\u00e9etodos de discretizac\u00e7\u00e3ao.\nMatsuura (2003) prop\u00e3oe um m\u00e9etodo de discretiza\u00e7c\u00e3ao para Redes Bayesianas chamado de Discretizac\u00e3o via Tabela de Probabilidades. O algoritmo discretiza todas as vari\u00e9veis cont\u00e9nuas via EWD ou EFD e usa um algoritmo de aprendizado de estrutura para gerar a mais adequada aos dados discretizados. E realizado um loop para a uni\u00e3ao de intervalos e eles s\u00e3ao avaliados por uma metrica chamada de Diferen\u00e7ca Media Quadratica. O metodo foi aplicado em uma base de dados relacionado a protec\u00e3o ao voo, obtendo resultados promissores e superiores quando comparados ao metodo EFD.\nYang e Webb (2009) propoe dois metodos heur\u00edsticos de discretiza\u00e7\u00e3o:\t\u201cDiscretiza\u00e7\u00e3o Proporcional\u201d (Proportional Discretization -\nPD) e \u201cDiscretiza\u00e7\u00e3o de Frequ\u00e2ncia Fixa\u201d (Fixed Frequency Discreti-\nzation - FFD). O m\u00e9todo PD procura minimizar o bias e a vari\u00e2ncia atrav\u00e9s de um conjunto com n inst\u00e2ncias de treinamento e determinando o valor de s (quantidade de dados em cada intervalo) e t (nUmero de intervalos), de forma que s = t e s x t = n. O m\u00e9todo FFD atua de forma semelhante ao metodo EFD, por\u00e9m em vez de delimitar k intervalos, ele delimita a quantidade m\u00e9nima de dados em cada estado e a quantidade maxima de intervalos. Ambos os m\u00e9todos apresentados obtiveram erros inferiores em bases de dados do repositorio da UCI quando comparados com outros m\u00e9etodos.\nWong (2012) prop\u00f5e uma medida n\u00e3o parametrica de avaliar o n\u00e9vel de depend\u00e2encia entre um atributo cont\u00e9nuo e uma classe, e ent\u00e3o essa medida \u00e9 utilizada em um m\u00e9todo h\u00edbrido de discretiza\u00e7ao de forma que a acuracia em um classificador do tipo Na'ive Bayes seja melhorada. O m\u00e9todo proposto combina quatro m\u00e9todos (EWD, EFD, PD e EMD) e obteve uma acur\u00e1cia geralmente superior que os metodos usados individualmente ao utilizar bases de dados do reposit\u00e9orio da UCI.\nKurtcephe e G\u00fcvenir (2013) propoe um metodo de discretizac\u00e3o global, est\u00e9tico e supervisionado baseado na curva ROC (receiver operating characteristic) utilizando o algoritmo QuickHill (PREPARATA; SHAMOS, 1993) que possui complexidade esperada de O(n log n) e O(n2) no pior caso. O m\u00e9etodo funciona de forma que a \u00e9area sob a curva ROC seja maximizada e s\u00e3ao propostos uma nova medida de discretiza\u00e7c\u00e3ao e um novo crit\u00e9erio de parada. Ao comparar o m\u00e9etodo proposto com outros da literatura, ele obteve um desempenho superior utilizando bases de dados do reposit\u00e9orio UCI. Os autores enfatizam que embora o m\u00e9todo tenha resultados promissores o tempo necessario para \u00e0 con-verg\u00e2encia pode ser alto.\nA Tabela 1 sintetiza os m\u00e9todos apresentados neste cap\u00e9tulo.\nM\u00e9todo\tDescri\u00e7\u00e3o\nEWD (1191, 1995)\tdiscretiza\u00e7\u00e3o por igual largura.\nEFD (1991, 1995)\tdiscretiza\u00e7\u00e3o por igual frequ\u00eancia.\nEFD (1991, 1995)\tdiscretiza\u00e7\u00e3o por minimiza\u00e7\u00e3o da entropia.\nLD (2000)\tdiscretiza\u00e7\u00e3o na hora da classifica\u00e7\u00e3o.\nvia Tabela de Probabilidades (2003)\tutiliza EWD ou EFD, algoritmo de aprendizagem e um loop que une intervalos.\nPD (2009)\tdetermina a quantidade de dados em cada intervalo e o n\u00famero de intervalos.\nFFD (2009)\tdelimita a quantidade m\u00ednima de dados em cada estado e a quantidade m\u00e1xima de intervalos.\nm\u00e9todo de Wong (2012)\tavalia o n\u00edvel de depend\u00eancia entre um atributo cont\u00ednuo e uma classe e combina os m\u00e9todos EWD, EFD, PD e EMD.\nm\u00e9todo de Kurtcephe (2013)\tm\u00e9todo de discretiza\u00e7\u00e3o global, est\u00e1tico e supervisionado baseado na curva ROC.\tw\n34\n3\tFUNDAMENTA\u00c7AO TE\u00d3RICA\nEsse cap\u00edtulo objetiva apresentar os principais conceitos utilizados para a composi\u00e7ao do m\u00e9todo proposto. Na Se\u00e7\u00e3o 3.1 \u00e9 introduzido o conceito de aprendizado supervisionado e o m\u00e9etodo adotado de pre-vis\u00e3ao nesse tipo de aprendizado.\nNa Se\u00e7ao 3.2 \u00e9 definido formalmente uma Rede Bayesiana, assim como sua modelagem. Na Sec\u00e3o 3.3 \u00e9 incorporado o conceito de Redes Bayesianas H\u00e9bridas e suas tecnicas de infer\u00eancia aproximada.\nE, finalmente, na Se\u00e7ao 3.4 sao discutidos os principais conceitos dos Algoritmos Gen\u00e9eticos e as formas de representac\u00e7\u00e3ao de um indiv\u00e9duo.\n3.1\tAPRENDIZADO SUPERVISIONADO\nO Aprendizado de M\u00e1quina (AM) \u00e9 um conjunto de t\u00e9cnicas computacionais que tem por objetivo a cria\u00e7\u00e3o de sistemas capazes de adquirir e organizar o conhecimento de forma automatica (MITCHELL, 1997).\nUma das t\u00e9cnicas do AM \u00e9 o Aprendizado Supervisionado (Supervised learning) que consiste em criar uma funcao atrav\u00e9s de um conjunto de treinamento (MITCHELL, 1997).\nEsse conjunto possui pares de objetos de entrada (tipicamente vetores) e sa\u00e9da desejada, que pode ser um numero real (para casos de regress\u00e3ao) ou um r\u00e9otulo de uma classe (para casos de classificac\u00e7a\u00e3o).\nO objetivo do Aprendizado Supervisionado \u00e9e utilizar a fun\u00e7c\u00e3ao criada para prever o valor de sa\u00e9da (resultado) atrav\u00e9es dos dados de entrada (Figura 1). Os algoritmos principais dessa t\u00e9cnica s\u00e3o: Redes Neurais, algoritmo do vizinho mais pr\u00e9oximo (Nearest Neighbor - NN), \u00e9rvores de decis\u00e3o, as maquinas de vetores suporte (Support Vector Machines - SVM) e as Redes Bayesianas.\n3.2\tREDES BAYESIANAS\nDefinindo formalmente, uma Rede Bayesiana \u00e9 uma dupla (G,P), onde G=(V,E) \u00e9 um DAG nos quais os nodos V = vi,v2,...,vn representam as vari\u00e9veis e as arestas E = e1,... ,en representam uma direta correlac\u00e3o entre cada nodo de V. O item P \u00e9 definido como os par\u00eaametros probabil\u00e9sticos expressos atrav\u00e9es de tabelas: dada uma de-\nFigura 1 - Aprendizado e previs\u00e3o dos algoritmos do tipo Aprendizado Supervisionado.\nterminada vari\u00e1vel e feita a distribui\u00e7\u00e3o de probabilidade condicional de cada uma de suas classes (estados) X = x1,...,xn em rela\u00e7\u00e3o a cada uma das classes de seus pais.\nOu seja, a RB estabelece que uma vari\u00e1vel \u00e1 independente de todas as outras vari\u00e1veis, exceto de seus descendentes no grafo dado o estado de seus pais. A infer\u00eancia na RB \u00e1 feita pelo Teorema de Bayes:\nP (V = v|X = x)= P (X =\t= v)\t(3.1)\nA probabilidade conjunta \u00e1 determinada pela chamada \u201cregra da cadeia\u201d e assume a independencia condicional entre as vari\u00e1veis:\nn\nP (vi,..., Vn) = n P (Vi\\pai(Vi))\t(3.2)\ni=1\nonde pai(Vi) determina o conjunto de nodos pais do nodo V).\nA infer\u00eancia exata seguindo a Equa\u00e7ao 3.2 nao \u00e1 uma tarefa trivial, pois a distribuic\u00e3o de probabilidade conjunta pode ser muito grande. Por exemplo, em um caso em que haja 10 nodos discretos com 2 estados cada. A distribui\u00e7\u00e3o de probabilidade \u00e1 de 210 \u2014 1 = 1023 valores expressos em uma tabela. E a tabela de cresce forma exponencial: se fossem 11 nodos haveriam 211 \u2014 1 = 2047 valores expressos.\nO racioc\u00ednio Bayesiano \u00e1 estabelecido em dois cenarios distintos:\nse \u201centrada\u201d entao \u201csa\u00edda\u201d\nse \u201csa\u00edda\u201d entao \u201centrada\u201d\n3.2.1\tModelagem Bayesiana\nA modelagem de uma RB \u00e9 feita a partir de uma estrutura (DAG) que incorpora a categoriza\u00e7\u00e3o das variaveis continuas. A partir dessa modelagem e necessario estabelecer as tabelas de probabilidade (for\u00e7a de associac\u00e3o entre variaveis) atraves do aprendizado do dominio a ser trabalhado. Existem tr\u00eas formas de realizar essa aprendizagem: exclusivamente dos dados (base de dados), exclusivamente dos especialistas do dominio ou aprender de forma h\u00edbrida tanto dos dados quanto dos especialistas.\nEntre as poss\u00edveis topologias de rede e vastamente conhecida a estrutura Naive Bayes, que e o mais simples entre esses modelos. dado o contexto de classe. Embora esse modelo n\u00e3ao traduza a realidade na maioria das tarefas do mundo real ele e bastante efetivo, pois os parametros de cada atributo podem ser aprendidos separadamente, facilitando o processo de aprendizagem (MCCALLUM; NIGAM et al., 1998).\nA topologia Na'ive Bayes e, portanto, um conjunto de variaveis de entrada independentes entre si que possuem em conjunto um unico pai (no de sa\u00edda). Um exemplo da topologia Naive Bayes pode ser vista na Figura 2. Nesse caso, o nodo A e a saida e os nodos B, C e D s\u00e3ao as entradas.\nFigura 2 - Topologia Naive Bayes.\nAl\u00e9m da topologia da rede, \u00e9 necess\u00e1rio especificar a Tabela de Probabilidade Condicional (Conditional Probability Table - CPT) de cada nodo, o que descreve a probabilidade de cada classe do nodo em combinac\u00e3o com cada classe de seus pais. Um exemplo de CPT para a\nRB da Figura 2 \u00e9 mostrado na Tabela 2.\nTabela 2 - Exemplo de Tabela de Probabilidade Condicional (CPT).\nA\tP (B = stateO)\tP (B = statel)\tP (B = state2)\nstate0\t0.2\t0.2\t0.5\nstatel\t0.1\t0.5\t0.4\nstate2\t0.1\t0.05\t0.85\nO nodo de sa\u00edda, em uma Rede Bayesiana, tem seus valores de probabilidade calculados por infer\u00eancia (Equac\u00e3o 3.2) ao utilizar a evid\u00eancia expressa pelos nodos de entrada. A cria\u00e7\u00e3o de uma topologia de rede hier\u00e1rquica e feita ao adicionar nodos intermediarios entre as entradas e a sa\u00edda (Figura 3).\nFigura 3 - Entradas e Sa\u00edda em uma Rede Bayesiana.\nOu seja, cada variavel do dom\u00ednio de aplica\u00e7\u00e3o se torna um nodo na RB e uma liga\u00e7ao entre dois nodos denota uma rela\u00e7\u00e3o de \u201ccausa e efeito\u201d (probabilidade condicional). Para especificar a \u201cforca\u201d dessa rela\u00e7\u00e3o s\u00e3o feitas (usualmente) tabelas do tipo CPT para cada um dos nodos (Figura 4).\nFigura 4 - Estrutura Geral de uma Rede Bayesiana.\n3.3\tREDES BAYESIANAS H\u00cdBRIDAS\nSao chamadas de Redes Bayesianas H\u00edbridas (RBH) aquelas que possuem tanto nodos cont\u00ednuos quanto nodos discretos. Nos casos em que h\u00e1 apenas nodos discretos no dom\u00ednio, a distribui\u00e7\u00e3o condicional pode ser representada atraves de uma tabela com valores de probabilidade.\nEntretanto, o problema se torna mais complexo nas Redes Bayesianas H\u00edbridas, pois trabalha-se com modelos para expressar a distri-buic\u00e3o de probabilidade condicional e nao com tabelas (CPT) como nas RBs cl\u00e1ssicas. A acur\u00e1cia da infer\u00eancia depende do dom\u00ednio a ser representado e nem sempre \u00ed exata (LANGSETH et al., 2009).\nPara lidar com os casos em que a infer\u00eaencia exata na\u00e3o \u00ede poss\u00edvel, s\u00e3o utilizados m\u00edtodos aproximados em Redes Bayesianas H\u00edbridas. Os m\u00edtodos mais populares de infer\u00eancia aproximada s\u00e3o: Discretiza\u00e7ao (Se\u00e7ao 3.3.1), Combina\u00e7\u00e3o de exponenciais truncadas (Sec\u00e3o 3.3.2) e ainda uma abordagem utilizando Cadeia de Markov (Se\u00e7\u00e3o 3.3.3).\n3.3.1\tDiscretiza\u00e7\u00e3o\nA t\u00edecnica mais comum para lidar com a infer\u00eaencia em Redes Bayesianas H\u00edbridas \u00e9 a discretiza\u00e7\u00e3o. Essa t\u00edcnica consiste em trocar\no valor cont\u00ednuo x da vari\u00e1vel por seu valor discreto equivalente x'.\nOs metodos EWD e EFD s\u00e3o muito utilizados para discretizac\u00e3o em Redes Bayesianas por sua baixa complexidade computacional e facilidade de de implementac\u00e3o, alem de sua boa performance (HSU; HUANG; WONG, 2003).\nO metodo EWD possui complexidade O(n) em um vetor ordenado, pois divide uma variavel em intervalos de igual largura, ou seja, os pontos de corte s\u00e3o definidos de forma que exista k intervalos com tamanhos de:\ni xmax xmin\\\t\\\nw = (------k------)\t(3.4)\nonde xmax \u00e9 o maior valor da variavel e xmin o menor valor (Algoritmo 1).\nAlgoritmo 1 Metodo de Discretiza\u00e7\u00e3o EWD\t\n1:\tv\ta vari\u00e1vel quantitativa ordenada a ser discretizada\n2:\txmin\tmenor valor em v\n3:\txmax\tmaior valor em v\n4:\tk\tquantidade de intervalos\n5:\txm,ax\txmin\n6:\tcorte\tw\n7:\tindice\t0\n8:\tclasseatuai\tclasse + indice (nome da primeira classe - a que possui os menores valores)\n9:\tfor all xi em v do\n10:\tif xi > corte then\n11:\tcorte\tcorte + w\n12:\tindice\tindice + 1\n13:\tclasseatuai\tclasse + indice\n14:\tend if\n15:\tdiscretize xi para classeatuai\n16:\tend for\n17:\treturn v discretizada (v*)\nO m\u00e9todo EFD tamb\u00e9m possui complexidade O(n) em um vetor ordenado, porem divide uma variavel em tamanhos de igual frequencia. Ou seja, os pontos de corte s\u00e3o definidos de forma que cada classe possua aproximadamente o mesmo numero de registros (Algoritmo 2).\nNos metodos EWD e EFD e necessario fornecer a variavel ordenada. O problema da ordena\u00e7\u00e3o possui complexidade 6(n log n), e\nAlgoritmo 2 M\u00e9todo de Discretiza\u00e7\u00e3o EFD\n1: v a variavel quantitativa ordenada a ser discretizada\n2: n quantidade de registros em v\n3: k quantidade de intervalos\n4: range n\n5: igual FALSO\n6: indiceclasse\t1\n7: indiceregistro\t0\n8: classeatuai\tclasse + indice\n9: classeanterior\n10: while indiceregistro &lt;n do\n11:\tif i &lt;(range * j) OU igual = verdadeiro then\n12:\tdiscretize Xi para classeatuai\n13:\telse\n14:\tindiceclasse \u2014 v[indiceregistro]\n15:\tend if\n16:\tclasseanterior \u2014 indice + 1\n17:\tif v[indiceregistro I 1] \u2014 classeanterior then\n18:\t// caso v[indiceregistro + 1] == n foi omitido para fins de\nsimplicidade\n19:\tigual\tVERDADEIRO\n20:\telse\n21:\tigual\tFALSO\n22:\tend if\n23: end while\n24: return v discretizada (v*)\nportanto a complexidade total dos metodos e O(n) * 0(n log n).\n3.3.2\tCombina\u00e7ao de Exponenciais Truncadas\nO Modelo de Combina\u00e7\u00e3o de Exponenciais Truncadas (Mixtures of Truncated Exponentials - MTE) (MORAL; RUM\u00cd; SALMERON, 2001) pode ser entendido como uma generaliza\u00e7\u00e3o da discretiza\u00e7\u00e3o (LANG-SETH et al., 2009).\nEntretanto, ao inves de utilizar pontos de corte para discretizar cada regi\u00e3ao do conjunto de dados, essa discretiza\u00e7ca\u00e3o e feita por uma combinac\u00e7a\u00e3o linear de fun\u00e7co\u00e3es exponenciais.\nNesse metodo, as func\u00f5es densidade das variaveis s\u00e3o representa\ndas pelas m\u00e9dias das MTEs, que agem como um modelo geral e pode se aproximar \u00e0 distribui\u00e7ao da variavel de forma satisfat\u00f3ria (LANGSETH et al., 2009).\nO principal benef\u00edcio dessa abordagem e uma maior flexibilidade para se aproximar da fun\u00e7\u00e3o de distribui\u00e7ao da variavel.\n3.3.3\tAbordagem via Cadeia de Markov - Monte Carlo\nNesse tipo de abordagem, e utilizada a ideia de amostragem. E garantido que se a quantidade de amostras for suficientemente grande, feita de forma independente e com a mesma distribui\u00e7\u00e3o, e poss\u00edvel obter qualquer grau de precis\u00e3o desejada na estima\u00e7\u00e3o:\n1 N\nEZ (P (T =1|Z )) =\tP (T =1|zi)\t(3.5)\ni=1\nonde Zi,..., zn sao amostras de f (z).\nEssa e uma tecnica de infer\u00eancia que tira vantagem da estrutura Bayesiana para aumentar a velocidade do processo de simula\u00e7\u00e3o.\nNesse tipo de tecnica e importante ficar atento quanto \u00e0 es-tima\u00e7ca\u00e3o de eventos raros, ja que muitas amostras devem ser geradas para poder obter uma unica amostra desse evento (LANGSETH et al., 2009).\n3.4\tALGORITMOS GENETICOS\nAlgoritmos Geneticos (AGs) sao otimizadores de fun\u00e7\u00e3es, ou seja, metodos que procuram os extremos de uma fun\u00e7\u00e3o objetiva f (x) baseando nos princ\u00edpios da sele\u00e7\u00e3o natural e da genetica populacional (GOLDBERG, 1989) (CANT\u00da-PAZ, 1995) (WEILE; MICHIELSSEN, 1997). A fun\u00e7ao objetivo do problema e usualmente usada para expressar a fun\u00e7\u00e3o fitness no AG.\nUm aspecto importante em rela\u00e7ao \u00e0 fun\u00e7\u00e3o fitness esta na sua responsabilidade de medir a performance da soluc\u00e7\u00e3ao (fun\u00e7c\u00e3ao objetiva) como uma maneira de gerar uma alocac\u00e7\u00e3ao de recursos para a reprodu\u00e7c\u00e3ao (WHITLEY, 1994).\nUm indiv\u00edduo e definido como uma solu\u00e7ao candidata valida no AG, expressa ou por uma string binaria ou por um vetor de numeros reais (JANIKOW; MICHALEWICZ, 1991) (WRIGHT et al., 1991), onde um conjunto de indiv\u00edduos e considerado uma populac\u00e3o. Tr\u00eas operadores\nsao comumente usados: sele\u00e7ao, crossover e muta\u00e7\u00e3o (Figura 5).\nFigura 5 - Fluxograma do Algoritmo Gen\u00e9tico.\nO operador de sele\u00e7\u00e3o usa o fitness de cada individuo para escolher aqueles que sao os mais adaptados da popula\u00e7\u00e3o atual para gerar uma nova popula\u00e7\u00e3o. H\u00e1 v\u00e1rias maneiras de realizar essa sele\u00e7\u00e3o de individuos, mas ela sempre garante que os individuos mais adaptados (melhores fitness) possuam uma maior probabilidade de serem selecionados.\nA reprodu\u00e7\u00e3o \u00e1 feita pelos operadores de crossover e muta\u00e7\u00e3o. O primeiro \u00e1 o mecanismo prim\u00e1rio de explora\u00e7\u00e3o do AG: ele escolhe aleatoriamente um par de indiv\u00edduos pr\u00e1-selecionados e troca in-formac\u00e3o (uma substring, no caso de representa\u00e7\u00e3o bin\u00e1ria) entre os dois indiv\u00e1duos para criar novos indiv\u00e1duos.\nO operador de muta\u00e7c\u00e3ao \u00e1e geralmente considerado como um operador secundario e \u00e1 usado para prevenir que a solu\u00e7ao fique estagnada em algum m\u00e1nimo ou m\u00e1aximo local. A mutac\u00e7\u00e3ao \u00e1e feita atrav\u00e1es da sele\u00e7ao rand\u00f4mica de uma substring em um indiv\u00e1duo e trocando o valor da mesma. O percentual da popula\u00e7c\u00e3ao atingido por esse operador \u00e1e geralmente muito menor que o percentual atingido pelo operador de crossover.\nO AG come\u00e7a com uma popula\u00e7\u00e3o atual e entao a sele\u00e7ao \u00e1 aplicada para criar uma popula\u00e7\u00e3o intermediaria. Recombina\u00e7\u00e3o (muta\u00e7\u00e3o e crossover) \u00e1 ent\u00e3o usada para criar a pr\u00f3xima popula\u00e7\u00e3o. O processo\nentre a popula\u00e7\u00e3o atual ate a pr\u00f3xima popula\u00e7\u00e3o.\nA convergencia do AG tende a evol\u00fair atraves de s\u00facessivas geracoes ate q\u00fae o fitness do melhor individ\u00fao e a media de fitness da pop\u00falac\u00e3o se aproximarem do otimo global (BEASLEY; MARTIN; BULL, 1993).\nAlgoritmos Geneticos n\u00e3o garantem q\u00fae a sol\u00fa\u00e7ao otima vai ser encontrada, e s\u00faa efetividade \u00e9 determinada pelo tamanho da pop\u00fala\u00e7\u00e3o n. O tempo req\u00faerido para q\u00fae o AG convirja e de O(n log n) avaliac\u00e3es de f\u00fan\u00e7oes (GOLDBERG, 1989).\n3.4.1\tRepresenta\u00e7\u00e3o de um indiv\u00edduo\nUm cromossomo representa um indiv\u00edduo, que e uma solu\u00e7\u00e3o candidata do problema a ser resolvido. Entre as representa\u00e7co\u00e3es mais comuns de um indiv\u00edduo no AG, encontram-se: codifica\u00e7\u00e3o binaria, codifica\u00e7\u00e3o em ponto flutuante, maquina de estados finitos e arvores.\nA representa\u00e7ao mais comum e a codifica\u00e7\u00e3o binaria, que descreve o cromossomo por um vetor de bits. A representa\u00e7c\u00e3ao binaria de um numero real esta sujeita \u00e0 seguinte precis\u00e3o:\n2 > (xmax - xmin) * 10p\t(3.6)\nonde l o tamanho da cadeia de bits,s p corresponde a precis\u00e3o, k a quantidade de bits e xmin, xmax definem o intervalo real [xmin,xmax] ao qual o valor a ser representado xr pertence. Logo, quanto maior a precisa\u00e3o desejada maior a quantidade de bits necessaria para obt\u00eae-la.\nE necessario, porem, que as cadeias de bits tenham o mesmo tamanho para a execu\u00e7\u00e3o dos operadores de reprodu\u00e7\u00e3o do AG (crossover, muta\u00e7\u00e3o). Outro quesito importante esta na quantidade de bits no cromossomo: ele deve ser grande o suficiente para permitir uma boa troca de informa\u00e7co\u00e3es durante a reprodu\u00e7c\u00e3ao. Portanto, a tecnica de mapeamento e utilizada.\nA tecnica de mapeamento funciona como uma regra de tr\u00eaes. Ao pegar um numero binario (b2) tradicional, ela aumenta a quantidade de bits necessaria para representa-lo. Para isso, utiliza-se os valores de xmin,xmax de forma similar \u00e0 Equa\u00e7ao 3.6:\nxr\n\u2014 xmin\n+ (xmax\nmin )\nb10\n21 \u2014 1\n(3.7)\nx\nonde b10 eq\u00faivale ao valor na base decimal.\nPor exemplo, considere o cromossomo de 16 bits:\nxb2 = 1011010101010101\t(3.8)\nAo decodific\u00e9a-lo da base 2 para a base 10, \u00e9e obtido o valor:\nxb10 = 46421\t(3.9)\nConsidere ainda que o intervalo [xmin, xmax] \u00e9 definido por [0, 50].\nLogo, o mapeamento na b10 desse valor \u00e9:\n46421\nxr = 0 + (50 - 0)^----i = 35.42\t(3.10)\nCaso o nu\u00e9mero representado seja um inteiro, \u00e9e s\u00e9o realizar o arredondamento, para cima ou para baixo, dependendo do crit\u00e9erio definido.\n46\n4\tPROCEDIMENTOS METODOL\u00d3GICOS\nNo metodo de discretiza\u00e7\u00e3o Pico e Vale proposto (DPV) assume-se que uma vari\u00e1vel numerica v e V possui valores em intervalos extremos e em um intervalo intermedi\u00edario. Ao analisar o intervalo intermediario \u00e1 poss\u00edvel obter os intervalos de valores extremos (valores acima e valores abaixo dos limites do intervalo intermedi\u00edario) e estabelecer suas probabilidades condicionais, assim como suas rela\u00e7c\u00e3oes de causa e efeito: \u201cO que causou esse comportamento? O que ele implica?\u201d.\nObservando o comportamento de uma vari\u00e1avel, \u00e1e poss\u00e1vel inferir se um valor x esta fora do intervalo intermediario, seja de forma positiva (alta) ou negativa (baixa). A delimitac\u00e3o dos intervalos utiliza dois pontos de corte expressos em percentil: o primeiro (pico) e restrito \u00e0 \u00e1rea considerada \u201calta\u201d e o segundo (vale) cobre a \u00e1rea considerada \u201cbaixa\u201d.\nO uso do percentil como medida para os pontos de corte incorpora o conceito de frequ\u00eaencia dos dados (seguindo a linha do EFD, EMD e FFD). Por\u00e1m, o metodo DPV nao segue uma regra pr\u00e9-definida de cortes, ou seja, a quantidade de dados em cada classe \u00e1e descoberta em tempo de processamento. Al\u00e1em disso, ao utilizar a medida de per-centil \u00e1e poss\u00e1vel restringir a \u00e1area de cobertura de cada um dos cortes, definindo seus limites de atua\u00e7c\u00e3ao.\nO uso dos dois pontos de corte sugere que uma varia\u00e1vel num\u00e1erica possui tr\u00eaes comportamentos distintos: \u201cbaixo\u201d, \u201cm\u00e1edio\u201d e \u201calto\u201d. Entretanto, essa premissa nem sempre \u00e1e verdadeira e a utiliza\u00e7c\u00e3ao desses tr\u00eaes comportamentos pode n\u00e3ao trazer benef\u00e1cios para a cria\u00e7c\u00e3ao de uma RB. Isso acontece quando os pontos de corte est\u00e3o muito pr\u00f3ximos dos valores limites, por exemplo, o corte de vale est\u00e1a muito pro\u00e1ximo do menor percentil da vari\u00e1avel ou o corte de pico est\u00e1a muito pro\u00e1ximo do maior percentil. E\u00e1 poss\u00e1vel ainda que os dois cortes estejam ta\u00e3o perto um do outro que um intervalo intermedia\u00e1rio \u00e1e considerado irrelevante.\nA Figura 6 mostra dois exemplos de dados classificados com o DPV. O primeiro gr\u00e1afico possui tr\u00eaes comportamentos distintos: um intermedi\u00e1ario, um superior e um inferior. O segundo gr\u00e1afico mostra apenas dois comportamentos um superior e outro inferior.\nO ponto fundamental para estabelecer os percentis dos cortes est\u00e1 no algoritmo de busca, nesse caso, o Algoritmo Gen\u00e1tico. A escolha do AG deve-se pela sua implementacao simples, resultados eficientes e adequa\u00e7\u00e3o ao problema (WRIGHT et al., 1991).\nFigura 6 - Eventos de Pico e Vale.\nNo m\u00e9todo DPV, cada vari\u00e1vel num\u00e9rica v do conjunto de dados tem seus dois pontos de corte. Esses pontos s\u00e3o encontrados atrav\u00e9s da busca pelo AG e e escolhido o conjunto mais \u201cbem adaptado\u201d \u00e0 fun\u00e7ao objetivo no que diz respeito \u00e0 RB. O conjunto de pontos, que representa um indiv\u00edduo, pode ser visto na Figura 7.\nFigura 7 - Representacao de um indiv\u00edduo no DPV.\nA Figura 8 mostra a vis\u00e3o geral do metodo proposto.\nE importante ressaltar que o metodo DPV e de discretiza\u00e7c\u00e3ao visando a descoberta de conhecimento na RB, ou seja, o conjunto de\nFigura 8 - Fluxograma do m\u00e9todo DPV.\nvari\u00e1veis discretizadas deve refor\u00e7ar o processo de aprendizagem. Dessa forma, a distribui\u00e7\u00e3o de probabilidade dentro de cada nodo da RB nao necessariamente ser\u00e9a sim\u00e9etrica.\nA escolha do melhor indiv\u00e9duo no AG esta diretamente associada ao mecanismo de classifica\u00e7ao na Rede Bayesiana. Cada nodo em uma RB \u00e9 expresso por um vetor probabilidades, sendo que cada um de seus estados tem uma probabilidade de ser \u201cverdadeiro\u201d. Nesta disserta\u00e7\u00e3o foi adotado o m\u00e9etodo de classificac\u00e7a\u00e3o que escolhe o maior valor no vetor de probabilidades do nodo de sa\u00edda para classificar a inst\u00e2ncia.\nExistem duas situac\u00e3es poss\u00edveis para se estabelecer o fitness de um indiv\u00edduo durante a execu\u00e7\u00e3o do DPV: quando a variavel de sa\u00edda \u00ede qualitativa e quando a vari\u00edavel de sa\u00edda \u00ede quantitativa.\nQuando a vari\u00e1vel de sa\u00edda \u00e9 qualitativa, um maior desempenho da rede est\u00ed diretamente ligado \u00e0 classifica\u00e7ao correta dos dados atraves da varia\u00edvel de sa\u00edda. Portanto, a medida de desempenho nesses casos \u00ed a pr\u00edpria acur\u00edcia e o objetivo do algoritmo \u00ed a sua maximiza\u00e7\u00e3o. Ou seja, o melhor indiv\u00edduo de uma popula\u00e7\u00e3o \u00ed aquele que possui a maior acur\u00edacia.\nEntretanto, quando a vari\u00edvel de sa\u00edda \u00ed quantitativa, objetivase estimar valores atrav\u00edes do seu vetor de probabilidade. Esses valores correspondem aos valores medios da distribuic\u00e3o e o desempenho da\nrede esta ligado \u00e0 minimiza\u00e7\u00e3o da taxa de erro entre os valores estimados e os valores num\u00e9ricos da vari\u00e1vel de saida.\nOutro ponto fundamental para as Redes Bayesianas esta na sua topologia. Neste trabalho foi utilizada a estrutura Naive Bayes e, portanto, todas as variiaveis s\u00e3ao consideradas de evid\u00eaencias de entrada com a exce\u00e7ao da variavel de saida. A escolha dessa topologia \u00e9 justificada pela sua efetividade e simplicidade ao facilitar o processo de aprendizagem (Se\u00e7\u00e3o 3.2.1).\nPara avaliar o desempenho do DPV, o m\u00e9todo foi aplicado em tres bases de dados com o objetivo de testar os casos onde a vari\u00e9vel de sa\u00edda \u00e9 qualitativa e onde ela e quantitativa.\nNo primeiro caso (sa\u00e9da qualitativa), foram utilizadas duas bases de dados publicas que retratam problemas de classifica\u00e7ao, sendo a primeira uma base de caracter\u00edsticas de diferentes tipos de flores Iris e a segunda uma analise qu\u00e9mica de diferentes tipos de vinho.\nNo segundo caso (sa\u00e9da quantitativa) foi utilizada uma base de dados de um dom\u00e9nio real que apresenta vari\u00e9aveis de um sistema de perfura\u00e7c\u00e3ao de po\u00e7cos de petr\u00e9oleo e sua respectiva taxa de perfurac\u00e7\u00e3ao. O objetivo, nesse caso, \u00e9e estimar o valor da taxa de perfura\u00e7c\u00e3ao.\nO m\u00e9etodo proposto foi comparado com dois outros m\u00e9etodos da literatura: EFD e EWD. A escolha desses m\u00e9todos se deve \u00e0 sua grande popularidade, efici\u00eaencia, baixo custo computacional e utilizac\u00e7\u00e3ao de forma h\u00e9brida com uma grande quantidade de m\u00e9etodos de discre-tiza\u00e7c\u00e3ao (Se\u00e7c\u00e3ao 2).\n5\tM\u00c9TODO PROPOSTO\nO m\u00e9todo proposto (DPV) \u00e9 composto de dois mecanismos fundamentais:\n\u2022\ta an\u00e9lise dos pontos de corte (pico e vale) estabelecidos ao determinar a sua relevancia;\n\u2022\ta escolha dos pontos de corte mais bem adaptados ao problema atrav\u00e9s do Algoritmo Gen\u00e9tico.\nO metodo DPV \u00e9 param\u00e9trico e define a relevancia dos pontos de corte atrav\u00e9s do coeficiente a. Esse coeficiente determina a proximidade m\u00e1xima permitida entre os pontos de corte e os valores limites da vari\u00e9avel (extremos).\nCaso os pontos de corte estejam muito pr\u00f3ximos, eles ser\u00e3o unidos. Caso um dos pontos de corte esteja muito pr\u00e9ximo de um dos valores extremos da vari\u00e9avel, este ponto sera\u00e9 desconsiderado. E, finalmente, se ambos os pontos de corte estiverem muito pro\u00e9ximos dos pontos extremos, \u00e9e criado um novo ponto de corte atrav\u00e9es da m\u00e9edia dos dois pontos (pico e vale).\nAs propriedades do m\u00e9todo DPV em relac\u00e3o aos pontos de corte s\u00e3o mostradas na Se\u00e7\u00e3o 5.1 e as configura\u00e7\u00f5es em rela\u00e7\u00e3o ao Algoritmo Gen\u00e9tico, assim como as fun\u00e7\u00e3es objetivo empregadas, s\u00e3o mostradas na Se\u00e7\u00e3o 5.2.\n5.1 PROPRIEDADES PICO E VALE\nPara descrever as propriedades dos pontos de corte no m\u00edetodo DPV, os seguintes conceitos s\u00e3o definidos no contexto de uma vari\u00edvel\nv\u00bf:\n\u2022\tp(x) como uma fun\u00e7\u00e3o que recebe um valor x como entrada e retorna o percentil que esse valor se encontra;\n\u2022\tP-1 (y) como a fun\u00e7\u00e3o inversa da fun\u00e7ao p(x): recebe um percentil y como entrada e retorna o valor x que ele representa;\n\u2022\tvale como o percentil expresso pelo ponto de corte vale;\n\u2022\tpico como o percentil expresso pelo ponto de corte pico;\n\u2022 vale &lt;pico;\n\u2022\tX* = x*,... ,xn como o vetor discretizado do conjunto de valores de vi (X = xi,..., x\u201e).\n\u2022\tcomo o percentil que representa o menor valor (xmin) em v\u00ed;\n\u2022\tPXmax como o percentil que representa o maior valor (xmax) em v\u00ed;\nl\u00ed poss\u00edvel mesclar ou desprezar pontos de corte se eles nao forem relevantes para a soluc\u00e3o. A relevancia dos pontos de corte e sua proximidade com os valores extremos (xmin e xmax) s\u00e3o expressos por um coeficiente de relev\u00e2ncia a (0 &lt;a &lt;1) definido por par\u00e2metro, que determina qu\u00e3ao perto o ponto de corte est\u00eda desses valores. A proximidade para os dois pontos de corte segue as seguintes equac\u00e3es:\nxmin\np-1(vale)\np 1(vale) >\nxmin\na\np 1(pico)\t___ -1 .\n---------- &lt;a =^ p (pico) &lt;Xmax * a xmax\ncomo vale &lt;pico, a seguinte inequa\u00e7\u00e3o \u00e9 v\u00e1lida:\n(5.1)\n(5.2)\nxmin\na\n&lt;xmax * a\n(5.3)\n&lt;a\nOu seja, para que exista um valor v\u00e1lido de a \u00e1 necess\u00e1rio satisfazer a inequa\u00e7\u00e3o:\n\n2\txmin\na2 > ----\nxmax\nxmin\nxmax\n(5.4)\na >\nl\u00ed necessario, portanto, aplicar uma corre\u00e7\u00e3o em a para assegurar que os pontos sempre possuam um intervalo de valores considerado relevante independente da proximidade de xmin e xmax. O valor ajustado do coeficiente, a', \u00e1 definido por:\na' = ((1 \u2014 S) \u2022 a) + S\n(5.5)\nonde S \u00e9 o coeficiente limite entre xmin e xmax, definido por:\n\nS=\nxmin\nxmax\n(5.6)\ncom essa defini\u00e7\u00e3o, \u00e9 poss\u00edvel inferir que o limite da Equa\u00e7\u00e3o 5.5 quando \u00f4 0 \u00e9:\nlim((1 \u2014 \u00f4) \u2022 a) + \u00f4 = a\t(5.7)\nOu seja, quando a dist\u00e2ncia entre xmin e xmax for muito grande (tender ao infinito), o valor de \u00f4 tende a zero e a' = a. A relev\u00e2ncia dos cortes, e portanto determinada pelo coeficiente ajustado a'. O menor valor relevante de vale \u00e9 dado por:\np-1(va/emi\u201e) = '\t(5.8)\na'\ne o maior valor relevante de pico \u00e9:\np (picomax)\txmax \u2022 a\t(5.9)\nAtrav\u00e9s das Equac\u00e3es 5.8 e 5.9 e considerando que ambos os pontos de corte possuam diferentes defini\u00e7\u00e3es, \u00e9 poss\u00e9vel definir a seguinte hierarquia:\npXmin &lt;vale &lt;y &lt;pico &lt;pXmax\t(5.10)\nonde y = valemin +representa o limite entre pico e vale.\nOs seguintes crit\u00e9erios s\u00e3ao usados para mesclar ou desprezar pontos de corte:\nvale+pico\n2\n>\ta', ent\u00e3o mescle por\n>\ta', ent\u00e3o despreze o corte de pico\n>\ta', entao despreze o corte de vale caso 3 entao mescle\ncaso\t1:\tse\ncaso\t2:\tse\ncaso\t3:\tse\ncaso\t4:\tse\np\u20141 (vale) p\u20141 (pico} p\u20141 (pico)\nxmax xmin p\u20141 (vale) caso 2 e\n\npor vale+pico por 2\n(5.11) A caracter\u00edstica da RB de representar o conhecimento de forma expl\u00e1cita cria uma preocupac\u00e3o quanto ao nome das classes em X *, que devem ser intuitivas e expressar suas propriedades. Dessa forma, os nomes das classes foram escolhidos levando em considera\u00e7\u00e3o a Equa\u00e7\u00e3o 5.11.\nO c\u00e1lculo da relev\u00f4ncia dos cortes pico e vale no m\u00e9todo DPV \u00e1 feito de forma param\u00e1etrica e \u00e1e necess\u00e1ario a defini\u00e7c\u00e3ao de alguns para\u00f4metros, como: a vari\u00e1avel a ser discretizada, o valor do coeficiente de relev\u00f4ancia a, e os dois pontos de corte respeitando a hierarquia da Equa\u00e7ao 5.5. Esses par\u00f4ametros s\u00e3ao definidos como entrada para o c\u00e1alculo.\nApos a definic\u00e7a\u00e3o dos para\u00eametros de entrada, e aplicada a corre\u00e7ca\u00e3o do valor alpha pela Equa\u00e7ao 5.5 e s\u00e3o calculados os valores de caso1, caso2 e caso3 atraves da Equa\u00e7\u00e3o 5.11. Os valores calculados deter-minar\u00e3ao a quantidade de classes para a discretiza\u00e7c\u00e3ao e o rotulo das mesmas.\nO fluxo geral do calculo da relev\u00eaancia dos cortes e sua respectiva discretiza\u00e7ao pelo metodo DPV e expresso no Algoritmo 3.\nAlgoritmo 3 Relev\u00f4ncia dos cortes de pico e vale e discretiza\u00e7ao\n1: v a variavel quantitativa a ser discretizada\n2: a algum coeficiente de relevancia a, (0 &lt;a &lt;1)\n3: vale algum percentil de acordo com a Equa\u00e7\u00e3o 5.10\n4: pico algum percentil de acordo com a Equac\u00e3o 5.10\n5: a' correcao do a (Equa\u00e7ao 5.5)\n6: casol\n<-\np 1(vale) p-1 (pico) p\u20141 (pico)\nxmax xmin p-1 (vale)\n> a' or ( caso2 > a'\n7: caso2\n8: caso3\n9: if casol\n10:\tdiscretize v usando \u201cbaixo\u201d e\n11: else if caso2 > a1 then\nand caso3 > a' ) then \u201calto\u201d (2 classes)\n\n12:\tdiscretize v usando \u201cbaixo\u201d e \u201cmedio\u201d (2 classes)\n13: else if caso3 > a' then\n14:\tdiscretize v usando \u201cmedio\u201d e \u201calto\u201d (2 classes)\n15: else\n16:\tdiscretize v usando \u201cbaixo\u201d, \u201cmedio\u201d e \u201calto\u201d (3 classes)\n17: end if\n18: return v discretizada (v*)\n5.1.1\tExemplo de Aplica\u00e7ao\nImagine uma situa\u00e7\u00e3o onde xmin = 10, xmax = 12 e a = 0.8. Caso fosse aplicado o coeficiente de relev\u00eaancia sem efetuar a corre\u00e7c\u00e3ao (a = a'), o menor valor poss\u00edvel para o vale ser considerado relevante, pela Equac\u00e7\u00e3ao 5.8, e:\np-1(valemin) = .^min = 10) = 12.5\t(5.12)\n(a' = a = 0.8)\nDe forma analoga, o maior valor poss\u00edvel para o pico ser consi-\nderado relevante, pela Equacao 5.9, \u00e9:\np \u00ed(picomax) = (xmax = 12) \u2022 (a' = a = 0.8) = 9.6\n(5.13)\nEsses valores geram uma contradic\u00e3o, pois nunca seria aceito como relevante nenhum corte de vale ou de pico.\nPara realizar a correc\u00e3o em a e necessario calcular o \u00f4 pela Equacao 5.6 e aplicar a corre\u00e7\u00e3o do coeficiente de relev\u00e2ncia pela Equa\u00e7\u00e3o 5.5:\n(5.14)\na' = ((1 - (\u00f4 = 0.8334)) \u2022 (a = 0.8)) + (\u00f4 = 0.8334) = 0.96668 (5.15)\nE por consequencia, os valores de p-1(valemin) e p-1 (picomax) s\u00e3ao alterados pelas Equac\u00e7\u00e3oes 5.8 e 5.9:\n(5.16)\np 1(picomax) = (xmax = 12) \u2022 (a' = 0.96668) = 11, 6001\t(5.17)\nApos definido os pontos de corte e necessario analisar a relev\u00e2ncia dos mesmos (Equa\u00e7c\u00e3ao 5.11).\nVamos supor que o algoritmo tenha definido os pontos de corte, e em uma variavel Vi tenha sido encontrado o valor de vale = 20. Ou seja, o corte de vale encontra-se no percentil 20. Vamos supor ainda, que por interpolac\u00e3o linear fosse encontrada p-1(vale) = 10.3. Da mesma forma para o corte de pico, imagine que pico = 98 e p-1(pico) = 11.9 Observe que nesse caso o corte de pico seria desprezado: o unico corte valido seria o de vale.\nCaso vale = 10, p-1(vale) = 10.15, pico = 80 e p-1(pico) = 11.5, seria desprezado o corte de vale.\nCaso os valores estejam pr\u00f3ximos, por exemplo,vale = 48, p-1(vale) = 10.98, pico = 52 e p-1(pico) = 11, nesse caso os dois cortes s\u00e3o unidos e e criado um novo corte pela Equac\u00e3o 5.11:\n(vale = 48) + (pico = 52)\n--------------------------= 50\n(5.18)\ne seu percentil estabelecido por interpola\u00e7ao linear. Nesse caso, corte = 50 e p-1(corte) = 10.99.\nSe o valor de vale for muito baixo e o de pico muito alto simultaneamente \u00e1e feito um novo corte de forma similar ao exemplo anterior. Nunca ocorrer\u00e1a um caso em que o corte de vale seja muito alto ou que o pico seja muito baixo, pois esses cortes obedecem a hierarquia estabelecida na Equa\u00e7cao 5.10.\n5.2 O PROBLEMA DE OTIMIZAC\u00c7AO\nOs seguintes conceitos sao definidos:\n\u2022\tvout como a vari\u00e1vel de sa\u00edda emV;\n\u2022\tV* = v*,... ,vn como o vetor de todas as vari\u00e1veis discretizadas em V: originalmente qualitativas ou discretizadas pelo DPV;\n\u2022\tv*out como a vari\u00e1vel de sa\u00edda em V*;\n\u2022\tX = x1,... ,xn comos os valores previstos de v0ut pela RB;\n\u2022\tX1 = xI,... ,xi como os valores previstos corretamente de vout pela RB;\n\u2022\tX0 = x0,... ,xn como os valores previstos incorretamente de v*out pela RB;\n\u2022\nn\nev(x)\tbeliefi \u2022 pontomedioi\t(5.19)\ni=1\ncomo uma fun\u00e7cao que retorna o valor quantitativo esperado de uma classe em vout, baseado nas probabilidades da rede (beliefs ) e em uma lista com os nu\u00e1meros reais que representam cada classe de vout. A lista de nu\u00e1meros reais \u00e1e criada atrav\u00e1es dos pontos m\u00e1edios de cada classe de vout comparados com vout.\nA discretizacao de uma variavel v no DPV depende dos pontos de corte pico e vale, al\u00e1em de um coeficiente de relev\u00eaancia pr\u00e1e-definido (a). Entretanto, a distribui\u00e7ao de probabilidade em v influencia o processo de infer\u00eaencia de toda a RB (Equa\u00e7cao 3.1).\nPortanto, \u00e9 necess\u00e1rio discretizar todas as vari\u00e1veis simultaneamente, o que gera um Problema de Otimiza\u00e7ao Global (HORST; RO-MEIJN, 2002), ou seja, encontrar o melhor conjunto de condicoes aceit\u00e1veis para atingir um objetivo formulado por termos matem\u00e1ticos.\nNesta dissertacao, a fun\u00e7ao objetivo consiste em discretizar todas as varia\u00e1veis do conjunto de dados, de forma que o erro de previsao da vari\u00e1vel de sa\u00e9da seja o menor poss\u00e9vel.\nAssumindo que vout pode tanto ser quantitativa quanto qualitativa, duas funcoes objetivo diferentes podem ser usadas. Se vout for qualitativa,\nencontre V* = maxacuracia(v*ut)\t(5.20)\nonde\nacuracia(v*ut) =\n|X0| + \u00a1X1!\n(5.21)\nPor\u00e9m, se vout for quantitativa, a fun\u00e7\u00e3o objetivo \u00e9 dada pela mi-nimiza\u00e7\u00e3o do erro NRMSE (normalized root mean square error), dado por\nencontre V* = minNRMSE(vout)\t(5.22)\nonde\nNRMSE(vout) =\n100\tn ZXi(x - ev(\u00a3i)')\u20182\nxmax\nxmin\n(5.23)\nO erro NRMSE \u00e9 calculado a partir do erro RMSE, que \u00e9 considerado uma boa medida de desempenho embora seja dependente de escala. A normaliza\u00e7cao do erro traz a vantagem de independente de escala e a poss\u00edvel compara\u00e7ao entre diferentes bases de dados (HYNDMAN; KOEHLER, 2006).\nA execu\u00e7ao do m\u00e9todo DPV segue o fluxo geral de execuc\u00e3o do Algoritmo Gen\u00e9tico (Figura 5). Por\u00e9m, \u00e9 necess\u00e1rio definir o valor de a (o mesmo para toda a execuc\u00e3o), as vari\u00e1veis V do dom\u00ednio e a vari\u00e1vel de sa\u00e9da vout.\nAp\u00e1s a defini\u00e7\u00e3o de par\u00e2metros, o algoritmo segue o fluxo do AG, com a cria\u00e7ao rand\u00e2mica de indiv\u00e9duos, a avalia\u00e7\u00e3o da popula\u00e7\u00e3o atraves da func\u00e3o fitness, a selec\u00e3o, o crossover e a mutac\u00e3o.\nPara cada indiv\u00edduo da popula\u00e7\u00e3o, \u00e9 feita a discretiza\u00e7\u00e3o de todas as vari\u00e9veis quantitativas (utilizando o Algoritmo 3), cria-se uma RB utilizando as vari\u00e9veis discretizadas e as qualitativas do tipo naive Bayes e calcula-se o valor de fitness seja pela acur\u00e9cia (vout qualitativa) ou pelo erro (vout quantitativa). Caso vout seja quantivativa, o DPV procura minimizar o fitness (erro) e caso vout seja qualitativa, o DPV procura maximizar o fitness (acur\u00e9rica).\nO resultado do m\u00e9todo \u00e9 aquele considerado o melhor indiv\u00edduo da execuc\u00e3o, ou seja, com o melhor fitness. Portanto s\u00e3o retornados os pontos de corte para cada vari\u00e9avel e a RB criada atrav\u00e9es desses pontos de corte.\nO algoritmo expresso em Algoritmo 4 mostra o fluxo de trabalho que satisfaz as fun\u00e7\u00e3es objetivo (Equa\u00e7\u00e3es (5.20) e (5.22)), utilizando a tecnica de Algoritmos Geneticos (AG).\nAlgoritmo 4 M\u00e9todo de Discretiza\u00e7\u00e3o pico e vale via AG\n1: a algum coeficiente de relevancia a, (0 &lt;a &lt;1)\n2: V vari\u00e9veis de algum dom\u00e9nio de aplicacao\n3: vout vari\u00e9vel de sa\u00e9da em V\n4: P = indi,..., indn\to vetor de indiv\u00e9duos randomicos contendo\nos cortes de pico e vale para cada vari\u00e9vel quantitativa em V (po-pulac\u00e7\u00e3ao)\n5: while n\u00e3o encontrou solu\u00e7ao do\n6:\tfor all indi in P do\n7:\tdiscretize todas as vari\u00e9veis quantitativas (Algoritmo 3)\n8:\tRBi uma RB com todas as vari\u00e9veis - qualitativas e quan-\ntitativas apos discretizac\u00e3o - topologia Naive Bayes\n9:\tif vout is qualitativa then\n10:\tfitnessi\tacuracia(vout) (Equ (5.20))\n11:\telse\n12:\tfitness^\tNRMSE(vout) (Equation (5.22))\n13:\tend if\n14:\tend for\n15:\tsele\u00e7c\u00e3ao()\n16:\tcrossover()\n17:\tmutac\u00e7\u00e3ao()\n18: end while\n19: return o melhor indi em P (aquele com o melhor fitness) e RBi (a RB criada por esse indiv\u00e9duo)\nA complexidade computacional do metodo DPV, assim como o fluxo geral do algoritmo, e semelhante a do Algoritmo Genetico classico, que, em uma populac\u00e3o de n indiv\u00edduos possui complexidade de O(nlog n) * O(fitness) para \u00e0 converg\u00eancia do algoritmo (GOLDBERG, 1989).\nA fun\u00e7\u00e3o fitness no DPV utiliza dois metodos fundamentais: a discretizac\u00e3o de todas as variaveis e a propria infer\u00eancia Bayesiana. A funcao de discretizacao possui a complexidade de O(k * m) onde k e a quantidade de variaveis cont\u00ednuas e m e a quantidade de registros em cada vari\u00e1vel. Portanto a complexidade geral do metodo DPV, e dada pela formula:\nO(nlog n) * [O(k * m) + O(inference)]\t(5.24)\nSendo que O(inference) depende do algoritmo de infer\u00eancia Bayesiana utilizado, que e considerado um problema do tipo NP-hard (COOPER, 1990). O algoritmo utilizado nesse trabalho foi implementado no shell Netica1 da Norsys Software Corp e utiliza t\u00e9cnicas do tipo \u201cjoin tree\u201d (SPIEGELHALTER et al., 1993).\n1Dispon\u00edvel em: http://wwww.norsys.com/netica.html\n60\n6\tRESULTADOS E DISCUSS\u00c3O\nPara avaliar a performance do DPV duas situa\u00e7c\u00e3oes foram testadas: quando a base de dados tem uma sa\u00edda qualitativa (Secao 6.1) e quando a variavel de sa\u00edda \u00ed quantitativa (Sec\u00e3o 6.2).\nQuando a vari\u00edvel de sa\u00edda \u00e9 qualitativa, o objetivo do algoritmo \u00ede realizar a classifica\u00e7c\u00e3ao da vari\u00edavel estimando a probabilidade de cada uma de suas classes. Portanto, a func\u00e7\u00e3ao objetivo do problema de otimizac\u00e7\u00e3ao est\u00eda em maximizar a acur\u00edacia (classifica\u00e7ca\u00e3o correta).\nQuando a vari\u00edavel de sa\u00edda \u00ede quantitativa o objetivo do algoritmo vai al\u00edem da classifica\u00e7c\u00e3ao: \u00ede necess\u00edario que a m\u00ededia estimada pelo vetor de probabilidade reflita o comportamento da variavel. Portanto, nesse caso, a fun\u00e7c\u00e3ao objetivo esta\u00ed relacionada com a minima\u00e7c\u00e3ao do erro (NRMSE) entre a m\u00ededia estimada e o valor real de cada registro.\nO DPV e um m\u00edtodo de Aprendizagem Supervisionada (MITCHELL, 1997) e os dados sao divididos em dois conjuntos: treinamento e teste.\nO coeficiente de relevancia a adotado neste trabalho foi de 0.8. Esse valor foi escolhido ap\u00edos uma busca por coeficientes melhores adaptados aos problemas apresentados.\nOs resultados obtidos foram comparados com dois m\u00edetodos de discretizac\u00e3o para Redes Bayesianas: EWD e EFD.\n6.1\tSA\u00cdDA QUALITATIVA - BASE DE DADOS\nPara representar os casos em que a vari\u00edavel de sa\u00edda \u00ede qualitativa, duas base de dados foram usadas: Iris Flower (Se\u00e7\u00e3o 6.1.1) e Wine (Se\u00e7\u00e3o 6.1.2) e foram separadas randomicamente de forma que (0.5n) dos dados pertencessem ao conjunto de treinamento e (0.5n) dos dados ao conjunto de teste.\nA varia\u00edvel de sa\u00edda em ambas as bases dados \u00ede chamada de class (tipo de flor iris ou de vinho).\n6.1.1\tO Problema Iris Flower\nPublicado por Fisher em 1936 (FISHER, 1936), a base de dados Iris Flower \u00ed uma das mais populares na literatura especializada em reconhecimento de padr\u00e3oes. Existem 150 registros nessa base, que foi\nadquirida atrav\u00e9s do reposit\u00f3rio p\u00fablico UCI.\nA base de dados possui tr\u00eas classes de 50 instancias cada, onde cada classe refere ao tipo de planta Iris: Iris-virginica, Iris-versicolor e Iris-setosa. Os par\u00eametros de entrada possuem valores quantitativos, chamados: sepal length (comprimento da sepala), sepal width (largura da sepala), petal length (comprimento da petala) e petal width (largura da petala).\nForam treinadas tr\u00eas Redes Bayesianas atraves do conjunto do treinamento com a topologia Naive Bayes. A distribui\u00e7\u00e3o dos dados entre os conjuntos de treinamento e teste foi feita de forma rand\u00eaomica e estratificada, ou seja, a quantidade de dados de cada classe e a mesma em ambos os conjuntos. Portanto, cada tipo de flor Iris possui 25 registros no conjunto de treinamento e 25 registros no conjunto de teste.\nA RBs treinadas da base Iris Flower podem ser vistas na Figura 9 (DPV), Figura 10 (EFD) e Figura 11 (EWD).\nFigura 9 - RB treinada pelo DPV para o Problema Iris Flower.\nplength\t\t\t\tclass\t\t\t\nbaixo\t34.5\t\t\tiris set\t33.3\ti\t\nmedio\t33.3\t\t\tiris versi\t33.3\t\t\nalto\t32 1\t\t\tiris virgi\t33.3\t\t\n\u00abwidth\t\t\nbaixo\t4Q.5\t\nmedio\t32.1\t\nalto\t27.4\t\n\np width\t\t\nbaixo\t34.5\t\nmedio\t33.3\t\nalto\t32.1\t\n\u00ablength\t\t\nbaixo\t35.7\t\nmedio\t35.7\t\nalto\t28.6\t\nFigura 10 - RB treinada pelo EFD para o Problema Iris Flower.\nplength\t\t\tclass\t\t\t\t\tpwidth\t\t\nbaixo\t3.57 medio\t13.1 alto\t83.3\t\u25a0: : :\t\tiris set\t33.3 iris versi\t33.3 iris virgi\t33.3\t\t\t\t\tbaixo\t8.33 medio\t21.4 alto\t7Q.2\t1 :\t\n\t\t\t\t\t\t\t\t\t\t\n\nswidth\t\t\nbaixo\t7.14\t\nmedio\t11.9\ti:\t:\t:\nalto\t81.0\t\nslength\t\t\nbaixo\t3.57\t\nmedio\t13.1\t\u25a0: : :\nalto\t83.3\t\nFigura 11 - RB treinada pelo EWD para o Problema Iris Flower.\n6.1.2\tO Problema Wine\nA base de dados Wine tambem \u00e9 muito popular na literatura de reconhecimento de padr\u00f5es. Ha 178 registros nessa base, tamb\u00e9m adquirida atrav\u00e9s do reposit\u00e9rio UCI.\nEssa base de dados possui tr\u00eas classes, onde cada uma se refere a um tipo de vinho cultivado na mesma regi\u00f5ao da It\u00e9alia, mas com diferentes caracter\u00edsticas: a classe 1 possui 59 registros, a classe 2 possui 71 registros e a classe 3 possui 48 registros.\nAs vari\u00e9veis de entrada possuem valores quantitativos, chamados: alcohol (\u00e9lcool), malic acid (\u00e9cido m\u00e9lico), ash (cinza), alkalinity of ash (alcalinidade das cinzas), magnesium (magn\u00e9sio), total phenols (fen\u00f3is totais), flavonoids (flavonoides), non-flavonoid phenols (fen\u00f3is n\u00f5o flavon\u00e9ides), pro-anthocyanins (pr\u00e9-antocianinas), color intensity (intensidade de cor), hue (tonalidade), OD280/OD315 of diluted wines (OD280/OD315 de vinhos dilu\u00e9dos) e proline (prolina).\nDe forma similar ao problema Iris Flower, foram treinadas tr\u00eaes Redes Bayesianas utilizando a topologia Naive Bayes. Embora a dis-tribui\u00e7\u00f5o de dados tenha sido feita de forma rand\u00eamica e estratificada, algumas classes de vinho possuem uma quantidade \u00e9mpar de registros. Portanto, a distribui\u00e7c\u00f5ao de dados adotada foi a seguinte:\n\u2022\twinel:\t30\tregistros\tno\ttreinamento,\t29\tregistros\tno\tteste\n\u2022\twine2:\t35\tregistros\tno\ttreinamento,\t36\tregistros\tno\tteste\n\u2022\twine3:\t24\tregistros\tno\ttreinamento,\t24\tregistros\tno\tteste\nA RBs treinadas da base Wine podem ser vistas na Figura 12 (DPV), Figura 13 (EFD) e Figura 14 (EWD).\ncolor\nod280\nalcohol\nmacid\nwinel wine2 wine3\nbaixo medio alto\nbaixo medio alto\nbaixo alto\nbaixo medio alto\nproliine\nmagnesium\nproant\t\nbaixo\t8.16 medio\t73.4 alto\t18.4\t\u2014\nnonflava\t\nbaixo\t29.6 medio\t34.8 alto\t35.6\t-\nflava\t\nbaixo 51.5 alto\t48.5\t\ntotal phe\t\nbaixo\t18.2 medio\t55.2 alto\t28 6\t\nFigura 12 - RB treinada pelo DPV para o Problema Wine.\ncolor\nod280\nalcohol\nmacid\nwinel wine2 wine3\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nproliine\nmagnesium\nproant\t\nbaixo\t33.5 M\nmedio\t34.8\t:\nalto\t31.7 \u25a0\u25a0\t:\nnonflava\t\nbaixo\t37.8\nmedio\t31.7 Hi : :\nalto\t30.5 \u25a0\u25a0\nflava\t\nbaixo\t33.4 M\nmedio\t32.9 M :\nalto\t33 8 \u25a0\u25a0\t:\ntotal phe\t\nbaixo\t33.5 \u25a0\u25a0\nmedio\t32.8 M :\nhigh\t33.8 \u25a0\u25a0\nFigura 13 - RB treinada pelo EFD para o Problema Wine.\ncolor\nod\u00ed\u00f1O\nalcohol\nmacid\nwine! wine2 wine3\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\nbaixo medio alto\n7 18 I ; ; ;\n9 24 i : : :\nproliine\nmagnesium\ntotalphe\nFigura 14 - RB treinada pelo EWD para o Problema Wine.\nproant\t\nbaixo\t4.09 medio\t3.06 alto\t92.9\t\nnontlava\t\nbaixo\t4.09 medio\t3.06 alto\t92.9\t\n\t\n6.1.3\tResultados e Compara\u00e7ao\nOs dados foram separados de forma estratificada e rand\u00eaomica nos conjuntos de treinamento e teste para ambas as bases que possuem variavel de sa\u00edda qualitativa. No conjunto de treinamento foram aplicados os metodos EWD, EFD e DPV para a gera\u00e7c\u00e3ao das Redes Baye-sianas. O conjunto de teste foi utilizado para confrontar a acuracia das redes ao se depararem com dados desconhecidos.\nA matriz de classifica\u00e7c\u00e3ao (confus\u00e3ao) para o Problema Iris Flower e mostrada na Tabela 3 e a matriz de classificac\u00e7\u00e3ao para o Problema Wine e mostrada na Tabela 4.\n6.1.4\tDiscuss\u00e3o\nDois exemplos de base de dados para variaveis de sa\u00edda qualitativas foram utilizados: Iris Flower e Wine. A base Iris Flower possui tr\u00eas classes de sa\u00edda, cada uma representando um tipo de flor (Iris-setosa, Iris-virginica e Iris-versicolor). Uma classe de sa\u00edda (Iris-setosa) e linearmente separavel das outras duas contribuindo para esse ser considerado um dom\u00ednio simples.\nTabela 3 - Matriz de classificac\u00e3o para o problema Iris Flower.\nM\u00f3todo\tReal\tPrevisto setosa versi. virgi.\t\t\tTotal\tAcur\u00f3cia\nTreinamento\t\t\t\t\t\t\nEWD\tsetosa\t21\t0\t4\t25\t65.33%\n\tversi.\t0\t6\t19\t25\t\n\tvirgi.\t0\t3\t22\t25\t\nEFD\tsetosa\t25\t0\t0\t25\t98.66%\n\tversi.\t0\t24\t1\t25\t\n\tvirgi.\t0\t2\t23\t25\t\nDPV\tsetosa\t25\t0\t0\t25\t98.66%\n\tversi.\t0\t24\t1\t25\t\n\tvirgi.\t0\t0\t25\t25\t\nTeste\t\t\t\t\t\t\nEWD\tsetosa\t17\t0\t8\t25\t62.66%\n\tversi.\t0\t7\t18\t25\t\n\tvirgi.\t0\t2\t23\t25\t\nEFD\tsetosa\t25\t0\t0\t25\t93.33%\n\tversi.\t0\t24\t1\t25\t\n\tvirgi.\t0\t4\t21\t25\t\nDPV\tsetosa\t25\t0\t0\t25\t96%\n\tversi.\t0\t24\t1\t25\t\n\tvirgi.\t0\t2\t23\t25\t\nA base Wine tamb\u00e9m \u00e9 considerada um dom\u00ednio simples e possui estruturas de classes \u201cbem-comportadas\u201d, sendo recomendada para m\u00e9todos novos de classificac\u00e3o principalmente por sua alta dimensionalidade atraves de 13 vari\u00e9veis de entrada.\nEssas caracter\u00edsticas em ambas as bases de dados asseguram uma alta acur\u00e9cia na classifica\u00e7ao. Nessas bases de dados observou-se uma acur\u00e9cia inferior na classificac\u00e3o quando o m\u00e9todo EWD foi usado e uma boa acur\u00e9cia tanto com o m\u00e9todo DPV quanto com o m\u00e9todo EFD.\nO problema Iris Flower em particular, mostra que os metodos\nTabela 4 - Matriz de classifica\u00e7\u00e3o para o problema Wine.\nMetodo\tReal\tPrevisto wine1 wine2 wine3\t\t\tTotal\tAcur\u00e1cia\nTreinamento\t\t\t\t\t\t\nEWD\twine1\t27\t0\t3\t30\t74.16%\n\twine2\t9\t20\t16\t35\t\n\twine3\t3\t2\t19\t24\t\nEFD\twine1\t30\t0\t0\t30\t98.88%\n\twine2\t0\t34\t1\t35\t\n\twine3\t0\t0\t24\t24\t\nDPV\twine1\t30\t1\t0\t30\t100%\n\twine2\t0\t35\t0\t35\t\n\twine3\t0\t0\t24\t24\t\nTeste\t\t\t\t\t\t\nEWD\twine1\t28\t0\t1\t29\t82.02%\n\twine2\t4\t27\t5\t36\t\n\twine3\t6\t0\t18\t24\t\nEFD\twine1\t28\t1\t0\t29\t94.38%\n\twine2\t1\t33\t2\t36\t\n\twine3\t0\t1\t23\t24\t\nDPV\twine1\t28\t1\t0\t29\t94.38%\n\twine2\t4\t32\t0\t36\t\n\twine3\t0\t0\t24\t24\t\nDPV e EFD classificaram sem erros o tipo de flor \u201csetosa\u201d (linearmente separ\u00e1vel) e obtiveram alguns erros ao classificar os tipos \u201cvirginica\u201d e \u201cversicolor\u201d. Entretanto, o metodo DPV possui uma melhor acuracia com o tipo \u201cvirginica\u201d, o que indica uma melhor separa\u00e7ca\u00e3o de classes com esse metodo.\nNas RBs treinadas para essa base de dados (Figuras 9, 10 e 11) e poss\u00edvel observar uma distribui\u00e7\u00e3o de probabilidade diferente em cada uma das redes. Na RB treinada pelo DPV n\u00e3ao ha um padr\u00e3ao de distribui\u00e7c\u00e3ao e ele varia de acordo com cada variavel, na RB treinada pelo EFD as distribui\u00e7c\u00e3oes tendem a ser iguais em todas as variaveis.\nE, na RB treinada pelo metodo EWD as vari\u00e1veis tendem para uma assim\u00e1etrica na distribuic\u00e7a\u00e3o de probabilidade, entretanto a distribui\u00e7ca\u00e3o \u00e1e muito semelhante em todas as vari\u00e1aveis.\nNo problema Wine, ambos os m\u00e1todos DPV e EFD obtiveram alguns erros ao classificarem o tipo Wine2. O metodo EFD tamb\u00e1m possui classificac\u00f5es incorretas em relac\u00e3o \u00e0 classe Wine3. Embora a acur\u00e1cia dos dois metodos seja a mesma, o metodo DPV separa as classes Wine2 e Wine3 com uma maior eficiencia que o m\u00e1todo EFD e n\u00e3ao h\u00e1a casos de classifica\u00e7ca\u00e3o errada de uma classe como outra.\nQuando observadas as RBs treinadas para a base Wine (Figuras 12, 13 e 14) tambem \u00e1 poss\u00e1vel notar diferentes distribui\u00e7\u00e3es de probabilidade em cada uma das redes. Na rede treinada pelo DPV cada vari\u00e1vel possui sua distribuic\u00e3o particular e duas vari\u00e1veis, proliine e ash, foram discretizadas em duas classes. A RB treinada pelo EFD possui uma distribui\u00e7ca\u00e3o de probabilidade com frequ\u00f4encias iguais. A rede treinada pelo m\u00e1etodo EWD mostrou um padr\u00e3ao em quase todas as vari\u00e1veis: mais de 75% dos dados foram considerados de uma classe espec\u00e1fica.\n6.2 SAr\u00f3A QUANTITATIVA - BASE DE DADOS\nPara representar o caso em que a vari\u00e1avel de sa\u00e1da \u00e1e quantitativa, foi utilizada uma base de dados de Taxa de Penetra\u00e7ao da broca (Rate of Penetration - ROP) em po\u00e7os de petr\u00e1leo sendo randomicamente separada de forma que (0.7n) dos registros pertencessem ao conjunto de treinamento e (0.3n) ao conjunto de teste. A variavel de sa\u00e1da \u00e1 chamada de \u201cROP\u201d.\n6.2.1\tProblema da Taxa de Penetra\u00e7ao da Broca (ROP)\nO problema da Taxa de Penetra\u00e7\u00e3o da Broca (ROP) \u00e9 caracter\u00edstico de ambientes com grande complexidade e risco que procuram otimizar o custo da perfura\u00e7c\u00e3ao de po\u00e7cos. A minimizac\u00e7\u00e3ao desses custos est\u00e1a diretamente relacionada com a maximiza\u00e7c\u00e3ao da ROP.\nPara reduzir os custos, \u00e1e necessa\u00e1rio planejar de forma correta as opera\u00e7co\u00e3es de perfura\u00e7ca\u00e3o. O tempo necessa\u00e1rio para perfurar um po\u00e7co tem que ser estimado com uma alta precisa\u00e3o, pois a maior parte dos custos sa\u00e3o associados ao aluguel dos equipamentos necess\u00e1arios para essa opera\u00e7\u00e3o (GANDELMAN, 2012).\nEntretanto, cada opera\u00e7\u00e3o possui propriedades \u00fanicas que tornam essa tarefa extremamente complexa. Muitas dessas propriedades variam durante a perfura\u00e7\u00e3o, como o tipo de rocha, a porosidade da rocha, a presenca de g\u00e1s, a pressao, a taxa de desgaste da broca, entre outras. Todas essas propriedades afetam a ROP, assim como outros muitos par\u00e2metros que s\u00e3o controlados pelo operador de broca.\nA base de dados utilizada possui 277 registros sobre um tipo especifico de broca de perfurac\u00e3o, usando o valor da ROP de forma quantitativa (m/s).\nAs varia\u00e1veis de entrada que possuem valores quantitativos, s\u00e3ao chamadas: RPM (Revolu\u00e7\u00e3es por Minuto), PSB (Peso sobre a Broca), HSI (Potencia Hidr\u00e1ulica por Polegada Quadrada) e metros acumulados. As tr\u00e2es primeiras varia\u00e1veis s\u00e3ao intr\u00e1nsecas ao processo de per-fura\u00e7ao e a ultima (metros acumulados) possui um comportamento acumulativo e linear trazendo informac\u00e7\u00e3ao sobre o desgaste da broca.\nExiste ainda uma vari\u00e1avel de entrada qualitativa, pr\u00e1e-discretizada por especialistas no dominio: UCS (Unconfined Compressive Strength - Resist\u00e2ncia \u00e0 Compressao Simples) relacionada \u00e0 geologia do solo.\nA RBs foram treinadas utilizando a topologia Naive Bayes e podem ser vistas na Figura 15 (DPV), Figura 16 (EFD) e Figura 17 (EWD).\nFigura 15 - RB treinada pelo DPV para o Problema da ROP.\nHSI\t\nbaixo\t4 93 medio\t75.3 alto\t19.6\t\nRPM\t\t\nbaixo\t78.5\t\nalto\t21.5\t\u25a0 !\nPSB\t\t\nbaixo\t117.3\t\u25a0 :\nmedio\t43.8\t\nalto\t39.0\t\nUCS\t\t\nmole media\t3.89\t\nmedia\t376\t\nmedio dura\t53.2\t\nduro\t5.36\t\u25a0\nmetro8 ac u mu lados\t\t\nbaixo\t14.2 medio\t58.1 alto\t27.7\tUM\t\n6.2.2\tResultados e Compara\u00e7ao\nForam criadas tr\u00eas Redes Bayesianas, cada uma relacionada \u00e0 um m\u00e9todo de discretiza\u00e7\u00e3o. As RBs criadas no caso de sa\u00edda quantitativa tem como objetivo a estimacao do valor m\u00e9dio da vari\u00e9vel. Portanto,\nFigura 16 - RB treinada pelo EFD para o Problema da ROP.\nFigura 17 - RB treinada pelo EWD para o Problema da ROP.\nHSI\t\t\nbaixo\t2.71\t\nmedio\t2.25\t\nalto\t95 0\t\n\t\t\nROP\t\t\t\t\nbaixo\t7.11 medio\t6.09 alto\t86.8\t\t\t\t\n9.26 \u00b13\t.2\t\t\t\nUCS\t\t\nmole media\t3.72\t\nmedia\t38.2\t\nmedia dura\t52.9\t\ndura\t5.22\t\nRPM\t\t\nbaixo\t32.4\t\nmedio\t49.9\t\nalto\t17.6\t\nPSB\t\t\nbaixo\t23.7\t\nmedio\t30.7\t\nalto\t45.6\t\nmetros acii mu lados\t\t\nbaixo\t62.0\t\nmedio\t36.6\t\nalto\t1.38\t\ncada classe da vari\u00e1vel de sa\u00edda possui pontos m\u00e1dios ligados \u00e0 ela (Tabela 5).\nCom os pontos medios de cada classe \u00e1 poss\u00e1vel estimar o valor da variavel de sa\u00e1da, pela Equa\u00e7\u00e3o 5.19. Um exemplo de entrada \u00e1 mostrado na Figura 18, utilizando a rede DPV. Para isso s\u00e3o instanciadas as evid\u00e2ncias de entrada (HSI = m\u00e1dio, RPM = baixo, PSB = alto, metros acumulados = medio e UCS = media). O valor estimado de sa\u00e1da nesse caso \u00e9 de 6.02 com desvio padr\u00e0o de 4.\nO conjunto de treinamento foi utilizado para a cria\u00e7c\u00e3ao das RBs e a estima\u00e7c\u00e3ao dos valores pela rede em compara\u00e7c\u00e3ao com os valores reais podem ser vistos nas Figura 19 (DPV), Figura 20 (EFD) e Figura 21 (EWD).\nA valida\u00e7c\u00e3ao das redes foi feita com o auxilio do conjunto de teste e os gr\u00e1aficos com os valores reais e valores estimados podem ser vistos nas Figura 22 (DPV), Figura 23 (EFD) e Figura 24 (EWD).\nPara avaliar o desempenho das mesmas, foi considerado o erro\nTabela 5 - Classes e Pontos M\u00e1dios para o problema ROP.\nM\u00e1etodo\tClasse\tPonto M\u00e1edio\nEWD\tbaixo\t0.645\n\tmedio\t1.32\n\talto\t10.52\nEFD\tbaixo\t1.247\n\tmedio\t3.106\n\talto\t11.71\nDPV\tbaixo\t1.48\n\tmedio\t4.27\n\talto\t12.68\nFigura 18 - Exemplo de entrada e estima\u00e7\u00e3o de valor para o problema ROP.\nentre o valor m\u00e1edio previsto e o valor real da varia\u00e1vel. O erro adotado nesse trabalho \u00e1 o NRMSE e o NRMSE(vout) obtido para cada abordagem pode ser visto na Tabela 6.\n6.2.3\tDiscuss\u00e3o\nPara ilustrar as situa\u00e7c\u00e3oes onde a vari\u00e1avel de sa\u00e1da \u00e1e quantitativa foi utilizada uma base de dados real sobre a ROP da broca em po\u00e7os de petr\u00f3leo. A ROP \u00e1 derivada do processo de perfura\u00e7ao sob a influencia\nFigura 19 - Valores estimados de ROP no metodo DPV (treinamento).\nFigura 20 - Valores estimados de ROP no metodo EFD (treinamento).\nTabela 6 - NRMSE obtido para o Problema da ROP.\nConjunto\tEWD\tEFD\tDPV\nTreinamento\t32.45%\t19.61%\t12.18%\nTeste\t51.39%\t46.51%\t16.09%\nde varios fatores, como: o equipamento dos operadores, a geologia e os sensores de medida. Portanto, os dados n\u00e3o s\u00e3o sempre confiaveis e a o dom\u00ednio de aplica\u00e7\u00e3o e considerado complexo.\nO comportamento das distribui\u00e7\u00e3es de probabilidade nas redes treinadas (Figuras 15, 16 e 17) foi semelhante ao comportamento das\nFigura 21 - Valores estimados de ROP no m\u00e9todo EWD (treinamento).\nFigura 22 - Valores estimados de ROP no metodo DPV (teste).\nbases Iris Flower e Wine. O metodo DPV obteve distribui\u00e7\u00f5es diferentes para cada variavel e a variavel RPM foi discretizada em duas classes, o metodo EFD obteve frequ\u00eancias aproximadamente iguais e o metodo EWD teve tend\u00eancia para a assimetria na maioria dos casos.\nNa Tabela 5 e mostrado os pontos medios de cada variavel e ha uma certa semelhan\u00e7a nos valores dos metodos DPV e EFD. Entretanto, a media dos dados e diferente devido ao vetor de probabilidades do nodo de sa\u00edda.\nPelas Figuras 15, 16 e 17 e poss\u00edvel observar que a media da ROP para essa distribuyo e de 4.32 com desvio padr\u00e2o de 3.64 na rede Bayesiana criada pelo metodo DPV, 5.37 com desvio padr\u00f5o de 4.57 na RB criada pelo metodo EFD e 9.26 com desvio padr\u00f5o de 3.23 na RB criada a partir do metodo EWD.\nFigura 23 - Valores estimados de ROP no m\u00e9todo EFD (teste).\nFigura 24 - Valores estimados de ROP no metodo EWD (teste).\nTanto pelos valores medios da rede quanto pela Tabela 5 nota-se uma maior similaridade entre os valores dos metodos EFD e DPV, de forma similar aos casos em que a variavel de sa\u00edda e qualitativa.\nEm rela\u00e7\u00e3o ao NRMSE na base da ROP, o metodo DPV mostra um erro inferior tanto no conjunto de treinamento quanto no conjunto de teste mostrando sua capacidade de generaliza\u00e7\u00e3o (Tabela 6). No conjunto de treinamento o metodo DPV possui um erro 37% inferior ao erro do metodo EFD e 62% inferior ao metodo EWD. Quando analisado o conjunto de teste, a diferen\u00e7a e ainda mais evidente: o DPV possui um erro 65% inferior ao metodo EFD e 68% inferior ao metodo EWD.\nNas Figuras 19, 20 e 21 sao mostrados os graficos dos valores estimados por cada um dos metodos no conjunto de treinamento. Ao\n75 observar a ader\u00eancia da curva de valores estimados a curva de valores reais, nota-se uma maior ader\u00eaencia do DPV, uma ader\u00eaencia m\u00ededia do m\u00edetodo EFD e uma curva superestimada no m\u00edetodo EWD.\nEssas diferenc\u00e7as s\u00e3ao mais ainda acentuadas nas Figuras 22, 23 e 24 que mostram o comportamento dos valores estimados para o conjunto de teste. Nota-se uma ader\u00eaencia maior do m\u00edetodo DPV e tanto o m\u00edetodo EFD quanto o m\u00edetodo EWD tendem a superestimar a curva. Esses resultados refor\u00e7cam a boa capacidade de generaliza\u00e7c\u00e3ao do DPV.\n76\n7\tCONSIDERACOES FINAIS\nEsse trabalho apresentou um metodo de discretiza\u00e7c\u00e3ao para Redes Bayesianas utilizando Algoritmo Genetico para a obten\u00e7\u00e3o de dois pontos de corte que identificam eventos de vale (\u201cbaixo\u201d), eventos de pico (\u201calto\u201d) e eventos intermediarios (\u201cmedio\u201d).\nO metodo foi aplicado em tr\u00eaes bases de dados diferentes: duas com sa\u00edda qualitativa tirados do reposit\u00f3rio UCI e uma com sa\u00edda quantitativa de um dom\u00ednio real com dados de perfura\u00e7\u00e3o de po\u00e7os de petr\u00edoleo.\nO m\u00edetodo DPV realiza a discretiza\u00e7c\u00e3ao com foco no desempenho geral da RB e n\u00e3ao apenas na distribuic\u00e7\u00e3ao de frequ\u00eaencia de cada vari\u00edavel. Ao observar as RBs geradas nota-se que a distribui\u00e7c\u00e3ao de probabilidade de cada classe (\u201cbaixo\u201d,\u201dm\u00ededio\u201d e \u201calto\u201d) pode tender para simetria ou para a assimetria, o que refor\u00e7ca a adequa\u00e7ca\u00e3o do m\u00edetodo para performance geral da rede. Ou seja, n\u00e3ao h\u00eda um padr\u00e3ao pr\u00ede-estabelecido de pontos de corte: os valores mudam de acordo com os dados.\nNos casos em que a vari\u00edvel de sa\u00edda \u00ed qualitativa, (classifica\u00e7\u00e3o) observou-se uma alta acura\u00edcia tanto no conjunto de treinamento quanto no conjunto de teste, mostrando uma adaptac\u00e7\u00e3ao do m\u00edetodo para esse tipo de problema. Tamb\u00edem observou-se que o m\u00edetodo DPV teve um desempenho superior aos outros m\u00edtodos utilizados (EFD e EWD) na separa\u00e7c\u00e3ao e delimita\u00e7c\u00e3ao de cada classe e um maior potencial de gene-raliza\u00e7c\u00e3ao.\nQuando a vari\u00edavel de sa\u00edda \u00ede quantitativa, o objetivo do m\u00edetodo n\u00e3ao \u00ede a classifica\u00e7c\u00e3ao da vari\u00edavel e sim diminuir o erro ao estimar o valor m\u00ededio. O comportamento da curva de valores estimados pelo DPV conseguiu ter uma boa ader\u00eancia a curva dos dados reais e n\u00e3o houve uma grande discrep\u00eaancia entre os erros dos dados de treinamento e dos dados de teste. Em ambos os conjuntos de dados o comportamento do m\u00edtodo DPV foi superior ao dos m\u00edtodos EFD e EWD.\nEmbora o m\u00edetodo DPV tenha obtido uma baixa taxa de erro na estima\u00e7c\u00e3ao dos valores m\u00ededios, ele na\u00e3o foi capaz de estimar com precisa\u00e3o valores extremos (muito altos ou muito baixos). Esse resultado era esperado pela caracter\u00edstica das Redes Bayesianas em fornecer um vetor com a distribui\u00e7c\u00e3ao de probabilidade da vari\u00edavel, e, nesse caso em estimar o valor m\u00ededio da distribui\u00e7c\u00e3ao.\nAo analisar os resultados, observa-se um bom comportamento do m\u00edetodo proposto tanto nos casos em que a vari\u00edavel de sa\u00edda \u00ede qualitativa quanto nos casos em que ela \u00ede quantitativa. A utiliza\u00e7c\u00e3ao do\nAlgoritmo Gen\u00e9tico para tratar o problema de otimiza\u00e7\u00e3o global se mostrou eficiente e atendeu aos requisitos do problema.\nApesar dos resultados promissores apresentados, esse trabalho possui algumas limita\u00e7c\u00e3oes, como a utiliza\u00e7ca\u00e3o de uma u\u00e9nica topologia de rede (Naive Bayes) e a falta de transic\u00e3o entre as classes (\u201cbaixo\u201d,\u201dmedio\u201d e \u201calto\u201d).\nOutro ponto relevante est\u00e1 na complexidade computacional do DPV: ela \u00e9 superior \u00e0 dos metodos EWD e EFD e est\u00e9 diretamente ligada \u00e0 quantidade de registros do dom\u00e9nio e de vari\u00e9veis, al\u00e9m da busca via AG e da infer\u00e2ncia Bayesiana (problema do tipo NP-hard). Essas caracter\u00e9sticas podem tornar o m\u00e9etodo proibitivo em alguns dom\u00e9nios de aplica\u00e7c\u00e3ao.\nConsiderando as limitac\u00e3es apresentadas e tendo como objetivo a melhoria do m\u00e9etodo proposto, \u00e9e poss\u00e9vel citar os seguintes trabalhos futuros:\n\u2022\tA utiliza\u00e7ao de t\u00e9cnicas h\u00e9bridas para a suavizar a transi\u00e7\u00e3o entre as classes, como por exemplo a t\u00e9cnica Fuzzy-Bayes (BRIGNOLI, 2013) ou a adi\u00e7\u00e3o de mais pontos de corte no algoritmo;\n\u2022\tA utiliza\u00e7\u00e3o de outras topologias de rede al\u00e9m da Naive Bayes apresentada e a combina\u00e7\u00e3o do m\u00e9todo a um m\u00e9todo de aprendizagem de estrutura de rede;\n\u2022\tEstudo de desempenho do uso do Algoritmo Gen\u00e9etico no m\u00e9etodo e a sua efic\u00e9acia quando comparado com outros algoritmos de busca;\n\u2022\tAplica\u00e7c\u00e3ao do m\u00e9etodo em bases com grande quantidade de dados.\n\u2022\tEstudo aprofundado da complexidade do m\u00e9etodo DPV.\nREFER\u00caNCIAS\nBEASLEY, D.; MARTIN, R.; BULL, D. An overview of genetic algorithms: Part 1. fundamentals. University computing, WHURR PUBLISHERS, v. 15, p. 58-58, 1993.\nBRIGNOLI, J. T. Um Modelo para Suporte ao Raciocinio Diagn\u00f3stico diante da Din\u00e2mica do Conhecimento sobre Incertezas. Tese (Doutorado) \u2014 Universidade Federal de Santa Catarina, 2013.\nCANO, A.; MORAL, S.; SALMERON, A. Penniless propagation in join trees. International Journal of Intelligent Systems, Wiley Online Library, v. 15, n. 11, p. 1027-1059, 2000.\nCANTU-PAZ, E. A summary of research on parallel genetic algorithms. 1995.\nCATLETT, J. On changing continuous attributes into ordered discrete attributes. In: SPRINGER. Machine learning-EWSL-91. [S.l.], 1991. p. 164-178.\nCOOPER, G. F. The computational complexity of probabilistic inference using bayesian belief networks. Artificial intelligence, Elsevier, v. 42, n. 2, p. 393-405, 1990.\nDOUGHERTY, J.; KOHAVI, R.; SAHAMI, M. Supervised and unsupervised discretization of continuous features. In: MORGAN KAUFMANN PUBLISHERS, INC. MACHINE LEARNINGINTERNATIONAL WORKSHOP THEN CONFERENCE-. [S.l.], 1995. p. 194-202.\nFAYYAD, U.; IRANI, K. Multi-interval discretization of continuous-valued attributes for classification learning. 1993.\nFISHER, R. A. The use of multiple measurements in taxonomic problems. Annals of eugenics, Wiley Online Library, v. 7, n. 2, p. 179-188, 1936.\nFRANK, E.; WITTEN, I. H. Making better use of global discretization. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1999.\nFRIEDMAN, N.; GEIGER, D.; GOLDSZMIDT, M. Bayesian network classifiers. Machine learning, Springer, v. 29, n. 2-3, p. 131-163, 1997.\nFRIEDMAN, N.; GOLDSZMIDT, M. Discretizing continuous attributes while learning bayesian networks. In: MORGAN KAUFMANN PUBLISHERS, INC. MACHINE LEARNINGINTERNATIONAL WORKSHOP THEN CONFERENCE-. [S.l.], 1996. p. 157-165.\nFUNG, R. M.; CHANG, K.-C. Weighing and integrating evidence for stochastic simulation in bayesian networks. In: NORTH-HOLLAND PUBLISHING CO. Proceedings of the Fifth Annual Conference on Uncertainty in Artificial Intelligence. [S.l.], 1990. p. 209-220.\nGANDELMAN, R. A. ROP prediction and real-time optimization of operational parameters on drilling offshore oil Wells. Disserta\u00e7\u00e3o (Mestrado) \u2014 Federal University of Rio de Janeiro, 2012.\nGOLDBERG, D. E. Genetic algorithms in search, optimization, and machine learning. Addison-Wesley Professional, 1989.\nHORST, R.; ROMEIJN, H. E. Handbook of global optimization. [S.l.]: Kluwer Academic Pub, 2002.\nHSU, C.-N.; HUANG, H.-J.; WONG, T.-T. Why discretization works for na ve bayesian classifiers. In: Proceedings of the Seventeenth International Conference on Machine Learning, Morgan Kaufmann, San Francisco, CA. [S.l.: s.n.], 2000. p. 399-406.\nHSU, C.-N.; HUANG, H.-J.; WONG, T.-T. Implications of the dirichlet assumption for discretization of continuous variables in naive bayesian classifiers. Machine Learning, Springer, v. 53, n. 3, p. 235-263, 2003.\nHYNDMAN, R. J.; KOEHLER, A. B. Another look at measures of forecast accuracy. International Journal of Forecasting, v. 22, n. 4, p. 679 - 688, 2006. ISSN 0169-2070. Dispon\u00edvel em:&lt;http://www.sciencedirect.com/science/article/pii/S0169207006000239>.\nJANIKOW, C. Z.; MICHALEWICZ, Z. An experimental comparison of binary and floating point representations in genetic algorithms. In: SAN DIEGO, CA. Proceedings of the fourth international conference on genetic algorithms. [S.l.], 1991. v. 31, p. 36.\nJENSEN, F. V.; LAURITZEN, S. L.; OLESEN, K. G. Bayesian updating in causal probabilistic networks by local computations.\nComputational statistics quarterly, v. 4, p. 269-282, 1990.\nKERBER, R. Chimerge: Discretization of numeric attributes. In: AAAI PRESS. Proceedings of the tenth national conference on Artificial intelligence. [S.l.], 1992. p. 123-128.\nKURTCEPHE, M.; GUVENIR, H. A. A discretization method based on maximizing the area under receiver operating characteristic curve. International Journal of Pattern Recognition and Artificial Intelligence, World Scientific, v. 27, n. 01, 2013.\nLANGSETH, H. et al. Inference in hybrid bayesian networks. Reliability Engineering &amp; System Safety, Elsevier, v. 94, n. 10, p. 1499-1509, 2009.\nLIU, H. et al. Discretization: An enabling technique. Data mining and knowledge discovery, Springer, v. 6, n. 4, p. 393-423, 2002.\nMADSEN, A. L.; JENSEN, F. V. Lazy propagation: a junction tree inference algorithm based on lazy evaluation. Artificial Intelligence, Elsevier, v. 113, n. 1, p. 203-245, 1999.\nMATSUURA, J. P. Discretiza\u00e7\u00e3o para Aprendizagem Bayesiana: Aplica\u00e7\u00e3o no Aux\u00edlio \u00e0 ValidaQ\u00e3o de Dados em ProteQ\u00e3o ao Voo. Tese (Doutorado) \u2014 Disserta\u00e7\u00e3o de Mestrado, Instituto Tecnol\u00f3gico de Aeronautica, S\u00e3o Jose dos Campos, 2003.\nMCCALLUM, A.; NIGAM, K. et al. A comparison of event models for naive bayes text classification. In: AAAI-98 workshop on learning for text categorization. [S.l.: s.n.], 1998. v. 752, p. 41-48.\nMITCHELL, T. M. Machine learning. 1997. Burr Ridge, IL: McGraw Hill, v. 45, 1997.\nMORAL, S.; RUMI, R.; SALMERON, A. Mixtures of truncated exponentials in hybrid bayesian networks. In: Symbolic and Quantitative Approaches to Reasoning with Uncertainty. [S.l.]: Springer, 2001. p. 156-167.\nPEARL, J. Probabilistic Reasoning in Intelligent Systems: Networks of Plausble Inference. [S.l.]: Morgan Kaufmann Pub, 1988.\nPREPARATA, F. P.; SHAMOS, M. I. Computational geometry: An introduction (monographs in computer science). Monographs in Computer Science (Springer-Verlag, New York, 1985), ISBN 35)0961313, 1993.\nROUSU, J. Efficient range partitioning in classification learning. In: Department of Computer Science, University of Helsinki. [S.l.: s.n.], 2001.\nSALMERON, A.; CANO, A.; MORAL, S. Importance sampling in bayesian networks using probability trees. Computational Statistics &amp; Data Analysis, Elsevier, v. 34, n. 4, p. 387-413, 2000.\nSHENOY, P. P.; SHAFER, G. Axioms for probability and belief-function propagation. In: Classic Works of the Dempster-S\u2019hafer Theory of Belief Functions. [S.l.]: Springer, 2008. p. 499-528.\nSPIEGELHALTER, D. J. et al. Bayesian analysis in expert systems. Statistical science, Institute of Mathematical Statistics, v. 8, n. 3, p. 219-247, 1993.\nWEILE, D. S.; MICHIELSSEN, E. Genetic algorithm optimization applied to electromagnetics: A review. Antennas and Propagation, IEEE Transactions on, IEEE, v. 45, n. 3, p. 343-353, 1997.\nWHITLEY, D. A genetic algorithm tutorial. Statistics and computing, Springer, v. 4, n. 2, p. 65-85, 1994.\nWONG, T.-T. A hybrid discretization method for naive bayesian classifiers. Pattern Recognition, Elsevier, v. 45, n. 6, p. 2321-2325, 2012.\nWRIGHT, A. H. et al. Genetic algorithms for real parameter optimization. Foundations of genetic algorithms, v. 1, p. 205-218, 1991.\nYANG, Y. Discretization for Naive-Bayes Learning. [S.l.]: Monash University, 2003.\nYANG, Y.; WEBB, G. I. Discretization for naive-bayes learning: managing discretization bias and variance. Machine learning, Springer, v. 74, n. 1, p. 39-74, 2009."}]}}}