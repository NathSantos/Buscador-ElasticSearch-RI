{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.21268"}, {"@name": "filename", "#text": "4985_337626.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE SANTA CATARINA\n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA MEC\u00c2NICA\n\nEderson Augusto Grein\n\nA PARALLEL COMPUTING APPROACH APPLIED TO PETROLEUM\n\nRESERVOIR SIMULATION\n\nFlorian\u00f3polis\n2015\n\n\n\n\n\nEderson Augusto Grein\n\nA PARALLEL COMPUTING APPROACH APPLIED TO PETROLEUM\n\nRESERVOIR SIMULATION\n\nDisserta\u00e7\u00e3o submetida ao Programa de P\u00f3s-\n\nGradua\u00e7\u00e3o em Engenharia Mec\u00e2nica da\n\nUniversidade Federal de Santa Catarina para\n\na obten\u00e7\u00e3o do grau de Mestre em Engenharia\n\nMec\u00e2nica.\n\nOrientador: Clovis Raimundo Maliska, Ph.D.\n\nCoorientador: Kamy Sepehrnoori, Ph.D.\n\nFlorian\u00f3polis\n2015\n\n\n\n\n\n\n\nCataloga\u00e7\u00e3o na fonte elaborada pela biblioteca da\n\nUniversidade Federal de Santa Catarina\n\nGrein, Ederson Augusto, 1989 -\n\nA Parallel Computing Approach Applied to Petroleum Reservoir Simula-\n\ntion / Ederson Augusto Grein - 2015.\n148 f. : il. color. ;\n\nOrientador: Clovis Raimundo Maliska, PhD.\n\nCoorientador: Kamy Sepehrnoori, PhD.\n\nDisserta\u00e7\u00e3o (mestrado) - Universidade Federal de Santa Catarina, Curso\n\nde Engenharia Mec\u00e2nica, 2015.\n\n1. Simula\u00e7\u00e3o num\u00e9rica de reservat\u00f3rios de petr\u00f3leo. 2. Computa\u00e7\u00e3o\n\nParalela. 3. UTCHEM. 4. Element-based Finite Volume Method. I. Maliska,\n\nClovis Raimundo. II. Universidade Federal de Santa Catarina. Curso de\n\nEngenharia Mec\u00e2nica\n\n\n\n\u00c0 Taisa, por todo\ncarinho e apoio.\n\n\n\n\n\nAcknowledgements\n\nI would like to express my gratitude to professor Clovis Maliska. Working\nunder his supervision at the SINMEC laboratory was fundamental for me\nto acquire the necessary experience required to develop this study. I am\nalso thankful to him for making the necessary arrangements for me to\ndevelop part of this study at the University of Texas at Austin.\n\nI am truly thankful to professor Kamy Sepehrnoori for accepting me\nas a collaborator in one of the most recognized centers of Petroleum Engi-\nneering in the world. Furthermore, he supervised my work closely, schedul-\ning weekly meetings to know my developments and giving appropriated\nfeedbacks, which helped me reaching my aims. With him and Mojtaba\nGhasemi Doroh I have learned the basic concepts of parallel computing\napplied to reservoir simulation. I also would like to express my gratitude\nto professor Sepehrnoori and to Center of Petroleum &amp; Geosystems En-\ngineering (CPGE) for the financial support during the period I was in the\nUnited States.\n\nI am also grateful to Tatiane Schveitzer and Axel Dihlmann for con-\ntinuously helping me and giving support to the execution of this study.\nMy gratitude also goes to the SINMEC laboratory, to the Programa de\nForma\u00e7\u00e3o de Recursos from the Ag\u00eancia Nacional do Petr\u00f3leo, and to\nPOSMEC for providing the necessary financial support and infrastructure\nfor this study.\n\nI am heartily thankful to Taisa Pacheco for giving me support and\nfor helping me to review this text. My gratitude also goes to Henrique\nPacheco, who helped me with the code and with the execution of some\ntests, and to my colleagues Giovani Cerbato, Victor Magri, Gustavo Ribeiro,\nHamid Lashgari, and Aboulghasem Nia, for all the discussions about this\n\n\n\nstudy and the support they gave me.\nAnd of course I am deeply thankful to my beloved parents Aliomar\n\nGrein and Marlene Joseli Moreira Grein. Without their support and guid-\nance it would be impossible to fulfil my dreams.\n\n\n\n\"What we observe is not nature itself, but nature\nexposed to our method of questioning.\"\n\nWerner Heisenberg\n\n\n\n\n\nContents\n\nList of Figures v\n\nList of Tables ix\n\nList of Symbols xi\n\nResumo xiii\n\nAbstract xv\n\n1 Introduction 1\n1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n\n1.2 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n\n1.3 Organization of the study . . . . . . . . . . . . . . . . . . . . . . . . . 4\n\n2 Parallel Computing 5\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n\n2.2 Algorithm classification . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n2.2.1 Serial algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n\n2.2.2 Parallel algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 8\n\n2.2.3 Serial-parallel algorithms . . . . . . . . . . . . . . . . . . . . 8\n\n2.2.4 Nonserial-parallel algorithms . . . . . . . . . . . . . . . . . 8\n\n2.2.5 Regular iterative algorithms . . . . . . . . . . . . . . . . . . 8\n\n2.3 Parallel architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n\n2.3.1 Shared-memory architecture . . . . . . . . . . . . . . . . . 9\n\n2.3.2 Distributed-memory architecture . . . . . . . . . . . . . . 11\n\n2.3.3 Hybrid-memory architecture . . . . . . . . . . . . . . . . . 11\n\ni\n\n\n\n2.4 Application Programming Interfaces . . . . . . . . . . . . . . . . . 12\n\n2.4.1 OpenMP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n\n2.4.2 MPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n\n2.5 Parallel efficiency and theoretical limits . . . . . . . . . . . . . . . 14\n\n3 UTCHEM Reservoir Simulator 19\n\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n3.2 Mathematical formulation . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n3.3 Grids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n\n4 The Element-based Finite Volume Method 25\n\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n\n4.2 Grid entities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n\n4.3 Numerical scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n4.4 Discretization of a conservation equation . . . . . . . . . . . . . 30\n\n5 The Proposed Approach 33\n\n5.1 UTCHEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n\n5.1.1 Domain decomposition . . . . . . . . . . . . . . . . . . . . . 34\n\n5.1.2 Inactive grid blocks . . . . . . . . . . . . . . . . . . . . . . . . 36\n\n5.1.3 IPARS framework . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.2 EFVLib . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n5.2.1 Graph partitioning . . . . . . . . . . . . . . . . . . . . . . . . . 41\n\n5.2.2 Ghost nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n\n5.2.3 Assembling in parallel a system of linear equations . 46\n\n5.2.4 Solving in parallel a system of linear equations . . . . 50\n\n5.2.5 Wells and boundaries . . . . . . . . . . . . . . . . . . . . . . . 54\n\n5.2.6 The code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n\n6 Experimental Environment and Results 61\n\n6.1 Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n\n6.2 Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n\n6.3 Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n\n6.4 Case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n\n6.5 Case 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n\n6.6 Case 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n\nii\n\n\n\n7 Conclusions 101\n7.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n7.2 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n7.3 Suggestions for future studies . . . . . . . . . . . . . . . . . . . . . . 104\n\nBibliography 107\n\nAppendix A UTCHEM\u2019s Input File 111\n\nAppendix B Example of code using EFVLib in parallel 115\n\niii\n\n\n\n\n\nList of Figures\n\n1.1 Frequency in MHz of several processors along the years\n[22] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n\n2.1 Common architecture of sequential computers (adapted\nfrom [23]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n\n2.2 Ilustration of a shared memory parallel computer. . . . . . 10\n\n2.3 Ilustration of a shared memory parallel computer. . . . . . 11\n\n2.4 Ilustration of a shared memory parallel computer. . . . . . 12\n\n2.5 Theoretical speedup according to Amdahl\u2019s Law. . . . . . . 16\n\n2.6 Theoretical speedup according to Gustafson-Barsis\u2019s Law. 17\n\n4.1 Discretization of an hypothetical reservoir using a un-\nstructured grid (adapted from [17]) . . . . . . . . . . . . . . . . 26\n\n4.2 Main entities of a triangular element . . . . . . . . . . . . . . . 27\n\n4.3 Control volume creation . . . . . . . . . . . . . . . . . . . . . . . . 28\n\n4.4 Mapping into a transformed space . . . . . . . . . . . . . . . . . 28\n\n4.5 Stencil of p . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n\n5.1 Example of a grid division . . . . . . . . . . . . . . . . . . . . . . . 35\n\n5.2 Example of ghost cells from a 6x8x3 grid . . . . . . . . . . . . . 36\n\n5.3 Loop modification when type 2 arrays are presented . . . . 36\n\n5.4 Example of inactive grid blocks been used to better de-\nscribe a reservoir domain . . . . . . . . . . . . . . . . . . . . . . . 37\n\n5.5 Organization of UTCHEMP . . . . . . . . . . . . . . . . . . . . . . 38\n\n5.6 Simulator workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n\n5.7 Graph in (b) is the graph of grid (a) . . . . . . . . . . . . . . . . 41\n\nv\n\n\n\n5.8 Unstructured grid divided into two subdomains . . . . . . . 45\n\n5.9 Subdomains extended . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n\n5.10 Local indexes of two subdomains . . . . . . . . . . . . . . . . . . 46\n\n5.11 PETSc indexes of two subdomains . . . . . . . . . . . . . . . . . 49\n\n5.12 Matrix assembled in parallel . . . . . . . . . . . . . . . . . . . . . 50\n\n5.13 Boundaries and well . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\n5.14 Boundaries and well . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n\n6.1 Water saturation after 800 simulation days of case 1 . . . . 63\n\n6.2 Average aqueous phase pressure of case 1 . . . . . . . . . . . 64\n\n6.3 Average water saturation of case 1 . . . . . . . . . . . . . . . . . 64\n\n6.4 Overall oil production rate of case 1 . . . . . . . . . . . . . . . . 65\n\n6.5 Total computational time according to the number of pro-\ncessors of case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n6.6 Global speedup of case 1 . . . . . . . . . . . . . . . . . . . . . . . . 66\n\n6.7 Contribution on the total computational time of case 1 . . 68\n\n6.8 Temperature after 510 simulation days of case 2 . . . . . . . 69\n\n6.9 Average aqueous phase pressure of case 2 . . . . . . . . . . . 70\n\n6.10 Average water saturation of case 2 . . . . . . . . . . . . . . . . . 70\n\n6.11 Overall oil production rate of case 2 . . . . . . . . . . . . . . . . 71\n\n6.12 Computational time according to the number of proces-\nsors of case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n\n6.13 Global speedup of case 2 . . . . . . . . . . . . . . . . . . . . . . . . 72\n\n6.14 Contribution on the total computational time of case 2 . . 73\n\n6.15 Case 3 aqueous phase saturation after 600 simulation days 74\n\n6.16 Average aqueous phase pressure of case 3 . . . . . . . . . . . 75\n\n6.17 Average water saturation of case 3 . . . . . . . . . . . . . . . . . 75\n\n6.18 Overall oil production rate of case 3 . . . . . . . . . . . . . . . . 76\n\n6.19 Computational time according to the number of proces-\nsors of case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n\n6.20 Global speedup of case 3 . . . . . . . . . . . . . . . . . . . . . . . . 77\n\n6.21 Contribution on the total computational time of case 3 . . 78\n\n6.22 Microemulsion phase saturation after 350 simulation days\nof case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n\n6.23 Average aqueous phase pressure of case 4 . . . . . . . . . . . 80\n\n6.24 Average water saturation of case 4 . . . . . . . . . . . . . . . . . 80\n\n6.25 Overall oil production rate of case 4 . . . . . . . . . . . . . . . . 81\n\nvi\n\n\n\n6.26 Computational time according to the number of proces-\nsors of case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n\n6.27 Global speedup of case 4 . . . . . . . . . . . . . . . . . . . . . . . . 82\n6.28 Contribution on the total computational time of case 4 . . 83\n6.29 Microemulsion phase saturation after 355 simulation days\n\nusing inactive cells of case 4 . . . . . . . . . . . . . . . . . . . . . . 84\n6.30 Illustration of the grid and wells of case 5 . . . . . . . . . . . . 85\n6.31 Saturation field after 170 days of case 5 . . . . . . . . . . . . . 86\n6.32 Grid division of case 5 using different number of processors 87\n6.33 Average pressure of case 5 . . . . . . . . . . . . . . . . . . . . . . . 88\n6.34 Average water saturation of case 5 . . . . . . . . . . . . . . . . . 89\n6.35 Overall oil production rate of case 5 . . . . . . . . . . . . . . . . 89\n6.36 Computational time according to the number of proces-\n\nsors of case 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n6.37 Global speedup of case 5 . . . . . . . . . . . . . . . . . . . . . . . . 90\n6.38 Contribution on the total computational time of case 5 . . 91\n6.39 Pressure computation speedup of case 5 . . . . . . . . . . . . 91\n6.40 Water saturation computation speedup of case 5 . . . . . . 92\n6.41 Computational time of some grid operations of case 5 . . 92\n6.42 Maximum ratio between the surplus number of nodes\n\nand the ideal number of nodes of case 5 . . . . . . . . . . . . . 93\n6.43 Average ratio between the number of ghost nodes and\n\nthe number of local nodes of case 5 . . . . . . . . . . . . . . . . 94\n6.44 Grid and wells of case 6 . . . . . . . . . . . . . . . . . . . . . . . . . 95\n6.45 Computational time according to the number of proces-\n\nsors of case 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n6.46 Global speedup of case 6 . . . . . . . . . . . . . . . . . . . . . . . . 96\n6.47 Contribution on the total computational time of case 6 . . 97\n6.48 Pressure computation speedup of case 6 . . . . . . . . . . . . 98\n6.49 Water saturation computation speedup of case 6 . . . . . . 98\n6.50 Computational time of some grid operations of case 6 . . 99\n6.51 Maximum ratio between the surplus number of nodes\n\nand the ideal number of nodes of case 6 . . . . . . . . . . . . . 100\n6.52 Average ratio between the number of ghost nodes and\n\nthe number local nodes of case 6 . . . . . . . . . . . . . . . . . . 100\n\nvii\n\n\n\n\n\nList of Tables\n\n6.1 Case 1 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n6.2 Computational times in seconds of case 1 . . . . . . . . . . . . . 67\n6.3 Case 2 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6.4 Computational times in seconds of case 2 . . . . . . . . . . . . . 72\n6.5 Case 3 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n6.6 Computational times in seconds of case 3 . . . . . . . . . . . . . 77\n6.7 Case 4 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n6.8 Computational times in seconds of case 4 . . . . . . . . . . . . . 83\n6.9 Case 5 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n6.10 Case 6 properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n\nix\n\n\n\n\n\nList of Symbols\n\n(?,?,?) Local coordinates\n\n(x , y , z) Global coordinates\n\nQ? Production/injection rate\n\n? Diffusivity coefficient\n\n? Specific gravity\n\n? Mobility\n\nK Medium absolute permeability\n\nu Fluid velocity\n\nN Shape function\n\nWI Well index\n\n\u00b5 Viscosity\n\n?i Grid element\n\n? Porosity\n\n? Well pressure\n\n? density\n\nC? Overall concentration\n\n? General function defined in the domain of interest\n\nxi\n\n\n\nxii LIST OF TABLES\n\n~D Dispersive flux\n\n~u Superficial velocity\n\nC Concentration\n\nC o Compressibility\n\nCr Rock compressibility\n\nCt Total compressibility\n\nE Parallel efficiency\n\nh Vertical depth\n\nJ Jacobian matrix\n\nK Preconditioner matrix\n\nk Relative permeability\n\nnp Number of phases\n\nnv c Number of volume-occupying phases\n\nP Pressure\n\nQ Source term\n\nr Reaction rate\n\nS Speedup\n\nSl Saturation of phase l\n\nT Computational time\n\n\n\nResumo\n\nA simula\u00e7\u00e3o num\u00e9rica \u00e9 uma ferramenta de extrema import\u00e2ncia \u00e0 in-\nd\u00fastria do petr\u00f3leo e g\u00e1s. A partir dela, podem-se prever os cen\u00e1rios de\nprodu\u00e7\u00e3o de um dado reservat\u00f3rio de petr\u00f3leo e, com base nos dados\nobtidos, tra\u00e7ar melhores estrat\u00e9gias de explota\u00e7\u00e3o. Entretanto, para que\nos resultados advindos da simula\u00e7\u00e3o sejam fidedignos, \u00e9 fundamental o\nemprego de modelos f\u00edsicos fi\u00e9is e de uma boa caracteriza\u00e7\u00e3o geom\u00e9trica\ndo reservat\u00f3rio. Isso tende a introduzir elevada carga computacional e,\nconsequentemente, a obten\u00e7\u00e3o da solu\u00e7\u00e3o do modelo num\u00e9rico corre-\nspondente pode demandar um excessivo tempo de simula\u00e7\u00e3o. \u00c9 evidente\nque a redu\u00e7\u00e3o desse tempo interessa profundamente \u00e0 engenharia de\nreservat\u00f3rios. Dentre as t\u00e9cnicas de melhoria de performance, uma das\nmais promissoras \u00e9 a aplica\u00e7\u00e3o da computa\u00e7\u00e3o paralela. Nessa t\u00e9cnica,\na carga computacional \u00e9 dividida entre diversos processadores. Ideal-\nmente, a car-ga computacional \u00e9 dividida de maneira igualit\u00e1ria e, assim,\nse N \u00e9 o n\u00fame-ro de processadores, o tempo computacional seria N vezes\nmenor. No presente estudo, a computa\u00e7\u00e3o paralela foi aplicada ao sim-\nulador de reservat\u00f3rios UTCHEM e \u00e0 biblioteca EFVLib. UTCHEM \u00e9 um\nsimulador qu\u00edmi-co-composicional desenvolvido pela The University of\nTexas at Austin. A EFVLib, por sua vez, \u00e9 uma biblioteca desenvolvida\npelo laborat\u00f3rio SINMEC \u2013 laborat\u00f3rio ligado ao Departamento de En-\ngenharia Mec\u00e2nica da Universidade Federal de Santa Catarina \u2013 cujo intu-\nito \u00e9 prover suporte \u00e0 aplica\u00e7\u00e3o do M\u00e9todo dos Volumes Finitos Baseado\nem Elementos. Em ambos os casos a metodologia de paralaleliza\u00e7\u00e3o \u00e9\nbaseada na decomposi\u00e7\u00e3o de dom\u00ednio.\n\nxiii\n\n\n\n\n\nAbstract\n\nNumerical simulation is an extremely relevant tool to the oil and gas in-\ndustry. It makes feasible the procedure of predicting the production sce-\nnery in a given reservoir and design more advantageous exploit strategies\nfrom its results. However, in order to obtain reliability from the numerical\nresults, it is essential to employ reliable numerical models and an accu-\nrate geometrical characterization of the reservoir. This leads to a high\ncomputational load and consequently the achievement of the solution of\nthe corresponding numerical method may require an exceedingly large\nsimulation time. Seemingly, reducing this time is an accomplishment\nof great interest to the reservoir engineering. Among the techniques of\nboosting performance, parallel computing is one of the most promising\nones. In this technique, the computational load is split throughout the set\nof processors. In the most ideal situation, this computational load is split\nin an egalitarian way, in such a way that if N is the number of processors\nthen the computational time is N times smaller. In this study, parallel\ncomputing was applied to two distinct numerical simulators: UTCHEM\nand EFVLib. UTCHEM is a compositional reservoir simulator developed\nat The University of Texas at Austin. EFVLib, by its turn, is a computational\nlibrary developed at SINMEC \u2013 a laboratory at the Mechanical Enginering\nDepartment of The Federal University of Santa Catarina \u2013 with the aim of\nsupporting the Element-based Finite Volume Method employment. The\nparallelization process were based on the domain decomposition on the\nboth cases formerly described.\n\nxv\n\n\n\n\n\nCHAPTER\n\n1\nIntroduction\n\n1.1 Preliminaries\n\nLarge-scale reservoir simulations may demand high computational per-\nformance and a huge amount of computer memory in order to be feasible\nits use in reservoir engineering. A lot of effort has been made to improve\nthe computation performance. Better linear system solvers, multiscale\nmethods and other technics has proven to be very important for such\nimprovement. However, when a code is optimized, it becomes an arduous\njob to achieve a great time reduction by simply enhancing the algorithms.\nThus, it is quite relevant to consider the hardware architecture in order to\nmake reservoir simulators faster.\n\nOne of the main parameters that interferes on the hardware perfor-\nmance is the CPU frequency. As the frequency increases, more operations\ncan be performed and consequently the algorithm runs faster. Figure 1.1\nshows the frequency of several processors along the years. It is clear that\nthe current processor architecture is saturating and hence no significant\nperformance gain may happen unless a radical architecture change is in-\ntroduced. Because of this, computer manufacturers adopted the strat-\negy of using several simple processors instead of a single and probably\n\n1\n\n\n\n2 INTRODUCTION\n\ncomplex one. Each processor works only on a part of the problem and\nthe overall computational performance is given by the time consumed\nby the most loaded processor.Employing a large number of processors\ndiminishes the problem size on which each processor works. Ideally, if\nN processors are used, the code will run N times faster.\n\nFigure 1.1 \u2013 Frequency in MHz of several processors along the years [22]\n\nParallel computer\u2019s relevant features go beyond having more than\none processor, forasmuch as the amount of available memory is usually\nmuch bigger. CPU clusters own a memory module for each node that is\ncontained by them. Consequently, a greater amount of memory becomes\navailable if a larger number of nodes is employed. This is of great interest\nsince a substantial number of simulation cases is unfeasible not due to a\nrunning time constraint, but because their models are so complex that it\nis not even possible to load them.\n\nOne should note, however, that using a parallel computer usually is\nnot enough to take the advantages of parallel computing. In fact, a code\noriginally conceived for serial computers may run even slower in a par-\nallel computer since only a single \u2013 and probably simple \u2013 processor is\nallocated. It is also necessary to program what operation each processor\nshould execute. Furthermore, an egalitarian division of the operations\noptimizes the code performance. Otherwise, the overloaded processor\ndictates the overall performance.\n\n\n\nINTRODUCTION 3\n\nThe main purpose of this study is the parallelization of two distinct\nsimulation codes: UTCHEM and EFVLib. The University of Texas Chem-\nical Compositional Simulator (UTCHEM) is a three-dimensional, multi-\ncomponent, multiphase, compositional, variable temperature, finite-dif-\nference reservoir simulator developed at The University of Texas at Austin.\nIt can be used to simulate the enhanced recovery of oil and the enhanced\nremediation of aquifers. Some of its features are modeling of capillary\npressures, three-phase relative permeabilities, dispersion, diffusion, ad-\nsorption, chemical reactions, and non-equilibrium mass transfer between\nphases [2]. UTCHEM is used worldwide and its parallelization will directly\nbenefit those who intend to run large and realistic reservoir simulation\ncases with this simulator.\n\nThe second simulation code intended to be parallelized is a library\n(EFVLib) in which the Element-based Finite Volume Method (EbFVM)\n[17] is implemented. It was developed at SINMEC, a CFD laboratory from\nThe Federal University of Santa Catarina, at Florian\u00f3polis. This library\nsupports two and three-dimensional hybrid unstructured grids. In 2D,\nthe grid is composed by triangles and quadrangles and in 3D by hexahe-\ndrons, tetrahedrons, prisms, and pyramids. EFVLib handles grid\u2019s oper-\nations, geometry, and topology in a user-friendly way. Also, it provides a\nconvenient environment to develop numerical methods in fluid mechan-\nics and heat transfer.\n\nThe motivation for parallelizing two softwares instead of just one is\nthat different aspect of the parallel computing will be contemplated. UT-\nCHEM is a huge code with a lot of physical models, but the grid is struc-\ntured, which simplifies the parallelization. EFVLib, on the other hand, is\na much smaller code and does not require the validation against several\nphysical cases, since those cases are implemented by the final user. The\ngrid however is unstructured and the calculation of fluxes is much more\ncomplex.\n\n1.2 Objectives\n\nThe main objectives of this study are\n\n\u2022 Parallelize UTCHEM;\n\n\u2022 Parallelize EFVLib.\n\n\n\n4 INTRODUCTION\n\nThese objectives may be split into the following secondary objectives:\n\n\u2022 Define an efficient methodology to divide the computational load\namong the available processing units;\n\n\u2022 Search and make usable external resources that may help to execute\nin parallel some of the required simulation steps;\n\n\u2022 Since Linux is the usual operation system of cluster of CPU\u2019s and\nEFVLib was implemented in Windows, adapt EFVLib to a Linux plat-\nform;\n\n\u2022 Validate both UTCHEM and EFVLib parallel versions against bench-\nmark cases;\n\n\u2022 Evaluate the performance gains achieved using parallel processing.\n\n1.3 Organization of the study\n\nIn chapter 2 has some basic concepts about parallel computing, including\nsoftware and hardware matters. Chapter 3 introduces UT-CHEM\u2019s main\nfeatures and its mathematical formulation. In chapter 4 the Element-\nbased Finite Volume Method (EbFVM) is discussed. In chapter 5 it is briefly\ndescribed the domain decomposition methodology used to distribute the\ncomputational load among processors. It is also described the inactive\ngrid blocks methodology and the new UTCHEM\u2019s input file format. Be-\nsides, it also gives some highlights of the EFVLib code itself and presents\nsome other details. Chapter 6 contains the results achieved and its anal-\nysis.\n\n\n\nCHAPTER\n\n2\nParallel Computing\n\n2.1 Introduction\n\nIt is quite hard to rigorously define what is parallel computing. Such area\nincludes aspects related to algorithms, applications, programming langua-\nges, operating system, and computer architecture. All of these aspects\nmust be specialized in order to provide support for computations that\ninvolve more than one processor [21]. The motivation to use more than\none processor comes from the fact that nowadays it is worthless trying to\nimprove computer performance adopting a single processor. Such pro-\ncessor would consume too much power and a very sophisticated refrig-\neration system would be required to dissipate the heat generated. It is\nmuch more practical to exploit several simple processors to attain the\nsame desired performance [10]. In fact, as the trend of parallel computers\nspreads, launching a single processor in these newer computers makes\nserial algorithms run even slower than they would in a serial machine.\n\nSeemingly, the main motivation of parallel computing is to speed up\ncomputations. With the nowadays computer technologies, it is quite easy\nto acquire and store huge amounts of data. However, process that data\nis something that, if performed by a single processor, would require a\n\n5\n\n\n\n6 PARALLEL COMPUTING\n\nprohibitively long time [23]. In this context, the basic idea of parallel com-\nputing is to split the job among several processors. Each one of them\nworks concurrently only in a small segment of the problem. Besides, they\ncommunicate between themselves in order to achieve the expected result.\n\nThe natural way to analyze a parallel algorithm is comparing it to\nthe best approach that solves the same problem in a sequential scenario.\nSingle-processor computers usually adopt the architecture illustrated in\nFigure 2.1. Throughout its calculations, only a single sequence of instruc-\ntions and data can be processed at a time by its sole process unit. The\ncontrol unit, in its turn, is aware of the operation that must be executed\nand by this knowledge it sets the operands. Furthermore, the control\nunit recognizes the available variables, although it cannot inform its val-\nues. Hence, to enable a complete loading of the desired data into the\nprocessor\u2019s internal registers the memory access creates a path between\nthe memory and the processor. Such design is commonly referred to as\nRandom Access Machine [23].\n\nProcess\nUnit\n\nInput\nUnit\n\nOutput\nUnit\n\nMemory\n\nControl\n\nInstructions\n\nData\n\nFigure 2.1 \u2013 Common architecture of sequential computers (adapted\nfrom [23])\n\nWhen more than one processor is addressed to execute a job, the\nway it might be performed may differ greatly from the traditional concept\nof sequential computing. From the algorithm standpoint, the problem\nis divided into subproblems, each one solved concurrently by a single\n\n\n\nPARALLEL COMPUTING 7\n\nprocessor. Those processors might communicate between themselves in\norder to yield the final result. However, there are important architecture\nissues that might interfere in the way that the program is parallelized,\nsuch as the arrangement of memory and processors, the possibility of\nexecuting different tasks in distinct processors, the processors\u2019 commu-\nnication system, and the processors\u2019 operation (synchronously or asyn-\nchronously) [23]. The following two sections are intended to discuss the\nalgorithms\u2019 task dependency and parallel architectures, respectively. In\nparticular, labelling the algorithms according to its task dependencies is\nquite convenient, once that will be useful to recognize which parts of\nUTCHEM and EFVLib may be efficiently parallelized.\n\n2.2 Algorithm classi?cation\n\nThe algorithm classification adopted here \u2013 the same as in [10] \u2013 is based\non task dependencies. According to this classification, there are five types\nof algorithms:\n\n1. Serial algorithms;\n\n2. Parallel algorithms;\n\n3. Serial-parallel algorithms;\n\n4. Nonserial-parallel algorithms;\n\n5. Regular iterative algorithms.\n\nThe following subsections expose briefly the types mentioned above.\n\n2.2.1 Serial algorithms\n\nSerial algorithms are those that require a sequential execution of their\ntasks. Due to the data dependency, launching a new task depends on\nfinalization of the previous one. Hence, a synchronously running is not\npossible and thus no gain is obtained by exploiting several processor uni-\nties.\n\n\n\n8 PARALLEL COMPUTING\n\n2.2.2 Parallel algorithms\n\nOpposed to the previous type, parallel algorithms have tasks that share\nno data dependency between each other. Thus the tasks are independent\nat such degree that they can be executed concurrently by several proces-\nsor unities. Moreover, the overall performance of parallel algorithms are\nlimited by the overloaded processor.\n\n2.2.3 Serial-parallel algorithms\n\nSerial-parallel algorithms may have their tasks grouped in stages. Each\nstage can have its tasks executed in parallel, but the stages themselves\nmust be executed sequentially. It is clear that if there is only one stage,\nthen the algorithm is parallel. On the other hand, if each stage has a single\ntask, the algorithm is serial.\n\n2.2.4 Nonserial-parallel algorithms\n\nA nonserial-parallel algorithm (NSPA) cannot be put in the above classes\nbecause its workflow follows no pattern at all. According to [10], an NSPA\ngraph is characterized by two types of constructs: nodes, which corre-\nsponds to the algorithm tasks, and directed edges, which describes the\ndirection of the data flow among the nodes. The graph provides important\ninformation, such as the work \u2013 amount of work to complete the algo-\nrithm \u2013, the depth \u2013 maximum path length between any input node and\nany output node \u2013, and the degree of parallelism \u2013 maximum number of\nnodes that can be processed in parallel.\n\n2.2.5 Regular iterative algorithms\n\nRegular iterative algorithms are designed by a fixed pattern. Identifying\nthis pattern can be an arduous job, though. Such algorithms are the most\ndifficult to be parallelized, but they deserve special attention due to their\nample presence in fields such signal, image and video processing, linear\nalgebra, and numerical simulation.\n\n\n\nPARALLEL COMPUTING 9\n\n2.3 Parallel architectures\n\nThe approach used to make an algorithm parallel is strictly related to the\nmultiprocessing architecture. It is crucial to choose a processor architec-\nture that is sufficiently qualified to perform the algorithm\u2019s instructions\nassuring reliability. Furthermore, the processors must communicate be-\ntween themselves using some kind of interconnection network that, if not\nfast enough, might be the bottleneck for the software performance. Thus,\nif the interconnection network is known to have poor quality, reducing\ndata exchange between processors is a pivotal aim for the algorithm de-\nsign. Besides the processor architecture, the memory system may also\nbe taken into account. The memory modules might be shared among\nthe processors so that all of them are able to access the global data. On\nthe other hand, it is also possible to dedicate a memory module to each\nprocessor. In this case, each processor has only a part of the global data\nand as a consequence data needs to be communicated between memory\nmodules via an interconnection network. If some data is updated in a\nprocessor unit, all of the other ones must be informed somehow in order\nto have the correct values [10].\n\nRegarding multiprocessing architectures, this study will focus in three\nof the main types:\n\n\u2022 Shared-memory;\n\n\u2022 Distributed-memory;\n\n\u2022 Hybrid-memory.\n\nThe above classification relies on how the memory resources are available\nto the processors. In the following sections the types above will be briefly\ndescribed.\n\n2.3.1 Shared-memory architecture\n\nIn a shared-memory architecture, all processors share a common mem-\nory address space and implicitly communicate between themselves via\nmemory. Usually they have local caches and are interconnected not only\nwith each other, but also with the common memory through an inter-\nconnection (a bus, for example) [21]. In general such architectures are\n\n\n\n10 PARALLEL COMPUTING\n\nsymmetric, which means that all processing units are equal, nevertheless\nthey also might be asymmetric.\n\nIn Figure 2.2 a symmetric shared-memory architecture is schema-\ntized. Despite the fact that in that figure all processors are connected\nto a single memory module, there might be several memory modules.\nEmploying a bank of memories increases the overall computational per-\nformance since only one processor may access a given memory at a given\ntime. That is, a bank of memories allows the simultaneously execution of\nseveral read/write memory operations [10].\n\nFigure 2.2 \u2013 Ilustration of a shared memory parallel computer.\n\nAccording to [10], programming for shared-memory multiprocessors\nis not difficult, since all memory read operations are hidden from the\nprogrammer and hence can be coded as a serial code. Memory write\noperations, on the other hand, might require locking the data access until\na thread has finished the work on it. It is necessary to identify the criti-\ncal code sections and synchronize the processors in order to assure data\nintegrity. Libraries based on OpenMP directives \u2013 discussed later \u2013 are\ncommonly used to handle synchronization and other related operations.\n\nThe main disadvantage regarding shared-memory architectures is\ntheir incapability of scaling to a large number of processors [7]. Gener-\nally the bus-based systems are limited to at most 32 processors, while\nthose based on crossbar switch can achieve as many as 128 processors.\nHowever, in the latter case the switch cost increases with the square of\nthe number of processors. If a computer with a exceedingly large num-\nber of processors were to be built, this incremental cost would make its\nconstruction unfeasible.\n\n\n\nPARALLEL COMPUTING 11\n\n2.3.2 Distributed-memory architecture\n\nScalability limitations of shared memory systems led the development of\ndistributed-memory parallel computers. Instead of a huge global mem-\nory, each processor owned by that machine is connected to a smaller local\nmemory, so that the memory access can be done faster than a shared-\nmemory computer would be able to perform it [7]. This structure is also\nclassified as non-uniform \u2013 Non-uniform Memory Access (NUMA) \u2013,\nsince it depends on which memory a given processor attempts to access\n[10].\n\nDistributed-memory computers resort to an interconnection\nnetwork in order to provide an adequate communication among the pro-\ncessors, as illustrated in Figure 2.3. Data are sent from a processor mem-\nory module to another via a message passing (MP) mechanism. The Mas-\nsage Passing Interface (MPI) may be used as a language-independent\nmessage protocol [7]. Aiming the improvement of the overall computa-\ntional performance, data should be carefully placed in the memory mod-\nules in order to lessen the number of messages exchanged between the\nprocessors themselves.\n\nFigure 2.3 \u2013 Ilustration of a shared memory parallel computer.\n\n2.3.3 Hybrid-memory architecture\n\nHybrid-memory architecture combines paradigms from shared and dis-\ntributed memory. In massively parallel processing this architecture is of-\n\n\n\n12 PARALLEL COMPUTING\n\nten termed SMP cluster. Its structure is similar to a distributed-memory\u2018s\narrangement, although in the hybrid case each node is a shared-memory\nsystem. This configuration takes advantage of both memory architec-\ntures, since it allows not only high parallel efficiency within a node but\nalso scaling the program to a large number of processors.\n\nFigure 2.4 \u2013 Ilustration of a shared memory parallel computer.\n\n2.4 Application Programming Interfaces\n\nAn Application Programming Interface (API) is a set of routines, proto-\ncols, and tools that assist the development of software applications. For\nparallel softwares, the most common APIs are the Open Multi Processing\n(OpenMP) and the Message Passing Interface (MPI) [8]. The former is\nused in shared-memory parallel computers; the latter, in both shared and\ndistruted-memory machines. Besides, those APIs can be used together in\nhybrid memory computers, as discussed in the previous section. In the\nfollowing subsections each one of them explained.\n\n2.4.1 OpenMP\n\nOpenMP is an API employed in shared-memory computers. It is com-\nposed by a set of compiler directives and environmental variables. Ad-\nditionally, it also includes a runtime library, which may be used to imple-\nment the desired parallelism. OpenMP is suitable to parallelize sequential\nprograms implemented in C, C++ or Fortran [21].\n\n\n\nPARALLEL COMPUTING 13\n\nOpenMP is considered a high level API. This means that the program-\nmer does not need to worry about too many technical details, such as data\ndecomposition and flow control, which are left to the compiler. The pro-\ngrammer only needs to use OpenMP directives to indicate what portions\nof the code must be executed in parallel. If a compiler does not support\nOpenMP, then the directives will be interpreted as comments and thus\nignored. Hence, the application is simultaneously sequential and parallel.\n\nOpenMP does not require a whole parallelized code. That is, it allows\nan incrementally parallelization. One may progressively parallelize a se-\nquential code until the desired performance is achieved. It is also worth\nnoting that the collection of OpenMP directives is relatively small, which\nmeans that the programmer does not need to learn a whole new language\nin order to use them [21].\n\n2.4.2 MPI\n\nThe Message Passing Interface (MPI) is proposed as a standard specifi-\ncation for message-passing. Nowadays MPI is the leading programming\nlanguage employed in highly scalable programs [21]. It treats communi-\ncations among processors explicitly and for this reason may be used in\nboth shared and distributed-memory parallel computers.\n\nIn the message-passing model, the processors communicate between\neach other by sending messages through a two-sided operation: a pro-\ncessor must send a message, while another one needs to recieve it. A MPI\nmessage consist of two parts: the envelope and the message body [20].\nThe envolpe has four parts:\n\n\u2022 Source: the processor that sends the message;\n\n\u2022 Destination: the processor that receives the message;\n\n\u2022 Communicator: the group of processors to which both source and\ndestination processors belong;\n\n\u2022 Tag: marker used to distinguish between different message types.\n\nThe message body, by its turn, is composed of the three following parts:\n\n\u2022 Buffer: the data to be sent;\n\n\u2022 Datatype: type of the message data;\n\n\n\n14 PARALLEL COMPUTING\n\n\u2022 Count: number of items in the buffer.\n\nAlmost all message-passing operations can be done using send/recieve\noperations (point-to-point operations). However, some operations that\ninvolve all processors. This kind of operation is common to such a de-\ngree that MPI provides routines (collective communication routines) to\nexecute them. Some of these routines are [20]:\n\n\u2022 Barrier synchronization;\n\n\u2022 Broadcast from one processor to all the other ones;\n\n\u2022 Global reduction operations, such as max, min, and sum;\n\n\u2022 Gather data from all processor to a single one;\n\n\u2022 Scatter data from a single processor to all the other ones.\n\n2.5 Parallel e?ciency and theoretical limits\n\nOne of the most common metric to measure the benefits of parallel com-\nputing is the speedup. The speedup is simply defined as the ratio of the\ntimes required to run a program with a single and multiple parallel pro-\ncessors. If T1 and TN denotes the time consumed by one and by N pro-\ncessors, respectively, then the speedup achieved is\n\nSN =\nT1\nTN\n\n. (2.1)\n\nIdeally, if the code is fully parallelized, there is no overloaded processor.\nFurthermore, in the ideal case the communication time between proces-\nsors and memory are negligible, the speedup is linear [10], which means\n\nSN = N . (2.2)\n\nIn general, however, the speedup is sub-linear, since the conditions above\nrarely are respected. There are situations, on the other hand, in which the\nspeedup is greater than the linear speedup (super-linear speedup). This\nmight happen, for example, if searching operations are important or due\nto hardware issues [8].\n\n\n\nPARALLEL COMPUTING 15\n\nAnother common metric used for parallel computing is the parallel\nefficiency. It is defined as the speedup divided by the number of proces-\nsors [21]:\n\nEN =\nSN\nN\n\n. (2.3)\n\nThe parallel efficiency may be used to measure the scalability of a pro-\ngram. Given a chosen level of efficiency and a number of processors, it\napprises the size of the problem to be solved.\n\nThere are theoretical limits for the speedup. According to Amdahl\u2019s\nLaw, an algorithm is composed by a parallizable fraction f and by a serial\nfraction 1? f . Considering that the execution of the parallizable fraction\nis N times faster when N processors are used, the time needed for the\ncode execution is\n\nTN = f\nT1\nN\n+(1? f )T1. (2.4)\n\nThe theoretical speedup is thus\n\nSN =\nT1\nTN\n=\n\n1\n\nf /N +(1? f )\n(2.5)\n\nand the maximum speedup according to Amdahl\u2019s Law is\n\nSmax = lim\nN ??\n\nSN =\n1\n\n1? f\n. (2.6)\n\nFigure 2.5 displays some speedup\u2019s curves and its behavior due to varia-\ntions in the number of processors and parallelizable fractions.\n\nAnother popular law for the theoretical speedup is the Gustafson-\nBarsis\u2019s. It states that the parallelism increases as the problem size in-\ncreases [10]. This law differs from the Amdahl\u2019s Law, in which the parallel\nfraction is constant. In the Gustafson-Barsis\u2019s formula, the time of execu-\ntion in parallel is taken as the reference. The time for execution in serial\nis thus\n\nT1 =(1? f )TN + f N TN , (2.7)\n\nwhich gives the theoretical speedup\n\nSN = 1+ f (N ?1). (2.8)\n\nFigure 2.6 exhibits the theoretical speedup for the same parallel fractions\n\n\n\n16 PARALLEL COMPUTING\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\n\nf = 50%\nf = 90%\nf = 99%\nf = 99.9%\n\nFigure 2.5 \u2013 Theoretical speedup according to Amdahl\u2019s Law.\n\nused in Figure 2.5. It is important to point out that the Gustafson-Barsis\u2019s\nLaw is much less pessimistic than Amdahl\u2019s.\n\n\n\nPARALLEL COMPUTING 17\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\n\nf = 50%\nf = 90%\nf = 99%\nf = 99.9%\n\nFigure 2.6 \u2013 Theoretical speedup according to Gustafson-Barsis\u2019s Law.\n\n\n\n\n\nCHAPTER\n\n3\nUTCHEM Reservoir Simulator\n\n3.1 Introduction\n\nUTCHEM is a three-dimensional, multicomponent, multiphase, compo-\nsitional, variable temperature, finite-difference reservoir simulator devel-\noped at The University of Texas at Austin. It can be used to simulate en-\nhanced recovery of oil and enhanced remediation of aquifers. Some of\nits features are the modeling of capillary pressures, three-phase relative\npermeabilities, dispersion, diffusion, adsorption, chemical reactions, and\nnon-equilibrium mass transfer between phases [2]. Moreover, for ground\nwater its features are:\n\n\u2022 NAPL spill and migration in both saturated and unsaturated zones;\n\n\u2022 Partitioning interwell test in both saturated and unsaturated zones\nof aquifers;\n\n\u2022 Remediation using surfactant/cosolvent/polymer;\n\n\u2022 Remediation using surfactant/foam;\n\n\u2022 Remediation using cosolvents;\n19\n\n\n\n20 UTCHEM RESERVOIR SIMUL ATOR\n\n\u2022 Bioremediation;\n\n\u2022 Geochemical reactions.\n\nFor oil reservoirs:\n\n\u2022 Waterflooding\n\n\u2022 Single well, partitioning interwell, and single well wettability tracer\ntests;\n\n\u2022 Polymer flooding;\n\n\u2022 Profile control using gel;\n\n\u2022 Surfactant flooding;\n\n\u2022 High pH alkaline flooding;\n\n\u2022 Microbial EOR;\n\n\u2022 Surfactant/foam and ASP/foam EOR.\n\nIn UTCHEM, the mass and energy equations are solved for an arbi-\ntrary number of chemical components. There may be up to four fluid\nphases \u2013 air, water, oil, and microemulsion \u2013 besides an arbitrary number\nof solid minerals. The transport equations are discretized using a finite\ndifference scheme and are solved via an IMPEC method, which solves\nthe aqueous phase pressure equation implicitly and the concentration\nequations explicitly. In the following section the UTCHEM mathematical\nmodel will be briefly described following the guidelines presented in [12].\n\n3.2 Mathematical formulation\n\nThe main equations to be solved are the aqueous phase pressure, the\nconcentration, and the energy equations. Moreover, the aqueous phase\npressure equation is the only one solved implicitly. It is obtained by sum-\nming all the concentration equations of the volume-occupying compo-\nnents. The pressure of the other phases is computed adding the capillary\npressures between the other phases. The knowledge of the pressure field\nis a sufficient requirement in order to solve all the other equations.\n\n\n\nUTCHEM RESERVOIR SIMUL ATOR 21\n\nThe mass equation of a component ? in a porous media is given by\n\n?\n\n? t\n\n?\n\n?C????\n?\n\n+?\u00b7\n\n? np\n?\n\nl=1\n\n??\n?\n\nC?,l ~ul ? ~D?,l\n?\n\n?\n\n= R??, (3.1)\n\nwhere C?? is the concentration of component ? over all phases, including\nthe adsorbed ones. Mathematically,\n\nC?? =\n\n?\n\n1?\nnv c\n?\n\nc=1\n\nC?c\n\n? np\n?\n\nl=1\n\nSl Cl ?+C??, (3.2)\n\nwhere np is the number of phases; nv c is the number of volume-occupying\nphases, Sl is the saturation of phase l , and Cl ? is the concentration of com-\nponent ? in phase l , and C? is the adsorbed concentration. In Equation\n(3.1), ? is the media porosity. It is assumed that it changes linearly with\nthe pressure according to the expression\n\n?=?0(1+Cr (P ?P0)), (3.3)\n\nwhere ?0 is the referential porosity, evaluated at pressure P0, and Cr is the\nrock compressibility, which is supposed to be constant.\n\nStill referencing Equation (3.1), ?? represents the ratio between the\ndensity of the pure component ? at reservoir conditions and its density at\nstandard conditions. It is modelled according to the expression\n\n?? = 1+C\no\n?\n(P ?Pstd), (3.4)\n\nwhere C o\n?\n\nis the component compressibility, also supposed constant. ~D?,l\nis the dispersive flux, which is assumed to have a Fickian form, while ~ul\nis the superficial velocity. It is assumed that the superficial velocity obeys\nDarcy\u2019s Law:\n\n~ul =??lK \u00b7(?Pl ??l ?h). (3.5)\n\n?l is the phase mobility, given by the expression,\n\n?l =\nkr,l\n\u00b5l\n\n, (3.6)\n\nwhere kr,l is the phase relative mobility and \u00b5 is the phase viscosity, K\n\n\n\n22 UTCHEM RESERVOIR SIMUL ATOR\n\nis the medium absolute permeability, Pl is the phase pressure, ?l is the\nphase specific gravity, and h is the vertical depth.\n\nThe source term R?? in Equation (3.1) is composed by a the rate of\ninjection/production of component ? plus the rate of consumption/pro-\nduction of ? in chemical reactions. It is expressed by\n\nR?? =?\nnp\n?\n\nl=1\n\nSl r?l +(1??)r?s +Q??. (3.7)\n\nr?l and r?s are the reaction rates in phase l and in the solid phase. Q??, on\nthe other hand, is the rate of production/injection of ? due to neighbour-\ning wells.\n\nSumming Equation (3.1) for each volume-occupying component the\ncomponent concentrations cancel out. The resulting equation is, then:\n\n?refCt\n? P1\n? t\n+?\u00b7(?T cK \u00b7?P1)=?\u00b7\n\n? np\n?\n\nl=1\n\n?l cK \u00b7(?Pc p 1 ??l ?h)\n\n?\n\n+\nnv c\n?\n\nc=1\n\nQ?c . (3.8)\n\nIn the above equation?l c includes the correction for fluid compressibility.\nIt is given by\n\n?l c =\nkr l\n\u00b5l\n\nnv c\n?\n\n?=1\n\n??C?l . (3.9)\n\nThe total relative mobility by its turn is given by\n\n?T c =\nnp\n?\n\nl=1\n\n?l c . (3.10)\n\nFinally, the total compressibility Ct is the volume-weighted sum of the\nrock (Cr ) and component (C\n\no\n?\n\n) compressibilities:\n\nCt = Cr +\nnv c\n?\n\n?=1\n\nC o\n?\n\nC?? (3.11)\n\nThe main equations to be solved are the (3.1) and the (3.8). Equation\n(3.8) is solved implicitly, which implies that a linear system must be solved\nin order to obtain the aqueous phase pressure field. From those calcula-\ntions arises the solution of Equation (3.1) for each component, solution\n\n\n\nUTCHEM RESERVOIR SIMUL ATOR 23\n\nfrom which the concentration fields are obtained. The knowledge of the\npressure and concentrations enables determining all the other variables.\nIt\u2019s noteworthy that a discourse about physical and chemical models im-\nplemented in UTCHEM would be an out-of-scope topic for this study.\nHence, it\u2019ll not be included in this description. For a complete description\nof UTCHEM and its features it is recommended to consult [2].\n\n3.3 Grids\n\nDespite UTCHEM having a version supporting unstructured grids, struc-\ntured grids are still the most used kind of grid in this simulator. The UT-\nCHEM\u2019s version received for this work had originally three types of coor-\ndinate system: Cartesian, radial, and curvilinear. The radial system was\nimplemented to study the flow near the wellbore. The flow is assumed to\nbe plenty radial and thus the domain is divided only in the radial direction\nr and in the vertical direction z . This type of coordinate system does not\ntake advantage of the parallelization of the code because the division of\nthe grid described in Chapter 5 is in the y direction, which is not associ-\nated to any of the radial system axes. The curvilinear coordinate system\nimplemented in UTCHEM is basically a two dimensional curvilinear grid\nin the x ? z plane extruded in the direction y . Besides these three types\nof grids, an implementation of a corner-point grid was moved from an-\nother version of UTCHEM to the version of this work and it was further\nparallelized.\n\n\n\n\n\nCHAPTER\n\n4\nThe Element-based Finite Vol-\n\nume Method\n\n4.1 Introduction\n\nFinite Volume Methods (FVM) are numerical methods used for solving\ndifferential equations derived from the application of conservation laws.\nThe customary approach to deduce those equations consists in applying\na balance of a given property to a control volume. Then, assuming that\nits size tends to zero leads to a differential equation. Instead of using\nthe differential equations directly, the Finite Volume Method takes a step\nback and utilize the prior balance equations. This procedure assures the\nmethod\u2019s conservative character.\n\nThe Element-based Finite Volume Method (EbFVM) is a Finite Vol-\nume Method that applies some Finite Element Method (FEM) concepts\nin order to provide to provide more geometrical flexibility to the Finite\nVolume Method. It allows applying the control volume approach to a\nunstructured grid. This study intends to use triangular and quadrangular\n\n25\n\n\n\n26 THE ELEMENT-BASED FINITE VOLUME METHOD\n\nelements in two-dimensional grids and tetrahedron, hexahedron, square-\nbased pyramid, and triangular based prism elements in three-di-mensional\ngrids. These grids are unstructured, which means that there is no pre-\ndefined rule to associate an element with its neighbors [16]. Figure 4.1\nshows how such grids may provide a good discretization of geometrically\ncomplex reservoirs.\n\nx y\n\nz\n\nReservoir\n\n(tetrahedrons)\n\nGeological fault \n\n(Prisms and hexahedron)\n\nNear-well region\n\n(prisms and hexahedrons)\n\nFigure 4.1 \u2013 Discretization of an hypothetical reservoir using a unstruc-\ntured grid (adapted from [17])\n\n4.2 Grid entities\n\nA grid is a collection of geometrical entities ?i such that for a domain ?\n\n?\n\ni\n\n?i =?. (4.1)\n\nEach ?i has a nonempty interior, but the interior of the intersection of ?i\nand ?j , with i 6= j , is empty [11]. The entities ?i are called the elements\nof the grid [16]. Here, it is considered that in two-dimensional domains\nthe elements may be triangles or quadrilaterals. In three-dimen-sional\ndomains, on the other hand, there may be tetrahedra, hexahedra, square-\nbased pyramids, or triangle based prisms. If a grid has elements of differ-\nent types, the grid is called hybrid. One may note that this broadens the\n\n\n\nTHE ELEMENT-BASED FINITE VOLUME METHOD 27\n\nscope of Cartesian grids, in which only rectangles and parallelepipeds are\nused.\n\nThe contour of the elements is formed by entities here called facets.\nFacets are edges in 2D elements and surfaces in 3D elements. If a facet\nis not at the boundary of the domain, then always exist two elements\nsharing that facet. Hence, it is not possible partial contact between facets,\nthat is, the grid is conformal. A more precise definition of conformal grids\nmay be found in [11].\n\nThe term face are let to geometrical entities that delimit the control\nvolumes. In 2D they are the segments that connect the edges\u2019s midpoints\nto the element centroids, as illustrated in Figure 4.2. In 3D the definition\nis analogous and may be found in [11]. The faces divide the element into\nsmaller regions called subelements, each one associated to a vertex. Fig-\nure 4.2 illustrates the main grid entities of a triangular element.\n\nFace\n\nFacet\nVertex Centroid\n\nSubelement\n\nFigure 4.2 \u2013 Main entities of a triangular element\n\nIn the EbFVM, the unknowns of the problem are associated to ele-\nments\u2019 vertices, also named nodes. The control volumes are assembled\nfrom subelements surrounding the nodes, as illustrated in Figure 4.3. As\na consequence, the control surfaces are formed by the faces themselves.\nRegarding that geometrical arrangement, it is quite convenient to sup-\npose that physical properties such as permeability are homogeneous in-\nside an element. Therefore, no interpolation scheme is required to eval-\nuate them at the surface of the control volumes [6].\n\n\n\n28 THE ELEMENT-BASED FINITE VOLUME METHOD\n\nControl Volume\n\nNode\n\nControl Surface\n\nFigure 4.3 \u2013 Control volume creation\n\n4.3 Numerical scheme\n\nAn EbFVM common approach to handling geometrical distortion of the\nelements in a grid is the transformation known as mapping. It maps each\nelement from the global coordinate system to a local coordinate system\nin which the element has a regular representation. Figure 4.4 illustrates\na triangle element being mapped to a transformed space represented by\nthe coordinates (?,?). As it will be demonstrated later, this procedure\nsimplifies the discretization of the modelling equations.\n\nV1\n\nV2\n\nV3\n\nV1=(0,0)\n\nV =(1,0)2\n\nV3=(0,1)\n\n?\n\n?\n\nx\n\ny\n\nFigure 4.4 \u2013 Mapping into a transformed space\n\nThe mapping from global to local coordinate systems are performed\nusing first order shape functions from the FEM. For every vertex i of an\nelement there is a shape function Ni which is 1 at i and 0 at all the other\n\n\n\nTHE ELEMENT-BASED FINITE VOLUME METHOD 29\n\nvertices. In such manner, each point p =(x , y , z) may be represented as\n\np =\nnv\n?\n\ni=1\n\nNi(?,?,?) pi , (4.2)\n\nwhere pi , i = 1, ..., nv , represents the element\u2019s vertices, nv the number of\nvertices, and (?,?,?) the local coordinates of P . The shape functions must\nbe continuous, differentiable, and partitions of the unity, so that they are\npositive and obey the relation\n\nnv\n?\n\ni=1\n\nNi(?,?,?)= 1 (4.3)\n\nfor any point (?,?,?) in the local coordinate system [24].\n\nLet ? be a function defined over the grid nodes (vertices of the el-\nements). The EbFVM proposes that the value of ? inside an element is\ngiven by the relation\n\n?(x , y , z)=\nnv\n?\n\ni=1\n\nNi(?,?,?)?i . (4.4)\n\nAnalyzing the properties of the shape functions shown above, one may\nrealize that the value of ? inside an element is an average of its values at\nthe element vertices and the weight is given by the shape function. This\nis actually consistent, once ? will not be higher than the maximum value\nat a vertex nor lower than its minimum value.\n\nSince shape functions are differentiable, the gradient of ? in the\nglobal coordinate system may be expressed as\n\n??=?\n\n?\n\nnv\n?\n\ni=1\n\nNi(?,?,?)?i\n\n?\n\n=\nnv\n?\n\ni=1\n\n?\n\n?\n\n?xNi\n?y Ni\n?zNi\n\n?\n\n??i =\n\n?\n\n?\n\n?xN1 ?xN2 \u00b7\u00b7\u00b7 ?xNnv\n?y N1 ?y N2 \u00b7\u00b7\u00b7 ?y Nnv\n?zN1 ?zN2 \u00b7\u00b7\u00b7 ?zNnv\n\n?\n\n??e , (4.5)\n\nwhere\n?e =\n\n?\n\n?1 ?2 . . . ?nv\n?T\n\n(4.6)\n\n\n\n30 THE ELEMENT-BASED FINITE VOLUME METHOD\n\nis a vector with the values of ? at the element vertices. However, the shape\nfunctions are usually given in terms of the local coordinates and thus the\nderivatives of Ni with respect to the global coordinates are inconvenient.\nApplying the chain rule,\n\n?\n\n?\n\n??Ni\n??Ni\n??Ni\n\n?\n\n?=\n\n?\n\n?\n\n??x ?? y ??z\n??x ?? y ??z\n??x ?? y ??z\n\n?\n\n?\n\n?\n\n?\n\n?xNi\n?y Ni\n?zNi\n\n?\n\n?. (4.7)\n\nThe matrix\n\nJ =\n\n?\n\n?\n\n??x ?? y ??z\n??x ?? y ??z\n??x ?? y ??z\n\n?\n\n? (4.8)\n\nis the well-known Jacobian matrix [11] and may be computed using Equa-\ntion (4.2). Defining\n\nD ?\n\n?\n\n?\n\n??N1 ??N2 \u00b7\u00b7\u00b7 ??Nnv\n??N1 ??N2 \u00b7\u00b7\u00b7 ??Nnv\n??N1 ??N2 \u00b7\u00b7\u00b7 ??Nnv\n\n?\n\n?, (4.9)\n\nEquation (4.5) may be expressed as\n\n??= J ?1D ?e . (4.10)\n\nMatrix J ?1D may be interpreted as a discrete gradient operator.\n\n4.4 Discretization of a conservation equation\n\nAccording to [16], the conservation equation of a generic property ? as-\nsociated to a fluid flowing may be written as\n\n?\n\n? t\n\n?\n\n??\n?\n\n+?\u00b7\n?\n\n?u?\n?\n\n=?\u00b7\n?\n\n???\n?\n\n+Q , (4.11)\n\nwhere ? is the specific mass, u is the fluid velocity, ? is a diffusivity coeffi-\ncient, and Q is a source term associated to the property ?. The discretiza-\ntion in a FVM is performed integrating the above equation in each control\nvolume. Consider a control volume ? as the one illustrated in Figure 4.3.\n\n\n\nTHE ELEMENT-BASED FINITE VOLUME METHOD 31\n\nNoting that the grid is static, the first integral becomes\n\n?\n\n?\n\n?\n\n? t\n\n?\n\n??\n?\n\ndV =\n?\n\n? t\n\n?\n\n?\n\n?\n\n?\n\n??\n\n?\n\n?=\n?\n\n? t\n\n?\n\nM????\n?\n\n?\n?\n\n? t\n\n?\n\nM???,n\n?\n\n, (4.12)\n\nwhere M? is the mass contained in? and ??? is the average value of ? in?.\nSuch average value was approximated by the value of ? at the node itself,\nwhich is a reasonable approximation since the node is usually closed to\nthe center of the control volume. The discretization of the source term is\nsimilar and gives\n\n?\n\n?\n\nQ dV ?Q?,n V?, (4.13)\n\nnoting that V? is the volume of ?.\n\nThe second term of Equation (4.11) models the advective transporta-\ntion of ?. Integrating and applying the divergence theorem\n\n?\n\n?\n\n?\u00b7\n?\n\n?u?\n?\n\ndV =\n\n?\n\n??\n\n(?u \u00b7n?)?dA =\n?\n\nf\n\n?\n\n??f\n\n(?u \u00b7n?)?dA. (4.14)\n\nApproximating the average value of the integral above by the value of the\ncenter of each face result\n\n?\n\n?\n\n?\u00b7\n?\n\n?u?\n?\n\ndV ?\n?\n\nf\n\n?\n\n?u \u00b7?A f\n?\n\n?f =\n?\n\nf\n\nm? f ?f , (4.15)\n\nwhere m? f is the mass flow rate crossing the face f . It would be intu-\nitive to evaluate ?f using Equation (4.4). However, this is usually not\nrecommended because the method may become numerically unstable.\nUpwind-like methods are preferably, despite they may introduce numer-\nical diffusion to the solution obtained [16].\n\nThe last term from Equation (4.11) to be integrated is the diffusive\nterm. Integrating and applying the divergence theorem again:\n\n?\n\n?\n\n???dV =\n?\n\n?\n\n(???) \u00b7n?dA ?\n?\n\nf\n\n(???)f \u00b7?A f . (4.16)\n\n\n\n32 THE ELEMENT-BASED FINITE VOLUME METHOD\n\nUsing the discrete gradient operator defined in Equation (4.10),\n\n?\n\n?\n\n???dV ?\n?\n\nf\n\n?\n\n? J ?1D ?e\n?\n\nf\n\u00b7?A f . (4.17)\n\nThe stencil of a numerical method is defined as the set that contains\nall the nodes that are involved in the discrete equation of a given node. Re-\ngarding the four terms of Equation (4.11) discretized previously, only two\nof them use values at neighbor nodes: the advection \u2013 Equation (4.15) \u2013\nand the diffusive \u2013 Equation (4.17) \u2013 terms. According to those equations,\nin the EbFVM context, the stencil is all the nodes that share an element\nwith the given node. Figure 4.5 illustrates the stencil of a general node p .\n\nP\n\nn1\nn2\n\nn3\n\nn4\n\nn5\nn6\n\nn7\n\nFigure 4.5 \u2013 Stencil of p\n\n\n\nCHAPTER\n\n5\nThe Proposed Approach\n\n5.1 UTCHEM\n\nAiming a performance improvement, the methodology adopted in the\nparallelization process must lay its focus on UTCHEM\u2019s operations that\nrequire the most significant part of the computational efforts. Evidently,\nrefined grids employed in large and/or complex domain simulations are a\nrelevant matter. This fact motivated employing a domain decomposition\nbased methodology. The reservoir grid is divided among the available\nprocessors. Each one of them works in only a part of the domain, while\nMPI routines are in charge of the communication. Since all implementa-\ntions are based on MPI routines, the code is fully functional for the most\ngeneral parallel architecture: hybrid-memory architecture. As a conse-\nquence, UTCHEM can run in parallel not only in CPU clusters, but also\nin personal computers. The use of the IPARS Framework (described in\nSection 5.1.3) made viable the introduction of a new input file format\nfor the simulator. This format is more user friendly and error prone, as\ndescribed in Appendix A.\n\n33\n\n\n\n34 THE PROPOSED APPROACH\n\n5.1.1 Domain decomposition\n\nUTCHEM\u2019s parallelization applies only to structured grids. A structured\ngrid has such an organization that is completely defined by the number\nof grid blocks on each coordinate direction and their dimension. Here it\nis supposed, without losing generality, that the grid is Cartesian and we\ndenote the number of grid blocks on each direction by Nx , Ny , and Nz ,\nrelated to the directions x , y , and z , respectively.\n\nSeemingly, the main target in parallelization is improving computa-\ntional performance. In order to achieve that goal, it\u2019s crucial to decom-\npose the domain in such a way that each processor take care of almost\nthe same number of grid blocks. That is, overloaded processors must be\navoided. Furthermore, dividing a structured grid is much easier than an\nunstructured one, since its topology is strictly dependent of the coordi-\nnate system. To each block is associated a topological coordinate (i , j , k),\nwhich represents its localization indexes along the directions x,y and z,\nrespectively. The division applied in this study occurs, in fact, along the\ndirections y and z (does not depend on x). The pivotal reference in the\ndecomposition procedure is the j direction. The k one, on its turn, is\nregarded as a layer. For each layer, then, is necessary to define the range in\nthe j direction. This can be done by two numbers, namely jP ? and jP + in\nthe following way: if a grid block S has topological coordinates (iS , jS , kS)\nand jP ? ? jS ? jP + , then S is in the domain of processor P . One may note\nthat this procedure could be simplified by limiting the division solely to\nthe y direction. This is not recommended since depending on Ny and on\nthe number of processors the load balance might be not good enough.\nHence, the values of jP ? and jP + may change according to the topological\ncoordinate k . For example, if a 3x3x2 case will be simulated with two\nprocessors, the first processor may take the first two grid blocks of the y\ndirection in the first horizontal layer, but only one in the last one. The grid\nthus is equally divided: each processor works on nine grid blocks. Figure\n5.1 exemplifies a grid division.\n\nIn UTCHEM there is an important variable named NBL. It was used to\nstore the total number of grid blocks. In the Parallel UTCHEM, this would\nnot make sense anymore, given that each processor sees only a part of the\nreservoir domain. Now NBL stores the number of local active grid blocks\n(inactive grid blocks will be discussed in the following section). The size of\n\n\n\nTHE PROPOSED APPROACH 35\n\nFigure 5.1 \u2013 Example of a grid division\n\ngrid-dependent arrays is set to be precisely NBL. As a consequence, grid-\nrelated operations do not need to be modified at all. There is however an\nexception: if a grid-related array is used in a stencil computation (compu-\ntation that requires values at neighbor grid blocks), values at grid blocks\nthat do not belong to the current processor domain may be needed. For\nsuch variables a continuous indexing such that the one that starts in 1\nand ends in NBL is not feasible. In this case, a coordinate-based index-\ning was applied: each processor domain is extended so that it contains\ngrid blocks of other processor domains. The additional grid blocks are\ncalled ghost cells and no computation is performed for them. They are\nused only to store values from other processor domains. Values at ghost\ncells are updated using MPI routines every time stencil computations are\nperformed. In Figure 5.2 ghost cells are illustrated.\n\nGrid-dependent arrays with continuum indexation will be labeled\nhere as type 1, while the others will be labeled as type 2. The indexes of a\ntype 2 array are named I1, J1, and K1, which are related to the directions\nx , y , and z . The range of I1 is from IL1 to IL2, while the range of K1 is\nfrom KL1 to KL2. For J1 the range is from JL1V(K1) to JL2V(K1). One must\nnote that the range of J1 depends on the local coordinate K1, reflecting\nthe nature of the grid division. Because of the changing in the indexation\nof some grid-dependent arrays, every loop of UTCHEM in which type 2\narrays are used must be changed from continuum to coordinate based,\nas illustrated in Figure 5.3. If the continuum index is needed but the loop\nis coordinate-based, then the continuum index may be accessed using\nthe variable IJKPOS. Due to inactive grid blocks, explained in the follow-\ning section, the opposite can not be performed: access the coordinate\nindexes I1, J1, and K1 from a continuum index I.\n\n\n\n36 THE PROPOSED APPROACH\n\nFigure 5.2 \u2013 Example of ghost cells from a 6x8x3 grid\n\nFigure 5.3 \u2013 Loop modification when type 2 arrays are presented\n\n5.1.2 Inactive grid blocks\n\nInactive grid blocks are characterized by very low porosity and perme-\nability such that the fluid flow through them is negligible. They facilitate\nan accurate description of a reservoir geometry using a structured grid,\nsuch as a cartesian or a corner-point grid. Figure 5.4 illustrates a case in\nwhich some grid blocks were set as inactive (those in grey), adjusting the\ngrid\u2019s geometry to the reservoir\u2019s shape. In UTCHEM\u2019s previous versions,\ninactive grid blocks were kept in almost all computation. It was assigned\nto them small values \u2013 but not null, due to convergence problems \u2013 of\nporosity and permeability and they were supposed to be fully saturated by\nwater. Sometimes, however, using them in computations may be physi-\ncally inconsistent (e.g. computation of concentration derivatives).\n\nAlong the parallelization process, the same approach used in [8] was\n\n\n\nTHE PROPOSED APPROACH 37\n\nFigure 5.4 \u2013 Example of inactive grid blocks been used to better describe\na reservoir domain\n\nimplemented: excluding inactive grid blocks from all computations. This\nis automatically done in type 1 arrays because their size is equal to the\nnumber of active grid blocks (inactive grid blocks do not have a contin-\nuum index). For type 2 arrays, on the other hand, a variable named KEY-\nOUT is used to indicate if a grid block is active or not. KEYOUT is a type\n2 array that has the value 1 at active grid blocks, 0 at inactive grid blocks,\nand -1 at ghost cells. If a grid block has KEYOUT 1, then it is an active\ngrid block. If KEYOUT is -1 the grid block is a ghost cell and if KEYOUT\nis 0 the grid block is inactive. Hence, before each computation the value\nof KEYOUT is verified, as described in Figure 5.3. For stencil computa-\ntions, inactive grid blocks are treated as impermeable boundaries and a\nproper methodology is applied in every case. For example, if a concentra-\ntion derivative needs to be computed but there is a neighbor grid block\nthat is inactive, a central difference scheme will substituted by a forward\nor backward difference scheme, as it is performed near to the reservoir\nboundaries.\n\n5.1.3 IPARS framework\n\nUTCHEM was parallelized with support of the IPARS framework. IPARS\nprovides several routines that facilitate good strategy execution of parallel\n\n\n\n38 THE PROPOSED APPROACH\n\nrelated operations. For example, it has routines to divides the grid accord-\ning to the number of processors, update type 2 array values at ghost cells,\nand measure the execution time expended by each processor. Figure 5.5\ndelineates the organization of the parallel version of UTCHEM, which for\nnow on will be named UTCHEMP.\n\nFigure 5.5 \u2013 Organization of UTCHEMP\n\nThe main routines of IPARS framework are placed in the folder Fra-\nmework. The folder Drive, inside Framework, contains the main subrou-\ntine, called IPARS, which directly or indirectly calls all the other subrou-\ntines. In folder Input there are subroutines to manage the reading, and\nthe initialization of the simulation. Memman, on the other hand, con-\ntains several functions implemented in C++ that are used by the IPARS\nframework. The types of the subroutines presented in the folders Parall,\nPrint, Well, and Util are straightforward: subroutines for communication\nbetween processors, to export results, to help well related operations, and\nsome of general purpose. In folder IPARS, inside folder Source, there are\nsome additional IPARS subroutines that are used mainly to initialize the\nsimulation. On the other side, the folder UTCHEM contains almost all\nsubroutines that were previously presented in UTCHEM, as well as some\nnew ones. Since now the simulation is driven by the IPARS framework,\nsome operations were moved from subroutines originally in UTCHEM to\nsubroutines of the IPARS framework. For example, the part of the sub-\nroutines INOUT that were used to read geometry data were moved to a\n\n\n\nTHE PROPOSED APPROACH 39\n\nsubroutine from the folder Input because such data is needed before the\ncalling of the subroutine that divides the grid. Thus, INOUT now executes\nless operations that it did before. Finally, the folders Makes, Size, and\nWork have files for the code compilation.\n\nFigure 5.6 \u2013 Simulator workflow\n\nThe algorithm workflow of UTCHEMP is sketched in Figure 5.6. At\nfirst the IPARS framework is initialized and the input files are read. Al-\nmost all data is read by the subroutine INOUT or its inner subroutines.\nAn exception to this are the geometry data reading, which occurs before\nINOUT is called, and the well data reading, which is performed by the sub-\nroutines WELLREAD, now called outside INOUT. After reading the input\nfiles, some additional initializations are performed and then it is called\nthe subroutine AAMAIN. AAMAIN was originally the the main UTCHEM\u2019s\nsubroutine, directly or indirectly calling all the other ones. In UTCHEMP,\nhowever, AAMAIN contains only the part of the code used to execute a\ntime step. After AAMAIN some additional outputs are generated and the\nsimulation is finalized.\n\n5.2 EFVLib\n\nThe parallel computing methodology applied in the EFVLib is also based\non domain decomposition. The reasons are the same: the main com-\nputational cost from the simulation usually comes from the grid-related\noperations. When the domain is decomposed, each processing unit works\n\n\n\n40 THE PROPOSED APPROACH\n\nonly in a part of the domain, which is much smaller than the whole do-\nmain. Since the processors operate simultaneously, the time required to\ncomplete a large scale simulation using several processors is equivalent\nto the time that a single processing unit takes to run a small simulation\ncase.\n\nThe straightforward methodology for dividing a domain is to distribute\nthe grid elements from Equation (4.1) among the available processors.\nThis was indeed the methodology applied in UTCHEM, which uses a cell-\ncen-tered numerical scheme. However in the EbFVM the unknowns are\nat the vertices of the elements. A division based on elements would then\ncause redundant computations since a vertex might be replicated in mul-\ntiple computational domains. Furthermore, usually there are many more\nelements than vertices. For example, in a typical tetrahedron grid, the\nnumber of elements is six times the number of vertices [19]. Thus the\neffort to divide a grid tends to be smaller if the division is based on vertices\nrather than on elements. For these reasons, the grid division methodology\nadopted in this work is based on vertices: each vertex is addressed to a\nunique computational domain.\n\nIn Section 4.4 a general conservation equation was discretized using\nthe Element-based Finite Volume Method. From Equation (4.17) one may\nnote that the flux of a property through a face depends on the value of that\nproperty at the element vertices. Thus, a vertex is in a sense connected to\nall the other vertices from the elements to which it belongs. The notion of\nconnection leads to the representation of a grid as a graph. The vertices\nare the graph\u2019s nodes and if two vertices belongs to the same element,\nthen there is an edge connecting them. Figure 5.7 illustrates the graph\nrepresentation of a grid.\n\nThe graph representation of a grid is convenient for the division task.\nDivide a grid means separate the nodes of its graph into different groups.\nIf two connected nodes go to different groups, then the edge connecting\nthem was crossed. However, those two nodes depend on each other for\nthe flux computation. If they are in separate groups, data must be in-\nterchanged between those groups. So, minimizing the number of edges\ncrossed improves the quality of the grid division.\n\n\n\nTHE PROPOSED APPROACH 41\n\n0 2 3\n\n4\n\n5 6 7\n\n9\n\n8\n\n1\n\n(a) Grid\n\n2 3\n\n4\n\n5 6 7\n\n9\n\n8\n\n1\n\n0\n\n(b) Graph\n\nFigure 5.7 \u2013 Graph in (b) is the graph of grid (a)\n\n5.2.1 Graph partitioning\n\nIn this study the partition of graphs is supported by Metis library [1]. This\nlibrary gives back a vector P whose size is the number of nodes. Each\ncomponent of P is the partition to which the node belongs. There are two\ngraph partitioning methods available: METIS_PartGraphRecursive\nand METIS_PartGraphKway. The first one implements a multilevel re-\ncursive bisection algorithm [14] and will be referred to as bisection. The\nsecond method implements a k-way partitioning scheme [15] and will\nbe referred to as k-way. The method chosen to parallelize the studied\ncodes was the METIS_PartGraphKway because it produces partitions of\ncomparable or even better quality and it also requires less time [15].\n\nBoth graph partitioning methods available in Metis \u2013 bisection and\nk-way \u2013 are based on the multilevel paradigm. The partitioning itself goes\nthrough three phases: coarsening, initial partitioning, and uncoarsening.\n\nCoarsening phase\n\nIn the coarsening phase, adjacent nodes are grouped yielding a coarser\ngraph in order to diminish its partitioning complexity. This grouping pro-\nceeds until a sufficiently small graph is achieved.\n\nA weight is attributed to each node and to each connection. Let N n\n\nbe a group of nodes that were collapsed into a single node n . The weight\nof n is simply the sum of the weights of the nodes in N n . Edges connecting\ntwo nodes that are both in N n are eliminated. On the other hand, all edges\nconnecting a node from N n to an external node p are collapsed into a\nsingle edge connecting n to p . The weight of this edge is of course the sum\n\n\n\n42 THE PROPOSED APPROACH\n\nof the weights of the collapsed edges. This weight-based strategy helps\nthe construction of good partitions in the coarser graphs with respect to\nthe finer graph. The edge-cut of the graphs are the same and a balanced\npartitioning in the coarser graphs tends to be also balanced in the finer\ngraph [15].\n\nThe method chosen to group the nodes is the Heavy Edge Matching\n(HEM) [15]. The objective of this method is the production of coarser\ngraphs which lead to partitions that minimize the edge-cut. This is achie-\nved by looking for coarsened graphs that reduce the sum of the edge\nweights. Denoting by ep q the edge that connects the nodes p and q and\nby w(ep ,q) the weight of such edge, the following relation holds\n\n?\n\nep ,q?G i+1\nw(ep ,q)=\n\n?\n\nep ,q?G i\nw(ep ,q)?\n\n?\n\np ,q?N n ;n?G i\nw(ep ,q), (5.1)\n\nwhere G i+1 is the graph obtained by coarsening G i . Thus in order to\nminimize the edge weight sum it is interesting to collapse the edges whose\nweights are the biggest. In the HEM method all nodes are visited in a\nrandom order. If a node n was not matched yet (grouped to other nodes),\nit is matched to the unmatched adjacent node whose corresponding edge\nweight is the biggest. If such adjacent node does not exist, the node n\nremains unmatched. According to [15], this methods produces good re-\nsults in practice, despite the fact that it does not guarantee that coarsened\ngraph obtained is the one that minimize the edge weight sum.\n\nInitial partitioning phase\n\nThe second phase is the initial partitioning phase. It takes place when\nthe coarsening method is not effective anymore, that is, the graph size\nreduction factor of successive graphs is greater than 0.8. This phase is the\nmajor difference between both Metis partitioning methods: the bisection\nmethod divides the coarsest graph in only two parts and the k-way, on the\nother hand, splits it directly into k parts.\n\nAs already mentioned, the bisection method divides the coarsest\ngraph in two parts, which are then uncoarsened. The resulting graphs are\nthen divided again until the required number of partitions is achieved.\nThe method must be applied log2 k times in order to obtain k partitions.\n\n\n\nTHE PROPOSED APPROACH 43\n\nThe complexity of the method is O (|E |log2 k), where |E | is the number of\nconnections (edges) [13].\n\nIn the k-way partition, the coarsest graph is divided directly into k\nparts and thus applying the method once is enough to obtain the number\nof partitions wanted. Curiously, the k-way method divides the coarsest\ngraph using the multilevel bisection algorithm [14]. Despite the fact that\na k-way division is much more laborious than a bisection, the coarsest\ngraph is so coarse that its division is computationally cheap. Since the\nmethod is applied only once, its complexity is reduced to O (|E |) [15].\n\nUncoarsening phase\n\nThe last phase is referred to as uncoarsening. A straightforward method-\nology would be simply ungrouping the nodes until the finer graph is rea-\nched. However, graphs with a larger number of nodes have also a larger\nnumber of degrees of freedom so are much more complex. Furthermore,\nthe best partition of a coarse graph may not be the best one of a finer\ngraph. For this reason, Metis improves the quality of the graph partition\nby swapping nodes among the partitions as the graph is uncoarsened [15].\nThe method chosen in this study is the Greedy Refinement (GR). In this\napproach, the nodes are visited in a random order and at each node n\nit is computed a gain function with respect to each partition b using the\nformula\n\ng b (n)= EDb (n)? ID (n). (5.2)\n\nEDb (external degree) is the sum of edge weights of the adjacent nodes\nthat are in b while ID (internal degree) is the sum of the edge weights\nof the nodes that are also in the partition a to which n belongs. The\nnode n will be moved to partition b if g b is positive and greater than the\ngain of any other partition. Besides, in order to preserve load balance the\nfollowing conditions must also be achieved:\n\n?\n\np?b\nw(p)+w(n)? W max (5.3)\n\n?\n\np?a\nw(p)?w(n)? W min. (5.4)\n\n\n\n44 THE PROPOSED APPROACH\n\nThe values used in Metis for W max and W min are 0.9|V0|/k and 1.03|V0|/k\nrespectively, where |V0| is the sum of all the node weights and k is the\nnumber of partitions. The GR algorithm is iterated until convergence.\nAccording to [15], the number of iteration is small.\n\n5.2.2 Ghost nodes\n\nIn Section 4.4 a general conservation equation was discretized using the\nElement-based Finite Volume Method. It was noted that the stencil as-\nsociated to a node n contains all the nodes that share an element with\nthis node, as illustraded in Figure 4.5. Those nodes are necessary not only\nto compute the shape functions, but also to store values of fields defined\non the grid. As a consequence, the subdomains obtained by partitioning\nthe related graph of a grid must be extended in order to embrace every\nneighboring node, resulting in subdomain overlapping.\n\nConsider for example the grid illustrated in Figure 5.8. This grid was\ndivided into two subdomains, each subdomain being assigned to a pro-\ncessor unity. The nodes and their corresponding control volumes ad-\ndressed to processor 0 are painted in yellow and the ones addressed to\nprocessor 1 are painted in red. Due to the stencil of the EbFVM, the dis-\ncretized equations of nodes 1, 5, 8, 11, and 12 in subdomain 0 require val-\nues at nodes 0, 9, 10, and 13, which are in another computational domain.\nThe same applies for subdomain 1: discretized equations of nodes 0, 9, 10,\nand 13 demand values at nodes 1, 5, 8, 11, and 12. The subdomains thus\nmust be extended, resulting in the grids illustrated in Figure 5.9.\n\nThe nodes that come from another computational domain are called\nghost nodes and are similar to the ghost cells used in UTCHEM. No com-\nputation is performed for them. They are used only to store values. If a\nvalue of a field changes in a node, then its value should be updated at all\ncomputational domains where such node is a ghost node. This operation\nis actually quite complicated, but the code developed for it hides most of\nthe technical details, leaving for the final user a simple interface. Subsec-\ntion 5.2.6 shows how simple such interface is.\n\nThe nodes as well as the elements have local indexes at their local\nsubdomains. The ordering of the nodes is performed in the following\nway: the first nodes are local \u2013 which means that they are addressed to\n\n\n\nTHE PROPOSED APPROACH 45\n\n1\n\n2\n3\n\n4\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n0\n\n5\n\n6\nProcessor\n\n0\nProcessor\n\n1\n\nFigure 5.8 \u2013 Unstructured grid divided into two subdomains\n\n1\n\n2\n0\n\n4\n\n7\n\n8\n\n9\n\n11\n\n12\n\n13\n\n10\n\n5\n\nProcessor 0 Processor 1\n\n1\n\n0\n\n8\n\n9\n\n11\n\n12\n\n13\n\n10\n\n5\n\n3\n\n6\n\nFigure 5.9 \u2013 Subdomains extended\n\nthe current subdomain \u2013 leaving the last indexes for the ghost nodes. This\nway of ordering is convenient for three reasons:\n\n\n\n46 THE PROPOSED APPROACH\n\n\u2022 If the user intends to access only the local nodes, it is sufficient\nswapping the vector of vertices until the last local vertex is reached;\n\n\u2022 The user may know if a node is local simply by checking if its index\nis lower than the number of local vertices;\n\n\u2022 It is not necessary to store the vertices in separate vector, which is\ncomputationally very convenient.\n\nFigure 5.10 illustrates the local indexes of the nodes of the grid from Figure\n5.9.\n\n0\n\n1\n8\n\n3\n\n5\n\n4\n\n9\n\n6\n\n7\n\n11\n\n10\n\n2\n\nProcessor 0 Processor 1\n\n6\n\n0\n\n8\n\n1\n\n10\n\n9\n\n5\n\n4\n\n7\n\n2\n\n3\n\nFigure 5.10 \u2013 Local indexes of two subdomains\n\n5.2.3 Assembling in parallel a system of linear equations\n\nIn almost all simulations it is necessary to solve a system of linear equa-\ntions (SLE). This is one of the most time consuming operations. It is es-\ntimated that about 60 to 70% of the computational time is spend solv-\ning systems of linear equation [16]. Since the main focus of this work is\non computational performance, the SLE must be solved very efficiently.\nHowever, the development of a parallel solver for SLE is beyond the objec-\ntives of this work. It was used instead the Portable, Extensible Toolkit for\n\n\n\nTHE PROPOSED APPROACH 47\n\nScientific Computation (PETSc) [4]as an external library to solve the SLE\u2019s.\nPETSc provides a lot of tools specially designed for scientific applications\nand is recognized as one of the most powerful libraries available.\n\nSince PETSc is intended to support very general applications, there\nare many details that must be configured in order to assemble and solve a\nsystem of linear equations. It is desired however the product of this work\nto be simple in order to encourage people to employ parallel computing\nin their future developments. For this reason, a relatively simple com-\nputational interface was created to hide most of the PETSc details. The\nmatrix are supposed to be sparse, while both matrix and the vectors of\nthe linear system are distributed along the available processing units. In\nother words, matrix and vectors are parallel. The default preconditioner\nand solver are the incomplete LU factorization and GMRES, respectively,\nbut the user is allowed to choose any other methods available in PETSc.\n\nConsidering a generic SLE A x = b , PETSc distribute its components\namong the available processors. It is possible either to define the size\nof the global SLE and let PETSc divide the SLE among the processors or\ndefine the size of each local SLE and thus the global size will be the sum\nof the local sizes. In any case, the first processor will have the first rows,\nthe second processor will have the rows following the first processor rows,\nand so on. Each processor p has a submatrix Ap with the rows of A as\nwell as subvectors xp and bp with the corresponding components of x\nand b . Despite each processor having only a portion of the SLE, PETSc\nhandles the SLE as a global one. This means that the indices used to\nset a SLE are global instead of local. A processor may set a value even\nif the corresponding row is outside its range. If that is the case, the value\nis communicated to the processor that actually have the corresponding\nrow. Such communication however should be avoided, since it degrades\nperformance. It is recommended each processor to set only local values.\n\nIn order to better handle communication and thus optimize perfor-\nmance, PETSc divide the matrix Ap into two other ones: the \u201cdiagonal\u201d\nmatrix Ap ,d and the \u201coff-diagonal\u201d matrix Ap ,o . Ap ,d is a square matrix\ncomposed by the entries Ai j of A such that the rows i and j are local to\nthe processor p . Ap ,o on the other hand contains all the entries of Ap that\nare not in Ap ,d . Such way of interpret Ap is convenient for example in a\nmatrix-vector multiplication, which is an operation that may be executed\nseveral times while solving a SLE. Let w be the vector to which A will be\n\n\n\n48 THE PROPOSED APPROACH\n\nmultiplied, wp its local segment in the processor p , and wo the segment\nof w outside p . The operation performed locally in p is\n\nAp w =\n?\n\nAp ,d Ap ,o\n?\n\n?\n\nwp\nwo\n\n?\n\n= Ap ,d wp +Ap ,o wo . (5.5)\n\nThe multiplication Ap ,d wp can be promptly executed, since it involves\nonly local values. The values wo on the other hand must be received\nfrom another processors. The division of Ap in diagonal and off-diagonal\nportions is also convenient for Block Jacobi preconditioners, described\nlater. In this case, operations with the off-diagonal matrix Ap ,o are simply\nignored.\n\nBased on the observations above and on the domain decomposition\nmethodology described in the previous sections, have in mind that a pro-\ncessor have local vertices and local SLE rows and that they are not nec-\nessarily related. The first processor for example will have for sure the\nfirst SPE rows but probably not all the first vertices. In fact, the proces-\nsor 0 in Figure 5.8 has the set of vertices {1, 2, 4, 5, 7, 8, 11, 12} and not the\nset {0, 1, 2, 3, 4, 5, 6, 7}. It is important however to ensure that the local\nrows corresponds to the local vertices. A processor has only the data to\ncompute the discretized conservation equation coefficients of the local\nvertices. If the SLE row of a local vertex is not local, the coefficient must\nbe transmitted to the processor in which the row is local, resulting in\ncommunication overhead. Furthermore, the solution x retrieved from\nPETSc is the solution at the local rows. If the local rows are not related to\nlocal vertices, such solution must be manually transmitted to the corre-\nsponding processors. Another way of retrieving the solution is ask PETSc\nfor the global solution. This of course is not smart because it requires\nPETSc to make lots of communication to have a copy of the global solution\non each processor and also demands too much memory.\n\nTo make sure the local rows are related to the local vertices, two mea-\nsures are taken. First, the local size of the SLE must be equal to the number\nof local vertices (supposing only a single variable is being solved). In order\nto ensure this, the local size of the SLE is directly set in PETSc instead of\nsetting the global size and let PETSc to divide the SLE. Secondly, a new\nglobal indexation is set to the vertices. The new indices are called PETSc\nindices. The PETSc index of a vertex in a processor p is its local index in\n\n\n\nTHE PROPOSED APPROACH 49\n\np plus the sum of the number of local nodes from processors q such that\nq &lt;p . In Figure 5.11 it is illustrated the PETSc indexes of the nodes of the\ngrid from Figure 5.9. Note that now the processor 0 has in fact the set of\nvertices{0, 1, 2, 3, 4, 5, 6, 7}as it was desired. It is worth noting that this new\nglobal indices are used only to handle the local SLE. The results exported\nfrom a simulation still respecting the original global indexation.\n\n0\n\n1\n8\n\n3\n\n5\n\n4\n\n9\n\n6\n\n7\n\n13\n\n12\n\n2\n\nProcessor 0 Processor 1\n\n0\n\n8\n\n4\n\n9\n\n6\n\n7\n\n13\n\n12\n\n2\n\n10\n\n11\n\nFigure 5.11 \u2013 PETSc indexes of two subdomains\n\nFigure 5.12 illustrates the structure of a matrix assembled in parallel\nusing the EbFVM on the grid of Figure 5.11. This matrix has four submatri-\nces: A0,d , A0,o , A1,d , and A1,o . A0,d and A1,d are the diagonal submatrices\nof processors 0 and 1, respectively. Their coefficients are associated to\nlocal nodes only. Matrices A0,o and A1,o on the other hand are the off-\ndiagonal submatrices. Their coefficients are associated to a local and a\nghost node. It is important to emphasize that each processor assembles\nits diagonal and off-diagonal submatrices, not interfering in submatrices\nof other processors. As an example, consider without losing generality\nthat the problem to be solved is purely diffusive. The matrix is symmet-\nric, which means that for every pair of nodes (m , n) the coefficients am ,n\nand an ,m will be the same. If m and n are in different subdomains, then\nsuch coefficient will be calculated twice: one time by the processor of the\n\n\n\n50 THE PROPOSED APPROACH\n\nsubdomain of m and another time by the processor of the subdomain of\nn . Regarding that values at ghost nodes are updated, the results at both\nprocessors will be the same. This shows that reducing interface between\nsubdomains is important not only to reduce the amount of data commu-\nnicated between processors, but also to reduce repeated calculations.\n\na0,0 a0,1 a0,2 a0,8\n\na1,0 a1,1 a1,2 a1,3 a1,4\n\na2,0 a2,1 a2,2 a2,3 a2,4 a2,7 a2,8 a2,9\n\na3,1 a3,2 a3,3 a3,4 a3,5\n\na4,1 a4,2 a4,3 a4,4 a4,5 a4,6 a4,7 a4,9\n\na5,3 a5,4 a5,5 a5,6 a5,7\n\na6,4 a6,5 a6,6 a6,7 a6,12 a6,13\n\na7,2 a7,4 a7,5 a7,6 a7,7 a7,9 a7,12 a7,13\n\na8,0 a8,2 a8,8 a8,9 a8,10\n\na9,2 a9,4 a9,7 a9,8 a9,9 a9,10 a9,11 a9,12\n\na10,8 a10,9 a10,10 a10,11\n\na11,9 a11,10 a11,11 a11,12\n\na12,6 a12,7 a12,9 a12,11 a12,12 a12,13\n\na13,6 a13,7 a12,12 a13,13\n\nA0,d A0,o\n\nA1,o A1,d\n\nFigure 5.12 \u2013 Matrix assembled in parallel\n\n5.2.4 Solving in parallel a system of linear equations\n\nThe matrix coming from the discretization of the conservation equations\nis sparse and thus it is not feasible to apply a direct method to solve the lin-\near system. Instead, iterative methods are used for that task. In iterative\n\n\n\nTHE PROPOSED APPROACH 51\n\nmethods, instead of solving A x = b it is proposed to solve a much more\ncheap linear system K x = b . Since K is different from A, the solution\nx0 = K\n\n?1 b probably is not the solution of the desired linear system. There\nis an error e0 = x0 ? x and a residual r0 = A x0 ? b such that Ae0 = r0.\nHowever, it is not practical to solve Ae0 = r0 and thus the linear system\nK e?0 = r0 is solved instead. e?0 is an approximation of the actual error and\nit is used to correct the approximate solution: x1 ? x0 ? e?0. This process\nis executed until convergence is reached. The general formula of such\nscheme is\n\nxi+1 = xi ?K\n?1 ri . (5.6)\n\nAll methods that obey the above formula are called stationary. The term\nstationary comes from the fact that the operations applied in each iter-\nation are the same. There is a generalization of stationary methods that\ncontemplates almost all known iterative methods. Instead of using only\nthe last residue to compute the new approximate solution, all the previous\nresidues are used:\n\nxi+1 = xi +\n?\n\nj?i\nK ?1 r j ?i j , (5.7)\n\nwhere ?i j is the weight of the term K\n?1 r j . For stationary methods,\n\n?i j =\n\n?\n\n?1, ifi = j\n0, ifi 6= j\n\n(5.8)\n\nK is a linear system preconditioner and the convergence of an itera-\ntive method depends on how close to the original matrix A is the matrix K .\nLetting DA , L A , and UA be the diagonal, lower triangle, and upper triangle\nparts of A, some classical iterative methods are:\n\n\u2022 Richardson: K =?I ;\n\n\u2022 Jacobi: K = DA ;\n\n\u2022 Gauss-Seidel: K = DA +L A ;\n\n\u2022 SOR: K =??1DA +L A ;\n\n\u2022 Symmetric SOR (SSOR): K =(DA +L A)D ?1A (DA +UA).\n\n\n\n52 THE PROPOSED APPROACH\n\nBased on Equation (5.6) (and on its general form given by Equation\n(5.7)), one may see that the following operations are common to every\niterative linear system solver:\n\n\u2022 Vector operations (like additions and inner products);\n\n\u2022 Matrix-vector product;\n\n\u2022 Construction of a preconditioner matrix K ? A and the solution of\na linear system K x = y .\n\nSome methodologies to make these operations parallel are presented in\n[9].\n\nVector operations\n\nThe main vector operations are vector additions and inner products. Vec-\ntor additions are operations of the form x ??x +? y . If all vectors are dis-\ntributed in the same way, this operation is intrinsically parallel: all proces-\nsors can execute it without any communication. Inner products, on the\nother hand, are of the form ?= x T y . This is a reduction operation, since\nthe inputs are vectors and the result is a scalar. This operation cannot be\nexecuted without communication because each processor contains only\na part of the vectors x and y . The common strategy is to compute the\nquantities ?P = x\n\nT\nP yP at each processor, transmit the values obtained to\n\nall processors, and then perform the sum\n\n?=\nN\n?\n\ni=1\n\n?P , (5.9)\n\nwhere xP and yP are the local segments of x and y and N is the number\nof processors.\n\nMatrix-vector product\n\nThe stencil of the numerical methods used to solve partial differential\nequations makes the matrix of the resulting linear system to be sparse.\nThus, for most j \u2019s, 1 ? j ? n , where n is the size of the linear system, the\noperation yi ? yi +ai j x j is redundant. Let IP be the index set of the ma-\ntrix rows owned by a processor P (indexes of the diagonal portion of the\n\n\n\nTHE PROPOSED APPROACH 53\n\nsumbmatrix AP , described in the beginning of this section). The values x j\nthat the processor P actually needs in order to perform the matrix-vector\nmultiplication are those such that j is in the set\n\nSP,i ={j : j /? IP , ai j 6= 0} (5.10)\n\nIt is not clever however to send to P a package for each set SP,i . Instead,\nthe sets SP,i are combined into a single one defined by SP ??i?IP SP,i . The\nprocessor P receives only one package with the values of x in SP and then\nis able to perform the multiplication.\n\nPreconditioner\n\nThe parallelization strategy applied for the preconditioner strongly de-\npends on what is the chosen of K . The Jacobi method is intrinsically\nparallel, since K is the diagonal of A and the operation x = K ?1 y can be\nperformed to each component of x independently. For other methods, on\nthe other hand, there are difficulties. One of the most common precon-\nditioners is the incomplete LU factorization (ILU). This type of factoriza-\ntion is performed using Gaussian elimination just like the complete LU\nfactorization, but the elements that would become non-zero are simply\nignored. This avoids the fill-in phenomenon, letting the matrix L+U with\nthe same sparsity of the original matrix A.\n\nThe problem with ILU happens when it is necessary to solve a system\nL x = y in parallel. The second processor must wait the first one solve its\nunknowns to start working. The third processor, by its turn, must wait the\nsecond one, and so on. Solving L x = y is recursive and thus sequential.\nOne of the strategies that has been proposed to scale the ILU factorization\nis the Block Jacobi method. The idea of the Block Jacobi method is to\nignore all matrix components outside the processor subdomain. In other\nwords, the connection between processors is simply ignored. Doing this\nis actually not wrong, since one must remember that K is only an approx-\nimation of A. More iterations will be needed, but the method becomes\nscalable.\n\nAnother strategy that may be applied to scale the ILU factorization\nis the graph coloring associated with permutation. To each node p it\nis associated a color in such a way that all its neighbours have a color\ndifferent from p . Hence, there are no neighbours with same color. The\n\n\n\n54 THE PROPOSED APPROACH\n\nmatrix is then permuted to group the nodes with same color. The resulting\nmatrix have diagonal blocks that are diagonal matrices. Each color is then\nsolved at a time. Since nodes of same color does not depend on each\nother, the process can be executed in parallel. After a color is processed,\ndata is exchanged between the processors so that all of them have the\nnecessary values to start processing another color.\n\n5.2.5 Wells and boundaries\n\nWells and boundaries are also geometrical entities and thus they must be\nconsidered in the domain decomposition. The treatment for both of them\nis straightforward.\n\nA boundary is a collection of element facets that are at the contour\nof the solution domain [11]. These element facets are defined as bound-\nary elements. In two-dimensional grids the boundary elements are lines\nwhereas in three-dimensional grids they may be triangles or quadrangles.\nIt is possible to define several boundaries, each one having a group of\nboundary elements and a particular boundary condition.\n\nThe methodology used to divide the boundaries is similar to the one\napplied for the elements. Consider again the grid of Figure 5.8 whose\nboundaries are defined in Figure 5.13. For a control volume whose vertex\np is at the boundary of the domain, it is also necessary to compute fluxes\nthrough boundary elements. All boundary elements that have p as one\nof their vertices are involved in the computation. Thus, the condition to\ndivide the boundaries among the processors is: if a boundary element e of\na boundary E is in a subdomain ?i if there is a vertex v in ?i such that v is\na vertex of e . If e is in ?i , not necessarily all boundary elements of E are in\n?i : only the one with vertices in?i . Figure 5.14 shows how the boundaries\nfrom the grid of Figure 5.13 may be split into two subdomains. Subdomain\n0 have the boundaries 0 and 1 and subdomain 1 have the boundaries 0,\n1, and 2. Note that neither subdomain 0 nor subdomain 1 have all the\nelements of boundary 1.\n\nAccording to Figure 5.14, the nodes at the contour of a subdomain\neither are at the contour of the global domain or are ghost nodes. To the\nones at the global subdomain contour, it is simply applied the boundary\ncondition associated to the boundary to which they belong. On the other\n\n\n\nTHE PROPOSED APPROACH 55\n\nBoundary 0\n\nBoundary 1\n\nBoundary 2\n\nWell 0\n\nFigure 5.13 \u2013 Boundaries and well\n\nBoundary 1\n\nBoundary 2\n\nWell 0\n\nBoundary 0\n\nProcessor 0 Processor 1\n\nFigure 5.14 \u2013 Boundaries and well\n\nhand, for the ghost nodes nothing needs to be done, since their purpose\nis only store data and thus no computation is performed at them.\n\nWells are represented in EFVLib as a sequence of line segments that\n\n\n\n56 THE PROPOSED APPROACH\n\nare coincident with element edges. One of those line segment is called\nwell element and is limited by two nodes. Each one of this nodes is asso-\nciated to half of the well element. The flow rate of a phase ? going from a\nwell to the control volume of a node p is the sum of the flow rates of each\nof the half well elements associated to p . he phase flow rate is expressed\nby\n\nqp =??,p WIp\n?\n\nPp ??p\n?\n\n, (5.11)\n\nwhere ??,p is the phase mobility, WIp is the well index, Pp is the reservoir\npressure, and ?p is the well pressure [11]. All of those variables excepting\nare evaluated at the reservoir, except for ?p , which is evaluated inside\nthe well. Since the above equation depends on the reservoir condition,\nit is straightforward that wells should be segmented following the grid\ndivision. Two well elements that share a node must be present in the\nsubdomain in which this node is local. Figure 5.14 shows the division of\na well into two subdomains.\n\n5.2.6 The code\n\nThe implementations in EFVLib for parallel running does not properly\nsolve a case in parallel. They are actually a set of utilities that helps an\nEFVLib user to adapt his/her code to be solved in parallel. The user must\nhave in mind that, instead of dealing with the whole domain, the code are\nnow being used for only a portion of the domain. Sometimes it is neces-\nsary to update values at ghost nodes, divide a global field into local fields,\nor reunite local fields into a global one. The parallel utilities developed\nfor EFVLib hide most of the technical details and provide a simple user\ninterface.\n\nThree external libraries were used: Metis 5.1.0 [1], PETSc 3.4.4 [4], and\nBoost 1.55 [5]. The first one is used to partition the grid nodes according\nto what was explained in Subsection 5.2.1. The second one is devoted to\nsolve in parallel systems of linear equations coming from the discretiza-\ntion of the conservation equations. Both Metis and PETSc are hidden\ninside the parallel utilities developed here and hence the user has almost\nno contact with them. Boost on the other hand provides a simplified\ninterface for the MPI functions that was made available to the user and\nalso used in the implementation of the parallel utilities.\n\n\n\nTHE PROPOSED APPROACH 57\n\nThere are four classes for supporting parallel computing in EFVLib:\nVertexPooler, GridDivider, FieldDivider, and FieldOnVerti-\ncesSynchronizer. The interface of class VertexPooler is presented\nin Listing 5.1. This class is used to pool the nodes (vertices) into sub-\ndomains using Metis functions. Both k-way and bisection Metis graph\npartitioning methods are available and can be chosen using the variable\npartitionMethod. K-way is the default method. The class Vertex-\nPooler is actually an abstract class and thus cannot be instantiated. It is\nnecessary to create a class derived from VertexPooler that implements\nthe method computeWeightArray, which is responsible by setting the\nweights of the graph edges. Two of such classes were implemented \u2013 Un-\nweightedVertexPooler and InverseDistanceWeightedVertex-\nPooler \u2013 but the user is free to create a custom VertexPooler.\n\nThe function divide is the main function of VertexPooler. This\nfunction properly does the partition of the nodes of a grid. There are\ntwo input parameters: GridData and nParts. GridData is a temporary\ncomputational structure that stores the essential data of a grid that will\nbe divided[18]. nParts is the number of partitions, which will be usually\nequal to the number of processors. The output of divide is the variable\nsubdomains, which is an array that has the subdomain index of each\nnode.\n\nListing 5.1 \u2013 class VertexPooler\n\n1 class VertexPooler {\n2 public:\n3 VertexPooler( PartitioningMethod partitioningMethod = KWAY );\n4\n\n5 void setPartitioningMethod( PartitioningMethod partitioningMethod );\n6 void divide( GridDataPtr gridData, int nParts, idx_t?&amp; subdomains );\n7\n\n8 virtual ~VertexPooler(){}\n9\n\n10 protected:\n11 // ... protected attributes\n12\n\n13 private:\n14 virtual void computeWeightArray( GridDataPtr gridData, idx_t?&amp; weights ) = 0;\n15 // ... other private methods\n16 };//class VertexPooler\n17\n\n18 typedef SharedPointer&lt;VertexPooler > VertexPoolerPtr;\n\n\n\n58 THE PROPOSED APPROACH\n\nGridDivider is a class that takes a global GridData and creates the\nlocal GridData for each subdomain. This is a very important class. It\npartitions the nodes using a VertexPooler, creates the subdomains\u2019s\nelements, divides wells and boundaries, and extends the subdomains to\ninclude ghost nodes. The interface of this class is in Listing 5.2. divide\nis the function that should be called to perform the actual division a grid.\nThe master processor (processor whose rank is 0) should call the func-\ntion divide passing as parameters the GridData of the global grid (only\nthe master processor has access to the whole grid) and, if the desired\nVertexPooler is different from UnweightedVertexPooler, a Vertex-\nPooler. The other processors call the function divide passing no pa-\nrameter. They receive from the master processor the GridData of their\nlocal subdomain. The output of divide is a structure that contains the lo-\ncal GridData as well as vectors that may be used by FieldDivider and\nFieldOnVerticesSynchronizer, classes that will be described next.\n\nListing 5.2 \u2013 class GridDivider\n\n1 class GridDivider {\n2 public:\n3 GridDivider(){}\n4\n\n5 GridDividerOutputPtr divide( GridDataPtr gridData, VertexPoolerPtr vertexPooler\n6 = VertexPoolerPtr( new UnweightedVertexPooler ) );\n7 GridDividerOutputPtr divide();\n8\n\n9 virtual ~GridDivider(){}\n10\n\n11 private:\n12 // ... private methods\n13 };\n14\n\n15 typedef SharedPointer&lt;GridDivider > GridDividerPtr;\n\nFieldOnVerticesSynchronizer, listed in Listing 5.3, is a class\nwhose purpose is updating values at ghost nodes. It can update any type\nof field defined in EFVLib: fields of scalars, vectors, vectors of vectors\n(they are not necessarily a matrix because the size of the vectors may\nnot be the same), or symmetric tensors. Other types of fields can also be\nupdated as long as the field type is properly serialized (class serialization\n\n\n\nTHE PROPOSED APPROACH 59\n\nis described in [5]). The class FieldOnVerticesSynchronizer uses\na structure called SynchronizerVerticesVectors, which is an out-\nput of the grid division. The function that updates a field is the function\nsynchronize. All processors must call this function passing as param-\neter the local field to be updated. It was used in the function an opti-\nmization provided by Boost that consist on the separation of the structure\nof a vector from its content [5]. The structure of the vectors that will be\ncommunicated using MPI methods is broadcast to the processors when\nthe class FieldOnVerticesSynchronizer is instantiated. When the\nfunction synchronize is called later, each processor already knows the\nstructure of the MPI package that will be received, not being necessary the\nallocation of more memory than what is needed.\n\nListing 5.3 \u2013 class FieldOnVerticesSynchronizer\n\n1 template&lt;class _FieldType >\n2 class FieldOnVerticesSynchronizer{\n3 public:\n4 // ... some typedefs\n5\n\n6 FieldOnVerticesSynchronizer( SynchronizerVerticesVectorsPtr\n7 synchronizerVerticesVectors );\n8\n\n9 void synchronize( FieldOnVerticesPtr field );\n10\n\n11 virtual ~FieldOnVerticesSynchronizer(){}\n12\n\n13 protected:\n14 // ... protected attributes\n15\n\n16 private:\n17 // ... private methods\n18 };\n\nFieldDivider is a class intended for scattering a global field into lo-\ncal ones and to gather local fields into a global one. The scatter operation\nis usually performed at the beginning of a simulation when the master\nprocessor have read the initial conditions and must spread the data to the\nlocal subdomains. The gather operation on the other hand is necessary\nto collect and export results using the master processor. There are two\nfunctions for scattering (lines 8 and 9 of Listing 5.4) and two functions for\n\n\n\n60 THE PROPOSED APPROACH\n\ngathering (lines 11 and 12 of Listing 5.4). The ones with two arguments are\nintended to be called only by the master processor, which is the only pro-\ncessor that has access to the global field. The ones with a single argument\nare called by the other processors.\n\nListing 5.4 \u2013 class FieldDivider\n\n1 template&lt;class _EntityType, class _FieldType >\n2 class FieldDivider{\n3 public:\n4 // ... some typedefs\n5\n\n6 FieldDivider( IntVector2DPtr entitiesOfSubdomains);\n7\n\n8 void scatter( FieldTypeVectorPtr global, FieldTypeVectorPtr local );\n9 void scatter( FieldTypeVectorPtr local );\n\n10\n\n11 void gather( FieldTypeVectorPtr local, FieldTypeVectorPtr global );\n12 void gather( FieldTypeVectorPtr local );\n13\n\n14 virtual ~FieldDivider(){}\n15\n\n16 protected:\n17 // ... protected attributes\n18\n\n19 private:\n20 // ... private methods\n21 };\n\nAn example of a simple program that solves a two-dimensional heat\ntransfer problem using EFVLib is presented in Appendix B.\n\n\n\nCHAPTER\n\n6\nExperimental Environment and\n\nResults\n\nThis chapter presents results obtained from the new parallel versions of\nUTCHEM and EFVLib. The parallel version of UTCHEM will be called\nUTCHEMP to distinguish it to its previous serial version. UTCHEMP is a\nreservoir simulator that have many models which enables the simulation\nof very complex fluid flow phenomena in reservoirs. It is necessary thus to\nvalidate UTCHEMP against several different cases to guarantee that all of\nits features are working as expected. Four cases are presented here. Each\none of them explores different models implemented in the simulator.\n\nThe cases used to evaluate UTCHEMP were run in the TACC-Lonestar\ncluster [3], a cluster from The University of Texas at Austin. It has a total\nof 1,888 computer nodes, each one with 12 cores. The clock frequency of\nthe cores is 3.33 GHz and each node has 24 Gb of RAM memory available.\nThe compiler used is an Intel Fortran, with the optimization flag O3. A\nsingle cluster node was used in the cases up to 8 processors and several\nnodes with 8 processors per node in the cases with more than 8 proces-\nsors. The evaluation of UTCHEMP started with its validation against some\n\n61\n\n\n\n62 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nof its benchmark cases. The number of processors that was possible to\nbe employed was small because of the small number of grid blocks. It\nwas used in this study only 1, 2, 4, and 8 processors. The second part\nis a performance evaluation. The number of grid blocks were sharply\nincreased keeping the size of the grid blocks the same to avoid numerical\ninstabilities. The wells were rearranged in a pattern similar to the original\ncase. It is very important to emphasize that the performance evaluation\ncase is different from the validation case. However, the workflow of the\nsimulation is the same so that the results are expected to be right. Further-\nmore, some cases had their original final simulation time reduced in the\nperformance evaluation due restrictions in the TACC utilization policy,\nwhich does not permit that a simulation take more than one day to finish.\n\nThe methodology used with EFVLib was slightly different. EFVLib is\na numerical library intended to help the user to develop its own appli-\ncations. So, it is not necessary to validate the library against cases that\nare physically different. What is important is that the cases cover most\nof EFVLib\u2019s functions. Only two cases are presented here, both of them\nsimulating a two-phase, incompressible, immiscible flow. The first one\nhas a small but geometrically complex grid intended mainly to validate\nthe library, despite some performance tests were executed. The geometry\nof the second case is much more simple, although the grid has many more\nelements. Both cases were run in the CPU cluster of SINMEC, a CFD\nlaboratory from the Federal University of Santa Catarina. The cluster has\n64 computer nodes, each one with 8 Gb of RAM memory and 8 cores with\n2.00 GHz of clock frequency. The compiler used is a GCC 4.1.2.\n\n6.1 Case 1\n\nThis is a water flooding case. There are four wells injecting water and 13\nproducer wells. All wells operate according to a flow constrained condi-\ntion. Figure 6.1 illustrates the water saturation field after 800 days. The\nreservoir permeability is anisotropic and heterogeneous. The simulation\nruns for 2526 simulation days. The coarse grid used has 31 grid blocks in\ndirection x , 45 grid blocks in direction y , and three in direction z . The\ngrid block sizes are constant and equal to 100 ft in directions x and y , and\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 63\n\nTable 6.1 \u2013 Case 1 properties\n\nProperty Value\nReservoir lenght 3100 ft\nReservoir width 4500 ft\nReservoir thickness 24 ft\nCoarse grid 31x45x3 (4185)\nFine grid 200x200x5 (200000)\nNumber of components 11\nMax. number of phases 4\nPorosity 0.1371\nPermeability in x dir. from 10 to 1250 mD\nPermeability in y dir. from 20 to 2500 mD\nPermeability in z dir. 5 mD\nInitial water saturation 0.1\nInitial reservoir pressure 1500 psi\nDepth 3400 ft\nNumber of injector wells 4\nNumber of producer wells 12\nSimulation days 2526 days\n\nsize 11, 9 and 4 ft (from top to bottom) in direction z . The main properties\nof the case are shown in Table 6.1.\n\nFigure 6.1 \u2013 Water saturation after 800 simulation days of case 1\n\nThe validation of UTCHEMP against UTCHEM is based on results of\n\n\n\n64 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\naverage aqueous phase pressure, average water saturation, and average\noil production rate. UTCHEMP was run using 1, 2, 4, and 8 processors.\nThe results obtained are presented in the Figures 6.2, 6.3, and 6.4. As one\nmay see, the results are in good match.\n\nTime [day]\n0 500 1000 1500 2000 2500 3000A\n\nv\ne\n\nra\ng\n\ne\n A\n\nq\nu\n\ne\no\n\nu\ns\n\n P\nh\n\na\ns\n\ne\n P\n\nre\ns\n\ns\nu\n\nre\n [\n\np\ns\n\ni]\n\n3000\n\n3050\n\n3100\n\n3150\n\n3200\n\n3250\n\n3300\n\n3350\n\n3400\n\n3450\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.2 \u2013 Average aqueous phase pressure of case 1\n\nTime [day]\n0 500 1000 1500 2000 2500 3000\n\nW\na\n\nte\nr \n\nS\na\n\ntu\nra\n\nti\no\n\nn\n\n0.1\n\n0.15\n\n0.2\n\n0.25\n\n0.3\n\n0.35\n\n0.4\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.3 \u2013 Average water saturation of case 1\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 65\n\nTime [day]\n0 500 1000 1500 2000 2500 3000\n\nO\nil\n\n P\nro\n\nd\nu\n\nc\nti\n\no\nn\n\n R\na\n\nte\n [\n\nft\n3\n/d\n\na\ny\n\n]\n\n#104\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.4 \u2013 Overall oil production rate of case 1\n\nWith only near 4000 grid blocks, the grid chosen is too coarse for\nan appropriate performance evaluation. Following the methodology de-\nscribed in the beginning of this chapter, the grid size was increased to 200\nx 200 x 5 (200, 200, and 5 grid blocks in directions x , y , and z , respectively),\nwhich gives a total of 200,000 grid blocks. The wells were rearranged in a\nsimilar pattern. It was used 1, 2, 4, 8, 16, 32, 64, and 128 processors. Figure\n6.5 presents the total computational time required by the simulations. It\nwas reduced from almost 5 hours using a single processor to about 7 min-\nutes with 64 and 128 processors. Figure 6.6 shows the speedup, defined\nin Equation (2.1). The red dashed line is the ideal speedup, defined in\nEquation (2.2). As one may see, the computational time achieved with 64\nprocessors is almost the same achieved with 128. This is an indication that\nthe increase in performance that it is possible to have with parallel com-\nputing is saturating. The relative computational weight of non-parallel\noperations and the cost of communication between processors becomes\nhigh compared to the cost of parallel operations. In fact, when 128 pro-\ncessors are used, the division based on direction y makes each processor\nhave one or two grid blocks in this direction. Thus, the number of ghost\nnodes at each subdomain is approximately the same of the number of\nghost nodes.\n\n\n\n66 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\n12000\n\n14000\n\n16000\n\n18000\n\nFigure 6.5 \u2013 Total computational time according to the number of proces-\nsors of case 1\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\nGlobal Speedup\n\nFigure 6.6 \u2013 Global speedup of case 1\n\nTable 6.2 presents the computational time of some UTCHEMP op-\nerations. The results indicate that the initialization time grows as the\nnumber of processors increase, which is reasonable since the input data\nmust be broadcast to a larger number of processors. The computation of\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 67\n\ntransmissibilities and salinity seems to be scalable operations due to their\ntime reduction. The computation of concentrations had its time reduced\nwith the increase of the number of processors, but then starts to grow up\nagain with a large number of processors. This happens because the main\nroutine of UTCHEMP that computes concentrations is actually huge and\ncontains not only explicit grid-related operations, but also communica-\ntion in order to update variables at ghost nodes.\n\nTable 6.2 \u2013 Computational times in seconds of case 1\n\nPrcs Total Time Init. Trans. Conc. Salinity LS Solving\n\n1 16894.7 3.044 143.9 1524.3 43.81 15122.2\n2 10656.8 2.677 84.53 783.24 31.39 9718.48\n4 8050.04 2.824 58.75 446.21 26.15 7487.80\n8 5210.12 2.738 33.80 265.18 14.77 4874.41\n\n16 2418.94 5.800 15.64 137.32 6.196 2241.78\n32 925.175 10.80 7.929 97.959 2.737 798.128\n64 407.275 18.06 4.584 83.246 1.504 290.474\n\n128 376.592 35.556 3.199 109.116 1.318 202.79\n\nFigure 6.7 shows the contribution of some operations over the global\ncomputational time. As one may see, the biggest contribution is the solv-\ning of linear systems coming from the aqueous phase pressure equation\ndiscretization. Following that, there are the explicit solving of concentra-\ntion equations and the computation of transmissibilities. Since PETSc is\nused to solve systems of linear equations, this library is the main respon-\nsible for the total computational time of this case.\n\n6.2 Case 2\n\nThe second case is a gel treatment problem. The main features of this case\nare summarized in Table 6.3. The reservoir has length 1100 ft, width 1000\nft, and thickness 27 ft. It is inclined in directions x and y with dip angles\n?x = 0.02923 rad and ?y = 0.00277 rad respectively. The reservoir has\nheterogeneous permeability and porosity field: the porosity field changes\nfrom 0.083 to 0.499 whilst the permeability is 72 mD at the first and 900\nmD at the second reservoir layers. The coarse grid has 14 grid blocks in\n\n\n\n68 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc 64 Prc 128 Prc\n\nContribution on the total computational time\n\nInitialization Transmissibility Comp. Concentration Eq.\n\nSalinity Comp. Lin. Syst. Solving Other\n\nFigure 6.7 \u2013 Contribution on the total computational time of case 1\n\ndirection x , 13 grid blocks in direction y , and 2 grid blocks in direction z ,\nwhich gives a total of 364 grid blocks. There is a single injector well and\nthree producer wells. The fluid injected is water and the producers oper-\nate at constant bottom hole pressure. The flow is not isothermal and thus\nthe energy equation must be solved. Initially the reservoir is at a uniform\ntemperature of 103?F, but the fluid injected is at 68?F, which makes the\naverage temperature decrease. The case runs until 816 simulation days.\nFigure 6.8 shows the temperature field after 510 simulation days.\n\nFigures 6.9, 6.10, and 6.11 presents the results of the validation of\nthis case. As one may see, the results are in good match, indicating that\nprobably there is no bug in UTCHEMP in this case. We should always\nemphasize that results correctness is much more important than a better\nsoftware performance once of course wrong results are worthless.\n\nAfter the validation the grid size was increased to 200 x 200 x 10, giving\na total of 400,000 grid blocks, to evaluate the software parallel perfor-\nmance. The case were run with 1, 2, 4, 8, 16, 32, 64, and 128 processors.\nThe final simulation time was decreased to 400 days, otherwise using a\nsingle processor would require more than one day to finish the simula-\ntion, exceeding the maximum time allowed in TACC. Figure 6.12 shows\nthe total computational time spent by the simulations. With a single pro-\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 69\n\nFigure 6.8 \u2013 Temperature after 510 simulation days of case 2\n\nTable 6.3 \u2013 Case 2 properties\n\nProperty Value\nReservoir lenght 1100 ft\nReservoir width 1000 ft\nReservoir thickness 27 ft\nCoarse grid 14x13x2 (364)\nFine grid 200x200x10 (400000)\nNumber of components 13\nPorosity from 0.083 to 0.499\nPermeability in x dir. first layer 72 mD, second layer 900 mD\nPermeability in y dir. first layer 72 mD, second layer 900 mD\nPermeability in z dir. first layer 72 mD, second layer 900 mD\nInitial water saturation 0.3001\nInitial reservoir pressure 2000 psi at 1300 ft\nDepth 1300 ft\nNumber of injector wells 1\nNumber of producer wells 3\nSimulation days 816 days\n\ncessor the simulation took about half a day to finish, while with 128 it was\nonly about 20 minutes. Figure 6.13 presents the speedup of this case. It is\ninteresting to note that up to 32 processors the results are similar to what\nis predicted by the Gustafson-Barsis\u2019s Law, discussed in section 2.5, but\nthen the speedup starts to saturate, as predicted by Amdahl\u2019s Law.\n\n\n\n70 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nTime [day]\n0 200 400 600 800 1000A\n\nv\ne\n\nra\ng\n\ne\n A\n\nq\nu\n\ne\no\n\nu\ns\n\n P\nh\n\na\ns\n\ne\n P\n\nre\ns\n\ns\nu\n\nre\n [\n\np\ns\n\ni]\n\n2250\n\n2300\n\n2350\n\n2400\n\n2450\n\n2500\n\n2550\n\n2600\n\n2650\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.9 \u2013 Average aqueous phase pressure of case 2\n\nTime [day]\n0 200 400 600 800 1000\n\nW\na\n\nte\nr \n\nS\na\n\ntu\nra\n\nti\no\n\nn\n\n0.3\n\n0.35\n\n0.4\n\n0.45\n\n0.5\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.10 \u2013 Average water saturation of case 2\n\nTable 6.4 has the computational time values obtained at the main op-\nerations executed by the simulator. The results are similar to the last case.\nAs the number of processors increase, the initialization time increase,\nbut the other times decrease. It was not noted increasing in the time to\ncompute concentrations, but its decreasing is not so sharp as the other\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 71\n\nTime [day]\n0 200 400 600 800 1000\n\nO\nil\n\n P\nro\n\nd\nu\n\nc\nti\n\no\nn\n\n R\na\n\nte\n [\n\nft\n3\n/d\n\na\ny\n\n]\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\n4000\n\n4500\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.11 \u2013 Overall oil production rate of case 2\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n#104\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\nFigure 6.12 \u2013 Computational time according to the number of processors\nof case 2\n\nvariables. Furthermore, the grid is more refined and as a consequence\nthe cost of the grid-related operations is higher. The grid operations in\nthe main routine that computes concentrations are explicit and thus its\nscalability is higher.\n\n\n\n72 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\nGlobal Speedup\n\nFigure 6.13 \u2013 Global speedup of case 2\n\nTable 6.4 \u2013 Computational times in seconds of case 2\n\nPrcs Total Time Init. Trans. Conc. Salinity LS Solving\n\n1 41196.65 4.833 466.2 4703.7 82.00 35432.4\n2 26555.64 4.525 296.0 2574.4 57.94 23339.6\n4 20290.73 4.264 225.6 1590.7 47.97 18242.7\n8 12824.40 4.252 114.1 1043.8 28.79 11521.7\n\n16 6056.945 7.186 49.79 514.05 13.73 5420.73\n32 2839.301 12.05 24.75 288.88 6.186 2480.31\n64 1688.573 17.69 14.74 272.75 3.218 1353.55\n\n128 1222.323 38.79 9.253 186.51 2.699 950.441\n\nFigure 6.14 shows the contribution of the main routines on the total\ncomputational time. The results are close to the ones of the last case.\nAgain, the solving of the system of linear equations is by far the most time-\nconsuming operation. This operation however is executed by an external\nsolver (PETSc [4]) and as a consequence we do not have much control on\nit. The speedup curve of the linear system solving is not presented here\nbecause it is very similar to the global speedup.\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 73\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc 64 Prc 128 Prc\n\nContribution on the total computation time\n\nInitialization Transmissibility Comp. Concentration Eq.\n\nSalinity Comp. Lin. Syst. Solving Other\n\nFigure 6.14 \u2013 Contribution on the total computational time of case 2\n\n6.3 Case 3\n\nThe third case is a polymer flooding. There is a single injector well sur-\nrounded by four producer wells, as illustrated in Figure 6.15. It is injected\na mixture of water, polymer, chloride, and calcium at a constant flow rate.\nThe producer wells operate with constant bottom hole pressure. The reser-\nvoir has length 1640.5 ft, width 1640.5 ft, and thickness 10.8 ft. The coarse\ngrid is 15x15x3, given a total of 675 grid blocks. Permeability and porosity\nobey a 3D stochastic distribution. Initially the aqueous phase pressure\nand saturation are uniform and have values of 100 psi and 0.38, respec-\ntively. The simulation runs until 1500 simulation days. Table 6.5 shows\nthe main characteristics of this case.\n\nSimilarly to the previous cases, this case was validated against an\nolder serial version of the simulator. The results are presented in Fig-\nures 6.16, 6.17, and 6.18. Again, it was used 1, 2, 4, and 8 processors in\nUTCHEMP and the variables compared are the aqueous phase pressure,\naqueous phase saturation, and the oil production rate. The results are in\ngood match.\n\nFor performance evaluation, the grid size was increased to 500x500x4,\nwhich gives a total of 1,000,000 grid blocks. The wells were placed in a\nsimilar pattern. It was used 1, 2, 4, 8, 16, 32, 64, 128, and 256 processors.\n\n\n\n74 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nFigure 6.15 \u2013 Case 3 aqueous phase saturation after 600 simulation days\n\nTable 6.5 \u2013 Case 3 properties\n\nProperty Value\nReservoir lenght 1,640.5 ft\nReservoir width 1,640.5 ft\nReservoir thickness 10.8 ft\nCoarse grid 15x15x3 (675)\nFine grid 500x500x4 (1000000)\nNumber of components 9\nPorosity from 0.184 to 0.416\nPermeability in x dir. 477.83 to 3,423.3 mD\nPermeability in y dir. equal to perm. in x dir.\nPermeability in z dir. 762.79 to 3,025.8 mD\nInitial water saturation 0.38\nInitial reservoir pressure 3000 psi\nDepth 2000 ft\nNumber of injector wells 1\nNumber of producer wells 4\nSimulation days 1500 days\n\nThe final simulation time was reduced from 1500 days to only 50 days,\notherwise the simulation running with small number of processor would\ntake much more than one day to finish, which is the maximum simulation\ntime allowed in TACC. Figure 6.19 shows the results of the total computa-\ntional time, while Figure 6.20 presents the corresponding speedup. The\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 75\n\nTime [day]\n0 500 1000 1500A\n\nv\ne\n\nra\ng\n\ne\n A\n\nq\nu\n\ne\no\n\nu\ns\n\n P\nh\n\na\ns\n\ne\n P\n\nre\ns\n\ns\nu\n\nre\n [\n\np\ns\n\ni]\n\n2550\n\n2560\n\n2570\n\n2580\n\n2590\n\n2600\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.16 \u2013 Average aqueous phase pressure of case 3\n\nTime [day]\n0 500 1000 1500\n\nW\na\n\nte\nr \n\nS\na\n\ntu\nra\n\nti\no\n\nn\n\n0.38\n\n0.39\n\n0.4\n\n0.41\n\n0.42\n\n0.43\n\n0.44\n\n0.45\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.17 \u2013 Average water saturation of case 3\n\ncomputational time could be reduced up to 130 times, from almost half\na day running with a single processor to about five minutes running with\n256 processors. As the previous case, the speedup is similar to what was\npredicted by the Gustafson-Barsis\u2019s Law. In fact, with up to 8 processors\nonly a single cluster node is used and thus a single memory module is\n\n\n\n76 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nTime [day]\n0 500 1000 1500\n\nO\nil\n\n P\nro\n\nd\nu\n\nc\nti\n\no\nn\n\n R\na\n\nte\n [\n\nft\n3\n/d\n\na\ny\n\n]\n\n260\n\n280\n\n300\n\n320\n\n340\n\n360\n\n380\n\n400\n\n420\n\n440\n\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.18 \u2013 Overall oil production rate of case 3\n\nused. With more processors, more nodes are used and consequently more\nmemory is also available, contributing to make the simulation faster.\n\nNumber of Processors\n0 50 100 150 200 250 300\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\nl \nT\n\nim\ne\n [\n\ns\n]\n\n#104\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\n3.5\n\n4\n\n4.5\n\nFigure 6.19 \u2013 Computational time according to the number of processors\nof case 3\n\nIn Table 6.6 the computational time of the main operations are pre-\nsented. On the other hand, the contributions on the total computational\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 77\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\nd\n\nu\np\n\n100\n\n101\n\n102\n\n103\nGlobal Speedup\n\nFigure 6.20 \u2013 Global speedup of case 3\n\ntime of those operations are shown in Figure 6.16. As before, the bot-\ntleneck is still solving the system of linear equations. It is interesting to\nnote that the drop on the computational time of this operation is relatively\nsmall with up to 8 processors and is sharp with more than 8 processors.\nFurthermore, as in the previous cases the initialization cost increase as the\nnumber of processor increases. With 256 processors, the time required to\nmake the initialization is about 20% of the total time.\n\nTable 6.6 \u2013 Computational times in seconds of case 3\n\nPrcs Total Time Init. Trans. Conc. Salinity LS Solving\n\n1 42044.4 11.79 949.7 4065.5 93.77 36533.2\n2 29265.9 10.94 643.3 2133.1 66.39 26190.2\n4 24969.1 11.70 564.9 1403.8 57.77 22780.8\n8 21157.3 10.35 273.2 844.32 33.58 19899.1\n\n16 9733.43 10.74 97.59 408.47 15.42 9157.53\n32 3148.89 16.60 39.36 194.72 7.101 2869.94\n64 1515.85 27.48 18.97 96.641 3.178 1329.86\n\n128 787.085 49.10 9.603 63.000 1.653 653.203\n256 321.943 61.46 6.022 55.234 1.236 185.550\n\n\n\n78 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc 64 Prc 128 Prc 256 Prc\n\nContribution on the total computational time\n\nInitialization Transmissibility Comp. Concentration Eq.\n\nSalinity Comp. Lin. Syst. Solving Other\n\nFigure 6.21 \u2013 Contribution on the total computational time of case 3\n\n6.4 Case 4\n\nThe fourth case is an aquifer recovering simulation. The reservoir initial\nwater and oil saturations are uniform and their values are 0.65 and 0.35,\nrespectively. There are an injector and a producer wells that are placed\naccording to the classic five-spot problem: one well at bottom left corner\nand the other one at the top right corner, as described in Figure 6.22. A\nmixture of water, surfactant, and polymer, which forms a microemulsion\nphase, is injected, helping to move the oil out of the reservoir. The reser-\nvoir has dimensions 250 x 250 x 10 ft and the coarse grid used is 11 x 11\nx 2 (242 grid blocks). The reservoir is also homogeneous with porosity\n0.20, x and y permeabilities 500 mD, and z permeability 50 mD. Table 6.7\npresents the main features of the case.\n\nFigures 6.23, 6.24, and 6.25 shows the validation results for this case.\nDifferently from the other cases, the validation results differ. However,\nFigure 6.25 reveals that the oil production rate is quite unstable and thus a\nsmall difference on computations may lead to a relatively large difference\non the final results. In fact, the code was extensively debugged and no\nerrors were found. It was verified that even if the linear system is exactly\nthe same, the results may differ a little bit depending on the number of\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 79\n\nFigure 6.22 \u2013 Microemulsion phase saturation after 350 simulation days\nof case 4\n\nTable 6.7 \u2013 Case 4 properties\n\nProperty Value\nReservoir lenght 250 ft\nReservoir width 250 ft\nReservoir thickness 10 ft\nCoarse grid 11x11x2 (242)\nFine grid 400x500x10 (2000000)\nNumber of components 11\nMax. number of phases 3\nPorosity 0.20\nPermeability in x dir. 500 mD\nPermeability in y dir. 500 mD.\nPermeability in z dir. 50 mD\nInitial water saturation 0.65\nInitial reservoir pressure 2500 psi\nDepth 1000 ft\nNumber of injector wells 1\nNumber of producer wells 1\nSimulation days 1500 days\n\nprocessors used. This fact combined with some instabilities probably\nexplain why the results are a little different. Despite the results being a\nlittle different, it is worth noting that all validation results follow a similar\n\n\n\n80 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\npattern, indicating that the code is probably right.\n\nTime [day]\n0 500 1000 1500A\n\nv\ne\n\nra\ng\n\ne\n A\n\nq\nu\n\ne\no\n\nu\ns\n\n P\nh\n\na\ns\n\ne\n P\n\nre\ns\n\ns\nu\n\nre\n [\n\np\ns\n\ni]\n\n2040\n\n2060\n\n2080\n\n2100\n\n2120\n\n2140\n\n2160\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.23 \u2013 Average aqueous phase pressure of case 4\n\nTime [day]\n0 500 1000 1500\n\nW\na\n\nte\nr \n\nS\na\n\ntu\nra\n\nti\no\n\nn\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.24 \u2013 Average water saturation of case 4\n\nStill referencing Figure 6.25, the oil production is close to zero at the\nbeginning of the simulation due to the small relative permeability of phase\noil. Its value is about 10?4, while the relative permeability of the aqueous\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 81\n\nTime [day]\n0 500 1000 1500\n\nO\nil\n\n P\nro\n\nd\nu\n\nc\nti\n\no\nn\n\n R\na\n\nte\n [\n\nft\n3\n/d\n\na\ny\n\n]\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\nUTCHEM\nUTCHEMP - 1 prc\nUTCHEMP - 2 prc\nUTCHEMP - 4 prc\nUTCHEMP - 8 prc\n\nFigure 6.25 \u2013 Overall oil production rate of case 4\n\nphase is about 10?1. As a consequence, the oil mobility is small and thus\nthe oil being produced, which is calculated using Equation (5.11), is also\nsmall. Another aspect of Figure 6.25 that may seem strange is the oil flow\nrate dropping to zero at about 700 days. What happened is that phase\noil starts to be considered a microemulsion phase because the surfactant\nconcentration surpass a predefined value.\n\nThe grid used to evaluate UTCHEMP\u2019s performance in this case has\n2,000,000 (400x500x10) grid blocks. The number of processors used is 1, 2,\n4, 8, 16, 32, 64, 128, and 256. As in the previous case, the final simulation\ntime was reduced to make possible the running of the cases with small\nnumber of processors. The total computational time decreased from ap-\nproximately 2,5 hours with one processors to only about 4 minutes with\n256 processors, as one may see in Figure 6.26. The speedup is plotted in\nFigure 6.27. Again, the speedup is similar to what Gustafson-Barsis\u2019s Law\npredicts, but in this case it saturates with large number of processors, as\npredicted by the Amdahl\u2019s Law.\n\nTable 6.8 presents the computational time of some UTCHEMP\"s main\noperations, whilst Figure 6.28 shows their contribution on the total com-\nputational time. The solving of the system of linear equations is the main\ncontributor to the computational time only when not so many processors\nare used. With 256 processors, the initialization time, which involves op-\n\n\n\n82 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n0 50 100 150 200 250 300\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\nl \nT\n\nim\ne\n [\n\ns\n]\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\n10000\n\nFigure 6.26 \u2013 Computational time according to the number of processors\nof case 4\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\nd\n\nu\np\n\n100\n\n101\n\n102\n\n103\nGlobal Speedup\n\nFigure 6.27 \u2013 Global speedup of case 4\n\nerations such as reading, transferring the input data, and setting up the\nreservoir initial state, takes most of the simulation time. This is actually\nnatural, since there is much communication in this process and the num-\nber of time steps is relatively small.\n\nThis case was also used to evaluate inactive grid blocks. As described\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 83\n\nTable 6.8 \u2013 Computational times in seconds of case 4\n\nPrcs Total Time Init. Trans. Conc. Salinity LS Solving\n\n1 9086.82 68.93 151.0 1268.0 73.36 7405.0\n2 8140.34 66.23 95.64 684.55 41.23 7178.1\n4 6448.90 67.88 81.49 462.36 31.10 5751.9\n8 3585.60 67.56 45.07 287.22 20.27 3132.9\n\n16 2354.57 63.72 22.11 143.65 9.256 2099.3\n32 1143.43 71.43 8.592 77.279 4.488 973.84\n64 609.544 79.13 3.892 45.771 2.318 474.68\n\n128 370.061 90.65 2.237 30.796 1.259 242.94\n256 245.197 120.9 1.290 24.866 0.818 95.585\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc 64 Prc 128 Prc 256 Prc\n\nContribution on the total computational time\n\nInitialization Transmissibility Comp. Concentration Eq.\n\nSalinity Comp. Lin. Syst. Solving Other\n\nFigure 6.28 \u2013 Contribution on the total computational time of case 4\n\nin section 5.1.2, inactive grid blocks may be used to more accurately rep-\nresent the reservoir geometry. They behave like impermeable boundaries\nand thus no flux through them should occur. In this case eight grid blocks\nat the middle of the reservoir was set as inactive and as a consequence\nthe fluids should flow around the inactive cells. Figure 6.29 shows the mi-\ncroemulsion phase saturation after 355 simulation days using the coarse\ngrid. As expected, the microemulsion saturation at inactive grid blocks\n\n\n\n84 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nremained null, indicating that in fact there is no flow through them.\n\nFigure 6.29 \u2013 Microemulsion phase saturation after 355 simulation days\nusing inactive cells of case 4\n\n6.5 Case 5\n\nThe fifth case is a two-phase problem solved using EFVLib. The grid is hy-\nbrid, unstructured, and it is nearly cylindrical around the wells. There are\n8073 nodes, 35286 tetrahedra, 544 hexahedra, 544 prims, and 576 pyra-\nmids. Its shape is similar to Brazil\u2019s territory, as illustrated in Figure 6.30.\nThere are two horizontal producer wells and three vertical injector wells.\nThe injectors injects a total of 50 ft3/day. The domain is supposed homo-\ngeneous and isotropic with permeability 40 mD and porosity 0.1. Table\n6.9 summarizes the case properties.\n\nBoth water and oil are considered incompressible and immiscible.\nThe problem is modelled by the water mass conservation equation\n\n?\n\n? t\n\n?\n\n?Sw\n?\n\n=?\u00b7\n?\n\n?wK(?P ??w?h)\n?\n\n+q ???w , (6.1)\n\nby the global mass conservation equation\n\n?\u00b7\n?\n\n?TK(?P ??T ?h)\n?\n\n+q ???T = 0, (6.2)\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 85\n\nFigure 6.30 \u2013 Illustration of the grid and wells of case 5\n\nTable 6.9 \u2013 Case 5 properties\n\nProperty Value\nReservoir lenght 197 ft\nReservoir width 205 ft\nReservoir thickness 132 ft\nGrid nodes 8073\nTetrahedra 35286\nHexahedra 544\nPrims 544\nPyramids 576\nNumber of phases 2\nNumber of components 2\nPorosity 0.10\nPermeability 40 mD\nInitial oil saturation 1.0\nNumber of injector wells 3\nNumber of producer wells 2\nSimulation days 200 days\n\nand by the continuity of the fluid saturations\n\nSw +So = 1 (6.3)\n\n\n\n86 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nThere are three equations and three unknowns (P , Sw , and So ). Since the\nmobility ? depends on the phase saturations, the problem is nonlinear.\nIn order to solve the above equations, it is used an IMPES method: the\nglobal mass conservation equation is solved implicitly to obtain the pres-\nsure field and then the saturations are computed explicitly with the other\ntwo remaining equations. This method however has stability constraints.\nFor this case, it was necessary a time step of 0.001 day, which required\n200000 iterations to reach 200 days. Once the time step is really small,\nthe saturation field barely changes from a simulation time to another and\nthus nonlinearity effects are negligible. In Figure 6.31 it is illustrated the\nwater saturation field after 170 days.\n\nFigure 6.31 \u2013 Saturation field after 170 days of case 5\n\nThe case was run with 1, 2, 4, 8, 16, and 32 processors. In Figure 6.32\nit is illustrated the division of the grid according to the number of pro-\ncessors. All nodes within a same subdomain have same color. It is worth\nnoting that a subdomain may be disconnected, as it happens when four\nprocessors are used. This however is not a problem since the method-\nology using ghost nodes is quite general and does not require connected\nsubdomains.\n\nThe average values of pressure, water saturation, and oil production\nrate are plotted in Figures 6.33, 6.34, and 6.35. As one may note, the re-\nsults match, which is strictly necessary to a parallel simulator. Figure 6.36\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 87\n\n(a) 1 processors (b) 2 processors\n\n(c) 4 processors (d) 8 processors\n\n(e) 16 processors (f ) 32 processors\n\nFigure 6.32 \u2013 Grid division of case 5 using different number of processors\n\n\n\n88 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nshows the total computational time spent by the simulation and Figure\n6.37, the corresponding speedup. The speedup is almost linear using up\nto 8 processors. If more processors are employed it will be required to use\nmore than one cluster\u2019s node, thus increasing the communication cost.\nThe current case is actually very small. When for example 32 processors\nare used, each processor handles only about 250 nodes. Despite being\nsmall, this case shows that it is possible to achieve a good speedup even\nwhen the case is small.\n\nTime [day]\n0 50 100 150 200\n\nA\nv\n\ne\nra\n\ng\ne\n\n P\nre\n\ns\ns\n\nu\nre\n\n [\np\n\ns\ni]\n\n3600\n\n3800\n\n4000\n\n4200\n\n4400\n\n4600\n\n4800\n\n5000\n\n5200\n\n5400\n\n1 prc\n2 prcs\n4 prcs\n8 prcs\n16 prc\n32 prc\n\nFigure 6.33 \u2013 Average pressure of case 5\n\nFigure 6.38 shows the contribution of some operations on the total\ncomputational time for different number of processors. Since there is a\nlot of time steps along the simulation (200000), the contribution of read-\ning, dividing, and assembling the grid on the total computational time is\nnegligible. The main computational cost comes from the computation\nof water saturation and pressure. In fact, the contribution of pressure\ncomputation grows as the number of processors increase, while the con-\ntribution of water saturation computation decrease. This is reasonable,\nbecause the computation of pressure is implicit \u2013 a linear system is solved\nto obtain the field \u2013 and thus data must be exchanged between the pro-\ncessors. Furthermore, the computation of the water saturation is explicit.\nAll of the processors have the data they need to compute the new water\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 89\n\nTime [day]\n0 50 100 150 200\n\nA\nv\n\ne\nra\n\ng\ne\n\n W\na\n\nte\nr \n\nS\na\n\ntu\nra\n\nti\no\n\nn\n\n0\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.1\n\n0.12\n\n0.14\n\n0.16\n\n1 prc\n2 prcs\n4 prcs\n8 prcs\n16 prcs\n32 prcs\n\nFigure 6.34 \u2013 Average water saturation of case 5\n\nTime [day]\n0 50 100 150 200\n\nA\nv\n\ne\nra\n\ng\ne\n\n O\nil\n\n P\nro\n\nd\nu\n\nc\nti\n\no\nn\n\n R\na\n\nte\n [\n\nft\n3\n/d\n\na\ny\n\n]\n\n30\n\n35\n\n40\n\n45\n\n50\n\n1 prc\n2 prcs\n4 prcs\n8 prcs\n16 prcs\n32 prcs\n\nFigure 6.35 \u2013 Overall oil production rate of case 5\n\nsaturation field based on a new pressure field. The only data exchange\nregards the updating of such variable at the ghost nodes. Figure 6.39 and\nFigure 6.40 presents the speedup of the pressure and water saturation\ncomputation. It is clear that the former is more scalable than the latter.\nIn fact, the time spent computing the pressure field gets higher when 32\n\n\n\n90 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n0 5 10 15 20 25 30 35\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n#105\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 6.36 \u2013 Computational time according to the number of processors\nof case 5\n\nNumber of Processors\n100 101 102\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\nGlobal Speedup\n\nFigure 6.37 \u2013 Global speedup of case 5\n\nprocessors are used instead of 16 processors.\n\nAlthough in the present case the time spent reading, dividing, and\nassembling the grid is negligible, it might be relevant if the number of\ntime steps were much smaller. Figure 6.41 shows the computational time\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 91\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc\n\nContribution on the total computational time\n\nGrid reading Grid assembling Grid division\n\nPressure computing Water saturation Other\n\nFigure 6.38 \u2013 Contribution on the total computational time of case 5\n\nNumber of Processors\n100 101 102\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\nPressure Computation Speedup\n\nFigure 6.39 \u2013 Pressure computation speedup of case 5\n\naccording to the number of processors. The cost of reading a grid from\na file is always the same because such operation is executed by a single\nprocessor, no matter the total number of processors allocated. Further-\nmore, when a single processor is used there is no cost dividing the grid, but\n\n\n\n92 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n100 101 102\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\nWater Saturation Computation Speedup\n\nFigure 6.40 \u2013 Water saturation computation speedup of case 5\n\nthe cost for assembling is higher because the processor must work on the\nwhole grid. As the number of processors is increased, the assembling cost\ndecreases, but the division cost increases. The optimal point happens\nwhen 16 processors are used.\n\nNumber of Processors\n0 5 10 15 20 25 30 35\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nReading\nDivision\nAssembling\nTotal\n\nFigure 6.41 \u2013 Computational time of some grid operations of case 5\n\nThe quality of the grid division was also evaluated for the present\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 93\n\ncase. It was noted that the load balance is an important parameter to\noptimize performance. All processors should have almost the same num-\nber of nodes, implying that the number of nodes should be the global\nnumber of nodes divided by the number of processors. This however is\nan ideal situation. Assuming the parallel architecture is symmetric, the\nperformance is ruled by the overloaded processor. The ratio between\nthe surplus number of nodes and the ideal number of nodes was used to\nquantify the load balance. The surplus number of nodes is here defined as\nthe difference between the number of local nodes and the ideal number\nof nodes. There is a different ratio for each processor, but the ratio that\nreally matters is the maximum. The variation of the maximum ratio are\nplotted in Figure 6.42. Note that all values are bellow 3%, which is a nice\nresult since it indicates that a performance slow down by load inbalance\nalso should be at most 3%.\n\nNumber of Processors\n0 5 10 15 20 25 30 35\n\nM\na\nx\nim\n\nu\nm\n\n R\na\nti\n\no\n [\n\n%\n]\n\n2.3\n\n2.4\n\n2.5\n\n2.6\n\n2.7\n\n2.8\n\n2.9\n\n3\n\nRatio between exceeding nodes and\n ideal number of nodes\n\nFigure 6.42 \u2013 Maximum ratio between the surplus number of nodes and\nthe ideal number of nodes of case 5\n\nAnother aspect concerning the grid division quality is the communi-\ncation. The grid division should minimize the size of subdomains\u2019s inter-\nfaces in order to communicate less data. It is hard however to evaluate if\nthe division is good because we do not know what is the optimal point.\nActually, even the concept of interface size is abstract. In this study is\nstraightforward to define the interface\u2019s size as the number of ghost nodes.\nFrom the communication point of view, the quality of the grid division\n\n\n\n94 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nis as good as smaller is the ratio between the ghost and the local nodes.\nFigure 6.43 shows the average ratio obtained using different number of\nprocessors. It is clear that as the number of processors increases, the\ndivision quality becomes poor. This helps to explain why the speedup\nresults are not good for more than 16 processors. Nevertheless, it does\nnot mean that the division quality can be improved because we do not\nknow what is the optimal ratio.\n\nNumber of Processors\n0 5 10 15 20 25 30 35\n\nA\nv\ne\nra\n\ng\ne\n R\n\na\nti\n\no\n [\n\n%\n]\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\nRatio between ghost nodes and local nodes\n\nFigure 6.43 \u2013 Average ratio between the number of ghost nodes and the\nnumber of local nodes of case 5\n\n6.6 Case 6\n\nThis case is similar to the last problem considered. A two-phase, incom-\npressible, immiscible model was implemented using EbFVM and IMPES.\nHowever, the grid used here is more refined. It has 1971750 tetrahedra\nand 378259 nodes. There are two vertical well: one producer and one\ninjector, as illustrated in Figure 6.44. We could of course run an even\nbigger case, but such case would probably require more memory than\nis available when a single processor is used. The properties of this case\nare similar to the last cases\u2019s and are summarized in Table 6.10. In this\ncase however there is no validation step because the physical model is\nthe same of the last problem.\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 95\n\nFigure 6.44 \u2013 Grid and wells of case 6\n\nTable 6.10 \u2013 Case 6 properties\n\nProperty Value\nReservoir lenght 1000 ft\nReservoir width 500 ft\nReservoir thickness 250 ft\nGrid nodes 378259\nTetrahedra 1971750\nNumber of phases 2\nNumber of components 2\nPorosity 0.10\nPermeability 10 mD\nInitial oil saturation 1.0\nNumber of injector wells 1\nNumber of producer wells 1\nNumber of time steps 100\n\nFigure 6.45 shows the results of the computational time spent by the\nwhole simulation according to the number of processors. As expected,\nthe computational time decreases smoothly as the number of processors\nused increase. The corresponding speedup is presented in Figure 6.46.\nUp to 8 processors the speedup is close to linear. With more processors\n\n\n\n96 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nthe communication cost increases and the speedup deviates from linear.\nWith 128 processors for example the speedup is only about 44. In this case\neach processor handles about 3000 nodes, which is not so many.\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\n8000\n\nFigure 6.45 \u2013 Computational time according to the number of processors\nof case 6\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\nGlobal Speedup\n\nFigure 6.46 \u2013 Global speedup of case 6\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 97\n\nFigure 6.47 shows the contribution of the main operations to the global\ncomputational time. Again, the most costly operations are the pressure\nand water saturation computation. As the number of processors increases,\nthe cost to compute the water saturation becomes smaller while the cost\nto compute the pressure is relatively the same. In fact, one may see from\nthe speedups illustrated in Figures 6.48 and 6.49 that the water saturation\ncomputation is a highly scalable operation. The computation is explicit.\nAs long as the pressure field are updated at the ghost nodes, no commu-\nnication is necessary except that for the updating of the saturation values\nat the ghost nodes. The computation of the pressure field, on the other\nhand, is implicit and thus demands the assembling and solving of a linear\nsystem. As the number of processors increase, the communication be-\ncomes more expensive and the speedup saturates. The speedups evalu-\nated in this case are actually similar to the ones of the case 5. However, the\nperformance deteriorates at a higher number of processors since the grid\nhave many more elements and thus the grid related operation contributes\nmore to the global computational time.\n\n0%\n\n10%\n\n20%\n\n30%\n\n40%\n\n50%\n\n60%\n\n70%\n\n80%\n\n90%\n\n100%\n\n1 Prc 2 Prc 4 Prc 8 Prc 16 Prc 32 Prc 64 Prc 128 Prc\n\nContribution on the total computational time\n\nGrid reading Grid assembling Grid division\n\nPressure computing Water saturation Other\n\nFigure 6.47 \u2013 Contribution on the total computational time of case 6\n\nFigure 6.50 shows the computational time associated to some of the\noperations required to build the grid computational structure. The results\nare similar to the ones obtained in the last case. The assembly cost is high\n\n\n\n98 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\nPressure Computation Speedup\n\nFigure 6.48 \u2013 Pressure computation speedup of case 6\n\nNumber of Processors\n100 101 102 103\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\n100\n\n101\n\n102\n\n103\nWater Saturation Computation Speedup\n\nFigure 6.49 \u2013 Water saturation computation speedup of case 6\n\nwith small number of processors, but it decreases sharply as this number\nincreases. With a large number of processors the operations that have\nmore influence on the computational time are the grid reading and grid\ndivision. However, there is no clear optimal point differently from the last\ncase. With 16 processors or more the computational time is almost stable.\n\n\n\nEXPERIMENTAL ENVIRONMENT AND RESULTS 99\n\nAs the number of processors increases the cost of division gets higher. The\ntrend is to have a higher total computational cost if more processors are\nused.\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nC\no\n\nm\np\n\nu\nta\n\nti\no\n\nn\na\n\nl \nT\n\nim\ne\n\n [\ns\n\n]\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nReading\nDivision\nAssembling\nTotal\n\nFigure 6.50 \u2013 Computational time of some grid operations of case 6\n\nFigures 6.51 and 6.52 present the result of the variables used to eval-\nuate the quality of the grid division. The load balance parameter is sur-\nprisingly good for small number of procesors. With two processors, it is\nalmost zero, indicating that the load balance is almost perfect. Even with\na highest number of processors the ratio is less than 3%. The communica-\ntion parameter on the other hand follows the same pattern noted in the\nlast case, although the values are much smaller than that case. With 32\nprocessors, the ratio is about 11% in this case whereas it is about 60% in\nthe other one. This of course happens because the grid in this case has\nmany more nodes than the last one\u2019s grid.\n\n\n\n100 EXPERIMENTAL ENVIRONMENT AND RESULTS\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nM\na\nx\nim\n\nu\nm\n\n R\na\nti\n\no\n [\n\n%\n]\n\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nRatio between exceeding nodes and\n ideal number of nodes\n\nFigure 6.51 \u2013 Maximum ratio between the surplus number of nodes and\nthe ideal number of nodes of case 6\n\nNumber of Processors\n0 20 40 60 80 100 120 140\n\nA\nv\ne\nra\n\ng\ne\n R\n\na\nti\n\no\n [\n\n%\n]\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\nRatio between ghost nodes and local nodes\n\nFigure 6.52 \u2013 Average ratio between the number of ghost nodes and the\nnumber local nodes of case 6\n\n\n\nCHAPTER\n\n7\nConclusions\n\n7.1 Summary\n\nThis study was developed with the objective of improving computational\nperformance of numerical reservoir simulators. The methodology to im-\nprove performance is by the application of parallel computing. Parallel\ncomputing was employed by using several processors concurrently. Each\nprocessor works on a small part of the problem. By reducing the size\nof the local problem it is possible to sharply increase the overall perfor-\nmance. The total computational time is the computational time spent by\nthe overloaded processor. Ideally the problem is divided evenly among\nthe processors and as a consequence the speedup is equal to the number\nof processors.\n\nThere are three main types of parallel architecture concerning the\nmemory arrangement: shared, distributed, and hybrid-memory comput-\ners. Shared-memory computers are commonly limited to a few num-\nber of processors. Distributed-memory computers, on the other hand,\nmay have a large number of processors, each one using a distinct mem-\nory module, but require communication between processors, which may\nslow down the performance. The code developed in this study uses the\n\n101\n\n\n\n102 CONCLUSIONS\n\nMPI paradigm and hence can be used in either distributed or hybrid-\nmemory computers.\n\nParallel computing was applied to two reservoir simulation softwares:\nUTCHEM and EFVLib. UTCHEM is a three-dimensional, multicompo-\nnent, multiphase, compositional, variable temperature, finite-difference\nreservoir simulator developed at The University of Texas at Austin. It can\nbe used to simulate enhanced recovery of oil and enhanced remediation\nof aquifers. EFVLib, on the other hand, is a library that helps its user to de-\nvelop its own codes using the Element-based Finite Volume Method. This\nlibrary has several methods that enable the user to read an unstructured\ngrid, assemble control volumes, easily access grid entities, evaluate shape\nfunctions, among other features. Despite being developed for application\non reservoir simulation, EFVLib is actually general and can be employed\nfor the development of any software that uses the EbFVM.\n\nThe idea of employing parallel computing on two softwares instead\nof just one is that in UTCHEM we deal with complex physical models but\nthe grid is structured and Cartesian. The grid treatment in EFVLib on\nthe other hand is much more complex since it deals with unstructured\ngrids. It was not necessary however treat complex physical models be-\ncause such models is the final user who implements. So, in this study\nthere was the opportunity of developing methodologies for both struc-\ntured and unstructured grids, as well as complex physical models.\n\nThe parallelization methodology employed in both cases was based\non domain decomposition. It is recognized that the main computational\ncost comes from grid related operations. Each processor works only on\na part of the global domain. In UTCHEM the domain is divided taking\nthe direction y as the reference. At the borders of each subdomain cre-\nated there are ghost cells, which are additional grid blocks used to repre-\nsent and store values from grid blocks that are in a neighbor subdomain.\nPETSc was used to solve systems of linear equations due to its efficiency\nand its parallel computing support. Additionally, it was implemented in\nUTCHEM inactive cells and a more user-friendly input file format. The\nparallel version of UTCHEM was called UTCHEMP to distinguish it from\nits serial version.\n\nThe division of the grid in EbFVM is more complex because the grid\nis unstructured and thus there is no predefined rule of how the control\nvolumes are connected to each other. The notion of control volumes (or\n\n\n\nCONCLUSIONS 103\n\nnodes) being connected to each other lead to the representation of the\ngrid as a graph. It was used an external graph partitioning library to par-\ntition the graph representation of a grid respecting the load balance and\nminimum communication criteria. Based on the graph, the local grids\nare assembled and their borders are extended to contemplate nodes that\nare in other subdomains. This nodes are called ghost nodes and their\npurpose is the same of the ghost cells used in UTCHEM: represent and\nstore values from nodes that are in another subdomain. Furthermore,\nproper methodologies were used to treat wells and boundaries. As in\nUTCHEM, PETSc was used to solve in parallel system of linear equation\nrising from the simulations.\n\n7.2 Conclusions\n\nFour cases were executed in UTCHEMP. Each one of them tries to evaluate\ndifferent physical models from the simulator. There was a good agree-\nment between the validation results of UTCHEM and UTCHEMP running\nwith different number of processors. The only exception was the forth\ncase. It is believed however that the code is correct and the difference is\ndue to some instability and to the fact that the result from the linear sys-\ntem solving is not exactly the same when different number of processors\nis used.\n\nIt was verified in UTCHEMP that usually the more expensive oper-\nation is the solution of system of linear equations. This operation is the\nmain responsible by the parallel performance achieved by the software.\nThe evolution of the speedup is not so good when a single cluster node is\nused probably because there is not much memory available. With more\nnodes the speedup curve tends to be parallel to the ideal speedup, as\npredicted by the Gustafson-Barsis\u2019s Law. This behaviour is more evident\nwhen the grid size is bigger and as a consequence more memory needs to\nbe allocated.\n\nTwo EFVLib cases were presented. In both of them it was simulated\na two phase flow. Since the physical model is the same, only the first case\nwere used to validate the software. The validations results are in good\nmatch. The first case has a small however complex grid. With 8 processors\nor more the speedup saturates. On the other hand, the second case has\n\n\n\n104 CONCLUSIONS\n\nmany more control volumes and thus it was achieved a better speedup.\nIts speedup curve is closer to the Amdahl\u2019s Law.\n\nIt was also verified in EFVLib that, as the number of processors in-\ncreases, the time require to divide the grid becomes bigger, but the time\nto assemble the grid at the same time gets smaller. There should be an\noptimal number of processors to minimize the cost to initialize a grid.\nAnother analysis was made was referred to the quality of the grid division.\nThere is two main aspects to consider: load balance and communication.\nIt was verified that the load unbalance is of at most 3%, which is relatively\nsmall. The communication was evaluated by the ratio of ghost to local\nnodes. As the number of processors increases, such ratio grow, reaching\nabout 60% in the first case. The increasing on the communication cost\nhelps to explain why in some cases the performance gaining using more\nprocessors is not what would be desired.\n\nAlthough the speedup is not so close to the ideal speedup, the per-\nformance improvement is really significant. Depending on the number\nof processors used, the software can be several times faster. In the forth\ncase for example UTCHEMP was about 130 times faster when running\nwith 256 processors. It is almost impossible to get such performance im-\nprovement simply by making the software algorithms more efficient. It is\nstrongly recommended here to take advantage of the computer architec-\ntures available nowadays. In fact, the trend is to use parallel computers\nand our codes should adapted to that.\n\n7.3 Suggestions for future studies\n\nFor future studies it is recommended:\n\n\u2022 Extend the validation and evaluation of UTCHEMP. This is a huge\nsimulator and there was not enough time to test all of its features;\n\n\u2022 Evaluate the PETSc options used in both UTCHEMP and EFVLib.\nInvestigate what is the configuration that optimizes performance;\n\n\u2022 Use ParMETIS instead of Metis to improve the division of graphs\nrepresenting grids;\n\n\u2022 Create a code interface in EFVLib to use the non-linear system solv-\ning tools available in PETSc;\n\n\n\nCONCLUSIONS 105\n\n\u2022 Evaluate EFVLib with more complex simulation models.\n\n\n\n\n\nBibliography\n\n[1] METIS: A Software Package for Partitioning Unstructured Graphs,\nPartitioning Meshes, and Computing Fill-Reducing Orderings of\nSparse Matrices. Department of Computer Science &amp; Engineering /\nUniversity of Minnesota.\n\n[2] Technical Documentation for UTCHEM-9.0: a Three-Dimensional\nChemical Flood Simulator. Reservoir Engineering Research Program,\nCenter for Petroleum and Geosystems Engineering, The University of\nTexas at Austin, Austin, Texas 78712, July, 2000.\n\n[3] Lonestar User Guide. https://portal.tacc.utexas.edu/user-\nguides/lonestar, 2015.\n\n[4] BALAY, S., BROWN, J., BUSCHELMAN, K., GROPP, W. D., KAUSHIK,\nD., KNEPLEY, M. G., MCINNES, L. C., SMITH, B. F., ZHANG, H.\nPETSc (Portable, Extensible Toolkit for Scientific Computation).\nhttp://www.mcs.anl.gov/petsc, 2013.\n\n[5] BOOST C++ LIBRARIES. boost.org , January, 2015.\n\n[6] CORDAZZO, J. Simula\u00e7\u00e3o de reservat\u00f3rios de petr\u00f3leo utilizando\no m\u00e9todo EbFVM e Multigrid alg\u00e9brico. Ph.D. Thesis, Universidade\nFederal de Santa Catarina, 2006.\n\n[7] DONGARRA, J. J. Sourcebook of Parallel Computing. Morgan\nKaufmann Publishers, 2003.\n\n[8] DOROH, M. G. Development and Application of a Parallel Compo-\nsitional Reservoir Simulator. Master\u2019s Thesis, The University of Texas\nat Austin, 2012.\n\n107\n\n\n\n108 BIBLIOGRAPHY\n\n[9] EIJKHOUT, V., CHOW, E., VAN DE GEIJN, R. Introduction to High\nPerformance Scientific Computing, 2nd edition. Creative Commons\nAttribution 3.0 Unported, Austin, Texas, 2015.\n\n[10] GEBALI, F. Algorithms and Parallel Computing. Wiley, 2011.\n\n[11] HURTADO, F. S. V. Formula\u00e7\u00e3o tridimensional de volumes fini-\ntos para simula\u00e7\u00e3o de reservat\u00f3rios de petr\u00f3leo com malhas n\u00e3o-\nestruturadas h\u00edbridas. Tese de doutorado, Departamento de Engen-\nharia Mec\u00e2nica, Universidade Federal de Santa Catarina, Florian\u00f3polis,\nBrasil, 2011.\n\n[12] KARPINSKI, L. Aplica\u00e7\u00e3o do m\u00e9todo de volumes finitos baseado\nem elementos em um simulador de reservat\u00f3rios qu\u00edmico-\ncomposicional. Master\u2019s Thesis, Universidade Federal de Santa\nCatarina, 2011.\n\n[13] KARYPIS, G., KUMAR, V. A Fast and High Quality Multilevel Scheme\nfor Partitioning Irregular Graphs. SIAM Journal on Scientific Comput-\ning , v. 20, pp. 359 \u2013 392, 1998.\n\n[14] KARYPIS, G., KUMAR, V. Multilevel Algorithms for Multi-Constraint\nGraph Partitioning. Technical report, University of Minnesota, De-\npartment of Computer Science / Army HPC Research Center, 1998.\n\n[15] KARYPIS, G., KUMAR, V. Multilevel k-way Partitioning Scheme for\nIrregular Graphs. Journal of Parallel and Distributed Computing , v. 48,\npp. 96 \u2013 129, 1998.\n\n[16] MALISKA, C. R. Transfer\u00eancia de Calor e Mec\u00e2nica dos Fluidos\nComputacional. LTC Editora, Rio de Janeiro, RJ, 2004.\n\n[17] MALISKA, C. R., DA SILVA, A. F. C., HURTADO, F. S. V., DONATTI,\nC. N., AMBRUS, J. Especifica\u00e7\u00e3o e planejamento da biblioteca EFVLib.\nRelat\u00f3rio t\u00e9cnico SINMEC/SIGER I-02, Parte 2, Departamento de En-\ngenharia Mec\u00e2nica. Technical report, Universidade Federal de Santa\nCatarina, 2008.\n\n[18] MALISKA, C. R., SILVA, A. F. C., HURTADO, F. S. V., DONATTI, C. N.,\nPESCADOR JR., A. V. B., RIBEIRO, G. G. Manual de classes da biblioteca\nEFVLib. Relat\u00f3rio t\u00e9cnico SINMEC/SIGER I-06, Parte 1, Departamento\n\n\n\nBIBLIOGRAPHY 109\n\nde Engenharia Mec\u00e2nica, Universidade Federal de Santa Catarina,\n2011.\n\n[19] MALISKA, C. R., DA SILVA, A. F. C., HURTADO, F. S. V., RIBEIRO,\nG. G., JR, A. A. V. B. P., CERBATO, G., GREIN, E. A. Comparativo geral\ndos m\u00e9todos de discretiza\u00e7\u00e3o e estudos sobre tratamentos de po\u00e7os e\ncomputa\u00e7\u00e3o paralela, Relat\u00f3rio T\u00e9cnico SINMEC/SIGER II. Technical\nreport 04, SINMEC/EMC/UFSC, Florian\u00f3polis, SC, 2013.\n\n[20] PACS TRAINING GROUP. Introduction to MPI. National Center\nfor Supercomputing Applications (NCSA) located at the University of\nIllinois at Urbana-Champaign, 2001.\n\n[21] PADUA, D., editor. Encyclopedia of Parallel Computing. Springer,\n2011.\n\n[22] SHANKLAND, S. Keeping Moore\u2019s Law ticking.\nhttp://www.cnet.com/ , October, 2012.\n\n[23] TROBEC, R., VAJTERSIC, M., ZINTERHOF, P. Parallel Computing :\nNumerics, Applications, and Trends. Springer, 2009.\n\n[24] ZIENKIEWICZ, O. C., TAYLOR, R. L. The Finite Element Method,\nfifth, vol. 1. Butterworth-Heinemann, 2000.\n\n\n\n\n\nAPPENDIX\n\nA\nUTCHEM's Input File\n\nThe IPARS framework enables the implementation of a new and much\nmore flexible format for the input files. To each variable that needs data\nfrom the input file it is associated a keyword. The keyword may be placed\nin whichever position in the input file. This is different from the original\nUTCHEM\u2019s input file format, that supported only placing each variable in\nits own \u2013 and single \u2013 place. If the datum was misplaced, the simulation\ncould not be executed or, in the worst case, the datum could be assigned\nto a wrong variable leading to wrong results.\n\nThe variables read from the input file are separated in scalars and\narrays. Datum from a scalar variable can be placed in the input file as\nthe example bellow\n\nV AR = 5\n\nThis means that the variable whose keyword is \u201cVAR\u201d will be set as 5.\nThe equal sign is not mandatory and thus may be omitted. For array\nvariables, the difference is that it is necessary to specify to each position in\nthe array data should be assigned. In the following example, the keyword\n\n111\n\n\n\n112 UTCHEM\u2019S INPUT FILE\n\nrepresents the whole array\n\nAR R()= 1 2 3 4\n\nIn this case, the variable whose keyword is \u201cARR\u201d is initialized with(1, 2, 3, 4).\nOne, however, may initialize each component at a time, as bellow\n\nAR R(1) = 1\n\nAR R(2) = 2\n\nAR R(3) = 3\n\nAR R(4) = 4\n\nFurthermore, it is also possible to initialize a subarray from the original\narray:\n\nAR R(1 TO 2) = 1 2\n\nAR R(3 TO 4) = 3 4\n\nFinally, it is worth to comment that if there is less data following a keyword\nthan it is expected, the last components are filled with the last datum. This\nis very convenient. If, for example, ARR is an array with equal components\n(let\u2019s suppose their values are 6), then in the input file one may write\n\nAR R()= 6\n\nMatrices are treated as arrays of arrays and thus the same rules apply.\nHowever, despite the following initialization\n\nM AT (,) = 1 2 3\n\n4 5 6\n\n7 8 9\n\nis instinctive, it should be avoided. It was discovered that the IPARS frame-\nwork, from were comes the function that reads the data, puts the first row\nof data in the first column of the matrix, the second row in the second col-\numn, and the third row in the third column. Thus, unless the user knows\n\n\n\nUTCHEM\u2019S INPUT FILE 113\n\nIPARS profoundly, it is recommended that the data should be placed as\nbellow\n\nM AT (1,) = 1 2 3\n\nM AT (2,) = 4 5 6\n\nM AT (3,) = 7 8 9\n\nTwo more observations should be done. First, to each variable it is\nassociated a data type. In order to avoid reading errors, data following a\nkeyword must respect the type. In the special case in which the data is\na string of characters, it must be between quotation marks and must not\nexceed the maximum number of characters specified for it. The second\nobservation is that comments are allowed. Whatever follow the symbol\n$ in the same line is ignored. This is very convenient if the user wants to\nadd observations concerning the data from the input file.\n\n\n\n\n\nAPPENDIX\n\nB\nExample of code using EFVLib\n\nin parallel\n\nListing B.1 have an example of application of EFVLib with the parallel\nsupport implemented in this work. The purpose of this example is only\nexplain the basic modifications required to run a case in parallel. More in-\nformation about how to use the library may be found in [18]. The example\nis a two-dimensional heat transfer problem. There are two boundaries \u2013\nTOP and OTHER \u2013, both of them with prescribed temperature.\n\nListing B.1 \u2013 Example case using EFVLib with parallel processing\n\n1 /? Grid\n2\n\n3 TOP_1 TOP_0\n4 6 ??????7?????? 8\n5 | / | \\ |\n6 | 7 / | \\ 4 |\n7 OTHERS_0 | / | \\ | OTHERS_5\n8 | / 6 | 5 \\ |\n9 | / | \\ |\n\n10 3 ??????4?????? 5\n\n115\n\n\n\n116 EX AMPLE OF CODE USING EFVLIB IN PARALLEL\n\n11 | \\ | / |\n12 | \\ 1 | 2 / |\n13 OTHERS_1 | \\ | / | OTHERS_4\n14 | 0 \\ | / 3 |\n15 | \\ | / |\n16 0 ??????1?????? 2\n17 OTHERS_2 OTHERS_3\n18 ?/\n19\n\n20 int main( int argc, char?argv[] ){\n21 PetscInitialize (NULL, NULL, (char?)0, NULL );{\n22 MPICommunicator world;\n23\n\n24 int numberOfVertices = 9;\n25 GridDataPtr globalGridData;\n26 if ( world.rank() == 0 ){\n27 globalGridData = ReadGrid::Grid2D( \"grid\" );\n28 }\n29\n\n30 // Grid division\n31 GridDivider gridDivider;\n32 GridDividerOutputPtr gridDividerOutput;\n33 if ( world.rank() == 0 ){\n34 gridDividerOutput = gridDivider.divide( globalGridData );\n35 }\n36 else{\n37 gridDividerOutput = gridDivider.divide();\n38 }\n39\n\n40 // GridBuilder\n41 GridBuilder builder;\n42 GridPtr localGrid = builder.build( gridDividerOutput?>localGridData );\n43\n\n44 // Diffusive Operator\n45 ScalarOrderedFieldOnVerticesPtr temperature( new\n46 ScalarOrderedFieldOnVertices( localGrid?>getNumberOfVertices(),\n47 \"T\", \"o C\", \"temperature\" ));\n48 Tensor2DSphericalPtr conductivity( new Tensor2DSpherical( 1.0 ) );\n49 TensorConstantPropertyPickerOnElementsPtr condutivityPicker( new\n50 TensorConstantPropertyPickerOnElements( conductivity ) );\n51 DiffusiveOperatorComputer operatorComputer( localGrid, condutivityPicker );\n52 VectorOrderedFieldOnFacesPtr diffusiveOperator = operatorComputer.compute();\n53\n\n54 // Linear System\n55 PetscMaskPtr mask( new\n56 PetscMask( localGrid?>getNumberOfLocalVertices(), temperature?>getValues() ) );\n57 IntVectorPtr localToPetsc = localGrid?>getLocalToPetscMapping();\n58 mask?>setLocalToGlobalMapping( localToPetsc );\n59 mask?>setTolerance( 1e?9 );\n60 mask?>setType( \"gmres\" );\n61 mask?>setPreconditioner(\"sor\");\n62\n\n63 PetscMatrixPtr matrix = StaticPointerCast&lt;PetscMatrix >( mask?>getMatrix() );\n64 DoubleVectorPtr independent = mask?>getIndependentVector();\n65\n\n\n\nEX AMPLE OF CODE USING EFVLIB IN PARALLEL 117\n\n66 // Matrix\n67 foreach( ElementPtr element, localGrid?>getInternalElements() ){\n68 InternalFacePtrArrayPtr internalFaces = element?>getFaces();\n69 foreach( InternalFacePtr face, (?internalFaces) ){\n70 int vertexLocalHandle = 0;\n71 foreach( VertexPtr vertex, element?>getVertices() ) {\n72 double coef =?(?diffusiveOperator?>getValue( face ))[ vertexLocalHandle ];\n73 VertexPtr backVertex =\n74 StaticPointerCast&lt;efvlib::Vertex >( face?>getBackwardNode() );\n75 if ( localGrid?>isVertexLocal( backVertex ) ) {\n76 matrix?>addValue( backVertex?>getPetscHandle(),\n77 vertex?>getPetscHandle(), coef );\n78 }\n79 VertexPtr forwardVertex =\n80 StaticPointerCast&lt;efvlib::Vertex >( face?>getForwardNode() );\n81 if ( localGrid?>isVertexLocal( forwardVertex ) ) {\n82 matrix?>addValue( forwardVertex?>getPetscHandle(),\n83 vertex?>getPetscHandle(), ?coef );\n84 }\n85 vertexLocalHandle++;\n86 }\n87 }\n88 }\n89\n\n90 // Boundary condition\n91 ParserPtr parser(new Parser(\"../AppParallelTutorial/BoundaryConditions.txt\"));\n92 BoundaryConditionBuilder bcBuilder;\n93 BoundaryConditionSetPtr boundaryConditions =\n94 bcBuilder.build( localGrid?>getBoundaries(), parser?>getRoot());\n95\n\n96 matrix?>initialize();\n97 foreach( DirichletBoundaryConditionPtr bc,\n98 boundaryConditions?>dirichletBoundaries ){\n99 BoundaryPtr boundary = bc?>getBoundary();\n\n100 foreach( VertexPtr vertex, boundary?>getVertices() ){\n101 if ( ! localGrid?>isVertexLocal( vertex ) ) {\n102 continue;\n103 }\n104\n\n105 int petscIndex = vertex?>getPetscHandle();\n106 PetscInt petscArray[1];\n107 petscArray[0]= petscIndex;\n108 MatZeroRows( matrix?>getPetscFormatMatrix(), 1, petscArray,\n109 1.0, NULL, NULL );\n110\n\n111 int localIndex = vertex?>getHandle();\n112 (?independent)[ localIndex ]= bc?>getValue( vertex );\n113 }\n114 }\n115\n\n116 mask?>solve();\n117\n\n118 ScalarFieldOnVerticesSynchronizerPtr sync(new\n119 ScalarFieldOnVerticesSynchronizer(\n120 gridDividerOutput?>synchronizerVerticesVectors));\n\n\n\n118 EX AMPLE OF CODE USING EFVLIB IN PARALLEL\n\n121 sync?>synchronize( temperature );\n122\n\n123 FieldDivider&lt;efvlib::Vertex, double > fieldDivider(\n124 gridDividerOutput?>verticesOfSubdomains );\n125\n\n126 if ( world.rank() == 0 ) {\n127 ScalarOrderedFieldOnVerticesPtr globalTemperature( new\n128 ScalarOrderedFieldOnVertices(globalGridData?>coordinates.size(),\n129 \"T\", \"o C\", \"temperature\" ) );\n130 fieldDivider.gather( temperature, globalTemperature );\n131\n\n132 TecPlotSaveFile::vString varNames;\n133 varNames.push_back(\"Temperature\");\n134 TecPlotSaveFile tecPlotSaveFile;\n135 tecPlotSaveFile.initializeForTecplot(globalGridData, \"../AppParallelTutorial\",\n136 \"Output.dat\", varNames);\n137 tecPlotSaveFile.iniAppendForTecplot( 0.0 );\n138 tecPlotSaveFile.appendFieldForTecplot(?globalTemperature );\n139 tecPlotSaveFile.endAppendForTecplot();\n140 tecPlotSaveFile.close();\n141 }else{\n142 fieldDivider.gather( temperature );\n143 }\n144\n\n145 PetscFinalize();\n146 }\n\nMPI methods can only be used after the MPI environment is initial-\nized. The MPI can be initialized only once during a program execution.\nSuch operation is performed at line 21 by the initialization of PETSc. The\nactual method in C/C++ to initialize a MPI environment is MPI_Init,\nbut since PETSc also use MPI methods, this calling is hidden inside the\nmethod PetscInitialize.\n\nAll processors in a parallel execution run the same code. However,\nthey usually do not perform the same operations. Each one of them have\nan index, usually called rank, that may be used to distinguish what are the\noperations that must be executed by the processor. It is usual to define the\nprocessor whose rank is 0 as the master processor. The master processor\nwill execute the operation that must be serial. A class from the library\nBOOST named communicator \u2013 renamed here to MPICommunicator \u2013\nis used to identify what is the rank of the current processor.\n\nThe operation of read a grid from a file is executed by a single proces-\nsor: the master processor. The reading is performed at line 27 and right\nberfore that it is checked if the processor is in fact the master processor.\n\n\n\nEX AMPLE OF CODE USING EFVLIB IN PARALLEL 119\n\nThe grid division is executed by calling the method divide of an instance\nof the class GridDivider. Only the master processor pass the global\ngrid data as a parameter of the method since only this processor have\nthe data of the global grid. The other processors call the method divide\nwithout passing any argument. After calling such method all processors\nreceive the data of their local grid and the data required to instantiate\nsome communication classes detailed later.\n\nLines 57 and 58 are another additional piece of code required for\nrunning the program in parallel. A computational interface was created\nto use either EFVLib\u2019s original solver or PETSc. The class PetscMask is\nderived of such interface and should be used if PETSc is intended as the\nsolver, remembering that from the EFVLib\u2019s solvers only PETSc has sup-\nport for parallel running. When the program runs in parallel, PetscMask\nrequires a mapping from the local indexes to the global indexes used in\nPETSc. Such mapping is available in the class Grid through the method\ngetLocalToPetscMapping. The user only needs to get the mapping\nand set it in PetscMask using the method setLocalToGlobalMapping.\n\nPay attention at lines 75, 81, and 100. The purpose of all of them is to\ncheck if the vertex is local. One should remember that all computations\nare performed for the local vertices, since ghost nodes stand only to store\nvalues from other computational domains. This includes the assembling\nof a linear system: each processor assembles only the lines of its local\nvertices.\n\nAt line 118 the class that performs the updating of values at ghost\nnodes is instantiated. This class requires some data that comes from the\ngrid division and may be accessed through the grid division output. The\ntemperature field is updated using this class at line 121.\n\nLines 123 to 143 stand for exporting results. Before line 123 the tem-\nperature field still distributed across the processors. However writing a\nfield into a file is a serial operation and thus all processors must sent their\ndata to the master processor. Such operation is execute using the class\nFieldDivider. An instance of this class is created at line 123 and used\nat lines 130 and 142. Only the master processor pass two parameters to\nthe method, one of them being the a global field in which the data received\nfrom the other processors will be stored. Lines after line 130 would be the\nsame whether the code were parallel or not.\n\n\n\n120 EX AMPLE OF CODE USING EFVLIB IN PARALLEL\n\nFinally PETSc and the MPI environment are finalized at line 145. After\nthis line no MPI operation can be executed.\n\nDespite the description of the example above being a little long, the\npoint is that there is not many modifications to use EFVLib\u2019s parallel sup-\nport. They can be summarized in the following: initialize and finalize\nPETSc at the beginning and at the end of the code, respectively; let only\none processor read a grid; divide the grid; ensure that the processors cal-\nculate variables only at their local vertices; update the values of a field at\nthe ghost nodes whenever such field is recalculated; and collect the fields\nwhen results of the simulation should be exported."}]}}}