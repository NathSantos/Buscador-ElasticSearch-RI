{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.25275"}, {"@name": "filename", "#text": "9923_dscidl.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\nModels for Quantifying Risk and Reliability Metrics via Metaheuristics and Support Vector Machines\nIsis Didier Lins\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\nUNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\nModels for Quantifying Risk and Reliability Metrics via Metaheuristics and Support Vector Machines\nA THESIS\nBY\nIsis Didier Lins\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\nUNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\nModels for Quantifying Risk and Reliability Metrics via Metaheuristics and Support Vector Machines\nA THESIS PRESENTED TO THE UNIVERSIDADE Federal de Pernambuco in partial fulfillment OF THE REQUIREMENTS FOR THE DEGREE OF DOUTOR\nBY\nIsis Didier Lins\nOrientador: Enrique Andr\u00e9s L\u00f3pez Droguett, PhD\nCataloga\u00e7ao na fonte\nBibliotec\u00e1ria: Rosineide Mesquita Gon\u00e7alves Luz / CRB4-1361 (BCTG)\nL757m\nLins, Isis Didier.\nModels for quantifying risk and reliability metrics via metaheuristics and support vector machines / Isis Didier Lins - Recife: O Autor, 2013. xii, 97f., il., figs., gr\u00e1fs., tabs.\nOrientador: Prof. Enrique Andr\u00e9s L\u00f3pez Droguett, PhD.\nTese (Doutorado) - Universidade Federal de Pernambuco. CTG. Programa de P\u00f3s-Gradua\u00e7ao em Engenharia de Produ\u00e7ao, 2013.\nInclui Refer\u00eancias e Ap\u00eandice.\n1.\tEngenharia de Produ\u00e7ao. 2. Risco e Confiabilidade nas Ind\u00fastrias de Petr\u00f3leo e G\u00e1s. 3. Aloca\u00e7ao de Redund\u00e2ncias. 4. Planos de Inspe\u00e7ao.\n5.\tPrevisao de Falhas. 6. Algoritmos Gen\u00e9ticos Multiobjetivo. 7. Support Vector Machines. I. Droguett, Enrique Andr\u00e9s L\u00f3pez (Orientador). II. T\u00edtulo.\n658.5 CDD (22.ed.)\nUFPE/BCTG-2013/050\nUNIVERSIDADE FEDERAL DE PERNAMBUCO\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM ENGENHARIA DE PRODU\u00c7\u00c3O\nPARECER DA COMISS\u00c3O EXAMINADORA DE DEFESA DE TESE DE\nDOUTORADO DE\nISIS DIDIER LINS\n\u201cMODELS FOR QUANTIFYING RISK AND RELIABILITY METRICS VIA METAHEURISTICS AND SUPPORT VECTOR MACHINES\u201d\n\u00c1REA DE CONCENTRA\u00c7\u00c3O: PESQUISA OPERACIONAL\nA comiss\u00e3o examinadora, composta pelos professores abaixo, sob a presid\u00eancia do(a) primeiro(a), considera a candidata ISIS DIDIER LINS APROVADA.\nRecife, 27 de fevereiro de 2013.\nProf. ENRIQUE ANDR\u00c9S L\u00d3PEZ DROGUETT. PhD (UFPE)\nProf. FERNANDO MENEZES CAMPELLO DE SOUZA, PhD (UFPE)\nProfa. ANA PAULA CABRAL SEIXAS COSTA, Doutor (UFPE)\nProf. PAULO FERNANDO FERREIRA FRUTUOSO E MEI.O, Doutor (COPPEUFRJ)\nProf. MARCELO RAMOS MARTINS, Doutor (USP)\nTo my parents Bernardete and Sostenes\nTo my husband Vicente\nA CKNOWL EDGEMEN TS\nFinishing the Doctorate marks the end of a cycle in my life: almost 10 years have gone by dedicated to university as a student. I owe the success in Undergraduation, Master and Doctorate primarily to God. I feel He speaks to me by means of a positive, peaceful and serene interior force that made me persist in various moments of doubt.\nI would like to thank:\n\u2022\tMy mother Bernardete and my father Sostenes for the fondness, support and incentive. In this work, my mother helped me with the English and my father helped me with enumeration techniques.\n\u2022\tMy husband Vicente, for the love, companionship and comprehension. In these years, several leisure moments were sacrificed for study activities. Vicente also taught me the basic concepts of Interior Point methods.\n\u2022\tMy brother Lauro that lives far away, but is always present.\n\u2022\tMy grandparents Myriam and Lauro that wherever they are, they are certainly happy with this important step and also my grandparents Lourdes and Luiz.\n\u2022\tTia Adelaide, tia L\u00f3, tia Dulce, tia Eneida, Nadja, Niedja, C\u00e2mara, tio Alcimar, Marco Ant\u00f4nio, Jorginho, Adriana, Joana, Jo\u00e3o, Pedrinho, Arthur, Fernandinha, Mariana, Sofia, D. Sonia, Sr. Vicente, Ester e Giovanna for the enjoyable moments we have passed together.\n\u2022\tMy advisor Professor Enrique L\u00f3pez Droguett, who believed in me, for the opportunity of being part of CEERMA - UFPE.\n\u2022\tM\u00e1rcio for some of the interesting ideas present in this work and Paulo for clarifying some issues related to Binary Decision Diagrams.\n\u2022\tProfessor Francisco Cribari for the discussions about bootstrap methods.\n\u2022\tProfessors Ana Paula Cabral, Fernando Campello, Marcelo Martins and Paulo Frutuoso for the valuable comments that enhanced the quality of the work.\n\u2022\tAlexandra, Alessandra, Thalles and Marcella for the long hours dedicated to study, works and preparation for the Qualifying.\n\u2022\tAna, Miriam and Joelma for the company and support.\n\u2022\tB\u00e1rbara and Juliane for helping me with the bureaucracy.\n\u2022\tCNPq for the financial support.\nFor you all, my sincere Thanks!\nIsis Didier Lins Recife, March 2013\nABSTRACT\nThis work develops models for quantifying risk and reliability-related metrics of systems in different phases of their life cycle. For systems in the design phase, a Multi-Objective Genetic Algorithm (MOGA) is coupled with Discrete Event Simulation (DES) to provide non-dominated configurations with respect to availability and cost. The proposed MOGA + DES incorporates a Generalized Renewal Process to account for imperfect repairs and it also indicates the optimal number of maintenance teams. For the operational phase, a hybridism between MOGA and Risk-Based Inspection is proposed for the elaboration of non-dominated inspection plans in terms of risk and cost that comply with local regulations. Regression via Support Vector Machines (SVR) is applied when the reliability-related metric (response variable) of an operational system is function of a number of environmental and operational variables with unknown analytical relationship. A Particle Swarm Optimization is combined to SVR for the selection of the most relevant variables along with the tuning of the SVR hyperparameters that appear in its training problem. In order to assess the uncertainty related to the response variable, bootstrap methods are coupled with SVR to construct confidence and prediction intervals. Numerical experiments and application examples in the context of oil industry are provided. The obtained results indicate that the proposed frameworks give valuable information for budget planning and for the implementation of proper actions to avoid undesired events. Keywords: Risk and Reliability in Oil and Gas Industries, Redundancy Allocation, Inspection Plans, Failure Prediction, Multi-objective Genetic Algorithms, Support Vector Machines.\nRESUMO\nNesse trabalho s\u00e3o desenvolvidos modelos de quantifica\u00e7\u00e3o de m\u00e9tricas de risco e confiabilidade para sistemas em diferentes etapas do ciclo de vida. Para sistemas na fase de projeto, um Algoritmo Gen\u00e9tico Multiobjetivo (MOGA) \u00e9 combinado \u00e0 Simula\u00e7\u00e3o Discreta de Eventos (DES) a fim de prover configura\u00e7\u00f5es n\u00e3o-dominadas com rela\u00e7\u00e3o \u00e0 disponibilidade e ao custo. O MOGA + DES proposto incorpora Processos de Renova\u00e7\u00e3o Generalizados para modelagem de reparos imperfeitos e tamb\u00e9m indica o n\u00famero \u00f3timo de equipes de manuten\u00e7\u00e3o. Para a fase operacional \u00e9 proposto um hibridismo entre MOGA e Inspe\u00e7\u00e3o Baseada no Risco para elabora\u00e7\u00e3o de planos de inspe\u00e7\u00e3o n\u00e3o-dominados em termos de risco e custo que atendem \u00e0s normas locais. Regress\u00e3o via Support Vector Machines (SVR) \u00e9 aplicada nos casos em que a m\u00e9trica relacionada \u00e0 confiabilidade (vari\u00e1vel resposta) de um sistema operacional \u00e9 fun\u00e7\u00e3o de vari\u00e1veis ambientais e operacionais com express\u00e3o anal\u00edtica desconhecida. Otimiza\u00e7\u00e3o via Nuvens de Part\u00edculas \u00e9 combinada \u00e0 SVR para a sele\u00e7\u00e3o simult\u00e2nea das vari\u00e1veis explicativas mais relevantes e dos valores dos hiperpar\u00e2metros que aparecem no problema de treinamento de SVR. Com o objetivo de avaliar a incerteza relacionada \u00e0 vari\u00e1vel resposta, m\u00e9todos bootstrap s\u00e3o combinados \u00e0 SVR para a obten\u00e7\u00e3o de intervalos de confian\u00e7a e de previs\u00e3o. S\u00e3o realizados experimentos num\u00e9ricos e s\u00e3o apresentados exemplos de aplica\u00e7\u00e3o no contexto da ind\u00fastria do petr\u00f3leo. Os resultados obtidos indicam que os modelos propostos fornecem informa\u00e7\u00f5es importantes para o planejamento de custos e para a implementa\u00e7\u00e3o de a\u00e7\u00f5es apropriadas a fim de evitar eventos indesejados.\nPalavras-chave: Risco e Confiabilidade nas Ind\u00fastrias de Petr\u00f3leo e G\u00e1s, Aloca\u00e7\u00e3o de Redund\u00e2ncias, Planos de Inspe\u00e7\u00e3o, Previs\u00e3o de Falhas, Algoritmos Gen\u00e9ticos Multiobjetivo, Support Vector Machines.\nCONTENTS\n1\tINTRODUCTION\t1\n1.1\tMotivation and Justification......................................... 4\n1.1.1\tMulti-objective\tRedundancy\tAllocation Problems................. 4\n1.1.2\tMulti-objective\tInspection Plans............................... 6\n1.1.3\tVariable Selection and Adjustment of Support Vector Regression\nHyperparameters............................................... 6\n1.1.4\tUncertainty assessment via Bootstrap and Support Vector Regression 7\n1.2\tA Comprehensive Framework ........................................... 8\n1.3\tObjectives.......................................................... 10\n1.3.1\tMain Objective................................................ 10\n1.3.2\tSpecific Objectives .......................................... 11\n1.4\tOutline of the Thesis .............................................. 11\n2\tTHEORETICAL BACKGROUND\t-\tPART\t1\t13\n2.1\tMulti-objective Optimization........................................ 13\n2.1.1\tMulti-objective Genetic Algorithms............................ 16\n2.2\tGeneralized Renewal Processes ...................................... 17\n2.3\tDiscrete Event Simulation........................................... 19\n2.3.1\tBinary Decision Diagrams...................................... 20\n2.4\tRisk-Based Inspection .............................................. 20\n3\tDESIGN OF SYSTEMS SUBMITTED TO IMPERFECT REPAIRS BY MOGA\n+ DES\t23\n3.1\tProblem Statement and Formulation .................................. 23\n3.2\tProposed Multi-objective Genetic Algorithm ......................... 26\n3.2.1\tIndividual Representation .................................... 26\n3.2.2\tGeneration of Initial Population ............................. 27\n3.2.3\tSelection and Update of the Auxiliary\tPopulation.............. 27\n3.2.4\tCrossover and Replacement .................................... 29\n3.2.5\tMutation ..................................................... 30\n3.3\tAssessment of Fitnesses by Discrete Event Simulation ............... 30\n3.4\tOverview ........................................................... 32\n3.5\tMetrics for Comparing Real and Simulated Pareto Fronts ............. 32\n3.5.1\tPoint-to-Point Distance ...................................... 33\n3.5.2\tCoordinate Distance .......................................... 33\n3.6\tNumerical Experiments............................................... 35\n3.6.1\tValidation Examples........................................... 35\n3.6.2\tApplication Example........................................... 37\n3.6.2.1\tReturn on Invesment Analysis......................... 41\n3.7\tSummary and Discussion ............................................. 42\n4\tELABORATION OF INSPECTION PLANS BY MOGA + RBI\t44\n4.1\tProblem Statement and Formulation .................................. 44\n4.2\tProposed Multi-objective Genetic Algorithm....................... 45\n4.2.1\tIndividual Representation.................................. 46\n4.2.2\tGeneration of Initial Population........................... 46\n4.2.3\tCrossover and Replacement ................................. 47\n4.2.4\tMutation .................................................. 48\n4.3\tEvaluation of Risk via Risk Based Inspection..................... 49\n4.4\tOverview......................................................... 50\n4.5\tApplication Example.............................................. 50\n4.5.1\tReturn on Investment Analysis ............................. 51\n4.6\tSummary and Discussion........................................... 52\n5\tTHEORETICAL BACKGROUND - PART 2\t53\n5.1\tSupport Vector Machines.......................................... 53\n5.1.1\tRegression via Support Vector Machines..................... 54\n5.1.2\tSupport Vector Regression via Interior Point Methods....... 57\n5.1.2.1\tPrimal-Dual\tInterior Point Method.................. 58\n5.2\tVariable and Model Selection\tProblems ........................... 60\n5.3\tParticle Swarm Optimization\t................................ 61\n5.4\tBootstrap ....................................................... 62\n6\tPSO FOR VARIABLE SELECTION AND SVR HYPERPARAMETER TUNING\t63\n6.1\tCoupling Particle Swarm Optimization and Support Vector Regression . . 63\n6.1.1\tCross-validation .......................................... 64\n6.2\tApplication Example ............................................. 65\n6.3\tSummary and Discussion .......................................... 69\n7\tUNCERTAINTY ASSESSMENT BY COUPLING BOOTSTRAP AND SVR 71\n7.1\tBootstrapped Support Vector Regression .......................... 71\n7.2\tNumerical Experiments............................................ 73\n7.2.1\tSimulated case ............................................ 74\n7.2.2\tCase Study: Prediction of Scale\tRate\ton Metal Surfaces.... 79\n7.3\tSummary and Discussion .......................................... 82\n8\tCONCLUSION\t83\n8.1\tLimitations and Suggestions for Future\tWorks..................... 84\nREFERENCES\t87\nAppendix Lagrangian and KKT First Order Conditions for SVR Training Problem\t96\nLIST OF FIGURES\n1.1\tRelationships among analyzed problems .................................... 10\n2.1\tMapping of solutions from search space to objective space: local and global Pareto sets into local and global Pareto fronts. (Adapted from Zitzler (1999),\np. 9) .................................................................... 15\n2.2\tSystem BDD ............................................................... 20\n3.1\tIndividual representation for MOGA (system configuration) ................ 27\n3.2\tMOGA + DES................................................................ 31\n3.3\tPoint to point distance .................................................. 34\n3.4\tCoordinate distance ...................................................... 34\n3.5\tValidation example 1 - Exact and\tsimulated\tPareto\tfronts................ 38\n3.6\tValidation example 2 - Exact and\tsimulated\tPareto\tfronts................ 38\n3.7\tApplication example MOGA + DES - Obtained Pareto fronts for each type\nof repair................................................................. 40\n3.8\tApplication example MOGA + DES - selected solutions related to perfect,\nimperfect and minimal repairs ............................................ 41\n4.1\tExample of binary crossover procedure (a); solving unfeasibility of child 2\n(b) ...................................................................... 48\n4.2\tApplication example MOGA + RBI - Obtained Pareto front.................... 51\n4.3\tApplication example MOGA + RBI - Selected inspection plans................ 52\n5.1\tRelation between model complexity and errors.............................. 54\n5.2\tThe role of mapping and Vapnik\u2019s e-insensitivity loss function............ 55\n6.1\tPSO + SVR for variable selection and hyperparameter tuning................ 64\n6.2\tSVR training results...................................................... 69\n6.3\tSVR test results.......................................................... 69\n7.1\tSimulated case - pairs and residuals bagging estimates vs. true mean . . .\t76\n7.2\tSimulated case - results over test set by bootstrapping pairs ............ 76\n7.3\tSimulated case - results over test set by bootstrapping residuals ........ 77\n7.4\tSimulated case - variance behavior over the interval of x................. 79\n7.5\tStudy case - results over test set by bootstrapping pairs ................ 80\n7.6\tStudy case - results over test set by bootstrapping residuals ............ 81\nLIST OF TABLES\n2.1\tMain methods of multi-objective optimization via evolutionary algorithms .\t17\n2.2\tRepair classification according to parameter q............................... 18\n3.1\tSubsystems\u2019 characteristics.................................................. 35\n3.2\tExample 1 - Components\u2019 characteristics...................................... 36\n3.3\tScaling factors.............................................................. 36\n3.4\tMOGA parameters.............................................................. 36\n3.5\tResults of validation Examples 1 and 2 - MOGA x MOACO........................ 37\n3.6\tApplication example - components\u2019 characteristics............................ 39\n3.7\tROI of selected solutions related to minimal, imperfect and perfect repairs. 42\n4.1\tIndividual representation for MOGA (inspection plan) ........................ 46\n4.2\tRBI parameters............................................................... 51\n4.3\tMOGA parameters.............................................................. 51\n6.1\tVariables that can influence wells\u2019 TBFs .................................... 66\n6.2\tCharacteristics of PSO decision variables.................................... 67\n6.3\tSummary of 100 PSO+SVR replications.......................................... 67\n6.4\tCharacterization of the optimal reduced and full SVR models ................. 68\n7.1\tSimulated case - MSE (left) and bias2 (right) over test set by bootstrapping\npairs ....................................................................... 75\n7.2\tSimulated case - MSE (left) and bias2 (right) over test set by bootstrapping\nresiduals ................................................................... 75\n7.3\tSimulated case - coverage results (%) of the Monte Carlo simulation with\n5000 replicates ............................................................. 78\n7.4\tIllustrative example - point (PE) and interval (CI, PI) estimates for the\nscale rate and for the time to attain threshold ............................. 81\nLIST OF ACRONYMS\nACO Ant Colony Optimization.\nANN Artificial Neural Network.\nB-P Brown-Proschan.\nBDD Binary Decision Diagram.\nDES Discrete Event Simulation.\nERM Empirical Risk Minimization.\nGA Genetic Algorithm.\nGRP Generalized Renewal Process.\nIP Interior Point.\nKKT Karush-Kuhn-Tucker.\nMOACO Multi-objective Ant Colony Optimization.\nMOGA Multi-objective Genetic Algorithm.\nMSE Mean Squared Error.\nMTBF Mean Time Between Failures.\nMTFF Mean Time to First Failure.\nMTTF Mean Time To Failure.\nMTTR Mean Time To Repair.\nNHPP Non-Homogeneous Poisson Process.\nNPGA Niched-Pareto Genetic Algorithm.\nNRMSE Normalized Root Mean Square Error.\nNSGA Nondominated Sorting Genetic Algorithm.\nNSGA-II Nondominated Sorting Genetic Algorithm II.\nPSO Particle Swarm Optimization.\nRAP Redundancy Allocation Problem.\nRBI Risk-Based Inspection.\nROI Return On Investment.\nRP Renewal Process.\nSMO Sequential Minimal Optimization.\nSPEA Strength Pareto Evolutionary Algorithm.\nSRM Structural Risk Minimization.\nSVM Support Vector Machine.\nSVR Support Vector Regression.\nTBFs Times Between Failures.\nTTFs Times To Failure.\nTTRs Times To Repair.\nVEGA Vector Evaluated Genetic Algorithm.\n1\tINTRODUCTION\nRisk is a measure of the potential losses due to natural or human activities and is quantitatively defined by the interaction of an event that leads to natural or artificial hazard exposure, its likelihood or frequency of occurrence and its related consequences (MODAR-RES, 2006). Regarding industrial activities and their associated hazards, the estimation of frequencies depends greatly on the reliability of systems\u2019 components (MODARRES et al., 1999). Hence, the consideration of reliability aspects of production processes are fundamental in the assurance of system, environmental and human safety.\nThe occurrence of failures can lead to system shutdowns, loss of production, equipment damage. In more complex systems, e.g. oil and gas industries and nuclear power plants, failures can incur accidents that degrade the environment and prejudice human integrity. All these undesired effects are translated into increased costs due to system recovery, legal penalties and organization\u2019s affected image in face of society. Failure prediction modeling of systems may be conducted during various phases of their life cycle, including the concept validation and definition, the design and operation. At any stage, obtained predictions serve the purpose of anticipating the reliability behavior of the systems so as to enable the implementation of appropriate actions for their maintaining and, possibly, improvement (ZIO et al., 2008). Also, those predictions allow for a cost evaluation due to maintenances and inspections, which is a valuable information for properly planning the allocation of resources to these activities.\nSystems in different stages of their life cycle may demand various approaches for reliability assessment. For example, in the design phase, simulation techniques can be applied to imitate components\u2019 and system\u2019s failure-operation processes, which depend on system\u2019s logic, number of redundancies, among other characteristics. Discrete Event Simulation (DES) is a powerful tool for systems\u2019 dynamic modeling, as realistic aspects can be introduced, e.g. effectiveness of repairs, availability of maintenance teams, among others. Besides, system\u2019s configurations can be determined with respect to various objectives such as cost and availability, by coupling the simulation method with an optimization procedure to define, for example, the types and the number of redundancies in each subsystem. As cost and availability are ususally conflicting objectives, a multi-objective approach can be adopted. In fact, this is a variation of a Redundancy Allocation Problem (RAP), which in its traditional single-objective formulation concerns the definition of the numbers of (equal) redundancies in each subsystem that maximize overall system reliability subject to cost constraints (KUO et al., 2001).\nIn the operational phase of an equipment, inspection activities play an important role as an integrity control technique to track the real state of the equipment often exposed to a damage mechanism. Risk-Based Inspection (RBI) (API, 2008) has been used for guiding\ninspection activities mainly on equipments of the petrochemical industry. Based on a predefined risk level - risk target - and on the information provided by an inspection, the actual risk to which the equipment is exposed is updated by RBI, which indicates when the next inspection should occur. However, RBI requires the risk target a priori and it does not consider the expenditures due to the inspection performance. In this way, as minimum cost and risk are both desired but conflicting, a multi-objective optimization emerges as an alternative to overcome the drawbacks of RBI: a risk target is not required, as risk is an objective to be minimized, and the cost due to inspections is also considered as an objective.\nIn multi-objective optimization, a solution that optimizes all objectives concurrently is very difficult to be reached or it does not exist. In this way, instead of having a unique solution as in single objective cases, one may obtain a set with multiple solutions. These solutions, named nondominated solutions, present a compromise among objectives and usually do not yield an optimal value for either of them individually. Once this set is obtained, the decision maker can choose any of its elements based on her preferences and then implement the selected solution.\nProbabilistic optimization methods, such as Genetic Algorithms (GAs) have interesting characteristics to handle multi-objective optimization problems (DEB, 1999): (i) they are population-based, that is, many potential solutions are simultaneously considered; (ii) they permit a separated treatment of the different objectives, thus not requiring any transformations of the multiple objectives into a unique function. GAs (GOLDBERG, 1989) attempt to computationally imitate natural evolution process in which the fittest individuals are more likely to remain in population. In the optimization context, an individual is a potential solution of the considered problem and a set of individuals is the population, which evolves according to some genetic-based operators, such as selection, crossover and mutation.\nFrequently, the reliability behavior of an equipment during its operational phase is influenced by a number of factors usually interdependent and an analytical model of the reliability behavior of these systems becomes impractical. For these situations, in which the underlying process that maps input - regressors, influential variables - into output - response or target variable - is not known, empirical regression via Support Vector Machines (SVMs) - Support Vector Regression (SVR) - is an effective option. SVM is a supervised learning method whose foundations stem from the statistical learning theory (VAPNIK, 2000). The training step of SVMs involves a quadratic optimization problem for which the Karush-Kuhn-Tucker first order conditions for a global optimum are necessary and sufficient, differently from other learning techniques such as Artificial Neural Networks (ANNs) that can be trapped on local minima (SCHOLKOPF &amp; SMOLA, 2002). Also, the SVM training objective function embodies the Structural Risk Minimization (SRM) principle, which consists in the minimization of the errors computed in the training\nphase and also of the errors associated with the machine capacity in accurately predicting the response variable related to input observations not in the original training data set (the so-called generalization ability). On the other hand, ANNs entail the Empirical Risk Minimization (ERM) principle, which only considers the minimization of the training error and is suitable for handling large amounts of data (VAPNIK, 2000).\nThe performance of SVM is highly dependent on the values of some hyperparameters that appear in the associated training problem. The simultaneous adjustment of these hyperparameters by trial and error is time-consuming and does not guarantee the achievement of good values for them. Thus, structured methods such as grid and pattern search (MOMMA &amp; BENNETT, 2002), gradient-based methods (CHAPELLE et al., 2002; CHANG &amp; LIN, 2005; ITO &amp; NAKANO, 2003) and heuristics such as Particle Swarm Optimization (PSO) (LIN et al., 2008; FEI et al., 2009; LINS et al., 2010a, 2010b) and GA (PAI, 2006; CHEN, 2007) have been used to tune SVMs hyperparameters. These procedures are usually driven by performance metrics based on validation data, which is a part of the available data set not used in actual training.\nFurthermore, in practice, among the many factors that are supposed to influence the response variable, only a subset of them may be important to describe its behavior. Actually, some of these potential regressors may be redundant, noisy or even irrelevant for predicting the response variable. Thus, a variable selection procedure may be applied to identify such a subset of variables. According to Guyon &amp; Elisseeff (2003), the objectives of variable selection are to improve the prediction performance of predictors (e.g. SVMs), to construct faster predictors and also to provide better understanding of the underlying process that might have generated the response variable. Given that the introduction or removal of input variables change the data set, a hyperparameter tuning may be performed in order to avoid an eventual decrease in the predictive ability of SVM.\nAs in any regression method, once the SVR regression function is estimated, prediction becomes a straightforward task: values of the input variables are applied to the regression formula and an estimated value of the response is obtained. In this way, if the same observed values are used in the estimated model, the point estimate will be exactly the same no matter how many calculations are performed. However, it is also important to evaluate the uncertainty related to the response variable so as to provide not only accurate point estimates, but also the more informed confidence and prediction intervals, which give an idea about the precision of the quantities under consideration. In order to preserve the interesting non-parametric properties of SVR, non-parametric bootstrap methods (EFRON, 1979; EFRON &amp; TIBSHIRANI, 1993) can be used to perform uncertainty analyses concerning the response variable.\nIn this section, some limitations of the techniques for reliability modeling have been presented and this work aims at solving them in the following contexts: (i) design of systems submitted to imperfect repairs considering availability and cost; (ii) elaboration of\nefficient inspection plans in terms of risk and cost; (iii) prediction of Times Between Failures (TBFs) via SVR models with adjusted hyperparameters and most relevant subset of regressors; (iv) construction of confidence and prediction intervals concerning failure times by means of bootstrapped SVRs. In next section, the main motivations and justifications of the work are given for each of these contexts.\n1.1\tMotivation and Justification\nBy an organizational perspective, systems without considerations about risk and reliability may cost less at a first glance, however the undesired failures\u2019 consequences in its various forms - unsatisfied demand, components\u2019 replacement, system recovery, human losses, environmental accidents - are more prone to occur, which certainly increase costs and can definitely harm the companies\u2019 activities. These effects can be reduced or even avoided by means of well performed risk and reliability analyses, which are specially important when the consequences are severe.\nOn the other hand, by the viewpoint of stakeholders - employees, costumers, suppliers, investors, nearby population, society, - organizations committed to offer goods and/or services originated from safe and reliable production systems seem \u201chealthier\u201d to support and to invest in. Thus, investments in reliable and safe processes turn into competitive advantage to the company for maintaining and increasing its market share.\nIn addition to these general motivations, in the following, further details on the limitations of the techniques in the areas of interest previously mentioned are provided. Some of these limitations are tackled in this work by the use of metaheuristics (multi-objective GAs, PSO), SVR and simulation methods.\n1.1.1\tMulti-objective Redundancy Allocation Problems\nAccording to (KUO &amp; WAN, 2007), it is usually difficult for a single objective to adequately describe a real problem for which an optimal design is required, thus multiobjective approaches deserve attention. In this context, Multi-objective Genetic Algorithm (MOGA) emerges as an alternative optimization procedure to tackle multi-objective RAPs. Moreover, the authors comment the existence of many unsolved topics related to redundancy allocation, including optimal design of nonrenewable systems. Indeed, the majority of the works considering repairable systems involve components with constant failure rates, thus use an underlying Exponential distribution to model their failure processes (BUSACCA et al., 2001; ELEGBEDE &amp; ADJALLAH, 2003; CHIANG &amp; CHEN, 2007; JUANG et al., 2008).\nHowever, the hypothesis of components with constant failure rates is often non-realistic, as it does not incorporate the effects of component degradation / improvement.\nTherefore, such a supposition can mislead the evaluation of some characteristics of the complete system such as system reliability. Specifically in the context of software reliability, Littlewood (2008) points out the irrelevant results provided by non-realistic assumptions upon reliability models. Jones &amp; Hayes (2001) state that, in the context of electronic systems, the hypothesis of constant failure rates are not met in real situations and may deceive the estimation of the entire system reliability. Also, Bowles (2002) discusses and illustrates the effects of assuming constant failure rates in modeling component reliability in system design with Weibull distributions with the same Mean Time To Failure (MTTF) and different shape parameters (P), including the special case when /3 = 1, i.e. an Exponential distribution. By considering a non-repairable parallel redundant system, it is shown that if components are supposed to have a constant failure-rate but actually have either increasing or decreasing hazard functions, the complete system reliability is either overestimated or underestimated, respectively.\nThe works of Cantoni et al. (2000) and Lins &amp; Droguett (2009) present some sophistication regarding the reliability portion of the redundancy allocation problem. The former couples GA with Monte Carlo simulation in order to obtain an optimal plant design. They tackle a single objective redundancy allocation problem in which it is desired to obtain a combination of several repairable components to be placed in a series-parallel layout in order to maximize system profit. They consider that all standby components are cold (there are no failures in standby mode) and that components failure rates are constant from the time they return from a maintenance intervention to the time of the very next failure. This means that the system deterioration during operational time is not modeled. Moreover, the authors take into account a modified Brown-Proschan (B-P) model of imperfect repairs, which is in the group of failure intensity models (DOYEN &amp; GAUDOIN, 2004). The modified B-P approach assumes that either a minimal or a deteriorating repair is performed with respective probabilities p and 1 \u2014 p (i.e. according to a Bernoulli distribution). If a minimal repair is performed, the system returns to operation with the same condition it had before the failure occurrence (failure-rate remains the same). Otherwise, if a deteriorating repair is executed, the system returns to operation worse than it was before the failure occurrence (failure-rate is increased by a given percentage).\nThe other work, from Lins &amp; Droguett (2009), tackles RAPs for repairable systems via MOGA and DES. It does not consider constant failure rates, but involves the supposition of perfect repairs, which are often not met in practice. Also, Lins &amp; Droguett (2008) propose a combination of a Multi-objective Ant Colony Optimization (MOACO) and DES for solving RAPs involving systems with imperfect repairs modeled by a Generalized Renewal Process (GRP), but do not optimize the number of maintenance teams.\nIn fact, any improvement in system reliability / availability demands resources, which in turn raise the associated costs. As a consequence, a more realistic treatment of RAPs\nin repairable systems should consider not only the choice of components and systems configurations, but also the number of available resources. Marseguerra et al. (2005), for example, aim to encounter the optimal number of spare parts, which are a sort of resource, but they do not aggregate it with finding optimal system configurations.\n1.1.2\tMulti-objective Inspection Plans\nPlanning inspections involve the definition of which techniques have to be adopted and when they have to be performed in full compliance with local and specific regulations. In the context of oil and petrochemical industries, RBI has been used to support the management and scheduling of inspections.\nHowever, in RBI methodology, the risk target is usually user defined and not an objective to be optimized. Thus, there is no guarantee about the efficiency of inspections plans elaborated based on such a risk level. Also, it does not take into account the costs associated with the inspection activities and does not suggest how resources should be allocated. For example, it does not indicate if it is better to perform a number of low cost / low-effective inspections or fewer inspections with higher effectiveness.\nSince techniques of high effectiveness are usually more expensive, it is not interesting to only adopt them instead of using simpler but cheaper inspections. Additionally, regulations often provide a maximum permitted period between inspections using different techniques. In this context, a multi-objective approach emerges as an alternative to handle the conflicting objectives of risk and cost so as to create efficient inspection plans that comply with regulation standards concerning several inspection techniques. Given a planning horizon, the idea is to find the optimal compromise between risk and cost by answering two questions: (i) in each period, an inspection should or should not be performed? and (ii) if the answer of (i) is affirmative, which techniques should be used?\nDepending on the number of periods considered within the planning horizon and on the quantity of inspection techniques, the number of possible inspection plans can be prohibitively large for an exhaustive evaluation of their performance. In this way, a probabilistic approach such as MOGA can be adopted for the quest of inspection plans representing the optimal compromise between risk and cost.\n1.1.3\tVariable Selection and Adjustment of Support Vector Regression Hyperparameters\nIn the SVM context, Rakotomamonjy (2003) performs variable selection for classification based on relevance criteria originated from SVM theory (weight vector and upper bound of the generalization error). The author uses a backward elimination strategy, in which the initial model involves all variables and the least promising ones are progres\nsively eliminated (GUYON &amp; ELISSEEFF, 2003). Search for SVM hyperparameters is not performed but the author states that more work should be devoted to the problem of hyperparameter selection in conjunction to that of variable selection.\nProbabilistic heuristics such as PSO (KENNEDY &amp; EBERHART, 1995) and GAs (GOLDBERG, 1989) are flexible to permit the simultaneous resolution of variable and SVM hyperparameter selection problems. For SVM classification, (FROHLICH et al., 2003) and (LIN et al., 2008) apply, respectively, GA and PSO methods to tackle both problems at the same time. However, GAs often require more computational effort than PSO, given the necessity of various genetic operators to mimic the evolutionary process, e.g. mutation, crossover, among others. The main idea of PSO is derived from the motion of groups of organisms, e.g. schools of fishes and flocks of birds. The swarm evolution is mainly governed by a couple of update equations concerning particles\u2019 velocities and positions in the search space. Indeed, besides the computational advantage of PSO over GA, (BABAOGLU et al., 2010) report the better accuracy of PSO-based feature selection for SVM classification problems.\nFor SVR, Yang &amp; Ong (2010) use a grid search method to tune SVR hyperparameters previous to variable selection by a recursive feature elimination scheme, which is an instance of backward feature elimination (GUYON et al., 2002). At each step of the algorithm, after the specification of the variables to be included in the regression model, an SVR training takes place without the re-tuning of the hyperparameters. However, their most suitable values are dependent on the data set used to guide their search. Thus, the obtained hyperparameters\u2019 values over a set with all regressors do not guarantee a satisfactory SVR performance over an adjusted training data.\nWu &amp; Wang (2009), in turn, present successive PSO algorithms for SVR hyperparameter tuning and feature selection. Firstly, a PSO involving a project pursuit technique (FRIEDMAN &amp; TUCKEY, 1974) is performed for feature selection. Afterwards, another PSO is applied to hyperparameter selection. Thus, the feature selection is essentially a preprocessing step and the obtained results are used to feed the SVR algorithm coupled with the PSO related to hyperparameter search. In this way, the SVR prediction accuracy is not used to guide the quest for the most relevant subset of features.\nThus, given the importance of selecting the most relevant variables and of tuning of the SVR hyperparameters for its predictive ability (usually dependent on the available data set), a method for the concurrent performance of both tasks is required. In this work, a PSO is combined to SVR to tackle such a problem.\n1.1.4\tUncertainty assessment via Bootstrap and Support Vector Regression\nAfter the SVR training step, in correspondence of a new observation of the input vector x, henceforth called x+, the estimate y+ of the true mean response yY (x+) can\nbe obtained via the adjusted regression function (i.e. the estimator). Besides point estimates, confidence intervals for \u00a1j,Y(x+) are also needed to account for the variability of the estimator. Furthermore, it is important to assess the uncertainty on the prediction of the response variable Y+ itself by means of prediction intervals (MONTGOMERY et al., 2006).\nGiven that SVR does not require any hypothesis about the distribution of the error term, the central limit theorem enables the approximation of confidence and prediction intervals when large data sets are available (BRABANTER et al., 2011). On the other hand, for small numbers of data points, the intervals based on bootstrap (EFRON, 1979; EFRON &amp; TIBSHIRANI, 1993) tend to be more accurate, given that they do not rely on asymptotic results but on the construction of the limit distribution from the available data.\nThe main idea of bootstrap methods is to estimate probability distributions for statistics of interest obtained from the available data. They are widely used in (generalized) linear, non-linear and nonparametric regression (DAVISON &amp; HINKLEY, 1997). For example, in linear regression, Cribari-Neto (2004) and Cribari-Neto &amp; Lima (2009) use bootstraped hypothesis testing and intervals tailored to account for heteroskedasticity with an estimator of the covariance matrix that considers the effects of leverage points in the design matrix. In non-parametric regression, Zio (2006), Cadini et al. (2008), Secchi et al. (2008) and Zio et al. (2010) analyze by bootstrap the uncertainty of ANNs predictions of nuclear process parameters. In the specific context of SVM, bootstrap approaches have been mainly applied to classification problems (ANGUITA et al., 2000; TSUJITANI &amp; TANAKA, 2011). For SVR, Lin &amp; Weng (2004) and Yang &amp; Ong (2010) have proposed probabilistic outputs, but assuming a probability distribution for the response variable.\nIndeed, Brabanter et al. (2011) compares the proposed approximation intervals with bootstrap intervals, but these were only based on a residuals sampling. In this work, bootstrap methods - both pairs and residuals schemes - are combined to SVR for the construction of confidence and prediction intervals. The proposed bootstrapped SVRs are first tested on a simulated example and then applied to a real case study for the prediction of scale growth rate on metal surfaces of an equipment used in offshore oil wells.\nGiven the abovementioned limitations and the general ideas to overcome some of them in each context, next section gives the main and specific objectives of the thesis.\n1.2\tA Comprehensive Framework\nIn spite of the different contexts to which the proposed models are applied, the used techniques can be combined to form a comprehensive framework to evaluate risk, reliability and cost of production systems. The outcomes can guide, for example, decisions concerning the allocation of resources related to components acquisition and to inspection\nand maintenance activities.\nIn Figure 1.1, a connection among the different problems, contexts and techniques is illustrated. Imagine that, during the operational phase of a component, data related to failure, maintenance and inspection as well as data concerning operational and environmental conditions can be observed and gathered. In a first moment, these data can feed a learning method, such as SVR, in order to produce accurate reliability prediction functions with the most relevant influential variables, proper hyperparameters and uncertainty analysis. These adjusted non-parametric models can substitute the probabilistic distributions concerning components\u2019 Times To Failure (TTFs) in DES, which is used to mimic the system operational behavior in the design step.\nThe collected data can also enable the calculation of risk - e.g. via RBI if the component is from oil or petrochemical industry - and cost so as a multi-objective optimization (e.g. MOGA) can be used for the quest of nondominated inspection plans. The selected inspection plan provided by a post-optimization procedure can be implemented and give new inspection observations and can also feed the simulation block.\nIn the case of a critical component, for which failures are extremely undesired, the failure times predicted by SVR models enable the implementation of preventive actions that can restore the components to an intermediate state between \u201cnew\u201d and \u201cas bad as before intervention\u201d (e.g. imperfect maintenance). Preventive maintenance can also be guided by inspection activities. Otherwise, if the component is not critical so that it can fail, corrective maintenance based on imperfect repairs can be adopted.\nA simulation method comprising all these features replicated for several components and for different combinations among them permits the computation of availability and cost associated with various system configurations. By coupling simulation with multiobjective optimization a set of nondominated system configurations can be obtained and one of them can be chosen to be implemented. When operational , the system provides observations of each of its components and this completes the cycle.\nTherefore, the proposed methods to evaluate risk and reliability during the components\u2019 operational phase can be transformed into valuable information for the design step of systems to be implemented in the future. However, the combination of all mentioned aspects into a unique model involving observation, estimation, simulation and (post-)optimization is a complex task. In this way, instead of directly handling the big problem, it is wise to divide it into parts and tackle each of them in a separate way so as to understand their particularities. Often, even the subproblems involve a great amount of complexity. Afterwards, once the methods used in each of the parts are developed and stabilized, they can be joined in order to tackle the more general problem.\nComponent (operation)\nSystem (design)\nObservation\nEstimation\nVariable selection\n\\ /\nLearning method I Uncertainty analysis\nSimulation\nHyperparameter tuning\n\tCorrective maintenance\t\nPrediction of\t\tPreventive\ntime to failure\t\tmaintenance\n\tInspection\tJ\nOperational conditions\nInspection data\nM ult i- ob j ect i ve optimization\nEnvironmental\n^conditions\n( Failure data\nMaintenance data\ndifferent\nNondominated system configurations\nMulti-objective optimization\nFigure 1.1: Relationships among analyzed problems\ncomponents and\nconfigurations\nImplementation\n1.3\tObjectives\n1.3.1\tMain Objective\nThis thesis proposes quantitative models based on metaheuristics and SVMs to assess risk and reliability-related metrics of systems in two different life cycle phases:\n\u2022\tDesign:\n\u2014 Development of a MOGA + DES method for the indication of efficient system designs subjected to imperfect repairs in terms of both availability and cost.\n\u2022\tOperation:\n\u2014 Development of genetic operators for the MOGA to be combined with the RBI methodology in order to provide inspection plans representing the optimal compromise between risk and cost.\n\u2014 Development of a PSO + SVR for obtaining the subset of the most important input variables influencing the response along with the proper values for the SVR hyperparameters, so as to improve the quality of the SVR predictions.\n\u2014 Combination of SVR with bootstrap techniques for uncertainty handling in the prediction of equipment failures in order to support maintenance-related decisions.\n1.3.2\tSpecific Objectives\nThe following specific objectives are defined in order to attain the main goals of the thesis:\n\u2022\tImplementation of the metaheuristics MOGA and PSO with the necessary adaptations for each problem.\n\u2022\tImplementation of SVR using Interior Point (IP) methods.\n\u2022\tComparison of results obtained by the proposed MOGA for system design with the ones provided by other metaheuristics, such as MOACO, and also with real Pareto fronts.\n\u2022\tPerformance comparison between the model considering all available input variables as regressors and the reduced model from the proposed PSO + SVR.\n\u2022\tAssessment of coverage properties of the different bootstrap techniques (pairs and residuals sampling) when combined to SVR.\n\u2022\tApplication of the proposed methods for the operation phase to examples and case studies in the context of oil and gas industries.\n1.4\tOutline of the Thesis\nBesides this introductory chapter, this thesis contains 7 additional chapters, whose contents are described as follows:\n\u2022\tChapter 2 concerns the theory underlying the two subsequent chapters involving MOGA developments. It includes: a description of multi-objective optimization; an introduction to MOGA; the main concepts of the techniques to be coupled with MOGA - GRP, DES and RBI.\n\u2022\tChapter 3 refers to the development of a MOGA and its coupling with DES for solving multi-objective RAPs with cost and availability as objective functions. Two\nperformance metrics for comparison between simulated and real Pareto fronts are presented and used in two validation examples of the MOGA. The proposed MOGA + DES is applied to an illustrative example with components submitted to imperfect repairs.\n\u2022\tChapter 4 is related to the development of genetic operators specific for the construction of feasible inspection plans and to the coupling of MOGA and RBI for obtaining inspection plans representing the optimal compromise between risk and cost. The proposed MOGA + RBI is applied to example involving an oil and gas separator.\n\u2022\tChapter 5 contains the theoretical background of the SVM-related chapters: an overview of SVM; a detailed description of SVR; an introduction to IP methods used to solve the SVR training problem; comments on the model and variable selection problems and a description of the PSO method used to tackle them; the general ideas concerning bootstrap methods.\n\u2022\tChapter 6 presents the combination of PSO and SVR for the simultaneous variable selection and SVR hyperparameters adjustment. The proposed PSO + SVR is applied to an example in the context of onshore oil wells.\n\u2022\tChapter 7 details the development of bootstrapped SVRs, which are validated in an artificial example and applied to a case study involving an equipment used in offshore oil wells.\n\u2022\tChapter 8 presents the main contributions and limitations of the work along with suggestions for future research.\n2\tTHEORETICAL BACKGROUND - PART 1\nThis chapter contains the theoretical background used in the MOGA developments that are present in Chapters 3 and 4. The essential concepts associated with multiobjective optimization and an introduction of MOGA are provided. Also, the techniques coupled with the proposed MOGAs - GRP, DES and RBI - are described.\n2.1\tMulti-objective Optimization\nPractical situations often require the achievement of many objectives simultaneously. In risk and reliability fields, usual goals are to minimize risk, to maximize reliabil-ity/availability, to minimize costs. The general formulation of a multi-objective optimization problem is:\nmax z = [fi(x),.. . ,fk(x)]\ns.t.\tgj (x) = 0, j = 1,...,p,\nhj(x) &lt;0, j = p +\n(2.1)\n(2.2)\n(2.3)\nin which z is a vector formed by the objective functions fi(x), i = 1,... ,k, x is the vector of decision variables, p is the number of equality constraints gj (x) and q \u2014 p is the number of inequality constraints hj (x).\nGiven the conflicting relation among objectives, a unique solution that optimizes all elements of z at the same time is very difficult to be found or such a solution does not even exist. In this way, instead of a unique optimal solution as in the single-objective situation, the resolution of a multi-objective problem may provide a set of solutions representing the compromise among objectives, all \u201coptimal\u201d according to the multi-objective viewpoint. This set, formed by nondominated solutions, is known as optimal Pareto set due to the Italian economist Vilfredo Pareto, who generalized the optimality concept for the multiobjective context (COELLO et al., 2002).\nA solution is nondominated if, for all objectives, it has a performance at least as good as the performance of the other solutions and, at least for one of the objectives, its performance overcomes the performance of the others. On the other hand, the dominance relation is mathematically defined as follows:\nxi x2 O fi(xi) > fi(x2), Vi and fi(xi) > fi(x2) for some i,\t(2.4)\nwhere denotes dominance, x1 is a nondominated solution for a maximization problem and x2 is a dominated solution for the same problem; fh denotes the hth objective function. If a minimization problem is considered, the signs > and > in Equation (2.4) are\nreplaced by &lt;and&lt;, respectively. If one of the conditions in the right side of Equation (2.4) is not satisfied, xi is said to be nondominated in relation to x2 and vice-versa. That is, for a number of objectives, xi overcomes the performance of x2 and, for the remaining objectives, x2 overcomes the performance of xi. The nondominance relation is also observed when xi = x2.\nThe concepts of local and global optimality in single-objective optimization are replaced by local optimal Pareto set (P) and global optimal Pareto set (P), respectively, in the multi-objective case (DEB, 1999; ZITZLER, 1999). To define these concepts, let X be the set of all x that satisfies the constraints in Equations (2.2) and (2.3), i.e., the feasible set:\n\u2022\tLocal optimal Pareto set (P): Vx G P, x' G X satisfying ||x' \u2014 x|| &lt;E which dominates any member of P (|| \u2022 || is the Euclidean distance between two points and E > 0).\n\u2022\tGlobal optimal Pareto set (P): Vx G P, x' G X such that x' >- x.\nIf the solutions in P are substituted in the objective functions, the local Pareto front (FP) is obtained (Equation (2.5)); following the same reasoning for the solutions of P, the global Pareto front FP is found (Equation (2.6)):\nFP = {fi(x),...,fk(x)),Vx G P},\t(2.5)\nFP = {fi(x),---,fk(x)),V x G P}-\t(2.6)\nFigure 2.1 illustrates the mapping of solutions for a problem with two maximization objectives, fi(x) and f2(x), and two decision variables, i.e. x = (xi,x2). In the search space, the light gray ellipse and the elements within it represent the feasible set X. The black circle with locally nondominated pairs (xi ,x2) in the same space is the local optimal Pareto set (P) that is mapped into the local Pareto front (black line) in the objective space. Similarly, the dark gray circle (P) containing the globally nondominated pairs (xi,x2) in the search space is mapped into the global optimal Pareto front (dark gray line) in the objective space. Note that the dominated pairs in the search space produces dominated Pareto fronts in the objective space (in light gray).\nAs multiple solutions are available, the multi-objective approach involves two distinct phases: (i) the search for potential solutions and (ii) the decision making associated to the selection of a single solution representing the adequate compromise among objectives. The manner these two phases are combined determines the classification of the multi-objective methods into three categories (ZITZLER, 1999):\n\u2022\tDecision making before search: the objectives are aggregated into a unique objective function that implicitly involves information about the decision maker preferences.\nFigure 2.1: Mapping of solutions from search space to objective space: local and global Pareto sets into local and global Pareto fronts. (Adapted from Zitzler (1999), p. 9)\nThe transformation of a multi-objective problem into a single-objective one enables the use of techniques devised for the resolution of single-objective optimization problems. However, the incorporation of the decision maker preferences, usually in the form of weights, is a relevant step and, in general, demands elicitation methods.\n\u2022\tDecision making during search: the decision maker provides information about her preferences during the search procedure; at each step they are used in the determination of new potential solutions. The methods within this category are called \u201cinteractive\u201d; they require the decision maker availability for direct participation in the search process and that she is familiar with the used elicitation and optimization procedures as well.\n\u2022\tDecision making after search: the optimization process is performed without any preference information. As a result, a set of solutions, - ideally Pareto optimal -is obtained and then the decision maker finally chooses one of them according to her own preferences. Sometimes, the obtained set of solutions is very large, which turns the selection of a unique solution into a difficult task. However, the decisionmaking after search avoid eventual problems with decision makers non-familiarized with elicitation and optimization processes required in the previous categories.\nThe outcome of the methods of the third category is a set of nondominated solutions. Then another problem arises: given that all solutions are similar from a multi-objective perspective, which of them the decision-maker must choose? She may select it according to her own preferences, but such a task often becomes a challenge in practical situations, specially when they involve budget and safety-related issues. In order to support decisionmaking for a given Pareto set, Taboada et al. (2007) suggested data clustering techniques in order to reduce the number of solutions of the Pareto set: the main idea is to form groups internally homogeneous externally heterogeneous, to take a representative solution of each group and to select the most relevant one - such a solution is likely to be related\nto the \u201cknee\u201d region of the Pareto front, in which small deterioration in one objective, e.g. cost, turns into high improvements on the other e.g. reliability.\nSantos et al. (2010), Lins et al. (2011a, 2013) present an interesting game theory approach to support decisions related to the design of security systems subjected to purposeful attacks. Given the Pareto solutions, the equilibria of a sequential game involving preferences of both agents (defender and attacker) are the selected solutions. In this work, however, as only deterioration processes are taken into account, Return On Investment (ROI) analyses are performed as in Lins &amp; Droguett (2009). The main objective is to provide the decision-maker with information about how investements (higher costs) are translated into gains (reduction) in avaialability (risk). ROI analyses are presented in Sections 3.6.2.1 and 4.5.1.\nThe two main traditional approaches for multi-objective optimization are the Weighted Sum and the E-Perturbation methods (DEB, 1999; COELLO et al., 2002). Both of them transform the multi-objective problem into a single-objective one. The Weighted Sum Method makes use of weights to aggregate all the objective functions into a single objective-function. The weights do not represent the relative importance among objectives: they are only factors that may be altered to locate different points in the optimal Pareto set. One of the main drawbacks of the Weighted Sum Method is the fact that it cannot find solutions in non-convex regions of the Pareto front if the Pareto front is non-convex (MESSAC et al., 2000).\nIn the E-Perturbation Method, an objective is arbitrarily chosen (or the one considered as the most important) and optimized as the others become constraints that must satisfy acceptable levels previously defined. Different Pareto optimal solutions are found by varying the acceptable levels. The choice of the acceptable levels demands a preliminary analysis since inappropriate values can result in empty feasible sets (DEB, 1999).\nIn addition, both traditional methods not only require multiple runs in order to obtain different optimal Pareto solutions (it is expected that they are indeed optimal Pareto solutions). Alternatively, nature-based algorithms such as GAs and Ant Colony Optimization (ACO) can be used to resolve multi-objective problems. These algorithms overcome some of the drawbacks of the above-mentioned traditional methods: they handle many potential solutions simultaneously, which allow the achievement of different Pareto solutions in a single run of the algorithm and do not impose any requirements regarding the convexity of the Pareto front.\n2.1.1\tMulti-objective Genetic Algorithms\nGAs attempt to computationally mimic the natural evolution process by the use of genetic operators such as selection, crossover and mutation. They are mainly applied in optimization problems that have some characteristics not accepted by traditional methods\nof mathematical programming and are within the group of probabilistic optimization methods. In addition to the fact that more than one objective is taken into account, the main difference between the single objective GA and the multi-objective GA is the selection phase. In the multi-objective case, the concept of dominance is directly or indirectly incorporated in that step. Table 2.1 lists the main multi-objective methodologies involving evolutionary algorithms. For further details in single and/or multi-objective GA, see Goldberg (1989), Michalewicz (1996), Deb (1999), Messac et al. (2000) and Coello et al. (2002).\nTable 2.1: Main methods of multi-objective optimization via evolutionary algorithms\nMethod\tAuthor(s) and date\nVector Evaluated Genetic Algorithm (VEGA) MOGA Niched-Pareto Genetic Algorithm (NPGA) Nondominated Sorting Genetic Algorithm (NSGA) Strength Pareto Evolutionary Algorithm (SPEA) Nondominated Sorting Genetic Algorithm II (NSGA-II)\tSchaffer (1985) Fonseca &amp; Fleming (1993) Horn et al. (1994) Srinivas &amp; Deb (1994) Zitzler (1999) Deb et al. (2002)\nIn this work, integer-coded MOGAs are adopted and they do not make use of elaborated fitness metrics; hence each individual has an associated fitness vector with size equal to the number of the considered objectives. One MOGA is coupled with DES to solve a multi-objective RAP, which is detailed in Chapter 3. Another MOGA is combined to RBI in order to provide optimal inspection plans in terms of risk and cost. The latter methodology is described in Chapter 4.\n2.2\tGeneralized Renewal Processes\nThe usual stochastic processes to model the failure-repair process of repairable components (or systems) are the Renewal Processs (RPs) and the Non-Homogeneous Poisson Processs (NHPPs). Both of them are counting processes and the Times To Repair (TTRs) are negligible if compared to the component operational time. If RP is chosen, the TBFs are independent and identically distributed (i.i.d.) with an arbitrary probability distribution. Besides, one assumes that the component, after a failure, is subjected to a perfect repair and returns to operation with a condition it presented when new (\u201cas good as new\u201d). On the other hand, using NHPP, the TBFs are neither independent nor identically distributed. In addition, it is supposed that the maintenance crew makes a minimal repair in the failed component, that is, it returns to an operational state with the same condition it had just before the failure occurrence (\u201cas bad as old\u201d).\nNevertheless, the assumption of minimal or perfect repairs required to utilize either NHPP or RP, respectively, is often not realistic. In practical situations, corrective mainte\nnance actions are likely to be imperfect repairs, i.e., they are intermediate actions between minimal and perfect repairs and the equipment returns to operation with a condition better than old and worse than new. In this way, GRPs can be used to model failure-repair processes of components subject to imperfect repairs. In GRP, a parameter q (rejuvenation parameter) is introduced in the model and the value it assumes is related to the maintenance action efficacy (see Table 2.2). The common values of q are in [0,1], but q &lt;0 and q > 1 are also possible and represent the improved and the worse repair, in this order. However, an improved repair might require some project modifications of the component and this is not a procedure used in ordinary repair actions (MOURA et al., 2007). Also, a corrective maintenance action that returns the equipment worse than immediately before the failure occurrence is not common in real situations. In this work, the rejuvenation parameter is considered to be in [0, 1].\nTable 2.2: Repair classification according to parameter q\nq value\tRepair type\nq &lt;0\tImproved\nq = 0\tPerfect\n0<q<1\tImperfect\nq=1\tMinimal\nq > 1\tWorse\nThe parameter q is used in the calculation of the component virtual age (Vn), which is defined as follows:\nVn = Vn-1 + qXnt\t(2.7)\nwhere Xn is the time between the (n \u2014 1)th and the nth failure. By definition, Vo = 0. Thus the expansion of Equation (2.7) yields:\nn\nVn = q^,\t(2.8)\ni=1\nwhere \u00a3i=i Xi is the real component age.\nThe definition of virtual age presented in Equations (2.7) and (2.8) is in accordance with Kijima Type I model, which assumes that the repair actuates just on the very last failure and compensates only the damage accumulated in the interval between the (n\u20141)th and nth failures. In this way, only the additional age Xn is reduced.\nThe distribution of the nth failure time (Tn) can be calculated by a probability distribution function conditioned to the (n \u2014 1)th component virtual age, as follows:\nF(xn|vn-1)\t\u2014\tP(X\t<\txnlVn\t>\tvn-1)\t\u2014 P(Tn\t&lt;vn-1 + xn|Vn\t> vn-1)\t(2.9)\n= F (xn + vn-1) F (vn-1)\t(9 10)\n=\t1 - F(vn-i)\t\u2019\t( \u2022 )\nIf the TBFs are distributed according to a Weibull distribution with a and /3 as scale and shape parameters, respectively, (2.10) becomes as follows:\nF(xn|vn-i) = 1 - exp\nvn-1 + xn^\nF]\n(2.11)\na\nIf the TTRs are not small when compared to the component operational time, they might be taken into account in the system failure-repair process evaluation. In this way, two different GRP counting processes can be considered: one related to failure occurrences and the other to repair actions. The superimposing of these stochastic processes yields an alternating process that characterizes the component state, i.e., if it is either operational or under a maintenance action.\nSuppose that a system is made of many components and that the failure-repair process of each one of them follows the two above-mentioned GRP. Thus, the failure-repair process of the entire system is a superimposition of all components failure-repair processes, which, in turn, are alternating processes. The analytical handling of the failure-repair process of the full system is not possible and, as an alternative, DES can be adopted. A detailed discussion about RP and NHPP is found in Rigdon &amp; Basu (2000) and Rausand &amp; Hoyland (2004). For more information on GRP, see Kijima &amp; Sumita (1986), Yanes et al. (2002) and Moura et al. (2007).\n2.3\tDiscrete Event Simulation\nThe DES framework is specially useful when the system characterization involves a complex logical structure among its components. The underlying idea of DES is to stochastically generate \u201cevents\u201d of interest over time in order to obtain quantities necessary for evaluating system performance. The basic elements of a DES are variables and events. The variables are usually categorized into: (i) time variable (t), which is the amount of simulated time; (ii) counter variable that refers to the number of times an event have occurred by time t; (iii) system state variable that concerns the state of the system on time t (ROSS, 2002). Whenever an event occurs, these variables are updated and the dynamics of the system can be observed. In the end of the simulation period, an overview of the system behavior during simulation time can be provided along with a performance assessment.\nIn the case of the system configurations related to RAPs, failure and repair of each component are the fundamental events of interest. By means of the states of the various components at a given time and of the system logic, the state of the entire system can be\ndetermined. Indeed, the system logic is often represented by reliability block diagrams (MODARRES et al., 1999) and computationally assessed by Binary Decision Diagram (BDD) (RAUZY, 2001).\n2.3.1\tBinary Decision Diagrams\nA BDD represents a Boolean function as a rooted, directed acyclic graph (BRYANT, 1992). As an illustration, suppose the series-parallel system represented by the block diagram in Figure 2.2 and its related BDD. Each node is characterized by the success event associated with a component (cjj). The solid line corresponds to the realization of the success event - Cj is operational, whereas the dashed line is associated with the occurrence of the failure event - cij is unavailable. The BDD is responsible for evaluating the system state on a given DES time step, in which the state of each component is already defined: Cj is either operational or failed. Thus the probabilities associated with the success event is either 0 or 1, as shown in Figure 2.2. All paths leading to either 0 (failed system) or 1 (available system) are mutually excluding. Furthermore, paths toward 1 represent the minimal paths of the related block diagram (MODARRES et al., 1999). For more on BDD, see Bryant (1992) and Rauzy (2001).\nSystem BDD\nIf Cij is operational, then P(Ej) = 1 and 1 \u2014 P(Ej) = 0; otherwise P(Ej) = 0 and 1 \u2014 P(Ejj) = 1.\nSystem logic function: P(E11)P(E21) + [1 \u2014 P(E11)]P(E12)P(E21) =\n01,,\nif system is down. if system is up.\nFigure 2.2: System BDD\n2.4\tRisk-Based Inspection\nIn risk analysis, usually a qualitative assessment is primarily performed in order to obtain a categorized risk level for the considered equipments. Probabilities of failures and consequences are divided into categories and then combined so as to find an appropriate category for the risk (e.g. high, medium high, medium and low). The equipments with (medium) high risk level are often submitted to a more detailed quantitative risk analysis\n(MODARRES, 2006).\nRBI emerges as a quantitative methodology to support inspection decision making in the context of pressurized equipment often used in petrochemical industry. The risk assessment relies on the evaluation of likelihood or frequencies or of equipment malfunction due to changes in material and structure (e.g. cracks, holes, ruptures) and of the related consequences such as leakage of dangerous substances, fires and explosions.\nFor RBI (API, 2008) the probability of failure is given by:\nPf (k) = gff \u2022 Df (k) \u2022 Fms,\t(2.12)\nin which k is a given time period, gff is the generic failure frequency obtained from a representative failure database; Df (k) is the damage factor related to the applicable damage mechanisms (e.g. corrosion) acting on the equipment and it modifies the gff to make it specific to the equipment under evaluation; FMS is the management systems factor that accounts for the influence of the facility\u2019s management system on the mechanical integrity of the plant equipment and is often obtained by the application of a questionnaire.\nIn RBI, the consequences are determined using consequence analysis techniques and are expressed in affected area or financial terms. In this work, only financial consequences (FC) are considered. The underlying idea of a consequence analysis is to estimate the consequences of releases of hazardous fluids for different hole sizes, which depend on the phase of the fluid. For a pressurized equipment from petrochemical industry, the steps of a consequence analysis are (API, 2008):\n\u2022\tDetermine representative fluid.\n\u2022\tSelect a set of release hole sizes.\n\u2022\tCalculate theoretical release rate/mass.\n\u2022\tEstimate fluid inventory.\n\u2022\tEstablish release type.\n\u2022\tEstimate impact of detection and isolation systems.\n\u2022\tCalculate adjusted release rate / mass.\n\u2022\tDetermine flammable / explosive consequences.\n\u2022\tDetermine toxic consequences.\n\u2022\tDetermine non-flammable non-toxic consequences.\n\u2022\tDetermine component damage and personnel injury consequence areas.\n\u2022 Determine FC.\nThe probability of failure Pf (k) and the FCs can be calculated for different types of damage mechanisms (e.g. internal or external corrosion) and for a number of different hole sizes, h = 1/4\u201d, 1\u201d, 4\u201d, 16\u201d, that are related to small, medium, large and rupture, respectively. For each damage mechanism, there are several costs taken into account: (i) costs of repair and replacement, (ii) costs of damage to surrounding equipment in affected areas, (iii) costs associated with production losses and business interruption as a result of downtime to repair or replace damaged equipment, (iv) costs due to potential injuries associated with a failure, (v) environmental cleanup costs (API, 2008). All of these costs are aggregated for the different types of holes and an overall financial consequence FC, per damage mechanism, is obtained. The total risk value to which the equipment is exposed for a given damage mechanism is as follows:\nR(k) = E Pf(k) \u2022 FC.\t(2.13)\nh\nIf two or more damage mechanisms are present, then the total risk is the sum of the risks related to each of them.\nInspection is an integrity control technique that permits the identification of damage mechanisms responsible for the equipment deterioration process. Although it does not necessarily reduce the involved risk, the knowledge about the presence of damage mechanisms reduces the uncertainty about the actual deterioration state of the equipment. The RBI approach basically consists in the determination of inspection plans with risks that do not exceed a predefined acceptable risk level (risk target). Thus, the calculation of risk should be updated whenever an inspection is performed in order to represent the equipment condition over time.\nActually, as gff, FMS and FC are obtained only once, i.e. they become constant, only the damage factor Df (k) needs to be computed after an inspection and it is responsible for modifying the risk level in time. Also the uncertainty about the Df (k) depends on the efficacy of the inspection technique adopted. For example, the application of an inspection of low (high) efficacy can result in high (low) uncertainty level about Df (k), even if applied in the beginning (end) of the considered period. This is contrary to the intuition of an increasing damage factor, as no interventions are performed to delay such a mechanism. The uncertainty about the Df (k), partially due to the efficacy of the inspection techniques, reflects on the calculated risk values over time.\nThis chapter presented the theoretical foundations used in the subsequent two chapters, which tackle multi-objective problems with cost and availability / risk as conflicting objectives in the design of systems with imperfect repairs and in the construction of inspection plans.\n3\tDESIGN OF SYSTEMS SUBMITTED TO IMPERFECT REPAIRS BY MULTI-OBJECTIVE GENETIC ALGORITHMS AND DISCRETE EVENT SIMULATION\nIn this chapter, the multi-objective RAP considered in this work is formally stated and a solution framework combining MOGA and DES is developed Two examples with known exact Pareto fronts are used to validate the proposed MOGA by means of two performance metrics Also, MOGA + DES is applied to a more realistic example involving systems subjected to imperfect repairs, which are incorporated to DES by a GRP model Indeed, the DES portion of the proposed methodology enables the evaluation of the system\u2019s failure-repair process and of the related costs during its lifetime cycle and not only at the acquisition moment Also in the more realisitic application, the maintenance teams are taken into account as a decision variable itself As a result, nondominated system configurations representing the compromise between availability and cost are obtained and the number teams necessary to maintain the system along its mission time is also indicated . The main aspects and findings of the proposed methodology are in Lins &amp; Droguett (2011)\n3.1\tProblem Statement and Formulation\nSuppose that one has to design a system formed by a predefined number of subsystems in series (s), which, in turn, may have several components in parallel . Also, assume that the designer objectives are to maximize system availability (A) and to minimize system cost (C). There are several components available in market, with different reliability and cost characteristics: suppose that for each subsystem j (j = 1,... ,s) there are ctj different component types that perform the same function. Moreover each subsystem may have a minimum and a maximum number of allowed components (n.j,min, iijmax, respectively). Thus, the designer may obtain a set of nondominated solutions (i.e. system configurations) that represent the compromise between availability and cost. The general mathematical formulation of this problem is as follows:\nmax x\tA(x)\t(3.1)\nmin x\tC (x)\t(3.2)\n\tcc,3\t\ns.t.\tmin _\txjk &lt;^j^maxy\t(3.3)\n\tk=1\t\n\txjk\t{0, 1 . . . ,nj,max\t(3.4)\nin which k = 1,... ,ctj and Xjk is the number of components of type k in the j th subsystem. The argument of both objective functions reflects the dependence of system performance on the performance of its components. In this way, input data regarding the components\u2019 availability and cost features are required. As a result, a set of nondominated solutions of the form\nx (x11 j . . . j x1k j \u25a0 \u25a0 \u25a0 j x1,cti- ... - xj1 \u25a0>...\u25a0> xjk j . . . j xj,ctj - ... - xs1 \u25a0>...\u25a0> xsk j . . . j xs,cts ) (3.5)\nis found.\nThe definition of the objective functions in Equations (3.1) and (3.2) depends on the considered reliability approach during the design phase of the system, whereas the constraints given by Equations (3.3) and (3.4) remain unchanged. If a static analysis is adopted, the components availabilities can be interpreted as steady-state availabilities (constant values) and the system acquisition cost can be computed. Then Equations (3.1) and (3.2) become Equations (3.6) and (3.7), respectively:\nmax\tA(x) =\t1 -\t(1 - Ajk)Xjk j\nj=1 [\tk=1\ns ctj\nmin\tCa(x) = \u00a3\u00a3 j xjkj\nj=1k=1\n(3.6)\n(3.7)\nwhere Ajk and cak are, respectively, the components\u2019 steady-state availabilities and acquisition costs. When the number of possible configurations allows the exhaustive calculation of the Pareto front, then one has an exact solution for the redundancy allocation problem under consideration. This is specially useful for validating heuristic algorithms (MOGAs, for example), since simulated and exact Pareto fronts can be compared.\nThe cost function of Equation (3.7) can be slightly modified to incorporate the steadystate unavailability cost:\nC(x) = Ca(x) + c\"s \u2022 tn \u2022 [1 - A(x)]j\n(3.8)\nin which cS is the cost per time unit related to system unavailability and tn is the mission time. Examples of works that incorporate a static approach to the reliability / availability are Taboada et al. (2007) and Taboada et al. (2008). Nevertheless, the static formulation by no means incorporates the dynamic behavior that the different configurations can have during their lifetime. Hence, a more realistic formulation may be implemented and can be combined with DES to compute system availability and also to provide some metrics for the cost calculation. Additionally, besides optimal system configurations in accordance with a multi-objective perspective, the number of maintenance teams related to a system design can be determined, given that the availability of maintenance teams\nmay be considered by DES when a component failure takes place and has to be repaired. Then the general solution presented in Equation (3.5) has an additional decision variable y at the end, regarding the number of maintenance teams, i.e.,\nx\t(x11 y ... ,x1k , \u25a0 \u25a0 \u25a0 , x1,cti; ... ; xj1, \u25a0 \u25a0 \u25a0 , xjkj . . . i xj,ctj ; ... ; xs1, \u25a0 \u25a0 \u25a0 , xsk , \u25a0 \u25a0 \u25a0 , xs,cts; y).\n(3.9)\nFor the more realistic formulation involving DES, the considered objective functions are:\nmax A(x) = Operational time/Mission time,\t(3.10)\nmin C (x) = Ca(x) + C0(x) + Ccm(x) + Cmt(x) + Cu(x),\t(3.11)\nwhere Ca(x) is the acquisition cost calculated as in Equation (3.7);\nCo(x) = \u00bfEE cok j\t(3.12)\nj = 1k=1 l=0\nis the operational cost, in which cok is the cost per unit time of operating a component of type k in jth subsystem and tokl is the operational time of the /th copy of that component;\nCm. (x) = \u00bfEE\t(3.13)\nj=1k=1l=0\nis the corrective maintenance cost, where ccm is the corrective maintenance cost of a component of type k in subsystem j and njkl is the number of repairs that the /th component undergoes during mission time;\nCmt(x) = cmt \u2022 y\t(3.14)\nis the cost associated with maintenance teams and cmt is the cost of a single maintenance team for the period of mission time;\nCu(x) = cus \u2022 tus\t(3.15)\nis the cost of system unavailability, where cuS is the cost of system unavailability per time unit and tUs is the time in which the system remains unavailable during mission time. This cost can be interpreted as a penalty to the system due to its unavailability for a certain amount of time. The quantities tjokl, njkl, tuS and the objective function concerning system availability (Equation (3.10)) are all obtained via DES. The failure-repair processes of each component are modeled via alternating GRP introduced in Section 2.2.\n3.2\tProposed Multi-objective Genetic Algorithm\nGenetic operators that provide only feasible individuals as outcomes significantly reduces the search space, which is formed by all possible solutions disregarding the physical constraints in Equation (3.3). For the RAP discussed in the previous section, the percentage of the search space concerning feasible solutions is given by:\nng\tfnjiMAx+k-1'\\\nj 1\tk=1 \\ nj,max-1 )\nng=1(nj,max + 1)Ctj\n\u2022 100%,\n(3.16)\nwhere the numerator concerns the number of feasible solutions, whereas the denominator is related to the number of total solutions in the entire search space.\nAs only the feasible search space is explored, the MOGA can not be \u201clost\u201d in some unfeasible location and also the use of penalty functions due to an eventual unfeasibility is not required. Additionally, the fitnesses\u2019 evaluation step can demand an increased computational cost (e.g. DES), which is only used to assess actual potential solutions -otherwise, such an effort would be unnecessarily applied to the evaluation of unfeasible individuals.\nIn the following, let N be the fixed size of population P, Pi the zth individual of P that represents a system configuration, Paux the auxiliary population that stores nondominated individuals and that is updated at each MOGA iteration. Also, let x denote an individual phenotype. The individual representation and the genetic operators devised to create feasible system configurations with respect to constraints in Equation (3.3) are presented in the next subsections.\n3.2.1\tIndividual Representation\nThe redundancy allocation problems tackled in this work have only integer-valued decision variables. Therefore, an integer representation of individuals is used. As an illustration, suppose that a system is formed by two subsystems in series and that each one of them can have at least 1 and at most 4 components in parallel (nj,min = 1 and n.j,max = 4, j = 1,2). Moreover, consider that the first subsystem has 4 component options (ct1 = 4), whereas the second subsystem has only 2 component options (ct2 = 2). Each xjk value (xjk G {0,1,..., 4}, j = 1, 2, k = 1,..., ctj) represents the number of every component type that may be put in parallel in its respective subsystem. In addition, y is the decision variable associated with the number of maintenance teams that can be hired and that y G {0,1,..., 10}. The individual phenotype as well as the system design it represents are depicted in Figure 3.1. It is important to note that y can be interpreted as an additional subsystem having only one available type of component and with minimum and maximum number of redundant components respectively equal to the\nminimum and maximum maintenance teams that can be used. Such interpretation is very useful, since the developed genetic operators do not make any difference among variables\u2019 nature. Hence, in the example just described and if at least one maintenance team is required, s = 3, ct3 = 1, n3)mm = 1 and n3>max = 10.\nSubsystem\t1\t2\t\t3\nVariable\tX11\tX12\txi3\tX14\tX21\tx22\ty\nSystem configuration\t2 0 11\t3\t0\t4\nSubsystem 1\n4 maintenance teams\nFigure 3.1: Individual representation for MOGA (system configuration)\n3.2.2\tGeneration of Initial Population\nEach one of the N individuals of the initial population is generated in accordance with Algorithm 1, whose main idea is to create one subsystem at a time and to generate random integers for the different component types within the considered subsystem. A vector pos containing a permutation of 1j . . . j ctj is used for the generation of the variables\u2019 values so that one component type has no advantages over the other ones. The parameters nj,min and nj,max along with the number of components already allocated in subsystem j enables the generation of feasible values for xj,posk. Whenever a subsystem attains the maximum number of components n.j,max, the other positions are set to 0. A feasible individual represented by x is the result of Algorithm 1.\n3.2.3\tSelection and Update of the Auxiliary Population\nIn the selection step, the dominance relation among individuals is evaluated according to their fitness values (fitness evaluation step is described in Section 3.3). Firstly, dominance relation is assessed between pairs of individuals within the current population. Dominated individuals are eliminated and the nondominated ones are then candidate solutions to become part of the auxiliary population Paux. Secondly, Paux update takes place\n1.\tFor j = 1,\na.\tSet S = 0.\nb.\tWhile S &lt;nj min:\nI.\tS = 0.\nII.\tpos random permutation of 1,..., ctj\nIII.\tFor k = 1,... ,ctj:\ni.\tIf nj;max S > 0, generate xj,posk from 0- . . . , nj,max S and S S + xj,posk .\nii.\tElse if nj,max \u2014 S = 0, set Xj,poSo = 0 for o = k + 1,..., ctj and k ctj + 1.\n2.\tReturn x.\nAlgorithm 1: Generation of a feasible individual\nin accordance with the following rules:\n\u2022\tIf a candidate solution is dominated by some individual in Paux, it is discarded.\n\u2022\tIf a candidate solution dominates individuals in Paux, all dominated solutions are deleted from Paux and a copy of the candidate solution is stored in Paux.\n\u2022\tIf a candidate solution neither dominates nor is dominated by individuals in Paux, it is inserted into Paux.\nSince the dominated individuals are eliminated, the size of P is reduced to Nr (Nr &lt;N). In order to maintain the population with N individuals, N \u2014 Nr solutions are randomly selected from Paux and inserted into P. Algorithm 2 summarizes the selection and the update of Paux; I represents the set of indexes related to dominated solutions in the current population P.\n1.\tFor i = 1,..., N:\na.\tIf Pi is nondominated in P:\nI. If Pi is nondominated in Paux, insert a copy of Pi into Paux.\nII.\tElse if Pi dominates individuals in Paux, eliminate dominated individuals from Paux and insert a copy of Pi into Paux.\nIII.\tElse if Pi is dominated by any solution in Paux, ignore Pi.\nb.\tElse if Pi is dominated in P, insert i into I.\n2.\tEliminate Pi from P for all i G I.\n3.\tRandomly select N \u2014 Nr individuals from Paux and insert them into P.\n4.\tReturn P and Paux.\nAlgorithm 2: Selection and update of Paux\nWith the elimination of dominated individuals from P, the update of Paux and the posterior random choice of its solutions to complete P, the MOGA tends to maintain solutions of higher expected quality, whose information can be exchanged and modified in the crossover and mutation steps, respectively. One may wonder if this approach would harm the MOGA ability in exploring the search space, as it could lead the algorithm to local Pareto sets. However, it is important to emphasize that the solutions that are nondominated in P but dominated by individuals in Paux remain in P. Such individuals\ncan have information of non-explored parts of the search space and provide the required variability among potential solutions.\nAnother possibility would be not updating Paux until the stop criterion is reached. Some early experiments indicated that such a strategy have better performance in realvalued problems than in integer-valued ones. The obtained Pareto fronts in the former type of problems were much better with the non-updating approach and the opposite was observed for the latter problems, i.e the Pareto fronts of the integer-valued problems were enhanced by the updating strategy. Also, there were some problems concerning the size of the auxiliary population, which sometimes became prohibitively large. Since only integer-valued problems are taken into account in this work, the update of Paux at every iteration is adopted.\n3.2.4\tCrossover and Replacement\nAfter selection, a random number in [0,1] is generated for each individual in P. If this number is less than the crossover probability pc for a certain Pi, this individual will participate in the crossover. In this work, the proposed crossover operator handles integer variables and avoids the creation of unfeasible individuals.\nFor each pair of individuals participating in crossover (parents), m different positions are defined in their phenotypes (ignoring subsystems\u2019 boundaries) by the generation of random numbers in [1, n], where n is the phenotype length or the total number of variables. As long as these positions are chosen, the subsystem to which they belong is verified and their contents are exchanged between parents in order to generate children.\nIn a first moment, these children can be unfeasible, i.e. they can present subsystems with forbidden quantities of components . With the aim of rendering an eventual unfeasible subsystem j into a feasible one, one position at a time is randomly chosen among the m selected in the preceding step and its content can be:\n\u2022\tAdded by one unit, if j has less than nj,min components or\n\u2022\tDiminished by one unit if it is greater than the minimum value the related variable can assume and if j contains more than nj,max components.\nSuch a procedure is repeated until the number of components in j is in [nj,min, nj,max]. Since only one unit is incremented / decremented at a time, the algorithm tendency in providing a lower / greater number of components in subsystem j is not severely modified.\nThe crossover step for a given couple of parents x1 and x2 is detailed in Algorithm 3. Note that the replacement strategy \u201cchildren replace parents\u201d is already incorporated in the crossover, as Algorithm 3 returns the modified x1 and x2 (children), which automatically replace the original parents x1 and x2.\n1.\tFor a given pair of parents x1 and x2, randomly select m positions and exchange their contents.\n2.\tFor j = 1,... ,s and for o =1, 2:\nS o\tV\"ctj xo\na.\tS\tn k=1 xjk.\nb.\tSelect a random position w in subsystem j whose content was exchanged.\nc.\tWhile So &lt;nj,min, x\u00b0w\tx\u00b0w + 1 and So\tSo + 1.\nd.\tWhile So > nj,max:\nI. If x\u00b0w > 0, x\u00b0w\tx\u00b0w - 1 and So\tSo - 1.\nII. Else if xow = 0, eliminate w from the possible positions to be selected.\n3.\tReturn x1 and x2.\nAlgorithm 3: Crossover and replacement return feasible individuals\n3.2.5\tMutation\nThe mutation step consists in changing the content of a position in an individual phenotype. If a position is selected in accordance with the predefined mutation probability pm, its content is substituted by a random integer uniformly generated in an interval that does not violate the individual\u2019s feasibility. If position k from the jth subsystem is selected, then the number of components of other types in the current subsystem (S-k), nj,min and nj,max are taken into account to generate a new value for xjk, as described in Algorithm 4.\n1. For j = 1,...,s:\na. For k = 1,... ,ctj:\nI. Generate u from U(0,1).\nII. If u &lt;pm:\nL S-k = xj1 + \u2022 \u2022 \u2022 + xj,k\u20141 + xj,k+1 + \u2022 \u2022 \u2022 + xj,ctj .\nii. If S\u2014k > nj,min generate xjk from 0- . . . , nj,max\tS\u2014k.\niii.\tElse if S\u2014k &lt;nj, min generate xjk from nj,min\tS\u2014k, . . . , nj,max\tS\u2014k.\nb. Return x.\nAlgorithm 4: Mutation returns a feasible individual\n3.3\tAssessment of Fitnesses by Discrete Event Simulation\nThe MOGA summarized in the previous section is coupled with Monte Carlo simulation for a more realistic representation of the dynamic behavior of systems. In particular, DES is used to analyze the behavior of a system, which consists in the generation of random discrete events during simulation time with the aim of creating a \u201ctypical\u201d scenario for a system so as to allow for the evaluation of some of its features that are of interest for the calculation of the objective functions of an individual Pi previously defined.\nAs depicted in Figure 3.2, the coupling takes place at the fitness evaluation step. For a given generation in the genetic algorithm evolution process, the availability objective function is estimated for every individual Pi G P, i = 1,... ,N, via the DES algorithm. For a predetermined mission time, tn, the ith individual (a candidate system) undergoes stochastic transitions between its possible states which, for the problem of interest, corre\nspond to available and unavailable states. The system availability estimation is based on its components, i.e., the system evolves through states of availability and unavailability depending on the components states: available or unavailable.\nThe failed components along with the available maintenance teams form a queueing system (ROSS, 2010), in which the former are the \u201ccustomers\u201d and the latter are the \u201cservers\u201d. When a component fails, it is necessary to evaluate if there is any maintenance team available to perform the repair action. If there is an available maintenance team, then the component has to wait until it becomes ready to initiate the repair. If there is not, the component is added to a queue (in this work, the modeled queue is without priority, since all components are supposed to have the same relevance). Under this circumstance, the component must wait until all components that were already in queue to be repaired. Moreover, when the considered component becomes the very next to be repaired, it may have to wait the delay previously commented. Therefore, a component is said to be unavailable if it is:\n\u2022\tFailed and in queue: this occurs when all maintenance teams are occupied.\n\u2022\tFailed and waiting to initiate repair: the component is the very next to be repaired, however there is a delay from the time a maintenance team becomes available to\nthe time it is ready to perform the repair action.\n\u2022 Under repair.\nIndeed, the availability quantification for the zth individual starts by subdividing the mission time tn into n time steps. For a given iteration, say tk, with k = 1,... ,n, the current state for each component is evaluated, as shown in Figure 3.2-b. In order to determine the system state from its components states at tk, one needs to construct the relationship characterizing the system logic as a function of the components. In the proposed approach, this is achieved by means of BDDs, briefly described in Section 2.3.1. Thus, in order to keep track of the simultaneous evolutions of all components of the zth individual as well as of the system itself during the mission time, the observed realizations of the following random variables are recorded: number of failures and repairs of each component, number of times the system fails, the time intervals for which each component is under repair (corrective maintenance) and the time intervals during which the system is in the available state. The DES is replicated several times such that independent realizations of these random variables are obtained and then used to estimate the point availability, A(t), for the zth system over the mission time. Then, one obtains estimates for the two objective functions, namely the system average availability, and the system total cost, where both metrics are estimated over the mission time. These results are then fed back to the MOGA, as illustrated in Figure 3.2.\nThe greater the number of steps and replications, the better the DES estimates. However, given that the redundancy allocation problem may involve a considerable number of components, each with a particular failure-repair process, DES may demand great computational and time efforts. Both MOGA and DES were implemented in C++.\n3.4\tOverview\nThe proposed MOGA described in Section 3.2 is summarized in Algorithm 5. Note that the coupling between MOGA and the DES detailed in Section 3.3 takes place at the fitness evaluation step. The algorithm is repeated until the maximum number of generations Ngen is attained. At the end, the algorithm returns Paux with the feasible and overall nondominated individuals, which represent efficient system configurations in terms of mean availability and cost. In addition, Figure 3.2 depicts the flowchart of the proposed MOGA + DES with further details of the DES portion of the methodology.\n3.5\tMetrics for Comparing Real and Simulated Pareto Fronts\nAs mentioned earlier, the static availability approach of a multi-objective redundancy allocation problem is useful to assess the performance of MOGA by comparing the sim-\n1.\tSet g = 0 and generate initial population.\n2.\tFor i = 1, . . . , N:\na.\tCompute A(xi) via DES.\nb.\tObtain metrics from DES and calculate C(xi).\n3.\tPerform selection and update of Paux.\n4.\tIf g = Ngen, go to step 7; else go to step 5.\n5.\tPerform crossover and replacement.\n6.\tPerform mutation, g g + 1, go to step 2.\n7.\tReturn Paux.\nAlgorithm 5: Proposed MOGA + DES\nulated Pareto fronts with the exact one. Such a comparison requires the use of metrics that can, even if heuristically, represent the convergence of the simulated Pareto front towards the exact one.\nSystem cost and system steady-state availability belong to different scales. Therefore, in order to avoid scale problems, the values of the objectives are scaled in [0,1]. The scaling factors, i.e. the minimum and maximum observed cost and availability, are provided by the exact Pareto front, which can be obtained by an exhaustive algorithm. Two distance metrics to be applied in the objective space are considered and then described.\n3.5.1\tPoint-to-Point Distance\nFor each point in the obtained front, the minimum Euclidean distance from it to one of the points in the real front is computed. In order to find a mean distance representing an entire front (<di, i = 1,... ,nf), all minimum distances (d, see Figure 3.3) are summed and divided by nsi (number of obtained nondominated solutions in the ith simulated front). Then, the following weighted mean is calculated:\nD=\nEi=1 di \u2022 nsi\n\u2022^nf\nEi=1 nsi\n(3.17)\nas an attempt to summarize the convergence of the obtained fronts in one single number. However, note that in this procedure there is loss of information since the solution is the entire front and not only one single point.\n3.5.2\tCoordinate Distance\nAs a consequence of the dominance / nondominance relation, of the nature of the handled objectives and also of the objective space presentation (e.g. cost as the horizontal axis and steady-state availability as the vertical one), a monotonically increasing Pareto front is obtained. The coordinate distance metric would be very natural if the real Pareto front were continuous. However, the considered problem is discrete and a suitable approximation consists in supposing the steady-state availability as a piecewise\n\u2022 Real Pareto front\n\u25a0 Simulated Pareto front\nd - minimum distance\nFigure 3.4: Coordinate distance\nlinear function of the system cost. Hence each two consecutive points j and j + 1 from the exact Pareto front are connected by a straight line segment with angular coefficient Uj and intercept , that is, Aj(C) = UjC + , C G [Cj, Cj+1]. Thus, each point (Csim, Asim) from the simulated front has two associated distances (see Figure 3.4): a horizontal distance h, shown in Equation (3.18) and a vertical distance v, given in Equation (3.19):\nh = Csim - (Asim\t) ,\tCsim G [Cj , Cj + 1],\nUj\n(3.18)\nv \u2014 Uj Csim + \u00a3,j\tAsim,\tCsim G [Cj , Cj+1]-\n(3.19)\nThen, for every simulated Pareto front, a mean horizontal distance (hi) and a mean vertical distance (vi) are calculated: all horizontal distances (h, Figure 3.4) are summed and divided by nsi and all vertical distances (v, Figure 3.4) are summed and divided by nsi. Similar to the case of the point-to-point distance, weighted means H and V regarding\nhorizontal and vertical distances, respectively, are calculated:\nH\nhi \u2022 nsi\n\\-<nf\nEi=i nsi\nnf\nEi=i Vi \u2022 nsi\n\u2022^nf\nEi=i nsi\n(3.20)\n(3.21)\nThese metrics are used as an approximation for the convergence of the obtained fronts toward the real one and small values for them are desired. For further details on metrics\nfor comparing Pareto fronts, see Knowles &amp; Corne (2002) and Yan et al. (2007).\n3.6\tNumerical Experiments\nAll experiments described in this section were executed in a PC with Windows operating system, 2.0 GHz processor and 2 GB of RAM.\n3.6.1\tValidation Examples\nIn order to validate the proposed MOGA, two analytical examples are devised so as to allow for the comparison between the exact Pareto fronts and the results obtained via MOGA. For both of them suppose a series system composed by 3 subsystems (s = 3), which, in turn, may have several components in parallel. The aim is to maximize system steady-state availability and minimize system cost. Table 3.1 lists some configuration features of each subsystem. Components steady-state availabilities (Ajk) and acquisition costs (cak) are shown in Table 3.2. These examples have each 1,394,525 feasible components combinations, which represent 0.0135% of the search space (see Equation (3.16)), and were resolved by means of an exhaustive recursive method in order to obtain the exact sets of Pareto nondominated solutions.\nTable 3.1: Subsystems\u2019 characteristics\nj\tnj,min\tnj,max\tctj\n1\t1\t3\t5\n2\t1\t5\t6\n3\t1\t5\t3\nIn the first validation example, Example 1, system cost is defined only by components acquisition cost. The multi-objective formulation is given by Equations (3.6), (3.7), (3.3) and (3.4). The associated real Pareto front is formed by nr = 144 nondominated system designs. The second example, Example 2, takes into account the cost due to system unavailability along with the acquisition cost, thus the cost objective is calculated by\nTable 3.2: Example 1 - Components\u2019 characteristics\n_\t, Subsystem 1\nComponent -----j\u2014  \nAjk\tcjk\nSubsystem 2 Subsystem 3\nAjk\tcjk\tAjk\tcjk\n1\t0.9861\t10,500\t0.9346\t6,400\t0.9272\t7,500\n2\t0.9699\t9,100\t0.9664\t8,000\t0.9175\t6,900\n3\t0.9684\t8,300\t0.9512\t7,600\t0.9343\t6,200\n4\t0.9777\t8,700\t0.9672\t9,300\t-\t-\n5\t0.9769\t8,000\t0.9568\t7,000\t-\t-\n6\t-\t-\t0.9570\t8,800\t-\t-\nEquation (3.8). The parameter cu is set to 500 monetary units per time unit and tn is set to 730 time units. A set of nr = 119 nondominated system designs outlines the exact Pareto front.\nAfterwards, nf = 100 trials of the MOGA were executed for each example and the distance metrics of Equations (3.17)-(3.21) were used to provide an idea of the proximity of simulated and real Pareto fronts. The scaling factors obtained from the exact Pareto front are presented in Table 3.3, whereas Table 3.4 shows the parameters used to feed the MOGA.\nTable 3.3: Scaling factors\nExample\tCost\t\tSteady-state availability\t\n\tMin.\tMax.\tMin.\tMax.\n1\t20,600.00\t109,000.00\t0.853026\t0.999996\n2\t43,859.94\t109,001.44\t0.977644\t0.999996\nTable 3.4: MOGA parameters\nParameter\tValue\nPopulation size (N) Number of generations (Ngen) Probability of crossover (pc) Number of variables for crossover Probability of mutation (pm)\t100 200 0.95 7 0.01\nTable 3.5 summarizes results from Examples 1 and 2. It presents D, H, V and some descriptive statistics (minimum, maximum, mean and standard deviation) regarding the number of solutions in each simulated Pareto front as well as the number of obtained exact Pareto solutions. Both examples were also solved by a MOACO algorithm (100 replications), whose essence is described in (LINS &amp; DROGUETT, 2008). The used parameters for the MOACO were: 100 ants, 1000 cycles, a = 1, /3 = 1, q = 0.5 and\nQ = 1. The related results are presented in the last two columns of Table 3.5. It can be observed that the proposed MOGA is, in general, more accurate than the MOACO. Indeed, the maximum number of solutions obtained by the MOACO is lower than the minimum number of solutions provided by the MOGA for Examples 1 and 2. The same reasoning can be applied to the number of exact solutions.\nTable 3.5: Results of validation Examples 1 and 2 - MOGA x MOACO\n\t\tMOGA\t\tMOACO\t\n\t\tExample 1\tExample 2\tExample 1\tExample 2\n\tD\t4.71 \u2022 10-4\t1.05 \u2022 10-3\t7.39 \u2022 10-4\t1.87 \u2022 10-3\nDistance metrics\tH\t3.10 \u2022 10-3\t5.48 \u2022 10-3\t3.88 \u2022 10-3\t7.13 \u2022 10-3\n\tV\t2.08 \u2022 10-4\t7.80 \u2022 10-4\t2.29 \u2022 10-4\t1.80 \u2022 10-3\n\tMin.\t107\t84\t74\t51\nNumber of solutions\tMax.\t143\t114\t89\t67\n\tMean\t130.6\t101.2\t81.6\t58.9\n\tStd. dev.\t6.06\t5.47\t3.19\t3.48\n\tMin.\t73\t45\t50\t29\nExact solutions\tMax.\t132\t96\t65\t41\n\tMean\t106.1\t77.7\t56.8\t34.8\n\tStd. dev.\t12.74\t11.97\t2.79\t2.47\nFigures 3.5 and 3.6 depict the exact and two simulated Pareto fronts for both Examples 1 and 2, respectively. Note that the Pareto front from Example 1 is more scattered than the one associated with Example 2, which is concentrated in higher values of the steadystate availability. Since Example 2 incorporates the unavailability cost, the solutions may reflect this fact, that is, designs with greater steady-state availabilities may incur in higher acquisition costs but lower unavailability costs. The simulated fronts in those figures correspond to the ones with minimum and maximum exact Pareto solutions. It can be noticed that the MOGA is able to find solutions on or near the real Pareto front.\n3.6.2\tApplication Example\nIn this section, an application example is discussed. As in the previous examples, the system is formed by 3 subsystems, whose characteristics are shown in Table 3.1. The objectives taken into account are the maximization of availability and minimization of system total cost. System availability, as well as several parameters used in the calculation of system total cost, is estimated by means of the DES portion of the methodology. The aim is to find the configurations, including the number of maintenance teams, that are compromise solutions between system availability and system total cost. The formulation of the redundancy allocation problem involves Equations (3.3), (3.4), (3.10) and (3.11).\nThe available components of each subsystem have diverse reliability and cost charac-\nCost (x 104)\nFigure 3.5: Validation example 1 - Exact and simulated Pareto fronts\nFigure 3.6: Validation example 2 - Exact and simulated Pareto fronts\nteristics, as presented in Table 3.6. The TTFs - X - are supposed to follow Weibull distributions with different scale ) and shape (0) parameters and with the same rejuvenation parameter (q). The TTRs - D - in turn, are supposed to be exponentially distributed with different parameters and the expected value associated to each D, Mean Time To Repair (MTTR), is also given. Notice that other probabilistic distributions, e.g. Log-normal, could also be used. Often, in practice, the adoption of a distribution depends on components\u2019 historical data related to TTFs and TTRs, so as parameter estimation and goodness-of-fit evaluation can be performed via statistical inference methods. Also, as mentioned in Section 1.2 and illustrated in Figure 1.1, instead of using probabilistic distributions, one can adopt previously adjusted SVR models to provide components\u2019 failure times.\nIn addition, cmt = 1200; cSS = 500; the number of maintenance teams (y) is in {0,1,..., 7}, but at least one is required and the time since a maintenance crew is or becomes available up to the beginning of repair is supposed to be exponentially distributed with parameter 1. The addition of the number of maintenance teams as a decision variable increased the number of feasible solutions to 9,761,765 that represents 0.0118% of the search space (see Equation (3.16)).\nThe Mean Time to First Failure (MTFF) is also presented in Table 3.6 in order to provide an idea of the components reliability features. If perfect repairs are assumed the MTFF is also equal to all the Mean Time Between Failures (MTBF), since components are delivered to operation as if they were new. On the other hand, in minimal and imperfect repairs approaches, MTBF are smaller than the mean time to first failure, given that component deterioration processes are taken into account.\nTable 3.6: Application example - components\u2019 characteristics\nJ\tComponent\tfX (x)\tMTFF\tfD (d)\tMTTR\tca cjk\tco cjk\tcjk\n\t1\tWeibull(40; 1.9; q)\t35.4945\tExp(2.0)\t0.5000\t10,500\t210\t1,050\n\t2\tWeibull(36; 1.6; q)\t32.2767\tExp(1.0)\t1.0000\t9,100\t182\t910\n1\t3\tWeibull(34; 1.5; q)\t30.6933\tExp(1.0)\t1.0000\t8,300\t166\t830\n\t4\tWeibull(32; 1.4; q)\t29.1655\tExp(1.5)\t0.6667\t8,700\t174\t870\n\t5\tWeibull(30; 1.2; q)\t28.2197\tExp(1.5)\t0.6667\t8,000\t160\t800\n\t1\tWeibull(19; 1.2; q)\t17.8725\tExp(0.8)\t1.2500\t6,400\t128\t640\n\t2\tWeibull(29; 1.5; q)\t26.1796\tExp(1.1)\t0.9091\t8,000\t160\t800\n2\t3\tWeibull(23; 1.2; q)\t21.6351\tExp(0.9)\t1.1111\t7,600\t152\t760\n\t4\tWeibull(27; 1.4; q)\t24.6084\tExp(1.2)\t0.8333\t9,300\t186\t930\n\t5\tWeibull(24; 1.3; q)\t22.1658\tExp(1.0)\t1.0000\t7,000\t140\t700\n\t6\tWeibull(31; 1.6; q)\t27.7938\tExp(0.8)\t1.2500\t8,800\t176\t880\n\t1\tWeibull(22; 1.1; q)\t21.2281\tExp(0.6)\t1.6667\t7,500\t150\t750\n3\t2\tWeibull(25; 1.7; q)\t22.2322\tExp(0.5)\t2.0000\t6,900\t138\t690\n\t3\tWeibull(26; 1.8; q)\t23.6970\tExp(0.6)\t1.6667\t6,200\t124\t620\nIn order to analyze the impacts caused by different types of corrective maintenance\nactions (minimal, imperfect and perfect repairs) in the multi-objective optimization of system designs, the same application example was solved three times, each one considering a specific repair policy. Hence, the rejuvenation parameter (q) was equal to 1 when components were subjected to minimal repairs and equal to 0 when perfect repairs were executed. In the situation in which maintenance teams performed imperfect repairs, all q were set to 0.5. As in the validation examples, the considered mission time was tn = 730 time units. For the MOGA, N = 75, Ngen = 150 and all the other parameters\u2019 values are the same as the ones shown in Table 3.4. Steps and replications of the DES portion were set to 30 and 100, respectively.\nThe number of globally nondominated solutions obtained from MOGA + DES for the minimal, imperfect and perfect repairs were, respectively, 78, 89 and 106, which are depicted in Figure 3.7. It can be observed that the points regarding perfect repairs are above the solutions related to imperfect repairs, which in turn, are over the compromise solutions associated with minimal repairs. Indeed, solutions of the perfect repair approach, in most cases dominate solutions concerning imperfect repairs and these latter results, in general, dominate minimal repair solutions. This is an expected behavior since the system with components subject to imperfect repairs may have an intermediate performance between systems with components that undergo either minimal or perfect repairs.\nCD o\nE\n\u25b2\nn ro\nro\n>\n00\no\n\u25a0\n\u25a0\n\u25a0 A\nH\nDo\nDU\n\u25b2 G\n\u25a0 Perfect repairs\n\u2022 Imperfect repairs A Minimal repairs\n0.4\t0.6\t0.8\t1.0\t1.2\t1.4\t1.6\nCost (x 106)\nFigure 3.7: Application example MOGA + DES - Obtained Pareto fronts for each type of repair.\nIf a repairable system with perfect repairs is considered, one assumes that maintenance\nImperfect\tF\nPerfect\tC\n2 teams\nMinimal\tI\n4 teams\nFigure 3.8: Application example MOGA + DES - selected solutions related to perfect, imperfect and minimal repairs\nteams are so efficient that they can bring failed components to a condition \u201cas good as new\u201d. If otherwise minimal repairs are supposed, maintenance teams can only repair a failed component and deliver it to operation in an \u201cas bad as old\u201d condition. However, in practice, imperfect repairs are often performed. Thus, in such real situations, if perfect repairs are assumed, system mean availability may be overestimated, whereas if minimal repairs are considered, it may be underestimated: note that points regarding perfect repairs in Figure 3.7 have the highest mean availabilities and the ones related to minimal repairs have the lowest mean availabilities. Figure 3.8 illustrates the system configurations related to some selected solutions from the three Pareto fronts.\n3.6.2.1\tReturn on Invesment Analysis\nSince all the Pareto solutions are optimal in a multi-objective perspective, a suggestion to the decision maker is to evaluate the gain in availability due to an investment in system design between pairs of solutions in the Pareto front, that is, to perform a ROI analysis.\nMathematically:\nROI =\nAi - Aj\nCi - Cj'\ni = J,\n(3.22)\nin which Ai and Aj are the mean availabilities of configurations i and j, in this order, and Ci and Cj are their respective costs. As an example, consider the solutions indicated in Figure 3.7. It can be inferred from Table 3.7 that the investment in system design to gain about 0.3 in mean availability is 384,123.83 (from solution A to solution B). On the other hand, it is necessary to invest 770,855.97 to obtain an addition of about 0.04 in system mean availability (solution B to solution C). Thus, high investments on system design do not necessarily result in great gains on system mean availability. The same analysis can be done for the minimal and perfect repair cases and the ROIs of selected solutions related to each repair type are also shown in Table 3.7.\nTable 3.7: ROI of selected solutions related to minimal, imperfect and perfect repairs.\nRepair\tSolution\tMean availability\tTotal system cost\tROI\n\tA\t0.6288\t533,345.37\t6.5899 x 10-7\nMinimal\tB\t0.9682\t1,048,371.86\t5.2418 x 10-8\n\tC\t0.9986\t1,628,322.45\t\n\tD\t0.6622\t514,312.39\t7.6798 x 10-7\nImperfect\tE\t0.9572\t898,436.22\t5.4615 x 10-8\n\tF\t0.9993\t1,669,292.19\t\n\tG\t0.7545\t443,753.28\t8.3226 x 10-7\nPerfect\tH\t0.9476\t675,771.58\t5.3070 x 10-8\n\tI\t0.9999\t1,661,268.77\t\n3.7\tSummary and Discussion\nThis chapter presented a procedure based on the coupling of a MOGA with DES to find compromise solutions for redundancy allocation problems related to systems subject to imperfect repairs. The genetic operators developed to handle only feasible solutions significantly shrink the search space to be explored by MOGA and the evaluation of fitnesses does not require penalty functions to tackle unfeasibility. The two considered objectives were the system mean availability and the system total cost. The second objective comprised not only the acquisition cost, but also other costs often incurred in practical situations such as operating and maintenance costs and the penalties due to system unavailability. Moreover, the number of maintenance teams was considered as a decision variable and the (un)availability of that resource during simulation time was also taken into account.\nIn order to tackle the unrealistic assumptions regarding the reliability behavior of systems that are imposed by the majority of the procedures to handle redundancy allocation\nproblems, the system failure-repair process was modeled according to GRP providing a more realistic treatment of the problem. In this context, the proposed approach is able to circumvent the perfect or minimal repairs assumptions underpinning other approaches such as NHPP or RP when components are in fact subjected to imperfect repairs.\nAs it was demonstrated by means of the validation examples, the proposed MOGA was able to provide solutions on or very near the exact Pareto fronts. With the purpose of measuring the difference between real and simulated Pareto fronts, two distance metrics were presented. Their achieved values were satisfactory, all of them smaller than 10-2. Moreover, the glsmoga provided more exact solutions of the real Pareto fronts than the MOACO, as well as better values for the presented distance metrics.\nThe application example was solved three times, each one considering a specific type of maintenance action. As expected, the solutions related to perfect repairs completely dominated the ones associated with either imperfect or minimal repairs. After obtaining the nondominated solutions, a ROI analysis was suggested to aid the decision maker in choosing a specific design and the number of maintenance teams to be hired. It was observed that considerable investments in system design do not necessarily result in great gains on system mean availability.\n4\tELABORATION OF INSPECTION PLANS BY MULTIOBJECTIVE GENETIC ALGORITHMS AND RISK-BASED INSPECTION\nThe multi-objective problem concerning the elaboration of inspection plans based on risk and cost is mathematically formalized in this chapter. The proposed solution framework combining MOGA and RBI is provided along with the description of the genetic operators adapted to return only feasible individuals (i.e. feasible inspection plans). The performance of the MOGA + RBI is evaluated by means of a validation example for which the true Pareto front is known. Also, the proposed methodology is applied to an example involving a separator vessel of oil and gas. The main results of this chapter can be found in Furtado et al. (2012).\n4.1\tProblem Statement and Formulation\nAn inspection plan x is represented by the following vector:\nx\t(x11 ,''',x1fc \u25a0>\u25a0\u25a0\u25a0\u25a0> x1m; \u2022 \u2022 \u2022 - xj1 j \u25a0 \u25a0 \u25a0 j xjk j \u2022 \u2022 \u2022 j xjm; \u25a0 \u25a0 \u25a0 - xn1 j \u25a0 \u25a0 \u25a0 j xnk j \u2022 \u2022 \u2022 j xnm) j\t(4.1)\nin which n is the number of available inspection techniques and m is the number of time steps considered in the planning horizon. Each element xjk of x is either 0 or 1, j = 1, \u25a0 \u25a0 \u25a0 , n and k = 1, \u25a0\u25a0\u25a0 ,m. If xjk = 1, then an inspection involving technique j is performed at period k. Otherwise, if xjk = 0, then there is no inspection of type j at period k. In this way, at most one inspection using a given technique is allowed in the same period.\nGiven a planning horizon with m periods and a number n of inspection techniques, the multi-objective optimization problem consists in the selection of inspection plans x that represent the compromise between risk and cost and that satisfy the maximum allowed intervals between inspections (tj,max, j = 1j \u25a0 \u25a0 \u25a0 jn) for each considered technique. The problem is mathematically defined as follows:\nmin x\tnm C(x) = y? (cj \u25a0 y? xjk)+ cp'\tnm e e xjk+\u00b0d\tm \u2022 E E xjk\t(4.2)\n\tj=1 \\\tk=1\t/\tj=1 k=1\tjEJ' k=1\t\nmin x\tm R(x) = e R(k) k=1 xj,l+1\t'\t+ xj, l+tj,max + 1 > 1\t\t\t(4.3)\ns.t.\t\t^jj\t\t(4.4)\nwhere I represents a period with an inspection (j = 1) and can possibly assume any of the values 1j \u25a0 \u25a0 \u25a0 m \u2014 tj,max \u2014 1. Thus, the constraints in Equation (4.4) concern the\nsummations of inspections over groups of tjmax + 1 periods, which have to be at least 1, that is, at least one inspection using technique j have to be performed in such an interval. These constraints are usually related to inspection regulations devised for a specific type of equipments.\nIn the cost objective function (Equation (4.2)), Cj is the cost of performing technique j, cp is the cost of qualified personnel per inspection, cd is the downtime cost per period and J' is the set of techniques requiring equipment interruption to be executed. The risk objective function R(x) (Equation (4.3)) concerns the total risk associated to an inspection plan x within the planning horizon m; it is the sum of the risks calculated at every period k, which are defined in Equation (2.13).\nIt is important to emphasize that the financial consequences that are part of the risk calculation (Equation (2.13)) and the cost objective function (Equation (4.2)) are of different nature. The former is related to expenditures due to failure occurrence, e.g. equipment and system repairs, environmental cleanup, injuries to personnel and nearby communities, legal penalties, among others. The latter, in turn, is associated to the inspection activity.\n4.2\tProposed Multi-objective Genetic Algorithm\nAs in the case of the proposed MOGA for RAPs described in Chapter 3, the genetic operators are devised to allow only for feasible individuals, i.e. inspection plans. As previously commented, such an approach have the following advantages: (i) it reduces the search space to be explored by the MOGA; (ii) it prevents the MOGA from being lost in an unfeasible part of the search space; (iii) it avoids unnecessary fitness evaluations of unfeasible individuals; (iv) the use of penalty functions due to unfeasibility is not required. For the problem characterized in the previous section, the total number of solutions in the search space, when the constraints in Equation (4.4) are not taken into account, is given by 2m'n. In turn, the number of feasible inspection plans for a given technique j is ampj = [xm] fPj (x)}, defined as the coefficient of xm in the x-power series expansion of fPj (x), where\nfP] (x) =\n2(x + x2 + \u2022 \u2022 \u2022 + xPj) \u2014 xPj\n1 \u2014 x \u2014 x2 \u2014 \u2022 \u2022 \u2022 \u2014 xPj\n(4.5)\nfor pj = t j,max + 1. Hence, the percentage of the search space concerning feasible inspection plans is given by\nam,pi \u2019 am,p2\nam,pn\n2m-n\n\u2022 100%\n(4.6)\nAs an illustration, suppose n =1, m = 20, p = 3 (thus, t1>max = 2). The x-power series expansion of fP1 (x) is\n2x + 4x2 + 7x3 + 13x4 + 24x5 + 44x6 + 81x7 + 149x8 + 274x9 + 504x10 + 927xn + 1705x12 + 3136x13 + 5768x14 + 10609x15 + 19513x16 + 35890x17+66012x18 + 121415x19+223317x20 + ...\nTherefore the number of feasible inspection plans is a20,3 = 223, 317 and it represents about 21.3% of the search space. For further details on enumeration techniques, the reader is referred to Lins (1981).\nNext sections detail the individual representation along with the proposed genetic operators for the generation of initial population, crossover and mutation. Differently from the genetic operators presented in Chapter 3 for the multi-objective RAP, recursive algo\nrithms are developed for the elaboration of inspection plans, since feasibility evaluation of\n+i is required whenever an inspection takes place (i.e. Xji = 1) and\nxj,l+1 , . . . , xjil+tj,max\nfor every technique j = 1,... ,n. The selection and update of the auxiliary population\nPaux is the same as presented in Section 3.2.3.\n4.2.1\tIndividual Representation\nAn individual is represented by the vector in Equation (4.1), whose entries are either 1 or 0 if either an inspection using the related technique is or not performed in the associated period. As in the case of MOGA for RAPs, an integer representation of individuals are adopted. For the sake of illustration, suppose n = 3, m = 6, t1>max = 2, t2,max = 3, t3,max = 4 and the inspection plan in Table 4.1. It indicates that inspections using: (i) the first technique should be performed at periods 2 and 5; (ii) the second technique should be performed at periods 1, 3 and 4; (iii) the third technique should be performed at periods 1 and 6.\nTable 4.1: Individual representation for MOGA (inspection plan)\n\tTechnique 1\tTechnique 2\tTechnique 3\nPeriod\t1\t2\t3\t4\t5\t6\t123456\t123456\nInspection plan\t0 10 0 10\t101100\t100001\n4.2.2\tGeneration of Initial Population\nEach of the N individuals of the initial population are randomly generated according to a discrete uniform distribution with additional features to handle unfeasibility, which are presented in Algorithm 6. The underlying idea of the algorithm is that, for a given technique j, every time an inspection is established (xj,? = 1), a new group of values for\nthe future periods xj,l+1,... ,xj,l+tjMAx+1 is generated for feasibility investigation. In this way, an inspection to be carried out at period I requires the restart of the algorithm from I +1.\nIndeed, groups of tj,max+1 periods, starting from I +1, are considered one at a time. The values xjr, r = I +1,... ,1 + tj,max+1, are randomly set either to 0 or 1 and, whenever xjr = 1, a recursion of the algorithm starting from r+1 is required. If all xjr are equal to 0, then no inspection is performed within the considered period (S = 0) and the constraint in Equation (4.4) is violated. In order to tackle the unfeasibility, a position p among the ones taken into account is selected and the corresponding value is set to 1. Then, a recursion with p as argument is called. These steps are repeated until the final period m is reached. Once this reasoning is applied for each of the n considered techniques, a feasible individual is generated.\nGenerateFeasiblePlan(j, tj,max, I, m, x)\n1.\tIf I &lt;m \u2014 1:\na.\tS = 0.\nb.\tFor r = I + 1,... ,1 + tj,max + 1 and r &lt;m:\nI.\tGenerate xjr from U({0,1}).\nII.\tIf Xjr = 1:\ni.\tS \u2014\u2014 S + Xjr.\nii.\tGENERATEFEASIBLEPLAN(j, tj max, r, m, x).\niii.\tr = m + 1.\nc.\tIf S = 0 and I &lt;m \u2014 tj,max:\nL p \u2014 U ({\u00a3 +1,...,\u00a3 + tj, max + 1}).\nII. xip \u2014 1\nIII.\tS \u2014 S + xip.\nIV.\tGenerateFeasiblePlan(j, tj,max, p, m, x).\n2.\tReturn x.\nAlgorithm 6: Generation of a feasible plan for a given technique\n4.2.3\tCrossover and Replacement\nSince the values of xjk are either 0 or 1, the usual binary crossover (MICHALEWICZ, 1996) is performed between two individuals (parents, e.g. x1 and x2). The parents\u2019 positions are interchanged at randomly chosen cut points (c) so as to generate two new individuals: child 1 and child 2 that are respectively the modified x1 and x2, since the replacement is automatically performed as in Section 3.2.4. Figure 4.1-a depicts the crossover between two individuals when n =3, m = 6, c =4, t1>max = 2, t2,max = 3 and t3,max = 4. Then the investigation and handling of an eventual unfeasibility for each new individual, per technique, takes place. The algorithm used to perform these tasks is essentially the same as Algorithm 6; the only exception is step 1(b)I, given that in the crossover the values xjr are not created. Notice that for the illustrated example, the crossover procedure generated an unfeasible offspring (child 2) that violated the maxi-\nmum number of periods without an inspection using technique 1. In Figure 4.1-b, the unfeasibility is identified and a possible solution is given. As an outcome, child 2 becomes feasible.\n(a) Parent 1 (x1)\nParent 2 (x2)\nTechnique 1\n0\t1\t0\t0\t1\t0\n1\t0\t0\t1\t1\t1\nTechnique 2\n1\t0\t1\t1\t0\t0\n0\t0\t0\t1\t1\t0\nTechnique 3\n1\t0\t0\t0\t0\t1\n0\t0\t1\t0\t1\t0\n\nChild 1\nChild 2\nCrossover at cut points \u201c|\u2019\n0\t1\t0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t1\n1\t0\t0\t0\t1\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t0\t1\t0\n(b) Child 2\nChild 2\n1\n1\n00110\n00110\n100010\n100010\nFigure 4.1: Example of binary crossover procedure (a); solving unfeasibility of child 2 (b)\n4.2.4\tMutation\nAs in the case of the crossover, given that either Xjk = 0 or Xjk = 1 for all j and k, the traditional binary mutation (MICHALEWICZ, 1996) is applied. For every position a uniform random number u G [0,1] is generated; given a position jk , if u is less or equal than the mutation probability (pm), the value of Xjk is changed either (i) from 0 to 1 or (ii) from 1 to 0; otherwise, if u> pm, Xjk remains the same.\nMutations that can render an individual unfeasible are only of type (ii), since additional inspections due to type (i) mutations by no means harm the individual\u2019s feasibility. In this way, whenever a mutation of type (ii) occurs, the related technique and period are respectively stored in vectors it and ip. Once all positions of an individual have been submitted to the binary mutation procedure, it and ip are of the same length (|it| = |ip|). If it and ip are both empty, which means no mutations of type (ii) have happened, the related individual is still feasible.\nOn the other hand, if | it| = | ip| > 0, it is necessary to investigate eventual unfeasibility arisen due to type (ii) mutations. If technique j and period k are, respectively, at the same positions of it and ip and the corresponding mutation resulted in an unfeasibility (a greater number of periods without inspections using technique j than permitted), the\nxjk value is restored to 1 as if no mutation had taken place at position jk. Otherwise, if a mutation of type (ii) has occurred but has not generated an unfeasibility, the product of the mutation remains unchanged, i.e. xjk = 0. The idea is to modify individuals as least as possible after mutation operator has been applied in order to preserve the MOGA evolution trend.\nAlgorithm 7 summarizes the investigation procedure for unfeasibility over an individual for a given technique as well as their associated treatment in order to render the inspection plan feasible. Notice that steps until (iii) are basically the same as in Algorithm\n6.\tThese steps are necessary because of alterations in feasibility analysis due to eventual 1\u2019s provided by mutations of type (i) that also demand the recursion of the algorithm starting from the immediate subsequent position. The elements of it and ip that are eliminated in steps D and E are those involving already solved unfeasibility. Thus, at the end, if |it| = |ip| > 0, the remaining elements refer to positions that have been submitted to type (ii) mutations but have not generated unfeasibility.\nRenderPlanFeasibleQ', tj,max, I, m, x, it, ip)\n1.\tIf I &lt;m \u2014 1:\na.\tS = 0.\nb.\tFor r = I + 1,... ,1 + tjmax + 1 and r &lt;m:\nI. If Xjr = 1:\ni.\tS \u2014\u2014 S + Xjr.\nii.\tRenderPlanFeasibleQ', tj,max, r, m, x, it, ip).\niii.\tr = m + 1.\nc.\tIf S = 0 and I &lt;m \u2014 tj max:\nF flag = 1.\nII. For q = 1,..., |ip| and flag:\ni. If I &lt;ip[q]:\nA.\tp \u2014 ip[q].\nB.\txj,ip[q]\t1.\nC.\tS \u2014 S + xj,ip[q] .\nD.\tit \u2014 it[\u2014q].\nE.\tip \u2014 ip[\u2014q].\nF.\tIf \\ip\\ > 0, RenderPlanFeasibleQ', tj,max,p, m, x, it, ip).\nG.\tflag \u2014 0.\n2.\tReturn x.\nAlgorithm 7: Evaluation and solution of eventual unfeasibility after mutation\n4.3\tEvaluation of Risk via Risk Based Inspection\nThe calculation of the cost objective is straighforward as Equation (4.2) is directly applied to the considered inspection plan. The risk objective, in turn, is computed by an RBI methodology using API (2008) recomendations. The financial consequences FCs via consequence analysis along with the gff and the FMS are obtained only once at the beginning of optimization procedure. The damage factor Df (k), in turn, is updated according to the specific inspection plan provided by MOGA for every technique and\nperiod. In this way, the overall risk R(x) is obtained. For further details in the risk evaluation by RBI, the interested reader is referenced to API (2008).\n4.4\tOverview\nThe proposed methodology couples the RBI methodology to an optimization procedure - MOGA - that entails constraints to comply with regulations. An overview of the MOGA + RBI is provided in Algorithm 8. Notice that most of the steps in Algorithm 8 are the same as in the Algorithm 5, however the used initial sampling, crossover and mutation are the ones detailed in Sections 4.2.2, 4.2.3 and 4.2.4 respectively.\n1.\tObtain FC, gff and FMS via RBI.\n2.\tSet g = 0 and generate initial population.\n3.\tFor i = 1,... ,N:\na.\tCompute R(xl) via RBI.\nb.\tCalculate C(x1).\n4.\tPerform selection and update of Paux.\n5.\tIf g = Ngen, go to step 8; else go to step 6.\n6.\tPerform crossover and replacement.\n7.\tPerform mutation, g \u2014 g + 1, go to step 3.\n8.\tReturn Paux.\nAlgorithm 8: Proposed MOGA + RBI\n4.5\tApplication Example\nIn this section, the proposed MOGA + RBI is applied to obtain non-dominated inspection plans for a separator vessel of oil and gas by considering 3 inspection techniques and a horizon of 20 years. Internal and external corrosion are damage mechanisms to which the vessel is exposed. For such an equipment, the recommended times between inspections, according to the Brazilian Regulation Standard (NR-13), are 3, 7 and 15 years, respectively (MARANGONE &amp; FREIRE, 2005). The fincancial consequences were obtained from the consequence analysis performed by Furtado et al. (2012). The RBI features are summarized in Table 4.2 and MOGA parameters are presented in Table 4.3. For this problem, the number of feasible inspection plans is in the order of 1017 and represents 50.77% of the entire search space.\nThe MOGA + RBI provided 15 non-dominated inspection plans comprising the Pareto set, which enabled the construction of the Pareto front of Figure 4.2. In this graph, three points A, B and C are identified, whose cost and risk values along with the associated schedule of inspections are shown in Figure 4.3. The gray cells represent that an inspection is to be performed in the associated period (column) using the related technique (row). On the other hand, the white cells indicate no inspection is required.\nTable 4.2: RBI parameters\nTechnique\t(years)\tci\n1 2 3\t3\t1,000.00 7\t5,000.00 15\t10,000.00\ncp Cd FC\t300.00 1, 000.00 6, 743, 238.37\nTable 4.3: MOGA parameters\nParameter\tValue\nPopulation size (N)\t50\nNumber of generations (Ngen)\t100\nProbability of crossover (pc)\t0.95\nNumber of cut points\t9\nProbability of mutation (pm)\t0.01\nID\nID\nCO\no co\n(D or\nID oi\no _ oi\nID\nO _\n\u2022 A\nO\n\u2022\t........................ C.\n\u201cI------1-------1------1-------1--------1------1-\n7.0\t7.5\t8.0\t8.5\t9.0\t9.5\t10.0\nCost (x104)\nFigure 4.2: Application example MOGA + RBI - Obtained Pareto front\n4.5.1\tReturn on Investment Analysis\nThe obtained non-dominated solutions can be submitted to ROI analysis so as to support inspection-related decision making. The ROI is given by:\nFigure 4.3: Application example MOGA + RBI - Selected inspection plans\nROI=\t, i = j,\t(4.7)\ni\tC j\nwhere Ri and Rj are the risks related to inspection plans i and j and Ci and Cj are their respective costs. The ROI of solution A to B is about -2.75 monetary units, that is, the risk is reduced by 2.75 monetary units for every unit invested in inspection. On the other hand, the ROI is about -0.04 from solution B to C, which means that the reduction in risk for each monetary unit invested in inspection is only about 0.04. Thus, high investments in inspection are not necessarily translated into significant reduction in risk.\n4.6\tSummary and Discussion\nIn this chapter, a combination of MOGA and RBI was developed to provide efficient inspection plans in terms of both cost and risk. The genetic operators of MOGA were adapted for the creation of only feasible inspection plans in compliance with the maximum allowed time between inspections. In this way, there is a significant reduction in the space to be explored by MOGA and there is no need to handle unfeasible inspection plans during MOGA steps.\nThe RBI was used to assess the risk related to the inspection plans provided by MOGA. As the risk was considered as an objective to be optimized, the user defined risk target was not required. Additionally, each inspection plan was submitted to a cost evaluation, with which expenditures related to the inspection activity become known. This is not possible if only RBI methodology is adopted.\nThe proposed MOGA + RBI was applied to an example involving an oil and gas separator vessel. A ROI analysis was illustrated on the obtained nondominated inspection plans. It could be inferred that high investments on inspection do not necessarily yield a great reduction in risk. The results suggest that the MOGA + RBI with the postoptimization ROI analysis is an effective tool to support decisions related to equipment integrity.\n5\tTHEORETICAL BACKGROUND - PART 2\nThis chapter provides the underlying theory related to the coupling of SVR with PSO and bootstrap methods presented in Chapters 6 and 7, respectively. An overview of SVM and a detailed description of SVR are given. A formulation of the SVR training problem to be solved by IP methods is also provided. Additionally, this chapter includes an introduction to the variable and SVR model selection problems that are tackled via PSO. Finally, the general ideas of the bootstrap techniques are presented.\n5.1\tSupport Vector Machines\nSVMs are well suited when the underlying process that maps a set of influential variables, represented by an input vector x G Rm, into the response variable of interest, denoted by the scalar output y, is not known or when analytical formulations relating them are difficult to be established. This feature renders SVM as a model-free or nonparametric approach.\nIn its classical formulation, SVM is a supervised learning method, given that the available set of observations D comprises not only the multidimensional inputs x, but also the associated outputs y, i.e. D = {(x1,y1),..., (xl,yl)} (VAPNIK, 2000; SCHOLKOPF &amp; SMOLA, 2002). The set D is the so-called training set as it is used in the SVM learning (or training) phase.\nIndeed, the SVM learning step concerns the resolution of a mathematical programming problem, whose objective function embodies the SRM principle that aims at balancing model\u2019s complexity and model\u2019s training accuracy in order to avoid underfitting and overfitting situations. The former case is characterized by models with low complexity that have both inaccurate training and generalization performances. The latter involves very complex models so specialized in the training examples that poorly generalize to unseen data (KECMAN, 2005). The behavior of training and generalization errors with respect to model complexity is illustrated in Figure 5.1.\nBesides, the SRM principle was proved to be useful when dealing with small data sets. This is an advantage of SVM over other learning techniques such as ANNs that are suited for large training sets as they only involve the minimization of the training error by means of the ERM principle (VAPNIK, 1999).\nThe SVM training problem is a quadratic, thus convex, mathematical program, for which the Karush-Kuhn-Tucker (KKT) first order optimality conditions are not only necessary but also sufficient for a unique global optimum. In this way, SVM is not trapped in local optima as ANNs are (BOYD &amp; VANDENBERGHE, 2004; SCHOLKOPF &amp; SMOLA, 2002).\n---Generalization error\n---Training error\nFigure 5.1: Relation between model complexity and errors\nThe nature of the scalar output defines the type of problem to be handled. If y assumes discrete values representing categories, then the problem is of classification. On the other hand, for real-valued y, one has a regression problem. In this work, only SVM for regression is taken into account. For an introduction to SVM classification, the interested reader is referred to Burges (1998), Scholkopf &amp; Smola (2002) and Kecman (2005).\n5.1.1\tRegression via Support Vector Machines\nNon-parametric regression can be formalized considering the response as a random variable Y generated by the model:\nY = yy (x) + e(x),\t(5.1)\nwhere yy (x) is the deterministic but unknown expected value of Y and e(x) is a random error term with zero mean and variance af(x) > 0. SVR aims at estimating yy (x) using the training set D for the adjustment of a regression expression of the form:\nf (x) = wT 0(x) + b,\t(5.2)\nin which both w, the \u00bf-dimensional weight vector, and b, the linear coefficient, are unknown. Also, in Equation (5.2), the operator maps x into a higher dimensional space F in order to account for possible nonlinearities between the input vector and the response variable. The underlying idea of the mapping is to translate a non-linear relationship between x and y in the input space into a linear association between&lt;p(x) and y in F, as illustrated in Figure 5.2 for the case of unidimensional x.\nThe SVR learning problem is as follows:\nmin\t1l - wTw + c ^2 &amp; + &amp; 2\ti=1\t\t(5.3)\ns.t.\tyi - wT$>(xi) - b &lt;e + &amp;,\tV l,\t(5.4)\n\twT0X) + b - yi &lt;e +\t,\tV l,\t(5.5)\n\t&amp; > 0,\tV l,\t\t(5.6)\n\t$ > 0, V l,\t\t(5.7)\nwhere the first part of the objective function (5.3) relates to the SVR model complexity and its ability in predicting data not in D and the second part is associated with training errors. The parameter c controls the compromise between these two parts. The slack variables &amp; and originate from Vapnik\u2019s e-insensitivity loss function which forms a \u201ctube\u201d around the y values, such that errors are computed only for points lying outside it (VAPNIK, 2000; KECMAN, 2005):\n\\yi \u2014 f X )| \u2014 e =\t,\tif\tobservation l is \u201cabove\u201d\tthe e-\u201ctube\u201d,\t(5.8)\n\\yi \u2014 f X)| \u2014 e =\t,\tif\tobservation l is \u201cbelow\u201d\tthe e-\u201ctube\u201d,\t(5.9)\nfor all\tl.\tThe parameter e represents\tthe \u201ctube\u201d width, i.e.\tthe accepted deviation of\nf X )\tfrom\tyl. Figure 5.2 contains a\tgraphical visualization of the e-\u201ctube\u201d formed\tby H_\nand H+ in F and of the e-insensitivity loss function. Also, in the same Figure, pairs (x, y) associated to the circled dots lying on or beyond H_ and H+ are named support vectors.\nFigure 5.2: The role of mapping $ and Vapnik\u2019s e-insensitivity loss function\nOnce the Lagrangian function related to the primal problem (5.3)-(5.7) is obtained, one may apply the KKT first order optimality conditions for a stationary point, so as to construct the dual form of the SVR training problem (see Appendix 1):\n\u25a0\u00a3 l l\tl\nmax -.)^zLini - af)(\u00ab<> - af MX)T^(xo) - $2[\u00a3(\u00aei + ai) + Vi(ai - ai)] (5.10) a>a 2 tri\trT\nI2(al\u2014 a*) l=1 0 &lt;al &lt;c,\t= 0,\t(5.11)\n\tV l,\t(5.12)\n0 &lt;a* &lt;c,\tV l,\t(5.13)\nwhere a and a* are l-dimensional vectors comprising the Lagrange multipliers related to primal constraints (5.4) and (5.5), respectively. The resolution of the dual training problem provides the adjusted regression function:\ni\nfo(x) = wTf(x) + bo ^^2(aio \u2014 a*l0)^(xi)Tf(x) + bo,\t(5.14)\ni=1\nin which the index 0 represents optimality. In fact, the optimal values assumed by al and a* provide the classification of the training example (xl,yl) as: (i) free support vector, when 0 &lt;al0 &lt;c or 0 &lt;a*0 &lt;c; (ii) bounded support vector, if al0 = c or a*0 = c; (iii) non-support vector, if both al0 and a*0 are equal to zero. Note that non-support vectors have no contribution in the regression function (5.14) as the support vectors are supposed to summarize all relevant information of the training set.\nIn practice, an appropriate mapping f is often difficult to be determined and the calculation of the dot products that appear in Equations (5.10) and (5.14) may be computationally expensive. Fortunately SVR allows the use of kernel functions K(xl,xo) = f(xl)Tf(xo), which are defined in the original space and can be used to implicitly compute the dot products (SCHOLKOPF &amp; SMOLA, 2002).\nThe Gaussian radial basis function (RBF)\nKlo = K(xi,xo) = exp (\u20147 ||x \u2014 xo||2),\t(5.15)\nis the most widely used kernel function. One of the advantages of Gaussian RBFs is the fact that they are dependent only on the parameter 7 to be tuned based on the available data. This is in line with the principle of parsimony according to which, provided that the achieved predictive ability is satisfactory, simpler models with less parameters are preferred. Also, Gaussian RBFs present relatively few numerical difficulties as compared, for example, with polynomial kernels whose values may go to infinity or zero as the degree of the polynomial increases or with the sigmoidal kernel, which may lose mathematical validity for certain values of the related parameters (HSU et al., 2003; LIN &amp; LIN, 2003). For further details on kernel functions and on the conditions they must satisfy, see Burges (1998), Cristiniani &amp; Shawe-Taylor (2000), Vapnik (2000), Kecman (2001) and Scholkopf &amp; Smola (2002).\nThe substitution of the dot products in Equation (5.10) by K(xl,xo) does not affect the general outline for solving the dual problem. Therefore, the dot products in the estimated regression expression may be replaced by the kernel function as well. Given a, a* and the kernel, there is no need to explicitly determine w0, remaining only b0 to be defined.\nActive-set methods have been extensively applied to SVM training problems. The main idea of these methods is to solve equality-constrained problems, which are in general simpler than nonlinear programs with inequality constraints. The procedure involves guesses of the set of active constraints at the solution. Then, the supposed active constraints are imposed as equalities and the innactive ones are simply ignored. If the guess is incorrect, the methods use Lagrange multiplier information to drop one index from the current working set and add a new one until optimality conditions are satisfied (NOCEDAL &amp; WRIGHT, 2006). The decomposition strategy used by Joachims (1999) in the open source SVM library SVMll'ght and the Sequential Minimal Optimization (SMO) implemented in LIBSVM (CHANG &amp; LIN, 2001) are examples of active-set methods for solving SVM training problems. With these methods, the estimation of bo can be performed by using the free support vectors (KECMAN, 2005).\nIP methods can also be used to solve SVM training problems. Contrary to the activeset methods, IP algorithms are not based on estimates concerning the true active sets. Also, the adoption of IP methods to solve the SVR dual problem provides bo as a byproduct, with no requirements of further computations. However, IP methods have been indicated for small to moderately sized SVM (PLATT, 1998; SCHOLKOPF &amp; SMOLA, 2002) due to the storage and handling of large matrices during the optimization process. In spite of that, Woodsend (2009) and Woodsend &amp; Gondzio (2011) explored the particularities of SVM training problems so as to efficiently apply IP algorithms in large-scale situations. The authors mainly tackle linear G-norm binary classification, even though general outlines for non-linear SVM classification and regression are also given. A brief description of SVR training using IP methods is presented in next section.\n5.1.2\tSupport Vector Regression via Interior Point Methods\nIn order to describe the IP method to solve the SVR dual training problem (5.10)(5.13), consider its matrix formulation as a classical quadratic programming problem:\nmin z\t1\tzT Qz \u2014 dT z 2\t(5.16)\ns.t.\taz = 0,\t(5.17)\n\t0 &lt;z &lt;ce,\t(5.18)\nin which\n\u2022\tz = [ a a* ]T, d = ITy \u2014 ITee and a = ITe are 2l x 1 vectors;\n\u2022\ty = [y1... yl]T is the vector of observed values of the response variable;\n\u2022\te is a column vector of ones with the appropriate dimension;\n\u2022\tQ = ITKI is a positive semidefinite 2l x 2l matrix;\n\u2022\tK is the l x l matrix with the kernel values Klo as elements, for l,o = 1... ,l;\n\u2022\tI and I are l x 2l matrices defined as [ I \u2014I ] and [ I I ], respectively, i.e. concatenations by columns of identity matrices Iixi.\nEquations (5.16)-(5.18) form the dual of the SVR training problem, which is the primal problem to be solved by the IP methods briefly described in next sections. Also, as Q is positive semidefinite, the KKT first order conditions are not only necessary but also sufficient for global optimality. Thus, the resolution of the optimization problem essentially consists in solving the system of equations resulted from the KKT first order conditions (NOCEDAL &amp; WRIGHT, 2006).\n5.1.2.1\tPrimal-Dual Interior Point Method\nThe primal-dual IP method solves a modified version of problem (5.16)-(5.18), in which the inequality constraints are transformed into equality constraints via the introduction of 2l-dimensional non-negative slack vectors s and t. Also, in order to handle the non-negativity of s and t, a log-barrier function is incorporated to the objective function (5.16):\nmin z,s,t\t1\t21 - zT Qz \u2014 dT z \u2014\t[ln(si) + ln(ti)] 2\ti=1\t(5.19)\ns.t.\taT z = 0,\t(5.20)\n\tz \u2014 ce + s = 0, s > 0,\t(5.21)\n\t\u2014z + t = 0, t > 0,\t(5.22)\nwhere /dk is a positive barrier parameter that is forced to decrease to zero as k x. The conditions of strict positiveness on the slack variables (s > 0 and t > 0) are implicitly considered during the step update (NOCEDAL &amp; WRIGHT, 2006). The Lagrangian function associated with problem (5.19)-(5.22) is defined as\n1\t21\nL(p,yk) = 2zTQz \u2014 dTz \u2014\t[ln(sj)+ln(tj)] + XaTz+nT(z \u2014 ce+s)+ uT(\u2014z+1), (5.23)\n2\ti=1\nwhere p = (s,t,n,v, X,z); X G R, n G R2\u00a3 and u G R2\u00a3 are the Lagrange multipliers, also known as dual variables. An optimal point p0 for (5.19)-(5.22) must be a stationary point of L(p; pk) and satisfy the KKT first order conditions, which form the following system of non-linear equations:\nSn\t= Pk e,\t(5.24)\nTv\t= Pk e,\t(5.25)\nz \u2014 ce + s\t= 0,\t(5.26)\n\u2014z +t\t= 0,\t(5.27)\nT a z\t= 0,\t(5.28)\nQz \u2014 d + Xa + n \u2014 u\t= 0,\t(5.29)\nwhere S and T are diagonal matrices whose diagonals are given by the vectors s and t, respectively. Equations (5.24) and (5.25) are the ^-complementarity conditions. Note that, from Equation (5.27), t = z; thus Equation (5.25) becomes Zu = pke, with Z as a diagonal matrix obtained from z. In this way, Equation (5.27) can be eliminated, i.e. t needs no longer to be explicitly considered, p = (s, n, u, X, z) and the system (5.19)-(5.22) can be rewritten as:\nSn\t= Pk e,\t(5.30)\nZu\t= Pk e,\t(5.31)\nz \u2014 ce + s\t= 0,\t(5.32)\nT az\t= 0,\t(5.33)\nQz \u2014 d + Xa + n \u2014 u\t= 0.\t(5.34)\nAn iteration of the primal-dual IP method involves the following steps:\n1. Apply one step of the Newton\u2019s method to find the roots of (5.30)-(5.34), which involves the resolution of the following sparse linear system:\n\u2019 n o s o o\t\tAs\t\tSn \u2014 pke\n0 Z 0\t0 Y\t\tAv\t\tZu \u2014 pk e\nI 0\t0\t0 I\t\tAn\t= \u2014\tz \u2014 ce + s\n0\t0\t0\t0\taT\t\tAX\t\taT z\n0\t\u2014I I a Q\t\t. Az _\t\tQz \u2014 d + Xa + n \u2014 u\n2.\tCalculate the step length ak in Newton\u2019s direction;\n3.\tUpdate variables: pk+1 = pk + akAp;\n4.\tReduce the barrier parameter pk.\nThese steps are repeated until the optimality conditions are reached. For a complete description of the primal-dual IP method and their most successful variants the reader is referred to Wright (1997), Nocedal &amp; Wright (2006).\n5.2\tVariable and Model Selection Problems\nVariable selection procedures can be generally divided into wrappers and filters (KO-HAVI &amp; JOHN, 1997). Wrappers consider the learning machine to score subsets of variables in accordance with their predictive power. Filters select a subset of variables in a preprocessing step and are independent from the learning machine used. For example, the work of Yang &amp; Ong (2010) presents a wrapper method, whereas Wu &amp; Wang (2009) use a filter for feature selection.\nA common filter method is to rank variables according to their coefficient of determination between each one of them and the response variable (Y) (GUYON &amp; ELISSEEFF, 2003). Indeed, such a statistic indicates the percentage of the total variability around the mean of Y explained by a linear fit between each regressor and Y. In this way, the use of the coefficient of determination as a ranking criterion enforces a variable ordering according to goodness of linear fit of individual variables.\nWrappers often give superior results, since information provided by the considered learning machine governs the search for the most relevant set of variables. These superior results are obtained at the expense of increased computational effort, even though it is not always the case (GUYON &amp; ELISSEEFF, 2003). Indeed, backward elimination and forward selection are common wrappers usually associated with computational advantages. These procedures are incremental and at each step the subset of considered variables is modified.\nThe predictive ability of SVR greatly depends on the values of its hyperparameters c, e and 7. Since the quest of the most suitable values for the SVR hyperparameters, known as model selection problem, is usually based on the specific data set under analysis, it may be performed whenever variable elimination or incorporation takes place. This renders the usual incremental wrappers computationally prohibitive, in spite of their original efficiency. For instance, Yang &amp; Ong (2010) asserts that, like other wrappers methods, SVR hyperparameters are not re-tuned before performing each SVR training required by their variable selection procedure due to the increased computational effort required.\nThus, in the present work, variable selection along with SVR hyperparameters\u2019 tuning are simultaneously performed by PSO. Variable selection procedures in learning machines are comprehensively discussed by Kohavi &amp; John (1997) and Guyon &amp; Elisseeff (2003). For more on SVR model selection problems, the reader is referred to Momma &amp; Bennett (2002), Ito &amp; Nakano (2003), Yan et al. (2004), Pai (2006), Fei et al. (2009), Lins et al. (2010a) and Lins et al. (2012a).\n5.3\tParticle Swarm Optimization\nFor PSO, the basic element is a particle i, i = 1,... , npart, which is characterized by its current position in the search space (s\u00bb), the best position it has visited (p\u00bb) and its velocity (v\u00bb). Also, a fitness function, i.e. the objective function to be optimized, is used to evaluate the particle performance. The npart particles comprising the swarm fly throughout the search space towards an optimum by using both individual and collective information. This process is governed by the following update equations:\nVij(t + 1)\t=\tX-{vij(t) + c1 \u2022\tU1 \u2022\t[p\u00bbj(t)\t\u2014 s\u00bbj(t)] +\t+c2 \u2022 U2 \u2022 [pgj(t) \u2014 s\u00bbj(t)]}, (5.36)\nsij(t + 1)\t=\tsij(t) + vij(t +\t1);\t(5.37)\nin which j regards the jth dimension of the d-dimensional search space, t indicates the time step (i.e. PSO iteration), x is the constriction factor used to avoid huge velocity values, c1 and c2 are constants, U1 and U2 are uniform random numbers in [0, 1] generated whenever the update takes place and for each j, pg = (pg1,... pgd) is the position associated with the best neighbor of particle i. Indeed, the second part of Equation (5.36) concerns the particle\u2019s cognition ability, whereas the third part is related to its social capacity of learning from its neighbors.\nThe number of neighbors each particle has (nnejgh) characterizes the swarm communication network. In a global topology (gbest), a particle is able to communicate with all others (i.e. the entire swarm, nneigh = npart), whereas in a local topology (lbest), a particle exchanges information with some of the others (nneigh &lt;npart). As the lbest approach is part of the standard PSO suggested by Bratton &amp; Kennedy (2007), this is the neighborhood structure adopted in this work. Also, some experiments performed by Lins et al. (2010a) contend that the lbest method is prone to require less computational effort (time and number of fitness evaluations).\nEquation (5.37) may render particles unfeasible, since it can yield positions outside the search space, which is bounded by the predefined variables\u2019 intervals. In these situations, Bratton &amp; Kennedy (2007) recommend the \u201clet particles fly\" strategy, which consists of skipping the fitness evaluation phase so as to avoid unfeasible positions from becoming the best. Besides the constriction factor x, particles\u2019 velocities may be in [\u2014vmax, vmax], where vj\u201cax is the maximum velocity allowed for the jth dimension. This procedure has been suggested to avoid particles from going too far beyond the feasible space.\nThe update of velocities and positions, and fitness evaluation phases are repeated until a stop criterion is met. For further details in PSO, the interested reader can consult Kennedy et al. (2001).\n5.4\tBootstrap\nBootstrap is a computer intensive method, whose main idea is to resample from the original data, either directly or via a fitted model, in order to create replicate data sets. These replicates enable the variability assessment of the quantities of interest (DAVISON &amp; HINKLEY, 1997). Bootstrap is particularly useful when no probabilistic model apply to the data under analysis and / or when the amount of data is not sufficient to use the central limit theorem.\nSuppose a data set x with n elements. The boostrap begins by generating a large number of independent bootstrap samples x1,x2,... ,xB, each of size n. Corresponding to each bootstrap sample is a bootstrap replication of the statistic of interest s(xb), b = 1,...,B (e.g. mean, media, standard error). The set of B replications can be used to construct an empirical probability distribution for s as well as confidence intervals (EFRON &amp; TIBSHIRANI, 1993).\nIn the case of regression models, the bootstrap samples can be obtained based on pairs or on residuals. In the first case, the original data pairs (x, y) are sampled with replacement from the original data set and each of them have the same constant probability 1/1 of being selected. The residuals sampling, in turn, require a fitted regression model over the original data set and the computation of residuals. Thus, the fitted model along with the resampled residuals are used to construct the bootstrap samples. The bootstrap techniques used linear regression is detailed in Efron &amp; Tibshirani (1993) and Davison &amp; Hinkley (1997). The adaptation for SVR is formalized in Chapter 7.\nThis chapter provided the theory underlying the proposed methodologies PSO + SVR and bootstrapped SVR detailed in Chapters 6 and 7, respectively. The first one is used to solve the variable and SVR model selection problems simultaneously and the second gives not only point estimates but also the confidence and prediction intervals related to the response variable of interest.\n6\tPARTICLE SWARM OPTIMIZATION FOR VARIABLE SELECTION AND SUPPORT VECTOR REGRESSION HYPERPARAMETER TUNING\nIn this chapter, a PSO + SVR is developed to simultaneously tackle the variable and SVR model selection problems commented in 5.2. Indeed, the methodology used in this work is an extension of the one presented by Lins et al. (2010a), which only involves SVR hyperparameter tuning. The PSO + SVR is applied to an example in the context of onshore oil wells and the predictive ability of SVR is evaluated on a reduced model -in which only a subset of variables identified as important is taken into account - and on a full model involving all available input factors. This chapter is based on Lins et al. (2011b).\n6.1\tCoupling Particle Swarm Optimization and Support Vector Regression\nFor the quest for SVR hyperparameters and variable selection, the PSO search space is formed by d = 3 + n dimensions, where the first three regard c, e, y, in this order, and the remaining n are the variables rh associated with regressors xh, h = 1,... ,n. The latter variables are defined in the range [0,1] and if rh > 0.5, the hth regressor is included in the model, otherwise it is not considered. The intervals of definition of c and y are arbitrarily chosen, whereas e is in\n1 1\n[0.001, 0.15] \u2022 -^yi.\t(6.1)\n1 i=i\nIn this way, the e range is based on the considered training+validation set.\nThe proposed PSO + SVR is summarized in the flowchart of Figure 6.1. The initial particles velocities and positions are uniformly initialized considering the definition intervals of the PSO decision variables (particles are initially feasible). In this work, vmax = smax \u2014 smin, for all i, and the initial maximum velocity range is set to 0.1 [\u2014vmax,vmax] in order to prevent particles from having great velocities in the early stages of the algorithm. After the initialization phase, particles\u2019 neighborhoods are determined, followed by an initial fitness evaluation of particles, which includes the update of their best positions. Then, particles\u2019 best neighbors, the overall best, velocities and positions are updated and fitness assessment phase is again reached and only performed for feasible particles. This cycle repeats until one of the following stop criteria is met: (i) the maximum number of iterations (njter) is achieved; (ii) consecutive iterations representing 10% of niter have provided the same best fitness value; (iii) the difference between two\nconsecutive best fitness values is less than a tolerance 3.\nvalidation NRMSE\nFigure 6.1: PSO + SVR for variable selection and hyperparameter tuning\nThe adopted fitness function is the mean validation Normalized Root Mean Square Error (NRMSE) (Equation (6.3)), given that the present algorithm includes a crossvalidation technique further detailed in Section 6.1.1. At the fitness evaluation step,\nthe hybridism of the PSO and SVR takes place. For each set of hyperparameters\u2019 values and a subset of variables, i.e. for each particle position si = (ci, Eiy Yi, rii,\u25a0\u25a0 \u25a0, rih,\u25a0\u25a0 \u25a0, rin), the SVR portion performs k trainings and predictions so as to enable the computation of\nthe mean validation NRMSE.\nAs a result, in relation to SVR accuracy in the validation phase, the PSO provides the optimal SVR hyperparameters values combined with the most relevant regressors to explain the variability of the response variable. Notice that the hyperparameters are tuned considering the selected set of variables and the problem of having inappropriate c, e and y for the group of chosen regressors is then avoided.\n6.1.1\tCross-validation\nIn order to evaluate the prediction performance of SVR given the values of c, e and y and a set of variables, one may compute the NRMSE, defined by the following formula:\nT,i(yi - yi)2\nNRMSE =\ni\n(6.2)\nwhere yl is the observed value for the response variable and yl is the related prediction provided by SVR.\nIn practice, the available data set is divided into two parts: (i) training + validation, formed by l observations and (ii) test, with m cases. The k-fold cross-validation is performed over the training + validation part, which is further divided into k subsets. Each one of them, with l/k elements, plays the role of a validation set, one at a time, and the remaining k \u2014 1 form the actual training set, upon which the SVR training is performed. This procedure is repeated k times until all subsets have been used as a validation set. Thus, k validation NRMSE values are obtained, which may be summarized as the mean validation NRMSE\nNRMSE1 + \u2022\u2022\u2022 + NRMSEk\n-----------------------------------------k------------\u25a0\t(6.3) The index l in Equation (6.2) varies from 1 to l/k for each NRMSE in Equation (6.3). The best NRMSE model (i.e. with optimal hyperparameters\u2019 values and most important variables) is related to the smallest value for the mean validation NRMSE.\nAfter finding such a model, a retraining step takes place, in which the SVR optimization problem with all l observations is solved. Finally, the test set is used to estimate the generalization ability of the obtained model by means of the test NRMSE, when l = 1,... ,m. The adoption of a cross-validation technique in the fitness evaluation step is also an extesion of the SVR + PSO presented in Lins et al. (2010a).\n6.2\tApplication Example\nThe resulting PSO + SVR methodology is used for the prediction of TBFs of onshore oil wells located in the Northeast of Brazil. Onshore activities in this area date back to the beginning of the oil exploration in Brazil (ZAMITH &amp; SANTOS, 2007). In spite of being related to mature wells of low productivity, these activities are responsible for a non-negligible part of the overall production. For example, for the period 2000-2010, onshore wells provided about 12% of the national oil production (ANP, 2011). Also, the prediction of TBFs of onshore wells permit the implementation of preventive actions to reduce or avoid failures and production downtime.\nThe data considered in this example originate from a database containing observations from 1983 to 2006 of TBFs and various aspects of different onshore wells located in the Northeast of Brazil. The database was analyzed by Barros Jr. (2006) and the rods were identified as one of the most critical components related to well failures. These equipments are responsible for transmitting the rotational energy of an engine to a pump, which artificially lifts the oil to the surface.\nIn this work, given the importance of the rods to the proper well operation, wells\u2019\nfailures are deemed to occur upon the failures of their installed rods. Hence, the present application aims at predicting wells\u2019 TBFs by means of SVR. The regressors shown in Table 6.1 are believed to influence the rods\u2019 (and wells\u2019) performance. Some of them are related to operational and environmental characteristics (xi - xii) and others are associated with the previous failure and maintenance of the rods (x12 - x18). For example, variables x4 - x7 regard the combination of rods of different sizes installed in the well, whereas x14 is related to the level of rods substitution in the previous maintenance action. The idea is to select a group of the the most relevant variables among x1 - x18 concurrently with the choice of SVR hyperparameters by means of the PSO methodology presented in Section 5.3. The classification of the variables (C - categorical; N - numerical) are also given in Table 6.1. The categorical variables x4 - x18 were handled by means of 0-1 dummy variables (MONTGOMERY et al., 2006).\nTable 6.1: Variables that can influence wells\u2019 TBFs\nVar.\tDescription\tType\tObserved range or categories\t\nX1\tWell depth (m)\tN\t[640, 830]\t\nX2\tWell production (m3)\tN\t[0.4,22.4]\t\nX3\t% of water and solids\tN\t[43, 98.3]\t\nX4\tPresence of 1\u201d rods\tC\t1 - No, 2\t- Yes\nX5\tPresence of 7/8\u201d rods\tC\t1 - No, 2\t- Yes\nX6\tPresence of 3/4\u201d rods\tC\t1 - No, 2\t- Yes\nX7\tPresence of 5/8\u201d rods\tC\t1 - No, 2\t- Yes\nX8\tLevel of H2S\tC\t1 - No, 2\t- Low, 3 - High\nX9\tLevel of paraffin\tC\t1 - No, 2\t- Low, 3 - High\nX10\tType of artificial oil lifting\tC\t1 - Mechanical, 2 - Progressive cavities\t\nX11\tFilter type\tC\t1, 2, 3\t\nX12\tLocation of previous failure (on rods)\tC\t1, 2, 3\t\nX13\tRods' mode of previous failure\tC\t1, 2, 3, 4\t\nX14\tSubstitution of rods in prev. maint.\tC\t1 - None,\t2 - Partial, 3 - All\nX15\tState of 1\u201d rods installed in prev. maint.\tC\t1 - New,\t..., 3 - Old, 4 - Other\nX16\tState of 7/8\u201d rods installed in prev. maint.\tC\t1 - New,\t..., 4 - Old, 5 - Other\nX17\tState of 3/4\u201d rods installed in prev. maint.\tC\t1 - New,\t..., 4 - Old, 5 - Other\nX18\tState of 5/8\u201d rods installed in prev. maint.\tC\t1 - New,\t..., 4 - Old, 5 - Other\nY\tTime between failures (TBF)\tN\t[10, 2039]\t\nAfter preprocessing the original database, a subset of 242 observations related to 26 wells located in the same geographical area was selected. From these, 192 (I) were allocated to the training+validation phase and the remaining 50 (m) formed the test data, which yielded a proportion of approximately 4:1 between training+validation and test data. A 5-fold cross-validation techique is also considered. The PSO + SVR methodology was replicated 100 times to evaluate its stochastic performance and all of them involved the same PSO parameters (npart = 20, nneigh = 8, niter = 5000, x = 0.7298, c1 = c2 = 2.05) and decision variables\u2019 characteristics (Table 6.2). In order to avoid numerical problems because of the different scales of the output and input variables, each of them was scaled\nin [0.1, 0.9]. The scale parameters (minimum and maximum values of each variable) were obtained in the training+validation set and were also used to scale the test set. Thus, the valid range of e is related to the scaled output and the obtained parameter values for c, 7 and e itself are all related to scaled data.\nTable 6.2: Characteristics of PSO decision variables\nDecision variable\tRange\tInitial vMax\nc\t[100,1500]\t140\n\u00a3\t[0.0021, 0.0322]\t0.0301\nY\t[0.1,150]\t14.9900\nrh, h =1,..., 18\t[0,1]\t0.1000\nThe summary of the 100 PSO + SVR replications are shown in Table 6.3, in which the mean validation and test NRMSE values concern the original scale of the output variable. By means of the standard deviation of the mean validation NRMSE in the 100 runs, one can infer that the PSO was able to find essentially the same best value for the fitness function. The variation of the PSO decision variables indicates that the mean validation NRMSE is quite difficult to be tuned, since it may present several local minima with slightly different values. The model associated with the smallest test NRMSE, that is, the one with best generalization ability is described in Table 6.4 and it is referred henceforth as \u201coptimal reduced model\u201d. It is important to emphasize that the test predictions to calculate the test NRMSE were obtained by means of the retrained SVR model considering l =192 observations.\nTable 6.3: Summary of 100 PSO+SVR replications\n\tMinimum\tMedian\tMaximum\tMean\tStd. Dev.\nc\t101.5147\t826.0446\t1498.9536\t826.1644\t359.6575\n\u00a3\t0.0199\t0.0255\t0.0322\t0.0276\t0.0039\nY\t0.1002\t144.8759\t149.9988\t122.3001\t40.6190\nNumber of selected variables\t4\t12\t13\t10.4300\t2.3323\nMean validation NRMSE\t0.7127\t0.7148\t0.7711\t0.7303\t0.0198\nTest NRMSE (after retraining)\t0.7108\t0.9637\t0.9874\t0.9082\t0.0883\nFrom Table 6.4, notice that only 9 of the original 18 regressors were identified as relevant to describe the wells\u2019 TBFs. PSO + SVR returned a regression model in which the TBFs are function of the presence of 1\u201d and 3/4\u201d rods (x4 and x6), levels of H2S and paraffin (x8 and x9), type of filter (x11), mode of previous failure (x13), level of rods\u2019 substitution in previous maintenance (x14) and the state of 1\u201d and 5/6\u201d rods in previous maintenance (x15 and x18). The level H2S, for example, may influence the metal corrosion. Also, given that imperfect maintenance is usually performed, i.e. the system\nTable 6.4: Characterization of the optimal reduced and full SVR models\n\tOptimal reduced model\tOptimal full model\nHyperparameter\tValue\tValue\nc\t240.8256\t178.5738\n\u00a3\t0.0320\t0.0231\nY\t103.6400\t149.9975\nSelected variables\t7 x67 x8 7 x97 x117 x 13, x14? x15 7 x 18\t-\nNRMSE\tValue\tValue\nMean validation\t0.7509\t0.7767\nTraining (after retraining)\t0.6322\t0.1251\nTest (after retraining)\t0.7108\t0.8545\nNumber of support vectors\t141\t177\nis not restored to its new condition, the variables related to the preceding failure and maintenance may indeed affect the time to next failure and are reasonable to be included in the model.\nFor comparison purposes, an SVR regression model considering all 18 variables presented in Table 6.1 was estimated by means of the PSO + SVR methodology. In this case, PSO was used only for tuning the SVR hyperparameters c, e and y- Once more 100 replications were performed and the model with least test NRMSE was chosen as the \u201coptimal full model\u201d. Its characteristics are also shown in Table 6.4, in which the NRMSE values concern the original data scale. In spite of the greater training NRMSE related to the reduced model, it presented a general better generalization, given the evidence of the smaller mean validation and test NRMSE values when compared to the full model counterparts. Also, the reduced model is able to summarize the essential information of the training set with less support vectors (141 vs. 177). This reflects the principle of parsimony, which states that if two techniques adequately model a given data set, the one with less parameters may have superior predictive ability when handling new data (SEASHOLTZ &amp; KOWALSKI, 1993). The prediction results provided by the optimal reduced and full models are presented in Figures 6.2 and 6.3, for the training and test sets, respectively. It can be noticed that the predictions of the full model over the test set assume the same value for a number of different cases. Otherwise, the reduced model has a better performance over the same test set, as its predictions tend to approach the real observations.\nAdditionally, with the plausible consideration of independence between PSO + SVR runs with and without variable seletion, a Wilcoxon-Mann-Whitney statistical test (HIG-GINGS, 2004) was applied to compare the hyperparameters values provided by both approaches. The PSO + SVR with variable selection tends to return higher values for c\nFigure 6.2: SVR training results\nFigure 6.3: SVR test results\nand e and lower values for 7 when compared to PSO + SVR without variable selection. The obtained p-value was 0.0905 for the test concerning c and, for the tests regarding the other two hyperparameters, the p-values were both smaller than 2.2 \u2022 10-16. Such an outcome suggests the importance of SVR hyperparameters\u2019 tuning whenever the set of considered variables changes.\n6.3\tSummary and Discussion\nPrevious works have shown the importance of SVR hyperparameters tuning to improve the SVR predicting performance. In this paper, besides the quest for the most suitable SVR hyperparameters , a variable selection procedure was implemented. The adoption of a PSO algorithm allowed the simultaneous application of both procedures.\nThe proposed PSO + SVR methodology was used on an application example from the Brazilian oil industry. The results show that the variable selection procedure enhanced the predictive ability of SVR. Also, a statistical test confirmed the necessity of the SVR hyperparameters\u2019 adjustment for the specific data set comprising the selected regressors. In general, the outcomes indicate that PSO + SVR is a promising tool for reliability prediction and it could be part of maintenance framework so as to support decisions concerning preventive actions.\n7\tUNCERTAINTY ASSESSMENT BY COUPLING BOOTSTRAP AND SUPPORT VECTOR REGRESSION\nIn order to add uncertainty-related information to the SVR point estimates of the response variable, a methodology involving bootstrap methods and SVR for the construction of confidence and prediction intervals is presented. Indeed, two different approaches based on two bootstrap methods frequently used in regression problems are provided. The first one involves pairs sampling and the second relies on residuals sampling. The boostrapped SVRs are applied to an artificial example and to a case study involving the prediction of scale growth rate on an equipment of the offshore oil industry. The performance of both bootstrapped SVRs are discussed. Some of the findings in this chapter can be found in Lins et al. (2012b).\n7.1\tBootstrapped Support Vector Regression\nThere are two different ways of bootstrap sampling a regression model: one is based\non pairs and the other on residuals (EFRON &amp; TIBSHIRANI, 1993). The pairs scheme\ninvolves bootstrap samples Db = {(x^y), . . . , (x^,yib)}, for b = 1,... ,B. The indices i1,... ,ib are uniformly generated with replacement from the 1,... ,1. Thus, the observed\npairs (x1,y1),. . . , (xl,yl) G D, introduced in Section 5.1, are directly used to form the bootstrap sets. This type of resampling requires no assumptions on the errors e(x) in Equation (5.1) other than independence (DAVISON &amp; HINKLEY, 1997).\nThe second approach relies on the model adjusted over the original D, henceforth called f\u00b0. This model provides the estimates yf, which enables the computation of the residuals el = yl \u2014y\u00b0, V l. Given that the error term is supposed to have zero mean, one may adopt the centralized version of the raw residuals, \u00a1y = el \u2014 221 e/l, Vl. Additionally, in\norder to consider the more general case involving heteroskedastic errors, instead of directly\nsampling the re-centered residuals, these are combined to Rademacher variables - defined\nas ni = 1 with probability 0.5 and yl = \u20141 with probability 0.5, Vl - given their efficient performance in practice (LIU, 1988; DAVIDSON et al., 2007). Therefore, q = elyl are sampled with replacement in order to obtain the sets Db = {(x1, ffiqb),\u2022 \u2022 \u2022, (%\u00a3,$+erib)}, for i1,... ,ibt as previously described. In this setting the input vectors x remain fixed and the predictions y0 have their values perturbed by the residuals e.\nIn both schemes, after the generation of each bootstrap set Db, an SVR training is performed and the corresponding adjusted model f0 is stored. The bootstrapped SVR based on pairs and on residuals are summarized in Algorithms 9 and 10, respectively.\nFor a given observation of the input vector x+, instead of taking y\u00b0+ as estimate for\n1.\tTrain an SVR over D; obtain f\u00b0.\n2.\tFor b =1,...,B:\na.\tGenerate ib, ...,ib from 1,... ,1.\nb.\tSet Db = {(Xib ,yib X ..., (Xib, yy,)}.\nc.\tTrain an SVR over Db; obtain fb.\nAlgorithm 9: Sampling pairs\n1.\tTrain an SVR over D; obtain f\u00b0 .\n2.\tCalculate residuals el = yl \u2014 y\u00b0, il.\n3.\tRe-center residuals, el = el \u2014 ^21 el/l, i l-\n4.\tFor b = 1,..B:\na.\tGenerate Rademacher variables g1,... ,ni.\nb.\tCalculate el = elnl, il.\nc.\tGenerate ib,... ,ib from 1,..., I.\nd.\tSet Db = {(X1,yo1 +\u00a3ib(x^y0 +\t)}.\ne.\tTrain an SVR over Db; obtain f0b.\nAlgorithm 10: Sampling residuals\nBy(x+), the bagging (bootstrap aggregating) predictor is taken (BREIMAN, 1996):\n.bag y+ + 52 B=1 y+ y+\t-----------------\n(7.1)\nB + 1\nwhere yb+ is obtained from the respective adjusted model fb (see Algorithm 11).\nIn this work, percentile intervals are adopted since they present advantages over intervals based on normality assumptions constructed with bootstrapped standard errors: they satisfy the transformation-respecting and range-preserving properties with no requirements of previous knowledge about appropriate transformations or specific ranges that the random variables must respect (EFRON &amp; TIBSHIRANI, 1993).\nThe construction of percentile confidence intervals for by (x+) is straightforward. For a given significance level a, one may take the a/2 and 1 \u2014 a/2 quantiles of yf, \u25a0 \u25a0 \u25a0 ,yB as lower and upper bounds of the interval, in this order. Algorithm 11 summarizes these procedures.\nIn order to determine prediction intervals for Y+ by bootstrapping residuals, an additional sampling is required to simulate the variation of Y+ about its mean by (x+). This is accomplished by estimating the distribution of the prediction error 3. = Y+ \u2014 fo(x+) via S\u2019m = [y\u00b0 + e^] \u2014 yf, where m = 1, \u25a0 \u25a0 \u25a0 , M and M is the number of resamplings. Afterwards, the a/2 and 1 \u2014 a/2 quantiles of jf1, \u25a0 \u25a0 \u25a0,\t, \u25a0 \u25a0 \u25a0,\t, \u25a0 \u25a0 \u25a0, +!M are taken\nto form the (1 \u2014 a)100% prediction interval for Y+, whose limits are set as \u00a1)\"\u2019g + 3' 2 and yb+ag + a/2 (see Algorithm 12). Note that no other SVR trainings are demanded, only the modified version of the residuals e are constructed with no significant increase in computational effort.\nAlso, one might adopt the bootstrap based on pairs and use Algorithm 4 to simulate\n1. For a given x , :\na.\tObtain y.,..., yB from f\u00a7,..., fB.\nb.\tObtain y.19, Equation (7.1).\nc.\tCI[gy(x+); a] = [y./2, y.a/2j .\nAlgorithm 11: Percentile confidence interval for //Y- (x+) by pairs or residuals bootstrapping\n1. For a new x.:\na.\tFor b = 1,... ,B:\nI. For m = 1,... ,M:\ni.\tGenerate Rademacher variable g\u2122 \u2022\nii.\tGenerate i+ from 1,... ,1.\niii.\tCalculate e\u2122 = 7^ n\u2122.\niv.\tCompute \u00bf+\u2122 = (y. + e.) \u2014 y..\nb.\tPI(F+; a) =\t' + ' y+a9 + \u00bf+-a/2].\nAlgorithm 12: Percentile prediction interval for Y+ by residuals bootstrapping\nprediction intervals. But then would be used to obtain the residuals, which is against the essence of the pairs scheme of being totally free of model assumptions. It might be expected that a second level of bootstrap sampling would generate the desired variability of Y+ about its mean. Thus, a double bootstrap (DAVISON &amp; HINKLEY, 1997) was performed over the application examples presented in Section 7.2. However, the obtained prediction intervals were close to the confidence intervals at the expense of a more computer-intensive procedure as additional SVR trainings were required. Alternatively, one could estimate a prediction interval by means of a wider confidence interval, e.g. a 95% prediction interval could be approximated by a 99% confidence interval. However, preliminary experiments involving such an approach provided prediction intervals with low coverages. For further details on bootstrap, the interested reader can consult Efron &amp; Tibshirani (1993) and Davison &amp; Hinkley (1997).\n7.2\tNumerical Experiments\nIn both examples presented in this Section, the output variable as well as each input variable are scaled on [0.1,0.9] at each bootstrap iteration; the test set is scaled on the same interval but considering the lower and upper observed values of y and each component of x on the respective training set. The hyperparameters of SVR are c = 100 and e = 0.0025 for all experiments, whereas 7 =10 for the simulated example and 7 = 150 for the case study. These hyperparameters values have been found by trial. The PSO + SVR presented in Lins et al. (2010a), Lins et al. (2012a) and in Chapter 6 was not adopted as it would introduce an additional source of uncertainty, involving c, e, 7. In this chapter, the goal is to create samples that could have been observed via bootstrap schemes and\nto assess their effects on the variability of the response variable. Additionally, since the bootstrap samples are different from each other, a PSO run would be demanded for each of them, which would require a prohibitive computational effort. Yet, as the bootstrap samples are all of equal sizes and originated from a unique source (i'.e. the available data set), no drastic variations are expected on the values of the SVR hyperparameters. Thus, a single set of values for c, s, 7 may not harm the performance of the SVR models.\n7.2.1\tSimulated case\nIn order to evaluate the ability of the bootstrapped SVR in providing accurate point and interval estimates, the following deterministic process is analyzed (HESKES, 1997; ZIO, 2006):\nUy(x) = sin(%x) cos(1.25%x)\t(7.2)\nwith normally distributed heteroskedastic errors of zero mean and variance given by\naf(x) = 0.0025 + 0.0025 [1 + sin(%x)]2.\t(7.3)\nTraining sets D = {(x1,y1),..., (%\u00a3,ye)} of different sizes (I = 100, 200, 400) were constructed by means of the Latin Hypercube Sampling (LHS - Helton &amp; Davis (2003)) scheme with fX(x) = |x| as probability density function of the inputs x, G (\u20141,1). The main idea of LHS is to divide the range of the input variables into I disjoint intervals of equal probability and then to select one value at random from each interval.\nThe test set comprised n =100 samples, with x values equally spaced over the (\u20141,1). It is important to emphasize that the test points were generated independently from the training points and were, by no means, used to construct the bootstrap samples: they were treated as new observations.\nThe Mean Squared Error (MSE) (Equation (7.4)) and the quadratic bias (Equation (7.5)) were used as performance metrics for the bootstrapped SVR over the test set. The MSE measures the quadratic deviations occurred due to the adoption of ybag as estimate for y and bias2 is estimated by the average of the quadratic difference between ybag and the true hy (x). Note that bias2 can be calculated because the true function of the mean is known, which is not usually possible in practice.\n1 n\nMSE = - \u00a3 (y, \u2014\t)2\t(7.4)\nn i=1\nbias2 = -\t[yba9 \u2014 Uy(x,)]\t(7.5)\nn i=1\nBootstrap simulations with B = 99,499,999 were performed and the number of\niterations to construct the prediction intervals via residuals sampling was defined as M = (B + 1)/2. The MSE and bias2 values for the pairs and residuals approaches are presented, respectively, in Tables 7.1 and 7.2 for each combination of B and l. Regarding the performance of the ybag point estimates, the pairs sampling had overall advantage over the residuals scheme. For l = 200 and l = 400, the MSE values were smaller than for l = 100 and were quite similar within each bootstrap type with slightly better results for l = 200, mainly in the pairs scheme. For both bootstraps, the bias2 values decreased as the training set sizes increased. For a fixed l, the increase of B did not greatly impact the performance of ybag, specially when considering MSE.\nTable 7.1: Simulated case - MSE (left) and bias2 (right) over test set by bootstrapping pairs\nB\tI = 100\tI = 200\tI = 400\n99\t6.583 \u2022 10-3; 6.764 \u2022 10-4\t5.564 \u2022 10-3; 3.113 \u2022 10-4\t5.877 \u2022 10-3; 9.158 \u2022 10-5\n499\t6.603 \u2022 10-3; 6.912 \u2022 10-4\t5.530 \u2022 10-3; 2.844 \u2022 10-4\t5.900 \u2022 10-3; 9.820 \u2022 10-5\n999\t6.522 \u2022 10-3; 6.555 \u2022 10-4\t5.520 \u2022 10-3; 2.693 \u2022 10-4\t5.876 \u2022 10-3; 9.348 \u2022 10-5\nTable 7.2: Simulated case - MSE (left) and bias2 (right) over test set by bootstrapping residuals\nB\tI = 100\tI = 200\tI = 400\n99\t8.320 \u2022 10-3; 2.167 \u2022 10-3\t5.932 \u2022 10-3; 5.975 \u2022 10-4\t6.106 \u2022 10-3; 2.742 \u2022 10-4\n499\t8.278 \u2022 10-3; 2.162 \u2022 10-3\t6.091 \u2022 10-3; 6.868 \u2022 10-4\t6.081 \u2022 10-3; 2.567 \u2022 10-4\n999\t8.295 \u2022 10-3; 2.181 \u2022 10-3\t6.034 \u2022 10-3; 6.451 \u2022 10-4\t6.094 \u2022 10-3; 2.646 \u2022 10-4\nAs an example, Figure 7.1 depicts yY (x) and ybag for the test set considering the pairs and residuals schemes, l = 200 and B = 499. Notice that both approaches convey bagging estimates very near the true mean, which indicates the ability of the bootstrapped SVR in providing accurate point estimates. For the same setting, Figures 7.2 and 7.3 show the simulated confidence intervals (a = 0.05) for bootstrapping pairs and residuals, respectively. The graph related to the residuals scheme (Figure 7.3) also depicts the prediction intervals for the response variable, which may be compared to the true intervals [yY(x\u00bb) \u00b1 1.96 \u2022 a(xi)],i = 1,... ,n. Note that the simulated intervals closely approximate the true ones. Confidence and prediction intervals are represented by lines so as to facilitate visualization.\nIn order to assess the coverage properties of the confidence and prediction intervals provided by the bootstrapped SVR, a Monte Carlo experiment with 5000 replicates was performed for each combination of B and l. The 5000 training sets were generated by the\nFigure 7.1: Simulated case - pairs and residuals bagging estimates vs. true mean\nx\nFigure 7.2: Simulated case - results over test set by bootstrapping pairs\nFigure 7.3: Simulated case - results over test set by bootstrapping residuals\nLHS approach. The test sets were all formed by selected xj+, j = 1,..., 10 (Table 7.3) and the corresponding y3+ were randomly created from Equations (7.2) and (7.3).\nThe coverage results for both bootstrap schemes are reported in Table 7.3; the nominal level of significance was set to a = 0.05, thus the nominal coverage was 95%. By the analysis of Table 7.3, it can be noticed the superior performance of the confidence intervals given by the pairs sampling over the ones provided by the residuals sampling. Indeed, the coverages of the confidence intervals of the latter approach are (much) lower than the nominal value. In this way, the reliance on as the true model that characterizes the residuals scheme negatively impacts the performance of the bootstrapped SVR. Nevertheless, the related prediction intervals presented excellent coverage values. The greater and the lower coverages - when compared to the nominal 95% - for the negative and positive x+, respectively, are justified by the behavior of af(x) over the interval (\u20141,1) (Equation (7.3), Figure 7.4). Note that smaller variances are associated with the negative part of the interval, whereas greater variances are related to positive x. Hence, if a positive x is observed, the prediction of the response variable tends to be more difficult than if a negative x had been observed. Additionally, the increase of I enhanced the performance of the bootstrapped SVR, since more information about the process was given in the SVR training step. On the other hand, the effects of the number of bootstrap iterations were less evident, as B = 499 and B = 999 provided similar results with a slight advantage over B = 99.\nTable 7.3: Simulated case - coverage results (%) of the Monte Carlo simulation with 5000 replicates\n\t\t\tI = 100\t\t\tI = 200\t\t\tI = 400\t\n\txj\tPairs CI\tResiduals CI\tPI\t\tPairs CI\tResiduals CI\tPI\t\tPairs CI\tResiduals CI\tPI\t\n\t\u20140.9\t86.96\t64.96\t98.78\t89.48\t75.48\t98.80\t93.30\t87.34\t98.98\n\t\u20140.7\t94.82\t95.72\t99.68\t95.42\t96.84\t99.62\t95.14\t98.04\t99.86\n\t\u20140.5\t75.16\t55.76\t99.48\t78.40\t62.48\t99.62\t81.58\t73.70\t99.86\nCi\t\u20140.3\t62.74\t35.96\t99.34\t72.26\t49.48\t99.68\t81.34\t68.48\t99.62\nCi\t\u20140.1\t90.50\t72.74\t98.04\t89.92\t77.16\t98.46\t91.16\t84.36\t99.14\ncq\t0.1\t84.42\t55.92\t91.48\t87.40\t61.96\t93.06\t89.72\t72.14\t93.94\n\t0.3\t81.94\t38.48\t85.70\t86.24\t46.98\t87.24\t88.80\t58.50\t87.90\n\t0.5\t85.92\t44.64\t83.70\t87.50\t50.10\t84.42\t89.34\t59.26\t84.32\n\t0.7\t94.62\t79.64\t88.30\t94.22\t80.38\t88.04\t95.16\t83.76\t87.88\n\t0.9\t88.70\t56.58\t93.90\t89.90\t64.64\t94.04\t92.76\t77.80\t94.18\n\t\u20140.9\t88.00\t65.62\t98.68\t90.48\t76.36\t98.82\t93.90\t88.32\t99.02\n\t\u20140.7\t95.76\t96.46\t99.70\t95.88\t97.14\t99.60\t96.58\t98.58\t99.88\n\t\u20140.5\t77.14\t57.28\t99.42\t79.06\t63.84\t99.62\t83.12\t76.06\t99.84\nCi\t\u20140.3\t63.76\t36.92\t99.40\t73.54\t49.00\t99.66\t82.04\t70.04\t99.64\n\t\u20140.1\t91.66\t73.76\t98.02\t90.74\t79.02\t98.56\t92.36\t85.48\t99.06\n11\t0.1\t85.52\t56.84\t91.32\t88.00\t63.18\t93.20\t90.84\t72.98\t94.10\ncq\t0.3\t82.18\t38.58\t85.58\t87.32\t47.32\t87.56\t89.78\t58.86\t88.00\n\t0.5\t87.28\t45.16\t83.86\t88.26\t50.68\t84.44\t90.32\t59.18\t84.64\n\t0.7\t95.44\t80.34\t88.40\t95.14\t80.60\t88.04\t95.72\t84.54\t87.70\n\t0.9\t89.84\t56.56\t93.80\t91.38\t65.48\t94.16\t94.04\t78.66\t94.16\n\t\u20140.9\t88.24\t65.72\t98.70\t90.70\t76.74\t98.80\t94.02\t88.08\t99.00\n\t\u20140.7\t95.78\t96.70\t99.70\t95.88\t97.30\t99.60\t96.44\t98.82\t99.88\n\t\u20140.5\t76.92\t56.90\t99.46\t79.26\t64.20\t99.62\t83.54\t76.08\t99.84\nCi\t\u20140.3\t63.96\t37.18\t99.38\t73.48\t49.06\t99.66\t82.36\t70.24\t99.64\nCi\t\u20140.1\t92.14\t73.70\t97.96\t91.02\t79.02\t98.52\t92.42\t85.46\t99.06\n11\t0.1\t85.62\t56.46\t91.32\t88.28\t63.14\t93.20\t90.88\t73.12\t93.96\ncq\t0.3\t82.84\t38.88\t85.62\t87.56\t47.88\t87.40\t90.08\t59.20\t87.98\n\t0.5\t87.80\t45.30\t83.80\t88.26\t50.56\t84.34\t90.54\t59.44\t84.54\n\t0.7\t95.42\t80.18\t88.42\t95.18\t81.44\t88.06\t95.94\t84.74\t87.64\n\t0.9\t90.14\t56.64\t93.92\t91.64\t65.74\t94.14\t94.12\t78.62\t94.12\nAs a general outcome, the bootstrapped SVR gave accurate interval estimates for the mean response, mainly by the pairs approach, and for the response variable itself via the residuals scheme. Also, great values for B are not necessarily required in order to obtain satisfactory results; this is an interesting feature as far as computational effort is concerned. However, in practical problems, the number of training examples is often a non-adjustable parameter as it is determined by budget constraints, e.g. an additional observation can represent more experiments involving the phenomenon under analysis.\nx\nFigure 7.4: Simulated case - variance behavior over the interval of x\n7.2.2\tCase Study: Prediction of Scale Rate on Metal Surfaces\nScaling build-up in subsea oil well systems is a result of the combination of a set of interacting variables, such as reservoir temperature, pressure and water composition, that characterize the subsea environment. These factors can be tracked to predict the amount of scale that will be deposited in the future and determine the time to next maintenance action for removing the scale layer before it leads to equipment failure (MOURA et al., 2011; AK et al., 2012).\nSome kinetic approaches have been used to predict scale formation, accounting for the effects of environmental variables (ZHANG et al., 2001; DYER &amp; GRAHAM, 2002; CHEN et al., 2005). These prediction models take the form of a deterministic mathematical formula relating the scale output variable to the multi-dimensional input variable. Establishing such formula is often not easy in practice, and uncertainties need also to be taken into account.\nThus, in the present case study, the proposed bootstrapped SVRs are applied so as to give point and interval predictions of the scale growth rate on metal surfaces of an equipment used in offshore oil wells. If the scale layer achieves a predefined width, the equipment fails to properly perform its function. Some experiments were performed as an attempt to reproduce the subsea environment so as to observe the deposited scale layer. The following influential variables were observed: (i) temperature (T) and (ii)\npressure (P) maintained over the tests, (iii) water composition (W), which characterizes the concentration of carbonates that might be expected in the real scenario and (iv) fluid velocity (V) near the metal surfaces. The response variable Y - scale growth rate - can be described by the general formula of Equation 5.1 with x = (t,p,w,v).\nThe experiments performed gave a set of 131 observations. From these, about 90% (l = 118) are allocated for SVR training and the remaining 10% (n = 13) comprise the test set. Confidence and prediction intervals have been obtained for the test samples via the bootstrapped SVR with B = 99. For the construction of prediction intervals by the residuals scheme, M is set equal to 50.\nThe MSE values are 6.1234 \u2022 10-3 and 5.8613 \u2022 10-3 for the pairs and residuals bootstrapping, respectively. Differently from the simulated example, for this case study the residuals approach has a slightly superior performance. Figures 7.5 and 7.6 depict the real observed values y, the estimates ybag and the confidence intervals for the mean response. Figure 7.6 also presents the prediction intervals for the various test points. Once again, line representation is adopted only for visualization purposes.\nFigure 7.5: Study case - results over test set by bootstrapping pairs\nApart from obtaining accurate point estimates for Y, the bootstrapped SVR provides interval estimations related to the output variable Y. As seen in Figures 7.5 and 7.6, the confidence intervals provided by the residuals approach have smaller widths than the ones given by the pairs scheme. This is probably due to the use of in the sampling of the bootstrap based on residuals, as discussed in Section 7.2.1.\nFigure 7.6: Study case - results over test set by bootstrapping residuals\nWith the interval estimates for the scale rate, a threshold for the thickness of the scale layer and the fact that scale rate = thickness/time, point and interval estimates related to the time to attain the threshold can be obtained so as to support maintenance-related decisions. As an illustrative example, consider the first test point x1. Suppose that the scale rate is given in cm/day and that 5cm is the threshold value. Table 7.4 presents the point and interval estimates associated to the scale rate, which are directly provided by the bootstrapped SVR along with the corresponding estimates for the time to reach the threshold. The results indicate that a prudent decision would be to perform a preventive maintenance action for the removal of the scale layer on day 14.\nTable 7.4: Illustrative example - point (PE) and interval (CI, PI) estimates for the scale rate and for the time to attain threshold\n\tScale rate (cm/day)\tTime to attain threshold (days)\nPairs Cl\t0.3059\t16\n\t0.2637 &lt;pY(x1) &lt;0.3512\t14 &lt;^time (x1) &lt;19\nPE\t0.2886\t17\nResiduals Cl\t0.2749 &lt;pY(x1) &lt;0.3051\t16 &lt;^time (x1) &lt;18\nPl\t0.2488 &lt;Y(x1) &lt;0.3447\t14 &lt;time(x1) &lt;20\n7.3\tSummary and Discussion\nIn this chapter, different bootstrap methods (pairs and residuals samplings) were combined with SVR for the evaluation of uncertainty about the response variable when analytical and/or stochastic models describing it are not available. Moreover, instead of using the outcome of a singe model, e.g. the one trained over the original D, as a point estimate, the bootstrapped SVR provided the more accurate bagging estimates (BREIMAN, 1996).\nThe proposed methodology was applied to a simulated example in order to assess its ability in providing accurate point and interval estimates. The small values of MSE and bias2 indicate that the outcomes of the bootstrapped SVR are accurate. For the evaluation of the coverage properties of the obtained intervals, a Monte Carlo experiment was performed and analyses about the effects of the bootstrap schemes, of the number of bootstrap iterations and of the training set size were also carried out. As general results: (i) confidence intervals are better estimated by the pairs scheme and the prediction intervals given by the residuals sampling have good coverages; (ii) increasing the bootstrap replications does not necessarily improve the performance of the bootstrapped SVR; (iii) larger training sets are expected to give better coverages, but for practical applications the data set is often subject to budget constraints.\nA real case study involving the prediction of scale growth rates on metal surfaces as a function of four influential variables representing the environment was successfully handled by the bootstrapped SVR. Also, an example to illustrate how the obtained results can support maintenance-related decisions was provided. In this way, the bootstrapped SVR can be part of a more general framework for the establishment of maintenance policies.\nThe proposed methodology is based on non-parametric techniques (SVR and bootstrap), which enables its application in a variety of practical situations as long as empirical data of inputs and related response is available. The SVR allows for the treatment of complex processes for which the mapping of inputs into output is unknown. However, once the regression function is estimated in the training step, the prediction becomes a trivial mathematical exercise and the application of a given observation of the inputs on the estimated formula will always return the same predicted value for the response, independently of how many times it is calculated. This drawback of SVR (and of all regression methods, e.g. linear regression) is overcome by its coupling with bootstrap techniques that have the advantage of not requiring assumptions of the probabilistic models related to the inputs and to the response. In this way, the bootstrapped SVR provides not only a point estimate of the response, but also an associated interval of probable values, which is a valuable information in practical decision-making.\n8\tCONCLUSION\nThis work proposed models for risk and reliability quantification of systems and equipments in different phases of their life cycle (design and operation). Concerning the design phase and in order to solve a generalization of RAPs, a MOGA was developed. Not only the trade-off between cost and availability was handled, but also the behavior of the system was taken into account during the optimization procedure. The results from the validation examples indicate a superior performance of the proposed MOGA when compared to a MOACO, as the simulated fronts provided by the former were closer to the real Pareto fronts than the simulated fronts given by the latter.\nRegarding the operational phase, a similar MOGA was adapted to the context of multi-objective inspection plans in which cost and risk were the objectives to be both minimized. The proposed MOGA + RBI was used in an example involving a oil and gas separator vessel, three different inspection techniques and a planning horizon of 20 years. As an outcome, nondominated inspection schedules were obtained. In this way, the shortcomings of the RBI methodology when applied alone were overcome, since a risk target level was not required and the costs related to inspection activity were considered as an objective itself.\nIn both proposed MOGAs, the genetic operators sampling, crossover and mutation were devised to provide only feasible individuals. In the case of multi-objective RAPs, the configurations were subject to physical constraints concerning the minimum and maximum number of components per subsystem, whereas the inspection plans were subject to regulations\u2019 requirements. Such an adaptation of the genetic operators enabled a reduction of the search space explored by MOGA that was prevented from being lost in unfeasible regions. Additionally, unnecessary fitnesses evaluations - sometimes involving expensive computational procedures - of unfeasible individuals were not performed.\nAfter obtaining the Pareto front from MOGA, a ROI analysis was suggested to aid the decision maker in choosing a specific solution (system design and number of maintenance teams or an inspection plan). It was observed that considerable investments in system design are not necessarily translated into a great gain on system mean availability. Analogously, considerable investments in inspection do not certainly imply a great reduction in risk.\nAlso related to the operational phase of systems and equipments, the SVR learning method was used for the prediction of TBFs of onshore oil wells and of failures related to the scale growth rate on an equipmet used in offshore systems. In the first case, the SVR was combined to a PSO so as to select the most important influential variables of the TBFs concurrently to the SVR hyperparameters\u2019 adjustment. The obtained results suggest that whenever a variable is introduced into or removed from the data set, a tuning of the SVR\nhyperparameters must be performed so as to avoid reduction of the SVR performance.\nIn the second case, pairs and residuals bootstrap schemes were combined to SVR for the construction of confidence and prediction intervals related to the response variable. In this way, along with the accurate point predictions resulted from the adopted bagging estimator, interval inferences could also be performed. Additionally, since both SVR and bootstrap are non-parametric techniques, a reduced number of probabilistic assumptions were required. This is an interesting property of the proposed method, as it can be applied when no probabilistic models fit the phenomenon under analyais and / or when a large amount of data can not be obtained, e.g. due to budget constraints. The proposed methods were validated on an simulated example and a Monte Carlo experiment was performed in order to assess their coverage properties. Such a simulation indicated that the coverages are better when the training data set increases and that a greater number of bootstrap iterations does not necessarily enhance the coverages. Also, an illustration of how the presented methodology could support decision making concerning preventive maintenance actions was also provided; it was based on the interval estimates resulted from the boostrapped SVR and on a given threshold related to failure occurrence.\n8.1\tLimitations and Suggestions for Future Works\nAlthough the presented techniques can be, in principle, used in various contexts, the obtained results as well as the related inferences may not be generalized to other applications without a preliminary investigation.\nMOGA is a probabilistic model and does not guarantee that the true optimal solutions have been obtained. However, such a drawback can be partially overcome if various MOGA runs are performed with different GA parameter values (e.g. probabilities of crossover and mutation, numbers of individuals and generations). Each of these sets of parameters is expected to enable the exploration of a specific location of the search space and, in the end, the comparison of the results from each run may provide an overall better front. Besides, MOGA is a flexible optimization technique that permits the coupling with other methods for the generation of more realistic frameworks to support decision making. Also, it is specially useful in the absence of analytical objective functions, as in the case of superimposing GRPs, when the problem is related to a prohibitive number of combinations for an exhaustive evaluation of solutions, and / or multiple objectives need to be separately handled. In fact, there are many real cases in which the optimal solution or the Pareto front is not known or the objective functions and / or constraints are intractable. If the methods used to compute the objectives is time consuming (e.g. DES), the previously commented approach of GA parameter variation to provide an overall better front may be too expensive. Like MOGA, PSO is a probabilistic optimization model and there are no guarantees that the optimal solution have been attained. However, it has been\nsuccessfuly applied, mainly to real-valued problems and analogously to the case of MOGA, different PSO parameters can be used to explore various locations of the search space so as to provide overall better solutions, mainly if the fitness evaluation step is not time consuming.\nIn the multi-objective redundancy allocation problems, only the constraints related to the minimum and maximum number of components per subsystem were considered. Other constraints related, for example, to volume and weight, can be taken into account. Also, instead of GRPs, other methods describing the failure-operation process of components that incorporate the effects of repairs can be used, e.g. modulated power law processes (RIGDON &amp; BASU, 2000; SALDANHA &amp; FRUTUOSO E MELO, 2012).\nIn this work, ROI analysis was suggested to guide the decision concerning the selection of a solution (e.g. system design and number of maintenance teams or inspection plan) from the obtained Pareto front. Such an approach can be combined, for example, to elicitation methods (KAHNEMAN et al., 1982; MOSLEH et al., 1988; COOKE, 1991; CAMPELLO DE SOUZA, 2007) that capture the preferences of the decision maker so as to construct a circumstance-adapted post-optimization method.\nInspections reduce the uncertainty about the deterioration state of the equipment but no intervention is performed in order to reduce the associated risk level. In this way, they are able to suggest the performance of preventive maintenance so as to reduce and / or delay the action of the damage mechanism. In this work, however, the effects of preventive maintenance actions were not taken into account in the elaboration of multi-objective inspection plans. The combination of multi-objective inspection and preventive maintenance plans would consider the impacts of the maintenance actions on the deterioration state of the equipment, which would be incorporated in the risk computation by the RBI. Thus, the inspection and the maintenance activities could provide information to each other so as to efficiently determine their scheduling.\nIndeed, RBI is used for calculating risk of equipment from oil and petrochemical industries. In this way, the proposed elaboration of multi-objective inspection plans can only be used in such a context. However, other methods for risk evaluation applicable to other industries can be combined to MOGA, so as to provide non-dominated inspection plans with respect to risk and cost.\nThe predictive ability of SVR was basically assessed by NRMSE and MSE on a test set. However, even though a good performance on unseen data is expected, there are no guarantees that the estimated SVR model will properly function. But, if an SVR gives poor predictions on unseen cases, it can be interpreted as an indicator for a retraining procedure incorporating new data that have become available. In this way, as more data are collected, the SVR could be retrained and updated.\nNot only SVR but also bootstrap methods are suitable for situations where small to moderate observation sets are accessible. Nevertheless, the quality of the estimated models\nand values directly depends on the quality of the data used in the estimation step. Hence, in order to construct accurate data bases it is essential to give the appropriate importance for the data gathering procedure. If this is accomplished, more observations mean that more information about the phenomenon or process under analysis become available, which may positively impact the accuracy and precision of the SVR and bootstrap models and estimates.\nAlthough the variable selection procedure by PSO + SVR suggests the subset of the most relevant input variables, the variability of the response that can be apportioned to them are not quantified. In this way, a global sensitivity analysis (SALTELLI et al., 2004) approach could be combined to the PSO + SVR so as to allow for the ranking of the input variables according to their contribution for the variability of the response variable.\nThe methods and techniques considered and proposed in this work have potential to be combined in several manners so as to compose more general and comprehensive frameworks, as discussed in Section 1.2. For example, the components in the MOGA + DES were supposed to have times between failures governed by Weibull probability densities. However, it might not be the case and historic data concerning the components\u2019 failures could feed an SVR. For a given component, the SVR could provide the MOGA + DES with the time of its next failure. Thus, data related to the operational phase of the equipment could furnish valuable information for the design of systems. The same reasoning could be applied to the logistic time and to the TTRs. Also, the simulation block could incorporate not only failure times and corrective maintenance, but also inspection and preventive actions, given the importance of these activities for critical components. Finally, each proposed method or their combination is able to support decision making involving risk and reliability aspects of systems.\nREFERENCES\nAK, L.; LI, Y.; VITELLI, V.; ZIO, E.; DROGUETT, E. L. NSGA-II-trained neural network approach to the estimation of prediction intervals of scale deposition rate in oil &amp; gas equipment. Expert Systems with Applications, v. 40, p. 1205-1212, 2013.\nANGUITA, D.; BONI, A.; RIDELLA, S. Evaluating the generalization ability of support vector machines through the bootstrap. Neural Processing Letters, v. 11, p. 51-58, 2000.\nANP. Ag\u00eancia Nacional do Petr\u00f3leo, G\u00e1s Natural e Biocombust\u00edveis. 2011. Available at:&lt;http://www.anp.gov.br>. Date of access: 30-Mar-2011.\nAPI. American Petroleum Institute. API publication 581: risk-based inspection base resource document. Washington, DC, United States, 2008.\nBABAOGLU, I.; FINDIK, O.; \u00dcLKER, E. A comparison of feature selection models utilizing binary particle swarm optimization and genetic algorithm in determining coronary artery disease using support vector machine. Expert Systems with Applications, v. 37, p. 3177-3183, 2010.\nBARROS JR., P. F. R. Uma metodologia para an\u00e1lise de disponibilidade de sistemas complexos via hibridismo de redes Bayesianas e processos Markovianos. Disserta\u00e7\u00e3o (Mestrado) \u2014 Universidade Federal de Pernambuco, Recife, 2006.\nBOWLES, J. B. Comementary - caution: constant failure-rate models may be hazardous to your design. IEEE Transactions on Reliability, vol. 51, n. 3, p. 375-377, 2002.\nBOYD, S.; VANDENBERGHE, L. Convex optimization. Cambridge: Cambridge University Press, 2004. Available at:&lt;http://www.stanford.edu/~boyd/cvxbook>.\nBRABANTER, K. D.; BRABANTER, J. D.; SUYKENS, J. A. K.; MOOR, B. D. Approximate confidence and prediction intervals for least squares support vector regression. IEEE Transactions on Neural Networks, v. 22, n. 1, p. 110-120, January 2011.\nBRATTON, D.; KENNEDY, J. Defining a standard for particle swarm optimization. In: Proceedings of the IEEE Swarm Intelligence Symposium. Honolulu, United States: 2007.\nBREIMAN, L. Bagging predictors. Machine Learning, v. 24, p. 123-140, 1996.\nBRYANT, R. Symbolic Boolean manipulation with ordered binary decision diagram. ACM Computing Surveys, v. 24, p. 375-377, 1992.\nBURGES, C. J. C. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, v. 2, p. 121-167, 1998.\nBUSACCA, P. G.; MARSEGUERRA, M.; ZIO, E. Multiobjective optimization by genetic algorithms: application to safety systems. Reliability Engineering &amp; System Safety, vol. 72, p. 59-74, 2001.\nCADINI, F.; ZIO, E.; KOPUSTINKAS, V.; URBONAS, R. A model based on boostrapped neural networks for computing the maximum fuel cladding temperature in an Rmbk-1500 nuclear reactor accident. Nuclear Engineering and Design, v. 238, p. 21652172, 2008.\nCAMPELLO DE SOUZA, F. M. Decis\u00f5es racionais em situa\u00e7\u00f5es de incerteza. 2ed. Recife: 2007.\nCANTONI, M.; MARSEGUERRA, M.; ZIO, E. Genetic algorithms and Monte Carlo simulation for optimal plant design. Reliability Engineering &amp; System Safety, vol. 68, p. 29-38, 2000.\nCHANG, C.-C.; LIN, C.-J. LIBSVM: a library for support vector machines. 2001. Available at:&lt;http://www.csie.ntu.edu.tw/~cjlin/libsvm>.\nCHANG, M.-W.; LIN, C.-J. Leave-one-out bounds for support vector regression model selection. Neural Computation, v. 17, n. 5, p. 1188-1222, 2005.\nCHAPELLE, O.; VAPNIK, V.; BOUSQUET, O.; MUKHERJEE, S. Choosing multiple parameters for support vector machines. Machine Learning, v. 46, p. 131-159, 2002.\nCHEN, K.-Y. Forecasting systems reliability based on support vector regression with genetic algorithms. Reliability Engineering and System Safety, v. 92, p. 423-432, 2007.\nCHEN, T.; NEVILLE, A.; YUAN, M. Calcium carbonate scale formation: assessing the initial stages of precipitation and deposition. Journal of Petroleum Science and Engineering, v. 46, p. 185-194, 2005.\nCHIANG, C.-H.; CHEN, L.-H. Availability allocation and multiobjective optimization for parallel-series systems. European Journal of Operational Research, vol. 180, p. 1231-1244, 2007.\nCOELLO, C. A. C.; VELDHUIZEN, D. A. V.; LAMONT, G. B. Evolutionary algorithms for solving multiobjective problems. New York: Kluwer Academic, 2002.\nCOOKE, R. Experts in uncertainty: opinion and subjective probability in science. New York: Oxford University Press, 1991.\nCRIBARI-NETO, F. Asymptotic Inference Under Heteroskedasticity of Unknown Form. Computational Statistics &amp; Data Analysis, v. 45, p. 215-233, 2004.\nCRIBARI-NETO, F.; LIMA, M. G. A. Heteroskedasticity-consistent interval estimators. Journal of Statistical Computation and Simulation, v. 79, n. 6, p. 787-803, 2009.\nCRISTINIANI, N.; SHAWE-TAYLOR, J. An introduction to support vetor machines and other kernel-based learning methods. Cambridge: Cambridge Universty Press, 2000.\nDAVIDSON, J.; MONTICINI, A.; PEEL, D. Implementing the wild bootstrap using a two-point distribution. Economics Letters, v. 96, p. 309-315, 2007.\nDAVISON, A. C.; HINKLEY, D. V. Bootstrap methods and their applications. New York: Cambridge University Press, 1997. (Cambridge Series in Statistical and Probabilistic Mathematics).\nDEB, K. Evolutionary algorithms for multi-criterion optimization in engineering design. In: Evolutionary Algorithms in Engineering and Computer Science (EUROGEN'99). 1999.\nDEB, K.; PRATAP, A.; AGARWAL, S.; MEYARIVAN, T. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Transactions on Evolutionary Computation, vol. 6, n. 2, p. 182-197, 2002.\nDOYEN, L.; GAUDOIN, O. Classes of imperfect repair models based on reduction of failure intensity or virtual age. Reliability Engineering &amp; System Safety, vol. 84, p. 45-56, 2004.\nDYER, S. J.; GRAHAM, G. M. The effect of temperature and pressure on oilfield scale formation. . Journal of Petroleum Science and Engineering, v. 35, p. 95-107, 2002.\nEFRON, B. Bootstrap methods: another look at the jackknife. The Annals of Statistics, v. 7, n. 1, p. 1-26, 1979.\nEFRON, B.; TIBSHIRANI, R. J. An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC, 1993. (Monographs on Statistics and Applied Probability 57).\nELEGBEDE, C.; ADJALLAH, K. Availability allocation to repairable systems with genetic algorithms: a multiobjective formulation. Reliability Engineering &amp; System Safety, vol. 82, p. 319-330, 2003.\nFEI, S.-W.; WANG, M.-J.; MIAO, Y.-B.; TU, J.; LIU, C.-L. Particle swarm optimizationbased support vector machine for forecasting dissolved gases content in power transformer oil. Energy Conversion and Management, v. 50, p. 1604-1609, 2009.\nFONSECA, C. M.; FLEMING, P. J. Genetic algorithms for multi-objective optimization: formulation, discussion and generalization. In: Proceedings of the Fifth International Conference on Genetic Algorithms. 1993.\nFRIEDMAN, J. H.; TUCKEY, J. W. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, v. 23, n. 9, p. 881-890, 1974.\nFROHLICH, H.; CHAPELLE, O.; SCHOLKOPF, B. Feature selection for support vector machines by means of genetic algorithms. In: Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence. 2003.\nFURTADO, M. J. H.; SOARES, R. F; LINS, I. D.; MOURA, M. C; DROGUETT, E. L; FIRMINO, P. R. A. Multi-objective optimization of risk and cost for risk-based inspection plans. In: Proceedings of the 11th International Probabilistic Safety Assessment and Management Conference (PSAM 11) and of the European Safety &amp; Reliability Association Conference (ESREL). Helsinki, Finland: 2012.\nGOLDBERG, D. E. Genetic algorithms in search, optimization and machine learning. Addison-Wesley, 1989.\nGUYON, I.; ELISSEEFF, A. An introduction to variable and feature selection. Journal of Machine Learning Research, v. 3, p. 1157-1182, 2003.\nGUYON, I.; WESTON, J.; BARNHILL, S.; VAPNIK, V. Gene selection for cancer classification using support vector machines. Machine Learning, v. 46, p. 389-422, 2002.\nHELTON, J. C.; DAVIS, F. J. Latin hypercube sampling and the propagation of uncertainty in analyses of complex systems. Reliability Engineering &amp; System Safety, v. 81, p. 23-69, 2003.\nHESKES, T. Practical confidence and prediction intervals. In: MOZER, M.; JORDAN, M.; HESKES, T. (Ed.). Advances in neural information processing systems 9. Cambridge: MIT Press, 1997. p. 466-472.\nHIGGINGS, J. J. Introduction to modern nonparametric statistics. Pacific Groove: Brooks/Cole - Thomson Learning, 2004.\nHORN, J.; NAFPLIOTIS, N.; GOLDBERG, D. E. A niched Pareto genetic algorithm for multiobjective optimization. In: Proceedings of the First IEEE Conference on Evolutionary Computation. 1994.\nHSU, C.-W.; CHANG, C.-C.; LIN, C.-J. A practical guide to support vector classification. 2003. Available at:&lt;http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf>.\nITO, K.; NAKANO, R. Optimizing support vector regression hyperparameters based on cross-validation. In: Proceedings of the International Joint Conference on Neural Networks. Portland, United States: 2003.\nJOACHIMS, T. Making large-scale SVM learning practical. In: SCHOLKOPF, B.; BURGES, C.; SMOLA, A. J. (Ed.). Advances in kernel methods: support vector learning. Cambridge: The MIT Press, 1999. p. 169-184.\nJONES, J.; HAYES, J. Estimation of system reliability using a \u201cnon-constant failure rate\u201d model. IEEE Transactions on Reliability, vol. 50, p. 286-288, 2001.\nJUANG, Y.-S.; LIN, S.-S.; KAO, H.-P. A knowledge management system for seriesparallel availability optimization and design. Expert Systems with Applications, vol. 34, p. 181-193, 2008.\nKAHNEMAN, D.; SLOVIC, P.; TVERSKY, A. Judgement under uncertainty: heuristics and biases. New York: Oxford University Press, 1982.\nKECMAN, V. Learning and soft computing: support vector machines, neural networks and fuzzy logic models. Cambridge: The MIT Press, 2001.\nKECMAN, V. Support vector machines: an introduction. In: WANG, L. (Ed.). Support vector machines: theory and applications. Berlin Heidelberg: Springer-Verlag, 2005, (Studies in Fuzziness and Soft Computing, v. 177). p. 1-47.\nKENNEDY, J.; EBERHART, R. Particle swarm optimization. In: Proceedings of the IEEE International Conference on Neural Networks. Perth, Australia: 1995.\nKENNEDY, J.; EBERHART, R.; SHI, Y. Swarm intelligence. San Francisco: Morgan Kaufmann, 2001.\nKIJIMA, M.; SUMITA, N. A useful generalization of renewal theory: counting process governed by non-negative Markovian increments. Journal of Applied Probability, vol. 23, p. 71-88, 1986.\nKNOWLES, J.; CORNE, D. On metrics for comparing nondominated sets. In: Proceedings of the 2002 Congress on Evolutionary Computation Conference (CEC02). IEEE Press, 2002. p. 711-716.\nKOHAVI, R.; JOHN, G. H. Wrappers for feature subset selection. Artificial Intelligence, v. 97, p. 273-324, 1997.\nKUO, W.; PRASAD, V. R.; TILLMAN, F. A.; HWANG, C.-L. Optimal reliability design: fundamentals and applications. United Kingdom: Cambridge University Press, 2001.\nKUO, W.; WAN, R. Recent advances in optimal reliability allocation. IEEE Transactions on Systems, Man and Cybernetics, vol. 37, n. 4, p. 143-156, 2007.\nLIN, C. J.; WENG, R. C. Simple probabilistic predictions for support vector regression. 2004.\nLIN, H.-T.; LIN, C.-J. A study on sigmoid kernels for SVM and the training of non-PSD kernels by SMO-type methods. 2003.\nLIN, S.-W.; YING, K.-C.; CHEN, S.-C.; LEE, Z.-J. Particle swarm optimization for parameter determination and feature selection of support vector machines. Expert Systems with Applications, v. 35, p. 1817-1824, 2008.\nLINS, I. D.; DROGUETT, E. L. Multiobjective optimization of redundancy allocaion problems in systemas with imperfect repairs via ant colony and discrete event simulation. In: Proceedings of the European Safety &amp; Reliability Conference (ESREL). Valencia, Spain: 2008.\nLINS, I. D.; DROGUETT, E. L. Multiobjective optimization of availability and cost in repairable systems via genetic algorithms and discrete event simulation. Pesquisa Operacional, vol. 29, p. 43-66, 2009.\nLINS, I. D.; DROGUETT, E. L. Redundancy allocation problems considering systems with imperfect repairs using multi-objective genetic algorithms and discrete event simulation. Simulation Modelling Practice and Theory, v. 19, n. 1, p. 362-381, 2011.\nLINS, I. D.; MOURA, M. C.; DROGUETT, E. L. Support vector machines and particle swarm optimization: applications to reliability prediction. Saarbrucken, Germany: Lambert Academic Publishing, 2010a. ISBN 9783838319407.\nLINS, I. D.; MOURA, M. C.; DROGUETT, E. L.; ZIO, E. Reliability prediction of oil production wells by particle swarm optimized support vector machines. In: Proceedings of the European Safety &amp; Reliability Conference (ESREL). Rhodes, Greece: 2010b.\nLINS, I. D.; MOURA, M. C.; DROGUETT, E. L.; ZIO, E.; JACINTO, C. M. Reliability prediction of oil wells by support vector machine with particle swarm optimization for variable selection and hyperparameter tuning. In: Proceedings of the European Safety &amp; Reliability Conference (ESREL). Troyes, France: 2011b.\nLINS, I. D.; MOURA, M. C.; ZIO, E.; DROGUETT, E. L. A particle swarm-optimized support vector machine for reliability prediction. Quality and Reliability Engineering International, v. 28, n. 2, p. 141-158, 2012a.\nLINS, I. D.; MOURA, M. C.; DROGUETT, E. L.; ZIO, E.; JACINTO, C. M. Uncertainty assessment through bootstrapped support vector regression. In: Proceedings of the 11th International Probabilistic Safety Assessment and Management Conference (PSAM 11) and of the European Safety &amp; Reliability Association Conference (ESREL). Helsinki, Finland: 2012b.\nLINS, I. D.; R\u00caGO, L. C.; MOURA, M. C.; DROGUETT, E. L. Security system designs via games of imperfect information and multi-objective genetic algorithms. In: Proceedings of the International Topical Meeting on Probabilistic Safety Assessment and Analysis (PSA). Wilmington, NC, USA: 2011a.\nLINS, I. D.; R\u00caGO, L. C.; MOURA, M. C.; DROGUETT, E. L. Selection of security system design via games of imperfect information and multi-objective genetic algorithm. Reliability Engineering &amp; System Safety, v. 112, p. 59-66, 2013.\nLINS, S. Princ\u00edpios de enumera\u00e7\u00e3o. Rio de Janeiro: Instituto de Matem\u00e1tica Pura e Aplicada, 1981.\nLITTLEWOOD, B. Letter to the Editor. Reliability Engineering &amp; System Safety, vol. 93, p. 1287, 2008.\nLIU, R. Bootstrap procedures under some non-i.i.d. models. The Annals of Statistics, v. 16, n. 4, p. 1696-1708, 1988.\nMARANGONE, F. C.; FREIRE, J. L. F. Gerenciamento da integridade de um vaso de press\u00e3o utilizando a inspe\u00e7\u00e3o baseada em risco. In: Proceedings of the 8a Confer\u00eancia sobre Tecnologia de Equipamentos. Salvador, BA, Brazil: 2005.\nMARSEGUERRA, M.; ZIO, E.; PODOFILLINI, L. Multiobjective spare part allocation by means of genetic algorithms and Monte Carlo simulation. Reliability Engineering &amp; System Safety, vol. 87, p. 325-335, 2005.\nMESSAC, A.; SUNDARARAJ, G. J.; TAPPETA, R. V.; RENAUD, J. E. Ability of objective functions to generate points on nonconvex Pareto frontiers. AIAA Journal, vol. 38, n. 6, p. 1084-1091, 2000.\nMICHALEWICZ, Z. Genetic algorithms + data structures. 3ed. Berlin: Springer-Verlag, 1996.\nMODARRES, M. Risk analysis in engineering: techniques, tools and trends. Boca Raton: Taylor &amp; Francis, 2006.\nMODARRES, M.; KAMINSKY, M.; KRIVTSOV, V. Reliability engineering and risk analysis. New York: Marcel Dekker, 1999.\nMOMMA, M.; BENNETT, K. P. A pattern search method for model selection of support vector regression. In: Proceedings of the 2002 SIAM International Conference on Data Mining. 2002. p. 261-274.\nMONTGOMERY, D. C.; PECK, E. A.; VINING, G. G. Introduction to linear regression analysis. 4ed. Hoboken: John Wiley &amp; Sons, 2006.\nMOSLEH, A.; BIER, V. M.; APOSTOLAKIS, G. A critique of current practice for the use of expert opinions in probabilistic risk assessment. Reliability Engineering &amp; System Safety, v. 20, p. 63-85, 1988.\nMOURA, M. C.; ROCHA, S. P. V.; DROGUETT, E. A. L.; JACINTO, C. M. C. Bayesian assessment of maintenance efficiency via generalized renewal process. In Portuguese. Pesquisa Operacional, vol. 27, p. 569-589, 2007.\nMOURA, M. C.; ZIO, E.; LINS, I. D.; DROGUETT, E. L. Failure and reliability prediction by support vector machines regression of time series data. Reliability Engineering &amp; System Safety, v. 96, p. 1527-1534, 2011.\nNOCEDAL, J.; WRIGHT, S. J. Numerical optimization. 2ed. New York: 2006.\nPAI, P.-F. System reliability forecasting by support vector machines with genetic algorithms. Mathematical and Computer Modelling, v. 43, p. 262-274, 2006.\nPLATT, J. C. Fast training of support vector machines using sequential minimal optimization. In: SCHOLKOPF, B.; BURGES, C. J. C.; SMOLA, A. (Ed.). Advances in kernel methods: support vector machines. Cambridge: The MIT Press, 1998.\nRAKOTOMAMONJY, A. Variable selection using SVM-based criteria. Journal of Machine Learning Research, v. 3, p. 1357-1370, 2003.\nRAUSAND, M.; HOYLAND, A. System reliability theory: models and statistical methods. 2ed. New York: John Wiley &amp; Sons, 2004.\nRAUZY, A. Mathematical foundations of minimal cutsets. IEEE Transactions on Reliability, v. 50, p. 389-396, 2001.\nRIGDON, S. E.; BASU, A. P. Statistical methods for the reliability of repairable systems. New York: John Wiley &amp; Sons, 2000.\nROSS, S. M. Simulation. 3ed. San Diego: Academic Press, 2002.\nROSS, S. M. Introduction to probability models. 10ed. San Diego: Academic Press, 2010.\nSALDANHA, P. L. C.; FRUTUOSO E MELO, P. F. Analysis of the qualified life extension of nuclear safety systems through modulated Poisson point processes. Progress in Nuclear Energy, v. 60, p. 117-128, 2012.\nSALTELLI, A.; TARANTOLA, S.; CAMPOLONGO, F.; RATTO, M. Sensitivity analysis in practice: a guide to assessing scientific models. Chichester: Wiley, 2004.\nSANTOS, C. R.; LINS, I. D.; FIRMINO, P. R. A.; MOURA, M. C.; DROGUETT, E. L. A method for optimal allocation of defensive alternatives: analysis of a strategic interaction with a multi-objective approach. In: Proceedings of the 10th International Probabilistic Safety Assessment and Management Conference (PSAM 10). Seattle, WA, USA: 2010.\nSCHAFFER, J. D. Multiple objective optimization with vector evaluated genetic algorithms. In: Proceedings of the First International Conference on Genetic Algorithms. 1985.\nSCHOLKOPF, B.; SMOLA, A. J. Learning with kernels: support vector machines, regularization, optimization and beyond. Cambridge: The MIT Press, 2002.\nSEASHOLTZ, M. B.; KOWALSKI, B. The parsimony principle applied to multivariate calibration. Analytica Chimica Acta, v. 277, n. 2, p. 165-177, 1993.\nSECCHI, P.; ZIO, E.; MAIO, F. D. Quantifying uncertainties in the estimation of safety parameters by using bootstrapped artificial neural networks. Annals of Nuclear Energy, v. 35, p. 2338-2350, 2008.\nSRINIVAS, N. K.; DEB, K. Multiobjective optimization using nondominated sorting in genetic algorithms. Journal of Evolutionary Computation, vol. 2, n. 3, p. 221-248, 1994.\nTABOADA, H. A.; BAHERANWALA, F.; COIT, D. W.; WATTANAPONGSAKORN, N. Practical solutions for multi-objective optimization: an application to system reliability design problems. Reliability Engineering &amp; System Safety, vol. 92, p. 314-322, 2007.\nTABOADA, H. A.; ESPIRITU, J.; COIT, D. W. MOMS-GA: a multiobjective multi-state genetic algorithm for system reliability optimization design problems. IEEE Transactions on Reliability, vol. 57, p. 182-191, 2008.\nTSUJITANI, M.; TANAKA, Y. Cross-validation, bootstrap, and support vector machines. Advances in Artificial Neural Systems, v. 3, p. 45-55, 2011.\nVAPNIK, V. An overview of statistical learning theory. IEEE Transactions on Neural Networks, v. 10, n. 5, p. 988-999, 1999.\nVAPNIK, V. N. The nature of statistical learning theory. 2ed. New York: Springer-Verlag, 2000.\nWOODSEND, K. Using interior point methods for large-scale support vector machine training. Tese (Doutorado) - University of Edinburgh, 2009.\nWOODSEND, K.; GONDZIO, J. Exploiting separability in large-scale linear support vector machine training. Computational Optimization and Applications, v. 49, p. 241269, 2011.\nWRIGHT, S. J. Primal-Dual Interior Point Methods. SIAM, 1997.\nWU, K.-P.; WANG, S.-D. Choosing the kernel parameters for support vector machines by the inter-cluster distance in the feature space. Pattern Recognition, v. 42, p. 710-717, 2009.\nYAN, J.; LI, C.; WANG, Z.; DENG, L.; SUN, D. Diversity metrics in multiobjective optimization: review and perspective. In: IEEE International Conference on Integration Technology. Shenzhen, China: 2007.\nYAN, W.; SHAO, H.; WANG, X. Soft sensing modeling based on support vector machines and Bayesian model selection. Computers and Chemical Engineering, vol. 28, p. 14891498, 2004.\nYANES, M.; JOGLAR, F.; MODARRES, M. Generalized renewal process for analysis of repairable systems with limited failure experience. Reliability Engineering &amp; System Safety, vol. 77, p. 167-180, 2002.\nYANG, J.-B.; ONG, C.-J. Feature selection for support vector regression using probabilistic prediction. In: Proceedings of the 16th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Washington, DC, United States: 2010. p. 343-351.\nZAMITH, R.; SANTOS, E. M. dos. Atividades onshore no Brasil: regula\u00e7\u00e3o, pol\u00edticas p\u00fablicas e desenvolvimento local. S\u00e3o Paulo: Annablume; Fapesp, 2007.\nZHANG, Y.; SHAW, H.; FARQUHAR, R.; DAWE, R. The kinetics of carbonate scaling: application for the prediction of downhole carbonate scaling. Journal of Petroleum Science and Engineering, v. 29, p. 85-95, 2001.\nZIO, E. A study of the bootstrap method for estimating the accuracy of artificial neural networks in predicting nuclear transient processes. IEEE Transactions on Nuclear Science, v. 53, n. 3, p. 1460-1478, June 2006.\nZIO, E.; APOSTOLAKIS, G. E.; PEDRONI, N. Quantitative functional failure analysis of a thermal-hydraulic passive system by means of bootstrapped artificial neural networks. Annals of Nuclear Energy, v. 37, p. 639-649, 2010.\nZIO, E.; BROGGI, M.; GOLEA, L.; PEDRONI, N. Failure and reliability predictions by Infinite Response Locally Recurrent Neural Networks. In: Proceedings of 5th European Congress on Computational Methods in Applied Science and Engineering (ECCOMAS). Venice, Italy: 2008.\nZITZLER, E. Evolutionary algorithms for multiobjective optimization: methods and applications. Tese (Doutorado) - Swiss Federal Institute of Technology Zurich, 1999.\nAppendix\nLagrangian and KKT First Order Conditions for SVR Training Problem\nThe Lagrangian related to the training problem (5.3)-(5.7) is:\n1\ti\ti\nC(w,b,&amp;,&amp;*,a,a*,ft,ft*) = -wTw + c \u2022 \u00a3)(&amp; + &amp;*) -^2 \u00aei ' [wT^(xi) + b ~ M + \u00a3 + &amp;]\n2\ti=i\ti=i\n-\ta* \u2022 [yi - wT^(xi} -b +e + c] - ^(A&amp; +\t)\nl=1\ti=1\n(A.1)\nin which a,a*,ft,ft* are the \u00bf-dimensional vectors of Lagrange multipliers associated to constraints (5.4) and (5.5) respectively. Note that ai and a* can not be strictly positive simultaneously, given that there is no point satisfying both (5.4) and (5.5) at the same time. Hence, aia* = 0. The Lagrangian in (A.1) must be minimized with respect to primal variables w,b,&amp;,&amp;* and maximized with respect to dual variables a, a*, /3, /3*. Then the saddle point (w0,b0,&amp;0,&amp;Q,a0,aQ,ft0,ftQ) of L has to be found. The related KKT first order optimality conditions are:\nd L(w,\tb,\ta\t*)\n\tdw\t\t\nd L(w,\tb,&amp;,C,a,\ta\t,P,ft *)\n\tdb\t\t\nd L(w,\tb,&amp;,C,a,\ta\t,P,ft *)\n\td&amp;i\t\t\nd L(w,\tb,&amp;,C,a,\ta\t,P,ft *)\n0,\n0,\n0,\n0,\nwT^(xi) + bo - yi + e +&lt;iq > 0,\nyi - wTj)(xi) - bo + e +&lt;i0 > 0,\nwo = ^2(aio - a*o)^(xi) i=1\ni\nE(aio - aio) = 0 i=1\nC - aio = ftio, V l\nC - aio = ft*o, VI\nV\tl\nV\tl\n&amp;io > 0, Vl\n&amp;io > 0, VI aio > 0, Vl al o > 0, Vl ftio > 0, Vl\nft*o > 0, Vl\naio \u2022 M^(xi) + bo - yi + e + $q] = 0, Vl\n(A.2)\n(A.3)\n(A.4)\n(A.5)\n(A.6)\n(A.7)\n(A.8)\n(A.9)\n(A.10)\n(A.11)\n(A.12)\n(A.13)\n(A.14)\n\naf0 \u2022 [yi - wo&lt;/>x - b0 + \u00a3 +&lt;1*0] = o,\tV l\t\t\t(A.15)\n/3i0^i0 = 0,\tc \u2014 ai0)\t\u2022&lt;i0 = 0,\tVl\t(A.16)\n. = 0,\t' \u2014 a i0)\t\u2022 &lt;= 0,\tVl\t(A.17)\nBy replacing equalities (A.2)-(A.5) in (A.1), the objective function (5.10) is defined. The constraint (A.3) remains in the problem. Constraints (5.12) and (5.13), in turn, are obtained by the combination of Equations (A.4), (A.8), (A.10), (A.12) and Equations (A.5), (A.9), (A.11), (A.13), rescpectively. Note that problem (5.10)-(5.13) presents only the dual variables ai and af as unknowns, which have c as upper bound, so as to respect the non-negativity of /3l and \u00bf0*, see (A.12) and (A.13)."}]}}}