{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.20255"}, {"@name": "filename", "#text": "3727_323092.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DE SANTA CATARINA\nDEPARTAMENTO DE AUTOMA\u00c7\u00c3O E SISTEMAS\n\nCaio Merlini Giuliani\n\nESTRAT\u00c9GIAS DE OTIMIZA\u00c7\u00c3O N\u00c3O\nDIFERENCI\u00c1VEL APLICADAS \u00c0 MAXIMIZA\u00c7\u00c3O DA\n\nPRODU\u00c7\u00c3O DE CAMPOS DE PETR\u00d3LEO\n\nFlorian\u00f3polis\n\n2013\n\n\n\nCaio Merlini Giuliani\n\nESTRAT\u00c9GIAS DE OTIMIZA\u00c7\u00c3O N\u00c3O\nDIFERENCI\u00c1VEL APLICADAS \u00c0 MAXIMIZA\u00c7\u00c3O DA\n\nPRODU\u00c7\u00c3O DE CAMPOS DE PETR\u00d3LEO\n\nDisserta\u00e7\u00e3o submetida ao Programa\nde P\u00f3s-Gradua\u00e7\u00e3o em Engenharia de\nAutoma\u00e7\u00e3o e Sistemas para a obten-\n\u00e7\u00e3o do Grau de Mestre em Engenharia\nde Automa\u00e7\u00e3o e Sistemas.\nOrientador: Prof. Eduardo Campo-\nnogara, Dr.\nCoorientador: Prof. Agustinho Pluce-\nnio, Dr.\n\nFlorian\u00f3polis\n\n2013\n\n\n\nFicha de identifica\u00e7\u00e3o da obra elaborada pelo autor,\n atrav\u00e9s do Programa de Gera\u00e7\u00e3o Autom\u00e1tica da Biblioteca Universit\u00e1ria da UFSC.\n\nGiuliani, Caio Merlini\n   Estrat\u00e9gias de otimiza\u00e7\u00e3o n\u00e3o diferenci\u00e1vel aplicadas \u00e0\nmaximiza\u00e7\u00e3o da produ\u00e7\u00e3o de campos de petr\u00f3leo / Caio Merlini\nGiuliani ; orientador, Eduardo Camponogara ; co-\norientador, Agustinho Plucenio. - Florian\u00f3polis, SC, 2013.\n   98 p.\n\n   Disserta\u00e7\u00e3o (mestrado) - Universidade Federal de Santa\nCatarina, Centro Tecnol\u00f3gico. Programa de P\u00f3s-Gradua\u00e7\u00e3o em\nEngenharia de Automa\u00e7\u00e3o e Sistemas.\n\n   Inclui refer\u00eancias \n\n   1. Engenharia de Automa\u00e7\u00e3o e Sistemas. 2. Otimiza\u00e7\u00e3o sem\nderivada. 3. Otimiza\u00e7\u00e3o da produ\u00e7\u00e3o de petr\u00f3leo. 4. Regi\u00e3o de\nconfian\u00e7a. 5. Busca direta. I. Camponogara, Eduardo. II.\nPlucenio, Agustinho. III. Universidade Federal de Santa\nCatarina. Programa de P\u00f3s-Gradua\u00e7\u00e3o em Engenharia de\nAutoma\u00e7\u00e3o e Sistemas. IV. T\u00edtulo.\n\n\n\nCaio Merlini Giuliani\n\nESTRAT\u00c9GIAS DE OTIMIZA\u00c7\u00c3O N\u00c3O\nDIFERENCI\u00c1VEL APLICADAS \u00c0 MAXIMIZA\u00c7\u00c3O DA\n\nPRODU\u00c7\u00c3O DE CAMPOS DE PETR\u00d3LEO\n\nEsta Disserta\u00e7\u00e3o foi julgada aprovada para a obten\u00e7\u00e3o do T\u00edtulo\nde \u201cMestre em Engenharia de Automa\u00e7\u00e3o e Sistemas\u201d, e aprovada em\nsua forma final pelo Programa de P\u00f3s-Gradua\u00e7\u00e3o em Engenharia de\nAutoma\u00e7\u00e3o e Sistemas.\n\nFlorian\u00f3polis, 10 de Setembro 2013.\n\nProf. Jomi Fred H\u00fcbner, Dr.\nCoordenador do Curso\n\nProf. Eduardo Camponogara, Dr.\nOrientador\n\nProf. Agustinho Plucenio, Dr.\nCoorientador\n\nBanca Examinadora:\n\nProf. Eduardo Camponogara, Dr.\nPresidente\n\nProf. Alexandre Trofino Neto, Dr.\n\n\n\n\n\nEng. Alex Furtado Teixeira, M. Sc.\n\nEng. Bruno da Costa Flach, Dr.\n\nProf. Daniel Juan Pagano, Dr.\n\n\n\n\n\nAGRADECIMENTOS\n\nInicialmente, agrade\u00e7o \u00e0 minha amada Fam\u00edlia, meu suporte em\ntoda a vida. Pai, M\u00e3e, Em\u00edlia, sou profundamente grato por todos voc\u00eas.\n\nAgrade\u00e7o \u00e0 Elisa, pela companhia, apoio e paci\u00eancia.\nAo meu orientador, Eduardo Camponogara, e coorientador, Agus-\n\ntinho Plucenio, agrade\u00e7o pela orienta\u00e7\u00e3o, por todas as oportunidades e\npela confian\u00e7a.\n\nAgrade\u00e7o aos colegas e amigos que me acompanharam durante\no curso.\n\nAgrade\u00e7o aos membros da banca, pela aten\u00e7\u00e3o, pelas cr\u00edticas e\nsugest\u00f5es.\n\nAgrade\u00e7o ao Programa de P\u00f3s-gradua\u00e7\u00e3o em Engenharia de Au-\ntoma\u00e7\u00e3o e Sistemas, pela oportunidade.\n\nAgrade\u00e7o ao Centro de Pesquisas da Petrobras (CENPES), pelo\napoio financeiro com a bolsa.\n\n\n\n\n\nSaruman believes it is only\ngreat power that can hold\nevil in check, but that is\nnot what I have found. I\nfound it is the small\neveryday deeds of ordinary\nfolk that keep the darkness\nat bay. Small acts of\nkindness and love.\n\nGandalf\n\n\n\n\n\nRESUMO\n\nEste trabalho apresenta m\u00e9todos de otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel\naplicados \u00e0 produ\u00e7\u00e3o de petr\u00f3leo. Na ind\u00fastria do petr\u00f3leo e g\u00e1s, a pro-\ndu\u00e7\u00e3o de reservat\u00f3rios, po\u00e7os e sistemas relacionados pode ser predita\ncom a utiliza\u00e7\u00e3o de simuladores num\u00e9ricos. Este trabalho estuda t\u00e9cni-\ncas de otimiza\u00e7\u00e3o que n\u00e3o fazem uso de derivadas da fun\u00e7\u00e3o objetivo,\nsendo adequadas para a utiliza\u00e7\u00e3o direta de ferramentas de simula\u00e7\u00e3o.\nS\u00e3o apresentadas a \u201cbusca direta direcional\u201d e \u201cregi\u00e3o de confian\u00e7a n\u00e3o-\ndiferenci\u00e1vel\u201d. A primeira n\u00e3o faz qualquer uso de modelos, enquanto\na segunda utiliza modelos que aproximam a fun\u00e7\u00e3o objetivo em uma\nregi\u00e3o limitada. Ambas s\u00e3o estudadas em suas formas irrestritas e com\nrestri\u00e7\u00f5es lineares nas vari\u00e1veis. Foi feita uma an\u00e1lise computacional\nem que ambos os m\u00e9todos foram utilizados para a aloca\u00e7\u00e3o de g\u00e1s\nde inje\u00e7\u00e3o a um campo de produ\u00e7\u00e3o de petr\u00f3leo, com as produ\u00e7\u00f5es\ndos po\u00e7os modeladas por fun\u00e7\u00f5es suaves, que garantiam suas condi\u00e7\u00f5es\nde converg\u00eancia. Os dois m\u00e9todos convergiram para os pontos \u00f3timos,\nsendo que o de regi\u00e3o de confian\u00e7a apresentou maior efici\u00eancia. Uma se-\ngunda an\u00e1lise computacional, contemplando apenas o m\u00e9todo de regi\u00e3o\nde confian\u00e7a foi realizada empregando um simulador fenomenol\u00f3gico de\npo\u00e7os de petr\u00f3leo. Ambos os algoritmos podem servir de base para a\notimiza\u00e7\u00e3o tamb\u00e9m com restri\u00e7\u00f5es n\u00e3o-lineares. Para tanto, propomos\na utiliza\u00e7\u00e3o do m\u00e9todo de Lagrangiano aumentado, que substitui as res-\ntri\u00e7\u00f5es n\u00e3o-lineares por penaliza\u00e7\u00f5es na fun\u00e7\u00e3o objetivo, transformando\no problema n\u00e3o-linear em uma sequ\u00eancia de problemas com restri\u00e7\u00f5es\nlineares. \u00c9 poss\u00edvel implement\u00e1-lo sem necessidade de informa\u00e7\u00f5es sobre\nas derivadas. Apresentamos a teoria de como isto pode ser feito, por\u00e9m\nsem uma an\u00e1lise num\u00e9rica.\n\nPalavras-chave: Otimiza\u00e7\u00e3o sem derivada. Otimiza\u00e7\u00e3o da produ\u00e7\u00e3o\nde petr\u00f3leo. Regi\u00e3o de confian\u00e7a. Busca direta.\n\n\n\n\n\nABSTRACT\n\nThis work presented methods of nondifferentiable optimization\napplied to the production of petroleum. In the petroleum industry, the\nproduction of reservoirs, wells and related systems can be accurately\npredicted using numerical computer simulators. This work presented\ntechniques of optimization that do not use the derivative of the objective\nfunction, hence better suited to use directly those simulation tools. The\nmethods of directional direct-search and nondifferentiable trust-region\nare presented. The former does not make use of any model, whereas the\nlatter samples the objective function to build models in a limited region.\nBoth are studied in their unconstrained form and with linear constraints\non the variables. A computational analysis has been carried out, in which\nboth methods where employed in order to optimize the lift-gas allocation\nin a field of petroleum wells, modeled by smooth functions, so that\ntheir convergence conditions were satisfied. Both methods converged\nto the optimum, being the trust-region the more effective. A second\nanalysis has been conduced, using only the trust-region method and a\nphenomenological simulator of petroleum wells. Both algorithms can be\nused also in optimization with nonlinear constraints. To that end, we\npropose the method of augmented Lagrangian, in which the nonlinear\nconstraints are substituted by penalizations on the objective function,\nrendering the solution of nonlinear problem a sequence of subproblems\nwith linear constraints. This method can also be used without knowledge\nof derivatives. Part of the underlying theory is presented.\n\nKeywords: Derivative free optimization. Petroleum production opti-\nmization. Trust region. Direct-search.\n\n\n\n\n\nSUM\u00c1RIO\n\n1 Introdu\u00e7\u00e3o 17\n1.1 Organiza\u00e7\u00e3o do documento . . . . . . . . . . . . . . . . 18\n\n2 Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita 21\n2.1 Busca direta direcional . . . . . . . . . . . . . . . . . . . 21\n2.2 Regi\u00e3o de confian\u00e7a . . . . . . . . . . . . . . . . . . . . 30\n\n2.2.1 Defini\u00e7\u00e3o do modelo . . . . . . . . . . . . . . . . 31\n2.2.2 Polin\u00f4mios de Lagrange . . . . . . . . . . . . . . 34\n2.2.3 C\u00e1lculo do passo . . . . . . . . . . . . . . . . . . 37\n2.2.4 Aceita\u00e7\u00e3o do passo e gerenciamento da regi\u00e3o . . 38\n2.2.5 Teste de criticidade . . . . . . . . . . . . . . . . . 39\n2.2.6 Algoritmo de regi\u00e3o de confian\u00e7a . . . . . . . . . 39\n\n2.3 Alternativas algor\u00edtmicas . . . . . . . . . . . . . . . . . . 42\n2.4 Sum\u00e1rio . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n\n3 Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares 45\n3.1 Material b\u00e1sico sobre otimiza\u00e7\u00e3o restrita . . . . . . . . . 45\n3.2 Busca direta direcional com restri\u00e7\u00f5es lineares . . . . . . 47\n\n3.2.1 Alternativas algor\u00edtmicas . . . . . . . . . . . . . 52\n3.3 Regi\u00e3o de confian\u00e7a . . . . . . . . . . . . . . . . . . . . 54\n\n3.3.1 Passo generalizado de Cauchy . . . . . . . . . . . 54\n3.3.2 Algoritmo de regi\u00e3o de confian\u00e7a com restri\u00e7\u00f5es\n\nlineares . . . . . . . . . . . . . . . . . . . . . . . 55\n3.3.3 Alternativas algor\u00edtmicas . . . . . . . . . . . . . 57\n\n3.4 Sum\u00e1rio . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\n4 Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares 59\n4.1 Lagrangiano aumentado . . . . . . . . . . . . . . . . . . 59\n\n4.1.1 Lagrangiano aumentado com resolu\u00e7\u00e3o aproxi-\nmada dos sub-problemas . . . . . . . . . . . . . . 60\n\n4.1.2 Lagrangiano aumentado com restri\u00e7\u00f5es de desi-\ngualdade . . . . . . . . . . . . . . . . . . . . . . 62\n\n4.1.3 Elimina\u00e7\u00e3o parcial das restri\u00e7\u00f5es . . . . . . . . . 64\n4.2 Um algoritmo de Lagrangiano aumentado . . . . . . . . 65\n4.3 Resolvendo os sub-problemas com busca direta direcional 69\n4.4 Resolvendo os sub-problemas com regi\u00e3o de confian\u00e7a\n\nn\u00e3o-diferenci\u00e1vel . . . . . . . . . . . . . . . . . . . . . . 69\n4.5 Sum\u00e1rio . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n\n5 An\u00e1lise computacional 73\n\n\n\n5.1 Otimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave . . . . . . . . . . . . . . . . 73\n5.1.1 Busca direta direcional . . . . . . . . . . . . . . . 74\n5.1.2 Regi\u00e3o de confian\u00e7a . . . . . . . . . . . . . . . . 80\n\n5.2 Otimiza\u00e7\u00e3o baseada no simulador . . . . . . . . . . . . . 83\n5.3 Sum\u00e1rio . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n\n6 Conclus\u00e3o 91\n\nRefer\u00eancias 93\n\nA Sobre a diferenciabilidade do Lagrangiano aumentado 97\n\n\n\n1 INTRODU\u00c7\u00c3O\n\nEste trabalho busca aplicar t\u00e9cnicas de otimiza\u00e7\u00e3o n\u00e3o diferenci\u00e1-\nvel ao caso da produ\u00e7\u00e3o de petr\u00f3leo. Neste cap\u00edtulo introduziremos este\ncaso de uso, as t\u00e9cnicas de otimiza\u00e7\u00e3o e nossa motiva\u00e7\u00e3o em estudar\ntal aplica\u00e7\u00e3o.\n\nT\u00e9cnicas de otimiza\u00e7\u00e3o matem\u00e1tica podem ser bem empregadas\nnas mais diversas \u00e1reas. A ind\u00fastria do \u00f3leo e g\u00e1s tem algumas particu-\nlaridades que motivam ainda mais tal aplica\u00e7\u00e3o. O petr\u00f3leo \u00e9 a principal\nfonte de energia do mundo moderno e suas reservas, embora grandes,\ns\u00e3o finitas e ir\u00e3o exaurir [1]. Al\u00e9m disso, h\u00e1 uma demanda crescente\npor \u00f3leo [2], e seu pre\u00e7o, nos \u00faltimos 15 anos alcan\u00e7ou valores muito\nmaiores que os do restante do s\u00e9culo passado.\n\nA otimiza\u00e7\u00e3o pode ser aplicada tanto no sentido de sugerir meios\nde se extrair de forma lucrativa a maior quantidade de \u00f3leo poss\u00edvel,\nquanto na pr\u00f3pria efici\u00eancia dos meios de produ\u00e7\u00e3o.\n\nAlguns trabalhos procuram encontrar a produ\u00e7\u00e3o \u00f3tima de cam-\npos de produ\u00e7\u00e3o de petr\u00f3leo utilizando procedimentos heur\u00edsticos [3]\nou programa\u00e7\u00e3o inteira-mista linear ou n\u00e3o-linear [4\u20136]. Para tanto,\nestes constroem modelos complexos, a fim de caracterizar a produ\u00e7\u00e3o\ndo campo de uma forma que pode ser utilizada eficientemente por algo-\nritmos de otimiza\u00e7\u00e3o. Tais modelos, embora n\u00e3o sejam fenomenol\u00f3gicos,\nrequerem bom conhecimento do problema para ser constru\u00eddos. Al\u00e9m\ndisso, nem todos os tipos de modelo s\u00e3o igualmente bem sucedidos na\nresolu\u00e7\u00e3o dos problemas de otimiza\u00e7\u00e3o [1, 7].\n\nEsses modelos de otimiza\u00e7\u00e3o s\u00e3o frequentemente obtidos por\nmeio de dados de simula\u00e7\u00e3o num\u00e9rica. Simuladores s\u00e3o amplamente\nutilizados em processos da produ\u00e7\u00e3o de petr\u00f3leo. Modelos de reser-\nvat\u00f3rio e ferramentas de simula\u00e7\u00e3o de processos de extra\u00e7\u00e3o de pe-\ntr\u00f3leo s\u00e3o cada vez mais r\u00e1pidos e exatos [2]. Al\u00e9m disso, por serem\nparte importante na previs\u00e3o e gerenciamento da produ\u00e7\u00e3o, modelos\nde simula\u00e7\u00e3o s\u00e3o frequentemente sintonizados de forma a reproduzir de\nmaneira confi\u00e1vel o comportamento de reservat\u00f3rios, po\u00e7os e sistemas\nrelacionados.\n\nTamb\u00e9m \u00e9 poss\u00edvel tentar otimizar a produ\u00e7\u00e3o fazendo uso direto\ndos simuladores, sem a constru\u00e7\u00e3o dos modelos de otimiza\u00e7\u00e3o menci-\nonados anteriormente. Uma poss\u00edvel dificuldade \u00e9 que os simuladores\npodem fornecer previs\u00f5es de produ\u00e7\u00e3o com uma boa exatid\u00e3o, mas\ntipicamente n\u00e3o fornecem derivadas, que s\u00e3o necess\u00e1rias para se apli-\ncar t\u00e9cnicas cl\u00e1ssicas de otimiza\u00e7\u00e3o. Alguns algoritmos de otimiza\u00e7\u00e3o\nbuscam justamente calcular as derivadas, seja por diferen\u00e7as finitas ou\nm\u00e9todos probabil\u00edsticos, para utilizar m\u00e9todos cl\u00e1ssicos. De qualquer\n\n17\n\n\n\n18 Cap\u00edtulo 1. Introdu\u00e7\u00e3o\n\nforma, \u00e9 importante ter em vista que, embora os simuladores forne\u00e7am\nresultados com exatid\u00e3o satisfat\u00f3ria em suas vari\u00e1veis finais, os valores\ncalculados frequentemente s\u00e3o o resultado de um processo iterativo, que\n\u00e9 interrompido quando o erro esperado fica suficientemente pequeno.\nEsta \u00e9 uma fonte de ru\u00eddo, que pode acabar ampliado no c\u00e1lculo de\nderivadas.\n\nNesta disserta\u00e7\u00e3o abordamos outra classe de m\u00e9todos de otimi-\nza\u00e7\u00e3o, os n\u00e3o-diferenci\u00e1veis. Estes s\u00e3o desenvolvidos exatamente para\no caso em que n\u00e3o se disp\u00f5e das derivadas da fun\u00e7\u00e3o objetivo, assim\npodem fazer uso dos valores calculados pelas ferramentas de simula\u00e7\u00e3o\ndiretamente. Alguns desses m\u00e9todos s\u00e3o aplicados em [8, 9], tamb\u00e9m no\ncontexto de produ\u00e7\u00e3o de petr\u00f3leo. Sob certas condi\u00e7\u00f5es, espec\u00edficas de\ncada m\u00e9todo, \u00e9 poss\u00edvel garantir que se encontre um ponto \u00f3timo.\n\nNesta disserta\u00e7\u00e3o, apresentaremos duas classes de algoritmos de\notimiza\u00e7\u00e3o sem derivadas: busca direta direcional e regi\u00e3o de confian\u00e7a\nn\u00e3o-diferenci\u00e1vel. Os algoritmos ser\u00e3o aplicados a um cen\u00e1rio de pro-\ndu\u00e7\u00e3o de petr\u00f3leo, com po\u00e7os operando por gas-lift, processo descrito\na seguir.\n\nEm casos onde a press\u00e3o do reservat\u00f3rio \u00e9 suficiente para fazer\nos fluidos irem da forma\u00e7\u00e3o, atrav\u00e9s do po\u00e7o at\u00e9 a superf\u00edcie, o po\u00e7o \u00e9\ndito surgente. Quando isso n\u00e3o ocorre, mecanismos de eleva\u00e7\u00e3o artificial\ns\u00e3o usados. Um importante m\u00e9todo de eleva\u00e7\u00e3o \u00e9 o gas-lift cont\u00ednuo,\ncorrespondendo a 70% da produ\u00e7\u00e3o de petr\u00f3leo no Brasil [10].\n\nNeste m\u00e9todo, g\u00e1s a alta press\u00e3o \u00e9 injetado na coluna de produ-\n\u00e7\u00e3o [11]. Por ser mais leve que o petr\u00f3leo, tende a subir, mas tamb\u00e9m\nacaba gaseificando o conte\u00fado do tubing, que tamb\u00e9m se torna mais\nleve [12]. Deste modo, a press\u00e3o do reservat\u00f3rio, juntamente com a\ninje\u00e7\u00e3o de g\u00e1s, impulsiona os fluidos at\u00e9 a superf\u00edcie.\n\n1.1 ORGANIZA\u00c7\u00c3O DO DOCUMENTO\n\nEste documento foi organizado da seguinte maneira: no cap\u00edtulo\n2 introduzimos os algoritmos de busca direta direcional e regi\u00e3o de\nconfian\u00e7a n\u00e3o-diferenci\u00e1vel. Ambos para otimiza\u00e7\u00e3o irrestrita.\n\nNo cap\u00edtulo 3, os dois algoritmos s\u00e3o extendidos a fim de resolver\nproblemas de otimiza\u00e7\u00e3o com restri\u00e7\u00f5es lineares nas vari\u00e1veis. Em am-\nbos os casos, os m\u00e9todos buscam por descenso na fun\u00e7\u00e3o objetivo apenas\npor pontos vi\u00e1veis, isto \u00e9, satisfazendo \u00e0s restri\u00e7\u00f5es. Para a busca direta\ndirecional, isto \u00e9 feito gerando-se dire\u00e7\u00f5es de busca que n\u00e3o saem do\nconjunto vi\u00e1vel. Para a regi\u00e3o de confian\u00e7a, os sub-problemas de regi\u00e3o\nde confian\u00e7a s\u00e3o resolvidos na intersec\u00e7\u00e3o entre a regi\u00e3o de confian\u00e7a e\n\n\n\n1.1. Organiza\u00e7\u00e3o do documento 19\n\no conjunto vi\u00e1vel.\nNo cap\u00edtulo 4 propomos o uso do m\u00e9todo de Lagrangiano aumen-\n\ntado, para a resolu\u00e7\u00e3o de problemas de otimiza\u00e7\u00e3o com restri\u00e7\u00f5es n\u00e3o-li-\nneares nas vari\u00e1veis. Tal m\u00e9todo substitui as restri\u00e7\u00f5es n\u00e3o-lineares por\npenaliza\u00e7\u00f5es na fun\u00e7\u00e3o objetivo, gerando uma sequ\u00eancia de sub-probl-\nemas contendo apenas restri\u00e7\u00f5es lineares. Como podem ser implementa-\ndos sem a necessidade de derivadas da fun\u00e7\u00e3o objetivo, os algoritmos do\ncap\u00edtulo anterior podem ser usados a fim de resolver tais sub-problemas.\n\nNo cap\u00edtulo 5 fazemos uma an\u00e1lise computacional dos m\u00e9todos\npropostos no cap\u00edtulo 3. Os m\u00e9todos s\u00e3o aplicados para aloca\u00e7\u00e3o de\ng\u00e1s de gas-lift em um campo de po\u00e7os produtores de petr\u00f3leo. Num\nprimeiro caso, ambos os m\u00e9todos resolvem o problema da aloca\u00e7\u00e3o de\ng\u00e1s considerando as fun\u00e7\u00f5es de produ\u00e7\u00e3o dos po\u00e7os modeladas por fun-\n\u00e7\u00f5es suaves. Posteriormente, o m\u00e9todo da regi\u00e3o de confian\u00e7a \u00e9 utilizado\npara resolver um problema de aloca\u00e7\u00e3o de g\u00e1s utilizando diretamente\num simulador fenomenol\u00f3gico.\n\nFinalmente, no cap\u00edtulo 6 apresentamos nossas conclus\u00f5es.\n\n\n\n\n\n2 OTIMIZA\u00c7\u00c3O N\u00c3O-DIFERENCI\u00c1VEL IRRESTRITA\n\nNa \u00e1rea de produ\u00e7\u00e3o de petr\u00f3leo e g\u00e1s h\u00e1 uma grande variedade\nde ferramentas para a simula\u00e7\u00e3o de reservat\u00f3rios, po\u00e7os, dutos e dos\nv\u00e1rios sistemas que integram a produ\u00e7\u00e3o de petr\u00f3leo e g\u00e1s. No entanto,\nas derivadas dos valores preditos podem n\u00e3o estar dispon\u00edveis. Embora\nos simuladores sejam capazes de calcular numericamente suas previs\u00f5es,\na exatid\u00e3o com que o fazem pode n\u00e3o ser suficiente para que se proceda\ncom o c\u00e1lculo de derivadas. Ou ent\u00e3o, o custo de avaliar a fun\u00e7\u00e3o obje-\ntivo ? em um n\u00famero de pontos suficiente para calcular numericamente\nsua derivada pode ser muito alto.\n\nEste trabalho busca investigar a utiliza\u00e7\u00e3o de m\u00e9todos de otimi-\nza\u00e7\u00e3o que n\u00e3o necessitam das derivadas das fun\u00e7\u00f5es objetivos, podendo\nusar diretamente os valores calculados pelos simuladores. Para isto,\nintroduziremos neste cap\u00edtulo m\u00e9todos de otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel.\n\nQuando temos uma fun\u00e7\u00e3o ? : R? ? R e desejamos resolver um\nproblema do tipo\n\nmin\n??R?\n\n? (?)\n\ne as derivadas ?? e ?2? est\u00e3o dispon\u00edveis, podem ser empregadas\nt\u00e9cnicas cl\u00e1ssicas, como o m\u00e9todo de Newton, para encontrar um \u00f3timo\nlocal. Dizemos que ?* ? R? \u00e9 \u00f3timo local de primeira ordem (ou ponto\nestacion\u00e1rio de primeira ordem), se ?? (?*) = 0.\n\nQuando n\u00e3o dispomos da derivada primeira ?? , podem ser uti-\nlizados m\u00e9todos de otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel. S\u00e3o m\u00e9todos que, a\npartir de uma solu\u00e7\u00e3o aproximada, buscam melhorar o valor assumido\npela fun\u00e7\u00e3o objetivo. Sob certas condi\u00e7\u00f5es, \u00e9 poss\u00edvel garantir que alguns\ndesses m\u00e9todos encontram um ponto \u00f3timo de primeira ordem.\n\n2.1 M\u00c9TODOS DE BUSCA DIRETA DIRECIONAL\n\nUma primeira classe de m\u00e9todos que ser\u00e1 apresentada \u00e9 a busca\ndireta direcional. \u00c9 composta de algoritmos que amostram a fun\u00e7\u00e3o\nobjetivo um n\u00famero finito de vezes a cada itera\u00e7\u00e3o e n\u00e3o constroem\nqualquer modelo dela. As decis\u00f5es de evolu\u00e7\u00e3o do algoritmo s\u00e3o sem-\npre tomadas com base nos pr\u00f3prios valores amostrados. Alguns destes\npodem ser usados mesmo em fun\u00e7\u00f5es que n\u00e3o s\u00e3o num\u00e9ricas.\n\nS\u00e3o algoritmos simples, podem n\u00e3o ser muito eficientes, mas di-\nante de um problema novo, pode ser mais r\u00e1pido programar um algo-\nritmo de busca direta direcional para chegar \u00e0 solu\u00e7\u00e3o do que construir\num algoritmo novo, de converg\u00eancia mais r\u00e1pida, por\u00e9m mais complexo\nde se implementar.\n\n21\n\n\n\n22 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nH\u00e1 v\u00e1rias formas de se descrever os algoritmos dessa classe, cada\numa com suas particularidades de terminologia ou pequenas diferen\u00e7as\nalgor\u00edtmicas, mas sempre preservando uma mesma estrutura, sem a cons-\ntru\u00e7\u00e3o de modelos. Neste trabalho, procuramos expor tais algoritmos\nseguindo a descri\u00e7\u00e3o e nota\u00e7\u00e3o de [13].\n\nPara descrever o funcionamento desses algoritmos, utilizaremos\num problema de aloca\u00e7\u00e3o de g\u00e1s a 2 po\u00e7os. Consideraremos que a\nprodu\u00e7\u00e3o de cada po\u00e7o \u00e9 fun\u00e7\u00e3o do g\u00e1s injetado, conforme a express\u00e3o\n[14]:\n\n??(?inj) = ?1 + ?2?inj + ?3(?inj)2 + ?4 ln(?inj + 1).\n\nConsideramos que a vaz\u00e3o total produzida ?? \u00e9 composta de\nvaz\u00f5es de \u00f3leo ??, g\u00e1s ?? e \u00e1gua ??. A propor\u00e7\u00e3o entre g\u00e1s e \u00f3leo\nproduzidos \u00e9 chamada GOR (Gas-Oil Ratio), enquanto o Water Cut\n\u00e9 a propor\u00e7\u00e3o de \u00e1gua na fase l\u00edquida produzida (composta de \u00f3leo e\n\u00e1gua).\n\nConsiderando os v\u00e1rios po\u00e7os (neste caso ? = 2), representamos\nas seguintes grandezas:\n\n? ??inj \u00e9 a taxa de inje\u00e7\u00e3o de g\u00e1s no po\u00e7o ?.\n\n? ??? \u00e9 a vaz\u00e3o total produzida no po\u00e7o ?.\n\n? ??? \u00e9 a vaz\u00e3o de \u00f3leo produzido no po\u00e7o ?.\n\n? ??? \u00e9 a vaz\u00e3o de \u00e1gua produzida no po\u00e7o ?.\n\n? ??? \u00e9 a vaz\u00e3o de g\u00e1s produzido no po\u00e7o ?.\n\nConsiderando os pre\u00e7os obtidos com a venda do \u00f3leo ??, e do\ng\u00e1s ?? , e os custos do tratamento da \u00e1gua ?? e da inje\u00e7\u00e3o de g\u00e1s ??,\ndefinimos a fun\u00e7\u00e3o de ganho econ\u00f4mico correspondente como\n\n? (?inj) =\n? =2??\n?=1\n\n(?\n???\n\n?\n? (?\n\n?\ninj) + ?? ?\n\n?\n? (?\n\n?\ninj) ? ?????(??inj) ? ????inj\n\n)?\n.\n\nDesejamos maximizar uma fun\u00e7\u00e3o de ganho econ\u00f4mico, para ? = 2\npo\u00e7os, da forma\n\nmax\n?inj\n\n? (?inj). (2.1)\n\nPara este caso, consideramos o po\u00e7o 1 modelado por\n\n? ?1? (?1inj) = ?1080 ? 0,26?1inj + 398 ln(?1inj + 1)\n\n? GOR = 0,286\n\n\n\n2.1. Busca direta direcional 23\n\n? Water cut = 12,5 %\n\n? ?1? = 0,7?1?\n? ?1? = 0,2?1?\n? ?1? = 0,1?1? .\n\ne, para o po\u00e7o 2,\n\n? ?2? (?2inj) = ?1630 ? 0,37?2inj + 671 ln(?2inj + 1)\n\n? GOR = 0,750\n\n? Water cut = 42,9 %\n\n? ?2? = 0,4?2?\n? ?2? = 0,3?2?\n? ?2? = 0,3?2? .\n\nA busca direta direcional inicia de uma estimativa de solu\u00e7\u00e3o\ne, a cada itera\u00e7\u00e3o, avalia a fun\u00e7\u00e3o objetivo um n\u00famero finito de vezes\nem torno da solu\u00e7\u00e3o corrente. Na Figura 2.1 ilustramos o dom\u00ednio da\nfun\u00e7\u00e3o ? , com as curvas de n\u00edvel tra\u00e7adas. A estrela marca o ponto \u00f3timo,\nenquanto a estimativa de solu\u00e7\u00e3o \u00e9 o maior dos pontos escuros. A cada\nitera\u00e7\u00e3o, a fun\u00e7\u00e3o objetivo \u00e9 avaliada em quatro pontos, localizados\nh\u00e1 uma certa dist\u00e2ncia da solu\u00e7\u00e3o corrente. Neste caso, tais pontos\nforam calculados utilizando a estimativa de solu\u00e7\u00e3o inicial (?1inj, ?2inj)0\ne somando-se, +? e ?? em cada vari\u00e1vel, a fim de se obter os quatro\npontos de teste, onde a fun\u00e7\u00e3o objetivo \u00e9 avaliada. Na figura, estes\npontos ficam nas pontas dos tra\u00e7os pretos. O ponto que apresenta o\nmelhor valor passa a ser a nova estimativa de solu\u00e7\u00e3o (neste caso, o\nponto acima).\n\nEste procedimento \u00e9 repetido, ent\u00e3o, em torno da nova solu\u00e7\u00e3o,\nconforme ilustrado na Figura 2.2, que \u00e9 a continua\u00e7\u00e3o da anterior.\n\nNa parte (a) da figura, entre todos os pontos considerados, aquele\n\u00e0 direita apresentou o melhor valor da fun\u00e7\u00e3o objetivo, passando a ser\nconsiderado a nova estimativa de solu\u00e7\u00e3o, conforme ilustrado em (b).\n\nEm (b) ocorre que nenhum dos pontos de teste apresenta um\nvalor para a fun\u00e7\u00e3o objetivo melhor que a solu\u00e7\u00e3o corrente. Nesse caso,\no iterando \u00e9 mantido no mesmo ponto, e a busca por descenso continua\nem uma regi\u00e3o mais pr\u00f3xima, reduzindo-se o comprimento do passo ?.\nEssa redu\u00e7\u00e3o \u00e9 ilustrada no quadro seguinte.\n\n\n\n24 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\n500 1.000 1.500 2.000\n500\n\n1.000\n\n1.500\n\n2.000\n\n? (?) = 1.641 ? (?) = 1.669\n\n? (?) = 1.670\n\n? (?) = 1.584\n\n? (?) = 1.581\n\n?1inj (inje\u00e7\u00e3o no po\u00e7o 1)\n\n?\n2 in\n\nj\n(i\n\nnj\ne\u00e7\n\n\u00e3o\nno\n\npo\n\u00e7o\n\n2)\n\nFigura 2.1: Busca direta direcional: a fun\u00e7\u00e3o objetivo \u00e9 avaliada em\nquatro pontos em torno da estimativa de solu\u00e7\u00e3o\n\nEm (c), \u00e9 encontrado um ponto melhor \u00e0 direita. Em (d), o\nmelhor ponto \u00e9 acima. No quadro (e), novamente, nenhum dos pontos\nde teste apresenta um valor melhor que a solu\u00e7\u00e3o corrente, e novamente\no comprimento do passo \u00e9 reduzido, enquanto mant\u00e9m-se a mesma\nsolu\u00e7\u00e3o (f).\n\nNeste cap\u00edtulo procuraremos formalizar e generalizar esse m\u00e9todo\nde solu\u00e7\u00e3o e discutir sobre suas possibilidades de sucesso.\n\nPara garantir que esses m\u00e9todos cheguem a um ponto \u00f3timo, os\nalgoritmos cumprem as seguintes etapas:\n\n1. Buscar uma dire\u00e7\u00e3o de descenso.\n\n2. Manter uma boa geometria das dire\u00e7\u00f5es de busca (em um sentido\nformal).\n\n3. Garantir que a regi\u00e3o de busca diminua tanto quanto desejado.\nIsto \u00e9, exige-se que as regi\u00f5es onde se busca por descenso decres-\n\u00e7am, no limite.\n\n\n\n2.1. Busca direta direcional 25\n\n(a) (b)\n\n(c) (d)\n\n(e) (f)\n\nFigura 2.2: Busca direta direcional na otimiza\u00e7\u00e3o da produ\u00e7\u00e3o em dois\npo\u00e7os.\n\n\n\n26 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nO primeiro ponto \u00e9 encontrar uma dire\u00e7\u00e3o de descenso, isto \u00e9,\numa dire\u00e7\u00e3o na qual a fun\u00e7\u00e3o objetivo ? decresce, a partir da estimativa\nde solu\u00e7\u00e3o corrente ?. M\u00e9todos convencionais de otimiza\u00e7\u00e3o escolhem\na dire\u00e7\u00e3o baseados no gradiente ?? (?). Por exemplo, o m\u00e9todo do\nm\u00e1ximo descenso emprega ??? (?). No entanto, quando n\u00e3o se tem\nacesso \u00e0 derivada, \u00e9 necess\u00e1rio fazer outra escolha.\n\nSuponha que a fun\u00e7\u00e3o ? \u00e9 continuamente diferenci\u00e1vel. Ent\u00e3o,\nqualquer vetor ? ? R? que forma um \u00e2ngulo agudo com ??? (?) \u00e9 uma\ndire\u00e7\u00e3o de descenso para ? a partir de ?. Isto \u00e9, se\n\n??? (?)? ? > 0 (2.2)\n\n? \u00e9 uma dire\u00e7\u00e3o em que, a partir de ?, ? decresce ao menos por uma\ndist\u00e2ncia (possivelmente curta).\n\nM\u00e9todos de busca direta direcional buscam em um conjunto de\ndire\u00e7\u00f5es ? = {?1, ?2, . . .}? R?, em que, para todo vetor ? ? R?, ? ?= 0,\nexiste um ?? ?? de modo que\n\n?? ?? > 0. (2.3)\n\nEm particular, para ? = ??? (?) ?= 0, existe um ?? que satizfaz (2.2).\nEnt\u00e3o, uma das dire\u00e7\u00f5es de ? \u00e9 dire\u00e7\u00e3o de descenso, ainda que n\u00e3o se\nsaiba, de antem\u00e3o, qual.\n\nResta mostrar que \u00e9 poss\u00edvel construir um conjunto ? conforme\nmencionado, com um n\u00famero finito de elementos. Em [15], \u00e9 demons-\ntrado o seguinte teorema:\n\nTeorema 1 O conjunto ? = {?1, ?2, . . . , ??} gera R? por meio de\ncombina\u00e7\u00f5es lineares de coeficientes n\u00e3o-negativos se, e somente se,\npara todo vetor n\u00e3o-nulo ? ? R?, existe um ? ?? tal que\n\n?? ? > 0.\n\nPelo fato desses conjuntos gerarem R? por meio de combina\u00e7\u00f5es n\u00e3o-\nnegativas (combina\u00e7\u00f5es c\u00f4nicas), eles tamb\u00e9m podem ser chamados\ngeradores positivos de R?. Por isso a classe de algoritmos apresentada\nnesta se\u00e7\u00e3o \u00e9 chamada em [13] de \u201cGenerating Set Search\u201d (busca [por\nmeio de] conjuntos geradores [positivos]). Alguns exemplos de conjuntos\ngeradores positivos est\u00e3o na Figura 2.3.\n\nTamb\u00e9m de [15], o seguinte teorema \u00e9 apresentado:\n\nTeorema 2 Suponha que ? = {?1, ?2, . . . , ??}, gera R? por meio de\ncombina\u00e7\u00f5es c\u00f4nicas, ent\u00e3o o n\u00famero de vetores ? ? ? + 1, e \u00e9 poss\u00edvel\n? = ? + 1.\n\n\n\n2.1. Busca direta direcional 27\n\nFigura 2.3: Tr\u00eas exemplos de conjuntos de vetores geradores positivos\ndo R2.\n\nPara construir um tal conjunto, suponha que {?1, ?2, . . . , ??} \u00e9\numa base de R?. Ent\u00e3o, para quaisquer ?? > 0, definindo\n\n??+1 = ??1?1 ? ?2?2 ? . . . ? ???? (2.4)\n\no conjunto {?1, ?2, . . . , ??, ??+1} \u00e9 gerador positivo dos R?. Este con-\njunto tamb\u00e9m \u00e9 chamado de base positiva m\u00ednima.\n\nOutra possibilidade para construir ? a partir de uma base de\nR? para combina\u00e7\u00f5es lineares {?1, . . . , ??} \u00e9 unir tais vetores aos seus\nnegativos:\n\n? = {?1, . . . , ??}?{??1, . . . ,???}. (2.5)\nEsta \u00e9 chamada de base positiva m\u00e1xima.\n\nEm algoritmos de busca direta direcional, para cada iterando\n?? com fun\u00e7\u00e3o objetivo ? (??), \u00e9 constru\u00eddo um conjunto ?? gerador\npositivo de R?, no qual se busca descenso para ? .\n\nPor\u00e9m, n\u00e3o basta o conjunto ?? ser gerador positivo de R?. \u00c9\nnecess\u00e1rio garantir que todos os conjuntos usados tenham uma boa\ngeometria, no sentido da seguinte defini\u00e7\u00e3o:\n\nDefini\u00e7\u00e3o 1 Definimos a medida cosseno de um conjunto ? ? R?\ncomo\n\n? (?) = min\n??R?\n? ?=0\n\nmax\n???\n\n?? ?\n\n??????\n\nIsto \u00e9, ? (?) \u00e9 o cosseno do maior \u00e2ngulo interno formado entre um vetor\nn\u00e3o-nulo ? ? R? e o vetor ? ? ? mais pr\u00f3ximo de ?. Nos algoritmos\nde busca direta direcional, trata-se de uma medida de qu\u00e3o distante\n??? (?) pode estar de ?.\n\nUma das formas de manter a boa geometria das dire\u00e7\u00f5es de busca\n\u00e9 impor uma medida cosseno m\u00ednima, um ?min > 0 de modo que:\n\n? (??) ? ?min.\n\n\n\n28 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nQualquer conjunto gerador positivo ? tem ? (?) > 0, por con-\nsequ\u00eancia de (2.3). O conjunto formado pelas dire\u00e7\u00f5es coordenadas e\nseus respectivos opostos\n\n{?1, ?2, . . . , ???1, ??}?{??1,??2, . . . ,????1,???}\n\u00e9 um gerador positivo do R?, com medida cosseno 1/\n\n?\n?.\n\nUm exemplo de busca direta direcional \u00e9 apresentado no Algo-\nritmo 2.1, baseado em [13].\n\nO algoritmo permite que se utilize, a cada itera\u00e7\u00e3o, um novo\nconjunto ?? de dire\u00e7\u00f5es de busca, o qual \u00e9 formado por um conjunto\n??, gerador positivo dos R?, e um outro conjunto ??. Este \u00faltimo\ncont\u00e9m dire\u00e7\u00f5es de busca adicionais, permitindo-se a implementa\u00e7\u00e3o\nde outros meios de busca por descenso durante as itera\u00e7\u00f5es. Por\u00e9m, h\u00e1\nlimites nos comprimentos das dire\u00e7\u00f5es:\n\n?min ????? ?max ?? ???, (2.6)\n?min ???? ?? ???. (2.7)\n\nOs valores de ?min ? ?max s\u00e3o arbitr\u00e1rios, mas \u00e9 importante que sejam\nfixos durante toda a execu\u00e7\u00e3o do algoritmo, de modo que o comprimento\nfinal do passo seja determinado por ?? (???min ?????????max).\n\nA itera\u00e7\u00e3o ? \u00e9 dita bem-sucedida (? ??) se existe uma dire\u00e7\u00e3o\n? ? ?? tal que\n\n? (?? + ???) &lt;? (??) ? ?(??)\nem que a fun\u00e7\u00e3o for\u00e7ante ? : R?{0}? R?{0} \u00e9 n\u00e3o-decrescente e\n\nlim\n??0+\n\n?(?)\n?\n\n= 0. (2.8)\n\nPode ser, por exemplo, qualquer fun\u00e7\u00e3o da forma ?(?) = ???, em que\n? > 0 e ? > 1, constantes.\n\nCaso n\u00e3o exista uma tal dire\u00e7\u00e3o, a itera\u00e7\u00e3o ? \u00e9 dita mal-sucedida\n(? ??).\n\nO tamanho do passo muda ao longo das itera\u00e7\u00f5es. Se n\u00e3o for\nencontrado descenso ele \u00e9 reduzido, caso contr\u00e1rio, \u00e9 aumentado, de\nacordo com as rela\u00e7\u00f5es:\n\n??+1 =\n{?\n\n????, se a itera\u00e7\u00e3o ? foi bem-sucedida\n????, se a itera\u00e7\u00e3o ? foi mal-sucedida\n\nem que o par\u00e2metro de contra\u00e7\u00e3o do passo ?? \u00e9 tal que 0 &lt;?? ? ?max&lt;\n1 e o de expans\u00e3o do passo ?? ? 1.\n\nTendo o algoritmo bem definido, podemos apresentar um primeiro\nresultado sobre sua converg\u00eancia [13]:\n\n\n\n2.1. Busca direta direcional 29\n\nAlgoritmo 2.1: Um exemplo de busca direta direcional, de [13]\nSeja ? : R? ? R dada;\nSeja ?0 ? R? um ponto inicial;\nDefina\n\n???? > 0 a toler\u00e2ncia de converg\u00eancia;\n?0 > ???? um tamanho de passo inicial;\n?max ? 1 um limite superior para o coeficiente de expans\u00e3o\n??;\n?0 ? [1, ?max] o valor inicial do coeficiente de expans\u00e3o do\npasso;\n?max &lt;1 um limite superior para o par\u00e2metro de contra\u00e7\u00e3o\n??;\n?0 ? (0, ?max] o valor inicial do coeficiente de contra\u00e7\u00e3o do\npasso;\n?max ? ?min > 0 limites superior e inferior, respectivamente,\npara os comprimentos dos vetores de qualquer conjunto\ngerador positivo;\n?min > 0 um limitante superior para a medida cosseno dos\nconjuntos geradores positivos;\n? : R+ ? R+ uma fun\u00e7\u00e3o n\u00e3o-decrescente satisfazendo a\nequa\u00e7\u00e3o (2.8);\n\nfor ? = 0, 1, . . . do\nDefina ?? conjunto gerador positivo para R? satisfazendo\n?min ????? ?max, ?? ???, com ? (??) ? ?min;\nif Existe ? ??? tal que ? (?? + ???) &lt;? (??) ? ?(??) then\n\n??+1 ? ?? + ???;\n??+1 ? ????;\n\nelse\n??+1 ? ??;\n??+1 ? ???? ;\nif ??+1 &lt;???? then\n\nEncerrar;\n\nDetermine ??+1 ? (0, ?max] e ??+1 ? [1, ?max];\n\n\n\n30 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nTeorema 3 Seja ? : R? ? R, com ?? Lipschitziana com constante\nde Lipschitz M. O Algoritmo 2.1 produz iterandos tais que, para toda\nitera\u00e7\u00e3o mal-sucedida ? ??\n\n??? (??)??\n1\n\n? (??)\n\n[?\n? ???max +\n\n?(??)\n???min\n\n]?\n. (2.9)\n\nAssim, obtemos uma cota superior (ainda que desconhecida) para\n??? (??)?, mesmo sem calcularmos tal valor em nenhum momento.\nAinda, como ?min limita os valores de ? (??), o termo 1/? (??) \u00e9 limi-\ntado superiormente ao longo das itera\u00e7\u00f5es. Al\u00e9m disso, como os valores\nde ? , ?max e ?min s\u00e3o fixos, e considerando (2.8), se o tamanho do\npasso for reduzido indefinidamente, isto \u00e9, se\n\nlim\n??0\n???\n\n?? = 0\n\nent\u00e3o\nlim\n??0\n???\n??? (??)? = 0.\n\nJ\u00e1 foram abordados a maneira de encontrar uma dire\u00e7\u00e3o de\ndescenso (por meio de conjuntos geradores positivos) e o controle da\ngeometria das dire\u00e7\u00f5es de busca (por meio da medida cosseno). O\nponto faltante \u00e9 garantir que a dire\u00e7\u00e3o de busca decres\u00e7a. Fazendo-se\nlim??? ?? = 0, o Teorema 3 garante converg\u00eancia para um ponto\nestacion\u00e1rio de primeira ordem. Isto \u00e9, um ponto ?* em que\n\n?? (?*) = 0.\n\nIsso \u00e9 enunciado no teorema seguinte:\n\nTeorema 4 Seja ? limitada inferiormente. Suponha a fun\u00e7\u00e3o ? como\ndada em (2.8). Ent\u00e3o, o Algoritmo 2.1 produz itera\u00e7\u00f5es de modo que\n\nlim inf\n??+?\n\n?? = 0.\n\nA demonstra\u00e7\u00e3o pode ser vista em [13].\n\n2.2 M\u00c9TODOS DE REGI\u00c3O DE CONFIAN\u00c7A\n\nM\u00e9todos de regi\u00e3o de confian\u00e7a partem de uma aproxima\u00e7\u00e3o\npara a solu\u00e7\u00e3o e utilizam os valores j\u00e1 calculados da fun\u00e7\u00e3o objetivo\npara construir modelos. Estes buscam aproximar ? localmente, em uma\nvizinhan\u00e7a do ponto atual chamada regi\u00e3o de confian\u00e7a.\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 31\n\nO modelo constru\u00eddo, espera-se, \u00e9 mais f\u00e1cil de otimizar que a\nfun\u00e7\u00e3o original. Por um lado, porque se disp\u00f5e de t\u00e9cnicas cl\u00e1ssicas de\notimiza\u00e7\u00e3o com derivadas. Por outro, porque a avalia\u00e7\u00e3o do modelo\npode ser muito mais r\u00e1pida que a fun\u00e7\u00e3o ? original.\n\nConstru\u00eddo um bom modelo, ele \u00e9 otimizado dentro da regi\u00e3o de\nconfian\u00e7a, gerando um ponto ?+? , para ent\u00e3o se calcular ? (?\n\n+\n? ). Assim,\n\n\u00e9 poss\u00edvel economizar no n\u00famero de vezes que se avalia ? . O que \u00e9 uma\nvantagem com rela\u00e7\u00e3o aos m\u00e9todos de busca direta quando, por exemplo,\ncada c\u00e1lculo de ? \u00e9 uma simula\u00e7\u00e3o que pode durar v\u00e1rios minutos. Por\noutro lado, m\u00e9todos de regi\u00e3o de confian\u00e7a s\u00e3o mais complexos de se\nimplementar.\n\nEntre os v\u00e1rios m\u00e9todos de regi\u00e3o de confian\u00e7a, h\u00e1 diferen\u00e7as\ncom rela\u00e7\u00e3o ao tipo de modelo, forma da regi\u00e3o de confian\u00e7a, maneiras\nde manter ou melhorar o modelo, aceita\u00e7\u00e3o do passo e gerenciamento\nda regi\u00e3o de confian\u00e7a.\n\nNesta se\u00e7\u00e3o, apresentamos m\u00e9todos de regi\u00e3o de confian\u00e7a n\u00e3o-\ndiferenci\u00e1veis, em grande parte, conforme a descri\u00e7\u00e3o e nomenclatura\nde [16]. Neste livro h\u00e1 v\u00e1rias explica\u00e7\u00f5es abrangentes para o uso de\nmodelos polinomiais de regress\u00e3o, m\u00ednima norma ou modelos n\u00e3o poli-\nnomiais. Por\u00e9m, nesta disserta\u00e7\u00e3o, vamos abordar apenas modelos de\ninterpola\u00e7\u00e3o por polin\u00f4mios de at\u00e9 segunda ordem, da forma\n\n??(?? + ?) = ??(??) + ??? ? +\n1\n2\n\n?? ???\n\nque aproximam a fun\u00e7\u00e3o objetivo ? na regi\u00e3o de confian\u00e7a dada por\numa bola de tamanho ?? em torno do iterando ??:\n\n?(??, ??) = {? : ?? ? ???? ??}\n\nonde a norma ?\u00b7? pode se tratar da norma 2 ou norma infinito, por\nexemplo.\n\nPara introduzir os m\u00e9todos de regi\u00e3o de confian\u00e7a n\u00e3o-diferenci-\n\u00e1veis, iniciamos com alguns conceitos b\u00e1sicos.\n\n2.2.1 Defini\u00e7\u00e3o do modelo\n\nEntre os v\u00e1rios algoritmos de otimiza\u00e7\u00e3o que usam regi\u00e3o de\nconfian\u00e7a, h\u00e1 uma grande diferen\u00e7a com rela\u00e7\u00e3o ao tipo de modelo\nusado. Nesta se\u00e7\u00e3o, introduzimos o modelo que usamos no trabalho, de\ninterpola\u00e7\u00e3o com polin\u00f4mios de segunda ordem.\n\nPara modelos de interpola\u00e7\u00e3o, definimos um conjunto de pontos\n? = {?0, ?1, . . . , ??} onde o modelo constru\u00eddo coincide com a fun\u00e7\u00e3o\n\n\n\n32 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nobjetivo, isto \u00e9,\n?(??) = ? (??) ??? ??. (2.10)\n\n\u00c9 necess\u00e1rio tamb\u00e9m fixar uma base ? = {?1, . . . , ??} que re-\npresente o espa\u00e7o dos polin\u00f4mios de segunda ordem no R?. Ent\u00e3o, as\ncombina\u00e7\u00f5es lineares dos seus elementos ?1, . . . , ?? geram os polin\u00f4mios,\ne o modelo ? pode ser escrito como:\n\n?(?) =\n???\n\n?=1\n????(?). (2.11)\n\nem que ?? ? ?.\nEnt\u00e3o, fixado um conjunto de pontos ?, para se determinar o\n\nmodelo polinomial de interpola\u00e7\u00e3o, a partir desta \u00faltima equa\u00e7\u00e3o, jun-\ntamente com as ? + 1 equa\u00e7\u00f5es (2.10), tem-se o sistema de equa\u00e7\u00f5es\nlineares:\n\n? (?,?)?? = ? (?) (2.12)\n\nem que\n\n? (?,?) =\n\n?\n????\n\n?0(?0) ?1(?0) \u00b7 \u00b7 \u00b7 ??(?0)\n?0(?1) ?1(?1) \u00b7 \u00b7 \u00b7 ??(?1)\n\n...\n...\n\n. . .\n...\n\n?0(??) ?1(??) \u00b7 \u00b7 \u00b7 ??(??)\n\n?\n????\n\ne\n\n? (?) =\n\n?\n????\n\n? (?0)\n? (?1)\n\n...\n? (??)\n\n?\n???? .\n\nPara o modelo ser calculado, basta que a matriz ? (?,?) seja\ninvert\u00edvel. Isto depende apenas do conjunto de pontos ?, e n\u00e3o da base.\nQuando isso ocorre, dizemos que ?, ou respectivamente o modelo ? nele\nbaseado \u00e9 bem posicionado para interpola\u00e7\u00e3o. A seguir apresentaremos\num algoritmo para o c\u00e1lculo deste conjunto, baseado na elimina\u00e7\u00e3o\nGaussiana.\n\nPor outro lado, o bom condicionamento do sistema (2.12) depende\nde ? e tamb\u00e9m da base. Este \u00e9 um ponto que interfere tanto na exatid\u00e3o\ncom que se resolve o sistema, como na estimativa dos erros ??(?)?? (?)?\nentre o modelo e a fun\u00e7\u00e3o objetivo.\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 33\n\nNeste trabalho, fixamos o uso da base de mon\u00f4mios, que chama-\nmos base natural e denotamos ?:\n\n? = {?0, . . . , ??}\n=\n{?\n\n1, ?1, ?2, . . . , ??,\n\n?21\n2\n\n, ?1?2,\n?22\n2\n\n, ?1?3, ?2?3,\n?23\n2\n\n, . . . ,\n?2?\n2\n\n}?\n.\n\nPara proceder com o c\u00e1lculo de um conjunto ? de pontos bem\nposicionados para interpola\u00e7\u00e3o, apresentamos o Algoritmo 2.2, de [16].\nO algoritmo procede de forma semelhante \u00e0 elimina\u00e7\u00e3o gaussiana, a-\nplicada \u00e0 matriz ? (?,?). Inicia com um conjunto de pontos ?, pos-\nsivelmente incompleto ou mal-posicionado, e com um conjunto de po-\nlin\u00f4mios, chamados polin\u00f4mios piv\u00f4. O la\u00e7o mais externo consiste em\nencontrar, entre os pontos dispon\u00edveis, aquele em que o polin\u00f4mio piv\u00f4\ncorrente assume maior valor absoluto. Na elimina\u00e7\u00e3o gaussiana, equivale\na encontrar para cada coluna de ? , o maior valor de elemento piv\u00f4. A\ndiferen\u00e7a \u00e9 que, quando um piv\u00f4 \u00e9 zero, ou quando em ? n\u00e3o h\u00e1 outros\npontos, um novo ponto \u00e9 calculado, substituindo o primeiro em ?.\n\nO segundo passo do algoritmo consiste em realizar a elimina\u00e7\u00e3o\npropriamente dita, operando com os polin\u00f4mios. Para cada um dos\npolin\u00f4mios seguintes, se subtrai o polin\u00f4mio piv\u00f4 corrente, multiplicado\npelo fator adequado. O resultado equivalente, na matriz dos polin\u00f4mios\npiv\u00f4 ? (?,?), \u00e9 que os elementos abaixo do piv\u00f4 s\u00e3o zerados.\n\nAo final deste procedimento, tem-se um conjunto de polin\u00f4mios\npiv\u00f4, e uma respectiva matriz ? (?,?) n\u00e3o-singular, o que garante que\no conjunto de pontos ? \u00e9 bem posicionado (note que, no primeiro\npasso, garantimos que todos os elementos-piv\u00f4 da elimina\u00e7\u00e3o gaussiana\nfossem n\u00e3o-nulos). Para a constru\u00e7\u00e3o do modelo, pode-se resolver o\nsistema triangular ? (?,?) = ? (?) com retro-substitui\u00e7\u00e3o, gerando os\ncoeficientes que multiplicam os polin\u00f4mios-piv\u00f4 para construir o modelo.\n\nPor\u00e9m, n\u00e3o basta que seja poss\u00edvel calcular um modelo de in-\nterpola\u00e7\u00e3o. \u00c9 necess\u00e1rio que o sistema resultante ? (?,?) seja bem-\ncondicionado. A maneira de limitar o n\u00famero de condicionamento de\n? \u00e9 feita de maneira semelhante ao algoritmo anterior. No entanto, o\ncrit\u00e9rio para descartar (recalcular) um ponto durante a elimina\u00e7\u00e3o gaus-\nsiana em ? \u00e9 mais forte. Exige-se que todos os elementos-piv\u00f4 tenham\nm\u00f3dulo maior que um valor ?, fixado. O procedimento \u00e9 ilustrado no\nAlgoritmo 2.3, de [16].\n\nCom este algoritmo, todos os piv\u00f4s da matriz ? s\u00e3o, garantida-\nmente, maiores que o limiar ? escolhido. Se necess\u00e1rio, o respectivo ponto\n\u00e9 trocado para que isso ocorra. Para tanto, a otimiza\u00e7\u00e3o que ocorre no\n\n\n\n34 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nprimeiro passo n\u00e3o \u00e9 estritamente necess\u00e1ria. Basta que se encontre um\nponto ? ? ? que satisfaz |??(?)| ? ?. A maneira de encontr\u00e1-lo ser\u00e1\nmostrada adiante.\n\nSegundo [16], as estimativas de erros podem ser relacionadas com\no n\u00famero de condicionamento da matriz ? , aplicada a um conjunto de\npontos modificado para uma regi\u00e3o de confian\u00e7a dada por uma bola de\nraio 1 centrada na origem:\n\n? = {0, ?1, . . . , ??}\n= {0, (?1 ? ?0)/?(?), . . . , (?? ? ?0)/?(?)}??(0, 1)\n\nem que ?(?) \u00e9 o fator de escala que faz com que todos os ele-\nmentos caibam (exatamente) em uma bola de raio 1:\n\n?(?) = max\n1????\n\n??? ? ?0?.\n\nde modo que o uso do conjunto ? limita a norma\n\n1 ????? (? + 1) 32 .\n\nO que motiva o uso do algoritmo neste conjunto ?. Por outro lado,\na decomposi\u00e7\u00e3o LU feita pelo algoritmo, prov\u00ea uma cota superior para\n?? ?1?. Considere a decomposi\u00e7\u00e3o dada, com ambas as matrizes L e U\ncom n\u00fameros 1 na diagonal: ? = LDU (? ?1 = U?1D?1L?1). O limiar\npara os piv\u00f4s d\u00e1 uma cota superior para a norma ?D?1??\n\n?\n? + 1/?,\n\nde modo que\n\n?? ?1? = ?D?1??L?1??U?1??\n?\n\n? + 1?L?1??U?1?\n?\n\n.\n\nO valor ?growth = ?L?1??U?1?, chamado fator de crescimento da fato-\nra\u00e7\u00e3o pode ser muito grande, mas \u00e9 limitado. A escolha do maior valor\nposs\u00edvel para os piv\u00f4s pode ajudar a diminuir esse termo.\n\nAssim, o n\u00famero de condicionamento de cond(? (?,?)) \u00e9 limi-\ntado.\n\n2.2.2 Polin\u00f4mios de Lagrange\n\nDado um conjunto de ? + 1 pontos ? = {?0, ?1, . . . , ??}, de-\nfinimos os polin\u00f4mios de Lagrange correspondentes como polin\u00f4mios\n?? : R? ? R, em que ? = 1, . . . , ?, e\n\n?? (??) =\n{?\n\n1 se ? = ?\n0 se ? ?= ? (2.13)\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 35\n\nAlgoritmo 2.2: Completando o conjunto de pontos de interpola-\n\u00e7\u00e3o\n\nEscolha uma aproxima\u00e7\u00e3o inicial para os polin\u00f4mios-piv\u00f4 ??,\npossivelmente usando a base: ??(?) = ??(?). Considere o conjunto\ninicial ? = {?0, ?1, . . . , ?????}, com ???? + 1 pontos;\nfor ? = 0, 1, . . . do\n\nEncontre ?? = argmax???????? |??(?\n? )|. Se |??(??? )| > 0 e\n\n? ? ????, troque os pontos ?? e ??? no conjunto ?. Sen\u00e3o,\ncalcule (ou recalcule, caso ? ? ????) como\n\n?? ? argmax\n???\n\n|??(?)|\n\nfor ? = ? + 1, . . . , ? do\n\n?? (?) ? ?? (?) ?\n?? (??)\n??(??)\n\n??(?)\n\nAlgoritmo 2.3: Melhorando o modelo e o conjunto de pontos\nEscolha uma aproxima\u00e7\u00e3o inicial para os polin\u00f4mios-piv\u00f4 ??,\npossivelmente usando a base: ??(?) = ??(?) para ? = 0, . . . , ?.\nEscolha um limiar ? > 0 para a sele\u00e7\u00e3o de elementos-piv\u00f4.\nConsidere o conjunto inicial ? bem-posicionado com ? + 1\npontos;\nfor ? = 0, 1, . . . , ? do\n\nEncontre se poss\u00edvel, ?? ?{?, . . . , ?} tal que |??(??? )| ? ?. Se\ntal ?? for encontrado, troque os pontos ?? e ??? de lugar em ?.\nCaso contr\u00e1rio, recalcule ?? como\n\n?? ? argmax\n???\n\n|??(?)|.\n\ninterrompa caso |??(??)| &lt;? (O limiar ? \u00e9 muito grande).\nfor ? = ? + 1, . . . , ? do\n\n?? (?) ? ?? (?) ?\n?? (??)\n??(??)\n\n??(?)\n\n\n\n36 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nIsto \u00e9, para todo \u00edndice ?, ?? \u00e9 um polin\u00f4mio que se anula em todos os\npontos de ?, exceto por ?? , com ?? (?? ) = 1. Al\u00e9m disso, tal polin\u00f4mio \u00e9\n\u00fanico, e independente de base, contanto que a matriz ? de interpola\u00e7\u00e3o\nseja n\u00e3o-singular.\n\nAssim, o modelo de interpola\u00e7\u00e3o para a fun\u00e7\u00e3o objetivo ? \u00e9:\n\n?(?) =\n???\n\n?=0\n??(?)? (??) (2.14)\n\nPara que os ? + 1 polin\u00f4mios de Lagrange estejam bem definidos,\n\u00e9 necess\u00e1rio que os pontos sejam bem posicionados; em caso contr\u00e1rio,\nn\u00e3o \u00e9 poss\u00edvel determinar ? + 1 polin\u00f4mios satisfazendo (2.13).\n\nPara chegar ao erro de interpola\u00e7\u00e3o, introduzimos a seguinte\ndefini\u00e7\u00e3o [16]:\n\nDefini\u00e7\u00e3o 2 Seja ? > 0 e um conjunto ? ? R? dado. Um conjunto\nbem posicionado ? = {?0, ?1, . . . , ??} \u00e9 dito ?-posicionado em ? se, e\nsomente se, para a base de polin\u00f4mios de Lagrange associados com ?,\ntem-se que\n\n? ? max\n?=0,...,?\n\nmax\n???\n|??(?)|\n\nEsta medida tamb\u00e9m estabelece cotas superiores para os erros de\npredi\u00e7\u00e3o tanto do modelo ?? ? ?? como de sua derivada ??? ????\n[17]:\n\nTeorema 5 Dada uma bola ?(?, ?) e um conjunto de pontos de inter-\npola\u00e7\u00e3o ? ? ?(?, ?) bem posicionado, e seus polin\u00f4mios de Lagrange\ncorrespondentes {??(?)}??=0, existem constantes ??? > 0 e ??? > 0 tais\nque, para todo polin\u00f4mio da forma (2.14) de grau maior que um e todo\nponto ? ??(?, ?),\n\n?? (?) ? ?(?)?? ???\n???\n\n?=0\n??? ? ??2 |??(?)| (2.15)\n\ne\n??? (?) ???(?)?? ??? ??. (2.16)\n\nDessa forma, mantendo-se ? limitado, o erro de interpola\u00e7\u00e3o cai com\no raio da regi\u00e3o ?. Estes resultados motivam a buscar modelos de\ninterpola\u00e7\u00e3o polinomial constru\u00eddos sobre pontos ?-posicionados, para\num ? mantido constante.\n\nAlguns algoritmos constroem modelos ?-posicionados fazendo\nuso expl\u00edcito do valor de ?. N\u00e3o \u00e9 o caso do Algoritmo 2.3, que cons-\ntr\u00f3i modelos ?-posicionados com um valor de ? desconhecido, por\u00e9m\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 37\n\nlimitado e que depende do par\u00e2metro ?. Optamos por este algoritmo\npor ser mais f\u00e1cil de adaptar ao caso restrito.\n\n2.2.3 C\u00e1lculo do passo\n\nPara o c\u00e1lculo do passo, uma possibilidade \u00e9 tentar encontrar o\n\u00f3timo global na regi\u00e3o de confian\u00e7a, resolvendo o problema\n\nmin\n???(?? ,?? )\n\n??(?)\n\nIsto pode ser feito em casos particulares. Se a regi\u00e3o de confi-\nan\u00e7a for dada por uma bola na norma-? do tipo ?(??, ??) = {? :\n?? ? ????}, trata-se de um problema de otimiza\u00e7\u00e3o com restri\u00e7\u00f5es\nlineares. Se o modelo ?? for linear, trata-se de um problema de progra-\nma\u00e7\u00e3o linear:\n\nmin ??(?) = ??? ?\n? ? ?? ? ??\n? ? ?? ????.\n\nO mesmo poderia ser dito de bolas na norma-1.\nDe modo semelhante, se o modelo for quadr\u00e1tico, e a bola for\n\ndada na norma-2 o problema de minimiza\u00e7\u00e3o\n\nmin ??(? + ?) = ??(??) + ??? ? +\n1\n2\n\n?? ???\n\nsujeito a ??? &lt;??\npode ser resolvido eficientemente (em tempo polinomial), utilizando o\nalgoritmo de Mor\u00e9-Sorensen, o que torna o uso da norma-2 vantajoso.\nTal algoritmo n\u00e3o ser\u00e1 apresentado neste trabalho porque estamos in-\nteressados no caso de restri\u00e7\u00f5es lineares, que n\u00e3o podem ser tratadas,\nembora haja algumas heur\u00edsticas [18].\n\nA otimiza\u00e7\u00e3o de um modelo quadr\u00e1tico em uma bola dada por\nrestri\u00e7\u00f5es lineares torna-se um problema NP-dif\u00edcil se a derivada se-\ngunda ? tiver ao menos um auto-valor negativo [19]. E segundo [20],\nnesse caso \u00e9 dif\u00edcil at\u00e9 mesmo decidir se um determinado ponto \u00e9 m\u00ednimo\nlocal.\n\nDe qualquer forma, n\u00e3o \u00e9 necess\u00e1rio chegar ao \u00f3timo global na\nregi\u00e3o. Alguns algoritmos exigem que a minimiza\u00e7\u00e3o encontre ao menos\numa fra\u00e7\u00e3o do decr\u00e9scimo obtido pela otimiza\u00e7\u00e3o global. Uma alterna-\ntiva ainda menos exigente \u00e9 por meio do passo de Cauchy. Defina:\n\n??? = argmin\n??0\n\n?? ???? ??(?? ,?? )\n\n??(?? ? ???)\n\n\n\n38 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\no passo de Cauchy \u00e9 dado por\n\n??? = ???? ??.\n\nEnt\u00e3o, o novo ponto \u00e9 dado por ?+? = ?? + ?\n?\n? . Os algoritmos que\n\napresentaremos exigem que se encontre, a cada itera\u00e7\u00e3o, uma fra\u00e7\u00e3o do\npasso de Cauchy.\n\nDe acordo com [20], verifica-se experimentalmente uma conver-\ng\u00eancia linear do algoritmo quando, repetidamente, se encontra o passo\nde Cauchy a cada itera\u00e7\u00e3o. Em tal livro a otimiza\u00e7\u00e3o do modelo na\nregi\u00e3o de confian\u00e7a \u00e9 tratada com grande riqueza de detalhes, seja para\nse encontrar um bom ponto de teste ou para encontrar o \u00f3timo global.\n\n2.2.4 Aceita\u00e7\u00e3o do passo e gerenciamento da regi\u00e3o\n\nDa otimiza\u00e7\u00e3o aproximada do problema na regi\u00e3o de confian\u00e7a,\no valor do decr\u00e9scimo encontrado com o modelo ??(??)???(?+? ) ser\u00e1\nsempre positivo. Algumas decis\u00f5es do algoritmo s\u00e3o dadas com base no\ngrau de concord\u00e2ncia entre o decr\u00e9scimo real obtido e o esperado:\n\n?? =\n? (??) ? ? (?+? )\n\n??(??) ? ??(?+? )\n. (2.17)\n\nSe este valor for alto (?? ? ?1, ?1 > 0 fixo), o passo \u00e9 aceito, o\niterando \u00e9 atualizado (??+1 ? ?+? ), e a regi\u00e3o de confian\u00e7a \u00e9 deslocada\nde acordo, ficando em torno de ??+1.\n\nAinda, [16] sugere que se aceite o passo em casos mais modestos\npara o descenso encontrado. Ent\u00e3o, o passo tamb\u00e9m \u00e9 aceito se ?? ? ?0,\ncom ?0 fixado, contanto que o modelo que produziu tal passo seja\nsuficientemente preciso (?-posicionado). Al\u00e9m disso, ?1 ? ?0 ? 0, e o\nuso de ?0 = 0 \u00e9 encorajado por tais autores, possibilitando que se aceite\ndescenso simples, na esperan\u00e7a de se aproveitar todas as avalia\u00e7\u00f5es da\nfun\u00e7\u00e3o objetivo, que podem ser custosas.\n\nO outro uso para o coeficiente de concord\u00e2ncia ?? \u00e9 para o ge-\nrenciamento da regi\u00e3o de confian\u00e7a. Caso seja alto, ?? ? ?1, o raio da\nregi\u00e3o pode ser aumentado, por exemplo, ??+1 ? min{?inc??, ?max},\n?inc > 1, sendo ?max o valor m\u00e1ximo para o raio da regi\u00e3o.\n\nCaso contr\u00e1rio, a falha em prever adequadamente o decr\u00e9scimo\nda fun\u00e7\u00e3o pode se dever a dois motivos: ou o modelo n\u00e3o \u00e9 bom, ou o\nmodelo \u00e9 bom, mas numa vizinhan\u00e7a menor do iterando, enquanto o\nraio da regi\u00e3o de confian\u00e7a est\u00e1 muito grande. Assim, caso o modelo j\u00e1\nseja ?-posicionado, o erro deve-se a um raio muito grande, ent\u00e3o este \u00e9\nreduzido de um fator ?: ??+1 ? ???.\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 39\n\nPor outro lado, caso o modelo ainda n\u00e3o seja ?-posicionado, n\u00e3o\npodemos concluir que o raio foi grande demais. Neste caso, o modelo\ndeve ser melhorado (por exemplo, com o Algoritmo 2.3). O raio \u00e9 man-\ntido (??+1 ? ??), j\u00e1 que uma redu\u00e7\u00e3o indevida afetaria, no m\u00ednimo, a\ntaxa de converg\u00eancia do algoritmo (e as provas de converg\u00eancia tamb\u00e9m\ndependem deste ponto).\n\n2.2.5 Teste de criticidade\n\nQuando se detectar que os iterandos est\u00e3o se aproximando de\num ponto estacion\u00e1rio, o raio da regi\u00e3o de confian\u00e7a \u00e9 reduzido, e novos\nmodelos ?-posicionados s\u00e3o calculados, de modo a se ter menores erros\nentre o modelo e a fun\u00e7\u00e3o objetivo.\n\nPor outro lado, essa redu\u00e7\u00e3o do raio n\u00e3o pode ser excessiva, de\nmodo a n\u00e3o prejudicar o avan\u00e7o do algoritmo. Para garantir a conver-\ng\u00eancia, \u00e9 necess\u00e1rio fazer o raio da regi\u00e3o pr\u00f3ximo de alguma medida\nde estacionariedade, como a norma da derivada do modelo ????. Isso\ntamb\u00e9m motiva ter o raio da regi\u00e3o de confian\u00e7a como o crit\u00e9rio de\nparada usual dos algoritmos de regi\u00e3o de confian\u00e7a: o raio da regi\u00e3o\nconverge para zero, ou fica limitado acima de zero junto com a medida\n(????), que est\u00e1 relacionada a ??? (??)?.\n\nSe, ao in\u00edcio de uma itera\u00e7\u00e3o, ???? for pequeno o suficiente\n(???? &lt;??, para um valor fixo ??), executa-se o teste de criticidade,\nde modo a obter um modelo ?-posicionado em uma regi\u00e3o com um raio\nde, no m\u00e1ximo ?????, para um valor fixo ?.\n\nIsto \u00e9 feito iterativamente, j\u00e1 que o valor de ?? depende do modelo\n??, que por sua vez depende da regi\u00e3o de confian\u00e7a, cujo raio ser\u00e1 (no\nfinal) limitado por ?????. Para tanto, o raio da regi\u00e3o \u00e9 repetidamente\nreduzido e novos modelos ?-posicionados s\u00e3o criados, at\u00e9 que se encontre\numa regi\u00e3o com o raio ??? que n\u00e3o excede ?????.\n\nTamb\u00e9m, para que esta redu\u00e7\u00e3o do raio n\u00e3o seja excessiva, este\nvalor ??? do raio \u00e9 usado, contanto que seja maior que ? ????. Caso con-\ntr\u00e1rio, utiliza-se o valor min(? ???? , ??). O procedimento encontra-se\nformalizado no Algoritmo 2.4.\n\n2.2.6 Algoritmo de regi\u00e3o de confian\u00e7a\n\nO Algoritmo 2.5 \u00e9 um exemplo de m\u00e9todo de regi\u00e3o de confian\u00e7a\nsegundo [16].\n\nAgora apresentamos as condi\u00e7\u00f5es suficientes para que o Algo-\nritmo 2.5 convirja para um ponto \u00f3timo de primeira ordem.\n\n\n\n40 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nAlgoritmo 2.4: Teste de criticidade\n? = 0;\n?0? = ??;\nfor ? = 0, 1, . . . do\n\nIncremente ? em 1;\n??? = ???1??;\nUtilize o modelo anterior ?(??1)? , para calcular um novo\nmodelo ?(?)? , ?-posicionado em ?(??, ???);\nif ??? ? ????? then\n\nRetorne ??? e ???? = ?\n(?)\n? .\n\nVamos fazer algumas suposi\u00e7\u00f5es sobre a fun\u00e7\u00e3o objetivo ? . Como\nos iterandos do algoritmo sempre apresentam descenso com rela\u00e7\u00e3o ao\nponto anterior, todos os eles pertencem ao conjunto de n\u00edvel (inferior)\ndefinido com rela\u00e7\u00e3o ao primeiro iterando:\n\n?(?0) = {? ? R?| ? (?) ? ? (?0)}\n\nPara a constru\u00e7\u00e3o dos modelos s\u00e3o usados pontos nas regi\u00f5es de\nconfian\u00e7a, definidas por bolas em torno dos iterandos. Ent\u00e3o, ? n\u00e3o \u00e9\navaliada apenas em ?(?0), mas no conjunto maior\n\n????(?0) =\n??\n\n???(?0)\n?(?, ?max).\n\nHip\u00f3tese 1 Sejam ?0 e ?max dados. A fun\u00e7\u00e3o ? \u00e9 diferenci\u00e1vel e tem\ngradiente Lipschitz cont\u00ednuo em um dom\u00ednio aberto contendo ????(?0).\n\nHip\u00f3tese 2 A fun\u00e7\u00e3o ? \u00e9 limitada inferiormente em ?(?0), isto \u00e9, que\nexiste uma constante ? * tal que para todo ? ? ?(?0), ? (?) ? ? *.\n\nE introduzimos, ainda, uma hip\u00f3tese sobre o modelo\n\nHip\u00f3tese 3 Suponha que existe uma constante ???? tal que para todo\niterando ??, a derivada segunda do modelo ?? satisfaz\n\n????? ????.\n\nE uma hip\u00f3tese sobre o passo escolhido durante o algoritmo:\n\n\n\n2.2. Regi\u00e3o de confian\u00e7a 41\n\nAlgoritmo 2.5: Um m\u00e9todo de regi\u00e3o de confian\u00e7a\nSeja ? : R? ? R dada;\nSeja ?0 ? R? um ponto inicial;\nSeja ?0 : R? ? R? um modelo inicial;\nSeja ?(?0, ?0) a regi\u00e3o de confian\u00e7a inicial;\nDefina 0 ? ?0 ? ?1 &lt;1 e ?1 ?= 0;\nDefina os fatores de redu\u00e7\u00e3o e aumento da regi\u00e3o de confian\u00e7a\n0 &lt;? &lt;1 &lt;????;\nDefina o raio m\u00e1ximo ?max da regi\u00e3o de confian\u00e7a;\nfor ? = 0, 1, . . . do\n\nConstrua o modelo ??(??) que interpola ? em ??. Calcule o\ngradiente ?? = ???(??);\nif ????? ?? e ?? n\u00e3o \u00e9 ?-posicionado em ?(??, ?????) then\n\nRefine o modelo at\u00e9 que ?? seja ?-posicionado em\n?(??, ??), ?? ? (0, ?????).\n\nCalcule o ponto ?+? = ?? + ?? que garanta descenso para o\nmodelo ?? em ?;\nCalcule o grau de concord\u00e2ncia entre a predi\u00e7\u00e3o de ?? e o\nvalor de ? :\n\n?? =\n? (??) ? ? (?+? )\n\n??(??) ? ??(?+? )\n; (2.18)\n\nAtualize o iterando:\n\n??+1 =\n{?\n\n?+? se ?? ? ?1 ou ?? ? ?0 e ?? ?-posicionado\n?? caso contr\u00e1rio\n\nAtualize o tamanho da regi\u00e3o de confian\u00e7a:\n\n??+1 =\n\n??\n?\n\nmin(??????, ?max), se ?? ? ?1\n???, se ?? &lt;?1 e ?? \u00e9 ?-posicionado\n??, se ?? &lt;?1 e ?? n\u00e3o \u00e9 ?-posicionado\n\nAtualize o modelo ??+1;\n\n\n\n42 Cap\u00edtulo 2. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\nHip\u00f3tese 4 Para toda itera\u00e7\u00e3o ?, o passo ?? escolhido satisfazendo ao\nmenos uma fra\u00e7\u00e3o ?? ?? do decr\u00e9scimo referente ao passo de Cauchy:\n\n??(??) ? ??(?? + ??) ? ?? ??\n[?\n??(??) ? ??(?? + ??? )\n\n]?\npara uma constante ?? ?? ? (0, 1).\n\nCom base nessas hip\u00f3teses, [16] apresenta o seguinte teorema:\n\nTeorema 6 Suponha as hip\u00f3teses 1, 2, 3 e 4. Ent\u00e3o\n\nlim\n??+?\n\n??? (??)? = 0\n\n2.3 ALTERNATIVAS ALGOR\u00cdTMICAS\n\nEm [17] \u00e9 apresentado um algoritmo para a manuten\u00e7\u00e3o do mo-\ndelo de interpola\u00e7\u00e3o polinomial mais simples que aquele apresentado\nneste cap\u00edtulo. Os resultados preliminares s\u00e3o interessantes, podendo-se\neconomizar no n\u00famero de avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo.\n\nTal abordagem procura utilizar ao m\u00e1ximo os pontos em que j\u00e1\nse conhece a fun\u00e7\u00e3o objetivo. N\u00e3o a apresentamos nesta disserta\u00e7\u00e3o\nporque n\u00e3o nos parece claro como estend\u00ea-la ao caso com restri\u00e7\u00f5es\nlineares, do qual trataremos no pr\u00f3ximo cap\u00edtulo.\n\n2.4 SUM\u00c1RIO\n\nNeste cap\u00edtulo introduzimos os m\u00e9todos de otimiza\u00e7\u00e3o n\u00e3o-dife-\nrenci\u00e1vel. Trata-se de m\u00e9todos de otimiza\u00e7\u00e3o que n\u00e3o necessitam de\ninforma\u00e7\u00e3o sobre as derivadas da fun\u00e7\u00e3o objetivo. Assim, podem ser\nusados em situa\u00e7\u00f5es inacess\u00edveis aos m\u00e9todos cl\u00e1ssicos de otimiza\u00e7\u00e3o,\nbaseados em derivadas. Podem ser usados, por exemplo, na otimiza\u00e7\u00e3o\nde fun\u00e7\u00f5es calculadas por simuladores ou por experimentos f\u00edsicos.\n\nApresentamos dois m\u00e9todos de otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel: bus-\nca direta direcional e regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel. O primeiro,\namostra a fun\u00e7\u00e3o objetivo um n\u00famero finito de vezes por itera\u00e7\u00e3o e\nutiliza tais valores para decidir sobre o andamento do algoritmo. N\u00e3o\nconstr\u00f3i qualquer modelo da fun\u00e7\u00e3o objetivo.\n\nO m\u00e9todo da regi\u00e3o de confian\u00e7a, ao contr\u00e1rio, primeiro amostra a\nfun\u00e7\u00e3o objetivo a fim de construir um modelo que a aproxime localmente.\nEste modelo \u00e9 utilizado para decidir onde se buscar\u00e1 por descenso.\n\nComo estes m\u00e9todos n\u00e3o fazem uso da informa\u00e7\u00e3o das derivadas,\n\u00e9 de se esperar que sua converg\u00eancia n\u00e3o seja t\u00e3o eficiente quanto a\n\n\n\n2.4. Sum\u00e1rio 43\n\ndos m\u00e9todos que as utilizam. Portanto, se derivadas est\u00e3o dispon\u00edveis,\n\u00e9 recomendado que se utilize um m\u00e9todo que fa\u00e7a uso delas [16].\n\n\n\n\n\n3 OTIMIZA\u00c7\u00c3O N\u00c3O-DIFERENCI\u00c1VEL COM\nRESTRI\u00c7\u00d5ES LINEARES\n\nNo cap\u00edtulo anterior apresentamos algoritmos de otimiza\u00e7\u00e3o n\u00e3o-\ndiferenci\u00e1vel, por\u00e9m n\u00e3o foram consideradas quaisquer restri\u00e7\u00f5es. Nes-\nte cap\u00edtulo estenderemos os m\u00e9todos de otimiza\u00e7\u00e3o j\u00e1 tratados para\nproblemas sujeitos a restri\u00e7\u00f5es lineares nas vari\u00e1veis, da forma seguinte:\n\nmin ? (?)\nsujeito a ? ? ? = {? : ?? ? ?}.\n\nOs m\u00e9todos deste cap\u00edtulo buscam por descenso apenas nos pon-\ntos do conjunto vi\u00e1vel. Assim, todos os iterandos s\u00e3o sucessivas aproxi-\nma\u00e7\u00f5es para o ponto \u00f3timo.\n\n3.1 MATERIAL B\u00c1SICO SOBRE OTIMIZA\u00c7\u00c3O RESTRITA\n\nO conjunto vi\u00e1vel ? cont\u00e9m os pontos que satisfazem \u00e0s restri\u00e7\u00f5es\ndo problema.\n\nDefinimos o cone polar de um conjunto ? ? R?, denotado ? ?\ncomo\n\n? ? = {? : ?? ? ? 0, ?? ? ?};\n\ncomo o nome sugere, o cone polar \u00e9 um cone, e \u00e9 convexo. Alguns\nexemplos de cones polares est\u00e3o na Figura 3.1.\n\nDefinimos o cone normal ao conjunto vi\u00e1vel ? no ponto ? como\no conjunto\n\n??(?) = {? ? R? : ?? (? ? ?) ? 0, ?? ? ?}\n\nFigura 3.1: Conjuntos e seus polares. Para conjuntos c\u00f4nicos convexos,\no cone polar do cone polar \u00e9 o pr\u00f3prio conjunto (? = (? ?)?).\n\n45\n\n\n\n46 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\n?\n\n?\n\n??(?)\n\n??(?)\n\n?\n\n?\n\n??(?) ??(?)\n\nFigura 3.2: Cone normal e cone tangente a ? em dois diferentes pontos\nda fronteira do conjunto vi\u00e1vel.\n\n?\n\n??? (?)\n\n?\n\nFigura 3.3: Ponto ? ? ? \u00f3timo sob restri\u00e7\u00f5es: ??? (?) ???(?)\n\ne o cone tangente a ? em ? como o polar do cone normal no mesmo\nponto, isto \u00e9,\n\n??(?) = ??(?)? = {? | ?? ? ? 0, ?? ???(?)}.\n\nInformalmente, o cone normal ??(?), cont\u00e9m as dire\u00e7\u00f5es que\n\u201capontam para fora\u201d de ?. Enquanto o cone tangente ??(?) cont\u00e9m\nvetores ?, em que, a partir de ?, se pode andar na dire\u00e7\u00e3o ? alguma\ndist\u00e2ncia sem sair de ?. Para um ponto interno a ?, ??(?) = R? e\n??(?) = {0}. Uma ilustra\u00e7\u00e3o se encontra na Figura 3.2.\n\nPara definir ponto \u00f3timo restrito, dizemos que ?* \u00e9 \u00f3timo de\nprimeira ordem sob restri\u00e7\u00f5es se, e somente se\n\n??? (?*)? ? ? 0, ?? ???(?*). (3.1)\n\nUma ilustra\u00e7\u00e3o de ponto \u00f3timo restrito encontra-se na Figura 3.3.\nDefinimos a proje\u00e7\u00e3o de um vetor ? ? R? em um conjunto ?, e\n\n\n\n3.2. Busca direta direcional com restri\u00e7\u00f5es lineares 47\n\n? ?\n\n?\n\n??? (?)\n?\n\nFigura 3.4: O vetor ? que resolve a equa\u00e7\u00e3o (3.3) na defini\u00e7\u00e3o de ?(?, ?).\n\ndenotamos ?? [?] como o m\u00ednimo para o problema\n\nmin\n???\n?? ? ??2\n\nA chamada decomposi\u00e7\u00e3o de Moreau do ponto ? ? R? relativa a\n? ? ? \u00e9 dada por\n\n? = ???(?) [?] + ???(?) [?] . (3.2)\n\nComo no caso restrito n\u00e3o podemos usar, por exemplo, ????\npara detectar um ponto estacion\u00e1rio, definimos a medida\n\n?(?, ?) = | min\n?+???\n?????\n\n?? ? ?|. (3.3)\n\nEsta medida \u00e9 ilustrada na Figura 3.4: ? \u00e9 a dire\u00e7\u00e3o mais pr\u00f3xima de\n??? (?) que, a partir de ?, se pode percorrer a dist\u00e2ncia ? sem sair do\nconjunto vi\u00e1vel.\n\nDe acordo com [20], temos o seguinte teorema, que mostra que\n? pode ser usada como medida de criticidade:\n\nTeorema 7 Suponha que ? tem derivada segunda cont\u00ednua e que o\nconjunto vi\u00e1vel ? \u00e9 n\u00e3o-vazio, fechado e convexo. Ent\u00e3o\n\n?(?) = ?(?, 1), ? ? ? (3.4)\n\n\u00e9 cont\u00ednua, n\u00e3o-negativa e ?(?) = 0 se, e somente se ? \u00e9 ponto \u00f3timo.\n\nIsto \u00e9, ?(?) pode ser vista como uma indica\u00e7\u00e3o da criticidade\nda fun\u00e7\u00e3o ? associada. Se ?(?) ? 0, consequentemente ??? (?)? ? ? 0\n?? ? ?(?).\n\n3.2 BUSCA DIRETA DIRECIONAL COM RESTRI\u00c7\u00d5ES LINEARES\n\nUma ilustra\u00e7\u00e3o das dificuldades introduzidas por restri\u00e7\u00f5es est\u00e1\nna Figura 3.5. Pelo Teorema 1, sabemos que ao considerar um conjunto\n\n\n\n48 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\n?\n\n??? (?)\n\n?\n(a)\n\n?\n\n??? (?)\n\n?\n(b)\n\nFigura 3.5: Conjunto vi\u00e1vel ? hachurado, abaixo da restri\u00e7\u00e3o (horizon-\ntal); dire\u00e7\u00f5es de descenso vi\u00e1veis sombreadas. Em um primeiro caso, a\ndire\u00e7\u00e3o de busca pr\u00f3xima a ??? (?) n\u00e3o \u00e9 vi\u00e1vel. Usando-se dire\u00e7\u00f5es\nde busca paralelas \u00e0s faces, \u00e9 poss\u00edvel encontrar descenso vi\u00e1vel.\n\nde dire\u00e7\u00f5es de busca gerador positivo, \u00e9 poss\u00edvel garantir que h\u00e1 uma das\ndire\u00e7\u00f5es formando \u00e2ngulo agudo com a dire\u00e7\u00e3o de m\u00e1ximo descenso. No\nentanto, pode ocorrer que justamente tal dire\u00e7\u00e3o se torne invi\u00e1vel (Fig.\n3.5), levando para fora do conjunto vi\u00e1vel, n\u00e3o importa qu\u00e3o pequeno\nseja o comprimento do passo. E no exemplo da figura, o iterando atual\n?? n\u00e3o \u00e9 \u00f3timo restrito. Ao mesmo tempo, desejamos prosseguir com a\nbusca sempre dentro do conjunto vi\u00e1vel, de modo que todos os iterandos\nsejam sucessivas aproxima\u00e7\u00f5es do \u00f3timo do problema e a busca possa\nser interrompida quando se obtiver uma exatid\u00e3o suficiente na resposta.\nDessa maneira, \u00e9 necess\u00e1rio construir conjuntos de dire\u00e7\u00f5es de busca\nmais abrangentes para o caso restrito. Para o caso da figura, alterando-\nse o conjunto das dire\u00e7\u00f5es de busca de modo a incluir dire\u00e7\u00f5es paralelas\n\u00e0s faces do conjunto ? j\u00e1 \u00e9 poss\u00edvel encontrar descenso vi\u00e1vel.\n\nAo utilizarmos a busca direta direcional, \u00e9 necess\u00e1rio definir quais\nas faces do conjunto vi\u00e1vel que est\u00e3o excessivamente pr\u00f3ximas do ite-\nrando ??. E proceder a busca paralelamente a elas, evitando-se pontos\ninvi\u00e1veis.\n\nConsidere que o conjunto vi\u00e1vel \u00e9 dado por restri\u00e7\u00f5es lineares,\nda forma ? = {? : ?? ? ?}.\n\nCada restri\u00e7\u00e3o linear, respectivamente cada linha da matriz ?,\ndefine uma face do conjunto vi\u00e1vel, da forma{?\n\n? : ??? ? = ??\n}?\n\n,\n\n\n\n3.2. Busca direta direcional com restri\u00e7\u00f5es lineares 49\n\n? ?1\n\n? ?(?, ?1)\n\n?(?, ?1)\n\n? ?2\n\n? ?(?, ?2)\n\n?(?, ?2)\n\nFigura 3.6: Os cones ?(?, ?) e seus polares. A partir de ?? \u00e9 poss\u00edvel\nandar ?? em qualquer dire\u00e7\u00e3o de ? ?(??, ??).\n\nem que ??? \u00e9 a linha ? da matriz ? e ?? \u00e9 a componente ? do vetor ?.\nAl\u00e9m disso, ??? \u00e9 o vetor ortogonal \u00e0 respectiva face.\n\nPara ? ? ?, definimos o conjunto das restri\u00e7\u00f5es ativas por\n\n?(?) =\n{?\n\n? ?{1, . . . , ?} : ??? ? = ??\n}?\n\ne, dado ? > 0, o conjunto das restri\u00e7\u00f5es ?-ativas por\n\n?(?, ?) =\n{?\n\n? ?{1, . . . , ?} : ??? ? ? ?? ???\n}?\n\n.\n\nAssim, ?(?, ?) cont\u00e9m justamente os \u00edndices das restri\u00e7\u00f5es que est\u00e3o a\numa dist\u00e2ncia ? de serem violadas. Isto \u00e9, todos os ? tais que ? + ??? /? ?.\n\nDefinimos tamb\u00e9m o cone\n\n?(?, ?) =\n\n??\n?? ? R? | ? = ??\n\n???(?,?)\n????, ?? > 0\n\n??\n? .\n\nEm termos gerais, trata-se do cone gerado pelas dire\u00e7\u00f5es ?? que apontam\npara fora do conjunto vi\u00e1vel. O polar correspondente, ? ?(?, ?), aproxima\nlocalmente a geometria do conjunto vi\u00e1vel ?. Assim, \u00e9 posss\u00edvel partir\nde ? e percorrer ao menos uma dist\u00e2ncia ? em qualquer dire\u00e7\u00e3o de\n? ?(?, ?), permanecendo-se em ?, conforme ilustrado na Figura 3.6 e\nformalizado na proposi\u00e7\u00e3o seguinte:\n\nProposi\u00e7\u00e3o 1 Se ? ? ? e ? ? ? ?(?, ?), ???? ?, ent\u00e3o ? + ? ? ?.\n\nDessa maneira, o tratamento de restri\u00e7\u00f5es que apresentaremos\nutiliza conjuntos geradores positivos que satisfazem uma das seguintes\ncondi\u00e7\u00f5es:\n\n\n\n50 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\nCondi\u00e7\u00e3o 1 Para ?? > 0, ?? gera positivamente ? ?(??, ??).\n\nCondi\u00e7\u00e3o 2 ?? inclui geradores positivos para todos os cones ? ?(?, ?),\n0 ? ? ? ?* para algum ?* > 0 independente da itera\u00e7\u00e3o ?.\n\nO teorema seguinte aponta uma maneira de se gerar as dire\u00e7\u00f5es\nde busca para satisfazer tais condi\u00e7\u00f5es [21, 22].\n\nTeorema 8 Suponha que um cone ?(?, ?0) ? R? \u00e9 gerado positiva-\nmente pelos vetores que comp\u00f5em as colunas de uma matriz ? . Suponha\nque tal matriz tem posto-coluna completo (os vetores s\u00e3o linearmente\nindependentes).\n\n? Sejam ?1, . . . , ?? os vetores que geram positivamente o n\u00facleo\n(espa\u00e7o-nulo) de ? ? .\n\n? Sejam ?1, . . . , ?? os vetores das colunas da matriz ? (? ? ? )?1.\n\nEnt\u00e3o,\n\n1. O conjunto de dire\u00e7\u00f5es ? = {??}1???? ?{???}1???? satisfaz a\ncondi\u00e7\u00e3o 1 (isto \u00e9, gera o cone polar ? ?(?, ?0)).\n\n2. O conjunto de dire\u00e7\u00f5es ? = {??}1???? ?{???, ??}1???? satisfaz\na condi\u00e7\u00e3o 2, (gera o cone polar ? ?(?, ?) para todo 0 &lt;? &lt;?0).\n\nOutra informa\u00e7\u00e3o dada pelo teorema \u00e9 a quantidade de dire\u00e7\u00f5es\nnecess\u00e1rias para conduzir a busca no R?. Note que o n\u00famero ? das\ncolunas de ? (? ? ? )?1 \u00e9 o posto de ? . Se ? tem posto completo (? =\n?), o n\u00famero de dire\u00e7\u00f5es necess\u00e1rias para atender \u00e0 condi\u00e7\u00e3o 2 \u00e9 2?.\nCaso ? &lt;?, o n\u00facleo de ? ? tem dimens\u00e3o ? ? ?, podendo ser gerado\npositivamente por ? = ??? + 1 vetores. Nesse caso, o n\u00famero de vetores\ndo conjunto ? das dire\u00e7\u00f5es de busca \u00e9 (? ? ? + 1) + ? + 2 &lt;2?.\n\nLembrando que ?(?), definida conforme o Teorema 7, \u00e9 uma\nmedida de estacionariedade, a seguinte proposi\u00e7\u00e3o ilustra como um ?\nsuficientemente pequeno induz uma cota superior para essa medida.\n\nProposi\u00e7\u00e3o 2 Considere ? > 0. Existe ? > 0 dependendo apenas de ?\ne da matriz de restri\u00e7\u00f5es ?, tal que vale o seguinte: suponha que ? ? ?,\n??? (?)? &lt;? e ?(?) > 0. Dado ? ? 0, seja ? o conjunto de geradores\nde ? ?(?, ?). Ent\u00e3o existe um n\u00famero ?????? (?) > 0, dependendo apenas\nde ? tal que se ? &lt;?, para algum vetor ? ??,\n\n??????(?)?(?)??????? (?)? ?. (3.5)\n\n\n\n3.2. Busca direta direcional com restri\u00e7\u00f5es lineares 51\n\n? ?1\n\n?\n\n? ?(?, ?1)\n\n?(?, ?1)\n\n??? (?)\n\n(a) ?1 n\u00e3o \u00e9 pequeno o suficiente\n\n? ?2\n?\n\n? ?(?, ?2)\n\n?(?, ?2)\n??? (?)\n\n(b) ?2 adequado\n\nFigura 3.7: Para se encontrar uma dire\u00e7\u00e3o de descenso vi\u00e1vel em\n? ?(?, ?), ? precisa ser pequeno.\n\nA condi\u00e7\u00e3o ? &lt;? do teorema \u00e9 ilustrada com a Figura 3.7.\nPara um valor ?1 muito grande, a dire\u00e7\u00e3o de m\u00e1ximo descenso ??? (?)\nn\u00e3o tem proje\u00e7\u00e3o em ? ?(?, ?1), enquanto para um ?2 suficientemente\npequeno, sim.\n\nIsto \u00e9, ?????? tem um papel an\u00e1logo ao feito pela medida cosseno\nno caso irrestrito. De modo semelhante, tamb\u00e9m \u00e9 necess\u00e1rio garantir\numa boa geometria das dire\u00e7\u00f5es de busca. Para tanto, introduzimos a\nseguinte condi\u00e7\u00e3o\n\nCondi\u00e7\u00e3o 3 Existe um ?min > 0 tal que para todo ? e todo conjunto\n??, que gera o cone ? ?(??, ?), a rela\u00e7\u00e3o (3.5) \u00e9 satisfeita com\n\n?????? > ?min. (3.6)\n\nDe acordo com [13], tal condi\u00e7\u00e3o s\u00f3 precisa ser levada em con-\nsidera\u00e7\u00e3o explicitamente quando ? ?(??, ?) cont\u00e9m um sub-espa\u00e7o. Ao\nmesmo tempo, conjuntos de dire\u00e7\u00f5es dados pelo Teorema 8 ainda admi-\ntem liberdade na escolha dos vetores que geram positivamente o n\u00facleo\nde ? ? .\n\nUma op\u00e7\u00e3o \u00e9 partir de uma base ortogonal e complet\u00e1-la com os\nnegativos de todas as suas dire\u00e7\u00f5es, ou conforme a equa\u00e7\u00e3o (2.4).\n\nAlguns teoremas de [13] mostram a converg\u00eancia do m\u00e9todo\napresentado:\n\nTeorema 9 Suponha que ?? \u00e9 lipchitziana com constante ? . Suponha\nque os iterandos {??} do Algoritmo 3.1 pertencem a um conjunto limi-\ntado, e que ??? (?)?? ? em tal conjunto. Ent\u00e3o, existem constantes ?\n\n\n\n52 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\ne ? independentes da itera\u00e7\u00e3o ?, tais que\n\n?(??) ? ?\n[?\n? ???max +\n\n?(??)\n???min\n\n]?\nse a condi\u00e7\u00e3o 2 for utilizada e\n\n?(??) ? ?\n[?\n? ???max +\n\n?(??)\n???min\n\n+ ?\n?\n\n?min\n??\n\n]?\nse a condi\u00e7\u00e3o 1 for utilizada.\n\nConclu\u00edmos os resultados de converg\u00eancia do m\u00e9todo com o teo-\nrema de [13]:\n\nTeorema 10 Se a condi\u00e7\u00e3o 1 for utilizada,\n\nlim inf\n??+?\n\n?(??) = 0,\n\nse a condi\u00e7\u00e3o 2 for utilizada,\n\nlim\n??+?\n\n?(??) = 0.\n\n3.2.1 Alternativas algor\u00edtmicas\n\nUm algoritmo semelhante \u00e0 busca direta direcional, com fortes\ncaracter\u00edsticas de converg\u00eancia e capaz de tratar restri\u00e7\u00f5es ainda mais\ngerais, bastando que o conjunto seja conexo, est\u00e1 em [23]. \u00c9 um algo-\nritmo capaz de tratar at\u00e9 mesmo restri\u00e7\u00f5es do tipo caixa-preta, isto \u00e9,\nque n\u00e3o se conhece de antem\u00e3o. Por outro lado, tal algoritmo n\u00e3o faz uso\ndas restri\u00e7\u00f5es conhecidas, como o que apresentamos nesta disserta\u00e7\u00e3o.\n\n\n\n3.2. Busca direta direcional com restri\u00e7\u00f5es lineares 53\n\nAlgoritmo 3.1: Um algoritmo de busca direta direcional para\nrestri\u00e7\u00f5es lineares\n\nSeja ? : R? ? R dada;\nSeja ?0 ? ? um ponto inicial vi\u00e1vel;\nSeja ???? > 0 a toler\u00e2ncia de converg\u00eancia;\nSeja ?0 > ???? um tamanho de passo inicial;\nSeja ?max ? 1 um limite superior para o coeficiente de expans\u00e3o\n??;\nSeja ?max &lt;1 um limite superior para o par\u00e2metro de contra\u00e7\u00e3o\n??;\nSejam ?max ? ?min > 0 limites superior e inferior,\nrespectivamente, para os comprimentos dos vetores de qualquer\nconjunto gerador positivo;\nSeja ?min > 0 um limitante superior para a medida cosseno dos\nconjuntos geradores positivos;\nSeja ? : R+ ? R+ uma fun\u00e7\u00e3o n\u00e3o-decrescente satisfazendo a\nequa\u00e7\u00e3o (2.8);\nSe for utilizada a condi\u00e7\u00e3o 1, seja ?0 a toler\u00e2ncia inicial para as\nrestri\u00e7\u00f5es localmente ativas e 0 &lt;?? &lt;1 o fator de redu\u00e7\u00e3o de ??;\nSe for utilizada a condi\u00e7\u00e3o 2, defina ? > 0 para a toler\u00e2ncia das\nrestri\u00e7\u00f5es localmente ativas;\nfor ? = 1, 2, . . . do\n\nDefina ?? conjunto gerador positivo satisfazendo a condi\u00e7\u00e3o 3\ne 2 ou 1 e ?min ????? ?max, ?? ???, com ? (??) ? ?min;\nfor ? ??? do\n\nDefina ??(?) como o maior valor tal que\n(?? + ??(?)?) ? ?;\nDefina ??? = min(??, ??(?));\nif Existe ? ??? tal que ? (?? + ????) &lt;? (??) ? ?(??)\nthen\n\n??+1 ? ?? + ????;\n??+1 ? ????, em que 1 ? ?? ? ?max;\n\nelse\n??+1 ? ??;\n??+1 ? ????, 0 &lt;?? ? ?max;\nSe est\u00e1 sendo usada a condi\u00e7\u00e3o 1, ??+1 ? ????;\nif ??+1 &lt;???? then\n\nEncerrar;\n\n\n\n54 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\n3.3 M\u00c9TODOS DE REGI\u00c3O DE CONFIAN\u00c7A COM RESTRI\u00c7\u00d5ES LI-\nNEARES\n\nM\u00e9todos de regi\u00e3o de confian\u00e7a podem ser estendidos a problemas\ncom restri\u00e7\u00f5es lineares de forma mais intuitiva que os de busca direta\ndirecional. Em princ\u00edpio, pode-se pensar em restringir os algoritmos\napresentados ao conjunto vi\u00e1vel. No entanto, se ? for amostrada somente\nem pontos vi\u00e1veis, pode n\u00e3o ser poss\u00edvel obter bons modelos (isto \u00e9,\nmodelos ?-posicionados), que s\u00e3o essenciais nas provas de converg\u00eancia\ndaqueles algoritmos.\n\nNesta se\u00e7\u00e3o mostraremos que, com pequenas altera\u00e7\u00f5es, o Algo-\nritmo 2.5 j\u00e1 apresentado funciona para o caso com restri\u00e7\u00f5es lineares\nnas vari\u00e1veis. Utilizaremos a abordagem que [20] apresenta ao abordar\no caso diferenci\u00e1vel com restri\u00e7\u00f5es convexas.\n\nIdealmente, a fun\u00e7\u00e3o objetivo seria avaliada apenas em pontos\nvi\u00e1veis, candidatos \u00e0 solu\u00e7\u00e3o do problema. Mas o formato do conjunto\nvi\u00e1vel ? pode n\u00e3o permitir que se consiga um modelo de interpola\u00e7\u00e3o\nadequado. Para manter um modelo ?-posicionado n\u00e3o faremos tal res-\ntri\u00e7\u00e3o. Os pontos usados na constru\u00e7\u00e3o do modelo na regi\u00e3o ?(??, ??)\nser\u00e3o tomados sem levar em considera\u00e7\u00e3o se s\u00e3o vi\u00e1veis ou n\u00e3o.\n\nPara tanto, propomos utilizar os Algoritmos 2.2 e 2.3 sem qual-\nquer altera\u00e7\u00e3o. Resta comentar sobre o c\u00e1lculo do passo.\n\n3.3.1 Passo generalizado de Cauchy\n\nComo no cap\u00edtulo anterior, a cada itera\u00e7\u00e3o precisamos encontrar\num ponto candidato \u00e0 solu\u00e7\u00e3o que satisfa\u00e7a uma condi\u00e7\u00e3o m\u00ednima de\ndescenso para o modelo.\n\nComo no caso irrestrito, se o modelo usado for linear, em uma\nbola na norma ? \u00b7 ??, o acr\u00e9scimo de (outras) restri\u00e7\u00f5es lineares n\u00e3o\nintroduz maiores dificuldades. Ainda \u00e9 poss\u00edvel chegar ao \u00f3timo global\nutilizando programa\u00e7\u00e3o linear. No entanto, para modelos quadr\u00e1ticos,\nencontrar o \u00f3timo global pode ser um problema NP-dif\u00edcil se o problema\nfor n\u00e3o-convexo. Nesta se\u00e7\u00e3o introduziremos o passo generalizado de\nCauchy conforme [20], que define um descenso suficiente para garantir a\nconverg\u00eancia do algoritmo e pode ser encontrado em todas as itera\u00e7\u00f5es.\n\nComo no caso anterior, o passo de Cauchy envolve a busca por\ndescenso utilizando a parte linear do modelo, isto \u00e9, a dire\u00e7\u00e3o ??, par-\ntindo do ponto ??, mas agora sujeito a ?? + ? ? ?.\n\nPara tanto, defina o caminho do gradiente projetado (projected\ngradient path de [20]) como\n\n?(?, ?) = ?? [? ? ??? (?)] , ? ? 0.\n\n\n\n3.3. Regi\u00e3o de confian\u00e7a 55\n\nIsto \u00e9, a proje\u00e7\u00e3o em ? de um ponto da reta que passa por ? e tem\ndire\u00e7\u00e3o ??? (?). Uma generaliza\u00e7\u00e3o do passo de Cauchy \u00e9 encontrar o\nm\u00ednimo do modelo sobre o caminho do gradiente projetado.\n\n3.3.2 Algoritmo de regi\u00e3o de confian\u00e7a com restri\u00e7\u00f5es lineares\n\nAgora estamos em condi\u00e7\u00f5es de definir o m\u00e9todo de regi\u00e3o de\nconfian\u00e7a para o caso com restri\u00e7\u00f5es lineares (Algoritmo 3.2). Difere,\ncom rela\u00e7\u00e3o ao anterior, apenas no fato de manter todos os seus ite-\nrandos (isto \u00e9, os pontos ?? onde s\u00e3o centradas as regi\u00f5es), vi\u00e1veis. A\nconstru\u00e7\u00e3o e manuten\u00e7\u00e3o de modelos \u00e9 feita da mesma forma.\n\nParte-se de um ponto vi\u00e1vel ?0 ? ?. E para o c\u00e1lculo do passo\n?+? = ?? + ??, se busca por descenso para ?? em ?(??, ??) ? ?, de\nforma a n\u00e3o sair do conjunto vi\u00e1vel.\n\n\n\n56 Cap\u00edtulo 3. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\nAlgoritmo 3.2: Um m\u00e9todo de regi\u00e3o de confian\u00e7a\nSeja ? : R? ? R dada;\nSeja ?0 ? R? um ponto inicial;\nSeja ?0 : R? ? R? um modelo inicial;\nSeja ?(?0, ?0) a regi\u00e3o de confian\u00e7a inicial;\nDefina 0 ? ?0 ? ?1 &lt;1 e ?1 ?= 0;\nDefina os fatores de redu\u00e7\u00e3o e aumento da regi\u00e3o de confian\u00e7a\n0 &lt;? &lt;1 &lt;????;\nDefina o raio m\u00e1ximo ?? da regi\u00e3o de confian\u00e7a;\nfor ? = 0, 1, . . . do\n\nConstrua o modelo ??(??) que interpola ? em ??. Calcule o\ngradiente ?? = ???(??);\nif ????? ?? e ?? n\u00e3o \u00e9 ?-posicionado em ?(??, ?????) then\n\nRefine o modelo at\u00e9 que ?? seja ?-posicionado em\n?(??, ??), ?? ? (0, ?????).\n\nCalcule o ponto ?+? = ?? + ?? que garanta descenso para o\nmodelo ?? em ?? ?;\nCalcule o grau de concord\u00e2ncia entre a predi\u00e7\u00e3o de ??(?+? ) e\no valor de ? (?+? ):\n\n?? =\n? (??) ? ? (?+? )\n\n??(??) ? ??(?+? )\n; (3.7)\n\nAtualize o iterando:\n\n??+1 =\n{?\n\n?+? se ?? ? ?1 ou ?? ? ?0 e ?? ?-posicionado\n?? caso contr\u00e1rio\n\nAtualize o tamanho da regi\u00e3o de confian\u00e7a:\n\n??+1 =\n\n??\n?\n\nmin(??????, ?max), se ?? ? ?1\n???, se ?? &lt;?1 e ?? \u00e9 ?-posicionado\n??, se ?? &lt;?1 e ?? n\u00e3o \u00e9 ?-posicionado\n\nAtualize o modelo ??+1;\n\n\n\n3.4. Sum\u00e1rio 57\n\n3.3.3 Alternativas algor\u00edtmicas\n\nEm [18], \u00e9 apresentado um algoritmo eficiente para o caso de\nlimites superiores e inferiores nas vari\u00e1veis, estendendo o apresentado em\n[17]. Quando uma vari\u00e1vel chega a um limite, ela \u00e9 fixada e o problema\nsegue, otimizando-se apenas as vari\u00e1veis restantes. Dessa maneira, os\nmodelos s\u00e3o constru\u00eddos apenas com vari\u00e1veis irrestritas, evitando o\nproblema descrito no par\u00e1grafo acima. Uma grande vantagem dessa\nabordagem \u00e9 que os modelos ficam mais simples conforme as vari\u00e1veis\nchegam aos seus limites, diminuindo o esfor\u00e7o de manuten\u00e7\u00e3o do modelo.\nO algoritmo procura economizar nos passos de melhora de modelo e\nnas avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo ? .\n\nPor outro lado, tal algoritmo n\u00e3o \u00e9 extens\u00edvel para outras res-\ntri\u00e7\u00f5es al\u00e9m de limites superiores e inferiores. A autora avalia a pos-\nsibilidade de tratar demais restri\u00e7\u00f5es usando m\u00e9todos como SQP e la-\ngrangiano aumentado. A abordagem apresentada neste cap\u00edtulo, trata\ndiretamente problemas que n\u00e3o s\u00e3o resolvidos por [18]. Ao custo de\num maior esfor\u00e7o na manuten\u00e7\u00e3o dos modelos e prov\u00e1vel maior n\u00fa-\nmero de avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo ? , por incluir pontos invi\u00e1veis na\nconstru\u00e7\u00e3o do modelo ?.\n\n3.4 SUM\u00c1RIO\n\nNeste cap\u00edtulo apresentamos alguns conceitos de otimiza\u00e7\u00e3o res-\ntrita e estendemos os algoritmos de busca direta direcional e de regi\u00e3o\nde confian\u00e7a. Em ambos os casos, os algoritmos buscam por descenso\nsem deixar o conjunto vi\u00e1vel.\n\nA busca direta direcional pode ser estendida ao caso com restri-\n\u00e7\u00f5es lineares por meio de conjuntos de dire\u00e7\u00f5es de busca que reprodu-\nzam, de certa maneira, a geometria local do conjunto vi\u00e1vel, possibili-\ntando a busca por descenso em dire\u00e7\u00f5es vi\u00e1veis.\n\nA extens\u00e3o do algoritmo de regi\u00e3o de confian\u00e7a \u00e9 mais simples.\nBasta resolver o sub-problema da regi\u00e3o de confian\u00e7a respeitando tam-\nb\u00e9m as restri\u00e7\u00f5es lineares. \u00c9 importante lembrar, por\u00e9m, que para a\nconstru\u00e7\u00e3o do modelo pode ser necess\u00e1rio incluir-se tamb\u00e9m pontos\ninvi\u00e1veis.\n\nO pr\u00f3ximo cap\u00edtulo apresenta um algoritmo de Lagrangiano au-\nmentado para a resolu\u00e7\u00e3o de problemas tamb\u00e9m com restri\u00e7\u00f5es n\u00e3o-\nlineares. Tal abordagem acaba gerando sub-problemas com restri\u00e7\u00f5es\nlineares, que podem ser resolvidos com os algoritmos do presente cap\u00ed-\ntulo.\n\n\n\n\n\n4 OTIMIZA\u00c7\u00c3O N\u00c3O-DIFERENCI\u00c1VEL COM\nRESTRI\u00c7\u00d5ES N\u00c3O-LINEARES\n\nNos cap\u00edtulos anteriores apresentamos algoritmos capazes de re-\nsolver o problema de otimiza\u00e7\u00e3o com restri\u00e7\u00f5es lineares. Na produ\u00e7\u00e3o\nde petr\u00f3leo, no entanto, se forem considerados itens como limites da\nprodu\u00e7\u00e3o de g\u00e1s ou de algum contaminante, os problemas resultantes\nter\u00e3o restri\u00e7\u00f5es n\u00e3o-lineares. Assim, n\u00e3o podem ser resolvidos com os\nm\u00e9todos vistos. A abordagem utilizada neste trabalho para resolver tais\nproblemas foi a de usar um m\u00e9todo de Lagrangiano aumentado.\n\n\u00c9 um m\u00e9todo que consiste em substituir as restri\u00e7\u00f5es do problema\nde otimiza\u00e7\u00e3o por uma sequ\u00eancia de sub-problemas de otimiza\u00e7\u00e3o. Estes\ns\u00e3o irrestritos, ou com restri\u00e7\u00f5es mais simples. As restri\u00e7\u00f5es n\u00e3o-lineares\noriginais s\u00e3o substitu\u00eddas por termos de penaliza\u00e7\u00e3o na fun\u00e7\u00e3o objetivo.\nAssim, os m\u00e9todos dos cap\u00edtulos anteriores podem ser empregados na\nresolu\u00e7\u00e3o dos sub-problemas.\n\n4.1 LAGRANGIANO AUMENTADO\n\nA fim de resolver o problema na fun\u00e7\u00e3o objetivo ? : R? ? R,\ncom restri\u00e7\u00f5es n\u00e3o-lineares ? : R? ? R?:\n\nmin ? (?)\nsujeito a ?(?) = 0.\n\nO m\u00e9todo consiste em resolver uma sequ\u00eancia de sub-problemas,\nderivados deste, por\u00e9m com as restri\u00e7\u00f5es de igualdade ? transformadas\nem penaliza\u00e7\u00f5es, inclu\u00eddas na fun\u00e7\u00e3o objetivo, em vez de serem tratadas\nexplicitamente. Ao contr\u00e1rio do cap\u00edtulo anterior, admitimos que a\nbusca ocorra tamb\u00e9m por pontos invi\u00e1veis.\n\nPara tanto, introduzimos a fun\u00e7\u00e3o Lagrangiano aumentado asso-\nciada a ? , dada por\n\n?(?, ?, c) = ? (?) +\n???\n\n?=1\n????(?) +\n\n1\n2\n\n???\n?=1\n\nc?2?(?)\n\n= ? (?) + ?? ?(?) +\nc\n2\n??(?)?2.\n\nPara pontos vi\u00e1veis (? ? ?), ?(?) = 0 e ?(?, ?, c) = ? (?). O\npar\u00e2metro de penaliza\u00e7\u00e3o c determina o quanto as viola\u00e7\u00f5es das res-\ntri\u00e7\u00f5es ? s\u00e3o penalizadas, enquanto ? consiste de estimativas para os\nmultiplicadores de Lagrange do problema original.\n\n59\n\n\n\n60 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\nOs problemas a serem resolvidos, ent\u00e3o, s\u00e3o dados por\n\nmin\n??R?\n\n?(?, ?, c)\n\nQuanto maior o valor de c, maior a penaliza\u00e7\u00e3o imposta aos\npontos invi\u00e1veis ? /? ?. Assim, se c for alto, a minimiza\u00e7\u00e3o de ?(?, ?, c)\ntender\u00e1 a produzir pontos onde ?(?) ? 0. Nesse caso, ?(?, ?, c) ? ? (?),\nj\u00e1 que o ponto ? quase \u00e9 vi\u00e1vel, assim a minimiza\u00e7\u00e3o de ?(?, ?, c)\naproxima a de ? (?). [24].\n\nSe ? \u00e9 uma boa estimativa para os multiplicadores de Lagrange\nexatos ?* (correspondentes ao ponto \u00f3timo ?*), a minimiza\u00e7\u00e3o de\n?(?, ?, c) prov\u00ea um ponto ? pr\u00f3ximo de ?*, contanto que c seja su-\nficientemente alto para que se possa realizar tal minimiza\u00e7\u00e3o [24].\n\n4.1.1 Lagrangiano aumentado com resolu\u00e7\u00e3o aproximada dos sub-\nproblemas\n\nPor\u00e9m, n\u00e3o \u00e9 necess\u00e1rio encontrar com exatid\u00e3o o ponto \u00f3timo\nem cada um dos sub-problemas. Estamos particularmente interessados\nem m\u00e9todos iterativos nos quais, em vez de buscar por um ponto ??\nem que\n\n????(?, ?, c)? = 0,\no usual \u00e9 que o m\u00e9todo pare quando satisfeito um crit\u00e9rio da forma\n\n????(?, ?, c)?? ??,\n\nisto \u00e9, com a norma do gradiente (ou crit\u00e9rio semelhante) baixa, ainda\nque n\u00e3o-nula.\n\nA converg\u00eancia de um algoritmo feito pela resolu\u00e7\u00e3o de tais sub-\nproblemas \u00e9 esclarecida, em parte, com a seguinte proposi\u00e7\u00e3o, de [24]:\n\nProposi\u00e7\u00e3o 3 Sejam ? e ? continuamente diferenci\u00e1veis. Para a ite-\nra\u00e7\u00e3o ? = 0, 1, . . ., considere que ?? satisfaz\n\n???(??, ??, c ?)?? ??,\n\nem que a sequ\u00eancia {??} \u00e9 limitada, bem como {??} e {c ?} satisfazem\n\n0 &lt;c ? &lt;c ?+1, ??, c ? ??,\n\n0 ? ??, ??, ?? ? 0\n\n\n\n4.1. Lagrangiano aumentado 61\n\nConsidere que a subsequ\u00eancia {??}? converge para um vetor ?*\ntal que ??(?*) tem posto ?. Ent\u00e3o\n\n{?? + c ??(??)}? ? ?*,\n\nem que ?* satisfaz, juntamente com ?*, as condi\u00e7\u00f5es de otimalidade de\nprimeira ordem para o problema original (com restri\u00e7\u00e3o de igualdade):\n\n?? (?*) + ??(?*)?* = 0, ?(?*) = 0.\n\nAs hip\u00f3teses da proposi\u00e7\u00e3o podem n\u00e3o ser satisfeitas, como coment\u00e1rios\na seguir [24].\n\nEm primeiro lugar, pode n\u00e3o ser poss\u00edvel encontrar um {??}\nsatisfazendo ???(??, ??, c ?)?? ??. A causa disso, normalmente, \u00e9 que\no Lagrangiano aumentado ?(\u00b7, ??, c ?) n\u00e3o \u00e9 limitado inferiormente.\n\nUma segunda possibilidade \u00e9 que, ou a sequ\u00eancia {??} n\u00e3o con-\nverge, ou converge para um ponto ?* em que ??(?*) tenha colunas\nlinearmente dependentes. Isto normalmente ocorre quando o Lagrangi-\nano aumentado \u00e9 limitado inferiormente, por\u00e9m o problema original n\u00e3o\ntem uma solu\u00e7\u00e3o vi\u00e1vel. Neste caso, quando se aumenta a penaliza\u00e7\u00e3o\nc, o termo quadr\u00e1tico c2??(?)?\n\n2 domina e a solu\u00e7\u00e3o do sub-problema \u00e9\njustamente um ponto estacion\u00e1rio desta fun\u00e7\u00e3o, de modo que\n\nc\n2\n???(?*)?2 = 0.\n\nUm terceiro caso, que raramente ocorre na pr\u00e1tica, \u00e9 que {??}\nconverge para um ponto {?*} ao qual n\u00e3o h\u00e1 multiplicadores de La-\ngrange associados. Neste caso, a sequ\u00eancia {?? + c ??(??)} tamb\u00e9m\ndiverge.\n\nApontados esses casos, patol\u00f3gicos, [24] comenta que o normal\n\u00e9 que o algoritmo chegue a um par (?*, ?*) satisfazendo as condi\u00e7\u00f5es\nde otimalidade. Ainda, a grande experi\u00eancia pr\u00e1tica consolida que o\nm\u00e9todo, de modo geral, \u00e9 confi\u00e1vel e normalmente converge para um\nponto que \u00e9, pelo menos, m\u00ednimo local do problema original. As falhas,\nusualmente se devem ao fato de que minimizar ?(\u00b7, ??, c ?) torna-se mais\ndif\u00edcil conforme c ? ??.\n\nExiste, ainda, outro motivo que torna ineficiente a busca pela\nsolu\u00e7\u00e3o exata dos sub-problemas: os multiplicadores de Lagrange n\u00e3o\ns\u00e3o exatos. A solu\u00e7\u00e3o do problema interno pode se afastar da solu\u00e7\u00e3o do\nproblema original justamente por isso. Assim, alguns m\u00e9todos chegam a\niterar de maneira ainda mais intensa entre a resolu\u00e7\u00e3o de sub-problemas\ne a atualiza\u00e7\u00e3o dos multiplicadores de Lagrange [24].\n\n\n\n62 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\n4.1.2 Lagrangiano aumentado com restri\u00e7\u00f5es de desigualdade\n\nTendo apresentado o caso do m\u00e9todo do Lagrangiano sujeito a\nrestri\u00e7\u00f5es de igualdade, seguimos com o problema com desigualdades\n\nmin ?\nsujeito a ?(?) = 0\n\n?(?) ? 0.\n\nem que ? : R? ? R? , e como antes, ? : R? ? R, ? : R? ? R?.\nEste pode ser facilmente colocado na forma anterior, introduzin-\n\ndo-se as vari\u00e1veis de folga ?1, . . . , ?? [25]:\n\nmin ? (?)\nsujeito a ?(?) = 0\n\n?(?) + ?2 = 0,\n\nem que ?2 representa o vetor (?21 , . . . , ?2? ).\nEnt\u00e3o, podemos usar o mesmo m\u00e9todo, usando nos sub-problemas\n\na fun\u00e7\u00e3o Lagrangiano aumentado, a seguir reescrita com as vari\u00e1veis\nadicionais:\n\n?(?, ?, ?, ?, c) = ? (?) + ?? ?(?) + c\n2\n??(?)?2+\n\n???\n?=1\n\n{?\n??(??(?) + ?2? ) +\n\nc\n2\n|??(?) + ?2? |2\n\n}?\n,\n\nem que ??(?) \u00e9 a componente ? de ?(?). Os sub-problemas consistem\nem\n\nmin\n?,?\n\n?(?, ?, ?, ?, c). (4.1)\n\nA fim de encontrar ? e ? que resolvam (4.1), \u00e9 poss\u00edvel, para cada\n?, minimizar em ? o Lagrangiano aumentado ?(?, \u00b7, ?, ?, c). Para tanto,\nbasta minimizar cada termo dependente de ?, problema este reescrito\nna vari\u00e1vel ?? = ?2? :\n\nmin\n???0\n{??[??(?) + ??] +\n\n1\n2\n\nc|??(?) + ??|2}. (4.2)\n\nNos pontos de m\u00ednimo irrestrito ?? de cada uma destas fun\u00e7\u00f5es, a\nderivada vale zero:\n\n?? + c[??(?) + ????] = 0,\n\n\n\n4.1. Lagrangiano aumentado 63\n\nconsequentemente,\n???? = ?[(??/c) + ??(?)].\n\nSe, por outro lado, este valor ???? for negativo, o m\u00ednimo restrito de (4.2)\n\u00e9 ?*? = 0 (A express\u00e3o de (4.2) \u00e9 convexa em ??).\n\nEnt\u00e3o:\n?*? = max{0,?[(??/c) + ??(?)]},\n\ne\n??(?) + ?*? = max{??(?),?(??/c)}.\n\nAssim, denotando\n\n?+(?, ?, c) =\n\n?\n?? max{?1(?),?(?1/?)}...\n\nmax{?? (?),?(?? /?)}\n\n?\n?? ,\n\na fun\u00e7\u00e3o Lagrangiano aumentado, j\u00e1 minimizada em ? \u00e9:\n\nmin\n?\n?(?, ?, ?, ?, c) = ? (?) + ?? ?(?) + 1\n\n2\nc??(?)?2+\n\n+ ?? ?+(?, ?, c) +\n1\n2\n\nc??+(?, ?, c)?2.\n\nIsto motiva a defini\u00e7\u00e3o do Lagrangiano aumentado para proble-\nmas com restri\u00e7\u00f5es de desigualdades\n\n?(?, ?, ?, c) = ? (?) + ?? ?(?) + ?? ?+(?, ?, c)+\n\n+\n1\n2\n\nc{??(?)?2 + ??+(?, ?, c)?2} (4.3)\n\nque pode ser reescrito [25] como:\n\n?(?, ?, ?, c) = ? (?) + ?? ?(?) + 1\n2\n\nc??(?)?2+\n\n+\n1\n2c\n\n???\n?=1\n\n{?\nmax[0, ?? + c??(?)]2 ? ?2?\n\n}?\n. (4.4)\n\nA equival\u00eancia entre (4.3) e (4.4) pode ser verificada coordenada por\ncoordenada. Note que se ??(?) ??(??/c), ent\u00e3o ?+? (?, ?, c) = ??(?), mas\npor outro lado,\n\n1\n2?\n{?\n\nmax[0, ?? + c??(?)]2 ? ?2?\n}?\n\n= ????(?) +\n1\n2\n\n???(?)2\n\n= ???+? (?, ?, c) +\n1\n2\n\n??+? (?, ?, c)\n2.\n\n\n\n64 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\nCaso contr\u00e1rio, ??(?) ??(??/c), ent\u00e3o ?+? (?, ?, c) = ?(??/c), enquanto\n1\n2?\n{?\n\nmax[0, ?? + c??(?)]2 ? ?2?\n}?\n\n=\n1\n2c\n\n(??2? )\n\n= ??\n(?\n???\n\nc\n\n)?\n+\n\n1\n2\n\nc\n(???\n\nc\n\n)?2\n= ???+? (?, ?, c) +\n\n1\n2\n\nc?+? (?, ?, c)\n2.\n\nEnt\u00e3o, os sub-problemas devem ser resolvidos utilizando o La-\ngrangiano aumentado dado por (4.3) ou (4.4), sem a necessidade de\nusar nenhuma das vari\u00e1veis adicionais ??. O problema continua com ?\nvari\u00e1veis.\n\nNo Ap\u00eandice A mostramos ainda, que se as fun\u00e7\u00f5es ? , ? e ? s\u00e3o\ncontinuamente diferenci\u00e1veis, ent\u00e3o o Lagrangiano (4.4) tamb\u00e9m o \u00e9.\n\nA atualiza\u00e7\u00e3o dos multiplicadores de Lagrange pode ser feita de\nforma semelhante ao caso irrestrito. Considerando que a minimiza\u00e7\u00e3o\nde ?(\u00b7, ??, ??, c) resulta em um ponto ??,\n\n??+1 =?? + c ??(??) (4.5a)\n??+1 =?? + c ??+(??, ??, ??). (4.5b)\n\nAinda, (4.5b) pode ser reescrito, coordenada por coordenada:\n\n??+1? = max{0, ?\n?\n? + c\n\n???(?)}. (4.6)\n\n4.1.3 Elimina\u00e7\u00e3o parcial das restri\u00e7\u00f5es\n\nNos casos anteriores, todas as restri\u00e7\u00f5es eram transformadas em\npenaliza\u00e7\u00f5es da fun\u00e7\u00e3o objetivo, e os sub-problemas consistiam em um\nproblema de minimiza\u00e7\u00e3o irrestrita do Lagrangiano aumentado. Por\u00e9m,\nn\u00e3o \u00e9 necess\u00e1rio fazer esta substitui\u00e7\u00e3o em todas as restri\u00e7\u00f5es. \u00c9 pos-\ns\u00edvel incluir no Lagrangiano aumentado apenas algumas, mantendo as\nrestantes nos sub-problemas, que ent\u00e3o as tratam de maneira expl\u00edcita.\n\nEm problemas com restri\u00e7\u00f5es lineares e n\u00e3o-lineares, \u00e9 poss\u00edvel\npor exemplo, incluir as n\u00e3o-lineares como penaliza\u00e7\u00f5es, mas seguir tra-\ntando as lineares explicitamente. Isto permitir\u00e1 utilizar os m\u00e9todos do\ncap\u00edtulo anterior na resolu\u00e7\u00e3o dos sub-problemas.\n\nPara o problema\n\nmin ? (?)\nsujeito a ?(?) = 0\n\n?(?) ? 0,\n\n\n\n4.2. Um algoritmo de Lagrangiano aumentado 65\n\nse incluirmos no Lagrangiano aumentado apenas as restri\u00e7\u00f5es referentes\n\u00e0 igualdade ?(?) = 0, chegamos ao sub-problema seguinte:\n\nmin\n??R?\n\n?(?, ?, c) = ? (?) + ?? ?(?) + c\n2\n??(?)?2\n\nsujeito a ?(?) ? 0.\n\nIsto n\u00e3o interfere na estrat\u00e9gia de atualiza\u00e7\u00e3o dos multiplicadores de\nLagrange, que pode ser mantida a mesma.\n\nDe modo geral, as restri\u00e7\u00f5es n\u00e3o inclu\u00eddas no Lagrangiano aumen-\ntado s\u00e3o mantidas nos sub-problemas. N\u00e3o h\u00e1 necessidade de manter as\nrestri\u00e7\u00f5es de desigualdades, incluindo no Lagrangiano as de igualdade.\nQualquer mistura das duas pode ser penalizada ou mantida [24].\n\nNeste trabalho, \u00e9 de interesse o caso em que as restri\u00e7\u00f5es mantidas\ns\u00e3o dadas por inequa\u00e7\u00f5es lineares, da forma ?? ? ? = ?(?) ? 0.\n\n4.2 UM ALGORITMO DE LAGRANGIANO AUMENTADO\n\nHavendo exposto a teoria sobre o m\u00e9todo de lagrangiano aumen-\ntado, apresentaremos a seguir um algoritmo completo (Algoritmo 4.1),\nbaseado em [26].\n\nO problema que queremos resolver cont\u00e9m restri\u00e7\u00f5es n\u00e3o-linea-\nres (a serem inclu\u00eddas no Lagrangiano aumentado) e restri\u00e7\u00f5es lineares\n(mantidas explicitamente). O problema \u00e9 dado por:\n\nmin ?\nsujeito a ?(?) ? 0\n\n?? ? ?.\n\nem que ? : R? ? R, ? : R? ? R?, ? ? R?\u00d7?, ? ? R?.\nEste algoritmo permite utilizar mais de um par\u00e2metro de pe-\n\nnaliza\u00e7\u00e3o c, permitindo que diferentes restri\u00e7\u00f5es tenham penaliza\u00e7\u00f5es\ndistintas. Para tanto, as ? restri\u00e7\u00f5es n\u00e3o-lineares s\u00e3o divididas em ?\ngrupos, denotados ?? , possivelmente segundo o tipo de n\u00e3o-linearidade.\nA cada grupo ?? , \u00e9 associada a penaliza\u00e7\u00e3o c? .\n\nA fun\u00e7\u00e3o Lagrangiano aumentado correspondente \u00e9 semelhante\n\u00e0 (4.4), com as restri\u00e7\u00f5es divididas nos grupos ?, cada um com um\npar\u00e2metro de penaliza\u00e7\u00e3o pr\u00f3prio:\n\n?(?, ?, c) =? (?) +\n???\n\n?=1\n\n1\n2c?\n\n??\n????\n\n(?{?\nmax[0, ?? + c? ??(?)]2 ? ?2?\n\n}?)?\n.\n\n\n\n66 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\nO sub-problema a ser resolvido envolve as restri\u00e7\u00f5es lineares, que\nn\u00e3o foram inclu\u00eddas no Lagrangiano aumentado:\n\nmin\n??R?\n\n?(?, ?, c) (4.7a)\n\nsujeito a ?? ? ?. (4.7b)\n\nA cada itera\u00e7\u00e3o ?, o iterando ?? \u00e9 calculado, resolvendo-se o\nsub-problema aproximadamente, utilizando-se como crit\u00e9rio de parada\na norma da proje\u00e7\u00e3o do gradiente da fun\u00e7\u00e3o no cone tangente ao espa\u00e7o\nvi\u00e1vel\n\n????(?,?? ) [???(?, ?, c)]?? ??, (4.8)\n\nem que ?? determina a exatid\u00e3o com que se resolve o problema.\nOs multiplicadores de Lagrange e os par\u00e2metros de penaliza\u00e7\u00e3o\n\ns\u00e3o atualizados de forma alternada. Caso as restri\u00e7\u00f5es inclu\u00eddas no\nLagrangiano aumentado estejam suficientemente satisfeitas, isto \u00e9\n\n?max{?(?)[?? ], 0}?? ??, (4.9)\n\nem que ?? > 0 e o sub-\u00edndice [?? ] denota uma parti\u00e7\u00e3o do vetor associ-\nada ao grupo de restri\u00e7\u00f5es [?? ], o par\u00e2metro de penaliza\u00e7\u00e3o do grupo\ncorrespondente \u00e9 mantido o mesmo,\n\nc ?+1? = c\n?\n? ,\n\ne os multiplicadores de Lagrange s\u00e3o atualizados, de forma semelhante\n\u00e0 (4.6):\n\n??+1[?? ] = max{0, ?\n?\n[?? ] + c\n\n?\n? ?[?? ](?\n\n?)}, ? = 1, . . . , ?.\n\nCaso as restri\u00e7\u00f5es n\u00e3o sejam satisfeitas conforme (4.9), os multi-\nplicadores de Lagrange s\u00e3o mantidos\n\n??+1[?? ] = ?\n?\n[?? ]\n\ne os par\u00e2metros de penaliza\u00e7\u00e3o correspondentes s\u00e3o atualizados con-\nforme a f\u00f3rmula\n\nc ?+1? = ?\n?\n? c\n\n?\n?\n\nem que\n\n? ?? =\n{?\n\n? se c ?? = ??\nmax(?, ??) caso contr\u00e1rio\n\n\n\n4.2. Um algoritmo de Lagrangiano aumentado 67\n\ne ?? \u00e9 o menor dos par\u00e2metros de penaliza\u00e7\u00e3o ao in\u00edcio da itera\u00e7\u00e3o:\n\n?? = min(c ?1 , . . . , c\n?\n? ).\n\nDessa maneira, caso os sub-problemas n\u00e3o cheguem a satisfazer suficien-\ntemente as restri\u00e7\u00f5es, os par\u00e2metros de penaliza\u00e7\u00e3o c correspondentes\ns\u00e3o progressivamente aumentados. No limite, c ? ?, que \u00e9 uma das\nformas de garantir a converg\u00eancia do m\u00e9todo.\n\nAinda, resta garantir que os ?? e ?? convirjam para zero, de modo\nque, no limite, as restri\u00e7\u00f5es sejam satisfeitas e o ponto encontrado seja\n\u00f3timo para a fun\u00e7\u00e3o original ? .\n\n\n\n68 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\nAlgoritmo 4.1: M\u00e9todo de Lagrangiano Aumentado, baseado\nem [26]\n\nInicializa\u00e7\u00e3o: defina as constantes positivas: ?* ? 1, ?* ? 1,\n? > 1, ?? &lt;1 e ?? &lt;1. Seja ? = 0, ?0 = min?=1,...,? c 0? , ?0 = ?0,\n?0 = (??)?? ;\nfor k = 1, 2, . . . do\n\nResolva o sub-problema (4.7) de modo a encontrar um ponto\n?? satisfazendo\n\n????(?,?? ) [???(?, ?, c)]?? ??\n\nCaso ????(?,?? ) [???(?, ?, c)]?? ?*| e\n?max{?(?)[?? ], 0}?? ?*, encerre.\nfor ? = 1, . . . , ? do\n\nif ?max{?(?)[?? ], 0}?? ?? then\nAtualize os multiplicadores de Lagrange\n\nc ?+1? = c\n?\n? ,\n\n??+1[?? ] = max{0, ?\n?\n[?? ] + c\n\n?\n? ?[?? ](?\n\n?)}, ? = 1, . . . , ?.\n\nelse\nAumente as penaliza\u00e7\u00f5es\n\n??+1[?? ] = ?\n?\n[?? ]\n\nc ?+1? = ?\n?\n? c\n\n?\n?\n\n??+1 = min\n?=1,...,?\n\nc ?+1?\n\nif ??+1 > ?? then\n\n??+1 =\n1\n\n??+1\n,\n\n??+1 =\n1\n\n(??+1)??\n.\n\nelse\n\n??+1 = ??\n1\n\n??+1\n,\n\n??+1 = ??\n1\n\n(??+1)??\n.\n\n\n\n4.3. Resolvendo os sub-problemas com busca direta direcional 69\n\n4.3 RESOLVENDO OS SUB-PROBLEMAS COM BUSCA DIRETA DI-\nRECIONAL\n\nO algoritmo apresentado na se\u00e7\u00e3o anterior \u00e9 bastante pr\u00e1tico, no\nsentido de exigir, na resolu\u00e7\u00e3o dos sub-problemas, crit\u00e9rios de parada\nque s\u00e3o usuais em algoritmos de otimiza\u00e7\u00e3o. Isto permitiu a [27] adapt\u00e1-\nlo para utilizar a busca direta direcional na resolu\u00e7\u00e3o dos sub-problemas.\n\nEm primeiro lugar, \u00e9 necess\u00e1rio resolver os sub-problemas com a\nexatid\u00e3o desejada. Um resultado an\u00e1logo ao Teorema 9 fornece uma cota\nsuperior para ????(?? ,?? )\n\n[?\n??? (??)\n\n]?\n? em fun\u00e7\u00e3o do tamanho do passo\n\n??. Dessa maneira, como crit\u00e9rio de parada para os sub-problemas,\npodemos utilizar\n\n?? ? ??\n\nem que ?? faz o papel de ??.\nO mesmo pode ser feito com o crit\u00e9rio de parada do algoritmo\n\nde Lagrangiano aumentado que passa a ser:\n\n?? ? ?* e ?max(?(?), 0)?? ?*.\n\nResta definir a forma de atualizar as toler\u00e2ncias ?? dos sub-pro-\nblemas que seja equivalente \u00e0 do algoritmo da se\u00e7\u00e3o anterior.\n\nPara tanto, considere ???? ? 1, e defina a fun\u00e7\u00e3o\n\n?(?, c) = max\n{?\n\n1,\n(?\n\n1 + ??? +\n???\n\n?=1\nc?\n\n)?\n/????\n\n}?\n.\n\nEm que ?max limita superiormente os comprimentos das dire\u00e7\u00f5es\nde busca ???? ? ?max. Considere que a toler\u00e2ncia ?0 utilizada para\na detec\u00e7\u00e3o de restri\u00e7\u00f5es quase-ativas, e consequentemente ?(?, ?) e\n? ?(?, ?), \u00e9 dada por ?0 = min{?max, ?max?}.\n\nEnt\u00e3o a toler\u00e2ncia para os sub-problemas \u00e9 atualizada conforme\na regra [27]:\n\n??+1 = ??+1/(?max?(??+1, c ?+1)).\n\n4.4 RESOLVENDO OS SUB-PROBLEMAS COM REGI\u00c3O DE CONFI-\nAN\u00c7A N\u00c3O-DIFERENCI\u00c1VEL\n\nPara o uso do algoritmo de Lagrangiano aumentado proposto, o\nAlgoritmo 3.2, de regi\u00e3o de confian\u00e7a com restri\u00e7\u00f5es lineares pode ser\nutilizado sem grandes altera\u00e7\u00f5es.\n\nO crit\u00e9rio de parada usual do algoritmo \u00e9 o raio da regi\u00e3o de\nconfian\u00e7a, mas vamos propor uma alternativa pr\u00f3xima ao proposto na\n\n\n\n70 Cap\u00edtulo 4. Otimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\nse\u00e7\u00e3o 4.2. Considere ???? ? ??. Vamos mostrar que, sob certas condi\u00e7\u00f5es,\npodemos considerar um crit\u00e9rio de parada baseado no modelo ?:\n\n????(?,?? ) [???(?)]?? ????.\nResta mostrar que este novo crit\u00e9rio prov\u00ea a exatid\u00e3o necess\u00e1ria\n\npara a converg\u00eancia do algoritmo. Se o modelo for ?-posicionado, de\n(2.16), temos:\n\n??? ?? ????(?, ?, c) ???(?)?? (4.10)\n?????(?,?? ) [??(?, ?, c)] ? ???(?,?? ) [??(?)]?? (4.11)\n?????(?,?? ) [??(?, ?, c)]??????(?,?? ) [??(?)]?? (4.12)\n?????(?,?? ) [??(?, ?, c)]?? ?? (4.13)\n\nem que utilizamos o fato de que proje\u00e7\u00f5es em conjuntos convexos en-\ncurtam dist\u00e2ncias [28] e uma desigualdade triangular. Ent\u00e3o, o modelo\n? pode ser utilizado no crit\u00e9rio de parada, j\u00e1 que:\n\n????(?,?? ) [??(?, ?, c)]?? ???? + ??? ??.\n\nEnt\u00e3o, se for escolhido ???? de modo que\n\n?? ? ???? + ??? ?? (4.14)\na cota 4.8 \u00e9 satisfeita. O valor de ? pode n\u00e3o ser conhecido, mas \u00e9 finito.\nJ\u00e1 o valor de ??? depende do condicionamento da fun\u00e7\u00e3o ?(\u00b7, ?, c), que\ndepende da penaliza\u00e7\u00e3o c. Se os multiplicadores ? estiverem convergindo\npara seus valores corretos, os par\u00e2metros de penaliza\u00e7\u00e3o c permanecer\u00e3o\nlimitados. Nesse caso, com um ? suficientemente pequeno (ou decres-\ncente, a fim de que ??? ?? ? 0) este crit\u00e9rio de parada alternativo \u00e9\nv\u00e1lido.\n\nCaso isto n\u00e3o ocorra, e c crescer indefinidamente, este crit\u00e9rio\nn\u00e3o pode ser usado. Nesta situa\u00e7\u00e3o, por\u00e9m, a dificuldade est\u00e1 em fazer\nque os modelos ? sejam boas aproxima\u00e7\u00f5es da fun\u00e7\u00e3o Lagrangiano\naumentado ?(\u00b7, ?, c). Ent\u00e3o, o algoritmo todo ter\u00e1 dificuldade em con-\nvergir e mesmo o crit\u00e9rio de parada usual (baseado apenas no raio da\nregi\u00e3o) pode apresentar problemas.\n\n4.5 SUM\u00c1RIO\n\nNeste cap\u00edtulo apresentamos o m\u00e9todo do Lagrangiano aumen-\ntado para a resolu\u00e7\u00e3o de problemas de otimiza\u00e7\u00e3o com restri\u00e7\u00f5es n\u00e3o-\nlineares. O m\u00e9todo substitui restri\u00e7\u00f5es do problema original em penali-\nza\u00e7\u00f5es para a fun\u00e7\u00e3o objetivo.\n\n\n\n4.5. Sum\u00e1rio 71\n\nA fun\u00e7\u00e3o lagrangiano aumentado inclui penaliza\u00e7\u00f5es para as vio-\nla\u00e7\u00f5es de restri\u00e7\u00f5es e estimativas para os multiplicadores de Lagrange\ndo problema original. O m\u00e9todo consiste em resolver uma sequ\u00eancia de\nsubproblemas de otimiza\u00e7\u00e3o do Lagrangiano aumentado, atualizando-\nse as estimativas dos multiplicadores de Lagrange e, eventualmente\naumentando as penaliza\u00e7\u00f5es associadas \u00e0s restri\u00e7\u00f5es.\n\nO m\u00e9todo, inicialmente para tratar problemas com restri\u00e7\u00f5es de\nigualdades, j\u00e1 foi estendido para o tratamento de desigualdades, neste\ncap\u00edtulo apresentamos o caso unilateral, mas tamb\u00e9m \u00e9 poss\u00edvel tratar\ndesigualdades como ? ? ?? ? ? sem a necessidade de introdu\u00e7\u00e3o de\nmais multiplicadores.\n\nPor fim, apresentamos um algoritmo desta classe em que as restri-\n\u00e7\u00f5es lineares n\u00e3o s\u00e3o inclu\u00eddas como penaliza\u00e7\u00f5es, podendo ser tratadas\nexplicitamente na otimiza\u00e7\u00e3o do Lagrangiano aumentado. Mostramos\ncomo os sub-problemas podem ser resolvidos utilizando os algoritmos\nn\u00e3o-diferenci\u00e1veis do cap\u00edtulo anterior.\n\n\n\n\n\n5 AN\u00c1LISE COMPUTACIONAL\n\nNeste cap\u00edtulo fazemos uma an\u00e1lise do desempenho dos algorit-\nmos de busca direta direcional e regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel\npara a resolu\u00e7\u00e3o de problemas com restri\u00e7\u00f5es lineares nas vari\u00e1veis. Es-\npecificamente, problemas envolvendo a aloca\u00e7\u00e3o de g\u00e1s, sujeitos a uma\ndisponibilidade limitada de g\u00e1s para gas-lift.\n\nPrimeiramente, apresentamos um conjunto de problemas que foi\nresolvido empregando-se os dois m\u00e9todos, com os dados de [29]. Em\nseguida, apresentamos a resolu\u00e7\u00e3o de um problema em que o m\u00e9todo de\nregi\u00e3o de confian\u00e7a empregou diretamente o simulador para a avalia\u00e7\u00e3o\nda fun\u00e7\u00e3o objetivo.\n\n5.1 OTIMIZA\u00c7\u00c3O DE FUN\u00c7\u00c3O SUAVE\n\nO problema resolvido na presente se\u00e7\u00e3o consiste em alocar g\u00e1s\nde gas-lift para um conjunto de po\u00e7os, com restri\u00e7\u00f5es lineares\n\nmax ? (?inj)\nsujeito a ??inj ? ?\n\nem que ?inj = (?1inj, . . . , ??inj) \u00e9 o vetor de vaz\u00f5es alocadas de g\u00e1s de\neleva\u00e7\u00e3o,\n\n? =\n\n?\n? ?????\n\ne1\u00d7?\n\n?\n?\n\n? = (?1, . . . , ?? ,??1, . . . ,??? , ?max), ?? \u00e9 a matriz identidade de ordem\n? , e ?1\u00d7? \u00e9 um vetor linha com todos os elementos iguais a 1. Os valores\ndas restri\u00e7\u00f5es foram ?1 = \u00b7 \u00b7 \u00b7 = ?? = 4000 Mscf/d e ?1 = \u00b7 \u00b7 \u00b7 = ?? =\n80 Mscf/d. Foram resolvidos 10 casos diferentes do problema, com a\ndisponibilidade total de g\u00e1s do campo variada logaritmicamente de um\nvalor muito restrito ?max = 2.800 Mscf/d, at\u00e9 ?max = 28.000 Mscf/d,\nem que o \u00f3timo irrestrito se torna vi\u00e1vel. Desta maneira obtivemos dez\ndiferentes vers\u00f5es do problema. Ainda, para cada vers\u00e3o, resolvemos o\nproblema partindo de 10 diferentes pontos iniciais.\n\nA fun\u00e7\u00e3o ? , como j\u00e1 apresentado, \u00e9 da forma\n\n? =\n???\n\n?=1\n\n(?\n???\n\n?\n? (?\n\n?\ninj) + ?? ?\n\n?\n? (?\n\n?\ninj) ? ?????(??inj)\n\n)?\n?\n\n???\n?=1\n\n?inj?\n?\ninj\n\nneste caso, por\u00e9m, utilizamos a aproxima\u00e7\u00e3o de [14] para modelar a\nprodu\u00e7\u00e3o total de cada po\u00e7o:\n\n??? (?\n?\ninj) = ?\n\n?\n1 + ?\n\n?\n2 ?\n\n?\ninj + ?\n\n?\n3 (?\n\n?\ninj)\n\n2 + ??4 ln(?\n?\ninj + 1).\n\n73\n\n\n\n74 Cap\u00edtulo 5. An\u00e1lise computacional\n\nOs par\u00e2metros ?1, . . . , ?4, foram identificados de um problema [3], adap-\ntado em [2], utilizando m\u00ednimos quadrados lineares, e est\u00e3o listados na\nTabela 5.1. Os valores das vaz\u00f5es de \u00f3leo ??? , g\u00e1s ??? e \u00e1gua ??? produ-\nzidas foram calulados, para cada po\u00e7o, utilizando os valores de GOR\n(rela\u00e7\u00e3o g\u00e1s-\u00f3leo) e Water cut (propor\u00e7\u00e3o de \u00e1gua na fase l\u00edquida) fixos,\nconforme a Tabela 5.2.\n\nPara cada um dos cen\u00e1rios propostos, a disponibilidade total\nde g\u00e1s para gas-lift e o valor \u00f3timo da fun\u00e7\u00e3o objetivo s\u00e3o dados na\nTabela 5.3. O ponto \u00f3timo foi calculado numericamente, utilizando-se\num algoritmo baseado em derivadas.\n\nDesta maneira, podemos testar o desempenho dos algoritmos para\nfun\u00e7\u00f5es suaves (em que h\u00e1 garantia te\u00f3rica de converg\u00eancia) mantendo\na estrutura do problema de interesse.\n\n5.1.1 Busca direta direcional\n\nNo algoritmo de busca direta direcional, na itera\u00e7\u00e3o ?, o conjunto\nde dire\u00e7\u00f5es de busca ?? gera ? ?(?, ?) para 0 &lt;? &lt;??, com ?0 = 0,5.\nPara tanto, utilizamos o procedimento do Teorema 8, colocando os\ngeradores dos cones ?(?, ?) como as colunas de uma matriz ? . Para o\nc\u00e1lculo dos vetores ?? mencionados no Teorema utilizamos uma base\nortogonal que gera o espa\u00e7o nulo de ? ? por combina\u00e7\u00f5es lineares e\na completamos com os negativos de todas as dire\u00e7\u00f5es, como em (2.5),\nde modo a ter uma base positiva. A estes vetores acrescentamos os\n??, colunas de ? (? ? ? )?1, com seus negativos ???, de modo a ter um\nconjunto ?? satisfazendo a Condi\u00e7\u00e3o 2 (p. 50).\n\nComo fun\u00e7\u00e3o for\u00e7ante, que determina o descenso m\u00ednimo para a\naceita\u00e7\u00e3o do passo, foi utilizada ?(?) = 14 ?\n\n2. O par\u00e2metro de controle\ndo tamanho do passo foi iniciado em ?0 = 1. O crit\u00e9rio de parada da\nbusca direta direcional \u00e9 dado pelo tamanho deste par\u00e2metro, enquanto\nque o crit\u00e9rio usado pelo m\u00e9todo de regi\u00e3o de confian\u00e7a sem derivadas \u00e9\no raio da regi\u00e3o. Para obtermos crit\u00e9rios de parada equivalentes, ambos\nos m\u00e9todos foram aplicados ao problema com um crit\u00e9rio de parada\nexigente, de modo a satisfazer com folga \u00e0 exatid\u00e3o desejada, de erro\nmenor que 1 em cada coordenada. A partir da\u00ed, foram derivados os\ncrit\u00e9rios de parada de um e de outro algoritmo. Para o de busca direta\ndirecional,\n\n?? &lt;???? = 0,0038.\nNa Tabela 5.4 apresentamos um sum\u00e1rio das resolu\u00e7\u00f5es dos pro-\n\nblemas, considerando, para cada cen\u00e1rio, as m\u00e9dias dos tempos e dos\nn\u00fameros de avalia\u00e7\u00f5es das fun\u00e7\u00f5es objetivos. Na segunda parte da ta-\nbela, tendo em vista que o valor \u00f3timo ?*inj \u00e9 conhecido (Tabela 5.3),\n\n\n\n5.1. Otimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave 75\n\nTabela 5.1: Par\u00e2metros do problema resolvido. ??? (??inj) = ??1 + ??2 ??inj +\n??3 (??inj)2 + ??4 log(??inj + 1).\n\nPo\u00e7o ?1 ?2 ?3 ?4\n1 ?1081 ?0,2559 1,400 \u00d7 10?5 398,6\n2 ?1132 ?0,2092 ?8,202 \u00d7 10?6 471,5\n3 ?1357 ?0,3079 6,750 \u00d7 10?7 559,5\n4 ?1191 ?0,2580 2,236 \u00d7 10?6 476,7\n5 ?1297 ?0,3074 1,687 \u00d7 10?5 478,4\n6 ?1357 ?0,2505 ?9,927 \u00d7 10?6 565,5\n7 ?1629 ?0,3698 9,045 \u00d7 10?7 671,5\n8 ?1282 ?0,2908 6,551 \u00d7 10?6 504,7\n9 ?1791 ?0,4068 9,881 \u00d7 10?7 738,5\n10 ?594,0 ?0,1285 9,991 \u00d7 10?7 238,1\n11 ?1084 ?0,2482 6,276 \u00d7 10?6 425,4\n12 ?1433 ?0,3254 7,900 \u00d7 10?7 590,8\n13 ?1151 ?0,2798 1,952 \u00d7 10?5 413,3\n14 ?1058 ?0,0749 ?3,558 \u00d7 10?5 443,7\n15 ?1059 ?0,1566 ?1,931 \u00d7 10?5 494,3\n16 ?1266 ?0,2569 2,363 \u00d7 10?6 489,8\n17 ?1293 ?0,3010 1,745 \u00d7 10?5 474,9\n18 ?1121 ?0,1566 ?2,101 \u00d7 10?5 517,7\n19 ?1182 ?0,1893 ?2,440 \u00d7 10?5 534,5\n20 ?1019 ?0,1601 ?1,402 \u00d7 10?5 445,2\n21 ?1725 ?0,3515 ?1,706 \u00d7 10?5 724,5\n22 ?478,7 ?0,0871 ?3,015 \u00d7 10?6 214,3\n23 ?843,3 ?0,1463 ?1,170 \u00d7 10?5 377,5\n24 ?1072 ?0,1564 ?2,598 \u00d7 10?5 517,6\n25 ?1121 ?0,1566 ?2,101 \u00d7 10?5 517,7\n26 ?1058 ?0,0749 ?3,558 \u00d7 10?5 443,7\n27 ?1059 ?0,1566 ?1,931 \u00d7 10?5 494,3\n28 ?1266 ?0,2569 2,363 \u00d7 10?6 489,8\n29 ?1191 ?0,2580 2,236 \u00d7 10?6 476,7\n30 ?1297 ?0,3074 1,687 \u00d7 10?5 478,4\n31 ?1357 ?0,2505 ?9,927 \u00d7 10?6 565,5\n32 ?1629 ?0,3698 9,045 \u00d7 10?7 671,5\n\n\n\n76 Cap\u00edtulo 5. An\u00e1lise computacional\n\nTabela 5.2: Valores de GOR e Water cut dos po\u00e7os considerados\nPo\u00e7o GOR Water cut (%)\n\n1 0.286 12,5\n2 0.227 9,6\n3 0.385 13,3\n4 0.308 18,7\n5 0.500 14,3\n6 0.150 9,1\n7 0.462 7,1\n8 0.212 3,6\n9 0.417 20,0\n10 0.329 9,1\n11 0.385 13,3\n12 0.267 6,3\n13 0.282 11,9\n14 0.208 8,6\n15 0.232 4,5\n16 0.235 12,3\n17 0.269 5,6\n18 0.317 6,4\n19 0.278 12,7\n20 0.333 6,1\n21 0.161 10,2\n22 0.137 11,8\n23 0.194 15,0\n24 0.262 5,6\n25 0.431 9,7\n26 0.276 31,0\n27 0.281 22,0\n28 0.088 1,1\n29 1.429 30,0\n30 1.000 33,3\n31 0.167 33,3\n32 0.750 42,9\n\n\n\n5.1. Otimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave 77\n\nTabela 5.3: Cen\u00e1rios propostos, com valores das solu\u00e7\u00f5es.\nCen\u00e1rio ?max ? (?*inj)\n\n1 2,80 \u00d7 103 4,36 \u00d7 105\n2 3,62 \u00d7 103 4,87 \u00d7 105\n3 4,67 \u00d7 103 5,35 \u00d7 105\n4 6,03 \u00d7 103 5,79 \u00d7 105\n5 7,79 \u00d7 103 6,21 \u00d7 105\n6 1,01 \u00d7 104 6,57 \u00d7 105\n7 1,30 \u00d7 104 6,89 \u00d7 105\n8 1,68 \u00d7 104 7,13 \u00d7 105\n9 2,17 \u00d7 104 7,28 \u00d7 105\n10 2,80 \u00d7 104 7,31 \u00d7 105\n\napresentamos o erro m\u00e9dio entre os valores produzidos ao final do al-\ngoritmo com este, tanto no valor da fun\u00e7\u00e3o objetivo (? (?*inj) ? ? (?inj)),\nquanto na dist\u00e2ncia entre as solu\u00e7\u00f5es (??*inj ? ?inj??). Os erros s\u00e3o bai-\nxos, tendo-se em considera\u00e7\u00e3o os valores \u00f3timos conhecidos na Tabela\n5.3.\n\nDe modo geral, o caso mais restrito teve uma converg\u00eancia mais\nr\u00e1pida, necessitou de menos avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo, e induziu\nerros menores. Provavelmente devido \u00e0 proximidade entre o ponto inicial\ne o ponto \u00f3timo.\n\nNa Figura 5.1 mostramos o valor assumido pela fun\u00e7\u00e3o objetivo\nno iterando corrente durante uma das execu\u00e7\u00f5es do algoritmo em tr\u00eas\ndos cen\u00e1rios apresentados. Os valores da fun\u00e7\u00e3o objetivo foram nor-\nmalizados, considerando-se 0 o ponto inicial e 1 o ponto \u00f3timo. Como\no ponto inicial \u00e9 vari\u00e1vel, a normaliza\u00e7\u00e3o \u00e9 diferente para cada curva.\nPercebemos que a evolu\u00e7\u00e3o do valor \u00e9 irregular, e fica mais lenta nas\nproximidades do ponto \u00f3timo.\n\nPara um dos casos, apresentamos a dist\u00e2ncia Euclidiana ao ponto\n\u00f3timo, na Figura 5.2, em que verificamos o mesmo comportamento, mais\nlento pr\u00f3ximo ao ponto \u00f3timo.\n\n\n\n78 Cap\u00edtulo 5. An\u00e1lise computacional\n\n10?2 10?1 100 101 102 103\n0\n\n0,2\n\n0,4\n\n0,6\n\n0,8\n\n1\n\nTempo [s]\n\n?\n(?\n\n)\n\nCen\u00e1rio 1\nCen\u00e1rio 6\nCen\u00e1rio 10\n\nFigura 5.1: Valor da fun\u00e7\u00e3o objetivo durante a busca direta direcional,\nnormalizado de 0 a 1.\n\n10?1 100 101 102 103\n0\n\n100\n\n200\n\n300\n\n400\n\nTempo [s]\n\ne\n\nDist\u00e2ncia euclidiana ao ponto \u00f3timo\n\nFigura 5.2: Dist\u00e2ncia Euclidiana entre o ponto \u00f3timo e o iterando\ncorrente da busca direta direcional.\n\n\n\n5.1. Otimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave 79\n\nTabela 5.4: Resolu\u00e7\u00e3o de todos os cen\u00e1rios usando busca direta direcio-\nnal. M\u00e9dia dos casos em cada cen\u00e1rio.\n\nCen\u00e1rio Tempo Avalia\u00e7\u00f5es ? ? (?*inj) ? ? (?inj) ??*inj ? ?inj??\n1 7s 8.183 4.58 \u00d7 10?5 4.40 \u00d7 10?3\n2 14s 22.087 8.66 \u00d7 10?5 6.36 \u00d7 10?3\n3 24s 40.629 8.93 \u00d7 10?5 8.77 \u00d7 10?3\n4 42s 70.664 1.43 \u00d7 10?4 1.18 \u00d7 10?2\n5 67s 116.602 2.79 \u00d7 10?4 2.27 \u00d7 10?2\n6 114s 196.331 3.18 \u00d7 10?4 4.37 \u00d7 10?2\n7 199s 339.503 5.04 \u00d7 10?4 9.27 \u00d7 10?2\n8 306s 517.005 1.05 \u00d7 10?3 1.36 \u00d7 10?1\n9 484s 819.350 3.08 \u00d7 10?3 2.98 \u00d7 10?1\n10 598s 1.100.179 4.30 \u00d7 10?3 1.88 \u00d7 10?1\n\n\n\n80 Cap\u00edtulo 5. An\u00e1lise computacional\n\n5.1.2 Regi\u00e3o de confian\u00e7a\n\nNesta aplica\u00e7\u00e3o, foram consideradas regi\u00f5es de confian\u00e7a dadas\npor bolas na norma ?? em torno do iterando:\n\n??(??, ??) = {? : ?? ? ???? ? ??}.\n\nOs modelos usados foram polinomiais de segunda ordem, com\nmatriz Hessiana diagonal ??:\n\n?(?? + ?) = ?(??) +\n1\n2\n\n?? ??? + ??? ?, ? ???(??, ??).\n\nO raio inicial foi ?0 = 1 e para a aceita\u00e7\u00e3o do passo foram usados\nos coeficientes ?1 = 0,4 e ?0 = 0, de modo que, mesmo descenso simples\n\u00e9 aceito, contanto que o modelo seja ?-posicionado.\n\nO raio foi aumentado e diminu\u00eddo utilizando-se os par\u00e2metros\n?inc = 2 e ? = 0,5, respectivamente, que foram mantidos constantes\nao longo do algoritmo. O teste de criticidade foi executado com ??&lt;\n?0 = 32, usando ? = 10 e ? = 9. Os passos de teste foram calculados\nresolvendo-se o sub-problema da regi\u00e3o de confian\u00e7a utilizando a parte\nlinear dos modelos.\n\nComo no caso anterior, o crit\u00e9rio de parada foi calculado para\nequivaler em exatid\u00e3o ao resultado alcan\u00e7ado pela busca direta direcio-\nnal, na se\u00e7\u00e3o anterior. Chegamos ao crit\u00e9rio\n\n?? &lt;???? = 0,0156.\n\nApresentamos um sum\u00e1rio da resolu\u00e7\u00e3o dos problemas na Tabela\n5.5. Para cada cen\u00e1rio \u00e9 apresentada a m\u00e9dia entre todos os pontos\niniciais. Tanto o tempo como o n\u00famero de avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo\nforam pouco sens\u00edveis \u00e0 restri\u00e7\u00e3o de disponibilidade de g\u00e1s para gas-lift.\n\nNa Figura 5.3 apresentamos a evolu\u00e7\u00e3o do valor corrente da fun-\n\u00e7\u00e3o objetivo, normalizado de 0 a 1. \u00c9 mostrado apenas o caso mediano\nde tr\u00eas dos cen\u00e1rios. Dos tr\u00eas, o caso mais restrito foi o mais r\u00e1pido de\nser resolvido, seguido do menos restrito. O caso intermedi\u00e1rio acabou\nlevando mais tempo. Nesta figura, ainda, para o Cen\u00e1rio 1, \u00e9 poss\u00edvel\nnotar um per\u00edodo em que a fun\u00e7\u00e3o objetivo fica num mesmo valor. Pode-\nmos especular que em tal momento foram necess\u00e1rias v\u00e1rias execu\u00e7\u00f5es\ndo algoritmo de melhora de modelo, ou do passo de criticidade, que\npodem ser custosos e, em si, n\u00e3o trazem melhora na fun\u00e7\u00e3o objetivo.\n\nA dist\u00e2ncia euclidiana ao ponto \u00f3timo durante as itera\u00e7\u00f5es de um\ndos casos \u00e9 mostrada na Figura 5.4, em um gr\u00e1fico linear. O algoritmo\nchega rapidamente na proximidade do ponto \u00f3timo, onde \u00e9 gasta a\nmaior parte do tempo.\n\n\n\n5.1. Otimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave 81\n\n10?2 10?1 100 101 102 103\n0\n\n0,2\n\n0,4\n\n0,6\n\n0,8\n\n1\n\nTempo [s]\n\n?\n(?\n\n)\n\nCen\u00e1rio 1\nCen\u00e1rio 6\nCen\u00e1rio 10\n\nFigura 5.3: Valor da fun\u00e7\u00e3o objetivo durante a execu\u00e7\u00e3o do algoritmo\nde regi\u00e3o de confian\u00e7a, normalizado de 0 a 1.\n\nTabela 5.5: Resolu\u00e7\u00e3o de todos os cen\u00e1rios usando regi\u00e3o de confian\u00e7a.\nM\u00e9dia dos casos em cada cen\u00e1rio.\n\nCen\u00e1rio Tempo Avalia\u00e7\u00f5es ? ? (?*inj) ? ? (?inj) ??*inj ? ?inj??\n1 4s 755 2.33 \u00d7 10?3 3.37 \u00d7 10?2\n2 17s 4.602 2.10 \u00d7 10?2 2.73 \u00d7 10?1\n3 19s 5.327 9.39 \u00d7 10?3 2.12 \u00d7 10?1\n4 27s 7.712 7.66 \u00d7 10?3 2.58 \u00d7 10?1\n5 28s 7.725 4.67 \u00d7 10?3 2.68 \u00d7 10?1\n6 23s 6.447 2.20 \u00d7 10?3 2.31 \u00d7 10?1\n7 19s 5.287 9.62 \u00d7 10?4 1.93 \u00d7 10?1\n8 18s 4.995 6.25 \u00d7 10?4 2.19 \u00d7 10?1\n9 17s 4.636 3.34 \u00d7 10?4 1.95 \u00d7 10?1\n10 16s 4.567 3.79 \u00d7 10?4 2.40 \u00d7 10?1\n\nComparando com a busca direta direcional, h\u00e1 uma grande redu-\n\u00e7\u00e3o no tempo de execu\u00e7\u00e3o e, principalmente, no n\u00famero de avalia\u00e7\u00f5es da\nfun\u00e7\u00e3o objetivo. Essa redu\u00e7\u00e3o ficar\u00e1 mais relevante quando tais fun\u00e7\u00f5es\nforem obtidas como resultado de um c\u00e1lculo de simulador.\n\n\n\n82 Cap\u00edtulo 5. An\u00e1lise computacional\n\n0 5 10 15\n0\n\n100\n\n200\n\n300\n\n400\n\nTempo [s]\n\ne\n\nDist\u00e2ncia euclidiana ao ponto \u00f3timo\n\nFigura 5.4: Dist\u00e2ncia Euclidiana entre o valor \u00f3timo encontrado e o\nvalor do iterando corrente durante a execu\u00e7\u00e3o do algoritmo de regi\u00e3o\nde confian\u00e7a.\n\n\n\n5.2. Otimiza\u00e7\u00e3o baseada no simulador 83\n\n5.2 OTIMIZA\u00c7\u00c3O BASEADA NO SIMULADOR\n\nNa se\u00e7\u00e3o anterior aplicamos os m\u00e9todos de otimiza\u00e7\u00e3o estudados\npara a otimiza\u00e7\u00e3o em um campo de petr\u00f3leo em que a produ\u00e7\u00e3o era\nmodelada por fun\u00e7\u00f5es suaves. Naquele caso, a teoria relacionada aos\nm\u00e9todos era capaz de garantir a converg\u00eancia.\n\nNa presente se\u00e7\u00e3o faremos a otimiza\u00e7\u00e3o da aloca\u00e7\u00e3o de g\u00e1s em um\ncampo de produ\u00e7\u00e3o de petr\u00f3leo utilizando diretamente um simulador\npara os po\u00e7os e sistemas relacionados. N\u00e3o apenas o modelo \u00e9 mais\ncompleto, mas tamb\u00e9m h\u00e1 presen\u00e7a de ru\u00eddo, proveniente do c\u00e1lculo das\nsimula\u00e7\u00f5es, realizado iterativamente. Como a utiliza\u00e7\u00e3o do simulador\ntorna a resolu\u00e7\u00e3o mais lenta, optamos por realizar os experimentos\napenas com o m\u00e9todo de regi\u00e3o de confian\u00e7a.\n\nConsideramos o cen\u00e1rio de [7], com a diferen\u00e7a que sua abor-\ndagem envolvia sofisticados modelos linearizados por partes, enquanto\nnossa ser\u00e1 com um simulador num\u00e9rico.\n\nO campo de produ\u00e7\u00e3o consiste de 16 po\u00e7os (Figura 5.5), com a\nprodu\u00e7\u00e3o direcionada para dois manifolds, e cada manifold direciona\nsua produ\u00e7\u00e3o a um separador. Os po\u00e7os numerados de 1 a 8 localizam-\nse a 1 km do manifold 1, e 10 km do manifold 2, enquanto os po\u00e7os de\n9 a 16, est\u00e3o a 1 km do manifold 2 e 10 km do manifold 1.\n\nFigura 5.5: O g\u00e1s para gas-lift deve ser alocado a 16 po\u00e7os, cuja produ\u00e7\u00e3o\n\u00e9 distribu\u00edda entre 2 manifolds [7].\n\nComo nosso estudo n\u00e3o \u00e9 capaz de otimizar o roteamento entre\npo\u00e7os e manifolds, mantivemos este constante, conforme a Tabela 5.6.\n\nA tubula\u00e7\u00e3o ligando o manifold 1 ao seu separador tem 100 m,\nenquanto a tubula\u00e7\u00e3o entre o manifold 2 e seu separador tem 50 m, com\ndi\u00e2metro interno de 4,5 in e rugosidade absoluta de ? = 0,001 in.\n\nOs po\u00e7os t\u00eam tubos de produ\u00e7\u00e3o (tubings) de di\u00e2metro interno\nde 3 in, comprimento total de perfura\u00e7\u00e3o de 3,7 km, profundidade de\n2,7 km e ponto de inje\u00e7\u00e3o a 2,7 km.\n\n\n\n84 Cap\u00edtulo 5. An\u00e1lise computacional\n\nTabela 5.6: Alinhamento entre po\u00e7os e manifolds\nPo\u00e7o Manifold\n\n1 1\n2 2\n3 1\n4 1\n5 2\n6 1\n7 1\n8 2\n9 2\n10 2\n11 2\n12 2\n13 1\n14 1\n15 2\n16 2\n\nApresentamos na Tabela 5.7 algumas caracter\u00edsticas dos po\u00e7os\nprodutores. Tamb\u00e9m est\u00e3o indicados a press\u00e3o est\u00e1tica do reservat\u00f3rio\n?? e o \u00edndice de produtividade ?? (productivity index). O modelo deste\ncampo foi simulado utilizando o software PIPESIM.\n\nO modelo deste campo foi simulado utilizando um simulador\nfenomenol\u00f3gico. Sempre que a fun\u00e7\u00e3o objetivo precisava ser avaliada,\nera feita uma simula\u00e7\u00e3o num\u00e9rica da produ\u00e7\u00e3o do campo.\n\nO problema resolvido foi o seguinte:\n\nmax ? =\n???\n\n?=1\n(????? + ?? ?\n\n?\n? ? ????) ?\n\n???\n?=1\n\n?inj?\n?\ninj\n\nsujeito a ? ? ??inj ? ?\n???\n\n?=1\n??inj ? ?maxinj ,\n\nem que as vaz\u00f5es de \u00f3leo ??, g\u00e1s ?? e \u00e1gua ?? de cada po\u00e7o s\u00e3o calculadas\npelo simulador, e a vaz\u00e3o de g\u00e1s para gas-lift ?inj est\u00e1 limitada, em cada\npo\u00e7o de ? = 0 at\u00e9 ? = 226.534 Nm3/d e ? = 16. A disponibilidade total\nde g\u00e1s do campo utilizada foi ?maxinj = 8\u00d7? = 1.812.278 Nm3/d. Utiliza-\nmos os pre\u00e7os de \u00f3leo ?? = 20, e g\u00e1s ?? = 2. O custo do tratamento da\n\n\n\n5.2. Otimiza\u00e7\u00e3o baseada no simulador 85\n\nTabela 5.7: Caracter\u00edsticas dos po\u00e7os\nPo\u00e7o GOR (Nm3/Nm3) Water cut (%) ?? (psi a) ?? (STB/d/psi)\n\n1 200 0 2100 15\n2 200 20 2300 2\n3 300 10 1950 12\n4 300 40 2050 15\n5 400 0 1750 4\n6 400 20 1700 9\n7 500 10 1700 11\n8 500 40 2100 10\n9 200 10 1900 5\n10 200 40 2200 9\n11 300 0 1850 11\n12 300 20 2300 6\n13 400 10 1825 14\n14 400 40 2200 7\n15 500 0 1600 8\n16 500 20 1800 5\n\n\u00e1gua foi ?? = 1 e o custo da inje\u00e7\u00e3o de g\u00e1s foi ???? = 5.\nPara a resolu\u00e7\u00e3o, implementamos o algoritmo de regi\u00e3o de confi-\n\nan\u00e7a em Matlab, que foi usado para gerar c\u00f3digo bin\u00e1rio, que faz uso\ndo simulador.\n\nComo na se\u00e7\u00e3o anterior, utilizamos uma regi\u00e3o de confian\u00e7a dada\npor uma bola na norma ?? em torno do iterando:\n\n??(??, ??) = {? : ?? ? ???? ? ??}.\n\nOs modelos usados foram de interpola\u00e7\u00e3o, polinomiais lineares:\n\n?(?? + ?) = ?(??) + ??? ?, ? ???(??, ??).\n\nO raio inicial foi ?0 = 1, o raio m\u00e1ximo foi ?max = 16.384 e o\ncrit\u00e9rio de parada, ? &lt;10?3.\n\nPara a aceita\u00e7\u00e3o do passo usamos os coeficientes ?1 = 0,1 e ?0 = 0.\nOs par\u00e2metros de aumento e contra\u00e7\u00e3o do raio foram os mesmos em\ntodas as itera\u00e7\u00f5es: ?inc = 2 e ? = 0,5. O limiar para a substitui\u00e7\u00e3o de\npiv\u00f4s durante o algoritmo de melhora de modelo foi ? = 116 .\n\nDesta vez, para o c\u00e1lculo do passo, resolvemos o problema de\notimiza\u00e7\u00e3o dentro da regi\u00e3o de confian\u00e7a, utilizando a fun\u00e7\u00e3o linprog,\ndo Matlab, com o algoritmo active-set.\n\n\n\n86 Cap\u00edtulo 5. An\u00e1lise computacional\n\nTabela 5.8: Sum\u00e1rio da resolu\u00e7\u00e3o do problema: A maior parte do tempo\nfoi gasta nas simula\u00e7\u00f5es\n\nPto. inicial n.o simula\u00e7\u00f5es Tempo simula\u00e7\u00e3o Tempo algoritmo\n1 1383 2 h 18 min 22 s\n2 1294 2 h 22 min 22 s\n3 1166 2 h 5 min 21 s\n4 1190 2 h 6 min 22 s\n5 1242 2 h 12 min 21 s\n6 1253 2 h 15 min 21 s\n7 1418 2 h 38 min 16 s\n8 1132 1 h 60 min 15 s\n9 1110 2 h 4 min 16 s\n10 1075 1 h 57 min 18 s\n11 1002 1 h 48 min 13 s\n12 1080 1 h 58 min 15 s\n\nResolvemos o problema a partir de 15 diferentes pontos inici-\nais, escolhidos aleatoriamente. Em 12 dos casos, o algoritmo convergiu,\naproximadamente, para um mesmo ponto, sendo ??? a m\u00e9dia deles, com\nvalor ? (???) = 2.136.131 para a fun\u00e7\u00e3o objetivo. Com rela\u00e7\u00e3o \u00e0 m\u00e9dia\n???, o desvio padr\u00e3o da dist\u00e2ncia dos pontos finais a ela ?? ? ????2 foi\n19,2 Nm3/d, enquanto que ?????2 = 1.258 Nm3/d. Considerando todos os\nvalores encontrados para a fun\u00e7\u00e3o objetivo, o desvio padr\u00e3o foi de 0,69.\n\nConsiderando estes 12 pontos iniciais, apresentamos alguns de-\ntalhes da resolu\u00e7\u00e3o na Tabela 5.8: n\u00famero de simula\u00e7\u00f5es feitas, tempo\ngasto com simula\u00e7\u00f5es e tempo gasto no restante do algoritmo. A maior\nparte do tempo foi gasta com as simula\u00e7\u00f5es, que foram executadas se-\nquencialmente. O tempo gasto pelo restante do algoritmo, em si, foi de\n22 s, no pior caso.\n\nNa tabela 5.9 fazemos um sum\u00e1rio da exatid\u00e3o das solu\u00e7\u00f5es\nencontradas. Para tanto, consideramos ??? o melhor ponto encontrado\nentre todos os cen\u00e1rios. Na tabela est\u00e3o os desvios absolutos, com\nrela\u00e7\u00e3o a tal ponto, e relativos, comparando-se com a norma deste\nponto ou com a dist\u00e2ncia entre o melhor ponto ??? e o ponto inicial de\ncada caso.\n\nNa Figura 5.6 mostramos o valor corrente da fun\u00e7\u00e3o objetivo\ndurante a execu\u00e7\u00e3o do algoritmo, para dois pontos iniciais diferentes,\nem fun\u00e7\u00e3o do n\u00famero de simula\u00e7\u00f5es feitas.\n\nNo entanto, em 3 dos 15 pontos, o algoritmo n\u00e3o foi capaz de se\nafastar muito do ponto original. As previs\u00f5es dos modelos n\u00e3o foram\n\n\n\n5.2. Otimiza\u00e7\u00e3o baseada no simulador 87\n\nTabela 5.9: Sum\u00e1rio da resolu\u00e7\u00e3o do problema: exatid\u00e3o das solu\u00e7\u00f5es\nencontradas.\n\nPto. inicial ?? ? ???? ????????????\n???????\n??????0? ? (???) ? ? (?)\n\n? (???)?? (?)\n? (???)\n\n1 0 0 0 0 0\n2 3.87 0.304 \u00d7 10?2 0.171 \u00d7 10?4 0.182 0.085 \u00d7 10?6\n3 3.87 0.304 \u00d7 10?2 0.462 \u00d7 10?4 0.182 0.085 \u00d7 10?6\n4 3.87 0.304 \u00d7 10?2 0.249 \u00d7 10?4 0.182 0.085 \u00d7 10?6\n5 3.87 0.304 \u00d7 10?2 0.259 \u00d7 10?4 0.182 0.085 \u00d7 10?6\n6 3.87 0.304 \u00d7 10?2 0.206 \u00d7 10?4 0.182 0.085 \u00d7 10?6\n7 21.77 1.711 \u00d7 10?2 1.179 \u00d7 10?4 0.720 0.337 \u00d7 10?6\n8 31.12 2.445 \u00d7 10?2 1.626 \u00d7 10?4 1.016 0.476 \u00d7 10?6\n9 32.05 2.519 \u00d7 10?2 1.639 \u00d7 10?4 1.017 0.476 \u00d7 10?6\n10 26.48 2.081 \u00d7 10?2 1.480 \u00d7 10?4 1.272 0.596 \u00d7 10?6\n11 33.67 2.646 \u00d7 10?2 1.607 \u00d7 10?4 1.276 0.597 \u00d7 10?6\n12 51.71 4.064 \u00d7 10?2 4.262 \u00d7 10?4 2.315 1.084 \u00d7 10?6\n\n0 200 400 600 800 1.000 1.200 1.400\n?4\n\n?2\n\n0\n\n2\n\n\u00b7106\n\nN\u00famero de simula\u00e7\u00f5es\n\nf(\nx)\n\nValor da fun\u00e7\u00e3o objetivo\n\nPonto inicial 1\nPonto inicial 12\n\nFigura 5.6: Valor da fun\u00e7\u00e3o objetivo ao longo da execu\u00e7\u00e3o da otimiza\u00e7\u00e3o\n\n\n\n88 Cap\u00edtulo 5. An\u00e1lise computacional\n\nboas o suficiente para o raio da regi\u00e3o crescer muito e foi feito pouco\nprogresso.\n\nNotamos que o custo atribu\u00eddo \u00e0 inje\u00e7\u00e3o de g\u00e1s acabou sendo\nmuito elevado. Isto fez com que v\u00e1rios dos pontos iniciais acabassem,\nao longo da resolu\u00e7\u00e3o, a quase zerar a inje\u00e7\u00e3o de g\u00e1s para depois voltar\na aument\u00e1-la.\n\nTamb\u00e9m comparamos a solu\u00e7\u00e3o obtida com a de [7], que uti-\nliza um modelo linear por partes do simulador. Esta \u00faltima encontrou\num valor de 2.234.150 para a fun\u00e7\u00e3o objetivo. Ao aplicar a solu\u00e7\u00e3o\nno simulador, o valor realmente obtido foi 2.155.338. No entanto, esta\nabordagem tamb\u00e9m encontrou um alinhamento \u00f3timo entre po\u00e7os e\nmanifolds, que foi diferente daquele que utilizamos durante nossa resolu-\n\u00e7\u00e3o. Se aplicarmos o resultado que encontramos (de inje\u00e7\u00e3o de g\u00e1s) com\no alinhamento de [7], nosso valor para o ganho econ\u00f4mico \u00e9 um pouco\nmaior: 2.203.409. O tempo de resolu\u00e7\u00e3o tamb\u00e9m foi muito diferente: a\nresolu\u00e7\u00e3o de [7] durou apenas 55 s, por\u00e9m com o modelo de otimiza\u00e7\u00e3o\nconstru\u00eddo previamente.\n\nEstas diferen\u00e7as entre as duas abordagens mostram a relev\u00e2ncia\ndo tratamento para os alinhamentos entre po\u00e7o e manifold. Al\u00e9m disso,\na estrat\u00e9gia que utiliza modelos lineares por partes pode apresentar\nimprecis\u00f5es, seja em fun\u00e7\u00e3o das escolhas as vari\u00e1veis modeladas, seja\nno n\u00famero de pontos utilizados para a constru\u00e7\u00e3o do modelo.\n\nUma possibilidade de corrigir essas dist\u00e2ncias entre o resultado\nobtido pela programa\u00e7\u00e3o inteira e o resultado obtido com o simulador\n\u00e9 utilizar m\u00e9todos sem derivada como meio de melhorar a solu\u00e7\u00e3o do\nresultado da programa\u00e7\u00e3o inteira.\n\n5.3 SUM\u00c1RIO\n\nNeste cap\u00edtulo mostramos aplica\u00e7\u00f5es dos algoritmos de busca\ndireta direcional e o de regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel. Em um\nprimeiro estudo de caso, ambos foram aplicados na maximiza\u00e7\u00e3o de\num ganho econ\u00f4mico, no problema da aloca\u00e7\u00e3o de g\u00e1s em um campo\nprodutor de petr\u00f3leo.\n\nAs curvas de produ\u00e7\u00e3o dos po\u00e7os eram modeladas por fun\u00e7\u00f5es\nsuaves e ambos os algoritmos foram capazes de encontrar o valor \u00f3timo\npara as inje\u00e7\u00f5es, sendo que a busca direta direcional necessitou de muito\nmais avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo.\n\nNo segundo caso, analisamos a otimiza\u00e7\u00e3o diretamente com um\nsimulador de po\u00e7os de petr\u00f3leo e sistemas relacionados. Essa possibili-\ndade de usar diretamente os simuladores \u00e9 um grande motivador para o\n\n\n\n5.3. Sum\u00e1rio 89\n\nuso de m\u00e9todos n\u00e3o-diferenci\u00e1veis. Neste caso, as fun\u00e7\u00f5es de produ\u00e7\u00e3o\ndos po\u00e7os n\u00e3o eram suaves, tendo em vista o ru\u00eddo causado pelo simula-\ndor num\u00e9rico. Al\u00e9m disso, h\u00e1 intera\u00e7\u00e3o entre as produ\u00e7\u00f5es de diversos\npo\u00e7os, tendo em vista que produzem em manifolds comuns. Para este\ncaso, foi aplicado apenas o algoritmo de regi\u00e3o de confian\u00e7a. Das 15\ntentativas de solu\u00e7\u00e3o, 12 foram bem-sucedidas e convergiram para a\nvizinhan\u00e7a de um mesmo ponto.\n\nComparamos a solu\u00e7\u00e3o encontrada com a obtida por terceiros\ncom outra abordagem, baseada em programa\u00e7\u00e3o inteira, que tamb\u00e9m\notimiza o alinhamento entre po\u00e7os e manifolds. A abordagem com\nprograma\u00e7\u00e3o inteira faz uso de um modelo espec\u00edfico para otimiza\u00e7\u00e3o,\nconstru\u00eddo previamente, tendo um tempo para solu\u00e7\u00e3o bastante inferior\n\u00e0 nossa, que utiliza diretamente o simulador.\n\nOs m\u00e9todos que estudamos n\u00e3o resolvem a quest\u00e3o do roteamento\nda produ\u00e7\u00e3o, enquanto que a abordagem que utiliza modelos lineares\npor partes pode ter erros de modela\u00e7\u00e3o. Uma possibilidade \u00e9 utilizar\nos m\u00e9todos n\u00e3o-diferenci\u00e1veis como uma alternativa para melhorar o\nresultado obtido com programa\u00e7\u00e3o inteira.\n\n\n\n\n\n6 CONCLUS\u00c3O\n\nNa ind\u00fastria do petr\u00f3leo h\u00e1 v\u00e1rias ferramentas de simula\u00e7\u00e3o nu-\nm\u00e9rica dos processos de produ\u00e7\u00e3o. No entanto, os simuladores normal-\nmente n\u00e3o fornecem derivadas das vari\u00e1veis calculadas, o que impede\nseu uso para otimiza\u00e7\u00e3o com algoritmos cl\u00e1ssicos. Neste trabalho apre-\nsentamos dois m\u00e9todos de otimiza\u00e7\u00e3o que n\u00e3o fazem uso de derivadas:\nbusca direta direcional e regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel. Eles fo-\nram expostos em sua forma irrestrita e com restri\u00e7\u00f5es lineares. Ambos\nforam utilizados em um estudo num\u00e9rico em que se otimizou a produ\u00e7\u00e3o\nde um campo que produz petr\u00f3leo por gas-lift. Para o tratamento de\nrestri\u00e7\u00f5es n\u00e3o-lineares, propomos o m\u00e9todo do Lagrangiano aumentado,\nque pode fazer uso dos m\u00e9todos anteriores para resolver a sequ\u00eancia de\nsub-problemas resultante. Neste m\u00e9todo, as restri\u00e7\u00f5es n\u00e3o-lineares s\u00e3o\nsubstitu\u00eddas por penaliza\u00e7\u00f5es na fun\u00e7\u00e3o objetivo, restando apenas as\nrestri\u00e7\u00f5es lineares.\n\nOs algoritmos de busca direta direcional s\u00e3o de implementa\u00e7\u00e3o\nmais simples, enquanto os de regi\u00e3o de confian\u00e7a demandam mais tra-\nbalho. Ambos t\u00eam garantia de converg\u00eancia, dependendo da suavidade\nda fun\u00e7\u00e3o que se est\u00e1 otimizando. Os algoritmos tamb\u00e9m podem fazer\nuso de ferramentas de simula\u00e7\u00e3o num\u00e9rica diretamente, embora neste\ncaso n\u00e3o se seja poss\u00edvel verificar a suavidade da fun\u00e7\u00e3o.\n\nNos estudos num\u00e9ricos que realizamos, o algoritmo de otimiza\u00e7\u00e3o\npor regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel foi mais eficiente que o de\nbusca direta direcional, chegando ao ponto \u00f3timo em menos tempo e\ncom menos avalia\u00e7\u00f5es da fun\u00e7\u00e3o objetivo. Tamb\u00e9m fizemos um estudo\ncom o algoritmo de regi\u00e3o de confian\u00e7a e um simulador de redes de\npetr\u00f3leo. A maioria do tempo de solu\u00e7\u00e3o foi gasta no c\u00e1lculo da fun\u00e7\u00e3o\nobjetivo (simula\u00e7\u00f5es), e n\u00e3o no algoritmo em si.\n\nEm nossos estudos, para ambos os algoritmos as avalia\u00e7\u00f5es de\nfun\u00e7\u00e3o objetivo foram feitas sequencialmente. O desempenho pode ser\nmelhorado se v\u00e1rias avalia\u00e7\u00f5es forem feitas em paralelo. Isto \u00e9 particular-\nmente marcante para o algoritmo de busca direta direcional, que utiliza\nv\u00e1rias avalia\u00e7\u00f5es por itera\u00e7\u00e3o. Em particular, a abordagem de busca\ndireta direcional tem um algoritmo que pode ser utilizado de forma\ndistribu\u00edda [30], permitindo a execu\u00e7\u00e3o paralela e com certo balan\u00e7o de\ncarga.\n\nNo caso de aloca\u00e7\u00e3o de g\u00e1s para gas-lift, se o usu\u00e1rio dispuser de\numa boa estimativa para a solu\u00e7\u00e3o, proveniente seja de conhecimentos\nde opera\u00e7\u00e3o, seja do resultado de uma otimiza\u00e7\u00e3o aproximada pr\u00e9via,\no uso de algoritmos n\u00e3o-diferenci\u00e1veis pode dar suporte para melhorar\ntal estimativa.\n\n91\n\n\n\n92 Cap\u00edtulo 6. Conclus\u00e3o\n\nOs algoritmos apresentados convergem para \u00f3timos locais. Se a\notimalidade global for necess\u00e1ria, devem ser utilizados outros m\u00e9todos.\nPara o caso em que as restri\u00e7\u00f5es s\u00e3o limites superiores e inferiores,\nexistem os algoritmos DIRECT (DIviding RECTangles) [31] e MCS\n(Multilevel Coordinate Search) [32]. Outra alternativa \u00e9 recorrer a m\u00e9-\ntodos auxiliares que indiquem um bom ponto inicial para a solu\u00e7\u00e3o.\n\nOs algoritmos apresentados n\u00e3o resolvem problemas com vari-\n\u00e1veis inteiras. No contexto de campos de produ\u00e7\u00e3o de petr\u00f3leo, estas\nocorrem, inevitavelmente, nas decis\u00f5es sobre o roteamento da produ\u00e7\u00e3o.\nAl\u00e9m disso, o problema da aloca\u00e7\u00e3o de g\u00e1s utilizando outras estrat\u00e9gias\ncomo programa\u00e7\u00e3o inteira mista pode ser mais eficiente em encontrar o\n\u00f3timo. Se o n\u00famero de otimiza\u00e7\u00f5es a ser feito for alto, ou se o custo de\nmanuten\u00e7\u00e3o de um modelo de otimiza\u00e7\u00e3o preciso for baixo, \u00e9 a melhor\nabordagem.\n\nO presente estudo apresentou m\u00e9todos que buscassem fazer uso\ndo conhecimento das restri\u00e7\u00f5es lineares. H\u00e1 m\u00e9todos semelhantes que\ntratam apenas limites superiores e inferiores. Neste caso, outras restri-\n\u00e7\u00f5es devem ser tratadas com m\u00e9todos de penaliza\u00e7\u00e3o, como Lagrangiano\naumentado. Um estudo comparando tal alternativa com nossa escolha\n(tratamento expl\u00edcito de restri\u00e7\u00f5es lineares), ainda pode ser feito.\n\nEm trabalhos futuros faremos um estudo num\u00e9rico do m\u00e9todo do\nLagrangiano aumentado para o tratamento das restri\u00e7\u00f5es n\u00e3o-lineares.\nNo caso espec\u00edfico de aloca\u00e7\u00e3o de g\u00e1s para gas-lift, tais restri\u00e7\u00f5es ocor-\nrem nas produ\u00e7\u00f5es admiss\u00edveis de g\u00e1s ou contaminantes, mas tamb\u00e9m\nna modela\u00e7\u00e3o da capacidade de compress\u00e3o de g\u00e1s do campo.\n\nAinda em trabalhos futuros, podemos utilizar os m\u00e9todos abor-\ndados para problemas que n\u00e3o est\u00e3o sendo resolvidos com programa\u00e7\u00e3o\ninteira-mista. Entre esses, incluem-se otimiza\u00e7\u00e3o em reservat\u00f3rios de\npetr\u00f3leo e escoamentos com mudan\u00e7a de fase (flashing).\n\n\n\nREFER\u00caNCIAS\n\n[1] SILVA, T. L. Formula\u00e7\u00f5es inteiras mistas para modelos lineares por\npartes multidimencionais. Disserta\u00e7\u00e3o (Mestrado) \u2014 Programa de\nP\u00f3s-gradua\u00e7\u00e3o em Engenharia de Automa\u00e7\u00e3o e Sistemas, Universi-\ndade Federal de Santa Catarina, Florian\u00f3polis, 2012.\n\n[2] CODAS, A. Otimiza\u00e7\u00e3o da produ\u00e7\u00e3o de po\u00e7os de petr\u00f3leo com inje\u00e7\u00e3o\ncont\u00ednua de g\u00e1s e alinhamento po\u00e7o-separador: modelos lineares por\npartes e algoritmos. Disserta\u00e7\u00e3o (Mestrado) \u2014 Programa de P\u00f3s-\ngradua\u00e7\u00e3o em Engenharia de Automa\u00e7\u00e3o e Sistemas, Universidade\nFederal de Santa Catarina, Florian\u00f3polis, 2012.\n\n[3] BUITRAGO, S.; RODR\u00cdGUEZ, E.; ESPIN, D. Global optimiza-\ntion techniques in gas allocation for continuous flow gas lift systems.\nIn: Proceedings of the Gas Technology Conference. [S.l.]: Society of\nPetroleum Engineers, 1996.\n\n[4] CODAS, A. et al. Integrated production optimization of oil fields\nwith pressure and routing constraints: the Urucu field. Computers &amp;\nChemical Engineering, v. 46, p. 178\u2013189, 2012. ISSN 0098-1354.\n\n[5] KOSMIDIS, V. D.; PERKINS, J. D.; PISTIKOPOULOS, E. N.\nOptimization of well oil rate allocations in petroleum fields. Industrial\n&amp; Engineering Chemistry Research, v. 43, n. 14, p. 3513\u20133527, 2004.\n\n[6] MISENER, R.; GOUNARIS, C. E.; FLOUDAS, C. A. Global opti-\nmization of gas lifting operations: A comparative study of piecewise\nlinear formulations. Industrial &amp; Engineering Chemistry Research,\nv. 48, n. 13, p. 6098\u20136104, 2009.\n\n[7] SILVA, T. L.; CODAS, A.; CAMPONOGARA, E. A computational\nanalysis of convex combination models for multidimensional piecewise-\nlinear approximation in oil production optimization. In: Proceedings\nof the 2012 IFAC Workshop on Automatic Control in Offshore Oil\nand Gas Production. Trondheim: [s.n.], 2012. v. 1, p. 292\u2013298.\n\n[8] GUNNERUD, V.; CONN, A.; FOSS, B. Embedding structural in-\nformation in simulation-based optimization. Computers &amp; Chemical\nEngineering, v. 53, p. 35 \u2013 43, 2013. ISSN 0098-1354.\n\n[9] CIAURRI, D. E.; ISEBOR, O. J.; DURLOFSKY, L. J. Application\nof derivative-free methodologies to generally constrained oil produc-\ntion optimisation problems. International Journal of Mathematical\nModelling and Numerical Optimisation, v. 2, n. 2, p. 134\u2013161, 2011.\n\n93\n\n\n\n94 Refer\u00eancias\n\n[10] GANZAROLI, C. A. Modelagem, simula\u00e7\u00e3o e controle da din\u00e2mica\nde po\u00e7os operando com gas-lift cont\u00ednuo. Disserta\u00e7\u00e3o (Mestrado) \u2014\nPrograma de P\u00f3s-gradua\u00e7\u00e3o em Engenharia de Automa\u00e7\u00e3o e Sis-\ntemas, Universidade Federal de Santa Catarina, Florian\u00f3polis, 2011.\n\n[11] THOMAS, J. E. (Ed.). Fundamentos de engenharia de petr\u00f3leo. 2.\ned. Rio de Janeiro, RJ: Editora Interci\u00eancia, 2001.\n\n[12] PLUCENIO, A. Automa\u00e7\u00e3o da produ\u00e7\u00e3o de po\u00e7os de petr\u00f3leo ope-\nrando com eleva\u00e7\u00e3o artificial por inje\u00e7\u00e3o cont\u00ednua de g\u00e1s. Disserta\u00e7\u00e3o\n(Mestrado) \u2014 Programa de P\u00f3s-gradua\u00e7\u00e3o em engenharia el\u00e9trica,\nUniversidade Federal de Santa Catarina, Florian\u00f3polis, 2003.\n\n[13] KOLDA, T. G.; LEWIS, R. M.; TORCZON, V. Optimization by di-\nrect search: New perspectives on some classical and modern methods.\nSIAM Review, v. 45, n. 3, p. 385\u2013482, August 2003.\n\n[14] ALARC\u00d3N, G. A.; TORRES, C. F.; G\u00d3MEZ, L. E. Global opti-\nmization of gas allocation to a group of wells in artificial lift using\nnonlinear constrained programming. ASME Journal of Energy Re-\nsources Technology, v. 124, n. 4, p. 262\u2013268, 2002.\n\n[15] DAVIS, C. Theory of positive linear dependence. American Journal\nof Mathematics, The Johns Hopkins University Press, v. 76, n. 4, p.\n733\u2013746, 1954. ISSN 00029327.\n\n[16] CONN, A. R.; SCHEINBERG, K.; VICENTE, L. N. Introduction\nto Derivative-Free Optimization. 1. ed. Philadelphia, PA: Society for\nIndustrial and Applied Mathematics, 2009.\n\n[17] SCHEINBERG, K.; TOINT, P. L. Self-correcting geometry in\nmodel-based algorithms for derivative-free unconstrained optimiza-\ntion. SIAM Journal on Optimization, SIAM Publications, Philadel-\nphia, PA, v. 20, n. 6, p. 3512\u20133532, 2010. ISSN 1052-6234.\n\n[18] TR\u00d6LTZSCH, A. Active sets in bound-constrained optimization\nwithout derivatives. Saarbr\u00fccken: Lambert Academic Publishing,\n2012.\n\n[19] PARDALOS, P. M.; VAVASIS, S. A. Quadratic programming with\none negative eigenvalue is NP-Hard. Journal of Global Optimization,\nKluwer Academic Publishers, v. 1, p. 15\u201322, 1991. ISSN 0925-5001.\n\n[20] CONN, A. R.; GOULD, N. I. M.; TOINT, P. L. Trust Region\nMethods. Philadelphia, PA: Society for Industrial and Applied Math-\nematics, 2000.\n\n\n\nRefer\u00eancias 95\n\n[21] LEWIS, R. M.; TORCZON, V. Pattern search methods for linearly\nconstrained minimization. SIAM Journal on Optimization, v. 10, n. 3,\np. 917\u2013941, 2000.\n\n[22] LUCIDI, S.; SCIANDRONE, M.; TSENG, P. Objective-derivative-\nfree methods for constrained optimization. Mathematical Program-\nming, Springer Berlin / Heidelberg, v. 92, p. 37\u201359, 2002. ISSN 0025-\n5610.\n\n[23] ABRAMSON, M. A. et al. OrthoMADS: A deterministic MADS\ninstance with orthogonal directions. SIAM Journal on Optimization,\nv. 20, n. 2, p. 948\u2013966, 2009. ISSN 10526234.\n\n[24] BERTSEKAS, D. P. Nonlinear Programming. 2. ed. Belmont, MA:\nAthena Scientific, 1999.\n\n[25] BERTSEKAS, D. P. Constrained optimization and Lagrange mul-\ntiplier methods. Belmont, MA: Athena Scientific, 1996.\n\n[26] CONN, A. R. et al. Convergence properties of an augmented la-\ngrangian algorithm for optimization with a combination of general\nequality and linear constraints. SIAM Journal on Optimization, v. 6,\nn. 3, p. 674\u2013703, August 1996.\n\n[27] KOLDA, T. G.; LEWIS, R. M.; TORCZON, V. A generating\nset direct search augmented Lagrangian algorithm for optimization\nwith a combination of general and linear constraints. Albuquerque,\nNM, 2006. Dispon\u00edvel em:&lt;http://www.prod.sandia.gov/cgi-\nbin/techlib/access-control.pl/2006/065315.pdf>.\n\n[28] PHELPS, R. R. Convex sets and nearest points. Proceedings of\nthe American Mathematical Society, American Mathematical Society,\nv. 8, n. 4, p. pp. 790\u2013797, 1957. ISSN 00029939.\n\n[29] GIULIANI, C. M.; CAMPONOGARA, E.; PLUCENIO, A. A com-\nputational analysis of nondifferentiable optimization: Applications to\nproduction maximization in gas-lifted oil fields. In: Proceedings of\nthe 9th IEEE International Conference on Automation Science and\nEngineering (CASE). Madison, WI: [s.n.], 2013. p. 292\u2013297.\n\n[30] HOUGH, P. D.; KOLDA, T. G.; TORCZON, V. J. Asynchronous\nparallel pattern search for nonlinear optimization. SIAM Journal on\nScientific Computing, v. 23, n. 1, p. 134\u2013156, June 2001.\n\n\n\n96 Refer\u00eancias\n\n[31] JONES, D. R.; PERTTUNEN, C. D.; STUCKMAN, B. E. Lips-\nchitzian optimization without the lipschitz constant. Journal of Opti-\nmization Theory and Applications, v. 79, n. 1, p. 157\u2013181, 1993. ISSN\n00223239.\n\n[32] HUYER, W.; NEUMAIER, A. Global optimization by multilevel\ncoordinate search. Journal of Global Optimization, v. 14, n. 4, p. 331\u2013\n355, 1999. ISSN 09255001.\n\n[33] SPIVAK, M. Significance of the derivative. In: . Calculus. 3.\ned. Houston, TX: Publish or Perish, 1994. cap. 11.\n\n\n\nA SOBRE A DIFERENCIABILIDADE DO LAGRANGIANO\nAUMENTADO\n\nNa se\u00e7\u00e3o 4.1.2, apresentamos uma maneira de tratar restri\u00e7\u00f5es\nde desigualdades no problema do Lagrangiano aumentado sem a neces-\nsidade de vari\u00e1veis adicionais.\n\nPara tanto, os sub-problemas resultantes envolviam uma fun\u00e7\u00e3o\nLagrangiano aumentado j\u00e1 otimizada nas vari\u00e1veis de folga, da seguinte\nmaneira:\n\n?(?, ?, ?, c) = ? (?) + ?? ?(?) + 1\n2\n\nc??(?)?2+\n\n+\n1\n2c\n\n???\n?=1\n\n{?\nmax[0, ?? + c??(?)]2 ? ?2?\n\n}?\n. (4.4)\n\nNa discuss\u00e3o subsequente, tal express\u00e3o foi presumida suave.\nNeste ap\u00eandice vamos provar que, de fato, (4.4) \u00e9 continuamente di-\nferenci\u00e1vel.\n\nPara tanto, a suposi\u00e7\u00e3o que fazemos \u00e9 que as fun\u00e7\u00f5es ? , ? e\n? s\u00e3o continuamente diferenci\u00e1veis. Diante dessa hip\u00f3tese, \u00e9 imediato\nque os primeiros termos de ?(\u00b7, ?, ?, c) s\u00e3o continuamente diferenci\u00e1veis.\nTrataremos de modo separado a parcela que envolve a fun\u00e7\u00e3o \u201cmax\u201d.\n\nPara provar sua diferenciabilidade, introduzimos um teorema\nde [33]:\n\nTeorema 11 Suponha que a fun\u00e7\u00e3o ? : R ? R \u00e9 cont\u00ednua no ponto\n?, e que ? ?(?) existe para todo ? em um intervalo contendo ?, exceto,\npossivelmente para ? = ?. Suponha, ainda, que lim??? ? ?(?) existe.\nEnt\u00e3o ? ?(?) existe, e\n\n? ?(?) = lim\n???\n\n? ?(?).\n\nVamos usar o teorema para mostrar a suavidade da fun\u00e7\u00e3o ?(?) =\nmax[0, ?(?)]2, com ? : R ? R continuamente diferenci\u00e1vel. Em (4.4)\nocorre um caso particular em que ?(?) = ?? + c??(?).\n\nA fun\u00e7\u00e3o ? tamb\u00e9m pode ser escrita como\n\n?(?) =\n{?\n\n0, se ?(?) ? 0\n?(?)2, se ?(?) ? 0.\n\nClaramente ? \u00e9 continuamente diferenci\u00e1vel em todos os pontos\nem que ?(?) &lt;0 ou ?(?) > 0.\n\nAgora suponha que existe um ponto ? ? R tal que ?(?) = 0. Se,\nna vizinhan\u00e7a de ? n\u00e3o ocorre troca de sinal, por exemplo, ?(?) ? 0,\n\n97\n\n\n\n98 Ap\u00eandice A. Sobre a diferenciabilidade do Lagrangiano aumentado\n\ntamb\u00e9m \u00e9 imediato que ? \u00e9 continuamente diferenci\u00e1vel. Resta a d\u00favida\ncom rela\u00e7\u00e3o \u00e0queles pontos em que h\u00e1 troca de sinal.\n\nSuponha que ?(?) = 0 e ?(?) > 0 para ? > ? e ?(?) &lt;0 se ? &lt;?,\nna vizinhan\u00e7a de ?. Ent\u00e3o, o limite, por um lado \u00e9\n\nlim\n???+\n\n??(?) = lim\n???+\n\n?(?)2 =\n\n= lim\n???+\n\n2?(?)??(?) =\n\n= 2\n(?\n\nlim\n???+\n\n?(?)\n)?(?\n\nlim\n???+\n\n??(?)\n)?\n\n=\n\n= 2 \u00d7 0 \u00d7 ??(?) =\n= 0.\n\nEnquanto, pelo outro lado, o limite \u00e9\n\nlim\n????\n\n??(?) = lim\n????\n\n0 =\n\n= 0.\n\nConsequentemente,\nlim\n???\n\n??(?) = 0. (A.1)\n\nComo ? \u00e9 cont\u00ednua, e sua derivada tem limite no ponto ? (A.1),\npelo Teorema 11, ? \u00e9 continuamente diferenci\u00e1vel em ?, com ??(?) = 0.\nO caso rec\u00edproco (?(?) > 0 se ? > ? e ?(?) &lt;0 se ? &lt;?) pode ser\nresolvido de forma an\u00e1loga.\n\nAssim, o Lagrangiano aumentado proposto (4.4) \u00e9 composto por\numa soma na qual todas as parcelas s\u00e3o continuamente diferenci\u00e1veis,\ne portanto, continuamente diferenci\u00e1vel.\n\n\n\tResumo\n\tAbstract\n\tSum\u00e1rio\n\tIntrodu\u00e7\u00e3o\n\tOrganiza\u00e7\u00e3o do documento\n\n\tOtimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel irrestrita\n\tBusca direta direcional\n\tRegi\u00e3o de confian\u00e7a\n\tDefini\u00e7\u00e3o do modelo\n\tPolin\u00f4mios de Lagrange\n\tC\u00e1lculo do passo\n\tAceita\u00e7\u00e3o do passo e gerenciamento da regi\u00e3o\n\tTeste de criticidade\n\tAlgoritmo de regi\u00e3o de confian\u00e7a\n\n\tAlternativas algor\u00edtmicas\n\tSum\u00e1rio\n\n\tOtimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es lineares\n\tMaterial b\u00e1sico sobre otimiza\u00e7\u00e3o restrita\n\tBusca direta direcional com restri\u00e7\u00f5es lineares\n\tAlternativas algor\u00edtmicas\n\n\tRegi\u00e3o de confian\u00e7a\n\tPasso generalizado de Cauchy\n\tAlgoritmo de regi\u00e3o de confian\u00e7a com restri\u00e7\u00f5es lineares\n\tAlternativas algor\u00edtmicas\n\n\tSum\u00e1rio\n\n\tOtimiza\u00e7\u00e3o n\u00e3o-diferenci\u00e1vel com restri\u00e7\u00f5es n\u00e3o-lineares\n\tLagrangiano aumentado\n\tLagrangiano aumentado com resolu\u00e7\u00e3o aproximada dos sub-problemas\n\tLagrangiano aumentado com restri\u00e7\u00f5es de desigualdade\n\tElimina\u00e7\u00e3o parcial das restri\u00e7\u00f5es\n\n\tUm algoritmo de Lagrangiano aumentado\n\tResolvendo os sub-problemas com busca direta direcional\n\tResolvendo os sub-problemas com regi\u00e3o de confian\u00e7a n\u00e3o-diferenci\u00e1vel\n\tSum\u00e1rio\n\n\tAn\u00e1lise computacional\n\tOtimiza\u00e7\u00e3o de fun\u00e7\u00e3o suave\n\tBusca direta direcional\n\tRegi\u00e3o de confian\u00e7a\n\n\tOtimiza\u00e7\u00e3o baseada no simulador\n\tSum\u00e1rio\n\n\tConclus\u00e3o\n\tRefer\u00eancias\n\tSobre a diferenciabilidade do Lagrangiano aumentado"}]}}}