{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.10868"}, {"@name": "filename", "#text": "16061_000501085.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL \n\nINSTITUTO DE INFORM\u00c1TICA \n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM  COMPUTA\u00c7\u00c3O \n\n \n\n \n\n \n\n \n\n \n\nDEISE DA SILVA C\u00d4RTES \n\n \n\n \n\n \n\n \n\nModelo Neuro-Evolutivo de Coordena\u00e7\u00e3o \n\nAdaptativa em Ambientes Din\u00e2micos \n \n\n \n\n \n\n \n\n \n\n \n\nDisserta\u00e7\u00e3o apresentada como requisito parcial \npara a obten\u00e7\u00e3o do grau de Mestre em Ci\u00eancia \nda Computa\u00e7\u00e3o \n\n \n \nDr. Luis Ot\u00e1vio Campos Alvares \nOrientador \n \n \n\n \n \n \n \n \n \n \n \n\nPorto Alegre,  outubro de 2005.\n\n\n\nCIP \u2013 CATALOGA\u00c7\u00c3O NA PUBLICA\u00c7\u00c3O \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n \n \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nUNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL \nReitor: Prof. Jos\u00e9 Carlos Ferraz Hennemann \nVice-Reitor: Prof. Pedro Cezar Dutra Fonseca \nPr\u00f3-Reitora de P\u00f3s-Gradua\u00e7\u00e3o: Profa. Valquiria Linck Bassani \nDiretor do Instituto de Inform\u00e1tica: Prof. Philippe Olivier Alexandre Navaux \nCoordenador do PPGC: Prof. Fl\u00e1vio Rech Wagner \nBibliotec\u00e1ria-Chefe do Instituto de Inform\u00e1tica: Beatriz Regina Bastos Haro \n\nC\u00f4rtes, Deise da Silva \n\nModelo Neuro-Evolutivo de Coordena\u00e7\u00e3o Adaptativa em \nAmbientes Din\u00e2micos / Deise da Silva C\u00f4rtes \u2013 Porto Alegre: \nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o, 2005. \n\n72 f.:il. \n\nDisserta\u00e7\u00e3o (mestrado) \u2013 Universidade Federal do Rio Grande \ndo Sul. Programa de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o. Porto \nAlegre, BR \u2013 RS, 2005. Orientador: Luis Ot\u00e1vio Campos Alvares. \n\n1. Sistemas multiagentes. 2. Coordena\u00e7\u00e3o. 3. Neuro-evolu\u00e7\u00e3o. 4. \nRedes Neurais. 5. Algoritmos Gen\u00e9ticos. \n\nI. Alvares, Luis Ot\u00e1vio Campos.  II. T\u00edtulo. \n\n\n\nAGRADECIMENTOS \n\n.  \n\nEu sempre soube que fazer uma disserta\u00e7\u00e3o de mestrado n\u00e3o seria f\u00e1cil. Muito \nembora, as maiores dificuldades que encontrei tenham sido as menos esperadas.  \n\nAntes de tudo e de todos, agrade\u00e7o a Deus, por ter me dado for\u00e7as para continuar \ntoda vez em que desistir parecia ser inevit\u00e1vel. \n\nAgrade\u00e7o a todas as pessoas que torceram por mim e me apoiaram na realiza\u00e7\u00e3o \ndeste trabalho.  \n\nEm especial, agrade\u00e7o:  \n\n\u00c0 minha fam\u00edlia, todos t\u00e3o distantes durante a realiza\u00e7\u00e3o deste trabalho e ao mesmo \ntempo t\u00e3o presentes em todos os momentos da minha vida. Em particular, agrade\u00e7o \u00e0 \nminha av\u00f3 Lidinha, meus pais e irm\u00e3os, meus dindos e Dinha, que nem por um \nmomento, deixaram de pedir por mim em suas ora\u00e7\u00f5es. \u00c0s minhas priminhas Nine e \nBele, que n\u00e3o se cansam de perguntar quando eu volto. \n\nAo meu querido orientador, professor Luis Ot\u00e1vio Alvares, pelo apoio, pelas \ndiscuss\u00f5es cient\u00edficas e pela aten\u00e7\u00e3o. \n\nAos meus amigos de ontem, hoje e sempre, Rick e Nivinha, e ao MSN e ao Skype \nque tornam a saudade suport\u00e1vel e poss\u00edvel a presen\u00e7a constante de voc\u00eas na minha \nvida. \n\n\u00c0 Mari e Nelma, pelo carinho, aten\u00e7\u00e3o e cumplicidade nos meses em que \nconvivemos aqui em POA. \n\n\u00c0 Vaninha, pela amizade, pelo carinho e pelos pux\u00f5es de orelha!  \n\n\u00c0 Didi e \u00e0s suas aulas magn\u00edficas de dan\u00e7a contempor\u00e2nea, que al\u00e9m de me \npropiciarem m\u00e1gicos momentos de auto-conhecimento, tornaram a minha vida aqui \nmuito mais prazerosa. \n\n\n\n \n\nSUM\u00c1RIO \n\n \n\nLISTA DE ABREVIATURAS E SIGLAS ....................................................... 6 \n\nLISTA DE FIGURAS ..................................................................................... 7 \n\nLISTA DE TABELAS .................................................................................... 9 \n\nRESUMO ...................................................................................................... 10 \n\nABSTRACT .................................................................................................. 11 \n\n1 INTRODU\u00c7\u00c3O .......................................................................................................12 \n\n2 CONCEITOS B\u00c1SICOS .......................................................................................14 \n\n2.1 Agentes e Sistemas Multiagentes........................................................................ 14 \n2.2 Coordena\u00e7\u00e3o......................................................................................................... 15 \n2.3 Tarefas de Decis\u00e3o Seq\u00fcencial............................................................................ 15 \n2.4 Aprendizado por Refor\u00e7o.................................................................................... 16 \n2.4.1 Exploration e exploitation .................................................................................. 18 \n2.4.2 Estrat\u00e9gias de Aprendizado por Refor\u00e7o ............................................................ 18 \n2.5 Algoritmos Gen\u00e9ticos........................................................................................... 22 \n2.5.1 Exploitation x Exploration em AG ..................................................................... 24 \n2.6 Redes Neurais ....................................................................................................... 25 \n\n3 NEURO-EVOLU\u00c7\u00c3O............................................................................................27 \n\n3.1 A abordagem evolucion\u00e1ria ................................................................................ 27 \n3.1.1 Treinamento esparso ........................................................................................... 29 \n3.1.2 Tempo de treinamento ........................................................................................ 29 \n3.2 Codifica\u00e7\u00e3o da Rede ............................................................................................ 29 \n3.3 Topologia da Rede ............................................................................................... 30 \n3.4 Vari\u00e1veis da Evolu\u00e7\u00e3o ......................................................................................... 30 \n3.5 M\u00e9todos de Neuro -Evolu\u00e7\u00e3o ............................................................................... 30 \n3.5.1 Symbiotic, Adaptative Neuro-Evolution ............................................................. 31 \n3.5.2 Enforced Sub-Populations .................................................................................. 33 \n3.5.3 NeuroEvolution of Augmenting Topologies........................................................ 35 \n\n4 MODELO DE COORDENA\u00c7\u00c3O MULTIAGENTE NEURO-EVOLUTIVO...40 \n\n4.1 Algoritmo evolucion\u00e1rio...................................................................................... 40 \n\n\n\n \n\n4.2 Caracter\u00edsticas do Modelo Proposto .................................................................. 42 \n4.2.1 Cromossomos ..................................................................................................... 42 \n4.2.2 Operadores Gen\u00e9ticos ......................................................................................... 44 \n4.2.3 Entrada e Sa\u00edda das Redes Neurais..................................................................... 44 \n4.3 Dom\u00ednio de Aplica\u00e7\u00e3o .......................................................................................... 45 \n4.4 Diagrama de Classes............................................................................................ 46 \n4.5 Ambiente de Simula\u00e7\u00e3o ....................................................................................... 50 \n4.5.1 Manipula\u00e7\u00e3o dos Comportamentos .................................................................... 50 \n4.5.2 Configura\u00e7\u00f5es ..................................................................................................... 52 \n4.5.3 Visualiza\u00e7\u00e3o das estrat\u00e9gias ............................................................................... 54 \n\n5 EXPERIMENTOS ..................................................................................................55 \n\n5.1 Experimentos Comparativos .............................................................................. 55 \n5.2 Experimentos Variando Diversos Par\u00e2metros.................................................. 57 \n5.2.1 Entrada do Agente .............................................................................................. 58 \n5.2.2 Quantidade de Neur\u00f4nios na Camada Oculta ..................................................... 60 \n5.2.3 Qua ntidade de ciclos evolucion\u00e1rios .................................................................. 61 \n5.2.4 Fun\u00e7\u00e3o de Aptid\u00e3o .............................................................................................. 62 \n5.2.5 Mundo toroidal x limitado .................................................................................. 62 \n5.2.6 Grau de dificuldade da tarefa-alvo ..................................................................... 62 \n\n6 CONCLUS\u00d5ES......................................................................................................64 \n\nREFER\u00caNCIAS.................................................................................................66\n\n\n\nLISTA DE ABREVIATURAS E SIGLAS \n\nAR  Aprendizado por Refor\u00e7o \n\nAE Algoritmos Evolucion\u00e1rios \n\nAG Algoritmo Gen\u00e9tico \n\nDP Dynamic Programming \n\nESP Enforced Subpopulations \n\nMDP  Markov Decision Process \n\nNEAT NeuroEvolution of Augmenting Topologies \n\nSANE Symbiotic, Adaptative Neuro-Evolution  \n\nTD Temporal Difference \n\n \n\n \n\n\n\n \n\nLISTA DE FIGURAS \n\nFigura 2.1: Modelo padr\u00e3o do aprendizado........................................................  17 \n\nFigura 2.2: Vis\u00e3o Geral da AHC........................................................................ 20 \n\nFigura 2.3: Algoritmo Gen\u00e9tico Simples............................................................ 23 \n\nFigura 2.4: Algoritmo de Sele\u00e7\u00e3o da Roleta....................................................... 24 \n\nFigura 3.1: Processos da abordagem evolucion\u00e1ria............................................ 28 \n\nFigura 3.2: Passos b\u00e1sicos em uma gera\u00e7\u00e3o do SANE....................................... 31 \n\nFigura 3.3: Neuro-evolu\u00e7\u00e3o no SANE................................................................ 32 \n\nFigura 3.4: Rede neural formada a partir dos cromossomos que definem a \ncamada oculta da rede...................................................................... \n\n33 \n\nFigura 3.5: Neuro-evolu\u00e7\u00e3o em ESP. A rede \u00e9 constitu\u00edda por um neur\u00f4nio de \ncada popula\u00e7\u00e3o de neur\u00f4nios............................................................ \n\n34 \n\nFigura 3.6: Mapeamento do gen\u00f3tipo para o fen\u00f3tipo........................................  36 \n\nFigura 3.7: Os dois tipos de muta\u00e7\u00e3o estrutural no NEAT.................................  37 \n\nFigura 3.8: Recombina\u00e7\u00e3o de duas redes usando o NEAT................................. 38 \n\nFigura 4.1: Algoritmo b\u00e1sico de treinamento..................................................... 41 \n\nFigura 4.2: Algoritmo de Delta-Coding.............................................................. 42 \n\nFigura 4.3: Codifica\u00e7\u00e3o do neur\u00f4nio no modelo proposto................................. 43 \n\nFigura 4.4: Codifica\u00e7\u00e3o da rede neural partindo de cromossomos bin\u00e1rios...... 43 \n\nFigura 4.5: Ciclo evolucion\u00e1rio para a captura da presa.....................................  46 \n\nFigura 4.6: Principais classes do modelo de coordena\u00e7\u00e3o evolucion\u00e1rio........... 47 \n\nFigura 4.7: Todas as classes do modelo evolucion\u00e1rio....................................... 48 \n\nFigura 4.8: Classes do ambiente de simula\u00e7\u00e3o...................................................  49 \n\nFigura 4.9: Manipula\u00e7\u00e3o de comportamentos no ambiente................................ 50 \n\nFigura 4.10: Carrega o comportamento dos predadores....................................... 51 \n\nFigura 4.11: Salva o comportamento atual dos agentes........................................ 51 \n\nFigura 4.12: Configura\u00e7\u00e3o dos predadores........................................................... 52 \n\nFigura 4.13: Configura\u00e7\u00e3o da presa...................................................................... 52 \n\n\n\n \n\nFigura 4.14: Configura\u00e7\u00e3o do treinamento........................................................... 53 \n\nFigura 4.15: Configura\u00e7\u00f5es do mundo................................................................. 53 \n\nFigura 4.16: Menu Execu\u00e7\u00e3o................................................................................ 54 \n\nFigura 4.17: Visualiza\u00e7\u00e3o das estrat\u00e9gias aprendidas........................................... 54 \n\nFigura 5.1: Entradas das nedes neurais dos agentes........................................... 58 \n\nFigura 5.2: M\u00e9dia das melhores aptid\u00f5es dos agentes por ciclo evolucion\u00e1rio. \nTime constitu\u00eddo por 3 agentes........................................................ \n\n61 \n\nFigura 5.3: M\u00e9dia das melhores aptid\u00f5es dos agentes por ciclo evolucion\u00e1rio. \nTime constitu\u00eddo por 4 agentes........................................................  \n\n62 \n\n \n\n\n\n \n\nLISTA DE TABELAS \n\n. \nTabela 5.1: Compara\u00e7\u00e3o das configura\u00e7\u00f5es adotadas para treinamento dos \n\nagentes.............................................................................................. \n55 \n\nTabela 5.2: Compara\u00e7\u00e3o de resultados com o ESP.............................................  56 \n\nTabela 5.3: Passos para captura da presa............................................................ 59 \n\nTabela 5.4: Taxa de captura por tamanho da camada oculta.............................. 60 \n\nTabela 5.5: Tempo de treinamento por tamanho da camada oculta.................... 61 \n\n\n\n \n\nRESUMO \n\nEm ambientes din\u00e2micos e complexos, a pol\u00edtica \u00f3tima de coordena\u00e7\u00e3o n\u00e3o pode ser \nderivada analiticamente, mas, deve ser aprendida atrav\u00e9s da intera\u00e7\u00e3o direta com o \nambiente. Geralmente, utiliza-se aprendizado por refor\u00e7o para prover coordena\u00e7\u00e3o em \ntais ambientes.  \n\nAtualmente, neuro-evolu\u00e7\u00e3o \u00e9 um dos m\u00e9todos de aprendizado por refor\u00e7o mais \nproeminentes. Em vista disto, neste trabalho, \u00e9 proposto um modelo de coordena\u00e7\u00e3o \nbaseado em neuro-evolu\u00e7\u00e3o. Mais detalhadamente, desenvolveu-se uma extens\u00e3o do \nm\u00e9todo neuro-evolutivo conhecido como Enforced Subpopulations (ESP).  \n\nNa extens\u00e3o desenvolvida, a rede neural que define o comportamento de cada agente \n\u00e9 totalmente conectada. Adicionalmente, \u00e9 permitido que o algoritmo encontre, em \ntempo de treinamento, a quantidade de neur\u00f4nios que deve estar presente na camada \noculta da rede neural de cada agente. Esta altera\u00e7\u00e3o, al\u00e9m de oferecer flexibilidade na \ndefini\u00e7\u00e3o da topologia da rede de cada agente e diminuir o tempo necess\u00e1rio para \ntreinamento, permite tamb\u00e9m a constitui\u00e7\u00e3o de grupos de agentes heterog\u00eaneos. \n\nUm ambiente de simula\u00e7\u00e3o foi desenvolvido e uma s\u00e9rie de experimentos realizados \ncom o objetivo de avaliar o modelo proposto e identificar quais os melhores valores \npara os diversos par\u00e2metros do modelo. O modelo proposto foi aplicado no dom\u00ednio das \ntarefas de persegui\u00e7\u00e3o-evas\u00e3o.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nPalavras-Chave: Sistemas Multiagentes, Coordena\u00e7\u00e3o, Neuro-evolu\u00e7\u00e3o, Redes \nNeurais, Algoritmos Gen\u00e9ticos. \n\n\n\nA Neuro-Evolutive Model for  \n\nAdaptative Coordination in Dynamic Systems \n\nABSTRACT \n\nIn dynamic and complex environments, the optimal policy for coordination cannot \nbe analytically derived; it must be learned through direct interactions with the \nenvironment.  Generally, reinforcement learning is used to provide coordination in those \nenvironments.  \n\nNowadays, neuro-evolution is one of the most prominent methods for reinforcement \nlearning.  Therefore, in the present work, we propose a model for coordination based in \nneuro-evolution mechanisms. More specifically, the proposed model is an extension of \na neuro-evolution method kno wn as Enforced Subpopulations (ESP).  \n\nIn our extension, an agent\u2019s neural network is fully connected. Additionally, we \nallow the algorithm to come up with the number of neurons that must be present in the \nhidden layer of an agent\u2019s neural network. This variation provides flexibility in the \ndefinition of the neural network topologies and decreases the amount of time necessary \nto training the agents.   \n\nFurthermore, we developed a simulation environment, which facilitates the \nexecution of a series of experiments. Some of these experiments help the evaluation of \nthe optimal values for the model parameters. The proposed model was applied in the \npursuit-evasion tasks domain. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nKeywords: Multi-Agent System, Coordination, Neuro-evolution, Neural Networks, \ngenetic Algorithms.\n\n\n\n1 INTRODU\u00c7\u00c3O \n\nEmbora n\u00e3o exista uma defini\u00e7\u00e3o universalmente aceita para agentes, geralmente, \npodemos visualizar um agente como uma entidade capaz de desempenhar algumas \natividades autonomamente para alcan\u00e7ar seus objetivos. Costuma-se tamb\u00e9m considerar \nque agentes s\u00e3o capazes de se comunicar com outros agentes e com o ambiente1.  \n\nAl\u00e9m da autonomia e da capacidade de comunica\u00e7\u00e3o, um agente pode possuir \ndiversas outras caracter\u00edsticas, tais como reatividade (capacidade de reagir \napropriadamente \u00e0s influ\u00eancias ou informa\u00e7\u00f5es de seu ambiente), proatividade \n(capacidade de um agente de tomar iniciativas sob circunst\u00e2ncias espec\u00edficas) e \nmobilidade (habilidade do agente navegar pela rede).  \n\nSistemas multiagentes, por sua vez, podem ser definidos como sistemas \ncomputacionais em que v\u00e1rios agentes interagem ou trabalham em conjunto para \ndesempenhar algum conjunto de tarefas ou satisfazer algum conjunto de objetivos \n(LESSER, 1995). \n\nDevido a sua natureza distribu\u00edda, sistemas multiagentes podem ser mais eficientes, \nmais robustos, e mais flex\u00edveis que abordagens centralizadas. Contudo, para serem \nefetivos, normalmente os agentes precisam agir de forma coordenada. \n\nCoordena\u00e7\u00e3o \u00e9 o processo pelo qual um agente raciocina sobre suas a\u00e7\u00f5es locais e as \na\u00e7\u00f5es dos outros para tentar garantir que a comunidade se comporte de uma maneira \ncoerente. Em ambientes din\u00e2micos e complexos, a pol\u00edtica \u00f3tima de coordena\u00e7\u00e3o n\u00e3o \npode ser derivada analiticamente, mas, deve ser aprendida atrav\u00e9s da intera\u00e7\u00e3o direta \ncom o ambiente. Aprendizado por refor\u00e7o \u00e9 a t\u00e9cnica geralmente utilizada para prover a \ncoordena\u00e7\u00e3o de agentes em tais ambientes (BERENJI, 2000; HAYNES, 1995; \nHAYNES, 1996; MORIARTY, 1996; YONG, 2001). Na aprendizagem por refor\u00e7o, os \nagentes aprendem atrav\u00e9s de sinais que fornecem alguma medida de desempenho ap\u00f3s a \nrealiza\u00e7\u00e3o de uma seq\u00fc\u00eancia de a\u00e7\u00f5es. \n\nExistem duas estrat\u00e9gias b\u00e1sicas para se resolver problemas de aprendizagem por \nrefor\u00e7o. A primeira \u00e9 a busca no espa\u00e7o de comportamentos, procurando por um \ncomportamento que desempenhe bem no ambiente. Esta abordagem vem sendo \ngeralmente implementada com algoritmos gen\u00e9ticos e programa\u00e7\u00e3o gen\u00e9tica. A segunda \nestrat\u00e9gia, mais tradicional, \u00e9 o uso de t\u00e9cnicas estat\u00edsticas e m\u00e9todos de programa\u00e7\u00e3o \n\n                                                 \n1 Algumas entidades podem agir completamente por conta pr\u00f3pria. Contudo, geralmente \nn\u00e3o se refere a este tipo de sistema como sistema baseado em agentes, desde que um dos \naspectos que despertam interesse, riqueza e complexidade aos sistemas de agentes s\u00e3o \nas itera\u00e7\u00f5es entre eles. \n\n\n\n \n\ndin\u00e2mica que estimam a utilidade de determinadas a\u00e7\u00f5es no espa\u00e7o de estados do \nambiente (mundo). \n\nSegundo Kaelbling (1996) n\u00e3o \u00e9 claro qual conjunto de abordagens \u00e9 melhor em que \ncircunst\u00e2ncias. Contudo, os m\u00e9todos tradicionais de aprendizado por refor\u00e7o, que \nutilizam programa\u00e7\u00e3o din\u00e2mica, se ap\u00f3iam num mapeamento dos estados e a\u00e7\u00f5es do \nmundo, tornando-os menos extens\u00edveis. Isto \u00e9, quanto mais o espa\u00e7o de estados e a\u00e7\u00f5es \ndo mundo crescer, mais lento ser\u00e1 o processo de aprendizagem. Alguns pesquisadores \n(YONG, 2001; MORIARTY, 1996) constataram solu\u00e7\u00f5es mais r\u00e1pidas e eficientes ao \nutilizarem neuro-evolu\u00e7\u00e3o, um dos m\u00e9todos dispon\u00edveis para aprendizado por refor\u00e7o \nem ambientes din\u00e2micos.  \n\nMoriarty (1997) mostra que m\u00e9todos evolucion\u00e1rios tomam vantagem sobre os \nm\u00e9todos tradicionais de aprendizado por refor\u00e7o por terem um mecanismo de atribui\u00e7\u00e3o \nde cr\u00e9dito mais robusto. Outros investigadores (BELEW, 1993; NOLFI, 1994) \ndescobriram que a neuro-evolu\u00e7\u00e3o, ou evolu\u00e7\u00e3o simulada de redes neurais, \u00e9 uma \nestrat\u00e9gia efetiva para resolver problemas de aprendizado por refor\u00e7o, mesmo quando o \nsinal de refor\u00e7o \u00e9 esparso. Isto \u00e9, mesmo quando o agente s\u00f3 \u00e9 avaliado ap\u00f3s a execu\u00e7\u00e3o \nde uma seq\u00fc\u00eancia de a\u00e7\u00f5es e n\u00e3o para cada a\u00e7\u00e3o tomada individualmente. \n\nNeste trabalho, prop\u00f5e-se um modelo de coordena\u00e7\u00e3o baseado em neuro-evolu\u00e7\u00e3o. \nEstendemos o Enforced Subpopulations, m\u00e9todo de neuro-evolu\u00e7\u00e3o proposto por \nGomez (1997), permitindo maior flexibilidade na defini\u00e7\u00e3o da topologia da rede neural \nde cada agente. \n\nNo cap\u00edtulo 2, \u00e9 fornecida uma vis\u00e3o geral sobre as principais disciplinas abordadas \nneste trabalho, fala-se brevemente sobre tarefas de decis\u00e3o seq\u00fcencial, aprendizado por \nrefor\u00e7o, algoritmos gen\u00e9ticos e redes neurais artificiais. No cap\u00edtulo 3 \u00e9 apresentada a \nneuro-evolu\u00e7\u00e3o, suas principais caracter\u00edsticas e alguns dos m\u00e9todos de neuro-evolu\u00e7\u00e3o \nexistentes na literatura. No cap\u00edtulo 4, apresenta-se o modelo de coordena\u00e7\u00e3o proposto. \nNo cap\u00edtulo 5, descreve-se um subconjunto dos experimentos realizados a fim de se \navaliar o modelo proposto e definir valores adequados para os par\u00e2metros do modelo. \nPor fim, no cap\u00edtulo 6, s\u00e3o apresentadas as conclus\u00f5es deste trabalho. \n\n\n\n \n\n2 CONCEITOS B\u00c1SICOS \n\nEste cap\u00edtulo prov\u00ea uma vis\u00e3o geral dos diversos conceitos relacionados ao presente \ntrabalho. O objetivo deste trabalho foi o desenvolvimento de um modelo para \ncoordena\u00e7\u00e3o de sistemas multiagentes em ambientes din\u00e2micos e complexos. \nAprendizado por refor\u00e7o \u00e9 uma das t\u00e9cnicas mais utilizadas para aprendizagem de \nm\u00e1quina, \u00e9 tamb\u00e9m uma das t\u00e9cnicas mais adequadas para aprendizagem em ambientes \ndin\u00e2micos e complexos, onde, na maioria das vezes, o comportamento ideal n\u00e3o \u00e9 \npreviamente conhecido. No aprendizado por refor\u00e7o, n\u00e3o se estabelece, a priori, como \ncada agente deve se comportar. Ao inv\u00e9s disso, o agente aprende seu comportamento a \npartir dos sinais de refor\u00e7o que recebe do ambiente. Estes sinais de refor\u00e7o indicam \nqu\u00e3o bem ou qu\u00e3o mal o agente se comporta. \n\nA neuro-evolu\u00e7\u00e3o, ou evolu\u00e7\u00e3o de redes neurais por algoritmos gen\u00e9ticos, \u00e9 uma das \nformas existentes de aprendizado por refor\u00e7o. Uma das vantagens da neuro-evolu\u00e7\u00e3o \u00e9 \nque ela possui um bom desempenho, mesmo para problemas de aprendizado por refor\u00e7o \nem que o sinal de refor\u00e7o \u00e9 esparso (MORIARTY, 1997). Isto \u00e9, ao inv\u00e9s do ambiente \nenviar um sinal de refor\u00e7o para cada a\u00e7\u00e3o do agente, o sinal de refor\u00e7o \u00e9 enviado ap\u00f3s a \nrealiza\u00e7\u00e3o de um conjunto de a\u00e7\u00f5es. Isto \u00e9 o que geralmente ocorre para uma classe de \nproblemas conhecida como tarefas de decis\u00e3o seq\u00fcencial. Uma tarefa \u00e9 dita de decis\u00e3o \nseq\u00fcencial quando uma seq\u00fc\u00eancia de a\u00e7\u00f5es precisa ser realizada para que o efeito delas \npossa ser medido.  \n\n2.1 Agentes e Sistemas Multiagentes \nNeste trabalho, estamos considerando um agente como uma entidade capaz de \n\nrealizar algumas tarefas, de forma aut\u00f4noma, para atingir seus objetivos. Geralmente, \nagentes s\u00e3o capazes de se comunicar com outros agentes e com o ambiente2.  \n\nAl\u00e9m da autonomia e da capacidade de comunica\u00e7\u00e3o, um agente pode possuir \ndiversas outras caracter\u00edsticas, tais como reatividade (capacidade de reagir \napropriadamente \u00e0s influ\u00eancias ou informa\u00e7\u00f5es de seu ambiente) e pro-atividade \n(capacidade de um agente de tomar iniciativas sob circunst\u00e2ncias espec\u00edficas).  \n\n                                                 \n2 Algumas entidades podem agir completamente por conta pr\u00f3pria. Contudo, geralmente \nn\u00e3o se refere a este tipo de sistema como sistema baseado em agentes, desde que os \naspectos que despertam interesse, riqueza e complexidade aos sistemas de agentes s\u00e3o \nas itera\u00e7\u00f5es entre agentes. \n\n\n\n \n\nPode-se visualizar sistemas multiagentes, como sistemas computacionais em que \nv\u00e1rios agentes interagem ou trabalham em conjunto para desempenhar algum conjunto \nde tarefas ou satisfazer algum conjunto de objetivos (LESSER, 1995). \n\nSistemas multiagentes tendem a ser mais eficientes, robustos e flex\u00edveis que \nabordagens centralizadas. Por\u00e9m, para serem serem efetivos, geralmente os agentes \nprecisam agir de forma coordenada. \n\n2.2 Coordena\u00e7\u00e3o \nSegundo Kahn (2000), o desafio das aplica\u00e7\u00f5es como um todo, do ponto de vista de \n\nprocessamento e comunica\u00e7\u00e3o, \u00e9 como implementar comportamentos comp lexos \njuntando o comportamento de um conjunto de indiv\u00edduos. \n\nTodos n\u00f3s temos um senso intuitivo do significado da palavra \u201ccoordena\u00e7\u00e3o\u201d. Ao \nassistirmos um jogo de v\u00f4lei ou de futebol, nos damos conta de qu\u00e3o bem coordenadas  \ns\u00e3o as a\u00e7\u00f5es desempenhadas por um grupo de pessoas. Freq\u00fcentemente, contudo, n\u00f3s \nnotamos mais facilmente a falta de coordena\u00e7\u00e3o: quando o hotel em que fizemos reserva \nest\u00e1 lotado, ou quando esperamos por horas no aeroporto porque a linha \u00e1rea n\u00e3o sabe \ninformar qual o port\u00e3o de embarque. \n\nDe maneira geral, esse significado intuitivo do que \u00e9 coordena\u00e7\u00e3o \u00e9 suficiente. \nContudo, ao tentar caracterizar uma nova \u00e1rea de estudo interdisciplinar, \u00e9 importante se \nter uma id\u00e9ia mais precisa do que \u00e9 coordena\u00e7\u00e3o. \n\nAlgumas defini\u00e7\u00f5es de coordena\u00e7\u00e3o incluem: \n\n\u201cCoordena\u00e7\u00e3o consiste dos protocolos, tarefas e mecanismos de tomada de decis\u00e3o \nprojetados para alcan\u00e7ar objetivos comuns entre unidades inter-dependentes\u201d \n(THOMPSON, 1967) \n\n\u201cOs esfor\u00e7os conjuntos de atores comunicantes independentes atrav\u00e9s de objetivos \nmutuamente definidos\u201d (NSF, 1989) \n\n\u201cA integra\u00e7\u00e3o e o ajuste harmonioso dos esfor\u00e7os individuais para cumprir um \nobjetivo maior\u201d (SINGH, 1992) \n\n\u201cCoordena\u00e7\u00e3o \u00e9 gerenciar depend \u00eancias entre atividades\u201d (MALONE, 1994) \n\nPara a proposta deste trabalho, contudo, consideraremos coordena\u00e7\u00e3o como o \nprocesso pelo qual um agente raciocina sobre suas a\u00e7\u00f5es locais e as a\u00e7\u00f5es dos outros \npara tentar garantir que a comunidade se comporte de uma maneira coerente. \n\n2.3 Tarefas de Decis\u00e3o Seq\u00fcencial \nTarefas de decis\u00e3o seq\u00fcencial (MORIARTY, 1997) est\u00e3o entre os problemas mais \n\ngerais e dif\u00edceis da aprendizagem de m\u00e1quina. Uma tarefa \u00e9 dita de decis\u00e3o seq\u00fcencial \nse seu resultado s\u00f3 pode ser conhecido ap\u00f3s a tomada de uma seq\u00fc\u00eancia inteira de \ndecis\u00f5es.  \n\nTarefas de decis\u00e3o seq\u00fcencial (BARTO, 1990; GREFENSTETTE, 1990) podem ser \ncaracterizadas pelo seguinte cen\u00e1rio: um agente observa o estado de um sistema \ndin\u00e2mico e escolhe uma a\u00e7\u00e3o a de um conjunto finito de a\u00e7\u00f5es. O sistema ent\u00e3o fornece \num novo estado, a partir do qual o agente deve selecionar uma outra a\u00e7\u00e3o. O sistema \npode retornar uma recompensa ou ap\u00f3s cada decis\u00e3o do agente ou ap\u00f3s um conjunto de \n\n\n\n \n\ndecis\u00f5es. O objetivo \u00e9 selecionar a seq\u00fc\u00eancia de a\u00e7\u00f5es que retorne a maior recompensa \nacumulada. Geralmente, a melhor estrat\u00e9gia n\u00e3o \u00e9 ma ximizar a recompensa por cada \na\u00e7\u00e3o individual. Pois, algumas a\u00e7\u00f5es podem produzir altas recompensas, mas, levar a \nestados a partir dos quais n\u00e3o \u00e9 poss\u00edvel receber recompensas altas posteriormente. \n\nO fato de que as decis\u00f5es freq\u00fcentemente levam a conseq\u00fc\u00eancias tanto imediatas \nquanto futuras dificulta a solu\u00e7\u00e3o de tarefas de decis\u00e3o seq\u00fcencial. Em jogos como o \nxadrez, por exemplo, muitas vezes \u00e9 dif\u00edcil saber se um movimento isolado deve ser \navaliado como bom, como ruim ou neutro. Considere, por exemplo, a captura de uma \npe\u00e7a. Esta a\u00e7\u00e3o pode conseguir uma recompensa imediata alta, pois, diminui a \nquantidade de pe\u00e7as do advers\u00e1rio. Contudo, esta mesma captura pode levar a uma \nrecompensa negativa no futuro se a pe\u00e7a respons\u00e1vel pela captura, sair de uma posi\u00e7\u00e3o \ndefensiva chave, desprotege ndo o rei. O resultado de uma partida de xadrez s\u00f3 \u00e9 \nconhecido ap\u00f3s muitas decis\u00f5es.  \n\nA estrat\u00e9gia de decis\u00e3o deve levar em conta tanto as recompensas imediatas quanto \nas futuras de forma a otimizar a recompensa total. Logo, \u00e9 dif\u00edcil determinar como \najustar pol\u00edticas em uma tarefa de decis\u00e3o seq\u00fcencial. Minsky (1963) chamou este \nproblema de atribui\u00e7\u00e3o de cr\u00e9dito. \n\nO aprendizado por refor\u00e7o \u00e9 uma abordagem simb\u00f3lica robusta para tarefas de \ndecis\u00e3o seq\u00fcencial. Moriarty (1997) compara m\u00e9todos evolucion\u00e1rios e m\u00e9todos \ntradicionais de aprendizado por refor\u00e7o e mostra que m\u00e9todos evolucion\u00e1rios t\u00eam \nvantagem devido ao mecanismo de atribui\u00e7\u00e3o de cr\u00e9dito mais robusto. Muitas outras \npesquisas (BELEW, 1993; NOLFI, 1994) demonstraram que a ne uro-evolu\u00e7\u00e3o, ou a \nevolu\u00e7\u00e3o simulada de redes neurais, \u00e9 uma estrat\u00e9gia efetiva para resolver problemas de \ndecis\u00e3o seq\u00fcencial. \n\nAlgoritmos evolucion\u00e1rios (AE) s\u00e3o adequados para tarefas de decis\u00e3o seq\u00fcencial \nporque eles trabalham naturalmente com refor\u00e7o esparso e buscam por uma solu\u00e7\u00e3o de \nforma global, realizando poucas suposi\u00e7\u00f5es sobre o dom\u00ednio de solu\u00e7\u00e3o. \n\n2.4 Aprendizado por Refor\u00e7o \nO aprendizado por refor\u00e7o (AR) \u00e9 o problema de um agente aprender seu \n\ncomportamento atrav\u00e9s de intera\u00e7\u00f5es de tentativa e erro em um ambiente din\u00e2mico. No \nAR, o agente tomador de decis\u00e3o recebe respostas do dom\u00ednio na forma de sinais de \nrefor\u00e7o. Tais sinais fornecem somente uma medida geral da profici\u00eancia na tarefa e n\u00e3o \ndirecionam explicitamente o agente para qualquer curso de a\u00e7\u00e3o. Ao contr\u00e1rio da \nmaioria das formas de aprendizagem de m\u00e1quina, n\u00e3o h\u00e1 nada dizendo ao agente quais \ndevem ser suas a\u00e7\u00f5es.  \n\nNeste trabalho, foi adotada a abordagem de alguns pesquisadores (KAELBLING, \n1996; SUTTON, 1998) que definem aprendizado por refor\u00e7o como uma classe de \nproblemas e n\u00e3o como um conjunto de t\u00e9cnicas. Por outro lado, qualquer m\u00e9todo que \nseja adequado para resolver tais problemas, \u00e9 considerado um m\u00e9todo de aprendizagem \npor refor\u00e7o. \n\nA principal diferen\u00e7a entre o aprendizado por refor\u00e7o e o problema mais largamente \nestudado de aprendizado supervisionado \u00e9 que, no aprendizado por refor\u00e7o, n\u00e3o h\u00e1 \nnenhuma apresenta\u00e7\u00e3o de pares de entrada/sa\u00edda, ou seja, n\u00e3o h\u00e1 exemplos de \ncomportamento fornecidos por um supervisor externo. Em aprendizado supervisionado, \no agente acessa exemplos do comportamento correto e aprende atrav\u00e9s dos erros entre \n\n\n\n \n\nsuas decis\u00f5es e as decis\u00f5es corretas conhecidas. Na aprendizagem por refor\u00e7o, o curso \ncorreto da a\u00e7\u00e3o n\u00e3o \u00e9 conhecido. O agente precisa aprender o comportamento bom \natrav\u00e9s de tentativa e erro, interagindo diretamente com o ambiente. O agente escolhe \numa a\u00e7\u00e3o, recebe uma recompensa por esta a\u00e7\u00e3o e conhece o estado subseq\u00fcente, mas, \nn\u00e3o sabe qual \u00e9 a a\u00e7\u00e3o que teria atendido melhor seus interesses a longo prazo. \u00c9 \nnecess\u00e1rio que o agente re\u00fana experi\u00eancia \u00fatil sobre os estados poss\u00edveis do sistema, \na\u00e7\u00f5es, transi\u00e7\u00f5es e recompensas para agir de forma \u00f3tima. \n\nEmbora o aprendizado supervisionado seja um tipo de aprendizagem importante, em \ndeterminados ambientes, \u00e9 freq\u00fcentemente impratic\u00e1vel obter exemplos corretos e \nrepresentativos do comportamento desejado do agente para todas as situa\u00e7\u00f5es poss\u00edveis. \nEm ambientes din\u00e2micos e complexos, um agente precisa ser capaz de aprender a partir \nde sua pr\u00f3pria experi\u00eancia. \n\nNo modelo padr\u00e3o de aprendizado por refor\u00e7o, um agente \u00e9 ligado a seu ambiente \nvia percep\u00e7\u00e3o e a\u00e7\u00e3o. Conforme retratado na Figura 2.1, em cada passo de intera\u00e7\u00e3o, o \nagente recebe como entrada alguma indica\u00e7\u00e3o do estado atual, s, do ambiente; o agente \nent\u00e3o escolhe uma a\u00e7\u00e3o, a, para gerar uma sa\u00edda. A a\u00e7\u00e3o muda o estado do ambiente, e \no valor desta transi\u00e7\u00e3o de estado \u00e9 informado ao agente atrav\u00e9s de um sinal de refor\u00e7o r. \nO comportamento B do agente deve levar a a\u00e7\u00f5es que tendem a aumentar a soma de \nseus sinais de refor\u00e7o ao longo da execu\u00e7\u00e3o. O agente pode aprender a fazer isso atrav\u00e9s \nde sistem\u00e1tica tentativa e erro. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 2.1: Modelo padr\u00e3o do aprendizado. \n\n \n\nFormalmente, o modelo consiste de: \n\n\u2022 Um conjunto discreto de estados do ambiente, S; \n\u2022 Um conjunto discreto de a\u00e7\u00f5es do agente, A; \n\u2022 Um conjunto escalar de sinais de refor\u00e7o; tipicamente {0,1}, ou n\u00fameros reais. \n \n\nA figura tamb\u00e9m inclui uma fun\u00e7\u00e3o de entrada I, que determina como o agente v\u00ea o \nestado do ambiente; o agente pode perceber o estado exato ou parcial do ambiente.  \n\nO trabalho do agente \u00e9 achar uma pol\u00edtica p, que mapeie estados para a\u00e7\u00f5es, \nmaximizando a medida de refor\u00e7o ao longo do tempo. Espera-se, em geral, que o \nambiente seja n\u00e3o-determin\u00edstico; isto \u00e9, tomando-se a mesma a\u00e7\u00e3o no mesmo estado \nem duas ocasi\u00f5es diferentes podem resultar diferentes pr\u00f3ximos estados e/ou em valores \nde refor\u00e7o diferentes. Entretanto, sup\u00f5e-se um ambiente estacion\u00e1rio; isto \u00e9, as \nprobabilidades de fazer transi\u00e7\u00f5es de estado recebendo sinais espec\u00edficos de refor\u00e7o n\u00e3o \nmudam com o tempo. \n\nI \nR \n\nB \n\nr \n\ni \na \n\ns \n\nAmbiente  \n\n\n\n \n\nQuando o refor\u00e7o est\u00e1 dispon\u00edvel somente ocasionalmente, o refor\u00e7o \u00e9 dito esparso. \nRefor\u00e7o esparso significa que uma solu\u00e7\u00e3o proposta ter\u00e1 de ser avaliada v\u00e1rias vezes \nantes de qualquer informa\u00e7\u00e3o de retorno se tornar dispon\u00edvel. Jogos, como o xadrez, s\u00e3o \nos principais exemplos de dom\u00ednios esparsos: muitos movimentos precisam ser \nrealizados antes do sinal de perda ou ganho estar dispon\u00edvel. \n\n2.4.1 Exploration e exploitation \n\nDe acordo com Sutton (1998), um dos maiores desafios que surgem para m\u00e9todos de \naprendizado por refor\u00e7o e que n\u00e3o existem em outros tipos de aprendizado \u00e9 o de \nbalancear exploration e exploitation.  \n\nA \u00fanica forma de obter nova informa\u00e7\u00e3o sobre o espa\u00e7o de solu\u00e7\u00f5es \u00e9 gerar uma \nnova solu\u00e7\u00e3o candidata e avali\u00e1- la. A id\u00e9ia central do algoritmo da subida da encosta \u00e9 \nque valores de aptid\u00e3o mais altos est\u00e3o pr\u00f3ximos dos pontos j\u00e1 avaliados como bons. \nIsto \u00e9, escolhe-se um ponto inicial arbitr\u00e1rio e investiga-se os elementos adjacentes a ele \nno espa\u00e7o de busca. Se algum desses pontos possuir um valor maior, ent\u00e3o esse ponto \npassa a ser o novo referencial (BARRETO, 2003). Se o espa\u00e7o de valores de aptid\u00e3o \u00e9 \nmonot\u00f4nico (possui somente um pico) ent\u00e3o a busca pr\u00f3xima da melhor solu\u00e7\u00e3o atual \nir\u00e1 sempre levar ao pico. A busca pr\u00f3xima de regi\u00f5es conhecidas como bias \u00e9 chamada \nexploitation, pois, explora o conhecimento corrente. A menos que o espa\u00e7o de solu\u00e7\u00f5es \nseja completamente ca\u00f3tico, pontos pr\u00f3ximos de um ponto conhecido como bom t\u00eam \nmais chance de produzir uma melhora na solu\u00e7\u00e3o que uma tentativa aleat\u00f3ria. \n\nContudo, problemas interessantes s\u00e3o multimodais. Eles possuem muitos picos, \nchamados \u00f3timos locais, alguns dos quais est\u00e3o bastante distantes do verdadeiro ponto \n\u00f3timo global. T\u00e9cnicas estritas de subida na encosta, mesmo fazendo uso sofisticado da \ninforma\u00e7\u00e3o de gradiente local, est\u00e3o pr\u00e9-dispostas a convergir para um ponto \u00f3timo \nlocal, simplesmente pelo fato deles serem maioria. Esta \u00e9 a marca registrada de uma \nt\u00e9cnica super-exploitative. Uma forma de resolver este problema \u00e9 re- iniciar em uma \nposi\u00e7\u00e3o aleat\u00f3ria quando uma converg\u00eancia \u00e9 detectada.  Um re- in\u00edcio aleat\u00f3rio \u00e9 um \nmovimento exploratory. Subidas da encosta s\u00e3o puramente exploitative, logo, s\u00e3o \nindicadas somente para descobrir \u00f3timos locais. \n\nExiste uma tens\u00e3o inerente entre exploitation e exploration em qualquer m\u00e9todo de \naprendizagem de m\u00e1quina gerado e testado.  Para cada teste de uma solu\u00e7\u00e3o \u00e9 \nnecess\u00e1rio decidir se explorar um novo territ\u00f3rio ou buscar por uma solu\u00e7\u00e3o perto das \nregi\u00f5es conhecidas como boas. Explorar uma nova regi\u00e3o pode revela r solu\u00e7\u00f5es \npromissoras, mas tentativas aleat\u00f3rias cont\u00ednuas s\u00e3o t\u00e3o ruins quanto enumera\u00e7\u00e3o \nexaustiva. T\u00e9 cnicas ideais devem continuamente balancear exploitation e exploration. \nNa se\u00e7\u00e3o 2.5, \u00e9 citado como algoritmos gen\u00e9ticos realizam o balanceamento entre a \nbusca em regi\u00f5es conhecidas como boas e regi\u00f5es ainda desconhecidas. \n\n2.4.2 Estrat\u00e9gias de Aprendizado por Refor\u00e7o \n\nH\u00e1 duas estrat\u00e9gias principais para resolver problemas de aprendizado por refor\u00e7o. \nA primeiro \u00e9 a busca no espa\u00e7o de comportamentos para achar um que execute bem no \nambiente. Esta estrat\u00e9gia \u00e9 geralmente implementada com algoritmos gen\u00e9ticos e \nprograma\u00e7\u00e3o gen\u00e9tica. A segunda abordagem consiste em tr\u00eas classes fundamentais de \nm\u00e9todos: programa\u00e7\u00e3o din\u00e2mica, m\u00e9todos de Monte Carlo e diferen\u00e7a temporal \n(SUTTON, 1998).  \n\n\n\n \n\nM\u00e9todos de programa\u00e7\u00e3o din\u00e2mica s\u00e3o bem desenvolvidos matematicamente, mas, \nrequerem um modelo completo e preciso do ambiente. M\u00e9todos de Monte Carlo n\u00e3o \nprecisam de um modelo e s\u00e3o conceitualmente simples, mas, a depender do tamanho \ndos espa\u00e7os de a\u00e7\u00f5es e de estados, a converg\u00eancia pode ser bastante lenta. Finalmente, \nm\u00e9todos de diferen\u00e7a temporal n\u00e3o precisam de modelo e s\u00e3o totalmente incrementais, \nmas s\u00e3o mais complexos de analisar. Estes m\u00e9todos tamb\u00e9m diferem em efici\u00eancia e \nvelocidade de converg\u00eancia. Para maiores detalhes, consulte Sutton (1998). \n\nO termo programa\u00e7\u00e3o din\u00e2mica (DP) se refere a uma cole\u00e7\u00e3o de algoritmos que \npodem ser usados para computar pol\u00edticas \u00f3timas, desde que seja fornecido um modelo \nperfeito do ambiente como um processo de decis\u00e3o de Markov (MDP). Algoritmos \ncl\u00e1ssicos de DP possuem utilidade limitada no aprendizado por refor\u00e7o principalmente \npor dois motivos: sup\u00f5em um modelo perfeito do mundo e possuem alto custo \ncomputacional.   \n\nM\u00e9todos de Monte Carlo n\u00e3o assumem o conhecimento completo do ambiente; ao \ninv\u00e9s disso, eles precisam de experi\u00eancia, ou seja, exemplos de estados, a\u00e7\u00f5es e \nrecompensas de intera\u00e7\u00f5es do agente com o ambiente. A maior desvantagem do m\u00e9todo \nde Monte Carlo \u00e9 a converg\u00eancia lenta. Embora, a converg\u00eancia pare\u00e7a inevit\u00e1vel, ela \nn\u00e3o possui uma prova formal. Segundo Sutton (1998), esta \u00e9 uma das quest\u00f5es mais \nimportantes no aprendizado por refor\u00e7o que ainda permanece em aberto.  \n\nAprendizado por diferen\u00e7as temporais \u00e9 uma combina\u00e7\u00e3o das id\u00e9ias de Monte Carlo \ne programa\u00e7\u00e3o din\u00e2mica. Da mesma forma que m\u00e9todos de Monte Carlo, m\u00e9todos de \nTD podem aprender diretamente da experi\u00eancia sem um modelo das din\u00e2micas do \nambiente. Como DP, m\u00e9todos de TD podem atualizar estimativas baseados, em parte, \nnas estimativas aprendidas, sem esperar por uma sa\u00edda final.  \n\nSegundo Moriarty (1997), TD \u00e9 o m\u00e9todo de aprendizado por refor\u00e7o mais popular. \nEm TD, uma fun\u00e7\u00e3o valor prediz o retorno esperado do ambiente dado o estado corrente \ndo mundo e a pol\u00edtica de decis\u00e3o corrente. Se a fun\u00e7\u00e3o valor for precisa, o agente pode \nbasear todas as suas decis\u00f5es nos valores previstos para os estados subseq\u00fcentes do \nmundo. Em outras palavras, ao selecionar a pr\u00f3xima decis\u00e3o, o agente considera o efeito \ndaquela decis\u00e3o atrav\u00e9s do exame do valor esperado da transi\u00e7\u00e3o de estado causada por  \naquela decis\u00e3o. \n\nA fun\u00e7\u00e3o valor \u00f3tima \u00e9 alcan\u00e7ada usando uma vers\u00e3o do algoritmo de aprendizagem \nTD(?). TD(?) usa observa\u00e7\u00f5es de diferen\u00e7as de previs\u00e3o de estados consecutivos para \naprender predi\u00e7\u00f5es de va lores corretos. Suponha que dois estados consecutivos i e j \nretornem valores de predi\u00e7\u00e3o de recompensa 5 e 2, respectivamente. A diferen\u00e7a sugere \nque a recompensa do estado i pode ter sido superestimada e deve ser reduzida para \nconcordar com a predi\u00e7\u00e3o do estado j.  A atualiza\u00e7\u00e3o do valor da fun\u00e7\u00e3o V \u00e9 realizada \nusando a seguinte regra: \n\n \n\nV(i) = V(i) + ?(( V(j) \u2013 V(i)) + R(i))                                                         (2.1)  \n\n \n\nonde ? representa a taxa de aprendizagem e R a recompensa imediata. Logo, a diferen\u00e7a \nna predi\u00e7\u00e3o (V(j) \u2013 V(i)) de estados consecutivos \u00e9 usada como uma medida de predi\u00e7\u00e3o \nde erro.  Pode-se imaginar um longo caminho de valores de predi\u00e7\u00e3o V(0)...V(n) de \ntransi\u00e7\u00f5es de estados consecutivos com o \u00faltimo estado V(n) contendo a verdadeira \n\n\n\n \n\nrecompensa do ambiente. Os valores de cada estado s\u00e3o ajustados de forma que eles \nconcordem com seus sucessores e eventualmente com a recompensa verdadeira em \nV(n). Em outras palavras, a recompensa verdadeir a \u00e9 propagada de volta atrav\u00e9s do \ncaminho das predi\u00e7\u00f5es de valores. O resultado da rede \u00e9 uma fun\u00e7\u00e3o valor precisa que \npode ser usada para acessar a utilidade das decis\u00f5es comparando valores de transi\u00e7\u00f5es \nde estados subseq\u00fcentes. \n\nAs duas implementa\u00e7\u00f5es mais proeminentes de TD s\u00e3o Adaptative Heuristic Critic \n(AHC)(BARTO, 1983) e Q-learning (WATKINS, 1989). \n\n2.4.2.1 Adptative Heuristic Critic \n\nUm dos primeiros m\u00e9todos de aprendizagem por refor\u00e7o que usou a aprendizagem \nTD \u00e9 o AHC. No AHC, uma fun\u00e7\u00e3o valor TD, chamada \u201ccr\u00edtica\u201d, \u00e9 treinada para \npredizer o desempenho de um segundo agente que \u00e9 respons\u00e1vel por gerar as decis\u00f5es. \nA fun\u00e7\u00e3o cr\u00edtica usa a equa\u00e7\u00e3o 2.1 para aprender as previs\u00f5es dos valores, dado as \ntransi\u00e7\u00f5es de estado causadas pelo agente de decis\u00e3o. O agente de decis\u00e3o \nsimultaneamente atualiza sua pol\u00edtica de decis\u00e3o para maximizar o valor recebido da \nfun\u00e7\u00e3o cr\u00edtica. Por exemplo, se o agente de decis\u00e3o recebe um valor baixo da fun\u00e7\u00e3o \ncr\u00edtica ap\u00f3s tomar uma decis\u00e3o, ele deveria reduzir a probabilidade de tomar aquela \ndecis\u00e3o na mesma situa\u00e7\u00e3o. Desde que o agente de decis\u00e3o recebe um retorno constante \nda fun\u00e7\u00e3o cr\u00edtica, modifica\u00e7\u00f5es da pol\u00edtica podem ser feitas atrav\u00e9s de diferentes \nm\u00e9todos de subida da encosta. A abordagem mais comum \u00e9 usar uma variante do \nm\u00e9todo backpropagation. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 2.2: Vis\u00e3o Geral da AHC \n\n \n\n2.4.2.2 Q-learning \n\nQ-learning \u00e9 a abordagem de aprendizado por refor\u00e7o mais utilizada.  Q- learning \ncombina os agentes de cr\u00edtica e decis\u00e3o em uma \u00fanica fun\u00e7\u00e3o chamada Q-fun\u00e7\u00e3o. A Q-\nfun\u00e7\u00e3o mapeia decis\u00f5es e estados do mundo em estimativas de recompensa esperadas. \nEm outras palavras, a Q- fun\u00e7\u00e3o Q(d,i) representa a utilidade de tomar uma decis\u00e3o \nespec\u00edfica d no estado i. Dados valores Q-fun\u00e7\u00e3o precisos, chamados Q-valores, uma \npol\u00edtica \u00f3tima seleciona para cada estado a decis\u00e3o com o mais alto valor Q associado \n(recompensa esperada).  \n\n \n\nA fun\u00e7\u00e3o-Q \u00e9 aprendida atrav\u00e9s da seguinte equa\u00e7\u00e3o de atualiza\u00e7\u00e3o TD: \n\nCr\u00edtica \n\nAgente \n\nRefor\u00e7o \n\nSensores \n\nA\u00e7\u00e3o \nValor \n\n\n\n \n\n \n\nQ(d,i) = Q(d,i) + ?(R(i) + maxd\u2019Q(d\u2019,i\u2019) - Q(d,i))                                          (2.2) \n\n    \n\nonde d\u2019 \u00e9 a pr\u00f3xima decis\u00e3o e i\u2019 \u00e9 o pr\u00f3ximo estado. Essencialmente, esta equa\u00e7\u00e3o \natualiza Q(d,i) baseada na recompensa corrente e a recompensa prevista se todas as \ndecis\u00f5es futuras s\u00e3o selecionadas otimamente. Watkins e Dayan (1992) provaram que se \nas atualiza\u00e7\u00f5es s\u00e3o desempenhadas desta maneira, a Q- fun\u00e7\u00e3o ir\u00e1 convergir para os \nvalores Q \u00f3timos. O sistema de aprendizagem por refor\u00e7o pode logo usar os Q valores \npara avaliar cada decis\u00e3o que \u00e9 poss\u00edvel a partir de um dado estado. A decis\u00e3o que \nretorna o maior Q-valor \u00e9 a escolha \u00f3tima. \n\n2.4.2.3 Abordagem Evolucion\u00e1ria \n\nAlgoritmos evolucion\u00e1rios fornecem uma ferramenta de treinamento geral em que \npoucas suposi\u00e7\u00f5es sobre o dom\u00ednio s\u00e3o necess\u00e1rias. Desde que algoritmos \nevolucion\u00e1rios somente precisam de uma \u00fanica fun\u00e7\u00e3o de avalia\u00e7\u00e3o (aptid\u00e3o) sobre toda \na tarefa (possivelmente composta de v\u00e1rios passos), eles s\u00e3o capazes de aprender em \ndom\u00ednios com refor\u00e7os esparsos, o que os torna bem adequados para avaliar o \ndesempenho de tarefas de decis\u00e3o. N\u00e3o \u00e9 preciso exemplos do comportamento correto.  \nO algoritmo evolucion\u00e1rio busca pelas estrat\u00e9gias de decis\u00e3o mais produtivas usando \nsomente recompensas raras retornadas por um sistema subjacente. Juntos, algoritmos \nevolucion\u00e1rios e redes neurais oferecem uma abordagem promissora para o aprendizado \ne aplica\u00e7\u00e3o de estrat\u00e9gias efetivas de decis\u00e3o em v\u00e1rias situa\u00e7\u00f5es diferentes. \n\nAlgoritmos evolucion\u00e1rios s\u00e3o t\u00e9cnicas de busca globais, inspiradas na teoria de \nDarwin da evolu\u00e7\u00e3o natural. Solu\u00e7\u00f5es potenciais s\u00e3o codificadas em estruturas \nchamadas cromossomos. Durante cada itera\u00e7\u00e3o, o AE avalia solu\u00e7\u00f5es e gera filhos \nbaseado na aptid\u00e3o de cada solu\u00e7\u00e3o na tarefa. Subestruturas, ou genes, das solu\u00e7\u00f5es s\u00e3o \nent\u00e3o modificados atrav\u00e9s de opera\u00e7\u00f5es gen\u00e9ticas tais como muta\u00e7\u00e3o e recombina\u00e7\u00e3o. A \nid\u00e9ia \u00e9 que estruturas que levam a boas solu\u00e7\u00f5es em avalia\u00e7\u00f5es anteriores podem sofrer \nmuta\u00e7\u00e3o ou serem combinadas para formar solu\u00e7\u00f5es melhores em avalia\u00e7\u00f5es \nposteriores. \n\nNo aprendizado por refor\u00e7o evolucion\u00e1rio, as solu\u00e7\u00f5es tomam a forma de agentes \ntomadores de decis\u00e3o que operam em ambientes din\u00e2micos.  Agentes s\u00e3o localizados no \nmundo onde tomam decis\u00f5es em resposta \u00e0s condi\u00e7\u00f5es ambientais. O AE seleciona o \nagente baseado no seu desempenho na tarefa, e aplica operadores gen\u00e9ticos para gerar \nnovos tipos de agentes. Desde que algoritmos evolucion\u00e1rios requisitam somente uma \n\u00fanica avalia\u00e7\u00e3o de aptid\u00e3o sobre a tarefa inteira do agente (normalmente envolvendo \nv\u00e1rios passos), eles encontram solu\u00e7\u00f5es efetivas em dom\u00ednios que retornam somente \nrefor\u00e7os ocasionais sobre uma seq\u00fc\u00eancia de a\u00e7\u00f5es. O \u00fanico retorno requerido do \nambiente \u00e9 uma medida geral da profici\u00eancia de cada agente. \n\nComo m\u00e9todos TD, o aprendizado por refor\u00e7o evolucion\u00e1rio \u00e9 uma abordagem livre \nde modelos, porque n\u00e3o requisita um modelo de simula\u00e7\u00e3o ou conhecimento das regras \nde transi\u00e7\u00e3o de estado para formar suas pol\u00edticas.  \n\nSegundo Kaelbling (1996), n\u00e3o \u00e9 bem claro qual das abordagens, m\u00e9todos \ntradicionais ou m\u00e9todos evolucion\u00e1rios de aprendizado por refor\u00e7o evolucion\u00e1rio, \u00e9 \nmelhor em que circunst\u00e2ncias. Contudo, neste trabalho, optou-se pela utiliza\u00e7\u00e3o da \nneuro-evolu\u00e7\u00e3o, uma das abordagens evolucion\u00e1rias existentes, por algumas raz\u00f5es: \n\n\n\n \n\nprimeiro porque em ambientes din\u00e2micos e complexos, \u00e9 impratic\u00e1vel criar um modelo \npreciso do mundo e algoritmos evolucion\u00e1rios n\u00e3o precisam de um modelo. Segundo \nporque redes neurais possuem uma boa generaliza\u00e7\u00e3o e suportam algumas varia\u00e7\u00f5es no \nmundo. Terceiro, o armazenamento das pol\u00edticas de decis\u00e3o nas redes neurais \u00e9 \necon\u00f4mico, a fun\u00e7\u00e3o de transi\u00e7\u00e3o \u00e9 representada por pesos nas conex\u00f5es da rede, isso \nsignifica que mesmo aumentando-se os espa\u00e7os de a\u00e7\u00f5es e estados do mundo, a \nquantidade de informa\u00e7\u00e3o armazenada n\u00e3o ser\u00e1 alterada. \n\nNo cap\u00edtulo 3, aborda-se a neuro-evolu\u00e7\u00e3o em maiores detalhes. Nas duas pr\u00f3ximas \nse\u00e7\u00f5es, \u00e9 fornecida uma vis\u00e3o geral de algoritmos gen\u00e9ticos e redes neurais, que \nconstituem os centros de apoio da neuro-evolu\u00e7\u00e3o. \n\n2.5 Algoritmos Gen\u00e9ticos \nAlgoritmos gen\u00e9ticos (AG) s\u00e3o m\u00e9todos de busca e otimiza\u00e7\u00e3o inspirados nos \n\nprocessos da evolu\u00e7\u00e3o natural dos seres vivos. Foram introduzidos por Holland (1975) e \npopularizados por um de seus alunos, David Goldberg (1989). Estes algoritmos seguem \no princ\u00edpio da sele\u00e7\u00e3o natural e sobreviv\u00eancia do mais apto declarado, em 1859, pelo \nnaturalista e fisiologista ingl\u00eas Charles Darwin em seu livro A origem das esp\u00e9cies. De \nacordo com Darwin, \u201cQuanto melhor um indiv\u00edduo se adaptar ao seu meio ambiente, \nmaior ser\u00e1 sua chance de sobreviver e gerar descendentes\u201d. \n\nNa natureza, os indiv\u00edduos mais adaptados ao ambiente ir\u00e3o vencer a competi\u00e7\u00e3o por \nrecursos limitados. A capacidade de sobreviv\u00eancia do indiv\u00edduo \u00e9 determinada por \nv\u00e1rias caracter\u00edsticas dele. As caracter\u00edsticas de cada indiv\u00edduo s\u00e3o, por sua vez, \ndeterminadas pelo conte\u00fado gen\u00e9tico do indiv\u00edduo. O conjunto de genes que controla \nestas caracter\u00eds ticas forma os cromossomos, que s\u00e3o a chave para a sobreviv\u00eancia do \nindiv\u00edduo em um ambiente competitivo. A sele\u00e7\u00e3o natural e a recombina\u00e7\u00e3o do material \ngen\u00e9tico durante a reprodu\u00e7\u00e3o constituem a for\u00e7a motora da evolu\u00e7\u00e3o. \n\nDesde que somente os indiv\u00edduos com maiores aptid\u00f5es sobrevivem e se \nreproduzem, os genes mais fracos ir\u00e3o desaparecer gradualmente. Se o ambiente n\u00e3o \nsofrer altera\u00e7\u00f5es, o processo de evolu\u00e7\u00e3o levar\u00e1 a um estado onde todos os indiv\u00edduos \nser\u00e3o constitu\u00eddos dos melhores genes. \n\nInspirados neste processo de evolu\u00e7\u00e3o natural, o uso de analogias do comportamento \nnatural levou ao desenvolvimento dos chamados algoritmos evolucion\u00e1rios. Estes \nalgoritmos s\u00e3o compostos de quatro elementos: uma estrutura de codifica\u00e7\u00e3o que ser\u00e1 \nreplicada, operadores que afetam os indiv\u00edduos da popula\u00e7\u00e3o, uma fun\u00e7\u00e3o de aptid\u00e3o \nque indica qu\u00e3o bom \u00e9 um indiv\u00edduo, e um mecanismo de sele\u00e7\u00e3o. Algoritmos gen\u00e9ticos \ns\u00e3o um dos principais paradigmas dentro dos algoritmos evolucion\u00e1rios. Eles operam \nem uma popula\u00e7\u00e3o de indiv\u00edduos, onde cada indiv\u00edduo representa uma poss\u00edvel solu\u00e7\u00e3o \npara o problema. Cada indiv\u00edduo recebe uma aptid\u00e3o baseado na fun\u00e7\u00e3o de aptid\u00e3o. Um \nmecanismo de sele\u00e7\u00e3o escolhe os indiv\u00edduos mais aptos para reprodu\u00e7\u00e3o. A reprodu\u00e7\u00e3o \nocorre atrav\u00e9s das t\u00e9cnicas de recombina\u00e7\u00e3o e muta\u00e7\u00e3o. \n\nN\u00e3o existem garantias de que AG encontrem o \u00f3timo global, mas eles s\u00e3o \ngeralmente bons em encontrar uma solu\u00e7\u00e3o aceit\u00e1vel dentro de um tempo razo\u00e1vel3.  \n\n                                                 \n3 O tempo necess\u00e1rio para busca ir\u00e1 variar dependendo de fatores como, por exemplo, a \ncomplexidade da tarefa e a codifica\u00e7\u00e3o do cromossomo. Contudo, diversos autores \n\n\n\n \n\nO primeiro passo de um algoritmo gen\u00e9tico t\u00edpico \u00e9 a gera\u00e7\u00e3o de uma popula\u00e7\u00e3o \ninicial de cromossomos. Esta popula\u00e7\u00e3o \u00e9 formada por um conjunto aleat\u00f3rio de \ncromossomos que representam poss\u00edveis solu\u00e7\u00f5es do problema a ser resolvido. Durante \no processo evolutivo, esta popula\u00e7\u00e3o \u00e9 avaliada e cada cromossomo recebe um valor de \naptid\u00e3o, refletindo qu\u00e3o bem ele resolve o problema. Os cromossomos mais aptos s\u00e3o \nselecionados e podem sofrer modifica\u00e7\u00f5es atrav\u00e9s dos operadores de recombina\u00e7\u00e3o e \nmuta\u00e7\u00e3o, gerando descendentes para a pr\u00f3xima gera\u00e7\u00e3o. \n\nHolland (1975) foi a primeira pessoa que prop\u00f4s programas de computador \ninspirados nos processos evolucion\u00e1rios da natureza. Este algoritmo gen\u00e9tico \u00e9 \ncomumente chamado de Algoritmo Gen\u00e9tico Simples, vide Figura 2.3.  \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 2.3: Algoritmo Gen\u00e9tico Simples. \n\n \n\nAGs operam em representa\u00e7\u00f5es codificadas das solu\u00e7\u00f5es, assim como os \ncromossomos dos indiv\u00edduos na natureza. \u00c9 assumido que uma solu\u00e7\u00e3o potencial pode \nser bem representada como um conjunto de par\u00e2metros e codificada como um \ncromossomo. No Algoritmo gen\u00e9tico Simples, Holland (1975) codificou as solu\u00e7\u00f5es \ncomo cadeias de bits de um alfabeto bin\u00e1rio.  \n\nCada indiv\u00edduo \u00e9 associado a um valor de aptid\u00e3o, retornado pela fun\u00e7\u00e3o de aptid\u00e3o, \nque reflete qu\u00e3o bom ele \u00e9. A sele\u00e7\u00e3o modela o mecanismo de sobreviv\u00eancia do mais \napto. O AG simples usa o algorit mo de sele\u00e7\u00e3o da roleta para realizar a sele\u00e7\u00e3o \nproporcional.  \n\nA id\u00e9ia do m\u00e9todo da Roleta \u00e9 que indiv\u00edduos de uma gera\u00e7\u00e3o sejam escolhidos para \nfazer parte da pr\u00f3xima gera\u00e7\u00e3o, atrav\u00e9s de um sorteio de roleta. Os indiv\u00edduos s\u00e3o \nrepresentados na roleta proporcionalmente ao seu \u00edndice de aptid\u00e3o. Finalmente, a roleta \n\u00e9 girada um determinado n\u00famero de vezes, dependendo do tamanho da popula\u00e7\u00e3o, e s\u00e3o \nescolhidos como indiv\u00edduos que participar\u00e3o da pr\u00f3xima gera\u00e7\u00e3o, aqueles sorteados na \nroleta. \n\n                                                                                                                                               \n\nmostram que a abordagem evolucion\u00e1ria \u00e9 bastante eficiente quando comparada aos \nm\u00e9todos de busca tradicionais (MORIARTY 1997). \n\nSeja S(t) a popula\u00e7\u00e3o de cromossomos na gera\u00e7\u00e3o t: \n \nt := 0 \nInicializar S(t) \nAvaliar S(t) \nFa\u00e7a geracao := 1 para n  \n         t := geracao \n         Selecione S(t) a partir de S(t-1) \n         Recombina\u00e7\u00e3o(S(t)) \n         Muta\u00e7\u00e3o(S(t)) \n         Avalie(S(t)) \nFim fa\u00e7a \n\n\n\n \n\nComputacionalmente, o m\u00e9todo da roleta pode ser implementado da seguinte \nmaneira: ordena-se os indiv\u00edduos da popula\u00e7\u00e3o por ordem ascendente de aptid\u00e3o, \ncalcula-se a soma das aptid\u00f5es de todos os indiv\u00edduos da popula\u00e7\u00e3o. Em seguida, gera-\nse um n\u00famero aleat\u00f3rio r (tirado de uma distribui\u00e7\u00e3o uniforme) no intervalo [0, total], \nonde total \u00e9 a soma de todas as aptid\u00f5es. Por fim, o cromossomo selecionado \u00e9 o \nprimeiro cromossomo que possui um total parcial maior que r, onde total parcial \u00e9 a \nsoma de sua aptid\u00e3o com as aptid\u00f5es dos cromossomos anteriores. A Figura 2.4 ilustra o \nalgoritmo da roleta. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 2.4: Algoritmo de Sele\u00e7\u00e3o da Roleta. \n\n \n\nOutro operador de sele\u00e7\u00e3o que pode ser utilizado \u00e9 a sele\u00e7\u00e3o por torneio. Na sele\u00e7\u00e3o \npor torneio, n cromossomos s\u00e3o escolhidos aleatoriamente e destes, o cromossomo com \nmaior aptid\u00e3o \u00e9 selecionado para reprodu\u00e7\u00e3o. O processo se repete at\u00e9 que a quantidade \nde cromossomos selecionados para reprodu\u00e7\u00e3o seja igual ao tamanho da popula\u00e7\u00e3o. \nGeralmente, utiliza-se o valor de n igual a 3. \n\nA fase de reprodu\u00e7\u00e3o do AG \u00e9 realizada atrav\u00e9s do mecanismo de recombina\u00e7\u00e3o.  O \nm\u00e9todo mais simples de recombina\u00e7\u00e3o \u00e9 escolher pares de cromossomos selecionados, \nem seguida, dividir os cromossomos de cada par em alguma posi\u00e7\u00e3o escolhida \naleatoriamente, e trocar seus segmentos, este mecanismo \u00e9 conhecido como \nrecombina\u00e7\u00e3o de 1-ponto. Uma outra opera\u00e7\u00e3o, chamada muta\u00e7\u00e3o, causa altera\u00e7\u00e3o \nespor\u00e1dica e aleat\u00f3ria dos bits das cadeias, que \u00e9 uma analogia direta da natureza e faz o \npapel de re-gerar material gen\u00e9tico perdido. A muta\u00e7\u00e3o \u00e9 aplicada aos indiv\u00edduos ap\u00f3s a \nrecombina\u00e7\u00e3o. Um par\u00e2metro, taxa de muta\u00e7\u00e3o, d\u00e1 a probabilidade de um bit ser \nalterado. \n\nUma op\u00e7\u00e3o freq\u00fcentemente usada em AG \u00e9 o elitismo. Esta op\u00e7\u00e3o permite que os \nmelhores cromossomos sejam propagados para a gera\u00e7\u00e3o seguinte sem sofrer \nrecombina\u00e7\u00e3o ou muta\u00e7\u00e3o. Isto garante que o AG ter\u00e1 uma converg\u00eancia monot\u00f4nica. \n\n2.5.1 Exploitation x Exploration em AG \n\nA batalha entre exploitation e exploration em AG \u00e9 manifestado principalmente pela \npress\u00e3o de sele\u00e7\u00e3o na computa\u00e7\u00e3o evolucion\u00e1ria. Press\u00e3o de sele\u00e7\u00e3o \u00e9 simplesmente \numa medida de qu\u00e3o fortemente o princ\u00edpio Darwiniano de \u201csobreviv\u00eancia do mais \n\nSeja fi o valor de aptid\u00e3o do cromossomo ci. \n             n \ntotal := ? fi  \n             i=1   \nr := random(0,total) \ntotalParcial := 0 \ni := 0 \nRepita \n        i  := i+1 \n        totalParcial := totalParcial + fi \nAt\u00e9 totalParcial >= r \nRetornar cromossomo ci \n\n\n\n \n\napto\u201d \u00e9 aplicado. Baixa press\u00e3o de sele\u00e7\u00e3o significa que mesmo indiv\u00edduos com baixa \naptid\u00e3o podem reproduzir. Alta press\u00e3o de sele\u00e7\u00e3o significa que indiv\u00edduos com alta \naptid\u00e3o possuem uma chance maior de gerar descendentes que os indiv\u00edduos com baixa \naptid\u00e3o. Escolher um indiv\u00edduo com alta aptid\u00e3o para procriar \u00e9 exploitative, enquanto \nque o casamento de indiv\u00edduos com baixa aptid\u00e3o \u00e9 exploratory. \n\nO balanceamento entre exploitation e exploration tamb\u00e9m pode ser realizado por \nmuta\u00e7\u00e3o. A muta\u00e7\u00e3o adiciona novo material gen\u00e9tico \u00e0 popula\u00e7\u00e3o. Uma alta taxa de \nmuta\u00e7\u00e3o ser\u00e1 mais exploratory.  \n\n2.6  Redes Neurais  \nAs redes neurais artificiais s\u00e3o t\u00e9cnicas computacionais que apresentam um modelo \n\ninspirado na estrutura neural de organismos inteligentes.  \n\nHoje em dia a maior parte dos pesquisadores concorda que as redes neurais s\u00e3o \nmuito diferentes do c\u00e9rebro em termos de estrutura. No entanto, como o c\u00e9rebro, uma \nrede neural \u00e9 uma cole\u00e7\u00e3o massivamente paralela de unidades de processamento \npequenas e simples inter-conectadas. A intelig\u00eancia da rede \u00e9 \u201carmazenada\u201d nos pesos \ndestas conex\u00f5es. Entretanto, em termos de escala, o c\u00e9rebro \u00e9 muito maior que qualquer \nrede neural. Al\u00e9m disso, as unidades usadas na rede neural s\u00e3o tipicamente muito mais \nsimples que os neur\u00f4nios biol\u00f3gicos e o processo de aprendizado do c\u00e9rebro (embora \nainda desconhecido) \u00e9, provavelmente, muito diferente do das redes neurais. \n\nAtualmente, Redes Neurais Artificiais \u00e9 um dos paradigmas mais indicados para o \nprojeto e an\u00e1lise de sistemas inteligentes adaptativos em uma grande faixa de aplica\u00e7\u00f5es \nem intelig\u00eancia artificial e modelagem cognitiva, por v\u00e1rias raz\u00f5es: redes neurais podem \nrepresentar qualquer fun\u00e7\u00e3o comput\u00e1vel (HERTZ, 1991), possuem potencial para \ncomputa\u00e7\u00e3o paralela massiva, robustez na presen\u00e7a de ru\u00eddos, adaptabilidade na falha \nde componentes, e generalizam a solu\u00e7\u00e3o para entradas n\u00e3o previamente testadas. \n\nA primeira vantagem da utiliza\u00e7\u00e3o de redes neurais \u00e9 o eficiente mecanismo de \narmazenamento do que foi aprendido. No contexto das tarefas de decis\u00e3o seq\u00fcencial, o \nmapeamento de estado do mundo a a\u00e7\u00f5es \u00e9 representada de forma distribu\u00edda nos pesos \ne conex\u00f5es da rede. A medida que novos estados e a\u00e7\u00f5es s\u00e3o observados, essa nova \ninforma\u00e7\u00e3o \u00e9 distribu\u00edda e unida de forma efetiva \u00e0 informa\u00e7\u00e3o antiga nos pesos. A \nestrutura de armazenamento n\u00e3o cresce de forma incontrol\u00e1vel enquanto o agente ganha \nmais experi\u00eancia, mas, permanece constante. Redes Neurais s\u00e3o uma forma de \narmazenamento compacto do que foi aprendido que pode cobrir um espa\u00e7o enorme de \nsitua\u00e7\u00f5es de entrada. \n\nAl\u00e9m do armazenamento constante, redes neurais fornecem tempo computacional \nconstante. A complexidade comp utacional \u00e9 limitada pelo n\u00famero de neur\u00f4nios e \nconex\u00f5es dentro da rede. Desde que estes componentes permane\u00e7am constantes, o \ntempo de computa\u00e7\u00e3o tamb\u00e9m permanecer\u00e1 constante. Esta caracter\u00edstica \u00e9 ainda mais \nimportante em tarefas de decis\u00e3o de tempo real, onde o tempo gasto para gerar uma \ndecis\u00e3o pode diminuir o desempenho do sistema. Tamb\u00e9m, desde que redes neurais s\u00e3o \nconstitu\u00eddas de muitos elementos computacionais separados, eles podem ser facilmente \nparalelizados a fim de acelerar a computa\u00e7\u00e3o. \n\nPor\u00e9m, talvez a vantagem mais importante de uma representa\u00e7\u00e3o neural seja a \ngeneraliza\u00e7\u00e3o efetiva do que foi aprendido. Ou seja, a capacidade de dar respostas \ncoerentes para dados n\u00e3o apresentados a ela durante o treinamento. Al\u00e9m disso, uma \n\n\n\n \n\nvez que o espa\u00e7o de armazenamento \u00e9 finito, a rede neural precisa consolidar um \nconhecimento geral baseado em caracter\u00edsticas ou faixas de valores do espa\u00e7o de \nentrada em vez de valores de entrada exatos. A generaliza\u00e7\u00e3o \u00e9 importante em espa\u00e7os \ngrandes de estado onde um agente n\u00e3o pode realisticamente experimentar todas as \nsitua\u00e7\u00f5es poss\u00edveis do dom\u00ednio durante o treinamento. Ao generalizar o comportamento, \na rede neural pode aplicar decis\u00f5es para estados n\u00e3o experimentados baseada em \ncaracter\u00edsticas comuns com os estados experimentados.  \n\nAdicionalmente, Mcquesten (2002) coloca que, Redes Neurais Artificiais s\u00e3o \nadequadas para a solu\u00e7\u00e3o de problemas complexos, pois, podem reconhecer padr\u00f5es em \nentradas complexas, e problemas substanciais possuem entradas complexas.  \n\nAs vantagens s\u00e3o bastante atrativas, mas, \u00e9 tamb\u00e9m importante considerar as \ndesvantagens de uma pol\u00edtica de decis\u00e3o expressa em rede neural. Primeiro, existem \nnumerosos par\u00e2metros que devem ser conhecidos a priori para garantir um bom \ncomportamento. A arquitetura de rede, o n\u00famero de neur\u00f4nios, a fun\u00e7\u00e3o de ativa\u00e7\u00e3o, \ns\u00e3o tr\u00eas exemplos de par\u00e2metros para os quais os valores \u00f3timos n\u00e3o s\u00e3o bem \ncompreendidos. A implementa\u00e7\u00e3o envolve v\u00e1rias tentativas e erros para gerar \nconfigura\u00e7\u00f5es de par\u00e2metros efetivos. Segundo, desde que a pol\u00edtica de decis\u00e3o \u00e9 \nrepresentada nas conex\u00f5es e pesos, \u00e9 muito dif\u00edcil extrair a pol\u00edtica de forma mais \nleg\u00edvel e compreens\u00edvel. Uma representa\u00e7\u00e3o l\u00facida pode ser necess\u00e1ria se for necess\u00e1rio \nimplementar a pol\u00edtica de decis\u00e3o de uma outra maneira ou mesmo para compreender \num sistema subjacente. Tradicionalmente, desenvolvedores de redes neurais aceitam \nsem questionamentos o comportamento final da rede, uma vez que o racioc\u00ednio por tr\u00e1s \ndeste comportamento \u00e9 de dif\u00edcil extra\u00e7\u00e3o.  \n\n \n\n\n\n \n\n3 NEURO-EVOLU\u00c7\u00c3O \n\nEm tarefas de aprendizagem no mundo real tais como controlar rob\u00f4s, jogar, \nperseguir ou escapar de um inimigo, n\u00e3o existe uma forma de especificar as a\u00e7\u00f5es \ncorretas para cada situa\u00e7\u00e3o. Muitas vezes, a a\u00e7\u00e3o correta n\u00e3o \u00e9 conhecida. Para tais \nproblemas, o comportamento \u00f3timo precisa ser aprendido atrav\u00e9s da explora\u00e7\u00e3o de \ndiferentes a\u00e7\u00f5es e boas decis\u00f5es precisam ser recompensadas baseadas num sinal de \nrefor\u00e7o esparso. \n\nNeuro-evolu\u00e7\u00e3o \u00e9 um m\u00e9todo de aprendizado por refor\u00e7o, no qual a busca pelo \ncomportamento ideal \u00e9 implementada atrav\u00e9s do aprendizado de redes neurais por \nalgoritmos gen\u00e9ticos. Na neuro-evolu\u00e7\u00e3o, cromossomos representam par\u00e2metros das \nredes neurais, que podem ser, por exemplo, pesos, limites, e conectividade. Estes \ncromossomos s\u00e3o recombinados baseados no princ\u00edpio de sele\u00e7\u00e3o natural com o \nobjetivo de se encontrar uma rede neural satisfat\u00f3ria para um dado problema.  \n\nComparado aos m\u00e9todos de aprendizagem por refor\u00e7o tradicionais, a neuro-evolu\u00e7\u00e3o \n\u00e9 mais robusta na presen\u00e7a de ru\u00eddos e entradas incompletas, e permite representar \nestados cont\u00ednuos e a\u00e7\u00f5es naturalmente.  \n\nEmbora a pesquisa que vem sendo realizada na \u00e1rea de Redes Neurais tenha levado \n\u00e0 descoberta de v\u00e1rios resultados te\u00f3ricos e emp\u00edricos e ao desenvolvimento de v\u00e1rias \naplica\u00e7\u00f5es pr\u00e1ticas nas \u00faltimas d\u00e9cadas, o projeto de redes neurais para aplica\u00e7\u00f5es \nespec\u00edficas ainda \u00e9 um processo de tentativa e erro, onde, geralmente se leva em \nconsidera\u00e7\u00e3o a experi\u00eancia passada em aplica\u00e7\u00f5es similares.  \n\nO desempenho de redes neurais em problemas particulares \u00e9 criticamente \ndependente, entre outras coisas, do n\u00famero de neur\u00f4nios, da arquitetura e do algoritmo \nde aprendizagem utilizado (MORIARTY, 1997). Estes fatores tornam o processo de \nprojeto de redes neurais uma tarefa dif\u00edcil. A falta de bons princ\u00edpios para projeto \nconstitui o maior obst\u00e1culo no desenvolvimento de sistemas de redes neurais em larga \nescala para uma variedade de problemas pr\u00e1ticos. \n\nAlgoritmos evolucion\u00e1rios, por sua vez, oferecem uma abordagem aleat\u00f3ria, \nrelativamente eficiente, para busca por solu\u00e7\u00f5es quase \u00f3timas em uma variedade de \ndom\u00ednios de problemas. O projeto de redes neurais eficientes para classes espec\u00edficas de \nproblemas \u00e9, portanto, um candidato natural para a aplica\u00e7\u00e3o de algoritmos \nevolucion\u00e1rios. \n\n3.1  A abordagem evolucion\u00e1ria \nOs processos chaves na abordagem evolucion\u00e1ria para o projeto de arquiteturas \n\nneurais s\u00e3o ilustrados na Figura 3.1: \n\n\n\n \n\n \nFigura 3.1: Processos da abordagem evolucion\u00e1ria. \n\n \n\n \n\nAlgoritmos evolucion\u00e1rios s\u00e3o modelados, de forma imprecisa, como os processos \nque parecem ocorrer na evolu\u00e7\u00e3o biol\u00f3gica. A id\u00e9ia central de sistemas evolucion\u00e1rios \u00e9 \na de uma popula\u00e7\u00e3o de cromossomos que s\u00e3o elementos de um espa\u00e7o de busca \nmultidimensional. Em um algoritmo gen\u00e9tico simples, cromossomos s\u00e3o, por exe mplo, \nstrings bin\u00e1rias de um tamanho fixo (n) que codificam os pontos de um espa\u00e7o de busca \nbooleano n-dimensional.  \n\nNo contexto deste trabalho, o cromossomo codifica uma classe de atributos de \narquiteturas neurais. O processo de codifica\u00e7\u00e3o/decodifica\u00e7\u00e3o de redes em \ncromossomos pode ser extremamente simples ou muito complexo. As redes neurais \nresultantes podem tamb\u00e9m ser equipadas com algoritmos de aprendizagem que as \ntreinam usando o est\u00edmulo do ambiente ou simplesmente avaliando uma sua execu\u00e7\u00e3o \nem uma dada tarefa; os pesos da rede tamb\u00e9m s\u00e3o determinados pelo mecanismo de \ncodifica\u00e7\u00e3o/decodifica\u00e7\u00e3o. A avalia\u00e7\u00e3o de desempenho de uma rede neural determina a \naptid\u00e3o do(s) cromossomo(s) correspondente(s). \n\nO procedimento neuro-evolucion\u00e1rio trabalha em uma popula\u00e7\u00e3o de cromossomos, \npreferencialmente selecionando e reproduzindo aqueles que codificam redes neurais  \ncom melhores aptid\u00f5es. Operadores gen\u00e9ticos, tais como muta\u00e7\u00e3o, recombina\u00e7\u00e3o, \ninvers\u00e3o, etc..., s\u00e3o usados para introduzir diversidade na popula\u00e7\u00e3o. Lo go, ap\u00f3s v\u00e1rias \ngera\u00e7\u00f5es, a popula\u00e7\u00e3o evolui gradualmente para cromossomos que correspondem \u00e0s \nmelhores redes neurais. \n\nSegundo Moriarty (1997), a combina\u00e7\u00e3o de redes neurais e algoritmos gen\u00e9ticos \noferece algumas vantagens importantes sobre a maioria dos m\u00e9todos de aprendizado de \nredes neurais tradicionais, tais como, backpropagation (RUMELHART, 1986) e \ncascade correlation (FAHLMAN, 1990). Nas se\u00e7\u00f5es seguintes, s\u00e3o destacados dois dos \nprincipais benef\u00edcios: treinamento esparso e tempo de treinamento. \n\nCromossomo  Redes Neurais  \n\nTreinando \n\nRede Neural \n\nFilhos \n\nSele\u00e7\u00e3o \n\nMuta\u00e7\u00e3o \n\nAvalia\u00e7\u00e3o da Aptid\u00e3o \n\nDecodifica\u00e7\u00e3o \n\nComponente Evolucion\u00e1rio \n\nComponente de Aprendizagem \n\n\n\n \n\n3.1.1 Treina mento esparso \n\nA motiva\u00e7\u00e3o prim\u00e1ria para utiliza\u00e7\u00e3o da neuro-evolu\u00e7\u00e3o sobre as t\u00e9cnicas mais \nusuais, tais como a backpropagation, \u00e9 a habilidade de se treinar agentes com refor\u00e7os \nmais esparsos (BALAKRISHNAN, 1995). Para muitas tarefas de decis\u00e3o seq\u00fcencial, \ncomo, por exemplo, tomar decis\u00f5es t\u00e1ticas em um jogo, freq\u00fcentemente n\u00e3o existe um \nretorno autom\u00e1tico para a avalia\u00e7\u00e3o das decis\u00f5es mais recentes. Em muitos dom\u00ednios, \ncomputar a informa\u00e7\u00e3o de gradiente sobre cada sa\u00edda da rede \u00e9 invi\u00e1vel porque o \nrefor\u00e7o do sistema n\u00e3o \u00e9 t\u00e3o freq\u00fcente. Por exemplo, em tarefas de decis\u00e3o seq\u00fcencial, \num sinal de refor\u00e7o pode somente ser dado ap\u00f3s a tomada de uma seq\u00fc\u00eancia de \ndecis\u00f5es. Estas atribui\u00e7\u00f5es de cr\u00e9dito s\u00e3o dif\u00edceis de serem feitas baseadas em uma \nm\u00e9trica de desempenho geral, porque n\u00e3o \u00e9 sempre \u00f3bvio como decis\u00f5es isoladas \nafetam a sa\u00edda final. Uma vez que algoritmos gen\u00e9ticos n\u00e3o requerem atribui\u00e7\u00f5es de \ncr\u00e9dito expl\u00edcitas para sa\u00eddas individuais da rede, eles podem resolver uma faixa de \nproblemas, incluindo problemas de refor\u00e7o esparso. \n\n3.1.2 Tempo de treinamento \n\nAlgumas pesquisas mostram que a neuro-evolu\u00e7\u00e3o \u00e9 competitiva em tempo de \ntreinamento com os m\u00e9todos tradicionais de treinamento de redes neurais. Montana  \n(1989) rodou v\u00e1rios experimentos comparando redes neurais evolucion\u00e1rias com redes \nbackpropagation tradicionais na tarefa de classifica\u00e7\u00e3o de dados. Duas implementa\u00e7\u00f5es \ndiferentes de redes neurais foram usadas: uma onde os pesos de uma rede feed-forward \nevolu\u00edam usando um \u00fanico algoritmo evolucion\u00e1rio, e uma onde um algoritmo \nevolucion\u00e1rio foi implementado com backpropagation em uma abordagem h\u00edbrida. \nMontana (1989) n\u00e3o reportou nenhuma vantagem clara em usar a abordagem h\u00edbrida \nsobre a evolucion\u00e1ria isolada. Contudo, reportou resultados superiores no n\u00famero de \nitera\u00e7\u00f5es de treinamento usando algoritmo evolucion\u00e1rio sobre backpropagation.  \n\n3.2 Codifica\u00e7\u00e3o da Rede \nAo se optar pela utiliza\u00e7\u00e3o de redes neurais, uma das quest\u00f5es principais \u00e9 como \n\nser\u00e1 realizada a codifica\u00e7\u00e3o da rede em cromossomos. A maioria das abordage ns de \nneuro-evolu\u00e7\u00e3o fixa a arquitetura a ser evolu\u00edda e o cromossomo meramente reflete a \nconcatena\u00e7\u00e3o de pesos da rede. Fixar a arquitetura e for\u00e7ar os pesos a corresponderem \ndiretamente a sua localiza\u00e7\u00e3o no cromossomo inibe muito da flexibilidade da \nabordagem de algoritmos evolucion\u00e1rios (MORIARTY, 1997). Por exemplo, \u00e9 muito \ndif\u00edcil construir estruturas de pesos altamente efetivos se os pesos est\u00e3o localizados em \nregi\u00f5es distantes do cromossomo. Ao fixar os pesos, uma tend\u00eancia (bias) \u00e9 introduzida \nconsiderando que pesos ser\u00e3o combinados em blocos de constru\u00e7\u00e3o \u00fateis: pesos que \nficam pr\u00f3ximos um do outro. Tal tend\u00eancia restringe a liberdade do algoritmo \nevolucion\u00e1rio para explorar muitos blocos de constru\u00e7\u00e3o e pode significativamente \naumentar o tempo de busca. \n\nA quest\u00e3o de como uma arquitetura neural \u00e9 representada \u00e9 cr\u00edtica. A representa\u00e7\u00e3o \nou codifica\u00e7\u00e3o usada n\u00e3o somente determina as classes de arquiteturas neurais que \npoder\u00e3o possivelmente evoluir, como tamb\u00e9m podem restringir a escolha do processo \nde decodifica\u00e7\u00e3o. Por exemplo, se o problema requer a descoberta de redes neurais com \numa estrutura recorrente, de forma a garantir uma probabilidade de sucesso diferente de \nzero, o esquema de codifica\u00e7\u00e3o precisa ser bastante preciso para descrever redes neur ais \n\n\n\n \n\nrecorrentes, e o mecanismo de decodifica\u00e7\u00e3o precisa ser capaz de transformar tal \ndescri\u00e7\u00e3o em uma rede recorrente apropriada.  \n\n3.3 Topologia da Rede  \nO sucesso de uma arquitetura neural para resolver um problema particular (ou uma \n\nclasse de problemas) depende criticamente da topologia da rede. Por exemplo, uma rede \nneural puramente feed-forward \u00e9 incapaz de descobrir ou responder a depend\u00eancias \ntemporais no seu ambiente; para esta tarefa, seria necess\u00e1ria uma rede recorrente. \nSimilarmente, decis\u00f5es n\u00e3o- lineares n\u00e3o podem ser descobertas por redes de apenas \numa camada; s\u00e3o necess\u00e1rios perceptrons multi-camada. \n\nTopologias de redes neurais podem ser classificadas basicamente em dois tipos: \nredes feed-forwards e redes recorrentes. \n\nCada um dos tipos b\u00e1sicos de topologia pode ainda ser classificado como redes \nmulti-camadas, conectadas aleatoriamente, esparsamente conectadas, regular, irregular, \nmodular, hier\u00e1rquica, etc... \n\n3.4 Vari\u00e1veis da Evolu\u00e7\u00e3o \nRedes neurais s\u00e3o tipicamente especificadas em termos de topologia (ou padr\u00e3o de \n\nconectividade), fun\u00e7\u00f5es computadas pelos neur\u00f4nios (sigmoid, threshold,...) e os pesos \nda conex\u00e3o (ou, um algoritmo de aprendizagem que atribua  valores a esses pesos). Uma \ndescri\u00e7\u00e3o mais completa de uma arquitetura neural requer a especifica\u00e7\u00e3o das estruturas \nde controle e aprendizagem. Virtualmente, qualquer subconjunto destas vari\u00e1veis \u00e9 \ncandidato a ser operado por processos evolucion\u00e1rios. Por exemplo, um sistema A pode \nevoluir a conectividade da rede tanto quanto os pesos (enquanto mant\u00e9m todo o resto \nconstante). Similarmente, um sistema B pode evoluir somente a conectividade, \nconfiando talvez em uma busca local mais eficiente por pesos dentro de cada rede. O \ntempo/desempenho para os dois sistemas, em um dado problema, ser\u00e1 diferente, \ntornando a escolha das vari\u00e1veis, sujeitas \u00e0 evolu\u00e7\u00e3o, um fator extremamente cr\u00edtico. \n\nEm adi\u00e7\u00e3o \u00e0 conectividade da rede e os pesos, \u00e9 poss\u00edvel evoluir o algoritmo de \naprendizagem, as fun\u00e7\u00f5es de controle ou reguladoras, as fun\u00e7\u00f5es computadas por v\u00e1rios \nneur\u00f4nios, a distribui\u00e7\u00e3o de diferentes tipos de neur\u00f4nio, densidades relativas de \nconex\u00f5es, par\u00e2metros (e/ou processos) governando a decodifica\u00e7\u00e3o de um gen\u00f3tipo em \num fen\u00f3tipo, e assim sucessivamente.  \n\n3.5 M\u00e9todos de Neuro-Evolu\u00e7\u00e3o \nNesta se\u00e7\u00e3o, s\u00e3o descritos alguns dos m\u00e9 todos de neuro-evolu\u00e7\u00e3o existentes na \n\nliteratura. Na se\u00e7\u00e3o 3.5.1, descrevemos o Symbiotic, Adaptative Neuro-Evolution \n(SANE), um m\u00e9todo de neuro-evolu\u00e7\u00e3o que evolui os neur\u00f4nios da camada oculta da \nrede neural, cada neur\u00f4nio codifica os pesos das conex\u00f5es dele com os outros neur\u00f4nios \naos quais est\u00e1 conectado. Na se\u00e7\u00e3o 3.5.2, o Enforced Subpopulations (ESP) \u00e9 descrito. \nO ESP \u00e9 uma extens\u00e3o do SANE , e \u00e9 a base do modelo proposto neste trabalho. Assim \ncomo o SANE, o ESP tamb\u00e9m considera redes neurais com topologias fixas. Por \n\u00faltimo, na se\u00e7\u00e3o 3.5.3, apresenta-se o NeuroEvolution of Augmenting Topologies \n(NEAT), que ao contr\u00e1rios de outros m\u00e9todos, como o SANE e o ESP, evolui n\u00e3o s\u00f3 os \npesos das conex\u00f5es da rede, como tamb\u00e9m sua topologia. \n\n\n\n \n\n3.5.1  Symbiotic, Adaptative Neuro-Evolution   \n\nSymbiotic, Adaptative Neuro-Evolution (MORIARTY, 1996; MORIARTY, 1997), \nou SANE, \u00e9 um m\u00e9todo de neuro-evolu\u00e7\u00e3o que evolui uma popula\u00e7\u00e3o de neur\u00f4nios que \nse interconectam para formar uma rede neural completa. Mais especificamente, SANE \nevolui uma popula\u00e7\u00e3o de neur\u00f4nios da camada oculta de uma rede para um dado tipo de \narquitetura tal como uma rede de duas camadas.  \n\nOs passos b\u00e1sicos do SANE podem ser descritos na Figura 3.2. Durante o est\u00e1gio de \nevolu\u00e7\u00e3o, subpopula\u00e7\u00f5es aleat\u00f3rias de neur\u00f4nios de tamanho S s\u00e3o selecionados e \ncombinados para formar uma rede neural. A rede \u00e9 avaliada na tarefa e recebe um \ncr\u00e9dito, que \u00e9 subseq\u00fcentemente adicionado \u00e0 vari\u00e1vel de aptid\u00e3o de cada neur\u00f4nio. O \nprocesso continua at\u00e9 que cada neur\u00f4nio tenha participado em um n\u00famero suficiente de \nredes. A aptid\u00e3o m\u00e9dia de cada neur\u00f4nio \u00e9 calculada dividindo-se a soma de suas \naptid\u00f5es pelo n\u00famero de redes em que ele participou. Os neur\u00f4nios que possuem uma \naptid\u00e3o mais alta cooperaram melhor que os outros neur\u00f4nios da popula\u00e7\u00e3o. Neur\u00f4nios \nque n\u00e3o cooperaram e s\u00e3o prejudiciais para as redes das quais eles fizeram parte \nrecebem um escore mais baixo e s\u00e3o menos selecionados. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 3.2: Passos b\u00e1sicos em uma gera\u00e7\u00e3o do SAN E. \n\n \n\nA Figura 3.3 ilustra como \u00e9 realizado o processo de neuro-evolu\u00e7\u00e3o no SANE. \nNeur\u00f4nios s\u00e3o escolhidos aleatoriamente, da popula\u00e7\u00e3o de neur\u00f4nios ocultos, para \ncompor a rede neural. A rede \u00e9 avaliada e sua aptid\u00e3o na execu\u00e7\u00e3o da tarefa \u00e9 atribu\u00edda \n\u00e0 todos os neur\u00f4nios que a constitu\u00edram. Novos neur\u00f4nios s\u00e3o escolhidos para montar \numa nova rede e assim sucessivamente at\u00e9 que todos os neur\u00f4nios tenham sido \navaliados. \n\n \n\n \n\n1. Limpe os valores de aptid\u00e3o de todos os neur\u00f4nios \n2. Monte a rede neural do agente \n3. Avalie o agente na execu\u00e7\u00e3o da tarefa \n4. Distribua a aptid\u00e3o do agente para os neur\u00f4nios que \n\nparticiparam da rede avaliada \n5. Repita os passos de 2 \u00e0 4 um n\u00famero suficiente de vezes \n6. Calcule a aptid\u00e3o m\u00e9dia de cada neur\u00f4nio, dividindo seu valor \n\ntotal de aptid\u00e3o pelo n\u00famero de redes em que ele participou. \n7. Desempenhe opera\u00e7\u00f5es de recombina\u00e7\u00e3o na popula\u00e7\u00e3o baseada \n\nno valor de aptid\u00e3o m\u00e9dia de cada neur\u00f4nio. \n\n\n\n \n\n \nFigura 3.3: Neuro-evolu\u00e7\u00e3o no SANE. \n\n \n\n \n\nUma vez que cada neur\u00f4nio possui um valor de aptid\u00e3o, opera\u00e7\u00f5es de recombina\u00e7\u00e3o \ns\u00e3o usadas para combinar os cromossomos em neur\u00f4nios com melhor desempenho. \nMuta\u00e7\u00e3o \u00e9 utilizada em uma taxa baixa para introduzir material gen\u00e9tico que pode estar \nfaltando nas popula\u00e7\u00f5es iniciais ou pode ter sido perdido durantes as opera\u00e7\u00f5es de \nrecombina\u00e7\u00e3o.  \n\nCada neur\u00f4nio oculto \u00e9 definido como um cromossomo onde os bits codificam uma \ns\u00e9rie de defini\u00e7\u00f5es de conex\u00e3o. A rede n\u00e3o \u00e9 totalmente conectada. Cada neur\u00f4nio \noculto se conecta somente com alguns dos neur\u00f4nios de entrada e alguns dos neur\u00f4nios \nde sa\u00edda. Cada defini\u00e7\u00e3o de conex\u00e3o identifica a posi\u00e7\u00e3o da conex\u00e3o e o peso da mesma; \ns\u00e3o utilizados 8 bits como campo r\u00f3tulo e 16 bits como campo peso. N\u00e3o existem \nconex\u00f5es entre os neur\u00f4nios da camada oculta.  \n\nO uso de uma precis\u00e3o limitada (16 bits como campo peso) certamente tem um \nimpacto significativo sobre o algoritmo de aprendizado e a evolu\u00e7\u00e3o dos pesos. \n\nO valor do r\u00f3tulo determina a posi\u00e7\u00e3o da conex\u00e3o. Se o valor decimal do r\u00f3tulo D \u00e9 \nmaior que 127, ent\u00e3o a conex\u00e3o \u00e9 realizada na unidade de sa\u00edda D mod O, onde O \u00e9 o \nn\u00famero total de unidades de sa\u00edda. Similarmente, se D \u00e9 menor ou igual a 127, a \nconex\u00e3o \u00e9 realizada na unidade D mod I, onde I \u00e9 o n\u00famero total de unidades de \nentrada. O campo peso codifica um peso em ponto flutuante para a conex\u00e3o. A Figura \n3.4 mostra como uma rede neural \u00e9 formada a partir da defini\u00e7\u00e3o de tr\u00eas neur\u00f4nios da \ncamada oculta. No exemplo, a primeira defini\u00e7\u00e3o de conex\u00e3o para o neur\u00f4nio A \u00e9 \n(15,1.242). Como 15 \u00e9 menor que 127, a conex\u00e3o liga o neur\u00f4nio A ao neur\u00f4nio 7 (15 \nmod 8) da camada de entrada. O peso desta conex\u00e3o \u00e9 1.242. \n\n \n\nAmbiente \nda Tarefa \n\n\n\n \n\n \nFigura 3.4: Rede neural formada a partir dos cromossomos que definem a camada \n\noculta da rede. \n\n \n\n \n\nUma vez que cada neur\u00f4nio tenha participado de um n\u00famero suficie nte de redes, a \npopula\u00e7\u00e3o \u00e9 ordenada de acordo com valores de aptid\u00e3o m\u00e9dia. Os 25% melhores \nneur\u00f4nios s\u00e3o recombinados com outros neur\u00f4nios de igual ou maior valor de fun\u00e7\u00e3o de \naptid\u00e3o. Um operador de recombina\u00e7\u00e3o de um ponto \u00e9 usado para combinar os \ncromossomos dos dois neur\u00f4nios, criando-se dois filhos por casamento/combina\u00e7\u00e3o. A \nprole substitui os neur\u00f4nios com pior desempenho na popula\u00e7\u00e3o. Muta\u00e7\u00e3o \u00e9 usada em \numa taxa baixa de 0,1% na nova prole como o \u00faltimo passo da evolu\u00e7\u00e3o. \n\nA sele\u00e7\u00e3o por rank \u00e9 empregada ao inv\u00e9s das sele\u00e7\u00f5es padr\u00f5es de proporcionalidade \nde aptid\u00f5es para garantir um bias para os neur\u00f4nios com melhor desempenho. Nas \nsele\u00e7\u00f5es com proporcionalidade de aptid\u00f5es, uma string s \u00e9 selecionada para casamento \ncom probabilidade fs/F, onde fs \u00e9 a aptid\u00e3o da cadeia e F \u00e9 a m\u00e9dia de aptid\u00e3o da \npopula\u00e7\u00e3o. A medida que a aptid\u00e3o m\u00e9dia das cadeias aumenta, a varia\u00e7\u00e3o na aptid\u00e3o \ndiminui. Sem varia\u00e7\u00e3o suficiente entre as cadeias que desempenham melhor e pior, o \nalgoritmo gen\u00e9tico ser\u00e1 incapaz de atribuir bias significante em prol das melhores \ncadeias. Selecionando-se cadeias baseadas no seu rank na popula\u00e7\u00e3o, as melhores \ncadeias sempre ir\u00e3o receber bias significante sobre as piores cadeias mesmo quando as \ndiferen\u00e7as no desempenho forem pequenas. \n\n3.5.2  Enforced Sub-Populations  \n\nO Enforced Sub-Populations ESP (GOMEZ, 1997) \u00e9 uma extens\u00e3o do SANE, e \nassim como no SANE, a popula\u00e7\u00e3o consiste de neur\u00f4nios individuais em vez de redes \ninteiras, e a rede \u00e9 formada por um subconjunto de neur\u00f4nios. Contudo, ESP aloca uma  \n\n1.242 15 -2.21 143  0.53 2 \n\n5.811 212 -3.41 32 -1.67 151 \n\n-0.4 65 2.556 100 8.141 134 \n\n-0.4 0.53 \n-\n\n3.41 \n2.556  \n\n1.242  \n\n-2.21 \n\n-1.67 \n\n5.881  8.131  \n\n0 1 2 3 4 5 6 7 \n\n0 1 2 3 4 \n\nCromossomo do neur\u00f4nio A :  \n\nCromossomo do neur\u00f4nio B:  \n\nCromossomo do neur\u00f4nio C:  \n\nA\n  \n\nB\n  \n\nC\n  \n\nR\u00f3tulo   Peso  \n\nUma Defini\u00e7\u00e3o de conex\u00e3o \n\n\n\n \n\npopula\u00e7\u00e3o separada para cada um dos u neur\u00f4nios na rede, e um neur\u00f4nio pode ser \nrecombinado somente com neur\u00f4nios de sua pr\u00f3pria sub-popula\u00e7\u00e3o. Vide Figura 3.5.  \n\n \nFigura 3.5: Neuro-evolu\u00e7\u00e3o em ESP. A rede \u00e9 constitu\u00edda por um neur\u00f4nio de cada \n\npopula\u00e7\u00e3o de neur\u00f4nios. \n\n \n\nCada popula\u00e7\u00e3o de neur\u00f4nios tende a convergir para um papel que apresenta as \nmelhores aptid\u00f5es quando a rede \u00e9 avaliada. Desta forma, ESP decomp\u00f5e o problema de \nencontrar uma rede satisfat\u00f3ria em v\u00e1rios problemas me nores. Segundo Yong (2001), \nesta abordagem resulta em uma evolu\u00e7\u00e3o mais eficiente.   \n\nESP acelera a evolu\u00e7\u00e3o SANE por duas raz\u00f5es: as especializa\u00e7\u00f5es progressivas dos \nneur\u00f4nios n\u00e3o s\u00e3o ocultadas pela recombina\u00e7\u00e3o atrav\u00e9s de especializa\u00e7\u00f5es que \nnormalmente completam de forma relativamente ortogonal os pap\u00e9is na rede. Segundo, \nas redes formadas por ESP sempre consistem de um representativo de cada \nespecializa\u00e7\u00e3o que evolui, um neur\u00f4nio \u00e9 sempre avaliado em qu\u00e3o bem ele \ndesempenha seu papel no contexto de outros neur\u00f4nios. Em SANE, redes podem conter \nm\u00faltiplos membros de alguma especializa\u00e7\u00e3o e omitir membros de outras, e suas \navalia\u00e7\u00f5es s\u00e3o, logo, menos consistentes. \n\nDesde que SANE forma redes aleatoriamente atrav\u00e9s da sele\u00e7\u00e3o de neur\u00f4nios dentro \nde uma mesma popula\u00e7\u00e3o, um neur\u00f4nio n\u00e3o pode confiar em ser combinado com \nneur\u00f4nios similares em quaisquer duas rodadas. Um neur\u00f4nio que se comporta de uma \nforma em uma rodada pode se comportar muito diferentemente em uma outra, \nresultando em avalia\u00e7\u00f5es de aptid\u00e3o de ne ur\u00f4nios que s\u00e3o muito ruidosas. A arquitetura \nde sub-popula\u00e7\u00e3o de ESP torna a avalia\u00e7\u00e3o dos neur\u00f4nios mais consistente. A medida \nque as subpopula\u00e7\u00f5es se especializam, neur\u00f4nios evoluem supondo, com uma certeza \ncada vez maior, os tipos de neur\u00f4nios com os quais eles ser\u00e3o conectados.  \n\nA medida que a evolu\u00e7\u00e3o progride, cada sub-popula\u00e7\u00e3o ir\u00e1 declinar em diversidade. \nIsto \u00e9 um problema, especialmente em evolu\u00e7\u00e3o incremental, pois, uma popula\u00e7\u00e3o que \nj\u00e1 convergiu pode n\u00e3o se adaptar facilmente a uma nova tarefa. Para acoplar a \ntransfer\u00eancia de tarefas a despeito da converg\u00eancia, ESP \u00e9 combinado com uma t\u00e9cnica \nde busca interativa conhecida como Delta-Coding.  \n\n3.5.2.1 Delta-Coding \n\nA id\u00e9ia do Delta-Coding (WHITLEY, 1991) \u00e9 buscar modifica\u00e7\u00f5es \u00f3timas para a \nmelhor solu\u00e7\u00e3o corrente. Quando a popula\u00e7\u00e3o de solu\u00e7\u00f5es candidatas converge, Delta-\n\nAmbiente \nda Tarefa \n\n\n\n \n\nCoding salva a melhor solu\u00e7\u00e3o corrente e inicializa uma popula\u00e7\u00e3o de cromossomos \nchamadas ?\u2013cromossomos. Os ?\u2013cromossomos possuem a mesma quantidade de genes \nque os cromossomos da melhor solu\u00e7\u00e3o corrente, e eles consistem de ?\u2013valores que \nrepresentam a diferen\u00e7a da melhor solu\u00e7\u00e3o. Uma nova popula\u00e7\u00e3o \u00e9 evolu\u00edda, \nselecionando-se ?\u2013cromossomos, adicionando seus ?\u2013valores \u00e1 melhor solu\u00e7\u00e3o \ncorrente e avaliando o resultado. Os ?\u2013cromossomos que me lhoram o resultado s\u00e3o \nselecionados para reprodu\u00e7\u00e3o. Logo, Delta-Coding explora a vizinhan\u00e7a da melhor \nsolu\u00e7\u00e3o. Delta-Coding pode ser aplicada v\u00e1rias vezes, com ?\u2013popula\u00e7\u00f5es sucessivas \nrepresentando as diferen\u00e7as da melhor solu\u00e7\u00e3o anterior. \n\n3.5.3 NeuroEvolution of Augmenting Topologies  \n\nUma das principais quest\u00f5es em sistemas de neuro-evolu\u00e7\u00e3o \u00e9 se a evolu\u00e7\u00e3o de \npesos juntamente com a topologia da rede pode melhorar o desempenho da neuro-\nevolu\u00e7\u00e3o. Por um lado, evoluir a topologia juntamente com os pesos torna a busca mais \ndif\u00edcil. Por outro lado, evoluir topologias pode economizar tempo de ter que se \nencontrar o n\u00famero certo de neur\u00f4nios ocultos para um problema particular.  \n\nAo contr\u00e1rio da maioria dos sistemas de neuro-evolu\u00e7\u00e3o que evoluem arquiteturas \nde redes neurais fixas, como \u00e9 o caso do SANE e do ESP, o NeuroEvolution of \nAugmenting Topologies (NEAT) (STANLEY, 2002) evolui n\u00e3o s\u00f3 os pesos das \nconex\u00f5es da rede, como sua pr\u00f3pria topologia, buscando um aprendizado mais r\u00e1pido.  \n\nNo NEAT, cromossomos s\u00e3o representa\u00e7\u00f5es lineares da conectividade da rede. Cada \ngenoma inclui uma lista de genes de conex\u00e3o. Cada gene de conex\u00e3o especifica o n\u00f3 de \nentrada, o n\u00f3 de sa\u00edda, o peso da conex\u00e3o, se a conex\u00e3o entre os n\u00f3s est\u00e1 habilitada ou \nn\u00e3o, e um n\u00famero inovador, que permite encontrar genes correspondentes.  \n\nNa Figura 3.6 \u00e9 apresentada o mapeamento de um gen\u00f3tipo para um fen\u00f3tipo no \nNEAT. Observe que o segundo gene est\u00e1 desabilitado, logo, a conex\u00e3o que ele \nespecifica n\u00e3o \u00e9 observada no fen\u00f3tipo. \n\n\n\n \n\n \nFigura 3.6: Mapeamento do gen\u00f3tipo para o fen\u00f3tipo.  \n\n \n\nA muta\u00e7\u00e3o em NEAT pode alterar tanto os pesos das conex\u00f5es quanto a estrutura da \nrede. Os pesos da conex\u00e3o sofrem muta\u00e7\u00e3o como em qualquer sistema de neuro-\nevolu\u00e7\u00e3o, com cada conex\u00e3o sendo perturbada ou n\u00e3o em cada gera\u00e7\u00e3o. Muta\u00e7\u00f5es \nestruturais ocorrem de duas formas. Na muta\u00e7\u00e3o de adi\u00e7\u00e3o de conex\u00f5es, uma nova \nconex\u00e3o \u00e9 adicionada conectando dois n\u00f3s previamente desconectados. Na muta\u00e7\u00e3o de \nadi\u00e7\u00e3o de n\u00f3s, uma conex\u00e3o existente \u00e9 dividida e um novo n\u00f3 \u00e9 colocado onde a \nconex\u00e3o antiga existia. A conex\u00e3o antiga \u00e9 desabilitada e duas novas conex\u00f5es s\u00e3o \nadicionadas ao genoma. Este m\u00e9todo de adi\u00e7\u00e3o de n\u00f3s integra, imediatamente, novos \nn\u00f3s \u00e0 rede. \n\nAtrav\u00e9s da muta\u00e7\u00e3o, os cromossomos em NEAT ficar\u00e3o gradualmente maiores. \nCromossomos de tamanhos variados ir\u00e3o resultar, algumas vezes com conex\u00f5es \ncompletamente diferentes nas mesmas posi\u00e7\u00f5es. \n\nNa Figura 3.7, s\u00e3o apresentados os dois tipos de muta\u00e7\u00e3o no NEAT, adi\u00e7\u00e3o de n\u00f3s e \nadi\u00e7\u00e3o de conex\u00f5es. O n\u00famero no topo de cada genoma \u00e9 o n\u00famero de inova\u00e7\u00e3o. Os \nn\u00fameros de inova\u00e7\u00e3o s\u00e3o marcadores hist\u00f3ricos que identificam o ancestral de cada \ngene. Cada novo gene recebe um novo n\u00famero de inova\u00e7\u00e3o. \n\nGenes de Conex\u00e3o \n\nEntrada: 1  \nSa\u00edda: 4 \nPeso: 0.7  \nHabilitado  \nInova\u00e7\u00e3o: 1  \n\nEntrada: 2  \nSa\u00edda: 4 \nPeso: -0.5 \nDesabilitado  \nInova\u00e7\u00e3o: 2  \n\nEntrada: 3  \nSa\u00edda: 4 \nPeso: 0.5  \nHabilitado  \nInova\u00e7\u00e3o: 3  \n\nEntrada: 2  \nSa\u00edda: 5 \nPeso: 0.2  \nHabilitado  \nInova\u00e7\u00e3o: 4  \n\nEntrada: 5  \nSa\u00edda: 4 \nPeso: 0.4  \nHabilitado  \nInova\u00e7\u00e3o: 5  \n\nEntrada: 1  \nSa\u00edda: 5 \nPeso: 0.6  \nHabilitado  \nInova\u00e7\u00e3o: 6  \n\nN\u00f3 1 \nSensor \n\nN\u00f3 2 \nSensor \n\nN\u00f3 3 \nSensor \n\nN\u00f3 4 \nSa\u00edda \n\nN\u00f3 5 \nOculto \n\nGenes dos n\u00f3s \n\n5 \n\n4 \n\n1 2 3 \n\n   (Fen\u00f3tipo) \n\nGenoma (Gen\u00f3tipo) \n\n\n\n \n\n \nFigura 3.7: Os dois tipos de muta\u00e7\u00e3o estrutural no NEAT. \n\n \n\nPara poder identificar quais genes combinam ao realizar recombina\u00e7\u00e3o, o NEAT \nmant\u00e9m um n\u00famero de inova\u00e7\u00e3o global que guarda a origem hist\u00f3rica de cada gene no \nsistema. Sempre que um novo gene \u00e9 criado, o n\u00famero de inova\u00e7\u00e3o global \u00e9 \nincrementado e atribu\u00eddo a ele; o n\u00famero de inova\u00e7\u00e3o global representa a cronologia do \naparecimento de cada gene no sistema. Como exemplo, suponha que as duas muta\u00e7\u00f5es \nna Figura 3.8 ocorreram uma ap\u00f3s a outra no sistema. O novo gene de conex\u00e3o criado \nna primeira muta\u00e7\u00e3o recebe o n\u00famero 7, e os dois novos genes de conex\u00e3o adicionados \ndurante a muta\u00e7\u00e3o de adi\u00e7\u00e3o de n\u00f3, recebem os n\u00fameros 8 e 9. No futuro, se estes genes \nvierem a se combinar, os filhos herdar\u00e3o o mesmo n\u00famero de inova\u00e7\u00e3o global de cada \ngene; n\u00fameros de inova\u00e7\u00e3o nunca s\u00e3o alterados. \n\nAo realizar recombina\u00e7\u00e3o, os genes em que os cromossomos possuem o mesmo \nn\u00famero de inova\u00e7\u00e3o global s\u00e3o alinhados. Genes que n\u00e3o combinam s\u00e3o chamados \ndisjuntos(D) ou excedentes (E), dependendo se eles ocorrem dentro ou fora da faixa de \nvalores dos n\u00fameros de inova\u00e7\u00e3o do outro pai. Eles representam estruturas que n\u00e3o \nest\u00e3o presentes no outro genoma. Ao compor os filhos, genes s\u00e3o aleatoriamente \nescolhidos dos pais cujos genes combinam, enquanto todos os genes excedentes ou \ndisjuntos s\u00e3o inclu\u00eddos do pai com maior aptid\u00e3o ou de ambos os pais se estes possuem \naptid\u00f5es equipar\u00e1veis. \n\n \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nDIS \n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n8 \n\n1-> 5 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nDIS \n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n6 \n\n1-> 5 \n\n7 \n\n3 -> 5 \n\nMuta\u00e7\u00e3o:  \n\nAdicione Conex\u00e3o \n\n4 \n\n1 2 3 \n\n5 \n\n4 \n\n1 2 3 \n\n5 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nDIS \n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n8 \n\n1-> 5 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nDIS \n\n3 \n\n3-> 4 \n\nDIS \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n6 \n\n1-> 5 \n\n8 \n\n3 -> 6 \n\nMuta\u00e7\u00e3o:  \n\nAdicione N\u00f3 \n\n4 \n\n1 2 3 \n\n5 \n\n4 \n\n1 2 3 \n\n5 \n\n9 \n\n6-> 4 \n\n6 \n\n\n\n \n\n \n\n \n\n \nFigura 3.8: Recombina\u00e7\u00e3o de duas redes usando o NEAT. \n\n \n\nAo adicionar novos genes \u00e0 popula\u00e7\u00e3o e casar cromossomos que representam \nestruturas diferentes, o sistema pode formar uma popula\u00e7\u00e3o de topologias diversas. \nContudo, estruturas menores otimizam mais r\u00e1pido que estruturas maiores, e a adi\u00e7\u00e3o de \nn\u00f3s e conex\u00f5es normalmente diminui a aptid\u00e3o da rede inicialmente. Estruturas \naumentadas recentemente possuem pouca chance de sobreviver por mais que uma \ngera\u00e7\u00e3o mesmo que as inova\u00e7\u00f5es que elas representam sejam cruciais para resolver a \ntarefa em quest\u00e3o. A solu\u00e7\u00e3o para proteger a inova\u00e7\u00e3o \u00e9 usar classifica\u00e7\u00e3o. \n\nA id\u00e9ia \u00e9 dividir a popula \u00e7\u00e3o em esp\u00e9cies, de forma que topologias similares \nperten\u00e7am a mesma esp\u00e9cie. O n\u00famero de genes disjuntos e excedentes entre um par de \ngenes \u00e9 uma medida natural da sua compatibilidade. Quanto mais disjuntos dois \ncromossomos s\u00e3o, menos hist\u00f3ria evolucion\u00e1ria eles compartilham, e logo, menos \ncompat\u00edveis eles s\u00e3o. Logo, pode-se medir a compatibilidade de duas estruturas \ndiferentes no NEAT como uma simples combina\u00e7\u00e3o linear do n\u00famero de genes \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nD\n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n8 \n\n1-> 5 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nD\n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\nD\n\n6 \n\n5-> 6 \n\n7 \n\n6-> 4 \n\n9 \n\n3-> 5 \n\n10 \n\n1-> 6 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nD\n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\n8 \n\n1-> 5 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nD\n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\nD\n\n6 \n\n5-> 6 \n\n7 \n\n6-> 4 \n\n9 \n\n3-> 5 \n\n10 \n\n1-> 6 \n\n1 \n\n1-> 4 \n\n2 \n\n2-> 4 \n\nD\n\n3 \n\n3-> 4 \n\n4 \n\n2-> 5 \n\n5 \n\n5-> 4 \n\nD\n\n6 \n\n5-> 6 \n\n7 \n\n6-> 4 \n\n9 \n\n3-> 5 \n\n10 \n\n1-> 6 \n\n8 \n\n1-> 5 \n\n \n\nPai 1 Pai 2 \n\ndisjunto disjunto \n\ndisjunto \n\nexcedente  excedente  \n\n4 \n\n1 2 3 \n\n5 \n\n3 1 2 \n\n5 6 \n\n4 \n\n3 2 1 \n\n5 \n\n6 \n\n4 \n\nPai 1 \n\nPai 2 \n\nProle \n\n\n\n \n\nexcedentes (E) e disjuntos(D), tanto quanto, a diferen\u00e7a m\u00e9dia dos pesos dos genes que \ncombina m (W): \n\n \n\nd = c1E + c2D + c3.W \n         N        N \n \n\nOs coeficientes, c1, c2 e c3 permitem ajustar a import\u00e2ncia dos tr\u00eas fatores, e o fator \nN, o n\u00famero de genes no genoma maior, normaliza o tamanho do genoma (N pode ser 1 \nse ambos os cromossomos s\u00e3o pequenos, isto \u00e9, se consistem de menos de 20 genes). \n\nA dist\u00e2ncia d permite classificar usando um limite de compatibilidade. \nCromossomos s\u00e3o comparados uns com os outros uma vez; se a dist\u00e2ncia dos \ncromossomos para membros escolhidos aleatoriamente \u00e9 menor que este limite, ele \u00e9 \ncolocado dentro da esp\u00e9cie. Cada genoma \u00e9 localizado dentro da primeira esp\u00e9cie para a \nqual esta condi\u00e7\u00e3o \u00e9 satisfeita, de forma que um genoma nunca est\u00e1 em mais de uma \nesp\u00e9cie. A medida d para um par de cromossomos \u00e9 linear no n\u00famero de conex\u00f5es \nmesmo se d expressa precisamente a compatibilidade entre topologias \nmultidimensionais. \n\nComo mecanismo de reprodu\u00e7\u00e3o, \u00e9 usado compartilhamento de aptid\u00f5es, onde \norganismos da mesma esp\u00e9cie compartilham a mesma aptid\u00e3o do seu nicho.  \n\nNEAT guia a busca atrav\u00e9s de espa\u00e7os mini-dimensionais come\u00e7ando sempre com \numa popula\u00e7\u00e3o uniforme de redes com zeros neur\u00f4nios ocultos. Novas estruturas s\u00e3o \ninseridas incrementalmente como ocorr\u00eancias de muta\u00e7\u00f5es estruturais, e somente \naquelas estruturas consideradas \u00fateis atrav\u00e9s da avalia\u00e7\u00e3o de aptid\u00e3o sobrevivem. Em \noutras palavras, as elabora\u00e7\u00f5es estruturais que ocorrem no NEAT s\u00e3o sempre \njustificadas.  Desde que a popula\u00e7\u00e3o come\u00e7a minimamente, a dimensionalidade do \nespa\u00e7o de busca \u00e9 minimizado, isto prov\u00ea uma vantagem em desempenho quando \ncomparado com outras abordagens. \n\n\n\n \n\n4 MODELO DE COORDENA\u00c7\u00c3O MULTIAGENTE \nNEURO-EVOLUTIVO \n\nNeste cap\u00edtulo  o modelo proposto \u00e9 descrito.  Na se\u00e7\u00e3o 4.1, ser\u00e1 apresentado o \nalgoritmo utilizado para a evolu\u00e7\u00e3o do comportamento dos agentes. Em seguida, ser\u00e1 \napresentado o diagrama de classes do modelo desenvolvido. Por fim, na se\u00e7\u00e3o 4.5, \ndescreve-se o ambiente de simula\u00e7\u00e3o desenvolvido.  \n\n4.1 Algoritmo evolucion\u00e1rio \nA base do nosso modelo de coordena\u00e7\u00e3o adaptativa \u00e9 uma extens\u00e3o do ESP, m\u00e9todo \n\nde neuro-evolu\u00e7\u00e3o descrito no cap\u00edtulo anterior.  \n\nNa nossa extens\u00e3o, cada neur\u00f4nio da camada oculta se conecta com todos os \nneur\u00f4nios da camada de entrada e todos os neur\u00f4nios da camada de sa\u00edda, ou seja, a \nrede que descreve o comportamento de cada agente \u00e9 uma 2-layer-feedforward \ntotalmente conectada.  \n\nDesta forma, o gen\u00f3tipo de cada neur\u00f4nio \u00e9 mantido \u201climpo\u201d, isto \u00e9, cada \ncromossomo \u00e9 uma cadeia dos pesos das conex\u00f5es do neur\u00f4nio oculto atual com os \noutros neur\u00f4nios das camadas de entrada e de sa\u00edda, n\u00e3o havendo necessidade de \narmazenar qualquer informa\u00e7\u00e3o r\u00f3tulo, como acontece no SANE e no ESP.  \n\nContudo, a diferen\u00e7a mais importante do presente modelo \u00e9 que a topologia da rede \nn\u00e3o \u00e9 fixa. \u00c9 poss\u00edvel definir o tamanho m\u00e1ximo de neur\u00f4nios na camada oculta. Por\u00e9m, \no algoritmo evolucion\u00e1rio proposto busca uma quantidade de neur\u00f4nios ocultos que \npossibilite aos agentes executarem bem a tarefa. Desta forma, a topologia da rede \u00e9 \nmantida mais simples. Adicionalmente, alguns experimentos (vide cap\u00edtulo 5) \ncomprovaram que o tempo de treinamento \u00e9 reduzido significantemente a medida que \ndiminu\u00edmos a quantidade de neur\u00f4nios na camada oculta. \n\nNo passo 1 do algoritmo evolucion\u00e1rio proposto, descrito na Figura 4.1, uma nova \nrede neural experimental \u00e9 constru\u00edda para cada agente. No sub-passo b, \u00e9 definido, \naleatoriamente, quantos s neur\u00f4nios devem existir na camada oculta. Posteriormente, no \npasso c, um neur\u00f4nio \u00e9 escolhido aleatoriamente da popula\u00e7\u00e3o correspondente. Isto \u00e9, o \nneur\u00f4nio i \u00e9 escolhido aleatoriamente da popula\u00e7\u00e3o de neur\u00f4nios i, onde i varia de 1 a s. \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 4.1: Algoritmo b\u00e1sico de treinamento. \n\n \n\nA depender da complexidade da tarefa, algumas vezes n\u00e3o \u00e9 poss\u00edvel obter um \ncomportamento coerente do agente com evolu\u00e7\u00e3o direta. Gomez (1997) prop\u00f5e que uma \nabordagem em que comportamentos complexos sejam obtidos atrav\u00e9s de um \naprendizado incremental. Isto \u00e9, os agentes s\u00e3o inicialmente treinados em tarefas mais \nsimples e a medida que seus comportamentos convergem, aumenta-se o grau de \ndificuldade da tarefa, tornando-se cada vez mais complexa.  \n\nUtilizou-se o algoritmo de Delta-Coding, apresentado na se\u00e7\u00e3o 3.5.2.1, para \nimplementar as transi\u00e7\u00f5es para tarefas cada vez mais complexas.  \n\n \n \n\nEm cada transi\u00e7\u00e3o de tarefa, a melhor solu\u00e7\u00e3o \u00e9 salva, uma popula\u00e7\u00e3o de ?-\ncromossomo \u00e9 inicializada e esta popula\u00e7\u00e3o \u00e9 evolu\u00edda para que o agente se adapte a \nnova tarefa. Quando a melhor solu\u00e7\u00e3o da tarefa ti evolui para a tarefa ti+1 , os ?-valores \ns\u00e3o adicionado ao melhor resultado do agente, formando um novo melhor resultado. A \nFigura 4.2, ilustra o algoritmo de Delta-Coding. \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n       ?         ?         ?          ? \n t1\n\n  ?   t2  \n?   t3  \n\n?    ...  ?   tn \n\n1. Crie p popula\u00e7\u00f5es de neur\u00f4nios ocultos para a rede neural de cada agente, onde p \u00e9 o \ntamanho m\u00e1ximo da camada oculta. \n\n2. Para cada agente \na. Zere os valores de aptid\u00e3o de cada neur\u00f4nio \nb. Gere um inteiro s ? [0,p] \nc. Crie uma rede neural com s neur\u00f4nios ocultos obtidos aleatoriamente da \n\nrespectiva popula\u00e7\u00e3o \n3. Avalie os agentes de acordo com uma fun\u00e7\u00e3o de aptid\u00e3o apropriada para a tarefa. \n4. Para cada agente \n\na. Adicione a aptid\u00e3o para a vari\u00e1vel de aptid\u00e3o de cada neur\u00f4nio que participou \nda rede  \n\n5. Repita os passos de 2 a 4 um n\u00famero suficiente de vezes \n6. Calcule a aptid\u00e3o m\u00e9dia de cada neur\u00f4nio, dividindo seu valor total de aptid\u00e3o pelo \n\nn\u00famero de redes em que ele participou. \n7. Execute opera\u00e7\u00f5es de recombina\u00e7\u00e3o na popula\u00e7\u00e3o baseada no valor de aptid\u00e3o m\u00e9dia \n\nde cada neur\u00f4nio. \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 4.2: Algoritmo de Delta-Coding. \n\n \n\n.  \n\n4.2 Caracter\u00edsticas do Modelo Proposto \nNesta se\u00e7\u00e3o, s\u00e3o apresentados alguns detalhes do modelo proposto. Na se\u00e7\u00e3o 4.2.1, \u00e9 \n\nabordada a codifica\u00e7\u00e3o dos neur\u00f4nios em cromossomos bin\u00e1rios e reais. Na se\u00e7\u00e3o 4.2.2. \ns\u00e3o apresentados os operadores gen\u00e9ticos utilizados. Na se\u00e7\u00e3o 4.2.3 apresenta-se as \ninforma\u00e7\u00f5es utilizadas como entrada para as redes neurais dos agentes, bem como, o \nsignificado da sa\u00edda da rede.  \n\n4.2.1 Cromossomos \n\nCada cromossomo corresponde a um neur\u00f4nio da camada oculta e armazena os \npesos das conex\u00f5es que o neur\u00f4nio que ele representa tem com os outros neur\u00f4nios das \ncamadas de entrada e sa\u00edda. Inicialmente, representou-se cada neur\u00f4nio da rede como \num cromossomo bin\u00e1rio. Posteriormente, foi possibilitado que o neur\u00f4nio fosse \ncodificado como uma cadeia de n\u00fameros reais.  \n\nConsiderou-se que as redes s\u00e3o completamente conectadas, de forma que se uma \nrede possui, por exemplo, dois neur\u00f4nios na camada de entrada e quatro na camada de \nsa\u00edda, o neur\u00f4nio oculto armazena um total de seis pesos. Vide Figura 4.3. \n\n \n\n \n\n \n\n \n\n1. Crie p popula\u00e7\u00e3o de ?-valores, onde p \u00e9 o tamanho m\u00e1ximo da camada \noculta. \n\n2. Para cada agente \na. Zere os valores de aptid\u00e3o de cada ?-valor \nb. Altere o conjunto de ?-valores que ser\u00e3o adicionados aos pesos da \n\nrede \nc. Crie uma rede neural, somando os ?-valores selecionados acima ao \n\nmelhor resultado corrente. \n3. Avalie os agentes de acordo com uma fun\u00e7\u00e3o de aptid\u00e3o apropriada para a \n\ntarefa. \n\n4. Para cada agente \na. Adicione a aptid\u00e3o para a vari\u00e1vel de aptid\u00e3o de cada ?-valor \n\nsomado aos pesos da rede  \nb. Se o desempenho do agente melhorou com estes ?-valores, some \n\nestes valores aos pesos do agente. \n5. Repita os passos de 2 a 4 um n\u00famero suficiente de vezes \n6. Calcule a aptid\u00e3o m\u00e9dia de cada ?-valor, dividindo seu valor total de \n\naptid\u00e3o pelo n\u00famero de redes em que ele participou. \n7. Desempenhe opera\u00e7\u00f5es de recombina\u00e7\u00e3o na popula\u00e7\u00e3o baseada no valor de \n\naptid\u00e3o m\u00e9dia de cada ?-valor. \n\n\n\n \n\n-0.573 \n-0.133 \n\n0.6187 \n\n0.0224 \n-0.515 \n\n0.1515 \n\n0 \n\n1 \n\n2 \n\n3 \n\n4 \n\n5 \n\n6 \n\n7 \n\n8 \n\n0.0224 \n-0.515 \n\n0.1515 \n \n\n \n\n \n\n \n\n  \n\n \n\n \n\n \n\n \n\nFigura 4.3: Codifica\u00e7\u00e3o do neur\u00f4nio no modelo proposto. \n\nNa representa\u00e7\u00e3o bin\u00e1ria, \u00e9 necess\u00e1rio especificar quantos genes codificam um valor \nreal. Desta forma, para decodificar os pesos do neur\u00f4nio, dividi-se a cadeia de genes em \nsubcadeias, e para cada subcadeia, calcula-se o valor real que ela representa. \n\nNa Figura 4.4, considere que o cromossomo bin\u00e1rio descrito representa o neur\u00f4nio 2 \nda rede neural. Para decodificar os pesos das conex\u00f5es do neur\u00f4nio 2 com os outros \nneur\u00f4nios da rede, foram seguidos os passos descritos na Figura 4.4: \n\n \n\nPartindo do Cromossomo bin\u00e1rio abaixo:  \n\n001111100000110110101001001101100000101101101110111100111100 \n\nE considerando que cada dez bits correspondem a um valor real, subdividimos a  \n\ncadeia bin\u00e1ria  em subcadeias, cada uma consistindo de 10 bits: \n\n0011111000    0011011010    1001001101    1000001011    0110111011    1100111100 \n\nPara cada subcadeia \u00e9 aplicada a f\u00f3rmula abaixo: \n\nX = min + (max-min)* base10(subcadeia)/ 2l-1, onde min = -1, max = 14, l= 10 \n\nNo final, \u00e9 obtido uma cadeia de reais : -0.515  -0.573  0.1515  0.0224  -0.133  0.6187 \n\n\u00c9 poss\u00edvel visualizar os pesos na rede abaixo \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 4.4: Codifica\u00e7\u00e3o da Rede Neural partindo de Cromossomos bin\u00e1rios. \n                                                 \n\n4 Restringir os pesos entre -1 e +1 impactam na aprendizagem, podendo justificar o n\u00e3o \naprendizado para  entradas mais elaboradas (vide se\u00e7\u00e3o 5.2.1). \n\n-0.57 \n\n-0.57 0.2 0.68 -0.25 -0.33 0.21 Cromossomo do neur\u00f4nio \noculto A  \n\nA \n\n0.2 \n\n0.68 \n\n-0.25 \n\n-0.33 \n\n0.21 \n\n\n\n \n\n \n\nAlternativamente, pode-se representar o cromossomo como uma cadeia de n\u00fameros \nreais. Neste caso, cada elemento da cadeia j\u00e1 representa o peso da conex\u00e3o \ncorrespondente. O comportamento evolutivo pode ser  \n\n4.2.2 Operadores Gen\u00e9ticos  \n\nO operador de sele\u00e7\u00e3o utilizado foi o algoritmo da roleta, j\u00e1 descrito na se\u00e7\u00e3o 2.3.  \n\nForam utilizados dois operadores de recombina\u00e7\u00e3o, a depender da codifica\u00e7\u00e3o \nutilizada para o cromossomo (bin\u00e1ria ou real). \n\nPara a recombina\u00e7\u00e3o de cromossomos bin\u00e1rios, utilizou-se um operador padr\u00e3o com \num ponto de corte.  \n\nPara a recombina\u00e7\u00e3o de cromossomo s reais, foi utilizado o operador de \nrecombina\u00e7\u00e3o aritm\u00e9tico, onde o gene de cada filho \u00e9 obtido pela combina\u00e7\u00e3o linear dos \ngenes correspondentes nos pais. \n\ngeneFilho1 = a * (genePai1 )+(1- a)*genePai2 ; \n\ngeneFilho2 = (1- a )*genePai1+ a *genePai2 ; \n\n \n\nExemplo:  \n\nPartindo dos pais \n\npai1 = -0.0292 -0.4206 -0.7125 -0.6795 0.4273  \n\npai2 = -0.3454 -0.4604  0.3289 -0.4708 0.4278  \n\n \n\ne considerando a = 0.3, tem-se: \n\nfilho1 = -0.2505 -0.4485 0.0165 -0.5334 0.4277  \n\nfilho2 = -0.1241 -0.4325 -0.4000 -0.6169 0.4275  \n\n \n\nPara muta\u00e7\u00e3o, foram utilizados tamb\u00e9m dois operadores. Quando o cromossomo \u00e9 \ncodificado como uma cadeia de n\u00fameros bin\u00e1rios, o operador de muta\u00e7\u00e3o \u00e9 aplicado \ncom dada probabilidade, em cada um dos filhos gerado. Essa probabilidade \u00e9 \nparametrizada, mas, recomenda-se que seja um valor baixo, como 0.2, por exemplo. Na \nmuta\u00e7\u00e3o bin\u00e1ria, inverte-se um alelo, escolhido aleatoriamente (0 passa para 1 e 1 passa \npara 0). \n\nNa muta\u00e7\u00e3o aritm\u00e9tica, o operador de muta\u00e7\u00e3o tamb\u00e9m \u00e9 aplicado com dada \nprobabilidade em cada filho gerado. Por\u00e9m aqui, um alelo, \u00e9 substitu\u00eddo por um n\u00famero \nreal gerado aleatoriamente pertencente ao intervalo [-1,+1]. \n\n4.2.3 Entrada e Sa\u00edda das Redes Neurais \n\nA entrada fornecida para a rede neural, bem como a sa\u00edda retornada pela mesma, \u00e9 \naltamente dependente do problema que se busca resolver com o modelo de coordena\u00e7\u00e3o \nproposto. Na se\u00e7\u00e3o 4.3, apresenta-se o dom\u00ednio de aplica\u00e7\u00e3o utilizado como estudo de \ncaso. \n\n\n\n \n\nPor hora, pode-se adiantar que se trata de um grupo de agentes que se locomove em \num ambiente bidimensional. \n\nAvaliou-se seis entradas poss\u00edveis para o estudo de caso utilizado. As entradas \nvariaram tanto em termos de valor quanto em termos de percep\u00e7\u00e3o. Em termos de \npercep\u00e7\u00e3o, considerou-se entradas onde o agente recebia apenas parte do estado do \nmundo e entradas onde ele recebia o estado total do mundo. Em termos de valor, as \nentradas variaram entre posi\u00e7\u00f5es absolutas, posi\u00e7\u00f5es relativas e sinais da dire\u00e7\u00e3o. Para \nmais detalhes sobre as entradas utilizadas, consulte a se\u00e7\u00e3o 5.1.1. \n\nNo estudo de caso, cada agente pode se mover em uma das quatro dire\u00e7\u00f5es: N, S, L, \nO. O tamanho da camada de sa\u00edda das redes neurais dos predadores \u00e9 sempre igual a \nquatro. Para saber qual a pr\u00f3xima dire\u00e7\u00e3o do agente, foi aplicado a pol\u00edtica do \n\u201cvencedor leva tudo\u201d (winner takes all). Ou seja, a dire\u00e7\u00e3o escolhida \u00e9 aquela cujo \nneur\u00f4nio correspondente apresenta o maior valor de sa\u00edda.  \n\n4.3 Dom\u00ednio de Aplica\u00e7\u00e3o \nA tarefa de captura de uma presa \u00e9 um caso especial de problemas de persegui\u00e7\u00e3o-\n\nevas\u00e3o (MILLER, 1994). Tais tarefas consistem de um ambiente com uma ou mais \npresas e um ou mais predadores. Os predadores se movem ao redor do ambiente \ntentando pegar as presas, e as presas tentam escapar dos predadores. Tarefas de \npersegui\u00e7\u00e3o-evas\u00e3o s\u00e3o interessantes porque existem no mundo real, e oferecem um \nobjetivo claro que requer coordena\u00e7\u00e3o complexa no que diz respeito ao ambiente, outros \nagentes com o mesmo objetivo e agentes advers\u00e1rios. Eles s\u00e3o verdadeiros desafios \nmesmo para os melhores sistemas de aprendizagem, permitindo medi\u00e7\u00e3o precisa, \nan\u00e1lise e visualiza\u00e7\u00e3o das estrat\u00e9gias evolu\u00eddas. \n\nNo presente trabalho explorou-se v\u00e1rias tarefas para o dom\u00ednio da presa e do \npredador. Dentre as quais, pode-se destacar: a varia\u00e7\u00e3o da quantidade de agentes \npredadores, a velocidade da presa, a posi\u00e7\u00e3o inicial da presa, o algoritmo utilizado pela \npresa para fugir dos predadores e o formato do mundo (toroidal ou limitado). \n\nO mundo \u00e9 constitu\u00eddo por uma presa e at\u00e9 quatro predadores. A presa \u00e9 controlada \npor um algoritmo simples. Os comportamentos dispon\u00edveis para a presa s\u00e3o realizar um \nmovimento aleat\u00f3rio, caminhar sempre em uma mesma dire\u00e7\u00e3o ou fugir do predador \nmais pr\u00f3ximo. Os predadores s\u00e3o controlados por redes neurais. O objetivo \u00e9 evoluir as \nredes neurais para formar um time eficiente na tarefa de capturar a presa.  \n\nO ambiente \u00e9 bidimensional e sua dimens\u00e3o pode ser configurada, na maioria dos \nexperimentos realizados, foi utilizada a dimens\u00e3o (30,30). O mundo pode ser \nconfigurado para se comportar de forma toroidal ou limitado. No mundo toroidal, \nquando o agente atinge a \u00faltima c\u00e9lula do mundo, ele automaticamente passa para a \nprimeira na mesma dire\u00e7\u00e3o contr\u00e1ria. Em outras palavras, a c\u00e9lula seguinte a \u00faltima \nabscissa/coordenada do mundo \u00e9 a primeira abscissa/coordenada do mundo. No mundo \nlimitado, ao atingir a \u00faltima abscissa/coordenada do mundo, o agente \u00e9 impedido de \nprosseguir naquela dire\u00e7\u00e3o. Em qualquer das situa\u00e7\u00f5es, n\u00e3o existem obst\u00e1culos. Todos \nos agentes podem se mover em quatro dire\u00e7\u00f5es: N (Norte), S (Sul), L (Leste), O \n(Oeste). A velocidade e a posi\u00e7\u00e3o inicial da presa podem ser configuradas de forma a \nfacilitar/dificultar o problema. A presa pode se mover t\u00e3o r\u00e1pido quanto os predadores. \n\nNa Figura 4.5, \u00e9 ilustrado o algoritmo evolucion\u00e1rio resultante para o problema de \ncaptura da presa.  \n\n\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigura 4.5: Ciclo evolucion\u00e1rio para a captura da presa. \n\n \n\nObserve que a tarefa de captura de uma presa n\u00e3o-estacion\u00e1ria \u00e9 uma tarefa de \ndecis\u00e3o seq\u00fcencial. S\u00f3 \u00e9 poss\u00edvel avaliar se o time de agentes possui uma boa pol\u00edtica de \ndecis\u00e3o ap\u00f3s a execu\u00e7\u00e3o de uma seq\u00fc\u00eancia de passos. Considere, por exemplo, que o \nambiente possui a seguinte configura\u00e7\u00e3o: mundo toroidal, onde a presa se locomove na \nmesma velocidade que os predadores, e fugindo sempre do predador mais pr\u00f3ximo. A \nprimeira vista, poderia se pensar que uma boa estrat\u00e9gia seria que a cada passo de \nexecu\u00e7\u00e3o, os agentes de aproximassem da presa. Contudo, como o mundo considerado \u00e9 \ntoroidal, os predadores nascem todos em um mesmo extremo e a presa se locomove na \nmesma velocidade que os predadores, a presa nunca seria capturada, mesmo que sua \ndist\u00e2ncia inicial para o predador mais pr\u00f3ximo fosse 1. \n\nOs agentes precisam \u201caprender\u201d uma estrat\u00e9gia mais inteligente, uns tem que \nimpedir o avan\u00e7o da presa em alguma dire\u00e7\u00e3o enquanto os outros tentam captura- la.   \n\nEm cada ciclo evolucion\u00e1rio, para avaliar se a estrat\u00e9gia do grupo de predadores \u00e9 \nboa, foram criados diferentes cen\u00e1rios. Cada cen\u00e1rio consiste na altera\u00e7\u00e3o das posi\u00e7\u00f5es \niniciais dos predadores. Para cada cen\u00e1rio criado, \u00e9 permitido que os agentes se \nlocomovam por n passos antes de seus comportamentos serem avaliados.  \n\n4.4 Diagrama de Classes \nNesta se\u00e7\u00e3o, apresenta-se alguns diagramas de classes do modelo desenvolvido. \n\n1. Para cada agente \na. Zere os valores de aptid\u00e3o de cada neur\u00f4nio \nb. Gere um inteiro s ? [0,Qtde m\u00e1xima de neur\u00f4nios ocultos] \nc. Crie uma rede neural com s neur\u00f4nios ocultos \nd. Posicione o agente no mundo bidimensional \n\n2. Posicione a presa de acordo com a posi\u00e7\u00e3o inicial especificada \n3. Para os cen\u00e1rios de 1 \u00e0 k \n\na. Posicione os predadores em um extremo aleat\u00f3rio \nb. Para os passos de 1 \u00e0 n \n\ni. Para cada agente, execute o pr\u00f3ximo movimento \nii.  Movimente a presa de acordo com a velocidade especificada \n\nc. Calcule a dist\u00e2ncia m\u00e9dia final df entre os predadores e a presa \nd. Avalie o time de acordo com a fun\u00e7\u00e3o abaixo: \n       aptidao += 100/ df; \n\n4. Calcule a aptid\u00e3o m\u00e9dia do grupo da seguinte maneira: \naptidaoMedia = aptid\u00e3o/k;  \n\n5. Para cada agente \na. Adicione a aptid\u00e3o para a vari\u00e1vel de aptid\u00e3o de cada neur\u00f4nio que \n\nparticipou da rede  \n6. Repita os passos de 2 \u00e0 5 um n\u00famero suficiente de vezes \n7. Calcule a aptid\u00e3o m\u00e9dia de cada neur\u00f4nio, dividindo seu valor total de aptid\u00e3o pelo \n\nn\u00famero de redes em que ele participou. \n8. Desempenhe opera\u00e7\u00f5es de recombina\u00e7\u00e3o na popula\u00e7\u00e3o baseada no valor de aptid\u00e3o \n\nm\u00e9dia de cada neur\u00f4nio. \n\n\n\nBehavioralNet\n\n- INPUT_LAYER _SIZE : int = 2\n- OUTPUT_LAYER _SIZE : int = 4\n- name : String\n- weights1 : double\n- weights2 : double\n- fitness : double = 0\n- bestFitness : double = 0\n- bestFitnessWithDeltaValues : double = 0\n\n+ getName (  )\n+ setName (  )\n+ BehavioralNet (   )\n+ bestConfiguration (  )\n+ cleanFitness (  )\n+ evolveChromosomePopulations (  )\n+ changeNeurons (  )\n+ changeDeltaValues (  )\n- changeNeuron (  )\n+ output (  )\n+ showWeights (  )\n- adjustWeights (   )\n- adjustWeightsConsideringDeltaValues (  )\n- cleanWeights (  )\n- initializeDeltaPopulations (  )\n- initialization (  )\n- multiplique (  )\n+ main (  )\n- testeAtualizacaoPesos (  )\n- show (  )\n+ setFitness (  )\n+ setDeltaValuesFitness (  )\n+ showDeltaCodes (  )\n+ showFitness (  )\n+ deltaCoding (   )\n+ showPopulations (  )\n+ getHiddenNeurons (  )\n+ setHiddenNeurons (  )\n+ getFitness (   )\n+ bestConfigurationWithDeltaCoding (  )\n+ getBestFitness (  )\n+ getBestFitnessWithDeltaValues (  )\n\nBinaryChromosome\n\n- genes : boolean\n\n+ BinaryChromosome (  )\n+ BinaryChromosome (  )\n+ size (  )\n+ equals (  )\n+ clone (  )\n+ toString (  )\n+ getRealRepresentation (  )\n+ setAlelo (  )\n+ getAlelo (   )\n+ getGene (  )\n+ getGenes (  )\n+ compareTo (  )\n+ mutate (  )\n\nChromosome\n\n+ BINARY : int = 0\n+ REAL : int = 1\n- type : int\n- fitness : double = 0.0\n- participation : int = 0\n- count : int = 0\n- id : int = 0\n\n+ clone (  )\n+ Chromosome (  )\n+ equals (  )\n+ size (  )\n+ mutate (  )\n+ getId (  )\n+ setId (  )\n+ getFitness (  )\n+ addFitness (  )\n+ setFitness (  )\n+ getParticipation (  )\n+ setParticipation (  )\n+ getMediumFitness (   )\n+ getType (  )\n+ setType (  )\n\nChromosomePopulation\n\n- elitism : int = 1\n- count : int = 0\n+ id : int\n\n+ ChromosomePopulation (  )\n+ getRandomChromosome (  )\n+ getChromosome (  )\n+ getBestChromosome (  )\n+ show (  )\n+ evolve (  )\n- replace (  )\n- cromossomos (  )\n+ size (  )\n+ getElitism (  )\n+ setElitism (  )\n- selecao (  )\n+ getChromosomes (  )\n- crossover (  )\n- bynaryChromosomeClassCast (  )\n- realChromosomeClassCast (  )\n- mutacao (  )\n- join (  )\n- add (  )\n- remove (  )\n- getBestChromosome (  )\n- getWorstChromosome (  )\n\nNeuron\n\n- count : int = 0\n\n+ Neuron (  )\n+ toString (  )\n+ equals (  )\n+ getMediumFitness (  )\n+ getChromosome (  )\n+ compareTo (  )\n+ getRealChain (  )\n+ getRealChain (  )\n\nPopulation\n\n- elitismo : int = 1\n\n+ getNeuronioAleatoriamente (  )\n+ getNeuronio (  )\n+ getMelhorNeuronio (  )\n+ evolua (  )\n+ evolua (  )\n- substitua (  )\n- cromossomos (  )\n+ getTamPopulacao (   )\n+ getElitismo (   )\n+ setElitismo (  )\n# selecao (  )\n- getCromossomos (  )\n# crossover (  )\n# mutacao (  )\n- junte (  )\n- adicione (  )\n- remova (   )\n- getMelhorCromossomo (  )\n- getPiorCromossomo (  )\n- gereNovaPopulacaoNeuronios (  )\n+ getNeuronios (  )\n+ setNeuronios (  )\n\nRealChromosome\n\n- genes : double\n\n+ RealChromosome (  )\n+ RealChromosome (  )\n+ RealChromosome (  )\n+ equals (  )\n+ clone (  )\n+ toString (  )\n+ size (  )\n+ setAllele (  )\n+ getAllele (  )\n+ getGenes (  )\n+ compareTo (  )\n+ mutate (  )\n\n- chromosomes\n\n- chromosome\n\n- neuronios\n\n- populations\n\n- deltaPopulations\n\n- hiddenNeurons\n\n- deltaValues\n\n- bestConfiguration\n\n- bestDeltaValues\n\nAgent\n\n+ NORTH : int = 0\n+ EAST : int = 1\n+ WEST : int = 2\n+ SOUTH : int = 3\n+ STOPPED : int = 4\n\n+ randomPosition (  )\n+ nextMove (   )\n\nNeuralAgent\n\n- x : int\n- y : int\n\n+ NeuralAgent (  )\n+ isXOccupied (  )\n+ isYOccupied (  )\n+ randomPosition (  )\n+ nextMove (   )\n- signal (  )\n+ getPosition (  )\n+ setPosition (  )\n+ getBehavior (  )\n+ setBehavior (  )\n\nPrey\n\n- x : int = PreyConfiguration.getInitialX()\n- y : int = PreyConfiguration.getInitialY()\n\n+ Prey (   )\n+ randomPosition (  )\n+ getPosition (  )\n+ nextRightMove (  )\n+ nextFarAwayMove (  )\n+ nextMove (   )\n+ nextRandomMove (  )\n+ setPosition (  )\n+ main (  )\n+ nextMove (   )\n\n- behavior\n\nSaveable\n\n+ save (   )\n+ load (  )\n\n \nFigura 4.6: Principais classes do modelo de coordena\u00e7\u00e3o evolucion\u00e1rio. \n\n\n\n \n\nConfigurationNEAgentConfigurationPreyConfigurationTrainingConfiguration\n\nWorldConfiguration\n\nArithmeticCrossover\n\nArithmeticMutation\n\nBehavioralNet\n\nBinaryChromosome\n\nBinaryCrossover\n\nBinaryMutation\n\nChromosome\n\nChromosomePopulation\n\nChromosomeSelection\n\nNeuron\n\nNeuronPopulation\n\nNeuronSelection\n\nPopulation\n\nRealChromosome\n\nUtil\n\n- chromosomes\n\n- chromosome\n\n- neuronios\n- neuronios\n\n-  populations-  deltaPopulations\n\n- hiddenNeurons\n\n- deltaValues\n\n- bestConfiguration\n\n- bestDeltaValues\n\nAgent\n\nNeuralAgent\n\nPosition\n\nPrey\n\n- behavior\n\nExecution\n\nLog\n\nSaveable\n\nUtil\n\n- log\n\n- log\n\n- log\n\n- log\n\n- log\n- log- log- log\n\n- log\n\n-  log-  log\n\n \n \n\nFigura 4.7: Todas as classes do modelo evolucion\u00e1rio. \n\n\n\n \n\nNEAgentConfigurationAction\nPreyConfigurationAction\n\nTrainingConfigurationAction\n\nWorldConfigurationAction\n\nCloseButtonListener\n\nExecuteAction\n\nExitAction\n\nFindAgent\n\nLoadAgentsAction\n\nNewAgentAction\n\nNewAgentsAction\n\nSaveAgentAs\n\nSaveAgentsAction\n\nBotaoSalvarAgentesListener\n\nLoadAgentsDialog\n\nNewAgentsDialog\n\nOkButtonListener\n\nSaveAgentsDialog\n\n- dialog\n\n~ dialog\n\nBotaoCancelarListener\n\nBotaoSalvarListener\n\nNEAgentConfigDialogSave\nButtonListener\n\nNEAgentConfigurationDialog\n\nPreyConfigDialogSaveButton\nListener\n\nPreyConfigurationDialog\n\nTrainingConfigurationDialog\n\nWorldConfigDialogSaveButton\nListener\n\nWorldConfigurationDialog\n\n~ dialog\n\n- dialog\n\n- dialog\n\n- dialog\n\n~ config\n\nWorld\n\nWorldFileFilter\n\n- world\n\n- world\n\n~ mundo\n\n- world\n\n~ world\n\n- world\n- world~ mundo\n\n- world\n~ world\n\n- mundo\n- mundo\n\n- world\n\n- mundo - mundo\n- world- world\n\n~ world\n\n- world\n\n- world\n\n- world- world- world\n\n- world\n\nExecution\n\n+ world\n\n+ execucaoThread\n\n \nFigura 4.8: Classes do ambiente de simula\u00e7\u00e3o.\n\n\n\n \n\n4.5 Ambiente de Simula\u00e7\u00e3o \nO modelo proposto foi implementado com JSDK1.5. Utilizou-se o Eclipse 3.0 como \n\nambiente de desenvolvimento. Al\u00e9m da codifica\u00e7\u00e3o do modelo proposto de forma \nparametriz\u00e1vel, desenvolve u-se tamb\u00e9m um ambiente de simula\u00e7\u00e3o. \n\nO ambiente de simula\u00e7\u00e3o que permite v\u00e1rias configura\u00e7\u00f5es do ambiente, a execu\u00e7\u00e3o \ndo treinamento, o armazenamento de comportamentos aprendidos, a simula\u00e7\u00e3o \npropriamente dita e a visualiza\u00e7\u00e3o dos resultados.  \n\nO ambiente possui uma interface visual intuitiva e facilita a execu\u00e7\u00e3o de v\u00e1rias \nsimula\u00e7\u00f5es, inclusive, dos experimentos relatados no cap\u00edtulo 5. \n\n4.5.1 Manipula\u00e7\u00e3o dos Comportamentos \n\nComportamentos podem ser gerados, armazenados e recuperados atrav\u00e9s do menu \nPredadores, conforme ilustrado na Figura  4.9. \n\n \n\n \nFigura 4.9: Manipula\u00e7\u00e3o de comportamentos no ambiente. \n\n \n\nAs op\u00e7\u00f5es do menu s\u00e3o: \n\n\u2022  \u201cNovo\u201d: permite especificar quantos predadores existir\u00e3o no mundo. \n\n\u2022  \u201cCarregar Agentes\u201d: especifica qual o comportamento, previamente salvo, \nde cada predador. \n\n\u2022 \u201cSalvar Agentes\u201d: salva o comportamento corrente de cada predador do \nsistema. \n\nAs Figuras 4.10 e 4.11 ilustram como carregar e salvar o comportamento dos \nagentes. \n\n\n\n \n\n \nFigura 4.10: Carrega o comportamento dos predadores. \n\n \n\nAo escolher a op\u00e7\u00e3o \u201cSalvar agentes\u201d, vir\u00e1 a indica\u00e7\u00e3o de quantos neur\u00f4nios o \nagente possui na camada oculta e ser\u00e1 solicitado que o usu\u00e1rio informe onde o \ncomportamento deve ser salvo. \n\n \n\n \nFigura 4.11: Salva o comportamento atual dos agentes. \n\n\n\n \n\n \n\n \n\n4.5.2 Configura\u00e7\u00f5es \n\nO menu de configura\u00e7\u00f5es, permite que se configure o predador, a presa, o mundo e o \ntreinamento. \n\nPara o predador, \u00e9 poss\u00edvel configurar: a quantidade m\u00e1xima de neur\u00f4nios na \ncamada oculta, a entrada da rede neural, o tamanho do cromossomo bin\u00e1rio, a \nprobabilidade de muta\u00e7\u00e3o, o elitismo e o tamanho das popula\u00e7\u00f5es de neur\u00f4nios. Vide \nFigura 4.12. \n\n \n\n \nFigura 4.12: Configura\u00e7\u00e3o dos predadores. \n\n \n\nPara a presa \u00e9 poss\u00edvel configurar: a posi\u00e7\u00e3o inicial, a velocidade (ou melhor, a \nprobabilidade dela realizar um movimento a cada passo de execu\u00e7\u00e3o) e o algoritmo a ser \nutilizado (fugir do predador mais pr\u00f3ximo, realizar movimentos aleat\u00f3rio e caminhar \nsempre na mesma dire\u00e7\u00e3o). A Figura 4.13 ilustra as configura\u00e7\u00f5es poss\u00edveis para a \npresa. \n\n \n\n \nFigura 4.13: Configura\u00e7\u00e3o da presa. \n\n \n\n\n\n \n\nPara cada treinamento \u00e9 poss\u00edvel configurar a quantidade de ciclos, de experimentos, \nde avalia\u00e7\u00f5es e de passos por avalia\u00e7\u00e3o. O ciclo determina a quantidade de ciclos \nevolucion\u00e1rios utilizados no treinamento, isto \u00e9, quantas vezes as popula\u00e7\u00f5es de \nneur\u00f4nios evoluam, atrav\u00e9s das aplica\u00e7\u00f5es dos operadores gen\u00e9ticos. Os experimentos \ndeterminam quantas redes s\u00e3o montadas em cada ciclo evolucion\u00e1rio. Quanto maior a \nquantidade de redes montadas, maior ser\u00e1 a quantidade de neur\u00f4nios avaliados em cada \npopula\u00e7\u00e3o e mais precisa ser\u00e1 a aptid\u00e3o m\u00e9dia de cada neur\u00f4nio. A aptid\u00e3o m\u00e9dia de \ncada neur\u00f4nio \u00e9 dada pela raz\u00e3o entre a aptid\u00e3o total do neur\u00f4nio pela quantidade de \nredes em que ele participou. A avalia\u00e7\u00e3o indica quantos cen\u00e1rios ser\u00e3o criados para \navaliar a rede montada no experimento atual. Por fim, os passos indicam quantos \nmovimentos os agentes devem fazer antes de terem suas estrat\u00e9gias avaliadas. Vide \nFigura 4.14. \n\n \n\n \nFigura 4.14: Configura\u00e7\u00e3o do treinamento. \n\n \n\nPara o mundo \u00e9 poss\u00edvel configurar o tipo (limitado ou toroidal), a dimens\u00e3o (altura \ne largura), grau de dificuldade (f\u00e1cil: basta que um agente pegue a presa, dif\u00edcil: os \nagentes precisam encurralar a presa) e regras de posicionamento (\u00e9 permitido ou n\u00e3o \nmais de um agente na mesma c\u00e9lula). Observe as configura\u00e7\u00f5es poss\u00edveis na Figura \n4.15. Todos estes fatores contribuem para a configura\u00e7\u00e3o da complexidade da tarefa. \n\n \nFigura 4.15: Configura\u00e7\u00f5es do mundo. \n\n\n\n \n\n \n\nO menu Execu\u00e7\u00e3o, permite que se controle a execu\u00e7\u00e3o do treinamento, a execu\u00e7\u00e3o \ndo algoritmo de  Delta-coding e a execu\u00e7\u00e3o da estrat\u00e9gia aprendida. A Figura 4.16 \nilustra o menu Execu\u00e7\u00e3o. \n\n \n\n \nFigura 4.16: Menu Execu\u00e7\u00e3o. \n\n \n\n4.5.3 Visualiza\u00e7\u00e3o das estrat\u00e9gias \n\nA janela principal da ferramenta possibilita a f\u00e1cil visualiza\u00e7\u00e3o das estrat\u00e9gias \naprendidas pelos agentes. Vide Figura 4.17. \n\n \n\n \nFigura 4.17: Visualiza\u00e7\u00e3o das estrat\u00e9gias aprendidas. \n\n \n\nNeste cap\u00edtulo, fo ram apresentadas as partes constituintes do modelo de coordena\u00e7\u00e3o \nproposto, dentre as quais, pode-se destacar: o algoritmo evolucion\u00e1rio, a codifica\u00e7\u00e3o da \nrede neural em cromossomos, os operadores gen\u00e9ticos considerados e a topologia de \nrede utilizada. Adicionalmente, apresentou-se o dom\u00ednio de aplica\u00e7\u00e3o onde o modelo foi \ntestado, o diagrama de classes da implementa\u00e7\u00e3o do modelo e o ambiente de simula\u00e7\u00e3o. \nNo pr\u00f3ximo cap\u00edtulo, ser\u00e3o apresentados diversos experimentos realizados tanto paa a \nvalida\u00e7\u00e3o do modelo quanto para a identifica\u00e7\u00e3o dos valores ideais para os par\u00e2metros \ndo modelo, considerando-se a tarefa da presa-predador. \n\n\n\n \n\n5 EXPERIMENTOS \n\nForam realizados dois conjuntos de experimentos de forma a avaliar o modelo \nproposto. No primeiro conjunto de experimentos, descritos na se\u00e7\u00e3o 5.1, foi realizada \numa compara\u00e7\u00e3o entre o modelo proposto e outros modelos existentes na literatura. No \nsegundo conjunto de experimentos, foi realizado um estudo sobre valores satisfat\u00f3rios \npara os diversos par\u00e2metros do modelo. \n\nEm cada experimento descrito neste cap\u00edtulo, o treinamento foi realizado de forma \nincremental. Os predadores eram inicialmente treinados com a presa estacion\u00e1ria. Em \ncada fase subseq\u00fcente do treinamento, a velocidade da presa era incrementada de 0,1 at\u00e9 \nque atingisse a velocidade de 1,0. Em cada troca de velocidade da presa, Delta-Coding \nera aplicado. \n\n5.1 Experimentos Comparativos \nYong(2001) utilizou ESP/Delta-Coding para evoluir o comportamento dos \n\npredadores, em uma tarefa de persegui\u00e7\u00e3o-evas\u00e3o, onde, a presa era considerada \ncapturada quando um dos predadores tocassem nela.  \n\nA estrat\u00e9gia padr\u00e3o utilizada pela presa nos experimentos de Yong(2001) era fugir \ndo predador mais pr\u00f3ximo. Os predadores de Yong(2001), assim como os predadores \nque tiveram seus comportamentos evolu\u00eddos com o modelo proposto no presente \ntrabalho, s\u00e3o capazes de capturar sempre a presa desde que o comportamento utilizado \npor ela, em tempo de execu\u00e7\u00e3o, seja o mesmo usado no treinamento.  \n\nYong(2001) realizou experimentos a fim de avaliar a adaptabilidade dos \ncomportamentos dos predadores \u00e0 mudan\u00e7a de estrat\u00e9gia da presa, em tempo de \nexecu\u00e7\u00e3o. Nos experimentos reportados, os predadores foram treinados para capturar \numa presa cuja estrat\u00e9gia era fugir do predador mais pr\u00f3ximo. Em tempo de execu\u00e7\u00e3o, a \nestrat\u00e9gia da presa era alterada para se movimentar semp re \u00e0 direita.  \n\nNeste trabalho, foi realizado um experimento similar. Utilizou-se uma configura\u00e7\u00e3o \npara treinamento parecida com a que foi adota por Yong(2001) no treinamento de seus \nagentes predadores. A compara\u00e7\u00e3o das configura\u00e7\u00f5es adotadas pode ser visua lizada na \nTabela 5.1. O tempo gasto para a realiza\u00e7\u00e3o de um experimento completo (treinamento \ninicial com presa estacion\u00e1ria e dez treinamentos com Delta-Coding incrementando-se a \nvelocidade da presa a cada novo treinamento) durou cerca de 10 horas. A m\u00e1quina \nutilizada para o treinamento possui um processador AMD Athlon\u2122 Xp 2200+ 1.80GHz \ne 1.00GB de mem\u00f3ria RAM. \n\n \n\n \n\n\n\n \n\n \n\nTabela 5.1: Compara\u00e7\u00e3o das configura\u00e7\u00f5es adotadas para treinamento dos agentes.  \n\n Configura\u00e7\u00e3o Utilizada \nem Yong (2001) \n\nConfigura\u00e7\u00e3o do Modelo \nproposto \n\nTamanho de cada \nPopula\u00e7\u00e3o de Neur\u00f4nios \n\n100 100 \n\nElitismo 50 20 \n\nTamanho (M\u00e1ximo) da \nCamada Oculta \n\n10 9 \n\nQuantidade de ciclos \nevolucion\u00e1rios \n\n400 400 \n\nQuantidade de \navalia\u00e7\u00f5es/cen\u00e1rios por \nciclo \n\n6 6 \n\nQuantidade de \nExperimenta\u00e7\u00f5es (redes \nformadas por ciclo \nevolucion\u00e1rio) \n\n1000 1000 \n\nDimens\u00e3o do Mundo (100,100) (100,100) \n\n \n\nAs configura\u00e7\u00f5es adotadas no presente trabalho divergiram sutilmente em dois \npar\u00e2metros: o elitismo e a quantidade de neur\u00f4nios presentes na camada oculta de cada \nagente. No algoritmo evolucion\u00e1rio utilizado por Yong(2001), em cada ciclo de \nevolu\u00e7\u00e3o, ele substitu\u00eda apenas os 50% piores neur\u00f4nios de cada popula\u00e7\u00e3o, usando, \ndesta forma, uma alta taxa de elitismo. No experimento realizado no presente trabalho, \noptou-se por uma taxa de elitismo de 20%. \n\nEm Yong(2001), a quantidade de neur\u00f4nios presente na camada oculta era fixa e \nigual a 10. Nos comportamentos evolu\u00eddos neste experimento, utilizando o modelo \nproposto, existiam nove neur\u00f4nios presentes na camada oculta de cada predador.  \n\nA Tabela 5.2 compara os resultados do modelo proposto com o modelo utilizado por \nYong(2001). Executou-se 10 simula\u00e7\u00f5es para os comportamentos evolu\u00eddos. Em cada \nsimula\u00e7\u00e3o, a presa iniciava em uma posi\u00e7\u00e3o diferente no mundo, sempre com uma \ndist\u00e2ncia de, no m\u00ednimo, dez casas do predador mais pr\u00f3ximo.  \n\n \n\nTabela 5.2: Compara\u00e7\u00e3o de resultados com o ESP. \n\n Taxa de captura quando a presa \ntroca de comportamento em tempo \n\nde execu\u00e7\u00e3o \n\nESP 14,5 % \n\nModelo \nProposto \n\n80 % \n\n\n\n \n\n \n\nN\u00e3o foram encontradas refer\u00eancias da utiliza\u00e7\u00e3o de SANE ou NEAT para problemas \nsimilares aos utilizados neste trabalho, no dom\u00ednio da presa-predador. \n\n5.2 Experimentos Variando Diversos Par\u00e2metros \nNa abordagem neuro-evolucion\u00e1ria, s\u00e3o muitos os par\u00e2metros a serem \n\nconfigurados. Realizou-se uma s\u00e9rie de experimentos a fim de avaliar quais os melhores \npar\u00e2metros para a tarefa proposta. \n\nFoi considerado um valor padr\u00e3o para cada par\u00e2metro. Na maioria dos \nexperimentos, realizados nesta se\u00e7\u00e3o, alterou-se apenas um dos par\u00e2metros.  \n\nOs par\u00e2metros podem ser categorizados como par\u00e2metros dos predadores, \npar\u00e2metros da presa, par\u00e2metros do mundo e par\u00e2metros de treinamento. \n\nOs valores adotados foram obtidos atrav\u00e9s de experimentos preliminares. Para os \npar\u00e2metros dos predadores considerou-se como padr\u00e3o,  os valores abaixo: \n\n\u2022 Quantidade m\u00e1xima de neur\u00f4nio na camada oculta: 10 \n\n\u2022 Entrada da rede neural do predador: sinal do offset entre o predador atual e a \npresa. \n\n\u2022 Probabilidade de muta\u00e7\u00e3o do neur\u00f4nio: 0.2 \n\n\u2022 Elitismo na popula\u00e7\u00e3o de cada neur\u00f4nio: 2 \n\n\u2022 Tamanho da popula\u00e7\u00e3o de cada neur\u00f4nio: 30 \n\nOs valores adotados, por padr\u00e3o, para os par\u00e2metros da presa foram: \n\n\u2022 Posi\u00e7\u00e3o Inicial: (15,15) \n\n\u2022 Algoritmo: Fugir do predador mais pr\u00f3ximo \n\nOs valores adotados, por padr\u00e3o, para os par\u00e2metros do treinamento foram: \n\n\u2022 Quantidade de ciclos evolucion\u00e1rios: 300 \n\n\u2022 Quantidade de experimenta\u00e7\u00f5es por ciclo: 50 \n\nExperimenta\u00e7\u00e3o, neste contexto, significa quantidade de redes que s\u00e3o testadas \npor ciclo evolucion\u00e1rio. Em outras palavras, em cada experimenta\u00e7\u00e3o, novos \nneur\u00f4nios s\u00e3o escolhidos, de cada popula\u00e7\u00e3o de neur\u00f4nio, para montar a rede \nneural do agente.  \n\n\u2022 Quantidade de avalia\u00e7\u00f5es por tentativa: 20 \n\n\u2022 Quantidade de passos por avalia\u00e7\u00e3o: 15 \n\nForam assumidas as seguintes suposi\u00e7\u00f5es sobre o mundo: \n\n\u2022 A dimens\u00e3o do mundo foi: (30,30) \n\n\u2022 \u00c9 suficiente que um agente toque na presa para que ela seja considerada \ncapturada \n\n\u2022 O mundo \u00e9 toroidal \n\n\n\n \n\n\u2022 \u00c9 permitido que quaisquer dois agentes ocupem a mesma c\u00e9lula \n\n\u2022 Os predadores sempre iniciam no canto superior esquerdo do mundo.  \n\n \n\n5.2.1 Entrada do Agente \n\nForam avaliados seis tipos de entrada para os predadores:  \n\nPode-se categorizar a percep\u00e7\u00e3o que o age nte tem do mundo como parcial ou total.  \nNa percep\u00e7\u00e3o parcial, o agente recebe informa\u00e7\u00f5es relativas a ele a presa. Na percep\u00e7\u00e3o \ntotal, o agente recebe informa\u00e7\u00f5es relativas a ele e ao resto do grupo de predadores: \n\nEntradas de tamanho dois: \n\n\u2022 Sinal do offset [x,y] entre o agente atual e a presa \n\n\u2022 Offset [x,y] entre o agente e a presa \n\nEntradas de tamanho quatro: \n\n\u2022 Coordenada absolutas [xa,ya,xp,yp] do agente atual e da presa \n\nEntradas de tamanho n: \n\n\u2022 Sinal do offset [x,y] entre o agente atual e a presa e o agente atual e os \noutros n-1 predadores \n\n\u2022 Offset [x,y] entre o agente atual e a presa e o agente atual e os outros n-1 \npredadores \n\nEntradas de tamanho 2*(n+1) \n\n\u2022 Coordenada absolutas [xa1,ya2,...,xan,yan,xp,yp] de todos os agentes do sistema  \n\n \n\nPercep\u00e7\u00e3o Parcial \n\nSinal do Offset (1) Offset (2) Coordenadas (3) \n\n \n\n \n \n\nPercep\u00e7\u00e3o Total \n\nSinal do Offset (4) Offset (5) Coordenadas (6) \n\nxp \n\nyP \nxA \nyA \n\nS \n\nN \n\nL \n\n(xp\u2013 xA) \n\n (yp\u2013 yA) \n\n \nO \nS \n\nsinal(xp \u2013 xA) \n\nsinal(yp \u2013 yA) \n\nN \nL \nO \nS \n\nN \n\nO \nL \n\n\n\n \n\n   \nSinal: retorna -1, +1 ou 0 \n\nxA : abcissa da posi\u00e7\u00e3o do agente predador atual \n\nyA : ordenada da posi\u00e7\u00e3o do agente predador atual \n\nxAi: abcissa da posi\u00e7\u00e3o do agente predador i \n\nyAi: ordenada da posi\u00e7\u00e3o do agente predador i \n\nxP: abcissa da posi\u00e7\u00e3o da presa \n\nyP: ordenada da posi\u00e7\u00e3o da presa \n\n \n\nFigura 5.1: Entradas das Redes Neurais dos agent es. \n\n \n\nOs agentes s\u00f3 se mostraram capazes de aprender como capturar a presa quando as \nentradas correspond iam ao sinal do offset. Informa\u00e7\u00f5es envolvendo dist\u00e2ncias e \nposi\u00e7\u00f5es absolutas s\u00e3o entradas mais complexas e dificultam o aprendizado da tarefa. \n\nPara este experimento, n\u00e3o foi utilizado Delta-Coding, apenas, o treinamento inicial \ncom a presa estacion\u00e1ria. O time com a percep\u00e7\u00e3o parcial do mundo, conseguia alcan\u00e7ar \na presa com uma quantidade menor de passos. Observe o resultado de 10 simula\u00e7\u00f5es \nconsecutivas, cada simula\u00e7\u00e3o constitu\u00edda de um novo treinamento e execu\u00e7\u00e3o, na \nTabela 5.3: \n\n \n\nTabela 5.3: Passos para captura da presa. \n\nExecu\u00e7\u00e3o Time com Percep\u00e7\u00e3o \nTotal do Mundo \n\nTime com percep\u00e7\u00e3o \nparcial do Mundo \n\n1 35 29 \n\n2 33 29 \n\n3 43 29 \n\n4 53 31 \n\n5 37 33 \n\n6 35 31 \n\nN \n\nL \n\nO \n\nS \n\nN \nL \n\nS \nO \n\nsinal(xp\u2013xA2) \nx ) \n\n (xp \u2013 xA1) \n (yp \u2013 yA1) \n\n (yp \u2013 yA2) \n\n (xp \u2013 xAn) \n (yp \u2013 yAn) \n\n (xp \u2013 xA2) \nsinal(yp\u2013yA1) \n\nsinal(yp-yA2) \n\nsinal(xp\u2013 xA1) \n\nsinal(yp\u2013yAn) \nsinal(xp\u2013xAn) \n\nN \nL \n\nS \nO \n\n (xp) \n (yp) \n (xA1) \n (yA2) \n\n (xAn) \n (yAn) \n\n\n\n \n\n7 34 31 \n\n8 41 31 \n\n9 55 29 \n\n10 47 31 \n\nM\u00e9dia 41.3 30.4 \n\n \n\n \n\n5.2.2 Quantidade de Neur\u00f4nios na Camada Oculta \n\nFoi avaliada a taxa de captura da presa, em fun\u00e7\u00e3o da quantidade m\u00e1xima de \nneur\u00f4nios na camada oculta. Foram realizadas 10 execu\u00e7\u00f5es para cada quantidade \nm\u00e1xima de neur\u00f4nio na camada oculta. O resultado destas execu\u00e7\u00f5es pode ser \nvisualizado na Tabela 5.4. \n\n \n\nTabela 5.4: Taxa de captura por tamanho da camada oculta. \n\nQuantidade de \nNeur\u00f4nios \n\nTaxa de Captura (%) \n\n3 80 \n\n4 90 \n\n5 100 \n\n6 100 \n\n7 90 \n\n8 100 \n\n9 90 \n\n10 90 \n\n \n\nAvaliou-se o tempo necess\u00e1rio para treinamento em fun\u00e7\u00e3o do tamanho da camada \noculta. Para este experimento, foi considerado apenas o treinamento inicial com a presa \nestacion\u00e1ria, sem Delta-Coding. \n\nNa Tabela 5.5, mostra-se o tempo de treinamento por quantidade de neur\u00f4nios na \ncamada oculta. A camada oculta com menor quantidade de neur\u00f4nios (3) possui um \ntempo de treinamento 20% menor que o da camada com a maior quantidade de \nneur\u00f4nios (10).  \n\n \n\n \n\n \n\n \n\n\n\n \n\n \n\nTabela 5.5: Tempo de treinamento por tamanho da camada oculta. \n\nQuantidade de \nNeur\u00f4nios \n\nTempo M\u00e9dio de Treinamento \n(ms) \n\n3 56367 \n\n4 59649 \n\n5 60955 \n\n6 59917 \n\n7 60528 \n\n8 62840 \n\n9 67503 \n\n10 70568 \n\n \n\n5.2.3 Quantidade de ciclos evolucion\u00e1rios \n\nNeste conjunto de experimentos, buscou-se identificar a quantidade adequada de \nciclos evolucion\u00e1rios. \n\n \n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n0 100 200 300 400 500 600\n\nCiclo\n\nA\np\n\nti\nd\n\n\u00e3o\n\n \nFigura 5.2: M\u00e9dia das melhores aptid\u00f5es dos agentes por ciclo evolucion\u00e1rio. Time \n\nconstitu\u00eddo por 3 agentes. \n\n \n\nForam realizadas algumas execu\u00e7\u00f5es com um n\u00famero maior de agentes (4). Os \nvalores de aptid\u00e3o foram maiores. Em ambos os experimentos, as aptid\u00f5es dos agentes \nse estabilizaram a partir do ciclo 300. Vide Figuras 5.2 e 5.3. \n\n\n\n \n\n0\n\n20\n\n40\n\n60\n\n80\n\n100\n\n120\n\n140\n\n160\n\n0 100 200 300 400 500 600\n\nCiclo\n\nA\np\n\nti\nd\n\n\u00e3o\n\n \nFigura 5.3: M\u00e9dia das melhores aptid\u00f5es dos agentes por ciclo evolucion\u00e1rio. Time \n\nconstitu\u00eddo por 4 agentes. \n\n \n\n5.2.4 Fun\u00e7\u00e3o de Aptid\u00e3o \n\nA fun\u00e7\u00e3o de aptid\u00e3o escolhida \u00e9 um dos fatores que mais influenciam a \naprendizagem do agente, pois, \u00e9 o que indica qu\u00e3o bom \u00e9 o comportamento do agente. \n\nAs fun\u00e7\u00f5es de aptid\u00e3o podem variar muito em termos de complexidade e quantidade \nde par\u00e2metros levados em considera\u00e7\u00e3o. \n\nUtilizou-se uma fun\u00e7\u00e3o de aptid\u00e3o simples. A aptid\u00e3o \u00e9 inversamente proporcional \u00e0 \ndist\u00e2ncia final dos predadores para a presa. \n\nForam realizadas algumas execu\u00e7\u00f5es com varia\u00e7\u00f5es desta fun\u00e7\u00e3o. Por\u00e9m, os \nresultados n\u00e3o foram satisfat\u00f3rios. \n\n5.2.5 Mundo toroidal x limitado \n\nRealizou-se diversos experimentos considerando um mundo limitado. Inicialmente, \nesta tarefa parecia mais f\u00e1cil, pois, os predadores teriam a \u201cajuda\u201d de obst\u00e1culos (limite \ndo mundo) na tarefa de captura da presa. Contudo, esta tarefa se mostrou mais dif\u00edcil, \npois, em nenhuma das simula\u00e7\u00f5es realizadas, a presa foi capturada com o mundo \nlimitado. A causa deste insucesso pode estar relacionada com a mudan\u00e7a abrupta de \nresposta que o ambiente d\u00e1 como retorno para os agentes. Por exemplo, o agente recebe \num retorno positivo ao caminhar para a direita. Contudo, ao barrar na parede, o retorno \nfica \u201cde repente\u201d estagnado, pela vis\u00e3o do agente. Uma hip\u00f3tese \u00e9 que se o agente \nrecebesse mais informa\u00e7\u00f5es sobre o mundo, como, por exemplo, o tamanho do mesmo e \n\u201crecordasse\u201d de suas a\u00e7\u00f5es passadas, ele seria capaz de aprender a tarefa.  \n\n5.2.6 Grau de dificuldade da tarefa-alvo \n\nA tarefa alvo considerada foi a captura da presa em um mundo toroidal, onde a presa \nse movimenta na mesma velocidade que os predadores. A presa \u00e9 considerada capturada \nquando um dos predadores toca nela.  \n\nEm nenhuma das simula\u00e7\u00f5es executadas, a presa foi capturada quando treinada \ndiretamente na tarefa alvo. Partir de uma tarefa simples, isto \u00e9, capturar uma presa \n\n\n\n \n\nestacion\u00e1ria e tornar a tarefa progressivamente mais complexa, utilizando Delta-Coding \npara os treinamentos subseq\u00fcentes, foi essencial para o aprendizado da tarefa alvo. O \ngrau de dificuldade da tarefa era incrementado com o aumento da velocidade da presa. \n\nQuando foi imposta a restri\u00e7\u00e3o de que a presa s\u00f3 seria considerada capturada quando \ntodos os agentes a cercassem, n\u00e3o foi obtido sucesso em nenhuma das simula\u00e7\u00f5es \nrealizadas. \n\n \n\n\n\n \n\n6 CONCLUS\u00d5ES \n\nEm amb ientes din\u00e2micos e complexos, a pol\u00edtica \u00f3tima de coordena\u00e7\u00e3o n\u00e3o pode ser \nderivada analiticamente, mas deve ser aprendida atrav\u00e9s da intera\u00e7\u00e3o direta com o \nambiente. Neste trabalho, propomos um modelo de coordena\u00e7\u00e3o baseado em neuro-\nevolu\u00e7\u00e3o, que, de acordo com v\u00e1rios pesquisadores (MORIARTY, 1997; \nMCQUESTEN, 2002; YONG, 2001; STANLEY, 2002) \u00e9 um dos m\u00e9todos mais \npromissores para aprendizagem em ambientes estoc\u00e1sticos. \n\nPara testar o modelo, utilizou-se a tarefa da presa-predador, um caso especial de uma \nclasse de problemas conhecidos como persegui\u00e7\u00e3o-evas\u00e3o. Nesta tarefa, uma presa tem \npor objetivo fugir de um ou mais predadores, enquanto que os predadores precisam se \ncoordenar de forma a captur\u00e1-la. \n\nA tarefa da presa-predador \u00e9 praticamente um benchmark para modelos de \ncoordena\u00e7\u00e3o, por basicamente duas raz\u00f5es: primeiro por simularem situa\u00e7\u00f5es que \nocorrem no mundo real e segundo porque, a depender da configura\u00e7\u00e3o do ambiente e da \nestrat\u00e9gia de fuga da presa, exige elevado grau de coordena\u00e7\u00e3o entre os predadores.  \n\nFoi realizado um estudo sobre os modelos de neuro-evolu\u00e7\u00e3o existentes na literatura, \ne proposto uma extens\u00e3o do m\u00e9todo neuro-evolutivo conhecido como ESP. Ao contr\u00e1rio \ndo ESP, que trabalha com uma topologia fixa para as redes neurais dos agentes, o nosso \nmodelo permite que o algoritmo evolucion\u00e1rio encontre, em tempo de treinamento, a \ntopologia da rede neural de cada agente.  \n\nAl\u00e9m de propiciar flexibilidade, o nosso modelo possibilita uma otimiza\u00e7\u00e3o do \ntempo necess\u00e1rio para o treinamento. Nossos experimentos mostraram que a quantidade \nde neur\u00f4nios presentes na camada oculta de cada agente, no final do treinamento, \u00e9, \nfreq\u00fcentemente, inferior \u00e0 quantidade m\u00e1xima especificada. Adicionalmente, os \nexperimentos tamb\u00e9m mo straram que o tempo de treinamento aumenta \nproporcionalmente com o tamanho da camada oculta. Nos experimentos foi  constatado \nque o tempo de treinamento quando o agente possui tr\u00eas neur\u00f4nios na camada oculta \u00e9 \n20% menor que o tempo necess\u00e1rio para o treinamento com 10 neur\u00f4nios na camada \noculta. Este \u00faltimo resultado j\u00e1 era esperado, uma vez que uma quantidade maior de \nneur\u00f4nios implica em maior complexidade computacional.  \n\nA varia\u00e7\u00e3o na topologia da rede dos agentes, em tempo de treinamento, levou a um \noutro resultado: a constru\u00e7\u00e3o de um time heterog\u00eaneo de agentes, no que diz respeito \u00e0 \ntopologia da rede neural de cada agente, o que \u00e9 mais coerente com o papel diferenciado \nque cada agente possui no grupo. Considere, por exemplo, a tarefa mais dif\u00edcil para a \nqual obteu-se sucesso com o modelo proposto: um mundo toroidal, onde a presa se \nmovimenta na mesma velocidade que os predadores, fugindo sempre do predador mais \npr\u00f3ximo. Se todos os agentes possu\u00edssem o papel de correr atr\u00e1s da presa, eles nunca \n\n\n\n \n\niriam conseguir captur\u00e1-la, pois, todos os agentes (presa e predadores) ficariam girando \nna mesma dire\u00e7\u00e3o ao redor do mundo. Desta forma, os agentes precisam evoluir \ncomportamentos diferenciados, isto \u00e9, parte dos agentes persegue a presa enquanto outra \nparte impede que a presa avance em determinada dire\u00e7\u00e3o. \n\nOs melhores comportamentos foram obtidos atrav\u00e9s de um processo de \naprendizagem incremental, isto \u00e9, come\u00e7ando-se com uma tarefa simples: capturar uma \npresa estacion\u00e1ria e finalizando com uma tarefa mais complexa, por exemplo, capturar \numa presa que se movimenta na mesma velocidade que os predadores, fugindo sempre \ndo predador mais pr\u00f3ximo.  \n\nA s\u00e9rie de experimentos realizados neste trabalho mostra como a escolha dos valores \npara os diversos par\u00e2metros do modelo influencia o resultado do processo de \naprendizagem. Abrindo espa\u00e7o para uma pesquisa sobre um modelo auto-ajust\u00e1vel de \npar\u00e2metros por tipo de tarefa. Incluindo varia\u00e7\u00f5es de par\u00e2metros n\u00e3o avaliados neste \ntrabalho, como, por exemplo, o algoritmo utilizado para sele\u00e7\u00e3o e outras fun\u00e7\u00f5es e \naptid\u00e3o.  \n\nNos experimentos apresentados no cap\u00edtulo anterior, mostrou-se que o modelo \nproposto tem boa capacidade de adapta\u00e7\u00e3o em fun\u00e7\u00e3o da altera\u00e7\u00e3o no comportamento \nda presa. Em trabalhos futuros, pretendemos desenvolver um modelo em que os agentes \nsejam capazes de compreender poss\u00edveis altera\u00e7\u00f5es no mundo e alterar seu \ncomportamento, em tempo de execu\u00e7\u00e3o, de forma a suportar estas altera\u00e7\u00f5es sem a \nnecessidade de um novo treinamento. Uma forma poss\u00edvel de realizar tal procedimento \nseria fazer com que os agentes armazenassem as v\u00e1rias estrat\u00e9gias aprendidas para as \ndiversas varia\u00e7\u00f5es do mundo e em tempo de execu\u00e7\u00e3o, identificassem qual estrat\u00e9gia  \nmelhor se aplica \u00e0 percep\u00e7\u00e3o atual do mundo. Essa identifica\u00e7\u00e3o poderia ser realizada \npor uma rede neural a parte.  \n\nO ambiente de simula\u00e7\u00e3o desenvolvido, como parte deste trabalho, tanto facilita a \nconfigura\u00e7\u00e3o dos diversos par\u00e2metros do modelo como permite visualizar graficamente \nas estrat\u00e9gias evolu\u00eddas para a tarefa da presa-predador.  \n\nPretende-se estender o ambiente de simula\u00e7\u00e3o de forma que seja capaz de suportar o \ntreinamento e simula\u00e7\u00e3o do comportamento dos agentes em outros dom\u00ednios de \naplica\u00e7\u00e3o, al\u00e9m do dom\u00ednio da presa-predador, utilizado neste trabalho. \n\n\n\n \n\nREFER\u00caNCIAS \n\nAGOGINO, A.; MIIKKULAINEN, R. Efficient Allele Fitness Assignment with Self-\norganizing Multi-agent System. In: GENETIC AND EVOLUTIONARY \nCOMPUTATION CONFERENCE, GECCO, 2004. Genetic and Evolutionary \nComputation. New York: Springer-Verlag, 2004. \n\n \n\nALDEN, M.; KESTEREN, A; MIIKKULAINEN, R. Eugenic Evolution Utilizing A \nDomain Model. In: GENETIC AND EVOLUTIONARY COMPUTATION \nCONFERENCE, GECCO, 2002. Genetic and Evolutionary Computation. New York: \nSpringer-Verlag, 2002, p. 279-286. \n\n \n\nBALAKRISHNAN, K; HONAVAR, V. Evolutionary Design of Neural \nArchitectures \u2013 A Preliminary  Taxonomy and Guide to Literature. Ames, Iowa: \nDepartment of Computer Science, Iowa State University, 1995. \n\n \n\nBARRETO, A. M. S. Algoritmo Gen\u00e9tico dos M\u00ednimos Quadrados Ortogonal para \no Treinamento de Redes RBF. 2003. Disserta\u00e7\u00e3o (Mestrado em Enge nharia Civil) -  \nCOPPE, UFRJ, Rio de Janeiro. \n\n \n\nBARTO, A. G.; SUTTON, R. S.; ANDERSON, C. W. Neuronlike adaptive elements \nthat can solve difficult learning control problems. IEEE Transactions on Systems, \nMan, and Cybernetics, [S.l.], v. SMC-13, p. 834-846, 1983. \n\n \n\nBARTO, A. G.; SUTTON, R. S.; WATKINS, C. J. C. H. Learning and sequential \ndecision making. In GABRIEL, M.; MOORE, J. W. (Ed.). Learning and \nComputational Neuroscience. Cambridge, MA: MIT Press, 1990. \n\n \n\nBELEW, R. K. Interposing an ontogenic model between genetic algorithms and neural \nnetworks. In: HAN-SON, S. J.; COWAN. J. D.; GILES; C. L. Advances in Neural \nInformation Processing Systems (NIPS). San Mateo: Morgan Kaufmann,1993. \n\n \n\nBERENJI, H. R.; VENGEROV, D. Advantages of Cooperation Between Reinforcement \nLearning Agents in Difficult Stochastic Problems. In: IEEE INTERNATIONAL \n\n\n\n \n\nCONFERENCE ON FUZZY SYSTEMS, FUZZ-IEEE, 9., 2000.Proceedings\u2026 [S.l.]: \nIEEE, 2000. \n\n \n\nBRUCE, J.; MIIKKULAINEN, R. Evolving Populations Of Expert Neural Networks. \nIn: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, GECCO, \n2001. Genetic and Evolutionary Computation: proceedings. San Francisco, CA: \nKaufmann, 2001, p. 251-257. \n\n \n\nBRYANT, B. D.; MIIKKULAINEN, R. Neuroevolution for Adaptive Teams. In: \nCONGRESS ON EVOLUTIONARY COMPUTATION, CEC, 2003, Camberra. \nProceedins\u2026 Camberra, Australia: [s.n.], 2003. \n\n \n\nCLIFF, D.; MILLER, G. Co-evolution of Pursuit and Evasion II: Simulation Methods \nand Results. In: INTERNATIONAL CONFERENCE ON SIMULATION OF \nADAPTATIVE BEHAVIOUR, SAB, 4., 1996. Proceedings\u2026 [S.l.]: SAB, 1996. \n\n \n\nFAN, J.; LAU, R.; MIIKKULAINEN, R. Utilizing Domain Knowledge in \nNeuroevolution. In: INTERNATIONAL CONFERENCE ON MACHINE LEARNING, \nICML, 20., 2003. Proceedings\u2026 Washington, DC: ICML, 2003  \n\n \n\nFAHLMAN, S. E.; LEBIERE, C. The cascade-correlation learning architecture. In: \nAdvances in Neural Information Processing Systems . San Mateo: Morgan \nKaufmann, 1990. \n\n \n\nGOLDBERG, D. E. Genetic Algorithms in Search, Optimization and Machine \nLearning.Reading. MA: Addison-Wesley, 1989. \n\n \n\nGOMEZ, F.; MIIKKULAIEN, R. Incremental evolution of complex general behavior. \nAdaptive Behavior, [S.l.], v. 5,  p. 317-342, 1997. \n\n \n\nGOMEZ, F. J. Robust Non-Linear Control through Neuroevolution. 2003. PhD \nThesis. Department of Computer Sciences, The University of Texas, Austin. \n\n \n\nGOMEZ, F. J.; MIIKKULAINEN, R. Transfer of Neuroevolved Controllers in Unstable \nDomains. In: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, \nGECCO, 2004. Genetic and Evolutionary Computation. New York: Springer-Verlag, \n2004. \n\n \n\nGRASEMANN, U.; MIIKKULAINEN, R. Evolving Wavelets using a Coevolutionary \nGenetic Algorithm and Lifting. In: GENETIC AND EVOLUTIONARY \n\n\n\n \n\nCOMPUTATION CONFERENCE, GECCO, 2004. Genetic and Evolutionary \nComputation. New York: Springer-Verlag, 2004. \n\n \n\nGREFENSTETTE, J. J.; RAMSEY, C. L.; SCHULTZ, A. C. Learning sequential \ndecision rules using simulation models and competition. Machine Learning, [S.l.], v.5, \np. 355-381, 1990. \n\n \n\nGRUAU, F.; WHITLEY, D.; PYEATT, L. A comparison between cellular encoding \nand direct encoding for genetic neural networks. Genetic Programming 1996: \nproceedings of the First Annual Conference. Cambridge, MA: MIT Press, 1996. p. 81-\n89. \n\n \n\nHAYNES, T.; SEN, S. Evolving Behavioral Strategies in Predators and Prey. In: \nAdaptation and Learning in Multiagent Systems. Berlim: Springer-Verlag, 1996. \n\n \n\nHAYNES, T.; WAINWRIGHT, R.; SEN S. Evolving Cooperation Strategies. In: \nINTERNATIONAL CONFERENCE ON MULTIAGENT SYSTEMS, ICMAS, 1., \n1995, San Francisco. Proceedings\u2026 San Francisco, CA: MIT Press, 1995. \n\n \n\nHERTZ, J.; KROGH, A.; PALMER, R. G. Introduction to the Theory of Neural \nComputation. Reading, MA: Addison-Wesley, 1991. \n\n \n\nHOLLAND, J. H. Adaptation in natural artificial systems. Ann Arbor: University of \nMichigan Press, 1975.  \n\n \n\nO\u2019HARE, G.M.P.; JENNINGS, N. R. Foundations of Artificial Intelligence. [S.l.]: \nJohn Wiley &amp; Sons, Inc, 1996. \n\n \n\nKAHN, J.; KATZ, R.H.; PISTER, K. Emerging Challenges: Mobile Networking for \n\u201cSmart Dust\u201d. J. Comm. Networks, [S.l.], p. 188-196, Sept. 2000. \n\n \n\nKAELBLING, L.; LITTMAN, M.; MOORE, A. Reinforcement Learning: A Survey. \nJournal of Artificial Intelligence Research, [S.l.], v.4, p. 237-285, May 1996. \n\n \n\nLESSER, V. R. Multiagent Systems: An Emerging Subdiscipline of AI. ACM \nComputing Surveys, New York, v. 27, n. 3, Sept. 1995. \n\n \n\nMALONE, T. W.; CROWSTON, K. The Interdisciplinary Study of Coordination. \nACM Computing Surveys, New York, v.26, n.1, p. 87-119, 1994.  \n\n\n\n \n\n \n\nMCQUESTEN. P. Cultural Enhancement of Neuroevolution. 2002. Ph.D. Thesis. \nDepartment of Computer Sciences, The University of Texas, Austin, TX. (Tech Report \nAI-02-295). \n\n \n\nMILLER, G.; CLIFF, D. Co-evolution of pursuit and evasion i: Biological and game-\ntheoretic foundations. Brighton, UK: School of Cognitive and Computing Sciences, \nUniverity of Sussex, 1994. \n\n \n\nMINSKY, M. Steps toward artificial intelligence. In:  FEIGENBAUM, E. A.; \nFELDMAN, J. A. (Ed.). Computers and Thought. New York: McGraw-Hill, 1963, p. \n406\u2013450. \n\n \n\nMITCHELL, M. An Introduction to Genetic Algorithms . Cambridge, MA: MIT \nPress, 1996. \n\n \n\nMONTANA, D. J.; DAVIS, L. Training feedforward neural networks using genetic \nalgorithms. In: INTERNATIONAL JOINT CONFERENCE ON ARITICIAL \nINTELLIGENCE, 11., 1989, San Mateo. Proceedings\u2026CA: Morgan Kaufmann, 1989. \np. 762-767. \n\n \n\nMORIARTY, D.; MIIKKULAIEN, R. Efficient Reinforcement Learning through \nSymbiotic Evolution. Machine Learning, [S.l.], v. 22, p. 11-33, 1996. \n\n \n\nMORIARTY, D. E. Symbiotic Evolution Of Neural Networks In Sequential \nDecision Tasks. 1997. Ph.D. Dissertation, Department of Computer Sciences, \nUniversity of Texas, Austin. \n\n \n\nNOLFI, S.; PARISI, D. Desired answers do not correspond ecological neural networks. \nNeural Processing Letters , [S.l.], n. 2, p. 1\u20134, 1994.  \n\n \n\nNSF (National Science Foundation). Review Panel for Research on Coordination \nTheory and Technology. 1989, A report by NSF-IRIS. NSSF Forms and Publication \nUnit, National Science Foundation, Washington, D.C. \n\n \n\nPARDOE, D.; RYOO, M.; MIIKKULAINEN, R. Evolving Neural Network Ensembles \nfor Control Problems. In: GENETIC AND EVOLUTIONARY COMPUTATION \nCONFERENCE, GECCO, 2005. Genetic and Evolutionary Computation: \nproceedings. [S.l.: s.n.], 2005. p. 1379-1384. \n\n \n\n\n\n \n\nPOTTER, M. A. A genetic cascade-correlation learning algorithm. In: \nINTERNATIONAL WORKSHOP ON COMBINATIONS OF GENETIC \nALGORITHMS AND NEURAL NETWORKS, COGANN, 1992, Baltimore, MD. \nProceedings\u2026[S.l.]: IEEE Computer Society Press, 1992. p. 123-133. \n\n \n\nREISINGER, J.; STANLEY, K. O.; MIIKKULAINEN, R. Evolving Reusable Neural \nModules. In: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, \nGECCO, 2004. Genetic and Evolutionary Computation. New York: Springer-Verlag, \n2004. \n\n \n\nRUMELHART, D. E.; HINTON, G. E.; WILLIAMS, R. J.  Learning internal \nrepresentations by error propagation. In: Parallel Distributed Processing : Exploration \nin the Microstructure of Cognition. Cambridge MA: MIT Press, 1986. \n\n \n\nSINGH, B.; REIN, G. L. Role Interaction Nets (RINs): A Process Definition \nFormalism. [S.l.]: MCC, 1992. (Technical Report  n. CT\u2013083\u201392). \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. Efficient Reinforcement Learning through \nEvolving Neural Network Topologies. In: GENETIC AND EVOLUTIONARY \nCOMPUTATION CONFERENCE, GECCO, 2002. Genetic and Evolutionary \nComputation: proceedings. San Francisco, CA: Morgan Kaufmann, 2002. (Winner of \nthe Best Paper Award in Genetic Algorithms). \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. Efficient Evolution Of Neural Network \nTopologies. In: CONGRESS ON EVOLUTIONARY COMPUTATION, CEC, 2002, \nPiscataway. Proceedings\u2026 NJ: IEEE, 2002. \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. Evolving Neural Networks Through \nAugmenting Topologies. Evolutionary Computation, [S.l.], v. 10, n. 2, p. 99-127, \n2002. \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. A Taxonomy for Artificial Embryogeny. \nArtificial Life, [S.l.], v. 9, n. 2, p. 93-130, 2003. \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. Achieving High-Level Functionality through \nEvolutionary Complexification. In: AAAI-2003 SPRING SYMPOSIUM ON \nCOMPUTATIONAL SYNTHESIS, 2003, Stanford. Proceedings\u2026CA: AAAI Press, \n2003. \n\n \n\nSTANLEY, K. O.; BRYANT, B. D.; MIIKKULAINEN, R. Evolving Adaptive Neural \nNetworks with and Without Adaptive Synapses. In: CONGRESS ON \n\n\n\n \n\nEVOLUTIONARY COMPUTATION, CEC, 2003. Proceedings\u2026 Camberra, \nAustralia, 2003. p. 2557-2564. \n\n \n\nSTANLEY, K. O.; MIIKKULAINEN, R. Competitive Coevolution through \nEvolutionary Complexification. Journal of Artificial Intelligence Research, [S.l.], \nv.21, p. 63-100, 2004. \n\n \n\nSTANLEY, K.; BRYANT, B.; MIIKLUAINEN. Evolving Neural Networks Agents in \nthe NERO Video Game. In: IEEE SYMPOSIUM ON COMPUTATIONAL \nINTELLIGENCE AND GAMES, CIG, 2005, Piscataway. Proceedings\u2026 NJ: IEEE, \n2005. \n\n \n\nSTANLEY, K. et al. Neuroevolution of an Automobile Crash Warning System. In:  \nGENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, GECCO, \n2005, Washington, D.C. USA. Genetic and Evolutionary Computation: proceedings. \n[S.l.:s.n.], 2005, p. 1977-1984. \n\n \n\nSUTTON, R. S. Learning to predict by the methods of temporal differences. Machine \nLearning, [S.l.], v. 3, p. 9-44, 1988. \n\n \n\nSUTTON, R. S.; BARTO, A. G. Reinforcement Learning: An Introduction. \nCambridge, MA: MIT Press,1998. \n\n \n\nTEWS, A.; LISTER, R. Self-Organisation in a Simple Pursuit Game . Dispon\u00edvel em: \n<http://life.csu.edu.au/complex/ci/vol6/tews/>. Acesso em: set. 2005. \n\n \n\nTHOMPSON, J. Organizations in action: social sciences bases of administrative \ntheory. New York: McGraw-Hill Book Co, 1967. \n\n \n\nVENGEROV, D.; BERENJI, R.; VENGEROV, A. Emergent Coordination Among \nFuzzy Reinforcement Learning Agents. In: LOIA, V. (Ed.). Soft Computing Agents : A \nNew Perspective for Dynamic Information Systems. Amsterdam, The Netherlands: IOS \nPress, 2003. \n\n \n\nYANNAKAKIS, G.; LEVINE, J.; HALLAM, J. An Evolutionary Approach for \nInteractive Computer Games. In: IEEE CONGRESS ON EVOLUTIONARY \nCOMPUTATION, 2004, Portland, Oregon, USA. Proceedings\u2026 [S.l.:s.n.], 2004. \n\n \n\nYANNAKAKIS, G.; HALLAM, J. Evolving Opponents for Interesting Interactive \nComputer Games. In: INTERNATIONAL CONFERENCE ON THE SIMULATION \n\n\n\n \n\nOF ADAPTIVE BEHAVIOR, SAB, 8., 2004. From Animals to Animats 8: \nproceedings. Cambridge: MIT, 2004. p. 499-508. \n\n \n\nYONG, C.; MIIKKULAIEN, R. Cooperative Coevolution of Multi-Agent Systems . \nAustin: Department of Computer Sciences, The University of Texas at Austin, 2001. \n(Technical Report AI01-287). \n\n \n\nWATKINS, C. J. C. H. Learning from Delayed Rewards . 1989. PhD thesis, \nUniversity of Cambridge, England. \n\n \n\nWATKINS, C. J. C. H.; DAYAN, P. Q-learning. Machine Learning, [S.l.], v. 8, n. 3, \np. 279-292, 1992. \n\n \n\nWHITESON, S. et al. Automatic Feature Selection in Neuroevolution. In: GENETIC \nAND EVOLUTIONARY COMPUTATION CONFERENCE, GECCO, 2005, \nWashington, D.C. USA. Genetic and Evolutionary Computation: proceedings. \n[S.l.:s.n.], 2005. p. 1225-1232. \n\n \n\nWHITLEY, D.; MATHIAS, K.; FITZHORN, P. Delta-coding: An iterative search \nstrategy for genetic algorithms. In: INTERNATIONAL CONFERENCE ON GENETIC \nALGORITHMS, 4., 1991. Proceedings\u2026 Los Altos, CA: Morgan Kaufmann, 1991."}]}}}