{"add": {"doc": {"field": [{"@name": "docid", "#text": "BR-TU.11372"}, {"@name": "filename", "#text": "16648_001077542.pdf"}, {"@name": "filetype", "#text": "PDF"}, {"@name": "text", "#text": "UNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nINSTITUTO DE INFORM\u00c1TICA\n\nPROGRAMA DE P\u00d3S-GRADUA\u00c7\u00c3O EM COMPUTA\u00c7\u00c3O\n\nV\u00cdCTOR EDUARDO MART\u00cdNEZ ABAUNZA\n\nPerformance Optimization of Geophysics\nStencils on HPC Architectures\n\nThesis presented in partial fulfillment\nof the requirements for the degree of\nDoctor of Computer Science\n\nAdvisor:\nProf. Dr. Philippe Olivier Alexandre Navaux\nAdvisor during Ph.D. internship:\nDr. Fabrice Dupros\n\nPorto Alegre\nAugust 2018\n\n\n\nCIP \u2014 CATALOGING-IN-PUBLICATION\n\nMart\u00ednez Abaunza, V\u00edctor Eduardo\n\nPerformance Optimization of Geophysics Stencils on HPC\nArchitectures / V\u00edctor Eduardo Mart\u00ednez Abaunza. \u2013 Porto Ale-\ngre: PPGC da UFRGS, 2018.\n\n129 f.: il.\n\nThesis (Ph.D.) \u2013 Universidade Federal do Rio Grande do Sul.\nPrograma de P\u00f3s-Gradua\u00e7\u00e3o em Computa\u00e7\u00e3o, Porto Alegre, BR\u2013\nRS, 2018. Advisor: Philippe Olivier Alexandre Navaux; Advisor\nduring Ph.D. internship: Fabrice Dupros.\n\nI. Navaux, Philippe Olivier Alexandre. II. Dupros, Fabrice.\nIII. T\u00edtulo.\n\nUNIVERSIDADE FEDERAL DO RIO GRANDE DO SUL\nReitor: Prof. Rui Vicente Oppermann\nVice-Reitora: Profa. Jane Fraga Tutikian\nPr\u00f3-Reitor de P\u00f3s-Gradua\u00e7\u00e3o: Prof. Celso Giannetti Loureiro Chaves\nDiretora do Instituto de Inform\u00e1tica: Profa. Carla Maria Dal Sasso Freitas\nCoordenador do PPGC: Prof. Jo\u00e3o Luiz Dihl Comba\nBibliotec\u00e1ria-chefe do Instituto de Inform\u00e1tica: Beatriz Regina Bastos Haro\n\n\n\n\u00ab(El cabalista que ofici\u00f3 de numen\n\na la vasta criatura apod\u00f3 Golem;\n\nestas verdades las refiere Scholem\n\nen un docto lugar de su volumen.)\u00bb\n\nEL GOLEM \u2014 J. L. BORGES\n\n\n\nACKNOWLEDGMENT\n\nI want to thank to:\n\nMy family, you are the reason\n\nMy friends, they become my family\n\nMy advisers, they become my friends\n\nMy colleagues, they become my advisers\n\nand all the people that support me...\n\nYou know who you are.\n\nV\u00edctor\n\n\n\nABSTRACT\n\nWave modeling is a crucial tool in geophysics, for efficient strong motion analysis, risk\n\nmitigation and oil &amp; gas exploration. Due to its simplicity and numerical efficiency, the\n\nfinite-difference method is one of the standard techniques implemented to solve the wave\n\npropagation equations. This kind of applications is known as stencils because they consist\n\nin a pattern that replicates the same computation on a multi-dimensional domain. High\n\nPerformance Computing is required to solve this class of problems, as a consequence of a\n\nlarge number of grid points involved in three-dimensional simulations of the underground.\n\nThe performance optimization of stencil computations is a challenge and strongly depends\n\non the underlying architecture.\n\nIn this context, this work was directed toward a twofold aim. Firstly, we have led our\n\nresearch on multicore architectures and we have analyzed the standard OpenMP imple-\n\nmentation of numerical kernels from the 3D heat transfer model (a 7-point Jacobi stencil)\n\nand the Ondes3D code (a full-fledged application developed by the French Geological\n\nSurvey). We have considered two well-known implementations (na\u00efve, and space block-\n\ning) to find correlations between parameters from the input configuration at runtime and\n\nthe computing performance; thus, we have proposed a Machine Learning-based approach\n\nto evaluate, to predict, and to improve the performance of these stencil models on the\n\nunderlying architecture. We have also used an acoustic wave propagation model provided\n\nby the Petrobras company and we have predicted the performance with high accuracy on\n\nmulticore architectures. Secondly, we have oriented our research on heterogeneous ar-\n\nchitectures, we have analyzed the standard implementation for seismic wave propagation\n\nmodel in CUDA, to find which factors affect the performance; then, we have proposed\n\na task-based implementation to improve the performance, according to the runtime con-\n\nfiguration set (scheduling algorithm, size, and number of tasks), and we have compared\n\nthe performance obtained with the classical CPU or GPU only versions with the results\n\nobtained on heterogeneous architectures.\n\nKeywords: HPC. Machine Learning. Multicore. Heterogeneous Architectures. Stencil\n\nComputations. Performance Simulation. Performance improvement.\n\n\n\nOptimiza\u00e7\u00e3o de Desempenho de Est\u00eanceis Geof\u00edsicos sobre Arquiteturas HPC\n\nRESUMO\n\nA simula\u00e7\u00e3o de propaga\u00e7\u00e3o de onda \u00e9 uma ferramenta crucial na pesquisa de geof\u00edsica\n\n(para an\u00e1lise eficiente dos terremotos, mitiga\u00e7\u00e3o de riscos e a explora\u00e7\u00e3o de petr\u00f3leo\n\ne g\u00e1z). Devido \u00e0 sua simplicidade e sua efici\u00eancia num\u00e9rica, o m\u00e9todo de diferen\u00e7as\n\nfinitas \u00e9 uma das t\u00e9cnicas implementadas para resolver as equa\u00e7\u00f5es da propaga\u00e7\u00e3o das\n\nondas. Estas aplica\u00e7\u00f5es s\u00e3o conhecidas como est\u00eanceis porque consistem num padr\u00e3o que\n\nreplica a mesma computa\u00e7\u00e3o num dom\u00ednio multidimensional de dados. A Computa\u00e7\u00e3o de\n\nAlto Desempenho \u00e9 requerida para solucionar este tipo de problemas, como consequ\u00eancia\n\ndo grande n\u00famero de pontos envolvidos nas simula\u00e7\u00f5es tridimensionais do subsolo. A\n\noptimiza\u00e7\u00e3o do desempenho dos est\u00eanceis \u00e9 um desafio e depende do arquitetura usada.\n\nNeste contexto, focamos nosso trabalho em duas partes. Primeiro, desenvolvemos nossa\n\npesquisa nas arquiteturas multicore; analisamos a implementa\u00e7\u00e3o padr\u00e3o em OpenMP dos\n\nmodelos num\u00e9ricos da transfer\u00eancia de calor (um est\u00eancil Jacobi de 7 pontos), e o aplica-\n\ntivo Ondes3D (um simulador s\u00edsmico desenvolvido pela Bureau de Recherches G\u00e9ologi-\n\nques et Mini\u00e8res); usamos dois algoritmos conhecidos (nativo, e bloqueio espacial) para\n\nencontrar correla\u00e7\u00f5es entre os par\u00e2metros da configura\u00e7\u00e3o de entrada, na execu\u00e7\u00e3o, e o\n\ndesempenho computacional; depois, propusemos um modelo baseado no Aprendizado de\n\nM\u00e1quina para avaliar, predizer e melhorar o desempenho dos modelos est\u00eanceis na arqui-\n\ntetura usada; tamb\u00e9m usamos um modelo de propaga\u00e7\u00e3o da onda ac\u00fastica fornecido pela\n\nempresa Petrobras; e predizemos o desempenho com uma alta precis\u00e3o (at\u00e9 99%) nas ar-\n\nquiteturas multicore. Segundo, orientamos nossa pesquisa nas arquiteturas heterog\u00eaneas,\n\nanalisamos uma implementa\u00e7\u00e3o padr\u00e3o do modelo de propaga\u00e7\u00e3o de ondas em CUDA,\n\npara encontrar os fatores que afetam o desempenho quando o n\u00famero de aceleradores \u00e9\n\naumentado; ent\u00e3o, propusemos uma implementa\u00e7\u00e3o baseada em tarefas para amelhorar\n\no desempenho, de acordo com um conjunto de configura\u00e7\u00e3o no tempo de execu\u00e7\u00e3o (al-\n\ngoritmo de escalonamento, tamanho e n\u00famero de tarefas), e comparamos o desempenho\n\nobtido com as vers\u00f5es de s\u00f3 CPU ou s\u00f3 GPU e o impacto no desempenho das arquite-\n\nturas heterog\u00eaneas; nossos resultados demostram um speedup significativo (at\u00e9 \u00d725) em\n\ncompara\u00e7\u00e3o com a melhor implementa\u00e7\u00e3o dispon\u00edvel para arquiteturas multicore.\n\nPalavras-chave: HPC, aprendizado de m\u00e1quina, multicore, arquiteturas heterog\u00eaneas,\n\ncomputa\u00e7\u00e3o de est\u00eanceis, simula\u00e7\u00e3o de desempenho, ganho de desempenho.\n\n\n\nLIST OF ABBREVIATIONS AND ACRONYMS\n\nANOVA Analysis of Variance\n\nAPU Accelerated Processing Architecture\n\nBRGM French Geological Survey\n\nCM Cache Misses\n\nCPU Central Processing Unit\n\nCUDA Compute Unified Device Architecture\n\nDAG Directed Acyclic Graph\n\nDMA Direct Memory Access\n\nDRAM Dynamic Random Access Memory\n\nDP Data Parallelism\n\nFDM Finite Difference Method\n\nEDP Energy Delay Product\n\nFPGA Field Programmable Gate Arrays\n\nGPU Graphics Processing Unit\n\nHC Heterogeneous Computing\n\nHCS Heterogeneous Computing System\n\nHT HyperThreading\n\nHPC High Performance Computing\n\nILP Instruction Level Parallelism\n\nISA Instruction-Set Architectures\n\nLLC Last Level Cache\n\nLU Lower Upper Decomposition\n\nMIPS Microprocessors with Interlocked Pipeline Stages\n\nML Machine Learning\n\n\n\nMLP Memory Level Parallelism\n\nMPI Message Passing Interface\n\nMSE Mean Squared Error\n\nNUMA Non-Uniform Memory Access\n\nPCIe Peripheral Component Interconnect Express\n\nPTX Parallel Thread Execution\n\nPU Processing Unit\n\nRISC Reduced Instruction Set Computer\n\nRMSE Root Mean Square Error\n\nS2S Source-to-source\n\nSCHP Single-Chip Heterogeneous Processor\n\nSIMD Single Instruction Multiple Data\n\nSM Streaming Multiprocessors\n\nSMP Symmetric Multi-Processing\n\nSMT Simultaneous Multithreading\n\nSoC System-on-a-Chip\n\nSVM Support Vector Machine\n\nTLB Translation Lookaside Buffer\n\nTLP Thread-level Parallelism\n\n\n\nLIST OF FIGURES\n\nFigure 2.1 Example of the Roofline model for an Intel Xeon (Clovertown) multi-\ncore architecture......................................................................................................32\n\nFigure 2.2 Architeture of Viking multicore machine presented in Table 2.1..................33\n\nFigure 3.1 Size of 7-point Jacobi stencil and its neighbor points. ..................................36\nFigure 3.2 Size of seismic stencil to calculate velocity and stress components. ............39\nFigure 3.3 Size of acoustic wave propagation stencil and its neighbor points................40\nFigure 3.4 Representation of solution by thread, for naive and space tiling algorithms. 42\nFigure 3.5 Impact of performance measures by number of threads................................44\nFigure 3.6 Impact of performance measures by problem size. .......................................45\nFigure 3.7 Impact of performance measures by code optimization. ...............................45\nFigure 3.8 Traces of one-time iteration for 7-point stencil execution.............................46\nFigure 3.9 Impact of performance measures by scheduling policy.................................48\nFigure 3.10 Impact of performance measures by scheduling and chunk size on Tur-\n\ning machine. ............................................................................................................49\nFigure 3.11 Impact of performance measures by stencil algorithm................................49\nFigure 3.12 Performance vs cache misses by input parameters in Turing machine .......50\nFigure 3.13 Linear fitting of cache memory by problem size and machine....................51\nFigure 3.14 Exponential fitting for problem size ............................................................52\n\nFigure 4.1 Flowchart of General Model for Performance Prediction of Geophysics\nStencils based on Machine Learning (Golem)........................................................56\n\nFigure 4.2 Flowchart of Golem on multicore architectures. ...........................................58\nFigure 4.3 Hardware counter behavior of 7-point Jacobi on Node Cryo. .......................61\nFigure 4.4 Normalized performance comparison between predicted results from\n\nthe ML-algorithm and results from the best performance experimens on mul-\nticore architectures ..................................................................................................64\n\nFigure 4.5 Flowchart of Golem on manycore architectures............................................65\nFigure 4.6 Hardware counters behavior on Manycore Node. .........................................68\nFigure 4.7 Normalized performance comparison between predicted results from\n\nthe ML-algorithm and results from the best performance experimens on many-\ncore architectures. ...................................................................................................70\n\nFigure 5.1 Block diagram of a NVIDIA GPU SM. ........................................................74\nFigure 5.2 Representation of a grid with the thread blocks. ...........................................75\nFigure 5.3 Representation of memory hierarchy for threads, blocks and grids. .............76\nFigure 5.4 Heterogeneous CUDA programming model. ................................................77\nFigure 5.5 Architecture of the Fermi SM........................................................................79\nFigure 5.6 Architecture of the Kepler SM. .....................................................................80\nFigure 5.7 Architecture of the Maxwell SM. ..................................................................81\nFigure 5.8 Architecture of the Pascal SM. ......................................................................82\nFigure 5.9 Architecture of Tegra machine presented in Table 5.1. .................................84\n\nFigure 6.1 Tiling division of 3D data domain, each slice is computed by the GPU. ......87\nFigure 6.2 Timeloop and speedup measures of Ondes 3D application when increas-\n\ning the number of GPUs. ........................................................................................88\nFigure 6.3 Kernel execution and communication time of Ondes 3D application\n\nwhen increasing the number of GPUs. ...................................................................89\n\n\n\nFigure 6.4 Measures (time, load and memory consumption) of Ondes 3D on a clus-\nter of GPUs. ............................................................................................................90\n\nFigure 6.5 Statistical estimators (average and standard deviation) of Ondes 3D for\nGPU load and memory usage. ................................................................................91\n\nFigure 7.1 Grid of blocks including inner grid-points corresponding to the physical\ndomain and outer ghosts zones ...............................................................................96\n\nFigure 7.2 Tasks dependency on a grid of 3\u00d73 blocks ...................................................97\nFigure 7.3 Impact of the scheduling algorithms for experiments on heterogeneous\n\nplatforms. Relative speedup over the worst situation on each platform. ................99\nFigure 7.4 Impact of the granularity on the efficiency of seismic wave modeling on\n\nGPUs+CPUs. ........................................................................................................100\nFigure 7.5 Speedup on the HPC node (in-core dataset) over multicore execution .......102\nFigure 7.6 Speedup for out-of-core dataset when running on the HPC node over\n\nmulticore execution...............................................................................................103\nFigure 7.7 Comparison of Time-to-solution metrics for Tegra K1 (Jetson), Guane-\n\n1 (HPC server) and BRGM node (Desktop platform). .........................................105\nFigure 7.8 Energy measures for the earthquake modeling on three heterogeneous\n\narchitectures. .........................................................................................................106\n\n\n\nLIST OF TABLES\n\nTable 2.1 Configurations of multicore machines. ...........................................................33\n\nTable 3.1 Measures for the parameters of input vector. ..................................................43\nTable 3.2 Correlation coefficient of cache access vs cache misses .................................51\nTable 3.3 RMSE, SD and R-SQUARE of cache fitting ..................................................52\nTable 3.4 RMSE, SD and R-SQUARE of exponential fitting .........................................53\n\nTable 4.1 Parameters of input vector for each algorithm. ...............................................58\nTable 4.2 Optimizations set for multicore architectures .................................................59\nTable 4.3 p-value of one-way ANOVA for the GFLOPS variable in the naive algo-\n\nrithm experiments. ..................................................................................................60\nTable 4.4 p-value of two-way ANOVA for the seismic wave kernel. .............................60\nTable 4.5 p-value of one-way and two-way ANOVA for the GFLOPS variable in\n\nSpace tiling algorithm experiments. .......................................................................61\nTable 4.6 Total number of experiments...........................................................................62\nTable 4.7 RMSE and R-square for predicted values of the 7-point Jacobi and the\n\nSeismic Wave kernels. ............................................................................................63\nTable 4.8 Available configurations for optimization procedure. .....................................66\nTable 4.9 p-value of one-way ANOVA for the manycore architecture. ..........................66\nTable 4.10 p-value of two-way ANOVA for the seismic wave kernel. ...........................67\nTable 4.11 Number of experiments .................................................................................69\nTable 4.12 Statistical estimators of our prediction model ...............................................69\n\nTable 5.1 Heterogeneous architecture configurations. ....................................................84\n\nTable 7.1 Memory consumption, number of blocks and number of parallel task for\nthe simulated scenarios ...........................................................................................98\n\nTable 7.2 Speedup on the commodity-based hardware configuration (in-core dataset)\nover multicore execution.......................................................................................101\n\nTable 7.3 Speedup on the commodity-based configuration (out-of-core dataset) over\nmulticore execution...............................................................................................102\n\nTable 7.4 List of runtime parameters that affect the performance of task-based im-\nplementation on heterogeneous architectures .......................................................104\n\n\n\nCONTENTS\n\n1 INTRODUCTION.......................................................................................................14\n1.1 Research issues ........................................................................................................15\n1.1.1 Exploiting multicore architectures .........................................................................15\n1.1.2 Machine Learning approaches on HPC platforms .................................................16\n1.1.3 Heterogeneous architectures ..................................................................................16\n1.1.4 Research context ....................................................................................................17\n1.2 Objectives and contributions .................................................................................18\n1.3 Outline......................................................................................................................21\n2 MULTICORE ARCHITECTURES AND PROGRAMMING MODELS.............24\n2.1 Parallelism ...............................................................................................................24\n2.2 Multicore architectures ..........................................................................................25\n2.2.1 Manycore architectures ..........................................................................................26\n2.3 Programming models on HPC architectures........................................................26\n2.3.1 Message Passing Interface .....................................................................................27\n2.3.2 Shared-Memory programming...............................................................................27\n2.3.3 Impact of compilers ...............................................................................................29\n2.4 Performance evaluation..........................................................................................31\n2.4.1 Hardware performance counters ............................................................................31\n2.4.2 Roofline model.......................................................................................................32\n2.5 Target machines ......................................................................................................33\n2.6 Concluding remarks ...............................................................................................34\n3 NUMERICAL BACKGROUND: GEOPHYSICAL KERNELS ON MULTI-\n\nCORE PLATFORMS ..........................................................................................35\n3.1 Stencil applications .................................................................................................35\n3.1.1 7-point Jacobi stencil .............................................................................................36\n3.1.2 Seismic wave propagation stencil ..........................................................................37\n3.1.3 Acoustic wave propagation stencil ........................................................................40\n3.2 Standard implementations of numerical stencil...................................................41\n3.2.1 Na\u00efve ......................................................................................................................41\n3.2.2 Space Tiling ...........................................................................................................41\n3.3 Performance charaterization of numerical stencils .............................................42\n3.3.1 Scalability ..............................................................................................................44\n3.4 Concluding remarks ...............................................................................................53\n4 MACHINE LEARNING STRATEGY FOR PERFORMANCE IMPROVE-\n\nMENT ON MULTICORE ARCHITECTURES ...............................................54\n4.1 Performance improvement by Machine Learning models ..................................54\n4.2 General model for performance prediction ..........................................................55\n4.2.1 Architecture of geophysics prediction model ........................................................55\n4.2.2 Performance prediction on multicore architectures ...............................................57\n4.2.3 Performance prediction on manycore architectures...............................................65\n4.3 Concluding remarks ...............................................................................................70\n5 HETEROGENEOUS ARCHITECTURES AND PROGRAMMING MODELS.72\n5.1 Streaming Multiprocessors ....................................................................................73\n5.2 Programming models on heterogeneous architectures........................................74\n5.2.1 OpenCL programming model ................................................................................77\n5.3 Evolution of NVIDIA GPU Architectures ............................................................78\n5.4 Assymetric low power architectures......................................................................82\n5.5 Target machines ......................................................................................................83\n\n\n\n5.6 Concluding remarks ...............................................................................................84\n6 NUMERICAL IMPLEMENTATION OF GEOPHYSICS STENCILS ON\n\nHETEROGENEOUS PLATFORMS .................................................................86\n6.1 Setup and Performance Measurement..................................................................87\n6.2 Elapsed Time ...........................................................................................................88\n6.3 GPU Load and Memory Usage ..............................................................................91\n6.4 Concluding Remarks ..............................................................................................92\n7 TASK-BASED APPROACH FOR PERFORMANCE IMPROVEMENT ON\n\nHETEROGENEOUS PLATFORMS .................................................................93\n7.1 Runtime systems for task-based programming....................................................93\n7.2 StarPU runtime system...........................................................................................94\n7.3 Elastodynamics over runtime system ....................................................................96\n7.4 Experiments.............................................................................................................97\n7.4.1 Scheduling strategies .............................................................................................98\n7.4.2 Size of the block.....................................................................................................99\n7.4.3 In-core dataset ......................................................................................................100\n7.4.4 Out-of-core dataset...............................................................................................101\n7.5 Summary of runtime parameters ........................................................................103\n7.6 Task-based implementation for energy efficiency ..............................................104\n7.6.1 Computing time ...................................................................................................105\n7.6.2 Energy efficiency .................................................................................................106\n7.7 Concluding remarks .............................................................................................107\n8 RELATED WORK ...................................................................................................109\n8.1 Perfomance improvement of stencil applications on multicore architectures.109\n8.2 Advanced optimizations, low-level and auto-tuning strategies.........................110\n8.3 Machine Learning approaches ............................................................................111\n8.4 Heterogeneous computing ....................................................................................113\n9 CONCLUSION AND PERSPECTIVES ................................................................116\n9.1 Contributions.........................................................................................................117\n9.2 Future Work ..........................................................................................................118\nREFERENCES.............................................................................................................120\n\n\n\n14\n\n1 INTRODUCTION\n\nThe behavior of scientific applications related to High Performance Computing\n\n(HPC) depends on many factors (non-uniform memory access, vectorization, compiler\n\noptimizations, memory policies, scheduling algorithms, etc.) that may severely influence\n\nthe performance. At the hardware level, the complexity of available computing nodes is\n\nincreasing, this includes several levels of hierarchical memories and more heterogeneous\n\ncores. At the software level, there are currently several programming models to exploit the\n\narchitecture (shared memory, message passing, parallel tasks, etc.). These evolutions lead\n\nto redesign the code of scientific applications to obtain the best performance for a spe-\n\ncific architecture with a specific programming model (BUCHTY et al., 2012; MITTAL;\n\nVETTER, 2015).\n\nExamples of HPC applications are the stencil-based applications (a nearest-neighbor\n\npattern replicated in a data domain), which are used to solve many problems related to\n\nPartial Differential Equations (PDE), the heart of many problems in areas as diverse as\n\nelectromagnetics, fluid dynamics or geophysics. This is particularly true in the case of\n\nthree-dimensional waves propagation in complex media, it is still one of the main chal-\n\nlenges in geophysics and the Finite-Difference Method (FDM) is a standard technique im-\n\nplemented to solve these equations (MOCZO; ROBERTSSON; EISNER, 2007; DATTA\n\net al., 2008; NGUYEN et al., 2010).\n\nAdditionally, this class of modeling heavily relies on parallel architectures in or-\n\nder to tackle large scale geometries including a detailed description of the physics. Last\n\ndecade, significant efforts have been devoted towards efficient implementation of the\n\nFDM on emerging architectures. These contributions have demonstrated their efficiency\n\nleading to robust scientific applications (DATTA et al., 2009; MICH\u00c9A; KOMATITSCH,\n\n2010).\n\nAlthough a large literature on the optimization of stencil numerical kernels is avail-\n\nable, predicting and optimizing its performance remains a challenge, because many input\n\nparameters are involved and affect the performance. In terms of computational efficiency,\n\none of the main difficulties is to deal with the disadvantageous ratio between the limited\n\npointwise computation and the intensive memory access required, leading to a memory-\n\nbound situation (DUPROS et al., 2008; DUPROS; DO; AOCHI, 2013).\n\n\n\n15\n\n1.1 Research issues\n\nThis thesis focuses on the performance optimization of stencil applications on\n\nHPC architectures. We consider classical geophysics numerical stencils that lie at the\n\nheart of earthquake modeling, and 3D underground imaging for the oil and gas industry.\n\n1.1.1 Exploiting multicore architectures\n\nIn addition to the challenge of geophysical modeling and the mathematical prob-\n\nlems behind seismic and acoustic wave propagation modeling, one major challenge is to\n\nleverage the various levels of parallelism involved. HPC is actually facing key challenges\n\non both the hardware and the software sides. Machines are reaching several millions of\n\ncores and the scalability of the applications could become a bottleneck. As reported in\n\nseveral recent research papers (ROTEN et al., 2016; TSUBOI et al., 2016; BREUER;\n\nHEINECKE; BADER, 2016), various geophysical applications show the variability of\n\nscaling up to thousands of cores. Indeed, regardless of the numerical method involved\n\n(Finite-Difference, Finite-Elements or Spectral-Elements), such applications benefit from\n\nthe limited amount of point-to-point communications between neighboring subdomains.\n\nTraditional methods to improve the performance of computing architectures were\n\nto increase the clock frequency, add high-speed, on-chip cache, and to optimize instruc-\n\ntions. Nowadays, companies have turned to offer parallel machines, these architectures\n\ninclude several processing units on smaller chips to provide several executions of instruc-\n\ntions in the same cycle (BLAKE; DRESLINSKI; MUDGE, 2009). Numerical implemen-\n\ntations are focused on automatic parallelization of stencil codes (SPAZIER; CHRIST-\n\nGAU; SCHNOR, 2016), analysis of the stencil performance on shared memory systems\n\nby compiler optimizations (ZHU et al., 2015), and to apply computational optimizations\n\nthat scale the performance over cores and vector units (GAN et al., 2014). At the shared-\n\nmemory level, efficient exploitation of the growing number of computing cores available\n\nremains a challenge.\n\n\n\n16\n\n1.1.2 Machine Learning approaches on HPC platforms\n\nApplication tuning represents one methodology to improve the performance on\n\nHPC architectures. In this case, several parameters such as machine architecture, domain\n\ndecomposition, compiler flags, scheduling or load balancing algorithms are considered\n\nto achieve the best performance. Unfortunately, this approach lead to the exploration of\n\na huge set of parameters, thus limiting its interest in complex platforms. Finding the\n\noptimal value for each parameter requires to search on a large configuration set, and\n\nseveral heuristics or frameworks have been proposed to speed up the process of finding\n\nthe best configuration in various contexts (DATTA et al., 2010; CHRISTEN; SCHENK;\n\nBURKHART, 2011; TANG et al., 2011; MIJAKOVIC; FIRBACH; GERNDT, 2016).\n\nAt this point, Machine Learning (ML) is a methodology for optimization that\n\ncould be applied to find patterns on a large set of input parameters. Recently, ML al-\n\ngorithms have been used on HPC systems under different situations and for various\n\nworkloads such as threads mapping and memory accesses (CASTRO; G\u00d3ES; M\u00c9HAUT,\n\n2014), I/O scheduling (BOITO et al., 2016; LI et al., 2017), or performance improvement\n\n(GANAPATHI, 2009). Building a suitable ML-based performance model for geophysics\n\nnumerical kernels remains a challenge, because the accuracy of current models don\u2019t\n\nachieve the target behavior. In this sense, a model may allow us to predict, to simulate\n\nand to optimize the performance behavior on HPC architectures with a limited amount of\n\nexperiments.\n\n1.1.3 Heterogeneous architectures\n\nThe importance of Heterogeneous Computing (HC) comes from the fact that a\n\nlarge fraction of main Top500 and Green500 lists of supercomputers use processors with\n\nboth CPUs and coprocessors (TOP500, 2017; GREEN500, 2017). The different archi-\n\ntectures and programming models on heterogeneous architectures also present several\n\nchallenges in achieving optimal performance. HC approaches have also been referred\n\nto as collaborative, hybrid, co-operative or synergistic execution, co-processing, divide\n\nand conquer approach and others (MITTAL; VETTER, 2015). In this work, we con-\n\nsider heterogeneous architectures where they are built with CPUs, as main processors,\n\nand accelerated by GPU devices. Early graphics processors were special-purpose accel-\n\nerators suitable only for applications related to graphics, image processing, and video\n\n\n\n17\n\ncoding. Current GPUs are general-purpose (also known as GPGPU) and programmable,\n\nmassively parallel processors (LINDHOLM et al., 2008).\n\nIn this sense, an approach called task-based parallelism is a data-oriented program-\n\nming model used at high-level on heterogeneous architectures, the main idea is to build a\n\ntask dependence graph, to create a queue of tasks with data directionality and to schedule\n\nthe tasks into all available processors. Task parallelism allows the creation of multiple\n\nthreads of control (processes or tasks) that can synchronize and communicate in arbitrary\n\nways (HASSEN; BAL; JACOBS, 1998). Several runtime systems have been designed\n\nfor programming and running applications on heterogeneous platforms, frameworks such\n\nas StarPU (AUGONNET et al., 2011), G-Charm (VASUDEVAN; VADHIYAR; KAL\u00c9,\n\n2013) or PaRSEC (BOSILCA et al., 2013a) have a growing impact in the scientific com-\n\nmunity. Nevertheless, the performance gains expected from the use of such runtime sys-\n\ntems come at a price. The challenge is to decouple as much as possible the algorithms and\n\nthe knowledge of the underlying architecture (STOJANOVIC et al., 2012), this situation\n\nis rather challenging for heterogeneous platforms and one of the main problems is to deal\n\nwith the costly memory transfers (KRAKIWSKY; TURNER; OKONIEWSKI, 2004).\n\n1.1.4 Research context\n\nThis research is conducted in the context of joint collaborations between the Insti-\n\ntute of Informatics of the Federal University of Rio Grande do Sul (INF-UFRGS) and the\n\nFrench Geological Survey (BRGM), Carnot Institute, under the High Performance Com-\n\nputing for Geophysics Applications project (HPC-GA) funded by the FP7-PEOPLE, grant\n\nagreement number 295217. Research has also received funding from the EU H2020 Pro-\n\ngramme and from MCTI/RNP-Brazil under the High Performance Computing for Energy\n\nProject (HPC4E), grant agreement 689772, and the Iberian-American Network for High\n\nPerformance Computing (RICAP), partially funded by the Ibero-American Program of\n\nScience and Technology for Development (CYTED), Ref. 517RT0529. This research was\n\nalso accomplished in the context of the International Laboratory in High Performance\n\nand Ambient Informatics (LICIA).\n\nAt UFRGS, the research has been developed in the Parallel and Distributed Pro-\n\ncessing Group (GPPD). This work has been granted by Coordination for the Improvement\n\nof Higher Education Personnel (CAPES), National Council for Scientific and Technolog-\n\nical Development (CNPq), Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Estado do Rio Grande do\n\n\n\n18\n\nSul (FAPERGS), and Petrobras company.\n\nIt was also supported by Intel Corporation under the Modern Code Project. For\n\ncomputer time, this research partly used the resources of Colfax Research Cluster. Some\n\nexperiments presented in this thesis were carried out using the GridUIS-2 experimen-\n\ntal testbed, being developed under the Universidad Industrial de Santander High Perfor-\n\nmance and Scientific Computing Centre (SC3UIS), development action with support from\n\nVicerrector\u00eda de Investigaci\u00f3n y Extensi\u00f3n (VIE-UIS) and several UIS research groups as\n\nwell as other funding bodies (<http://www.sc3.uis.edu.co>).\n\n1.2 Objectives and contributions\n\nThe main objective of this research is to increase the performance of stencil com-\n\nputations from geophysics models. Many work has been guided in two alternatives: im-\n\nprovement of architectural features or improvement of algorithms and implementations\n\nin specific programming models; the first alternative is out of the scope of this research\n\nand second alternative limits the performance for each implementation on the underlying\n\narchitecture. Thus, we research into a third alternative that has been recently used: how\n\nto tune the application by finding an optimal input set from a configuration set of run-\n\ntime parameters. Then, our hypothesis is: finding the optimal parameters from a input\n\nconfiguration set improve the performance of stencil applications. Considering our\n\nobjective and hypothesis, the steps are:\n\n\u2022 To define which parameters from a configuration set, at runtime level, affect the\n\nperformance of stencil computations;\n\n\u2022 On multicore architectures, to optimize the performance of stencil computations by\n\nfinding the optimal input configuration set based on an ML approach; and\n\n\u2022 On heterogeneous architectures, to exploit the computing power by searching for\n\nan optimal runtime configuration set that uses all available processing units.\n\nFirst, and according to our objectives, we contribute to the analysis and character-\n\nization of stencil computations performance on both multicore and heterogeneous archi-\n\ntectures, they are composed by CPUs and Graphics Processing Units (GPU). On heteroge-\n\nneous architectures, several high-level parameters such as data size, memory capability,\n\nhttp://www.sc3.uis.edu.co\n\n\n19\n\nscheduling algorithms and the number of processing units have been considered. This\n\ncontribution was part of HPC-GA project, the preliminary results were published in:\n\n\u2022 V\u00edctor Mart\u00ednez, David Mich\u00e9a, Olivier Aumage, Fabrice Dupros, and Philippe\n\nNavaux. \"Hybrid CPU-GPU Computing for a Finite-Difference Numerical Seis-\n\nmic Kernel: First Results with StarPU\". In: High Performance Computing for\n\nGeophysics Applications, Workshop (HPC-GA). Oral presentation. October, 2014.\n\nGrenoble, France.\n\nOn multicore architectures, we studied the influence of several input and runtime\n\nconfigurations with respect to classical algorithm implementations. This contribution was\n\npart of HPC4E project, the results have been presented in the Latin American High Per-\n\nformance Computing Conference (CARLA 2016) and in the Workshop de Processamento\n\nParalelo e Distribu\u00eddo (WSPPD 2016):\n\n\u2022 V\u00edctor Martinez, Philippe Navaux, Fabrice Dupros, Hideo Aochi, and M\u00e1rcio Cas-\n\ntro. \"Stencil-based applications tuning for multi-core architectures\". In: Latin\n\nAmerican High Performance Computing Conference (CARLA 2016). Oral presen-\n\ntation. August, 2016. Ciudad de M\u00e9xico, M\u00e9xico. Available in:&lt;https://hpc4e.eu/\n\nsites/default/files/files/presentations/Paper_Victor.pdf>.\n\n\u2022 V\u00edctor Martinez, Philippe Navaux, Fabrice Dupros, Hideo Aochi, and M\u00e1rcio Cas-\n\ntro. \"Tuning space optimization for stencil-based applications on multi-core\". In:\n\nXIV Workshop de Processamento Paralelo e Distribu\u00eddo (WSPPD). Oral presenta-\n\ntion. September, 2016. Porto Alegre, Brazil. Available in:&lt;http://www.inf.ufrgs.\n\nbr/gppd/wsppd/2016/>.\n\nSecond, we introduce a Machine Learning (ML) model to predict and to opti-\n\nmize the performance of stencil computations on multicore architectures. The key idea\n\nis to provide adaptability of the input parameters, a training execution set is used in a\n\nlearning process until the model is available to reach the best performance. The final\n\nmodel could be integrated into auto-tuning frameworks to find the best configuration for\n\na given stencil application. This contribution was part of HPC4E project, the Modern\n\nCode Project and the Petrobras 2016/00133-9 project, the results are available in the pro-\n\nceedings of International Conference on Computational Science (ICCS 2017) published\n\nin Procedia Computer Science, proceedings of the Latin American High Performance\n\nComputing Conference (CARLA 2017) published in Communications in Computer and\n\nhttps://hpc4e.eu/sites/default/files/files/presentations/Paper_Victor.pdf\nhttps://hpc4e.eu/sites/default/files/files/presentations/Paper_Victor.pdf\nhttp://www.inf.ufrgs.br/gppd/wsppd/2016/\nhttp://www.inf.ufrgs.br/gppd/wsppd/2016/\n\n\n20\n\nInformation Science, proceedings of 18o Escola Regional de Alto Desempenho do Es-\n\ntado do Rio Grande do Sul (ERAD/RS), and has been published at HPC4E: High Perfor-\n\nmance Computing for Energy Workshop along with The Eighth International Conference\n\non Smart Grids, Green Communications and IT Energy-aware Technologies (ENERGY\n\n2018):\n\n\u2022 V\u00edctor Mart\u00ednez, Fabrice Dupros, M\u00e1rcio Castro and Philippe Navaux. \"Perfor-\n\nmance Improvement of Stencil Computations for Multi-core Architectures based\n\non Machine Learning\". In: International Conference on Computational Science\n\n(ICCS) Z\u00fcrich, Switzerland: Procedia Computer Science, 2017, V. 108, pp. 305\u2013\n\n314. DOI:10.1016/j.procs.2017.05.164. Available in:&lt;https://www.sciencedirect.\n\ncom/science/article/pii/S1877050917307408>. Qualis: A1.\n\n\u2022 V\u00edctor Mart\u00ednez, Matheus Serpa, Fabrice Dupros, Edson L. Padoin, and Philippe\n\nNavaux. \"Performance Prediction of Acoustic Wave Numerical Kernel on Intel\n\nXeon Phi Processor\". In: In: Mocskos E., Nesmachnow S. (eds) High Perfor-\n\nmance Computing. (CARLA 2017). Communications in Computer and Informa-\n\ntion Science, vol 796, pp. 101\u2013110. Springer, Cham. Buenos Aires, Argen-\n\ntine. DOI:10.1007/978-3-319-73353-1_7. Available in:&lt;https://link.springer.com/\n\nchapter/10.1007/978-3-319-73353-1_7>. Qualis: B4.\n\n\u2022 V\u00edctor Mart\u00ednez, and Philippe Navaux. \"Performance Prediction of Stencil Appli-\n\ncations on Accelerator Architectures\". In: 18o Escola Regional de Alto Desem-\n\npenho do Estado do Rio Grande do Sul (ERAD/RS), 2018. Available in:&lt;http:\n\n//www.inf.ufrgs.br/erad2018/anais/FPG/179903.pdf>.\n\n\u2022 V\u00edctor Mart\u00ednez, Matheus Serpa, Philippe Navaux, Edson L. Padoin, and Jairo\n\nPanetta. \"Performance Prediction of Geophysics Numerical Kernels on Accelera-\n\ntor Architectures\". Contribution presented at HPC4E: High Performance Comput-\n\ning for Energy Workshop along with The Eighth International Conference on Smart\n\nGrids, Green Communications and IT Energy-aware Technologies (ENERGY 2018).\n\nQualis: B5.\n\nFinally, the third contribution of this work is the introduction of a task-based im-\n\nplementation of the elastodynamics equation for heterogeneous architectures. This contri-\n\nbution on heterogeneous architectures use the maximum number of available processing\n\nunits and have been demonstrated a better performance than standard implementations.\n\nhttps://www.sciencedirect.com/science/article/pii/S1877050917307408\nhttps://www.sciencedirect.com/science/article/pii/S1877050917307408\nhttps://link.springer.com/chapter/10.1007/978-3-319-73353-1_7\nhttps://link.springer.com/chapter/10.1007/978-3-319-73353-1_7\nhttp://www.inf.ufrgs.br/erad2018/anais/FPG/179903.pdf\nhttp://www.inf.ufrgs.br/erad2018/anais/FPG/179903.pdf\n\n\n21\n\nThis contribution was part of HPC-GA project, the results were published in the proceed-\n\nings of 27th International Symposium on Computer Architecture and High Performance\n\nComputing (SBAC-PAD 2015):\n\n\u2022 V\u00edctor Mart\u00ednez, David Mich\u00e9a, Fabrice Dupros, Olivier Aumage, Samuel Thibault,\n\nHideo Aochi, and Philippe Navaux. \"Towards Seismic Wave Modeling on Hetero-\n\ngeneous Many-Core Architectures Using Task-Based Runtime System\". In: 27th\n\nInternational Symposium on Computer Architecture and High Performance Com-\n\nputing (SBAC-PAD 2015). Florian\u00f3polis, Brazil: IEEE Computer Society, 2015,\n\npp. 1\u20138. DOI: 10.1109/SBAC-PAD.2015.33. Available in:&lt;https://ieeexplore.\n\nieee.org/document/7379827/>. Qualis: B1.\n\nThis contribution for heterogeneous architectures has been extended to the analy-\n\nsis of the energy consumption. A low-power many-core heterogeneous architecture was\n\nused as an alternative to solve the seismic model with a better energy efficiency. Our\n\nsolution makes a better usage of the available resources (CPU and GPU cores) with a sig-\n\nnificant reduction of the energy consumption and communication cost. This contribution\n\nwas a joined work with SC3UIS, the results were presented in the Latin American High\n\nPerformance Computing Conference (CARLA 2015) and in the Workshop de Processa-\n\nmento Paralelo e Distribu\u00eddo (WSPPD 2015):\n\n\u2022 V\u00edctor Mart\u00ednez, John Garc\u00eda, Carlos Barrios, Fabrice Dupros, Hideo Aochi, and\n\nPhilippe Navaux. \"Task-based Programming on Low-power Nvidia Jetson TK1\n\nManycore Architecture: Application to Earthquake Modelling\". In: Latin Ameri-\n\ncan High Performance Computing Conference (CARLA 2015). Oral presentation.\n\nAugust, 2015. Petr\u00f3polis, Brazil.\n\n\u2022 John Garc\u00eda, V\u00edctor Mart\u00ednez, Philippe Navaux, and Carlos Barrios. eGPU for\n\nMonitoring Performance and Power Consumption on Multi-GPUs. In: XIII Work-\n\nshop de Processamento Paralelo e Distribu\u00eddo (WSPPD). Oral presentation. Au-\n\ngust, 2015. Porto Alegre, Brazil. Available in:&lt;http://inf.ufrgs.br/gppd/wsppd/\n\n2015/papers/footer/WSPPD_2015_paper_13.pdf>\n\n1.3 Outline\n\nWe divided this document into two parts. The First Part focuses on performance\n\nimprovement of stencil applications on multicore architectures, and is organized as fol-\n\nhttps://ieeexplore.ieee.org/document/7379827/\nhttps://ieeexplore.ieee.org/document/7379827/\nhttp://inf.ufrgs.br/gppd/wsppd/2015/papers/footer/WSPPD_2015_paper_13.pdf\nhttp://inf.ufrgs.br/gppd/wsppd/2015/papers/footer/WSPPD_2015_paper_13.pdf\n\n\n22\n\nlows:\n\n\u2022 Chapter 2 reviews the fundamental concepts involved in this work. We present the\n\narchitectural features of multicore systems and the common parallel programming\n\nmodels;\n\n\u2022 Chapter 3 describes the numerical background for the geophysics numerical ker-\n\nnels, and we characterize the performance of classical implementations on multi-\n\ncore architectures;\n\n\u2022 Chapter 4 focuses on the use of ML to predict and to optimize the performance of\n\ngeophysics numerical kernels on multicore and many-core architectures.\n\nThe Second Part is focused on performance improvement of a geophysics numer-\n\nical stencil on heterogeneous architectures, and corresponds with following chapters:\n\n\u2022 Chapter 5 reviews the basic features of heterogeneous architectures and common\n\nprogramming models;\n\n\u2022 Chapter 6 presents the standard implementation of a seismic wave propagation sten-\n\ncil on heterogeneous architectures;\n\n\u2022 Chapter 7 details the task-based implementation and compare the performance ob-\n\ntained with the classical CPU or GPU only versions.\n\nFinally, we present the related works and the conclusion of this research in follow-\n\ning chapters:\n\n\u2022 Chapter 8 presents the related works;\n\n\u2022 Chapter 9 concludes this document, and presents the perspectives.\n\n\n\nPart 1: Performance Optimization on Multicore Architectures\n\n\n\n24\n\n2 MULTICORE ARCHITECTURES AND PROGRAMMING MODELS\n\nWe start the first part of this document with this chapter. We present a background\n\nof the HPC platforms and the programming models. Traditional methods to improve the\n\nperformance of computing architectures were to increase the clock frequency, add high-\n\nspeed, on-chip cache. These methods worked until physical issues limited the processor\n\nmanufacturing. Nowadays, companies have turned to offer parallel machines, these archi-\n\ntectures includes on processors several processing units to provide multiple executions of\n\ninstructions in the same cycle. The performance improvement is achieved by replicating\n\nthe processing units, adding additional processing instructions, improving communica-\n\ntion between the cores, and the calculations are solved simultaneously, in parallel. In\n\nthis context, Moore\u2019s law (BLAKE; DRESLINSKI; MUDGE, 2009) has also driven a\n\nconstant increase in parallelism and performance.\n\n2.1 Parallelism\n\nParallelism is considered for several levels, in (BUCHTY et al., 2012) the authors\n\npresent six levels: (1) Instruction Level Parallelism (ILP) provides techniques to parallel\n\nprocessing at runtime, (2) Data Parallelism (DP) exploited mainly by Single Instruction\n\nMultiple Data (SIMD) processing, (3) Hardware-Supported Multithreading performs a\n\nthread level parallelism (TLP) with simultaneous multithreading (SMT) or HyperThread-\n\ning (HT), when the inactive resources could be used to execute instructions from another\n\nthread, (4) Core-Level Parallelism builds a homogeneous multicore processor with strong\n\ncoupling of cores, (5) Socket-Level Parallelism uses various processing devices (CPUs,\n\nGPUs, FPGAs, etc.), and finally (6) Node Level Parallelism is provided with a set of nodes\n\nconnected by specific network topologies.\n\nPerformance optimization of parallel architectures is mainly related to adding\n\nmore and more processing units, and applications need to be adapted to a wide range\n\nof platforms. Most common parallel architectures are the general-purpose multicore em-\n\nploying a low number of heavy-weight and highly complex cores, and multilevel cache\n\nhierarchies (i.e., private L1 and L2 caches and a shared L3 cache), as the memory access\n\ntime depends on the memory location relative to the processor, each core is addressed to\n\naccess its own private local memory, these architectures evolved to Non-Uniform Memory\n\nAccess (NUMA) machines.\n\n\n\n25\n\n2.2 Multicore architectures\n\nArchitectural features on parallel architectures can be improved in two ways: First,\n\nmultiple pipelines can be added to fetch and issue more instructions in parallel, creating a\n\nsuperscalar processing element; second, by increasing the number of stages, thus reducing\n\nthe logic per stage. These improvements are successful for in-order processing elements.\n\nOn the other hand, out-of-order architecture gains as much single thread performance as\n\npossible by dynamic scheduling to keep the pipelines full. This kind of optimization is\n\nimplemented on multicore machines.\n\nMulticore architectures can be classified according to their attributes: the applica-\n\ntion domain, the power/performance ratio, the class of processing elements, the memory\n\nsystem, and accelerators/integrated peripherals (SHUKLA; MURTHY; CHANDE, 2015).\n\nThe application domain has two classes of processing: data processing dominated and\n\ncontrol dominated; digital signal processors (DSPs) are the example for data processing-\n\ndominated applications and control-dominated applications include devices for file com-\n\npression, decompression, and network processing. Power/Performance relation is also an\n\nimportant goal for multicore processors, power consumption has become a concern for\n\ncomputers.\n\nAt the architectural level, the memory system was a rather simple component,\n\nconsisting of a few levels to feed the single processor with a data and instructions pri-\n\nvate cache. In multicores, the caches are just one part of the memory system, the other\n\ncomponents include the consistency model, cache coherence support, and the intrachip\n\ninterconnect. A consistency model defines how the memory operations may be reordered\n\nwhen the code is executing.\n\nCaches have increased importance in multicore processors. They give a fast local\n\nmemory to work with processing elements. The amount of cache required depends on\n\nthe application. Bigger caches are better for performance but show diminishing returns as\n\ncaches sizes grow. The number of cache levels has been increasing as processing elements\n\nget faster and become more numerous. The L1 cache is accessed on every instruction\n\ncycle as part of the instruction pipeline and is broken into separate instruction and data\n\ncaches, and it is usually rather small, fast, and private to each processing element. The L2\n\ncache can be private for each core or shared between cores. L3 is a shared cache for all\n\nprocessing elements and reduced delays in multi-threaded environments.\n\n\n\n26\n\n2.2.1 Manycore architectures\n\nIn this work, we consider the Xeon Phi processor as a special case of multicore\n\narchitectures. The Knights Landing (KNL) is the code name for the second-generation\n\nIntel Xeon Phi family. It is a many-core architecture that delivers massive thread paral-\n\nlelism, data parallelism, and memory bandwidth in a CPU form factor for high throughput\n\nworkloads. It is a standard, standalone processor that can boot an off-the-shelf operating\n\nsystem.\n\nThe KNL brings two types of memory: multichannel DRAM (MCDRAM) and\n\ndouble data rate (DDR) memory. MCDRAM is a high bandwidth and low capacity (up\n\nto 16GB) memory comprising eight devices (2 GBytes each) integrated on-package and\n\nconnected to the KNL die via a proprietary on-package I/O. All eight MCDRAM de-\n\nvices together provide an aggregate Stream triad benchmark bandwidth of more than 450\n\nGBytes per second. KNL has six DDR4 channels running up to 2,400 MHz, with three\n\nchannels on each of two memory controllers, providing an aggregate bandwidth of more\n\nthan 90 GBps. Each channel can support at most one memory DIMM. The total DDR\n\nmemory capacity supported is up to 384 GBytes.\n\nKNL has three memory modes and can be configured as Cache mode to work as a\n\nthird level cache; in Flat mode, both the MCDRAM memory and the DDR memory act as\n\nregular memory and are mapped into the same system address space as a distinct NUMA\n\nnode (allocatable memory); and the Hybrid mode, the MCDRAM is partitioned such that\n\neither a half or a quarter of the MCDRAM is used as cache, and the rest is used as flat\n\nmemory (SODANI et al., 2016).\n\n2.3 Programming models on HPC architectures\n\nAccording to the evolution of HPC architectures, the programming models also\n\nhave been developed to exploit the processing capability. They mainly involved the fol-\n\nlowing aspects: how to send data in a network of processors and how to share and process\n\ndata from the main memory between several cores. Parallel programming models are usu-\n\nally based on three fields: message passing, shared memory, and data-parallel. The first\n\nprovides a high level of controlling architecture mapping and forces the programmer to\n\ndetailed partitioning and orchestration. The second has standardized programming envi-\n\nronments such as OpenMP for targeting compiler-exploitable TLP. The third, well-known\n\n\n\n27\n\nexamples are the Compute Unified Device Architecture (CUDA) and the Open Computing\n\nLanguage (OpenCL), the first standard for kernel invocation on multiple heterogeneous\n\nsystems (NVIDIA, 2016; INC., 2018). CUDA and heterogeneous architectures will be\n\ndiscussed in Chapter 5\n\n2.3.1 Message Passing Interface\n\nThe Message Passing Interface (MPI) is the most common programming model\n\nfor parallel machines with distributed memory and has been used widely in parallel and\n\ndistributed computing systems. The basic content of MPI is point-to-point communication\n\nbetween processes and collective communications.\n\nThe point-to-point message-passing routines form the core of the MPI standard,\n\nthe basic operations being send and receive. They allow messages to be sent between\n\npairs of processes, with message selectivity based explicitly on message tag and source\n\nprocess, and implicitly on communication context. MPI defines a group as an ordered set\n\nof process identifiers, each of which is assigned a numerical rank, between zero and the\n\nsize of the group. The identifier is used to send/receive data between processors.\n\nCollective communications are provided where all processes in a group are in-\n\nvolved in a collective operation. A collective function is called, in a group synchroniza-\n\ntion, when it is necessary; although this is not mandated and some implementations may\n\nnot synchronize. These collective communications allow processing activities as broad-\n\ncasting or data reducing. (CLARKE; GLENDINNING; HEMPEL, 1994)\n\n2.3.2 Shared-Memory programming\n\nIn a shared-memory system, every processor has direct access to the memory of\n\nevery other processor, meaning each one can load or store any shared address. The pro-\n\ngrammer also can declare data variables as privates to the processor. With shared-memory,\n\nthe processes can exchange data more quickly than by MPI by using a designated area of\n\nmemory. The data can be made directly accessible to all processes without having to use\n\nthe communications (DAGUM; MENON, 1998).\n\nBecause of the ability to directly access memory throughout the system (with mini-\n\nmum latency and no explicit address mapping), combined with fast shared-memory locks,\n\n\n\n28\n\nOpenMP is used to implement the shared-memory applications. At its most elemental\n\nlevel, OpenMP is a set of compiler directives and runtime library routines that extend\n\nFortran, C and C++ languages to express shared-memory parallelism. Program execu-\n\ntion begins as a single process. This initial process executes serially and can set up the\n\nproblem in a standard sequential manner until encounter a parallel construct defined by\n\nthe directive #pragma omp parallel. The runtime forms a team of one or more\n\nthreads and creates the data environment for each team member.\n\nParallel looping\n\nThe shared-memory model in OpenMP makes possible to parallelize at loop level\n\nwithout decomposing the data structures. The construct is #pragma omp for. The\n\ninner iterations from loops are executed by several threads. A parallel construct by itself\n\ncreates a Single Program Multiple Data (SPMD) program, each thread redundantly exe-\n\ncutes the same code on different areas from shared data. Usually, there are many more\n\niterations in a loop than the number of threads. Thus, a scheduling policy is necessary to\n\nassign the loop iterations to the threads.\n\nOpenMP scheduling can be defined in runtime by the OMP_SCHEDULE environ-\n\nment variable, this variable is a string formatted by two parameters: scheduling policy and\n\nchunk size. Four different loop scheduling policies can be provided to OpenMP: Static\n\ndivide the loop into equal-sized chunks; Dynamic uses the internal work queue to give a\n\nchunk-sized block of loop iterations to each thread; Guided is similar to dynamic, but the\n\nchunk size starts off large and decreases to better handle load imbalance between itera-\n\ntions; and Auto, when the decision regarding scheduling is delegated to the compiler. The\n\noptional parameter (chunk), when specified, must be a positive integer and defines how\n\nmany loop iterations will be assigned to each thread at a time (INTEL, 2014).\n\nParallel tasking\n\nTasks in OpenMP are code blocks that the compiler wraps up and makes available\n\nto be executed in parallel threads. The construct is #pragma omp task. Teams of\n\nthreads are created and tasks can be executed in arbitrary order, one per thread. They are\n\nsynchronized by the master thread using a barrier, to check when all tasks are completed.\n\nWhen a thread encounters the task construct, it may execute the task immediately\n\nor delay its execution. If delayed, the task is located in a pool of tasks associated with\n\n\n\n29\n\nthe current parallel region. All threads in a team will take tasks out of the pool and\n\nexecute them until the pool is empty. A thread that executes a task might be different\n\nfrom the thread that originally encountered it. At some point, if the list of delayed tasks\n\nis too long, the runtime may stop the task generating, and switches all threads to execute\n\nalready generated tasks\n\n2.3.3 Impact of compilers\n\nCompilers are very important in performance optimization. To handle the multi-\n\ncore architectures and to parallelize a program, the compiler must perform three tasks:\n\nfirst, it analyzes the program to determine the dependencies between instructions; second,\n\nit performs ILP optimizations which remove dependencies between instructions; third, it\n\nreorders the instructions, a process known as code scheduling. For memory accesses, it is\n\nalso useful to know if two or more instructions read the same memory location (HWU et\n\nal., 1995).\n\nOptimization in compiling is a process where the control and data flows are an-\n\nalyzed and may transform the order of instructions to satisfy performance requirements.\n\nThere are two common compilers for multicore architectures: the GNU Compiler Collec-\n\ntion (GCC) is an integrated distribution of compilers for many programming languages\n\n(C, C++, Objective-C, Objective-C++, Java, Fortran, Ada, and Go) (STALLMAN; COM-\n\nMUNITY, 2017), and the Intel C++ Compiler (ICC) (INTEL, 2018) to compile and gen-\n\nerate applications that can run on the Intel R\u00a9 64 and IA-32 architectures (available to\n\nLinux, Windows an MacOS).\n\nThe compilers support several optimization levels to control compiling time, mem-\n\nory usage, speed and code scaling at runtime. But finding optimal performance is quite\n\ndifficult because to expand compiler options creates a large set. For example, to evaluate\n\nthe compiler optimizations (-O1, -O2, and -O3 flags) of six parallel codes (implemented\n\nin sequential, Pthreads, C++11, OpenMP, Cilk Plus, and TBB) on two different multicore\n\narchitectures (Intel Xeon and AMD Opteron) used a total of 240 performance combina-\n\ntions (MACHADO et al., 2017).\n\nAdditionally to OpenMP, there are some approaches to exploit the multicore ar-\n\nchitectures based on compiler directives. First, OpenCL is an open standard that provides\n\na common language, programming interfaces, and hardware abstraction for developing\n\napplications in HPC environments. The environment consists of a host CPU and any at-\n\n\n\n30\n\ntached accelerator device. OpenCL offers classic compilation and linking features, and\n\nit also supports runtime compilation that allows the execution of kernels on devices un-\n\navailable when applications were developed. Runtime compilation in OpenCL allows an\n\napplication to be independent of instruction sets of devices.\n\nSecond, the OpenACC approach is also represented by a set of compiler direc-\n\ntives. OpenACC is a nonprofit corporation founded by four companies: CAPS Enter-\n\nprise, CRAY Inc., the Portland Group Inc., and NVIDIA. Their objective was to create\n\na cross-platform API that would easily allow acceleration of applications on manycore\n\nand multicore processors using directives. OpenACC API-enabled compilers and run-\n\ntimes hide concerns about the initialization of accelerators, or data and program transfer\n\nbetween the host and the accelerator, inside the programming model. This allows the\n\nprogrammer to provide additional information to the compilers, including locality of data\n\nto an accelerator and mapping of loops onto an accelerator (CANTIELLO; MARTINO;\n\nMOSCATO, 2014).\n\nSource-to-source transformation\n\nRecent alternatives to support compilers and model programming for multicore\n\narchitectures are the source-to-source (S2S) transformers. They convert a program source\n\nwritten in a given language to a new version in the same language or in a different one,\n\nand are conceived mainly with one or more of the following objectives: to transform a\n\nsequential version of code into a parallel version for a target architecture, to transform\n\na parallel source code written with a particular paradigm (i.e., OpenMP) to a different\n\nlanguage (i.e., OpenCL), to apply source code optimization on regions of code (i.e., loop-\n\nnests) in order to take advantage of hardware features or to improve data locality.\n\nThe process of transformation of code generally is not a priori fixed, but it can be\n\ndriven in several ways: by users who can annotate code regions they want to transform,\n\nby analyzing dependences on data inside loop nests, or by the size of the data involved.\n\nThere are systems that produce multiple versions of the translated code with software\n\nprobes that can select among them at runtime, depending on the architecture performance,\n\nthese are known as auto-tuning systems. One example of this is the BOAST framework\n\n(CRONSIOE; VIDEAU; MARANGOZOVA-MARTIN, 2013)\n\n\n\n31\n\n2.4 Performance evaluation\n\nTo exploit the parallelism, HPC applications must be programmed to consider-\n\nably reduce the overhead between the processors. Because of the high cost involved in\n\nhardware implementation or software simulation of HPC architectures, a performance\n\nevaluation needs to be carried out through analytic techniques. A mathematical model\n\nto analyze the performance makes it possible to study the efficiency in terms of various\n\ndesign parameters used as inputs to a performance model. And it is necessary to take a\n\ngeneral approach, independent of the application (BHUYAN; YANG; AGRAWAL, 1989).\n\nIn computer systems, performance evaluation is a fundamental phase. The most\n\ndirect method of performance evaluation is by the executing applications, collecting the\n\noutput, and observing the system performance by analyzing the output. Another method\n\nof performance evaluation is when the architecture is not available, this method helps\n\nto predict the requirements at previous stages of application execution (KUMAR; BAL-\n\nAMURUGAN, 2017). Furthermore, characterizing and predicting the performance of\n\napplications at runtime on multicore architectures help to design best performing config-\n\nurations and to optimize the execution of applications on new systems. To build linear\n\nmodels to predict the performance in different chips is useful to characterize the perfor-\n\nmance of common HPC applications (ROSALES et al., 2017).\n\n2.4.1 Hardware performance counters\n\nKnowing the status on multicore architectures, when they are running the applica-\n\ntions, allows to improve the performance. In this sense, the hardware performance coun-\n\nters exist as a small set of registers that counts the occurrences of specific signals related\n\nto the processor functions (cache misses, cycles, branch instructions, FLOPS, etc.). Mon-\n\nitoring these events facilitates correlation between the structure of code and the efficiency\n\nof that code to the underlying architecture.\n\nIn this work, we use a common library to access the registers from hardware coun-\n\nters called the Performance Application Programming Interface (PAPI). It provides two\n\ninterfaces to the underlying counter hardware: a high-level interface for the acquisition\n\nof measurements, it provides the ability to start, stop and read the counters for a spec-\n\nified list of events, and a low-level interface that deals with hardware events in groups\n\ncalled EventSets. An EventSet consists of countable events that the user can count as\n\n\n\n32\n\npart of a group, it can reflect how the counters are most frequently used, such as taking\n\nsimultaneous measurements of different hardware events (MUCCI et al., 1999).\n\n2.4.2 Roofline model\n\nThe Roofline model (WILLIAMS; WATERMAN; PATTERSON, 2009) is a per-\n\nformance model used for kernel computations and combines together the floating-point\n\nperformance, the operational intensity, and memory performance in a 2D graph. The peak\n\nfloating-point performance can be found through hardware specifications or microbench-\n\nmarks. The operational intensity means the operations per byte of DRAM traffic, defining\n\ntotal bytes accessed as those bytes that go to the main memory after they have been filtered\n\nby the cache hierarchy.\n\nThe graph is on a log-log scale. The y-axis is the floating-point performance.\n\nThe x-axis is the operational intensity. The Roofline model provides several upper bound\n\nto performance that gives this model its name, the horizontal lines represent the paral-\n\nlel optimizations (TLP, ILP, SIMD, and floating-point balance) and diagonal lines rep-\n\nresent the peak memory bandwidth and related optimizations (restructure loops for unit\n\nstride accesses, memory affinity, and software prefetching). An example of the Roofline\n\nmodel for an Intel Xeon multicore architecture is illustrated in Figure 2.1. The red ver-\n\ntical dashed lines indicate the operational intensity for different kernels (7-point stencil,\n\nLattice-Boltzmann MagnetoHydro-Dynamics, and 3D the Fast Fourier Transform), the X\n\nmarks the performance achieved for each kernel. As we can see, the different kind of\n\napplications and the parallelism level achieve different performance.\n\nFigure 2.1: Example of the Roofline model for an Intel Xeon (Clovertown) multicore\narchitecture.\n\ncontributed articles\n\nA P R I L  2 0 0 9   |   V O L .  5 2   |   N O .  4   |   C O M M U N I C AT I O N S  O F  T H E  A C M     71\n\nnot those systems are parallel. \nOne advantage of using these high-\n\ner-level descriptions of programs is \nthat we are not tied to code that might \nhave been originally written to opti-\nmize an old computer to evaluate fu-\nture systems. Another advantage of the \nrestricted number is that efficiency-lev-\nel programmers can create autotuners \n\nfor each kernel that would search the \nalternatives to produce the best code \nfor that multicore computer, includ-\ning extensive cache optimizations.13 \n\nTable 2 lists the four kernels from \namong the Seven Dwarfs we use to dem-\nonstrate the Roofline model on the four \nmulticore computers listed in Table 1; \nthe autotuners discussed in this sec-\n\ntion are from three sources:12, 28, 29  \n\nFor these kernels, there is sufficient \nparallelism to utilize all the cores and \nthreads and keep them load balanced; \nsee online Appendix A.2 for how to han-\ndle cases when load is not balanced. \n\nRoofline models and results. Figure \n3 shows the Roofline models for Xeon, \nX4, and Cell. The pink vertical dashed \nlines indicate the operational inten-\nsity and the red X marks performance \nachieved for that particular kernel. \nHowever, achieving balance is difficult \nfor the others. Hence, each computer \nin Figure 3 has two graphs: the left one \nhas multiply-add balance as the top \nceiling and is used for Lattice-Boltz-\nmann Magnetohydrodynamics (LB-\nMHD), Stencil, and 3D FFT; the right \none has multiply-add as the bottom \nceiling and is used for SpMV. Since the \nT2+ lacks a fused multiply-add instruc-\ntion nor can it simultaneously issue \nmultiplies and adds, Figure 4 shows a \nsingle roofline for the four kernels on \nthe T2+ without the multiply-add bal-\nance ceiling. \n\nThe Intel Xeon has the highest peak \ndouble-precision performance of the \nfour multicores. However, the Roofline \nmodel in Figure 3a shows this level of \nperformance can be achieved only with \noperational intensities of at least 6.7 \nFlops/Byte; in other words Clovertown \nrequires 55 floating-point operations \nfor every double-precision operand \n(8B) going to DRAM to achieve peak \nperformance. This high ratio is due in \npart to the limitation of the front-side \nbus, which also carries the coherency \ntraffic that can consume up to half the \nbus bandwidth. Intel includes a snoop \nfilter to prevent unnecessary coheren-\ncy traffic on the bus. If the working set \nis small enough for the hardware to fil-\nter, the snoop filter nearly doubles the \ndelivered memory bandwidth. \n\nThe Opteron X4 has a memory \ncontroller on chip, its own path to \n667MHz DDR2 DRAM, and separate \npaths for coherency. Figure 3 shows \nthat the ridge point in the Roofline \nmodel is to the left of the Xeon, at an \noperational intensity of 4.4 Flops/Byte. \nThe Sun T2+ has the highest memory \nbandwidth so the ridge point is an ex-\nceptionally low operational intensity \nof just 0.33 Flops/Byte. It keeps mul-\ntiple memory transfers in flight by us-\ning many threads. The IBM Cell ridge \n\nFigure 3a\u20133c: Roofline model for Intel Xeon, AMD Opteron X4, and IBM Cell. \n\nOperational Intensity (Flops/Byte)\n\n(a) Intel Xeon (Clovertown)\n\npeak DP\n\n+balanced \nmul/add\n\n+SIMD\n\n+ILP\n\nTLP only\n\npea\nk st\n\nrea\nm b\n\nand\nwid\n\nth\n\n+sn\noop\n\n filt\ner e\n\nffec\ntive\n\nL\nB\n\nM\nH\n\nD\n\nF\nF\n\nT\n (\n\n51\n23\n\n)\n\nF\nF\n\nT\n (\n\n12\n8\n\n3 )\n\nsno\nop \n\nfilt\ner i\n\nnef\nfec\n\ntive\n\nS\nte\n\nnc\nil\n\nG\nF\n\nlo\np\n\ns/\ns\n\n128\n\n64\n\n32\n\n16\n\n8\n\n4\n\n2\n\n1\n\n1/16 1/8 1/4 1/2 1 2 4 8 16\n\nOperational Intensity (Flops/Byte)\n\n(c) AMD Opteron X4 (Barcelona)\n\npeak DP\n\n+balanced \nmul/add\n\n+SIMD\n\n+ILP\n\nTLP only\n\npea\nk st\n\nrea\nm b\n\nand\nwid\n\nth\n\npea\nk co\n\npy \nban\n\ndw\nidth\n\nwit\nhou\n\nt m\nem\n\nory\n affi\n\nnity\n\nS\nte\n\nnc\nil\n\nL\nB\n\nM\nH\n\nD\n\nF\nF\n\nT\n (\n\n51\n23\n\n)\n\nF\nF\n\nT\n (\n\n12\n8\n\n3 )\n\nG\nF\n\nlo\np\n\ns/\ns\n\n128\n\n64\n\n32\n\n16\n\n8\n\n4\n\n2\n\n1\n\n1/16 1/8 1/4 1/2 1 2 4 8 16\n\nOperational Intensity (Flops/Byte)\n\n(b) Intel Xeon (Clovertown)\n\npeak DP\n\n+SIMD\n\n+ILP\n\nTLP only\n\npea\nk st\n\nrea\nm b\n\nand\nwid\n\nth\n\nsno\nop \n\nfilte\nr in\n\neffe\nctiv\n\ne\n\nSpMV+s\nnoo\n\np fi\nlter\n\n eff\nect\n\nive\n\n+balanced \nmul/add\n\nG\nF\n\nlo\np\n\ns/\ns\n\n128\n\n64\n\n32\n\n16\n\n8\n\n4\n\n2\n\n1\n\n1/16 1/8 1/4 1/2 1 2 4 8 16\n\nSource: (WILLIAMS; WATERMAN; PATTERSON, 2009)\n\n\n\n33\n\n2.5 Target machines\n\nCharacteristics of multicore architectures used in this research are described in Ta-\n\nble 2.1. Although these machines are quite different in terms of the number of cores and\n\nmemory size, the processor architecture is similar: a multicore chip with a memory hier-\n\narchy of several levels of cache. They are located at BRGM (France) and at Informatics\n\nInstitute of UFRGS (Brazil).\n\nTable 2.1: Configurations of multicore machines.\nHostname Viking (UFRGS) Cryo (BRGM) Server 185 (BRGM) Turing (UFRGS) KNL (UFRGS)\n\nProcessor Xeon E5530 Xeon E5-2650 Xeon E5-4650 Xeon X7550 Xeon Phi 7520\n\nClock (GHz) 2.40 2.00 2.70 2.00 1.40\n\nPhysical cores 4 8 8 8 68\n\nSockets 2 2 4 4 1\n\nThreads 16 16 32 64 272\n\nCache (MB) L3 (8) L3 (20) L3 (20) L3 (18) L2 (34)\n\nCompiler gcc 4.6.4 gcc 5.4.0 gcc 5.4.0 gcc 4.6.4 icc 18.0.1\n\nTopology of Viking node is presented in Figure 2.2 and is a typical multicore ma-\n\nchine. Each core runs two threads simultaneously (TLP) by the HyperThreading tech-\n\nnology, and the memory hierarchy is easily exposed: the data and instruction L1 private\n\ncaches for each core, it continues with an L2 private cache, also for each core, and the last\n\nlevel cache (L3) is a shared memory for all cores in the same socket.\n\nFigure 2.2: Architeture of Viking multicore machine presented in Table 2.1.\nMachine (24GB total)\n\nNUMANode P#0 (12GB)\n\nPackage P#1\n\nL3 (8192KB)\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#0\n\nPU P#0\n\nPU P#8\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#1\n\nPU P#2\n\nPU P#10\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#2\n\nPU P#4\n\nPU P#12\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#3\n\nPU P#6\n\nPU P#14\n\nNUMANode P#1 (12GB)\n\nPackage P#0\n\nL3 (8192KB)\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#0\n\nPU P#1\n\nPU P#9\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#1\n\nPU P#3\n\nPU P#11\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#2\n\nPU P#5\n\nPU P#13\n\nL2 (256KB)\n\nL1d (32KB)\n\nL1i (32KB)\n\nCore P#3\n\nPU P#7\n\nPU P#15\n\nSource: The author\n\n\n\n34\n\n2.6 Concluding remarks\n\nIn this chapter we presented the basis of common multicore architectures and how\n\nthey are evolved in complex systems with many components, and how there is a constant\n\nincrease in parallelism. In this context, programming models have to take into account\n\nthe particular characteristics of different architectures to improve the performance. Thus,\n\nprogramming of hardware devices and scalability of software need to be optimized to\n\nreach the theoretically available performance.\n\nWith the rising of multicore architectures for several application domains, it is\n\nimportant to understand the common characteristics among all platforms (processing ele-\n\nments, memory hierarchy, and power/performance rates). However, as long as there is no\n\nconvergence towards unified programming approaches applicable to a variety of different\n\narchitectures, efforts into general solutions are a challenge. The solution for performance\n\nimprovement is frequently one particular application for one detailed architecture. More-\n\nover, although current compilers can deal with parallelism, freeing the programmer from\n\nthe task of parallelizing and orchestrating is quite difficult to obtain the optimal process-\n\ning, much of the responsibility is still put on the programmer.\n\nTrending of current efforts may be focused on multicore aware algorithms, mul-\n\nticore enabled libraries, and multicore capable tools. The goal is not to design isolated\n\nsolutions for particular configurations but to develop methodologies and concepts that\n\napply to a wide range of problem classes and architectures. It is important for multi-\n\ncore systems to be able to satisfy the different computing requirements of a large fraction\n\nof users with multicore resources. To predict and to improve the performance on these\n\nsystems depend on the underling architecture.\n\nThe research is necessary for exploring new models of performance optimization\n\nthat are well adapted to the prerequisites of hardware and programming models. The\n\napproach of hardware aware computing is trying to find a balance between programmer\n\ncapabilities from best performing implementations and the compiler based optimizations.\n\nCurrently, restructuring of algorithms is needed to optimize the performance and to mini-\n\nmize the execution time.\n\n\n\n35\n\n3 NUMERICAL BACKGROUND: GEOPHYSICAL KERNELS ON MULTICORE\n\nPLATFORMS\n\nIn this chapter, we present the geophysics numerical models. Because of its sim-\n\nplicity, the Finite-Difference Method (FDM) is widely used to design the geophysics mod-\n\nels, when discretizing Partial Differential Equations (PDE). From the numerical analysis\n\npoint of view, the FDM computational procedure consists in using the neighboring points\n\nin horizontal, vertical or diagonal directions to calculate the current point.\n\n3.1 Stencil applications\n\nIn the case of a 3D Cartesian grid, the computational procedure consists in using\n\nthe neighboring points in the north-south, east-west and forward-backward directions to\n\nevaluate the current grid point. The stencil sweep can be expressed as iterative time do-\n\nmain (represented by the first loop controlled by n_times variable), and a triply nested\n\nparallel loop presented in Algorithm 1. The algorithm then moves to the next point ap-\n\nplying the same stencil computation until the entire spatial grid have been traversed, and\n\nthe time domain is completed. The number of points used in each direction depends on\n\nthe order of the approximation and is of great importance for the overall performance.\n\nAlgorithm 1: Pseudocode for stencil algorithms\n1: for t = 1 to n_times do\n\n2: compute in parallel\n\n3: for i = 1 to SIZE_X_direction do\n\n4: for j = 1 to SIZE_Y _direction do\n\n5: for k = 1 to SIZE_Z_direction do\n\n6: compute stencil(3D tile)\n\n7: end for\n\n8: end for\n\n9: end for\n\n10: end for\n\n\n\n36\n\n3.1.1 7-point Jacobi stencil\n\nThe 7-point Jacobi stencil is a reference example of numerical kernel used in vari-\n\nous context in order to evaluate the impact of advanced reformulation or the impact of the\n\nunderlying architecture. This numerical kernel can be described as a proxy of complex\n\nstencils like those corresponding to geophysical applications. A review can be found in\n\n(DATTA et al., 2010). This stencil model also corresponds to the standard discretization\n\nof the elliptic 3D Heat equation 3.1.\n\nBi,j,k =?Ai,j,k + ?(Ai?1,j,k + Ai,j?1,k + Ai,j,k?1 + Ai+1,j,k + Ai,j+1,k + Ai,j,k+1) (3.1)\n\nRepresentation of stencil size is presented in Figure 3.1. Calculation of this nu-\n\nmerical equation needs 7 values, one from current point plus 6 from neighbor points (one\n\nprevious and one next on 3D axes).\n\nFigure 3.1: Size of 7-point Jacobi stencil and its neighbor points.\n\nxy\nz\n\nSource: (NGUYEN et al., 2010)\n\nA standard metric available to characterize a stencil kernel is the Arithmetic In-\n\ntensity (AI) that can be defined as the ratio between the floating point operations and the\n\nmemory transfers. In the case of the Seven-point Jacobi kernel, the lower-bound of the\n\narithmetic intensity is 0.18. A synthetic pseudo-code of this kernel could be found in\n\nAlgorithm 2.\n\n\n\n37\n\nAlgorithm 2: Pseudo-code for the Seven-point Jacobi stencil.\nfor i = 1 to Nx do\n\nfor j = 1 to Ny do\n\nfor k = 1 to Nz do\n\nXn+1(i,j,k) = Xn(i,j,k) + Xn(i,j,k + 1) + Xn(i,j,k ? 1)\n\n+ Xn(i,j + 1,k) + Xn(i,j ? 1,k)\n\n+ Xn(i + 1,j,k) + Xn(i? 1,j,k)\n\nend for\n\nend for\n\nend for\n\n3.1.2 Seismic wave propagation stencil\n\nEvaluation of damages occurred during strong ground motion is critical for urban\n\nplanning. The seismic wave equation waves radiated from an earthquake are often simu-\n\nlated under the assumption of an elastic medium although the waves attenuate due to some\n\nanelasticity. If it considers a 3D isotropic elastic medium, the seismic wave equation is\n\ngiven by equation 3.2:\n\n?\n?vi\n?t\n\n=\n??ij\n?j\n\n+ Fi (3.2)\n\nThe discretization of previous equation using a finite-difference method gives the\n\nfollowing system of equations:\n\n??????\n?????\n? ?\n?t\nvx =\n\n?\n?x\n?xx +\n\n?\n?y\n?xy +\n\n?\n?z\n?xz + fx\n\n? ?\n?t\nvy =\n\n?\n?x\n?yx +\n\n?\n?y\n?yy +\n\n?\n?z\n?yz + fy\n\n? ?\n?t\nvz =\n\n?\n?x\n?zx +\n\n?\n?y\n?zy +\n\n?\n?z\n?zz + fz\n\n(3.3)\n\nAdditionally, the constitutive relation in the case of an isotropic medium is pre-\n\nsented in equation 3.4.\n\n??ij\n?t\n\n= ??ij\n\n(\n?vx\n?x\n\n+\n?vy\n?y\n\n+\n?vz\n?z\n\n)\n+ \u00b5\n\n(\n?vi\n?j\n\n+\n?vj\n?i\n\n)\n(3.4)\n\n\n\n38\n\n?????????????????\n????????????????\n\n?\n?t\n?xx = ?\n\n(\n?\n?x\nvx +\n\n?\n?y\nvy +\n\n?\n?z\nvz\n\n)\n+ 2\u00b5 ?\n\n?x\nvx\n\n?\n?t\n?yy = ?\n\n(\n?\n?x\nvx +\n\n?\n?y\nvy +\n\n?\n?z\nvz\n\n)\n+ 2\u00b5 ?\n\n?y\nvy\n\n?\n?t\n?zz = ?\n\n(\n?\n?x\nvx +\n\n?\n?y\nvy +\n\n?\n?z\nvz\n\n)\n+ 2\u00b5 ?\n\n?z\nvz\n\n?\n?t\n?xy = \u00b5\n\n(\n?\n?y\nvx +\n\n?\n?x\nvy\n\n)\n?\n?t\n?xz = \u00b5\n\n(\n?\n?z\nvx +\n\n?\n?x\nvz\n)\n\n?\n?t\n?yz = \u00b5\n\n(\n?\n?z\nvy +\n\n?\n?y\nvz\n\n)\n(3.5)\n\nIn the previous equations 3.3 and 3.5, v and ? represent the velocity and the stress\n\nfield respectively and f denotes a known external source force. The medium is character-\n\nized by the elastic (Lam\u00e9) parameters ? and \u00b5 and ? is the density. A time derivative is\n\ndenoted by ?\n?t\n\nand a spatial derivative with respect to the i-th direction is represented by\n?\n?i\n\n. The Kronecker symbol ?ij is equal to 1 if i = j and zero otherwise. Exponents i, j, k\n\nindicate the spatial direction with ?ijk = ?(i?s,j?s,k?s), ?s corresponds to the space\n\nstep and ?t to the time step.\n\nDue to its simplicity, the finite-difference method is widely used to compute the\n\npropagation of seismic waves. The numerical kernel under study relies on the classical\n\n4-th order in space and second-order in time approximation (VIRIEUX, 1986; MOCZO;\n\nROBERTSSON; EISNER, 2007). Considering the classical 4-th order in space and second-\n\norder in time approximation, the stencil applied for the computation of the velocity com-\n\nponent in the x-direction is given by equation 3.6. The same numerical scheme is used to\n\ncompute the stress components.\n\nv\n(l+ 1\n\n2\n)jk\n\nx\n\n(\nl +\n\n1\n\n2\n\n)\n= v\n\n(i+ 12 )jk\nx\n\n(\ni?\n\n1\n\n2\n\n)\n+ a1F\n\n(i+ 12 )jk\nx\n\n+ a2\n\n[\n?\n(i+1)jk\nxx ??\n\nijk\nxx\n\n?x\n+\n\n?\n(i+ 12 )(j+ 12 )k\nxy ??\n\n(i+ 12 )(j? 12 )k\nxy\n\n?y\n+ ?\n\n(i+ 12 )j(k+ 12 )\nxz ??\n\n(i+ 12 )j(k? 12 )\nxz\n\n?z\n\n]\n\n?a3\n\n[\n?\n(i+2)jk\nxx ??\n\n(i?1)jk\nxx\n\n?x\n+\n\n?\n(i+ 12 )(j+ 32 )k\nxy ??\n\n(i+ 12 )(j? 32 )k\nxy\n\n?y\n+ ?\n\n(i+ 12 )j(k+ 32 )\nxz ??\n\n(i+ 12 )j(k? 32 )\nxz\n\n?z\n\n]\n(3.6)\n\nFigure 3.2 illustrates the size of the seismic stencil. In this case, the stencil needs\n\n13 values, one from current point plus 12 from neighbor points (2 previous and 2 next on\n\n3D axes).\n\n\n\n39\n\nFigure 3.2: Size of seismic stencil to calculate velocity and stress components.\n\nSource: (MICH\u00c9A; KOMATITSCH, 2010)\n\nThe governing equations associated with the three-dimensional modeling of seis-\n\nmic wave propagation in elastic media are implemented by finite-difference discretization\n\nfor x86 cores and for GPU platforms corresponding to Ondes3D application developed\n\nby the French Geological Survey (BRGM). Algorithm 3 provides an overview of the com-\n\nputational flowchart. The stress and velocity components are evaluated following an odd-\n\neven dependency (i.e.the computation of the stress field reuses the results of the update of\n\nthe velocity field).\n\nAlgorithm 3: Pseudo-code of the stress component (?xx) in the seismic\n\nwave kernel.\nfor i = 1 to Nx do\n\nfor j = 1 to Ny do\n\nfor k = 1 to Nz do\n\n?n+1xx (i,j,k) = ?\nn\nxx(i,j,k)\n\n+ A1[a1(V\nn\nx (i +\n\n1\n2\n,j,k) ?V nx (i?\n\n1\n2\n,j,k))\n\n+ a2(V\nn\ny (i,j +\n\n1\n2\n,k) ?V ny (i,j ?\n\n1\n2\n,k))\n\n+ a3(V\nn\nz (i,j,k +\n\n1\n2\n) ?V nz (i,j,k ?\n\n1\n2\n))]\n\n+ B1[a1(V\nn\nx (i +\n\n3\n2\n,j,k) ?V nx (i?\n\n3\n2\n,j,k))\n\n+ a2(V\nn\ny (i,j +\n\n3\n2\n,k) ?V ny (i,j ?\n\n3\n2\n,k))\n\n+ a3(V\nn\nz (i,j,k +\n\n3\n2\n) ?V nz (i,j,k ?\n\n3\n2\n))]\n\nend for\n\nend for\n\nend for\n\n\n\n40\n\n3.1.3 Acoustic wave propagation stencil\n\nAcoustic wave propagation approximation is the current backbone for seismic\n\nimaging tools. It has been extensively applied for imaging potential oil and gas reser-\n\nvoirs beneath salt domes. The problem to simulate the propagation of a single wavelet\n\nover time is solved by the isotropic acoustic wave propagation (Equation 3.7), and the\n\nisotropic acoustic wave propagation with variable density (Equation 3.8) under Dirich-\n\nlet boundary conditions over a finite three-dimensional rectangular domain, prescribing\n\np = 0 to all boundaries, where p(x,y,z,t) is the acoustic pressure, V (x,y,z) is the prop-\n\nagation speed and ?(x,y,z) is the media density.\n\n1\n\nV 2\n.\n?2p\n\n?t2\n= ?2p (3.7)\n\n1\n\nV 2\n.\n?2p\n\n?t2\n= ?2p?\n\n??\n?\n.?p (3.8)\n\nThe Laplace Operator is discretized by a 12th order finite differences approxima-\n\ntion on each spatial dimension. The derivatives are approximated by a 2nd finite differ-\n\nences operator. Propagation speed depends on variable density, the acoustic pressure,\n\nand the media density. Numerical solution is detailed in (VILELA, 2017). Figure 3.3\n\nillustrates the size of the acoustic wave propagation stencil, it needs 19 values, one from\n\ncurrent point plus 18 from neighbor points (3 previous and 3 next on 3D axes)\n\nFigure 3.3: Size of acoustic wave propagation stencil and its neighbor points.\nPzz\n\nPxx\n\nPyy\n\nSource: (VILELA, 2017)\n\nThe numerical method is solved by Algorithm 4; and Petrobras, the leading Brazil-\n\nian oil company, provides a standalone mini-app of the numerical method. The code was\n\nwritten in standard C and leverage from OpenMP directives for shared-memory paral-\n\nlelism. But Indeed, the parallelization strategy relies on the decomposition of the three-\n\ndimensional domain based on OpenMP loop features.\n\n\n\n41\n\nAlgorithm 4: Pseudo-code of the acousitc wave propagation kernel.\nfor i = 1 to Nx do\n\nfor j = 1 to Ny do\n\nfor k = 1 to Nz do\n\nCi,j,k = a0Ci,j,k\n\n+ a1(Ci?1,j,k + Ci+1,j,k + Ci,j?1,k + Ci,j+1,k + Ci,j,k?1 + Ci,j,k+1)\n\n+ a2(Ci?2,j,k + Ci+2,j,k + Ci,j?2,k + Ci,j+2,k, + Ci,j,k?2 + Ci,j,k+2)\n\n+ a3(Ci?3,j,k + Ci+3,j,k + Ci,j?3,k + Ci,j+3,k + Ci,j,k?3 + Ci,j,k+3)\n\nend for\n\nend for\n\nend for\n\n3.2 Standard implementations of numerical stencil\n\nIn this section, we present the implementation of stencil algorithms for parallel\n\nplatforms, and we will analyze the performance on common HPC architectures. Classical\n\nimplementations on multicore architecture are related to how the threads solve each point\n\non the space and time domain.\n\n3.2.1 Na\u00efve\n\nOn shared-memory architectures, a popular way to extract the parallelism for such\n\napplications is to exploit the triple nested loops coming from the spatial dimensions of the\n\nproblem under study. This strategy allows straightforward benefits of OpenMP directives.\n\nAdditional optimizations should be considered in order to limit the impact of NUMA\n\narchitectures (DUPROS et al., 2008).\n\n3.2.2 Space Tiling\n\nThe second algorithm uses the cache blocking technique (DUPROS; DO; AOCHI,\n\n2013). The standard simulation algorithm typically scans an array spanning several times\n\nthe size of the cache using each retrieved grid point only for a few operations. There-\n\n\n\n42\n\nfore, the cost to bring the needed data from the main memory to the fast local cache\n\nmemories accounts for an important share of the total simulation time, especially for a\n\nthree-dimensional problem. In this case, the main idea is to exploit the inherent data\n\nreuse available in the triple nested loop of the kernel by ensuring that data remains in\n\ncache across multiple uses, this is achieved by creating blocks to improve data locality.\n\nDependency between blocks is exploited to implement a space-time decomposition.\n\nThe advantage of space tiling algorithm is to improve the computational intensity\n\nby keeping a relatively small amount of data in cache memory and by performing many\n\nmore floating-point operations on them. Figure 3.4 shows the solution space of each\n\nalgorithm. Each color represents a thread when solving one point or a set of points.\n\nFigure 3.4: Representation of solution by thread, for naive and space tiling algorithms.\n\n(a) Naive: one point by thread (b) Space tiling: a group of points by\nthread.\n\nSource: The Author\n\n3.3 Performance charaterization of numerical stencils\n\nIn order to understand the performance of numerical kernels, we used two multi-\n\ncore architectures to run a set of experiments for the 7-point Jacobi stencil by varying the\n\navailable runtime parameters in OpenMP. The machines are NUMA platforms. Configu-\n\nrations of testbed (nodes Viking and Turing) have been listed in Table 2.1 from Chapter\n\n2. A configuration runtime input vector was created and its corresponding performance\n\noutput vector was obtained, for each stencil experiment. Parameters for input vector are\n\nlisted in Table 3.1. Common measures to performance characterization are executing time\n\nand speedup, defined as a ratio of the time of a given program on a single core processor\n\nover the performance obtained in a multicore architecture (KRISHNAN; VEERAVALLI,\n\n2014).\n\n\n\n43\n\nTable 3.1: Measures for the parameters of input vector.\n\nTotal configurations\n\nViking Turing\n\nThread counting 3 6\n\nProblem size 3 3\n\nParallel looping 2 2\n\nScheduling 3 3\n\nChunk size 4 4\n\nAlgorithm 2 2\n\nTotal 432 864\n\nThe input parameters are related to code optimization and execution runtime. The\n\nfirst parameter is the thread counting, and it is determined by the OMP_NUM_THREADS\n\nvariable. Second, the problem size determines the memory size and the total of compu-\n\ntations, we used three sizes labeled as small, medium, and large. Next, related to code\n\noptimization, we used one parameter for parallel looping, we implemented the #pragma\n\nomp parallel for directive to parallelize the triple nested loops with collapse\n\noption, and the #pragma omp task directive, in the second for without collapse.\n\nLater, one parameter for scheduling policy used by OpenMP (Static, Guided and Dy-\n\nnamic) and one parameter for the chunk size configured by the OMP_SCHEDULE variable.\n\nFinally, we used the two standard implementations (Naive and Space Tiling) described in\n\nsection 3.2. The performance vector is determined with following values:\n\n\u2022 Time, which corresponds to the total execution time to solve the stencil.\n\n\u2022 Speedup, it was calculated as the ratio between the execution time on the multicore\n\nmachine and the single core execution time on Turing machine as the reference\n\nvalue.\n\n\u2022 GFLOPS, it was calculated from the execution time, the 3D domain size and the\n\nstencil size.\n\n\u2022 Total cache misses L3, it was obtained through PAPI_L3_TCM event.\n\n\u2022 Total cache access L3, it was obtained through PAPI_L3_TCA event.\n\nEach one of experiments was executed 15 times to compute the average, and the\n\n\n\n44\n\nShapiro-Wilk test to time measurement was performed to confirm normality, and we ob-\n\ntained that all data were normally distributed.\n\n3.3.1 Scalability\n\nRelated to the scalability of stencil applications, we measured the execution time\n\nfor different values of threads on each machine. The results are presented in Figure 3.5\n\nand show an expected behavior: when the number of threads (on the same machine)\n\nis increased the performance also increases. But, the performance improvement is not\n\noptimal. The speedup is under-linear and not correspondent with the number of threads.\n\nFigure 3.5: Impact of performance measures by number of threads.\n\n0\n\n50\n\n100\n\n150\n\n200\n\nViking Turing\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nScaling 2 4 8 16 32 64\n\n(a) Execution time by machine\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n2 4 8 16 32 64\nThreads\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nMachine Viking Turing\n\n(b) Speedup by machine\n\nSource: The Author\n\nProblem size\n\nTo confirm the assumption that runtime parameters affect the performance, we also\n\nanalyzed three different values for the 3D problem size (it represents small, medium and\n\nlarge memory usage), because this parameter is associated with memory consumption.\n\nThe maximum number of threads was considered on each node. Figure 3.6 illustrates the\n\nperformance by 3D Cartesian grid size.\n\nWe noted when the small and the medium problems are executed on the simplest\n\nmachine the speedup is better. On the contrary when the large problem is executed on\n\nthe more complex machine (Turing) speedup is better for both the problem size and the\n\nmulticore architecture, as it has be shown in Figure 3.6b.\n\n\n\n45\n\nFigure 3.6: Impact of performance measures by problem size.\n\n0\n\n100\n\n200\n\n300\n\nViking Turing\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nSize Small Medium Large\n\n(a) Execution time by machine\n\n0.0\n\n2.5\n\n5.0\n\n7.5\n\n10.0\n\nViking Turing\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nSize Small Medium Large\n\n(b) Maximum speedup by machine\n\nSource: The Author\n\nParallel loop vs tasking\n\nThe 3D stencil is calculated by three for loops, as explained in section 3.1. Then\n\nwe compare two possible parallel implementations relevant to code optimization: i) by\n\ncollapsing all for loops and performed the parallelization with #pragma omp for\n\ncollapse(3); and (ii) by using #pragma omp task in the second for to create\n\nparallel tasks and a taskwait directive at the end of this for. Figure 3.7 presents the\n\nresults for each node with the maximum number of available threads in each platform. As\n\nit can be observed, the parallel for implementation achieved better performance\n\nthan tasking. The main reason is that it creates a lot of parallel tasks and each thread has\n\nto solve a set of task and to wait and to synchronize them with each other as a result of\n\ntaskwait clause.\n\nFigure 3.7: Impact of performance measures by code optimization.\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\nViking Turing\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nLooping Collapse Tasking\n\n(a) Execution time by machine\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nViking Turing\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nLooping Collapse Tasking\n\n(b) Maximum speedup by machine\n\nSource: The Author\n\n\n\n46\n\nTo confirm this fact, the execution was traced with pajeNG1 and it was found\n\nthat task implementation takes more time to synchronize all the threads than for\n\ncollapse implementation.\n\nFigure 3.8: Traces of one-time iteration for 7-point stencil execution.\n\n0\n\n20\n\n40\n\n60\n\n0 250 500 750 1000\nTime\n\nT\nh\n\nre\na\n\nd\ns\n\nProcess Main OMP parallel OMP parallel for OMP implicit barrier\n\n(a) #pragma omp for collapse(3)\n\n0\n\n20\n\n40\n\n60\n\n0 250 500 750 1000\nTime\n\nT\nh\n\nre\na\n\nd\ns\n\nProcess\nMain\n\nOMP parallel\n\nOMP parallel for\n\nOMP create task\n\nOMP task\n\nOMP implicit barrier\n\nOMP taskwait\n\n(b) #pragma omp task\n\nSource: The Author\n\n1https://github.com/schnorr/pajeng\n\n\n\n47\n\nFigure 3.8 shows the traces of threads (y axis) in the time domain (x axis), at first\n\niteration represented by the n_times variable in Algorithm 1. The traces measured in\n\na execution of for implementation show that OMP parallel for distribute all the\n\ncomputations between the available threads, and calculate the nested loops in a parallel\n\nway. To calculate the next iteration in the time domain, the threads have to wait in the\n\nOMP implicit barrier to complete the 3D space domain. In correspondent, the\n\ntraces measured in task implementation show a different behavior. The threads solve\n\nthe computations by segmented calculations assigned by OMP task. Some threads stay\n\nall time in a OMP Task wait process, or when threads complete the computations they\n\nenter in this waiting process. Later, when the time iteration finishes, the threads have to\n\nwait in the OMP implicit barrier. We can see that threads in task take more\n\ntime in waiting than for implementation. We also noted that one-time iteration for all\n\nthe 3D space is solved near to 750 seconds, in the for implementation; at this time, the\n\ntask implementation is just starting the threads synchronization.\n\nScheduling policies\n\nLoop scheduling on OpenMP is defined by two parameters of OMP_SCHEDULE\n\nvariable: policy and chunk size (INTEL, 2016). Analyzing how these parameters influ-\n\nence the overall performance is important to reach optimal performance in complex archi-\n\ntectures. As it was mentioned in section 2.3.2, the first parameter of loop scheduling is the\n\npolicy. We used the available strategies in OpenMP: Dynamic, Guided and Static. Figure\n\n3.9 presents the impact of scheduling policies on the Turing machine. In consequence,\n\nwe found that policy does improve the performance of more complex platforms.\n\n\n\n48\n\nFigure 3.9: Impact of performance measures by scheduling policy.\n\n0\n\n40\n\n80\n\n120\n\nViking Turing\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nScheduling Static Dynamic Guided\n\n(a) Execution time by machine.\n\n0\n\n10\n\n20\n\n30\n\nViking Turing\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nScheduling Static Dynamic Guided\n\n(b) Maximum speedup by machine.\n\nSource: The Author\n\nChunk size\n\nThe second parameter in the OMP_SCHEDULE variable is the chunk size, it de-\n\nfines the loop iterations to be assigned to each thread. Our minimum value correspond to\n\nthe default value for Static scheduling, and the maximum value of chunk size correspond\n\nto the size of the 3D Cartesian grid. In Figure 3.10, we can observe two aspects: because\n\nthe standard deviation of time in the experiments, we can say that chunk size is not signif-\n\nicant. But, actually, a small chunk size has more overhead because there are more threads\n\nsolving the stencil. In the same context, if we have a chunk size quite similar to problem\n\nsize the stencil is solved by few threads. The performance would be affected by these\n\nconditions. In terms of scheduling policy, we can see that Dynamic is more affected if the\n\nchunk is undersized.\n\n\n\n49\n\nFigure 3.10: Impact of performance measures by scheduling and chunk size on Turing\nmachine.\n\n0\n\n20\n\n40\n\n60\n\nStatic Dynamic Guided\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nChunk 32 128 256 512\n\n(a) Execution Time.\n\n0\n\n10\n\n20\n\n30\n\nStatic Dynamic Guided\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nChunk 32 128 256 512\n\n(b) Maximum speedup by chunk size.\n\nSource: The Author\n\nAlgorithms\n\nThen, we analyzed the impact of algorithm implementations on the performance.\n\nEach algorithm presents a different performance for each machine. Results are presented\n\nin Figure 3.11, it shows that best performance on node Viking is achieved with Space\n\ntiling algorithm whereas on node Turing the naive algorithm achieves the best perfor-\n\nmance.\n\nFigure 3.11: Impact of performance measures by stencil algorithm.\n\n0\n\n50\n\n100\n\n150\n\n200\n\nViking Turing\n\nT\nim\n\ne\n (\n\nS\ne\n\nc)\n\nAlgorithm Naive Space tiling\n\n(a) Time by machine\n\n0\n\n10\n\n20\n\n30\n\n40\n\nViking Turing\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nAlgorithm Naive Space tiling\n\n(b) Maximum speedup by machine\n\nSource: The Author\n\n\n\n50\n\nPerformance fitting and cache misses predictors\n\nIn this section, we present a previous statistical analysis and fitting of data. This\n\nwould be useful to construct predictors for some parameters. Performance and cache\n\nmisses are compared for all experiments. As is noticed in Figure 3.12, if we compare L3\n\ncache misses with execution time, some input parameters tend to create separable groups\n\nin the graphical representation. Actually, this condition is clearly presented in Figures\n\n3.12a, and 3.12d; instead, in Figures 3.12b and 3.12c the separation by their values is\n\nquite difficult.\n\nFigure 3.12: Performance vs cache misses by input parameters in Turing machine\n\n10\n\n1000\n\n1e+04 1e+06 1e+08 1e+10\nL3 cache misses\n\nT\nim\n\ne\n\n128 256 512\n\n(a) Problem size\n\n10\n\n1000\n\n1e+04 1e+06 1e+08 1e+10\nL3 cache misses\n\nT\nim\n\ne\n\nDynamic Guided Static\n\n(b) Scheduling\n\n10\n\n1000\n\n1e+04 1e+06 1e+08 1e+10\nL3 cache misses\n\nT\nim\n\ne\n\n32 128 256 512\n\n(c) Chunk size\n\n10\n\n1000\n\n1e+04 1e+06 1e+08 1e+10\nL3 cache misses\n\nT\nim\n\ne\n\nSpace tiling Naive\n\n(d) Algorithm\n\nSource: The Author\n\nLinear fitting of L3 cache\n\nNaive algorithm offers poor cache reuse. Thus, understanding and predicting\n\ncache behavior would help to optimize the algorithm performance. For each machine\n\n\n\n51\n\nand according to the problem size we found a linear correlation between cache misses\n\nand cache access. The correlation coefficient (Table 3.2) was calculated and confirmed a\n\nlinear dependency between the number of L3 cache misses and cache accesses.\n\nTable 3.2: Correlation coefficient of cache access vs cache misses\n\nProblem size Small Medium Large\n\nViking 0.9984507 0.9997246 0.9999551\n\nTuring 0.9716369 0.9995467 0.9969517\n\nTherefore a linear fitting by least squares was built to predict L3 cache behavior.\n\nFigure 3.13 shows this adjust.\n\nFigure 3.13: Linear fitting of cache memory by problem size and machine\n\n0e+00\n\n1e+06\n\n2e+06\n\n3e+06\n\n4e+06\n\n5e+06\n\n0e+00 2e+08 4e+08 6e+08\nCache access\n\nC\na\n\nch\ne\n\n m\nis\n\nse\ns\n\nMachine Viking Turing\n\n(a) Small size.\n\n0.0e+00\n\n5.0e+07\n\n1.0e+08\n\n1.5e+08\n\n0e+00 5e+09 1e+10\nCache access\n\nC\na\n\nch\ne\n\n m\nis\n\nse\ns\n\nMachine Viking Turing\n\n(b) Medium size.\n\n0.0e+00\n\n5.0e+09\n\n1.0e+10\n\n1.5e+10\n\n2.0e+10\n\n0.0e+00 4.0e+10 8.0e+10 1.2e+11\nCache access\n\nC\na\n\nch\ne\n\n m\nis\n\nse\ns\n\nMachine Viking Turing\n\n(c) Large size.\n\nSource: The Author\n\nWe used the Root Mean Squared Error (RMSE), the Standard Deviation (SD),\n\nand the Coefficient of Determination (R-SQUARE) to describe the accuracy of our fitting\n\nmodel. Results are calculated in Table 3.3. If we compare RMSE and SD we find that\n\nerror is quite lower than the standard deviation, thus it shows that good predictions can\n\n\n\n52\n\nbe made by linear fitting. To confirm this fact, the R-SQUARE presents accuracy of\n\nprediction near to 99%; then our fitting model of cache memory helps to predict the cache\n\naccess.\n\nTable 3.3: RMSE, SD and R-SQUARE of cache fitting\nRMSE SD R-SQUARE\n\nProblem size Small Medium Large Small Medium Large Small Medium Large\n\nViking 96207.09 1322882 58487888 1761918 57443329 6291905304 0.9968 0.9994 0.9999\n\nTuring 222768 979262.6 73942621 950870.9 32830986 956618380 0.9407 0.9990 0.9938\n\nExponential fitting of performance\n\nAs performance (Gflops) depends on the execution time, the idea in this section is\n\nto build a predictor for this parameter. We used the spline fitting because an exponential\n\ndependency was found. Figure 3.14 illustrates the fitting for the na\u00efve and space tiling\n\nalgorithm.\n\nFigure 3.14: Exponential fitting for problem size\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0 10 20 30 40 50\nTime (s)\n\nG\nflo\n\np\ns\n\nAlgorithm Space tiling Naive\n\n(a) Small size.\n\n0.0\n\n2.5\n\n5.0\n\n7.5\n\n10.0\n\n12.5\n\n0 50 100 150\nTime (s)\n\nG\nflo\n\np\ns\n\nAlgorithm Space tiling Naive\n\n(b) Medium size.\n\n0\n\n5\n\n10\n\n15\n\n0 500 1000 1500\nTime (s)\n\nG\nflo\n\np\ns\n\nAlgorithm Space tiling Naive\n\n(c) Large size.\n\nSource: The Author\n\n\n\n53\n\nTable 3.4 presents RMSE, SD and R-SQUARE for exponential fitting. In the same\n\nway that linear fitting, measures of statistical estimators show that a good performance\n\npredictor for Gflops and execution time was found.\n\nTable 3.4: RMSE, SD and R-SQUARE of exponential fitting\nRMSE SD R-SQUARE\n\nProblem size Small Medium Large Small Medium Large Small Medium Large\n\nNaive 0.0004 0.0032 0.0014 1.1797 2.9209 3.5935 0.9999 0.9999 0.9999\n\nSpace tiling 0.0004 0.0002 0.0003 0.5369 0.8307 0.7338 0.9999 0.9999 0.9999\n\n3.4 Concluding remarks\n\nIn this chapter, we presented the stencil applications, the basis of geophysics nu-\n\nmerical kernels, and their common implementations. Shared-memory programming is\n\nthe most common model for this kind of applications. We studied the influence of sev-\n\neral input configurations for runtime (problem size, threads, looping, scheduling policy,\n\nchunk size and algorithm implementation) to the performance on multicore architectures.\n\nIt was observed that two known algorithms (naive and space tiling) may present different\n\nperformance in several scenarios.\n\nFor more complex architectures, input parameters like chunk size and scheduling\n\nalgorithms play an important role and can contribute to achieving a peak of performance\n\nwhen threads do not perform intensive data communications (MARTINEZ et al., 2016).\n\nIt was possible to tune the stencil execution until to reach a peak of performance, with an\n\nacceleration up to 23 and 34 times (for naive and space tiling algorithm respectively) com-\n\npared to the sequential solution. Moreover, we also observed that tasks looping achieves\n\nbetter performance when the algorithm does not use cache intensively (space-time tiling)\n\non architectures with few cores.\n\nFinally, some of these parameters can be predicted by simple and common fitting.\n\nFor example, the number of cache misses can be approximated by linear fitting depending\n\non the number of cache access; whereas the performance in Gflops could be predicted by\n\nspline exponential fitting of execution time. The accuracy of statistical estimators shows\n\na good precision and this prediction could help to make performance predictors more\n\ncomplex.\n\n\n\n54\n\n4 MACHINE LEARNING STRATEGY FOR PERFORMANCE IMPROVEMENT\n\nON MULTICORE ARCHITECTURES\n\nIn this Chapter, a model to optimize the input configuration set at runtime to pre-\n\ndict and to improve the performance of geophysics numerical kernels on multicore archi-\n\ntectures is presented. First, we introduce how to improve the performance based on new\n\napproaches; second, we propose the general model for performance prediction; third, we\n\npresent an implementation of the model applied on two multicore architectures and two\n\ndifferent algorithm implementations; fourth, the characteristics of model for manycore\n\narchitectures are described and implemented to three stencil applications. Finally, we\n\ndiscuss how this model could be used to improve the performance.\n\n4.1 Performance improvement by Machine Learning models\n\nThe term Machine Learning (ML) was coined by (SAMUEL, 1959), the main\n\nidea was to program computers to learn from experience using the game of checkers\n\nthrough spanning the possible solutions and to select the better way by scoring the results.\n\nApplications of ML algorithms have been developed for various fields: pattern and object\n\nrecognition, text categorization, time-series prediction, bioinformatics, etc.\n\nThe model of learning proposed by ML algorithms can be described by three com-\n\nponents: 1) a generator of random vector x; 2) a supervisor that returns an output vector\n\ny for each input vector x, according to a conditional function P(y|x); and 3) a learning\n\nmachine capable of implementing a set of functions f(x,?). The problem of learning is\n\nthat of choosing from a set of functions f(x,?) the one which predicts the supervisors\u2019\n\nresponse in the best possible way (VAPNIK, 1999).\n\nML algorithms can be classified by the following two tasks: 1) supervised al-\n\ngorithms in which the input vector is identified to a predefined output, 2) unsupervised\n\nalgorithms (e.g., clustering) in which the input vector is assigned to an unknown class or\n\nresponse. The pattern recognition problem is posed as a classification or categorization\n\ntask, where the classes are either defined by the system designer (in supervised classifi-\n\ncation) or are learned based on the similarity of patterns (in unsupervised classification)\n\n(JAIN; DUIN; MAO, 2000; XU; WUNSCH, 2005).\n\nKernel-based algorithms are common ML methods. They are used for both classi-\n\n\n\n55\n\nfication and regression. The task of classification is to find a rule, which, based on external\n\nobservations, assigns an object to one of several classes, it is made by a hyperplane cre-\n\nated by one function. When the kernel function approximates another unknown function\n\nwe obtain the solution for regression problems. The goal of learning is to find the kernel\n\nfunction that minimizes the error between observed values and values obtained by the ML\n\nalgorithm (MULLER et al., 2001).\n\nRecently, ML algorithms have been used on HPC systems in different situations.\n\nIn (WANG; O\u2019BOYLE, 2009) the authors presented ML-based predictors to map paral-\n\nlelism to multicores. They considered several different parameters such as the number\n\nof threads and scheduling policies in OpenMP programs. In (VLADUIC; CERNIVEC;\n\nSLIVNIK, 2009) the authors used ML algorithms to select the best job scheduling algo-\n\nrithm on heterogeneous platforms whereas in (CASTRO; G\u00d3ES; M\u00c9HAUT, 2014) the\n\nauthors proposed an ML-based approach to automatically infer a suitable thread mapping\n\nstrategy for a given application. In (BOITO et al., 2016) the authors proposed an ML-\n\nbased scheme to select the best I/O scheduling algorithm for different applications and\n\ninput parameters.\n\n4.2 General model for performance prediction\n\nThe proposed model is based on Support Vector Machine (SVM) method, which\n\nis a kernel-based and supervised approach proposed in (CORTES; VAPNIK, 1995) and\n\nit was extended to regression problems where support vectors are represented by kernel\n\nfunctions (DRUCKER et al., 1997). The main idea of SMV is to expand hyperplanes\n\nthrough the output vector. It has been employed to classify non-linear problems with non-\n\nseparable training data by a decision surface, as we presented in Figure 3.12 from Section\n\n3.3.1. Our model was implemented using e1071 R package (MEYER et al., 2015).\n\n4.2.1 Architecture of geophysics prediction model\n\nThe General Model for Performance Prediction of Geophysics Stencils based on\n\nMachine Learning (Golem) is built on top of three consecutive layers, where the output\n\nvalues of a layer are used as the input values of the next layer. Figure 4.1 shows the\n\nflowchart of this strategy.\n\n\n\n56\n\nThe input layer contains the runtime configuration parameters from the input vec-\n\ntor. The hidden layer contains a set of SVMs and takes the values from input vector to\n\nsimulate the available hardware counters on the HPC architecture, measures for training\n\nstage were taken by PAPI library (MUCCI et al., 1999). Because hardware counters have\n\nvery large values it was necessary to perform a dynamic range compression (log trans-\n\nformation) between the hidden and the output layers (GONZALEZ; WOODS, 2002).\n\nFinally, the output layer contains another set of SVMs and takes each value from hard-\n\nware counters layer to obtain the corresponding predicted performance (GFLOPS, and\n\nexecution time).\n\nFigure 4.1: Flowchart of General Model for Performance Prediction of Geophysics Sten-\ncils based on Machine Learning (Golem).\n\nInput\nVector\n\nHardware\nCounters\n\nSVMs\n\nPerformance\nSVMs\n\nTraining Set\n\nTesting Set\n\nInput Layer Output LayerHidden Layer\n\nLog\n\nPerformance\nValues\n\nOptimization\n(Max/Min)\n\nSearching\n\nSource: The Author\n\nWe use two separated experiment sets for training and testing stages. The train-\n\ning set is used for input, hidden and output layers. Since the model has been trained we\n\nuse an input testing set to predict the performance and we compare the output perfor-\n\nmance between predicted and actual values to obtain the regression accuracy. Finally, we\n\nsearch into these predicted values the maximum of GFLOPS, the minimum of time, and\n\nits corresponding input configuration to optimize the execution time.\n\n\n\n57\n\nStatistical analysis\n\nWe conduct two statistical analysis. First, before training stage and in order to re-\n\nfine the results, we applied the Analysis of Variance (ANOVA) statistical model to analyze\n\nif variables for hardware counters and performance measures have different populations\n\naffected by the input values.\n\nWe assume the hypothesis that different populations for each variable have equal\n\nmean, it is called H0. Thus, we compute the statistical significance (p-value) to determine\n\nwhether the hypothesis H0 must be rejected or not: if this value is lower than 0.05 then the\n\nhypothesis H0 is rejected and populations have different means, that is hardware counters\n\nand performance measures are affected by input parameters with statistical significance.\n\nThis analysis was divided into two classical ANOVA models: one-way ANOVA, when\n\nonly one factor affects all populations; and two-way ANOVA, when two factors affect all\n\npopulations.\n\nThe second statistical analysis is related to the accuracy of the regression model,\n\nwe evaluate the trained model with two statistical estimators: the root mean square error\n\n(RMSE) and the coefficient of determination (R-square). The former represents the stan-\n\ndard deviation of the differences between predicted values and actual values whereas the\n\nlatter represents how close the regression approximates the actual data (R-square equal to\n\n1 indicates a perfect fit of data regression).\n\n4.2.2 Performance prediction on multicore architectures\n\nFigure 4.2 presents the flowchart of Golem approach on multicore architectures,\n\nand we made the experiments with naive and space tiling implementations, explained in\n\nSection 3.2 from Chapter 3, for 7-point Jacobi and seismic wave propagation stencils. We\n\nnote that quantity of available hardware counters depends on each HPC architecture.\n\n\n\n58\n\nFigure 4.2: Flowchart of Golem on multicore architectures.\n\nNumber of threads L3 CM SVM Time SVM\n\nTraining Set\n\nTesting Set\n\nInput Layer Output LayerHidden Layer\n\nLog\n\nMinimum\n\nSearching\n\nChunk size\n\nScheduling policy\n\nBlock size X\n\nBlock size Y Cycles SVM\n\nTLB DM SVM\n\nGFLOPS\n\nFit\n\nMaximum\n\nPerformance\n\nSource: The Author\n\nThe parameters in each layer are described as:\n\n\u2022 Input Layer: Values in the input vector depend on algorithm implementation, and\n\nthey are defined by the runtime parameters listed in Table 4.1. In both algorithms,\n\nOpenMP runtime parameters are considered, such as the number of threads defined\n\nby the OMP_NUM_THREADS environment variable, that will perform the compu-\n\ntation in parallel, loop scheduling policy (Static and Dynamic) and the chunk size\n\ndefined by the OMP_SCHEDULE environment variable. For the space tiling algo-\n\nrithm, the block size in the X and Y domains are also considered.\n\nTable 4.1: Parameters of input vector for each algorithm.\n\nNaive Space tiling\n\nNumber of threads\n\nScheduling policy\n\nChunk size\n\nNumber of threads\n\nScheduling policy\n\nChunk size\n\nBlock size X\n\nBlock size Y\n\n\u2022 Hardware Counters Layer: The metrics considered in this layer depend on the\n\nnumber of available PAPI events. Because stencil computations are a memory-\n\nbound problem, we choose as the most relevant events the L3 total cache misses\n\n\n\n59\n\n(PAPI_L3_TCM), the data translation lookaside buffer misses (PAPI_TLB_DM)\n\nand the total of cycles (PAPI_TOT_CYC).\n\n\u2022 Output Layer: The performance vector contains the measure of the billions of\n\nfloating-point operations per second (GFLOPS) and the execution time to solve de\n\ngeophysics stencil.\n\nTestbed and configuration domain\n\nTwo multicore platforms were used to carry out the experiments, their hardware\n\nconfigurations are shown in Table 2.1 from Chapter 2 (Cryo and Server 185 Nodes).\n\nBased on these platforms, Table 4.2 details all the available configurations for the opti-\n\nmization categories. As it can be observed, a brute force approach would be unfeasible,\n\nrequiring more than 3 millions in simulations for the space tiling algorithm.\n\nTable 4.2: Optimizations set for multicore architectures\n\nOptimization Parameters\nTotal configurations\n\nCryo Server 185\n\nNumber of threads 1 8 12\n\nScheduling policy 1 2 2\n\nChunk size 1 32 32\n\nBlock size X 1 64 64\n\nBlock size Y 1 64 64\n\nTotal for Naive 3 512 768\n\nTotal for Space Tiling 5 2,097,152 3,145,728\n\nAnalysis of variance and hardware counters behavior\n\nIn this analysis, GFLOPS, L3 cache misses, TLB data misses and the number\n\nof cycles were used as population variables and factors are defined by all parameters in\n\nvector input (the number of threads, the scheduling policy, and the chunk size). The\n\nresults of p-value for the ANOVA of GFLOPS variable for naive algorithm are presented\n\nin Table 4.3. As it can be observed, all factors rejected the hypothesis for the 7-point\n\nJacobi, since the p-value is lower than 0.05. In other words, the statistical significance\n\nindicates that variable GFLOPS have different populations.\n\n\n\n60\n\nTable 4.3: p-value of one-way ANOVA for the GFLOPS variable in the naive algorithm\nexperiments.\n\n7-point Jacobi Seismic Wave\n\nScheduling policy 2.58e-16 0.5284\n\nChunk size 1.37e-12 0.9985\n\nNum. of threads&lt;2.2e-16&lt;2.2e-16\n\nNum. of cores&lt;2.2e-16&lt;2.2e-16\n\nFor seismic stencil, the hypothesis cannot be rejected, for scheduling and chunk\n\nsize variables. Then, we also used a two-way ANOVA to determine if combined variables\n\naffect populations. Table 4.4 shows the results that combine scheduling and chunk size,\n\nwith executed and available threads, the p-value rejects the hypothesis, and variables have\n\nthe statistical difference if two factors are combined. Then, analysis of variance introduces\n\nfirst assumption: the input factors (number of cores, number of threads, scheduling policy,\n\nand chunk size) produce statistical significance into selected GFLOPS variable in the\n\nnaive algorithm experiments.\n\nTable 4.4: p-value of two-way ANOVA for the seismic wave kernel.\n\np-value\n\nScheduling policy:Num. of threads&lt;2.2e-16\n\nScheduling policy:Num. of cores 0.4664\n\nChunk:Num. of threads&lt;2.2e-16\n\nChunk:Num. of cores&lt;2.2e-16\n\nWe also performed the one-way ANOVA to data from Space tiling algorithm ex-\n\necutions, for the same variables, and a two-way ANOVA for block size X and block size\n\nY variables. Results are presented in Table 4.5. For these analyses all the p-value were\n\n<0.05; then, the hypothesis is also rejected and the input factors (number of cores, num-\n\nber of threads, scheduling policy, chunk size, block size X and block size Y) affects the\n\nGFLOPS variable in the Space tiling algorithm experiments.\n\n\n\n61\n\nTable 4.5: p-value of one-way and two-way ANOVA for the GFLOPS variable in Space\ntiling algorithm experiments.\n\n7-point Jacobi Seismic Wave\n\nScheduling policy&lt;2.2e-16 4.08e-05\n\nChunk size&lt;2.2e-16&lt;2.2e-16\n\nNum. of threads&lt;2.2e-16&lt;2.2e-16\n\nNum. of cores&lt;2.2e-16&lt;2.2e-16\n\nBlock size X:Block Size Y&lt;2.2e-16&lt;2.2e-16\n\nAbout hardware counters behavior, Figure 4.3 illustrates how the performance\n\nof the 7-point Jacobi kernel is affected by the input variables and their relations with\n\nhardware counters, creating the non-separable training data surfaces.\n\nFigure 4.3: Hardware counter behavior of 7-point Jacobi on Node Cryo.\n\n10\n\n20\n\n30\n\n1e+04 1e+06 1e+08\nL3 cache misses\n\nG\nflo\n\np\ns\n\nScheduling dynamic static\n\n(a) L3 cache misses when varying the\nscheduling policy.\n\n10\n\n20\n\n30\n\n1e+10\n\nG\nflo\n\np\ns\n\nChunk size 0 1 2 3 4 5 6 7\n\n(b) Total cycles when varying the chunk\nsize.\n\n10\n\n20\n\n30\n\n1e+05 1e+07\nTLB data misses\n\nG\nflo\n\np\ns\n\nExecuted threads 2 4 8 12 16\n\n(c) TLB data misses when varying the\nnumber of threads.\n\nSource: (MARTINEZ et al., 2017)\n\nEach point represents one experiment. For instance, Figure 4.3a shows the impact\n\n\n\n62\n\nof the scheduling policy that creates two separated areas when the GFLOPS values are\n\nanalyzed with respect to the L3 cache misses. The same behavior is observed in Figure\n\n4.3b for the chunk size when the GFLOPS values are observed with respect to the total\n\nnumber of cycles. The situation is rather different when the GFLOPS values are related\n\nto the amount of TLB data misses as we can see in 4.3c.\n\nTraining and validation sets\n\nA random training set was created by selecting a subset from the experiments\n\nset with random input values (number of threads, chunk size, scheduling policy, block\n\nsize X and block size Y). For each input configuration, the hardware counters (L3 cache\n\nmisses, data translation lookaside buffer misses and total cycles) and performance values\n\n(GFLOPS and execution time) were measured.\n\nA random testing set was used since all SVMs in both hidden and output layers\n\nare trained to calculate new GFLOPS and execution time values through simulation. After\n\nthat, it was measured the accuracy of the model using statistical estimators. Finally, the\n\nmaximum value of GFLOPS and the minimum value of the execution time are selected\n\nand matched with their input values. Simulated and real values are compared to determine\n\nif simulated best performance is the same as the real best performance. Table 4.6 presents\n\nthe total number of experiments that were performed to obtain the training and validation\n\nsets.\n\nTable 4.6: Total number of experiments.\n\nNaive Space tiling\n\nStencil Set Cryo Server 185 Cryo Server 185\n\n7-point Jacobi\n\nTotal 55 48 44794 49152\n\nTraining 44 38 2355 4054\n\nTesting 11 10 589 1014\n\nSeismic Wave\n\nTotal 264 297 6849 1020\n\nTraining 211 237 2176 371\n\nTesting 53 60 544 93\n\nAccuracy of predictive modeling\n\nAs it can be observed in Table 4.7, the regression model is highly accurate. For\n\nthe naive algorithm, the model presented an accuracy of up to 99.70% and 99.87% for\n\n\n\n63\n\nthe GFLOPS and execution time metrics, respectively. For the space tiling algorithm,\n\non the other hand, the model presented an accuracy of up to 98.22% and 99.71% for\n\nthe GFLOPS and execution time metrics, respectively. These results are similar to the\n\nprediction of multicore architectures presented in (CRUZ; ARAYA-POLO, 2015).\n\nTable 4.7: RMSE and R-square for predicted values of the 7-point Jacobi and the Seismic\nWave kernels.\n\nNaive Space tiling\n\nCryo Server 185 Cryo Server 185\n\n7-point\n\nRMSE\nGflops 0.7941 1.0179 1.4185 1.6065\n\nTime 0.6642 2.5089 2.2537 3.4211\n\nR-square\nGflops 0.9782 0.9313 0.9627 0.8540\n\nTime 0.9879 0.8689 0.8881 0.8049\n\nSeismic\n\nRMSE\nGflops 0.2273 0.6351 0.3158 0.4597\n\nTime 13.5391 212.282 15.6548 347.4940\n\nR-square\nGflops 0.9970 0.8334 0.9822 0.7313\n\nTime 0.9987 0.6263 0.9971 0.7494\n\nPerformance optimization\n\nSince the goal is to obtain the best performance, the model was compared with\n\nmeasurements from all actual data. Figure 4.4 presents the results of this comparison.\n\nBlue bars represent the normalized output of the predicted best performance from the\n\nML-based model (maximum GFLOPS) whereas red bars represent the normalized best\n\nperformance from actual data. Perfect fit of best performance is obtained when the pre-\n\ndicted values are the same (or very close) to actual best performance values. This means\n\nthat the predicted best performance actually matches the best performance from all data.\n\nBest performance for space tiling algorithm is the human-optimized implementation de-\n\nscribed in (DUPROS et al., 2015).\n\nFor 7-point Jacobi stencil, Figures 4.4a and 4.4b compare the performance of pre-\n\ndicted and actual data: the model achieves the best performance for the naive algorithm on\n\nthese multicore architectures but predicted values for space tiling algorithm did not reach\n\nexactly the same best performance, although they are close. For the Seismic Wave kernel,\n\nFigures 4.4c and 4.4d show predicted performance is so close to the best performance,\n\nthen our model achieves the best performance.\n\n\n\n64\n\nFigure 4.4: Normalized performance comparison between predicted results from the\nML-algorithm and results from the best performance experimens on multicore\n\narchitectures\n\n0.00\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n1.25\n\nNaive Space tiling\n\nN\no\n\nrm\na\n\nliz\ne\n\nd\n G\n\nflo\np\n\ns\n\nBest Predicted\n\n(a) GFLOPS for 7-point Jacobi: Node\nServer 185.\n\n0.00\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n1.25\n\nNaive Space tiling\n\nN\no\n\nrm\na\n\nliz\ne\n\nd\n G\n\nflo\np\n\ns\n\nBest Predicted\n\n(b) GFLOPS for 7-point Jacobi: Node\nCryo.\n\n0.00\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n1.25\n\nNaive Space tiling\n\nN\no\n\nrm\na\n\nliz\ne\n\nd\n G\n\nflo\np\n\ns\n\nBest Predicted\n\n(c) GFLOPS for Seismic Wave: Node\nServer 185.\n\n0.00\n\n0.25\n\n0.50\n\n0.75\n\n1.00\n\n1.25\n\nNaive Space tiling\n\nN\no\n\nrm\na\n\nliz\ne\n\nd\n G\n\nflo\np\n\ns\n\nBest Predicted\n\n(d) GFLOPS for Seismic Wave: Node\nCryo.\n\nSource: (MARTINEZ et al., 2017)\n\nApproximation of best performance for 7-point Jacobi was 97.46% and 95.61% for\n\nnaive in node Server 185 and node Cryo respectively; for space tiling, it was 76.79% and\n\n87.01% in node Server 185 and node Cryo respectively. For seismic, the approximation\n\nwas 97.78% and 97.04% for naive in node Server 185 and node Cryo respectively, for\n\nspace tiling in node Server 185 was 99.62% and it outperformed by 1.06% in node Cryo.\n\nPrediction for naive is easier than space tiling because the model has fewer parameters\n\nto build and it used all configurations described in Table 4.6. The speedup of the best\n\nperformance for 7-point Jacobi stencil was \u00d75.57 and \u00d712.57, for naive and space tiling\n\nrespectively, compared to the worst performance. For the seismic stencil, the speedup of\n\nbest performance was \u00d79.33 and \u00d718.88, for naive and space tiling respectively.\n\n\n\n65\n\n4.2.3 Performance prediction on manycore architectures\n\nIn this section, we present the results of our prediction model on many-core archi-\n\ntectures. We used the stencil naive for 7-point Jacobi, the seismic and the acoustic wave\n\npropagation with a three-dimensional grid of size 512x512x512, and 190 time iterations,\n\nas the benchmark for our experiments. Figure 4.5 presents the flowchart to this approach.\n\nFigure 4.5: Flowchart of Golem on manycore architectures.\n\nNumber of threads\n\nL2 CM SVM\n\nTime SVM\n\nTraining Set\n\nTesting Set\n\nInput Layer Output LayerHidden Layer\n\nLog Minimum\n\nSearching\n\nChunk size\n\nScheduling policy\n\nMemory mode\n\nCycles SVM\n\nPerformance\n\nSource: The Author\n\nParameters in each layer are defined as:\n\n\u2022 Input Layer:, As well as the model for multicore architectures, values for the input\n\nvector depend on OpenMP runtime parameters, as the number of threads defined by\n\nthe OMP_NUM_THREADS environment variable, the loop scheduling policy (Static,\n\nDynamic and Guided) and the chunk size defined by the OMP_SCHEDULE envi-\n\nronment variable; additionally, for this architecture we also considered the memory\n\nmode explained in Section 2.2.1 (cache and flat).\n\n\u2022 Hardware Counters Layer: We choose as the most relevant events the L2 total\n\ncache misses (PAPI_L2_TCM), and the total of cycles (PAPI_TOT_CYC).\n\n\u2022 Output Layer: The performance vector contains the execution time to solve de\n\ngeophysics stencil.\n\n\n\n66\n\nTestbed and configuration domain\n\nWe used one manycore platform o carry out the experiments, an Intel Xeon Phi\n\nprocessor shown in Table 2.1 from Chapter 2. Based on this platform, Table 4.8 details\n\nall the available configurations for the optimization categories. As it can be observed, a\n\nbrute force selection would take many time on experiments.\n\nTable 4.8: Available configurations for optimization procedure.\n\nOptimization Parameters Total configurations\n\nNumber of threads 1 272\n\nChunk size 1 272\n\nScheduling policy 1 3\n\nMemory mode 1 2\n\nTotal 4 443,904\n\nAnalysis of variance and hardware counters behavior\n\nIn this analysis, the execution time, the number of L2 cache misses, and the num-\n\nber of cycles were used as population variables and factors are defined by all values in\n\nvector input (the number of threads, the scheduling policy, the chunk size and memory\n\nmode). The results of p-value for the 7-point Jacobi, seismic and acoustic wave propaga-\n\ntion stencils are presented in Table 4.9.\n\nTable 4.9: p-value of one-way ANOVA for the manycore architecture.\nExecution time L2 cache misses Cycles\n\nJacobi Seismic Acoustic Jacobi Seismic Acoustic Jacobi Seismic Acoustic\n\nScheduling policy 1.26e-13 1.000 1.95e-08&lt;2e-16 0.992&lt;2e-16&lt;2e-16 1.000&lt;2e-16\n\nChunk size&lt;2e-16 0.949&lt;2e-16&lt;2e-16 0.927&lt;2e-16&lt;2e-16 0.956&lt;2e-16\n\nNum. of threads&lt;2e-16&lt;2e-16 0.444 1.82e-10&lt;2e-16 0.000757&lt;2e-16&lt;2e-16 2.80e-07\n\nMemory mode 0.77 0.949&lt;2e-16 0.132 0.919 0.424529 0.969 0.915 5.81e-05\n\nGeneral considerations can be observed with results from Table 4.9. First, the\n\nmemory mode is a factor with no statistical significance in most of variables and stencils,\n\nonly for the acoustic stencil, the memory mode produces statistical significance in exe-\n\ncution time and cycles variables. It could be explained because the code optimizations\n\nmade for this stencil (SERPA et al., 2017); the Jacobi and seismic stencils have been im-\n\nplemented with no code optimization for the Xeon Phi architecture. Second, the Jacobi\n\nstencil has statistical significance in all variables (execution time, L2 cache misses and cy-\n\n\n\n67\n\ncles) for the other factors. Third, variables for seismic stencil have statistical significance\n\nby threads counting. Fourth, the acoustic stencil has statistical significance for almost all\n\nvariables, except for execution time by thread counting. Finally, we calculate a two-way\n\nANOVA to determine if combined variables affect the seismic stencil performance. Table\n\n4.10 shows the results of two-way ANOVA.\n\nTable 4.10: p-value of two-way ANOVA for the seismic wave kernel.\n\nExecution time L2 cache misses Cycles\n\nScheduling policy:Chunk 1.000 0.997 1.000\n\nScheduling policy:Num. of threads&lt;2e-16&lt;2e-16&lt;2e-16\n\nChunk:Num. of threads 0.992 0.956 0.988\n\nResults in Table 4.10 show that combining scheduling and thread counting, in the\n\nseismic stencil, rejects the hypothesis, and the variables have the statistical difference\n\nif the two factors are combined. But, when we combine chunk with another factor, we\n\nstill do not find statistical significance. Then, we can summarize that most of the input\n\nvariables produce statistical significance into performance variables, except the chunk\n\nsize.\n\nOn manycore architectures, Figure 4.6 illustrates how the hardware counters mea-\n\nsurements are affected by the input variables. Each point represents one experiment when\n\nvarying the input parameters described in Section 4.2.2, X domain represents the L2\n\ncache misses, Y axis represents the total cycles, and the color represents the different\n\nvalues for the input parameter. For instance, Figure 4.6a represents the scheduling policy\n\n(green is dynamic, red is guided, and blue is static) for the Jacobi stencil, we can see\n\nhow the static scheduling tends to be separated from other values. Figure 4.6b shows the\n\nnumber of threads used to solve the seismic stencil, we can see how each value create\n\none easily separated area; this fact also confirms the ANOVA results of statistical signifi-\n\ncance by the number of threads. Figure 4.6c also presents how chunk size tends to create\n\nseparated areas for the acoustic stencil.\n\nWe can resume this behavior as follows, changing values in input parameters af-\n\nfects the application performance and creates several separated areas in the graphic rep-\n\nresentation, as each color represents a different value for the input value these areas could\n\nbe separated by hyperplanes from an SVM.\n\n\n\n68\n\nFigure 4.6: Hardware counters behavior on Manycore Node.\n\n1e+10\n\n1e+11\n\n1e+06 1e+07 1e+08\nL2 cache misses\n\nC\nyc\n\nle\ns\n\nScheduling Dynamic Guided Static\n\n(a) Jacobi stencil.\n\n1e+11\n\n1e+08\nL2 cache misses\n\nC\nyc\n\nle\ns\n\nThreads\n16\n32\n\n48\n64\n\n80\n96\n\n112\n128\n\n144\n160\n\n176\n192\n\n208\n224\n\n240\n256\n\n(b) Seismic stencil\n\n1e+11\n\n1e+08 1e+09 1e+10\nL2 cache misses\n\nC\nyc\n\nle\ns\n\nChunk\n\n1\n9\n17\n25\n\n33\n41\n49\n57\n\n65\n73\n81\n89\n\n97\n105\n113\n121\n\n129\n137\n145\n153\n\n161\n169\n177\n185\n\n193\n201\n209\n217\n\n225\n233\n241\n249\n\n(c) Acoustic stencil\n\nSource: The Author\n\nTraining and validation sets\n\nWe also created a training set by randomly selecting a subset from the configu-\n\nration parameters presented in Table 4.8. Then, for each experiment, we measured the\n\nhardware counters (L2 cache misses, and total cycles) and performance (execution time).\n\nThe random testing set was used since all SVMs in both the hidden and the output layers\n\nare trained to predict the execution time values.\n\nAfter that, we measured the accuracy of the model using the statistical estimators\n\nexplained in 4.2.1. Table 4.11 presents the total number of experiments that were per-\n\nformed to obtain the training and validation sets. It is remarkable that the total number\n\nof experiments used for testing and validation, for each stencil, is lower than 1% of total\n\nconfigurations described in Table 4.8.\n\n\n\n69\n\nTable 4.11: Number of experiments\n\nTraining Testing Total\n\nJacobi 334 3007 3341\n\nSeismic 346 3122 3468\n\nAcoustic 335 3021 3356\n\nAccuracy of predictive modeling\n\nTable 4.12 presents the results for accuracy of the regression model. For the 7-\n\npoint Jacobi implementation, the model presented an accuracy of up to 97.39%. For the\n\nseismic wave implementation, on the other hand, the model presented an accuracy of up\n\nto 85.62% and the acoustic wave propagation model has 93.2% of accuracy.\n\nTable 4.12: Statistical estimators of our prediction model\n\nRMSE R-squared\n\nJacobi 2.9894 0.9739\n\nSeismic 21.0183 0.8562\n\nAcoustic 40.0748 0.9322\n\nPerformance optimization\n\nOn manycore architectures, we use the same strategy to obtain the best perfor-\n\nmance on multicore architecture. The model was compared with measurements from all\n\nactual data. Figure 4.7 present results of this comparison. Blue bars represent the nor-\n\nmalized output of the predicted best performance from the ML-based model (minimum\n\nexecution time) whereas red bars represent the normalized best performance from actual\n\ndata.\n\nPerfect fit of best performance is obtained when the predicted values are the same\n\n(or very close) to actual best performance values. For Jacobi stencil, the model achieves\n\nthe best performance with accuracy of 98.88%. While for the Seismic and the Acoustic\n\nkernel, the best execution time was outperformed in 12.95% and 0.59% respectively. The\n\nspeedup of best performance was \u00d749.76, \u00d735.53 and \u00d739.83 for 7-point Jacobi, seismic\n\nand acoustic stencils respectively, compared to worst performance.\n\n\n\n70\n\nFigure 4.7: Normalized performance comparison between predicted results from the ML-\nalgorithm and results from the best performance experimens on manycore architectures.\n\n0.0\n\n0.5\n\n1.0\n\n1.5\n\nJacobi Seismic Acoustic\n\nN\no\n\nrm\na\n\nliz\ne\n\nd\n T\n\nim\ne\n\nBest Predicted\n\nSource: The Author\n\n4.3 Concluding remarks\n\nIn this chapter, we proposed an ML-based model to predict the performance of\n\nstencil computations on multicore and manycore architectures when using a shared mem-\n\nory programming model. We showed that performance of three different stencil kernels\n\n(7-point Jacobi, seismic wave, and acoustic wave propagation) and two classical algo-\n\nrithm implementations (naive and space tiling) can be predicted with a high accuracy\n\nusing the hardware counters measurements and the best configuration can be obtained\n\nwith this methodology.\n\nML approaches appear as an efficient way to predict the performance of stencil\n\ncomputations. It allows finding the optimal input configuration to improve the perfor-\n\nmance. One major limitation on this model is related with the size of training and testing\n\nsets; for this situation, the challenge of performance prediction in real time may be ob-\n\ntained by research on unsupervised ML algorithms.\n\nIn this sense, complementary works would be researched towards to extend ML\n\nmethodologies in order to capture complex behaviors on advanced architectures (compil-\n\ners flags, vectorization, space-time blocking algorithms, or heterogeneous architectures);\n\nsecond, the proposed prediction model could be integrated into an auto-tuning framework\n\nto find the best performance configuration for a given stencil kernel; it may be ease by\n\nusing automatic S2S transformations like a target (i.e, BOAST framework) (VIDEAU et\n\nal., 2018).\n\n\n\nPart 2: Performance Improvement on Heterogeneous Architectures\n\n\n\n72\n\n5 HETEROGENEOUS ARCHITECTURES AND PROGRAMMING MODELS\n\nThe second part of this work starts with this chapter. Heterogeneous architec-\n\ntures are growing fast. Commodity NVIDIA\u2019s GPUs are the most popular accelerators,\n\nthey are built with several stream processors with SIMD elements. Another common het-\n\nerogeneous architecture is the Accelerated Processing Architecture (APU) from AMD,\n\nwhere CPU and GPU cores are integrated into one chip as System-on-a-Chip (SoC). The\n\nmain problem on these parallel platforms is related to data movement from main memory\n\nto accelerator memory, a Dynamic RAM (DRAM). Hence, memory-aware programming\n\nis strongly recommended to achieve maximum performance.\n\nCPUs and GPUs possess distinct architectural features. Modern multicore CPUs\n\nuse up to a few tens of cores, they run at high frequency and use large-sized caches to\n\nminimize the latency of a single thread. Clearly, CPUs are suited for latency-critical ap-\n\nplications. In contrast, GPUs use a much larger number of cores, which are in-order\n\ncores that share their control unit. GPU cores use a lower frequency and smaller-sized\n\ncaches. In some cases, for applications where data transfers dominate execution time\n\nor branch divergence does not allow for the uninterrupted execution on all GPU cores,\n\nCPUs can provide better performance than GPUs. There are factors relating to architec-\n\nture of Heterogeneous Computing Systems (HCS) to be taken into account: computation\n\npower of the Processing Units (PU), current load on PUs to achieve load balancing be-\n\ntween them, memory bandwidth and CPU-GPU data transfer overhead, size of GPU and\n\nCPU memory, number of GPU threads and CPU cores, reduced performance of GPU for\n\ndouble-precision computations, etc.\n\nAt the application level, the performance of HCS is affected by nature of algo-\n\nrithms, amount of parallelism, the presence of branch divergence, subdividing the work-\n\nload and selecting suitable work sizes to be allocated to all PUs and data dependencies.\n\nWorkload division between CPU and GPU is used into two criteria: dynamic division, the\n\ndecision about running the subtasks or program phases or code portions on a particular\n\nPU is taken at runtime; and static division, the subtasks that are executed on a particular\n\nPU are already decided before program execution; in other words, the mapping of sub-\n\ntasks to PUs is fixed. The decision about where a subtask should be mapped depends on\n\nwhich PU provides higher performance and which mapping helps in achieving load bal-\n\nancing. However, if subtasks differ, it may be more suitable to map a particular subtask to\n\na particular PU; for example, highly parallel subtasks can be mapped to the GPU, while\n\n\n\n73\n\nthe sequential subtasks can be mapped to the CPU. Workload division techniques sched-\n\nule tasks on devices based on several factors such as the contention of devices, historical\n\nperformance data, number of cores, processor speed, problem size, device status (busy or\n\nfree), and location of data.\n\n5.1 Streaming Multiprocessors\n\nThe GPUs are addressed to problems that express data-parallel computations (the\n\nsame program is executed on many data elements in parallel). Because the same program\n\nis executed for each data element, there is a lower requirement for sophisticated flow con-\n\ntrol, and because it is executed on many data elements and has high arithmetic intensity,\n\nthe memory access latency can be hidden with calculations instead of big data caches.\n\nData-parallel processing maps data elements to parallel processing threads. Many appli-\n\ncations that process large data sets can use a data-parallel programming model to speed\n\nup the computations. The most common example is the image processing algorithms,\n\nlarge sets of pixels are mapped to parallel threads; similarly, many algorithms outside\n\nthis field are accelerated by data-parallel processing, from general signal processing or\n\nphysics simulation to computational finance or computational biology (NVIDIA, 2016).\n\nIn this way, the main architectural component on a GPU is the Streaming Multiprocessor\n\n(SM).\n\nThe principal components of an SM are the cores and the several memories. There\n\nare also thousands of registers that can be partitioned among threads of execution and\n\nwarp schedulers that can quickly switch contexts between threads and issue instructions\n\nto warps that are ready to execute. The cores of the most recent SM are specialized\n\nfor integer and single-precision floating point operations, double-precision floating point,\n\nLoad/Store Units, and Special Function Units (SFUs) for single-precision floating-point\n\ntranscendental functions such as sin, cosine, reciprocal, and square root. Figure 5.1 rep-\n\nresents the block diagram of the common SM from an NVIDIA GPU. The memory of\n\nan SM is composed by the shared memory for fast data interchange between threads,\n\nconstant caches for a fast broadcast of reads from constant memory, texture caches to\n\naggregate bandwidth from texture memory, and an L1 cache to reduce latency to local or\n\nglobal memory.\n\n\n\n74\n\nFigure 5.1: Block diagram of a NVIDIA GPU SM.\n\nWarpWarp  schedulersschedulers\n\nInstructionInstruction  buffersbuffers\n\nDispatchDispatch  unitsunits\n\nSpecialSpecial\nFunctionsFunctions\n\nunitsunits\n\nTexture cachesTexture caches\n\nInstructionInstruction cache cache\n\nSingle PSingle Precision recision corescores\n\nSharedShared  memorymemory\n\nLoadLoad\nStoreStore\n\nDoubleDouble\nPPrecisionrecision\n\ncorescores\n\nL1 cacheL1 cache\n\nRegistersRegisters File File\n\nSource: The author\n\n5.2 Programming models on heterogeneous architectures\n\nThe common programming model for NVIDIA GPUs is the CUDA programming.\n\nThe CPU is known as the host and the GPU is known as the device, the data is transferred\n\nfrom the main RAM memory by the PCI express (PCIe) bus to the DRAM on GPU.\n\nCUDA comes with a software environment that allows developers to use C as a high-level\n\nprogramming language, and at its core are three key abstractions: a hierarchy of thread\n\ngroups, shared memories, and barrier synchronization. CUDA C extends C by defining\n\nC functions called kernels that are executed N times in parallel by N different CUDA\n\nthreads. A kernel is defined using the __global__ declaration and the number of\n\nCUDA threads that execute that kernel is specified using a KernelName<<<...>>>\n\nexecution configuration syntax. Each thread that executes the kernel is given a unique\n\nthread ID that is accessible within the kernel through the built-in threadIdx variable\n\nthat is a 3-component vector. The threads can be identified using a one-dimensional,\n\ntwo-dimensional, or three-dimensional thread index, forming a block of threads. Blocks\n\nare organized into a one-dimensional, two-dimensional, or three-dimensional grid as pre-\n\n\n\n75\n\nsented in Figure 5.2. The number of thread blocks in a grid is usually dictated by the size\n\nof the data being processed or the number of processors in the system. There is a limit\n\nto the number of threads per block since all threads of a block are expected to reside on\n\nthe same processor core and must share the limited memory resources of that core. On\n\ncurrent GPUs, a thread block may contain up to 1024 threads (NVIDIA, 2016).\n\nFigure 5.2: Representation of a grid with the thread blocks.\n\nSource: (NVIDIA, 2016)\n\nCUDA threads may access data from multiple memory spaces during their execu-\n\ntion as presented in Figure 5.3. Each thread has private local memory. Each thread block\n\nhas shared memory visible to all threads of the block. All threads have access to the same\n\nglobal memory. There are also two additional read-only memory spaces accessible by\n\nall threads: the constant and texture memory spaces. The global, constant, and texture\n\nmemory spaces are optimized for different memory usages. Texture memory also offers\n\ndifferent addressing modes, as well as data filtering, for some specific data formats. The\n\nglobal, constant, and texture memory spaces are persistent across kernel launches by the\n\nsame application (NVIDIA, 2016).\n\n\n\n76\n\nFigure 5.3: Representation of memory hierarchy for threads, blocks and grids.\n\nSource: (NVIDIA, 2016)\n\nThe heterogeneous programming model proposed by CUDA is presented in Figure\n\n5.4, the model assumes that the threads execute on the device that operates as a copro-\n\ncessor to the host running the C program. The programming model also assumes that\n\nboth the host and the device maintain their own memory spaces. Therefore, a program\n\nmanages the global, constant, and texture memory spaces visible to kernels through calls\n\nto the CUDA runtime. This includes device memory allocation and deallocation as well\n\nas data transfer between host and device memory (NVIDIA, 2016).\n\n\n\n77\n\nFigure 5.4: Heterogeneous CUDA programming model.\n\nSource: (NVIDIA, 2016)\n\n5.2.1 OpenCL programming model\n\nThe OpenCL is another programming model for heterogeneous architectures. It\n\nis a low-level API that runs on GPUs. Using the OpenCL API, developers can launch\n\ncompute kernels written using the C programming language. OpenCL is a cross-vendor\n\nprogramming language used for massively parallel multi-core graphic processors, and\n\nOpenCL kernel functions define the operations carried out by each data-parallel hardware-\n\nthread (MAGNI; DUBACH; O\u2019BOYLE, 2014). OpenCL is also a framework for writing\n\nprograms for heterogeneous systems. The framework defines resources and a set of inter-\n\nfaces that programmers use to construct an application. The OpenCL specification defined\n\na hierarchy of models (BALAJI, 2015):\n\n\n\n78\n\n\u2022 Platform model: It defines the heterogeneous system available for computations.\n\nAn OpenCL platform consists of a single host resource, this is a general-purpose\n\ncomputer. Connected to the host are one or more OpenCL devices. An OpenCL\n\ndevice can be a CPU, a GPU, an FPGA, or a specialized processor. For OpenCL, the\n\ndevice is decomposed into one or more compute units each of which is composed\n\nof one or more processing elements.\n\n\u2022 Execution model: It defines how a computation is launched and executed on the\n\nplatform. The computation occurs on the device and this execution model is based\n\non the kernel parallelism design pattern. A function is provided by the programmer\n\nto execute on the device. This is called a kernel. The kernel is submitted to a\n\ncommand queue for execution. A command is anything submitted by the host to\n\nthe queue.\n\n\u2022 Memory model: The OpenCL memory is organized into regions. The Host mem-\n\nory is available and managed by the host and provided by the native host platform.\n\nThe Global memory is a memory region that is globally accessible to all work-items\n\nexecuting within a context; the Constant memory is defined as a region that is glob-\n\nally accessible on the device that can be read only during the execution of a kernel;\n\nthe Local memory is a memory region associated with a computing unit and visible\n\nto the work-items within a work-group; and the Private memory region associated\n\nwith a processing element and visible only within a work-item.\n\n\u2022 Programming model: defines the fundamental abstractions used to map an algo-\n\nrithm onto source code. OpenCL supports two basic programming models. The\n\nData parallelism, a single sequence of instructions is applied concurrently to each\n\nelement of a data structure; and the Task parallelism, a task is a sequence of instruc-\n\ntions and the data required by those instructions. In task parallelism, multiple tasks\n\nare run concurrently.\n\n5.3 Evolution of NVIDIA GPU Architectures\n\nThe NVIDIA\u2019s Tesla architecture, introduced in November 2006, was one the ear-\n\nliest improvements on GPUs, this architecture was based on a scalable processor array\n\nwith 128 streaming-processor cores organized as 16 SMs in eight independent processing\n\n\n\n79\n\nunits called texture/processor clusters (TPCs) (LINDHOLM et al., 2008). Consequently,\n\nNVIDIA GPU architectures have been evolved through the time, they have been released\n\nby family codenames and the principal features are the following:\n\nFermi\n\nThis architecture was the first with implementation of real floating point (single\n\nand double precision), the Error Correcting Codes (ECC) on main memory and caches,\n\na single 64-bit address to unify the memory space, and atomic instructions to read from\n\na shared location in memory, to check its value, and to write a new value back without\n\nany other processors being able to change the memory value during the execution of the\n\natomic instruction (PATERSON, 2009; NVIDIA, 2009). Figure 5.5 illustrates the SM of\n\nFermi architecture.\n\nFigure 5.5: Architecture of the Fermi SM.\n\nSource: (NVIDIA, 2009)\n\n\n\n80\n\nKepler\n\nThe principal improvements on this architecture were the Dynamic Parallelism\n\nand the Hyper-Q. First, it allows the GPU device to generate new work for itself, synchro-\n\nnize on results, and control the scheduling of that work via dedicated, accelerated paths,\n\nall without involving the CPU host. Second, it allows more connections from multiple\n\nCUDA streams, from multiple MPI processes, or from multiple threads by allowing 32\n\nsimultaneous, hardware-managed connections (NVIDIA, 2014b). Figure 5.6 illustrates\n\nthe SM of Kepler architecture.\n\nFigure 5.6: Architecture of the Kepler SM.\n\nSource: (NVIDIA, 2014b)\n\n\n\n81\n\nMaxwell\n\nIt improves on Kepler by separating shared memory from the L1 cache, providing\n\na dedicated 64KB shared memory in each SM. This architecture introduces native shared\n\nmemory atomic operations for 32-bit integers and native shared memory 32-bit and 64-\n\nbit compare-and-swap (CAS) (HARRIS, 2014). Figure 5.7 illustrates the SM of Maxwell\n\narchitecture.\n\nFigure 5.7: Architecture of the Maxwell SM.\n\nSource: (HARRIS, 2014)\n\n\n\n82\n\nPascal\n\nThe main feature improvements on Pascal architecture are the Unified Memory,\n\na significant advancement for NVIDIA GPU computing and software-based feature that\n\nprovides a single, seamless unified virtual address space for CPU and GPU memory, it\n\nprovides a memory accessible from all CPUs and GPUs in the system as a single, coherent\n\nmemory image with a common address space, and the Compute Preemption, this allows\n\ncompute tasks running on the GPU to be interrupted at instruction-level granularity, and\n\ntheir context swapped to GPU DRAM. The Pascal architecture is also optimized to better\n\nperformance of algorithm implementations for Artificial Intelligence (NVIDIA, 2017).\n\nFigure 5.5 illustrates the SM of Pascal architecture.\n\nFigure 5.8: Architecture of the Pascal SM.\n\nSource: (NVIDIA, 2017)\n\n5.4 Assymetric low power architectures\n\nThe Asymmetric Manycore Processor (AMP) is a special case of HCS, which\n\nuses cores of different types in the same processor and thus embraces heterogeneity as\n\n\n\n83\n\na first principle. Different cores in an AMP may be optimized for power/performance,\n\ndifferent application domains or for exploiting different levels of parallelism (ILP, TLP,\n\nor Memory-Level Parallelism, MLP). They are used mainly in mobile systems because\n\nthey are important to optimize energy for prolonging battery life during idle periods as it\n\nis to optimize performance for multimedia applications and data processing during active\n\nuse.\n\nAlthough big and little cores generally provide better performance and energy\n\nefficiency, respectively, in several scenarios no single winner may be found on the metric\n\nof energy-delay product (EDP). The big core may show better EDP in applications that\n\ncompute intensive and have predictable branching and high data reuse. By contrast, the\n\nsmall core may be better for memory-intensive applications and applications with many\n\natomic operations and little data reuse. Reconfigurable AMPs facilitate flexibly scaling\n\nup to exploit MLP and ILP in single-threaded applications and can scale down to exploit\n\nTLP in multithreaded (MT) applications.\n\nSeveral challenges must be addressed to fully realize the potential of AMPs: The\n\nheterogeneous nature of AMPs demands complete re-engineering of the whole system.\n\nThe cores of an AMP may have different supply voltages and frequencies, which presents\n\nmanufacturing challenges; some techniques use offline analysis to perform static schedul-\n\ning; however, they cannot account for different input sets and application phases; in static\n\nAMPs, thread migration may take millions of cycles, for example, in a big.LITTLE sys-\n\ntem with Cortex A15 and A7 processors the latency of moving tasks from the A15 to\n\nA7 and vice versa could be 3.75ms and 2.10ms; thread schedulers, threads running on an\n\nAMP may be unfairly slowed down, which leads to starvation and unpredictable per-task\n\nperformance. (MITTAL, 2016)\n\n5.5 Target machines\n\nExamples of HCS and AMP architectures used in this work are presented in Table\n\n5.1 and Figure 5.9 presents the architecture of asymmetric node called Tegra K1. These\n\nmachines are located at Industrial University of Santander (Colombia), BRGM (France)\n\nand Informatics Institute of UFRGS (Brazil).\n\n\n\n84\n\nTable 5.1: Heterogeneous architecture configurations.\nBRGM Salm\u00e3o node Guane-1 Tegra K1\n\nProcessor i7-3720QM i7-930 Xeon E5645 ARM Cortex-A15\nClock (GHz) 2.60 2.80 2.40 2.3\nCPU Cores 4 4 6 4\n\nCPU Sockets 1 1 2 1\nAccelerator GeForce GTX 670M Tesla K20c Tesla M2075 NVIDIA Kepler GPU\nArchitecture Fermi Kepler Fermi Kepler\nGPU Cores 336 2496 448 192\n\nGPU Sockets 1 1 8 1\nGPU RAM (GB) 1.3 5 4.9 2\n\nFigure 5.9: Architecture of Tegra machine presented in Table 5.1.\n\nP a g e  | 5 \n \n\nNVIDIA Jetson TK1 Development Kit  April 2014 \n\n \n\nNVIDIA Tegra K1 \u2013 A New Era in Mobile Computing \nNVIDIA\u2019s latest and most advanced mobile processor, the Tegra\u00ae K1, creates a major discontinuity in the \nstate of mobile graphics by bringing the powerful NVIDIA Kepler\u2122 GPU architecture to mobile and \ndelivering tremendous visual computing capabilities and breakthrough power efficiency. The NVIDIA \n\nTegra K1 mobile processor is designed from the ground up to create a major discontinuity in the \n\ncapabilities of mobile processors, and delivers the industry\u2019s fastest and most power efficient \nimplementation of mobile CPUs, PC-class graphics, and advanced GPU-accelerated computing \n\ncapabilities. \n\nSome of the key features of the Tegra K1 SoC (System-on-a-Chip) architecture are: \n\nx 4-PLUS-1 Cortex A15 \u201cr3\u201d CPU architecture that delivers higher performance and is more power \nefficient than the previous generation. \n\nx Kepler GPU architecture that utilizes 192 CUDA cores to deliver advanced graphics capabilities, \nGPU computing with NVIDIA CUDA 6 support, breakthrough power efficiency and performance \nfor the next generation of gaming and GPU-accelerated computing applications. \n\nx Dual ISP Core that delivers 1.2 Giga Pixels per second of raw processing power supporting \ncamera sensors up to 100 Megapixels.  \n\nx Advanced Display Engine that is capable of simultaneously driving both the 4K local display and \na 4K external monitors via HDMI  \n\nx Built on the TSMC 28 nm HPM process to deliver excellent performance and power efficiency. \n \n\n \n\nFigure 1 NVIDIA Tegra K1 Mobile Processor Source:(NVIDIA, 2014a)\n\n5.6 Concluding remarks\n\nIn this chapter, we introduced the heterogeneous architectures. We focused on\n\nplatforms composed by the CPU host and the GPU devices. Improvements on GPUs are\n\nevolved to efficient float point precision work, and the limitations on these architectures\n\nare related to data movement between main memory and accelerator memory, and the idle\n\ntime of free CPUs cores while GPU is computing. The GPU memory is a limitation that\n\ncould be a bottleneck for the processing of large-scale problems. Consequently, for the\n\nCPU/GPU systems, out-of-core data management techniques are required to tackle the\n\nmemory overflow problem in the GPU, when the size of a data exceeds the capacity of\n\nthe DRAM. Although, the strategy of data partitioning between CPU memory and GPU\n\nmemory could also affect the quality and efficiency of data processing when these out-of-\n\n\n\n85\n\ncore techniques are applied due to the several data transferences between main memory\n\nand DRAM.\n\nThe most common programming model for NVIDIA GPUs is CUDA, it is very\n\neasy to use as programming procedure. CUDA is a conventional C adding a few of GPU\n\nplatform specific keywords and syntax to define the parallel kernels. Older implemen-\n\ntations of CUDA applications had to consider an explicit data movement from the main\n\nmemory to DRAM, with unified memory lets programmers focus on developing parallel\n\ncode without getting bogged down in the details of allocating and copying device memory.\n\nTaking into account architectural improvements and limitations, the GPUs offer a\n\nplatform for efficiency and speed-up optimization to applications with the possibility of\n\ndata parallelism. In this sense, the geophysics numerical stencils are the kind of appli-\n\ncations that can be implemented into these architectures with a well-suited performance\n\nimprovement.\n\n\n\n86\n\n6 NUMERICAL IMPLEMENTATION OF GEOPHYSICS STENCILS ON HET-\n\nEROGENEOUS PLATFORMS\n\nThe main idea is to divide the 3D grid into 2D slices. In this manner, each core of\n\nstream multiprocessors exploits the multi-threading parallelism to solve each point on this\n\nslice. In (MICIKEVICIUS, 2009), the authors use GPUs to solve a 3D stencil from the\n\nfinite difference discretization of the wave equation. They create a 2D tile and their halos\n\nthat are loaded into shared memory, then each thread block computes the 2D stencil. They\n\nalso extended the solution for multi GPUs: first, they divided each slice into 2 GPUs and\n\na computation of order k in space, data is partitioned by assigning each GPU half the data\n\nset plus (k/2) slices of ghost nodes. Each GPU updates its half of the output, receiving the\n\nupdated ghost nodes from the neighbor; and second, in order to maximize scaling, they\n\noverlap the exchange of ghost nodes with each kernel execution.\n\nIn (ABDELKHALEK, 2007; ABDELKHALEK et al., 2009), the authors solve the\n\nacoustic wave equation and the Reverse Time Migration (RTM), a technique for creating\n\nseismic images in areas of complex wave propagation, and they used two approaches:\n\nfirst, they limited the stencil domain to use global memory on each GPU and to dedicate a\n\nCUDA thread for each grid point in the space domain; and second, instead of dedicating\n\na thread to each grid point, they used a sliding window algorithm, creating slices in the z\n\ndirection.\n\nWe used a standard implementation of seismic wave propagation model described\n\nin (MICH\u00c9A; KOMATITSCH, 2010), the authors also subdivided the 3D space in 2D\n\ntiles, in the X and Y directions, and each tile corresponding to a block of threads for the\n\nGPU, to iterate along the third direction, in this case, the Z direction. The data of the 2D\n\nmesh tile and its halos are loaded in shared memory from global memory. Data sharing\n\nbetween GPUs is performed by Message Passing Interface. Figure 6.1 represents the data\n\ndomain and the tiling division.\n\n\n\n87\n\nFigure 6.1: Tiling division of 3D data domain, each slice is computed by the GPU.\n\nSource: (MICH\u00c9A; KOMATITSCH, 2010)\n\n6.1 Setup and Performance Measurement\n\nIn order to analyze the workload on a cluster of GPUs, the seismic model (Ondes\n\n3D) was executed on the HCS called Guane-1 (SANTANDER, 2015). Experiments were\n\nconsidered as follows: first, execution of seismic model in one GPU as the baseline to\n\ncompare the application performance; second, to increment the number of GPUs to get\n\nan unbalanced problem, when the GPUs show an uneven utilization; third, to measure\n\nthe time for execution and communication, the load and the memory usage of GPU; fi-\n\nnally, to compare measures to find a relation between unbalanced load and the measures\n\ninvolved in this problem. The objective of this section is to find correlations between time\n\n(execution and communications), GPU load and memory usage. For each experiment was\n\nmeasured following data:\n\n\u2022 Timeloop: average time to solve the model on each GPU (Kernel 1 plus Kernel 2)\n\n\u2022 Kernel 1: time to calculate the stress kernel in the model.\n\n\u2022 Comm 1: time of kernel 1 used to wait communications.\n\n\u2022 Kernel 2: time to calculate the velocity kernel in the model.\n\n\u2022 Comm 2: time of kernel 2 used to wait communications.\n\n\u2022 GPU load: percent of GPUs utilization.\n\n\u2022 GPU memory: percent of available memory usage.\n\n\n\n88\n\n6.2 Elapsed Time\n\nPerformance on HCS can be improved by increasing the number of GPUs, Fig-\n\nure 6.2 shows the minimum time, the average time and the maximum time of stencil\n\nexecution on a multi-GPU node, we found a typical behavior for parallel applications, if\n\nthe number of GPUs is incremented the execution time will be reduced, but the speedup\n\nof GPU implementation is not optimal, average execution with 8 GPUs is almost a \u00d73\n\nacceleration.\n\nFigure 6.2: Timeloop and speedup measures of Ondes 3D application when increasing\nthe number of GPUs.\n\n40\n\n60\n\n80\n\n100\n\n120\n\n2 4 6 8\nNumber of GPUs\n\nT\nim\n\ne\n (\n\nse\nc)\n\nAverage Maximun Minimum\n\n(a) Timeloop\n\n2\n\n4\n\n6\n\n8\n\n2 4 6 8\nNumber of GPUs\n\nS\np\n\ne\ne\n\nd\nu\n\np\n\nIdeal Speedup\n\n(b) Speedup\n\nSource: The Author\n\nAnother measure to analyze the performance of HCS is the communication time\n\nbetween the host and the device. The seismic model executes two kernels on each GPU\n\n(Stress and velocity calculation). Figure 6.3 shows average time for kernel 1 and kernel\n\n2. It shows that time for each one is quite similar. Another thing remarkable about this\n\nfigure is that communication is increasing when the number of GPUs is increasing, the\n\naverage time is almost the same for all executions.\n\n\n\n89\n\nFigure 6.3: Kernel execution and communication time of Ondes 3D application when\nincreasing the number of GPUs.\n\n0\n\n20\n\n40\n\n60\n\n2 4 6 8\nNumber of GPUs\n\nT\nim\n\ne\n (\n\nse\nc)\n\nComm K1 Comm K2 Kernel 1 Kernel 2\n\nSource: The Author\n\nMeasures presented in Figure 6.4 in this experiment are as expected, for one GPU\n\nthere are no communication on kernels because there is only one GPU executing the\n\nsimulation and GPU load is almost 100%. We measured for complete simulation since\n\nthe time that CPU is processing to make the stencil and to send data to GPU memory. At\n\nthe start of the simulation, the GPU is not working and waits until getting all data. It is\n\nremarkable that not all memory is used, almost 1/4 of memory is used in the simulation.\n\nThis was one of the limitations related to (MICH\u00c9A; KOMATITSCH, 2010).\n\nIf the number of GPUs is increased, the experiments start to reveal something that\n\nappears with many more GPUs. There is two kind of GPUs, the first solve the domain\n\nboundaries of simulation and for these, the load is greater than the other GPUs; the second,\n\nGPUs with lower load is solving the inner domain in grid space. Another thing, GPUs in\n\nthe inner domain is using less memory than the others. Then, we can say that there are\n\ntwo factors affecting the load balancing: the first, data is not located in the GPU when\n\nis requiered; and second, getting new data causes waste of time for communications.\n\nAnother factor that could be affecting the load balancing is the geometry in the simulation.\n\nGPUs that are solving the grid points near in the Y-axis have lower load than the GPUs\n\nthat are solving the grid on the greatest values.\n\nThe experiment with the worst load balancing was with 8 GPUs, then we can\n\nsay that uneven balance is caused by GPUs with high rates of communication has lower\n\nutilization, GPUs with greater load are solving the inner domain and values near to lowest\n\nvalues on Y-axis, for the grid space, GPUs with lower load is using less memory than the\n\n\n\n90\n\nothers with higher load.\n\nFigure 6.4: Measures (time, load and memory consumption) of Ondes 3D on a cluster of\nGPUs.\n\n0\n\n25\n\n50\n\n75\n\n100\n\n0\nGPU id\n\nP\ne\n\nrc\ne\n\nn\nt \n(%\n\n)\n\nGPU load Memory usage\n\n(a) GPU and memory usage for 1 GPU\n\n0\n\n20\n\n40\n\n60\n\n0\nGPU id\n\nT\nim\n\ne\n (\n\nse\nc)\n\nComm K1 Comm K2 Kernel 1 Kernel 2\n\n(b) Time measures for 1 GPU\n\n0\n\n25\n\n50\n\n75\n\n100\n\n0 1 2 3\nGPU id\n\nP\ne\n\nrc\ne\n\nn\nt \n(%\n\n)\n\nGPU load Memory usage\n\n(c) GPU and memory usage for 4 GPUs\n\n0\n\n10\n\n20\n\n30\n\n0 1 2 3\nGPU id\n\nT\nim\n\ne\n (\n\nse\nc)\n\nComm K1 Comm K2 Kernel 1 Kernel 2\n\n(d) Time measures for 4 GPUs\n\n0\n\n25\n\n50\n\n75\n\n100\n\n0 1 2 3 4 5 6 7\nGPU id\n\nP\ne\n\nrc\ne\n\nn\nt \n(%\n\n)\n\nGPU load Memory usage\n\n(e) GPU and memory usage for 8 GPUs\n\n0\n\n10\n\n20\n\n0 1 2 3 4 5 6 7\nGPU id\n\nT\nim\n\ne\n (\n\nse\nc)\n\nComm K1 Comm K2 Kernel 1 Kernel 2\n\n(f) Time measures for 8 GPUs\n\nSource: The Author\n\n\n\n91\n\n6.3 GPU Load and Memory Usage\n\nWe found there are four factors that are affecting the load balancing for the seismic\n\nmodel, then we evaluate if these factors actually have influence for this problem, to prove\n\nthis situation we measured GPUs load and memory usage and communication and we\n\npresent these measures in Figure 6.5.\n\nFigure 6.5: Statistical estimators (average and standard deviation) of Ondes 3D for GPU\nload and memory usage.\n\n0\n\n25\n\n50\n\n75\n\n100\n\n1 2 3 4 5 6 7 8\nNumber of GPUs\n\nP\ne\n\nrc\ne\n\nn\nt \n(%\n\n)\n\nGPU load Memory usage\n\nSource: The Author\n\nFigure 6.5 presents average values for GPU load (red bars) and memory usage\n\n(blue bars) when we increased the number of GPUs, it shows clearly how the average\n\nload on GPUs is affected when more GPUs are used. Another situation in this figure\n\nis that memory usage has a similar behavior than the GPU load. Then, we estimate the\n\ncorrelation coefficient between these measures, and founded that this value is 0.88; be-\n\ncause of this correlation between memory and load we can prove that these two factors\n\nare highly correlated, that is if GPUs will have a lower load on execution they will have\n\nlower utilization of memory. Instead, if we estimate the value of the correlation coeffi-\n\ncient between the GPU load and the kernel communications the result is -0.30, this value\n\nis not enough to prove any correlation between them.\n\nNow, we will analyze the standard deviation (error bars) of these measures. We\n\ncan see that multi-GPU experiment with 7 GPUs has the better balance and the experi-\n\nment with 8 GPUs has the worst. The increment of standard deviation indicates a major\n\ndifference between the values of this measure. Another thing we found on this figure is\n\nthat a better-balanced experiment has a major value on the standard deviation of memory\n\n\n\n92\n\nusage. Then we estimate the correlation between the standard deviation of load an mem-\n\nory usage, this value is -0.71. Again, this new value is proving the correlation between\n\nload and memory usage, in this case, indicates if an experiment is unbalanced there is\n\na high correlation to get lower utilization of memory. On the other hand, we estimate\n\nthe correlation between the standard deviation of load and communications, the value is\n\n0.80. In this case, the value is shown that there is a high correlation between unbalanced\n\nexperiments and time of communication, proving that these measures are also correlated.\n\n6.4 Concluding Remarks\n\nIn this chapter, we presented another classical alternative to solve the geophysics\n\nnumerical kernels. The standard implementation (based on Message Passing and CUDA)\n\non heterogeneous architectures shows a performance improvement in terms of speedup\n\nand execution time.\n\nAlthough there is a performance improvement, the available architecture is un-\n\nderused most of the time, and the unbalanced performance is increased for multi-GPU\n\nsettings. In this sense, the performance for GPU-only systems remains not optimal. Fur-\n\nthermore, after allocating the workload to the GPU (i.e., starting the kernel) the CPU usu-\n\nally stays idle and waiting for the GPU to finish. The GPU memory bandwidth would act\n\nas a bottleneck (i.e., kernel communications), and the computational resources of GPUs\n\ncould remain underutilized. This situation can be solved by data prefetch mechanisms\n\non most recent architectures. Another challenge with standard implementation on HC is\n\nthat the input configuration at runtime depends only on the number of GPUs and number\n\nof threads used by block, and there are no scheduling, or load balancing, strategies to\n\nimprove the computing use of available heterogeneous cores.\n\nOne alternative to solve the unbalance problem, when increasing the number of\n\navailable GPUs, could be to research on non-classical implementations to involve the free\n\nCPU cores into computations, and to avoid unnecessary memory transfers; in this sense,\n\none challenge on HC is to exploit its computing capability, because common program-\n\nming models on HC uses the CPU only as manager of the GPU device.\n\n\n\n93\n\n7 TASK-BASED APPROACH FOR PERFORMANCE IMPROVEMENT ON HET-\n\nEROGENEOUS PLATFORMS\n\nTask-based parallelism is a data-oriented programming model at the high level\n\nfor heterogeneous architectures. The main idea is to build a task dependence graph, the\n\nruntime creates a queue of tasks with data directionality and schedules them into available\n\nprocessors. This programming model is exploited and developed mainly at Barcelona\n\nSupercomputing Center (BSC). Tasking works well when computations can be divided\n\ninto blocks. The programmer specifies the distribution of data structures, and the compiler\n\ntakes care of low-level details, such as the generation of messages and synchronization\n\n(HASSEN; BAL; JACOBS, 1998).\n\n7.1 Runtime systems for task-based programming\n\nOne of these runtime systems is SMP superscalar (SMPSs), a programming en-\n\nvironment focused for shared memory systems, as multicore and SMP platforms. A pro-\n\ngram in SMPSs follows the model of #pragma sentences that identify atomic parts and\n\nfunctions in the code that are candidates to be run in parallel in the different cores. Data\n\ndirectionality of parameters is explicit by input (Read only), output (Write only) and\n\ninout (Read/Write) clauses. At execution time, the runtime takes the memory address,\n\nsize and directionality of each parameter at each task invocation and uses them to analyze\n\nthe dependencies between them. Whenever a task is called, a node in a task graph is\n\nadded for each task instance and a series of edges indicating their dependencies. At the\n\nsame time, the runtime schedules the tasks to the different processors when their input de-\n\npendencies are satisfied. Threads consume ready tasks from their own list in LIFO order,\n\nthey get tasks from the main list in FIFO order, and they can steal from other threads in\n\nFIFO order. (PEREZ; BADIA; LABARTA, 2008; PEREZ; BADIA; LABARTA, 2010)\n\nRecently works at BSC are oriented on a family of task-based runtime systems\n\ncalled StarSs. In this context, they are developed OmpSs, based on OpenMP for a task-\n\nbased programming model with the indication of data directionality (DURAN et al.,\n\n2011). OmpSs specification includes a target device clause, for example, CUDA\n\nor OpenCL, the programmer needs to provide the code of the kernel for each one, and this\n\ncode can be part of a task. Another support of OmpSs is the possibility of providing more\n\n\n\n94\n\nthan one version of a given task (implements clause). The versions can target one or\n\nmore devices. At runtime, the scheduler will decide which version should be scheduled\n\ntaking into account some parameters such as execution time or locality of the data (FER-\n\nN\u00c1NDEZ et al., 2014). They have also integrated OmpSs with OpenCL framework to\n\nexploit the underlying hardware platform with greater ease in programming and to gain\n\nsignificant performance using data parallelism (ELANGOVAN; BADIA; PARRA, 2013).\n\nAnother framework is PaRSEC, it employs the dataflow programming and ex-\n\necution model to provide a dynamic platform that can handle heterogeneous hardware\n\nresources. The runtime combines the source program and the data flow information with\n\nsupplementary information provided by the user and orchestrates task execution on the\n\navailable processors. When a task is completed, the runtime reacts by examining the data\n\nflow to find what tasks can be executed and handles the data exchange between nodes.\n\nWhen no tasks are triggered because the hardware is busy executing application code,\n\nthe runtime gets out of the way, allowing all hardware resources to be devoted to the\n\napplication execution (BOSILCA et al., 2013b).\n\nXKaapi is a task-based framework with locality-aware work-stealing algorithm\n\nto manage data locality and scheduling, fully asynchronous task execution strategy on\n\nGPUs, a light representation of tasks that allows to generate high degree of parallelism\n\nat low cost, and lazy computation of dependencies with an optimization that enables to\n\nmove the overhead on the critical path rather than on the work (GAUTIER et al., 2013).\n\nStarPU is also a task-based runtime and it will be discussed in the next section.\n\n7.2 StarPU runtime system\n\nThe StarPU runtime system performs well on task graphs with less regular pat-\n\nterns (matrix factorization, n-body problems using the fast multipole method, sparse lin-\n\near algebra, h-matrix linear algebra). The main characteristics that may results in good\n\nor bad performance results are the amount of parallelism (if too few tasks are available\n\nfor scheduling due to dependences, this may result in a bottleneck) and the granularity of\n\ntasks which must be heavy enough to limit the scheduling overhead, but high enough to\n\nallow for a good load balancing (AUGONNET et al., 2011).\n\nCommunication costs are evaluated by sampling the bus transfer performance be-\n\ntween every pair of computing units (main CPU and accelerators). If several CPU sockets\n\nare available on the system, the transfer capability between each socket and each acceler-\n\n\n\n95\n\nator is measured to account for non-uniform memory access effects (if an accelerator is\n\ncloser from a given socket than from another socket). The results about bus performance\n\nsampling are stored in a file, for each computer node, and it is reused for subsequent runs\n\nunless a re-calibration is requested.\n\nThe StarPU runtime system developed by STORM Team provides a framework\n\nfor task scheduling on heterogeneous platforms. It is able to calibrate the relative perfor-\n\nmance of multiple, heterogeneous implementations of computing stencils, as well as the\n\ncost of data transfers between the main memory space and accelerator memory spaces,\n\nsuch as to optimize work mapping dynamically among heterogeneous computing units,\n\nduring the execution of the application. This scheduling framework jointly works with\n\na distributed shared-memory manager in order to optimize data transfers, to perform\n\nreplication and consistency management for avoiding redundant transfers, and to overlap\n\ncommunications with computations. A complete description of the available schedulers\n\ncould be found in (BORDEAUX; CNRS; INRIA, 2014). StarPU considers the following\n\nscheduling algorithms:\n\neager: A central queue from which all workers pick tasks concurrently.\n\nprio: A set of central queues, each associated with a priority level, from which all worker\n\npick tasks concurrently.\n\nws: A work-stealing scheduler, where idle workers may steal work from busy workers.\n\nrandom: A scheduler that maps work randomly among per-worker queues, according to\n\nthe assumed worker performance.\n\ndm: The Deque Model (DM) scheduler maps tasks onto workers using an history-based\n\nstencil performance model.\n\ndmda: An variant of the DM scheduler also taking transfer costs into account.\n\ndmdas: A variant of the DMDA scheduler such that per-worker task queues are sorted\n\naccording to the priority of tasks.\n\ndmdar: A variant of the DMDA scheduler such that per-worker task queues are sorted\n\naccording to the number of already available dependent pieces of data.\n\npeager: A variant of the eager scheduler with the ability to schedule parallel tasks on\n\nmultiple CPU cores.\n\n\n\n96\n\npheft: A variant of the DMDA scheduler with the ability to schedule parallel tasks on\n\nmultiple CPU cores.\n\n7.3 Elastodynamics over runtime system\n\nFinite-difference methods naturally express data parallelism whereas the model\n\nworks at tasks level. In order to express task-based parallelism with such a numerical\n\napproach, one needs to split the model into a fine-grained grid of blocks where each part\n\nof the code (mainly the update of the velocity and the stress components) is executed on\n\na single block as a computing task. Each block includes inner grid-points corresponding\n\nto the physical domain and outer ghosts zones for the grid points exchanged between\n\nneighboring subdomains (Figure 7.1). The three-dimensional domain is cut along the\n\nhorizontal directions as models in seismology often describe a thin and wide crust plate\n\nin order to evaluate surface effects. A naive implementation would require expensive\n\ncopies between blocks because the boundary data are not contiguous in memory (several\n\ncalls of memcpy, or cudaMemCpy, for small size data). Therefore the GPU RAM buffer\n\nwhich is filled using a CUDA kernel is created and then copied only once.\n\nFigure 7.1: Grid of blocks including inner grid-points corresponding to the physical do-\nmain and outer ghosts zones\n\nSource: (MARTINEZ et al., 2015b)\n\nThe task-based parallelism model leads to the creation of a large number of tasks,\n\nideally loosely coupled. These tasks could be arranged as a Directed Acyclic Graph\n\n(DAG) according to the data dependencies. Typically, it can identify the tasks devoted\n\nto data transfers (used to save data from block boundaries to buffers and vice versa) and\n\n\n\n97\n\nthe two computing kernels (computation of the six stress and the three velocity compo-\n\nnents). For each time step, the same pattern of task creation is repeated. At this stage,\n\nit has simplified the original implementation by discarding the absorbing boundary con-\n\nditions. As these numerical conditions introduce load imbalance at the boundaries of the\n\ncomputational domain, it makes much more complex the analysis of the results on hetero-\n\ngeneous platforms. The time spent in data management kernels is very small compared to\n\nthe elapsed time for the computation kernels. Nevertheless, these tasks are crucial as they\n\nexpress the dependencies between blocks that are adjacent in horizontal directions.\n\nFigure 7.2: Tasks dependency on a grid of 3\u00d73 blocks\n\nSource: (MARTINEZ et al., 2015b)\n\nFigure 7.2 illustrates this situation considering a grid of 3\u00d73 blocks. For instance,\n\nif a single task is scheduled on slow computing resources and the remaining tasks are\n\nexecuted on faster resources, the colored tasks cannot be scheduled before finishing the\n\nfirst one. The main programming effort has been the creation of the relevant CPU and the\n\nGPU kernels corresponding to the numerical scheme.\n\n7.4 Experiments\n\nIn this section, the architectures and configuration used for experiments are de-\n\nscribed. Experiments were executed on BRGM (Desktop) and GUANE-1 (HPC) nodes,\n\nand configuration of heterogeneous testbed are listed in Table 5.1 from Chapter 5.\n\nSeveral scenarios for each node have been created, based on the memory con-\n\nsumption in the GPU (in-core, the data domain fits into GPU RAM, and out-of-core, the\n\n\n\n98\n\ndata domain do not fit on GPU RAM) and the number of parallel tasks. The first example\n\nis based on a Cartesian mesh of an average of 2 million of grid points in order to fit in the\n\nmemory available on the GPU. For the out-of-core example, requiring several data trans-\n\nfers between global and GPU RAM, it was selected two different sizes of the problem,\n\nfor the commodity node was used a three-dimensional grid of approximately 32 million\n\nof points and 80 million of points for the HPC node. The other parameters (number of\n\ntasks for instance) strongly depends on the block size variable. Table 7.1 presents memory\n\nconsumption and the number of tasks to be scheduled.\n\nTable 7.1: Memory consumption, number of blocks and number of parallel task for the\nsimulated scenarios\n\nIn-core\nOut-of-core\n\nBRGM node GUANE-1 node\nMemory (GB) 0.18 2.71 6.57\nNumber of blocks 1 9 16\nComputation Tasks 40 200 600\nCommunication Tasks 0 424 1800\n\nExperiments were performed using several configurations in terms of processing\n\ncores usage. For pure GPU experiments, the model is simulated only using the GPU\n\ncores available on the target architecture. Symmetrically, the pure CPU executions only\n\ntarget x86 cores. The hybrid experiments tackle all the computing cores available (CPU\n\nand GPU cores). It is because StarPU runtime system has two flags for heterogeneous\n\ncores utilization (workers): STARPU_NCPU to define the number of CPU cores and\n\nSTARPU_NCUDA to define the number of GPU cards to be used on the computation of\n\nstencils. According to each node, the number of cores for CPU computation corresponds\n\nto the number of available physical cores (each GPU need a CPU core to management).\n\nThe STARPU_CUDA_ASYNC flag enables concurrent stencil execution on graphics cards\n\nthat support this feature (BORDEAUX; CNRS; INRIA, 2014). In this case, this flag is\n\navailable on both machines.\n\n7.4.1 Scheduling strategies\n\nOne of the key contributions of StarPU runtime system is to provide several schedul-\n\ning algorithms adapted to various computing load and hardware heterogeneity. In this\n\nsection, several different schedulers are compared. Figure 7.3 shows the speedup over\n\nthe worst result on each platform. On the commodity-based platform, it considers one\n\n\n\n99\n\nGPU and three CPU cores. In this case, the best results are provided by the DMDAR\n\nalgorithm that takes into account the cost of data transfers. Eager and Work Stealing\n\nalgorithm appear like alternatives to DMDAR. These two algorithms do not use informa-\n\ntion from the memory bus. The rather good results underline the limited contention at\n\nthe memory bus level for this configuration. The situation is rather different on the HPC\n\nnode. The best results are also obtained with the DMDAR algorithm but the performance\n\nwith the other schedulers is very poor. In this case, it uses eight GPU and four CPU\n\ncores and data transfers optimization is critical, especially for multi-accelerator platforms\n\nwith major bottlenecks. The scheduling strategy can be selected by the STARPU_SCHED\n\nenvironment variable.\n\nFigure 7.3: Impact of the scheduling algorithms for experiments on heterogeneous\nplatforms. Relative speedup over the worst situation on each platform.\n\nCommodity node HPC node\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n1.00\n\n7.47\n\n2.68\n\n12.63\n\n2.41\n\n1.00\n\n2.50\n\n1.02\n\nDM DMDAR EAGER WS\n\nSource: (MARTINEZ et al., 2015b)\n\n7.4.2 Size of the block\n\nSelecting the best granularity is crucial to maximizing the performance. Consider-\n\ning heterogeneous platforms, this parameter is of great importance. Figure 7.4 shows the\n\nspeedups over one CPU core for in-core and out-of-core data set. The results have been\n\nobtained on the commodity node. It can be observed that depending on the hardware con-\n\nfiguration, the most efficient granularity may vary. In both cases, the efficiency with one\n\nGPU is optimal with a block size of 256. When it decreases the size of the block the GPU\n\nefficiency is significantly reduced (from \u00d717.3 to \u00d74.3 for in-core problems). This is\n\nbecause GPU architecture could deliver an optimal level of performance when all stream\n\nprocessors are used. Then, it means that larger block performs better on such architecture.\n\n\n\n100\n\nThe situation is rather different on CPU platforms as tiny blocks could improve\n\nlocality and cache effects. In this case, using a large number of blocks is also mandatory\n\nto extract enough concurrency from the task-based algorithm. Indeed, the wavefront de-\n\ncomposition reaches a good level of efficiency with blocks of the size equal to 64 or less\n\nfor the in-core problem. For the out-of-core problem, it generates enough tasks to benefit\n\nfrom the four CPU cores in all cases. The bigger problem size corresponding to more\n\ncomputing tasks explains this result.\n\nFigure 7.4: Impact of the granularity on the efficiency of seismic wave modeling on\nGPUs+CPUs.\n\n1 CPU 4 CPU 3 CPU - 1 GPU 1 GPU\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n18\n\n20\n256 128 64 32\n\n(a) In-core\n\n1 CPU 4 CPU 3 CPU - 1 GPU 1 GPU\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n256 128 64 32\n\n(b) Out-of-core\n\nSource: (MARTINEZ et al., 2015b)\n\nIndeed, the choice of the appropriate granularity relies on a tradeoff between the\n\nperformance level of the computing tasks depending on their sizes and the suitable number\n\nof tasks necessary to provide enough parallelism. Obviously, the optimal configuration\n\nis not always possible depending on the overall size of the domain and speedup ratio\n\nbetween the processing units.\n\n7.4.3 In-core dataset\n\nIn this section, the overall performance is analyzed considering a problem size that\n\nfit in the GPU memory. Obviously, this situation is not the most suitable to benefit from\n\nthe heterogeneous implementation as the price to pay for data transfers between the CPU\n\nand the GPU may overcome expected gains from the additional CPU cores.\n\nTable 7.2 shows the results obtained on the commodity computing node (the elapsed\n\ntime for pure CPU cores is the baseline). Firstly, it can be noticed that the speedup mea-\n\nsured with one GPU over four CPU cores (\u00d75.03) is in the same order of magnitude of\n\n\n\n101\n\nTable 7.2: Speedup on the commodity-based hardware configuration (in-core dataset)\nover multicore execution\n\npure GPU hybrid (dmdar) hybrid (ws)\nSpeedup \u00d75.03 \u00d75.02 \u00d70.42\n\nthe results reported in (ABDELKHALEK et al., 2012; MARTINS et al., 2014; MICIKE-\n\nVICIUS, 2009). In this case, the overhead coming from StarPU runtime system is limited\n\nas the number of blocks is very small and the computing stencils are solely scheduled on\n\nthe GPU.\n\nHybrid simulations are supposed to exploit both GPU and CPU cores with the\n\nDMDAR scheduler that takes into account the cost of data transfer. Indeed, this strategy\n\nschedules all the computing tasks on the GPU because of the high cost of data transfers.\n\nAs a result, the similar level of performance is observed if compare to the pure GPU im-\n\nplementation. In order to force the usage of both type of cores, the scheduling policy was\n\nchanged using the work-stealing algorithm. This strategy simply implements a situation\n\nwhere idle workers steal work from busy workers. The results are very poor as the original\n\nand elapsed time is almost multiplied by a factor of \u00d72.3.\n\nFigure 7.5 shows the results for the HPC computing node (the twelve CPU cores\n\nresults are the baseline). Similarly to the previous results, the speedup measured with one\n\nGPU appears consistent with the scientific literature (\u00d76.94). The impact of the Tesla\n\ncard available on the HPC node is significant, a ratio of 4.27 is observed between the pure\n\nGPU results on these two architectures. Previous remarks on the DMDAR and the work-\n\nstealing algorithm remain also valid. In this case, a stronger degradation of the speedup is\n\nnoticed when using all the CPU cores compared to previous experiments. This probably\n\ncould be explained by the higher level of performance of the GPU on this machine. The\n\nsituation is even worst when eight GPU and four CPU cores are used. The elapsed time\n\nis increased by more than a factor due to the data transfers and the poor usage of the\n\navailable resources. The granularity of the problem also plays an important role in this\n\ndegradation.\n\n7.4.4 Out-of-core dataset\n\nThis section discusses the results on heterogeneous architectures when the size of\n\nthe data exceeds the memory available on the GPU. In this case, the accelerators are fully\n\nused as additional computing resources to the computing power delivered by the CPU\n\n\n\n102\n\nFigure 7.5: Speedup on the HPC node (in-core dataset) over multicore execution\n\n6.94 \n\n6.24 \n\n0.11 0.09 \n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\npure GPU 1 GPU + 11 CPU\n(DMDAR)\n\n1 GPU + 11 CPU\n(WS)\n\n8 GPU + 4 CPU\n\nSource: (MARTINEZ et al., 2015b)\n\ncores. Data transfers are of great importance and should be carefully controlled by the\n\nscheduling strategies.\n\nPerformance obtained on the commodity node is detailed in Table 7.3. Four CPU\n\ncores results are used as a baseline. With one GPU, the simulation is slowed down in\n\ncomparison with the baseline configuration. It is because of the price of data movements\n\nbetween the CPU and the GPU main memory. Indeed, the idle time ratio for the GPU\n\ncores reaches a maximum of 80.16% preventing any acceleration over the pure CPU ver-\n\nsion.\n\nTable 7.3: Speedup on the commodity-based configuration (out-of-core dataset) over mul-\nticore execution\n\npure GPU hybrid (4 CPU cores and 1 GPU)\nSpeedup \u00d70.92 \u00d71.32\n\nThe heterogeneous results are slightly better with a speedup of \u00d71.32. The best\n\nresults are obtained with the DMDAR scheduler that takes into account the cost of data\n\ntransfers. Contrary to the results obtained with the in-core dataset, the fine-grained of the\n\ndecomposition of the large problem (at least 624 tasks) allows to schedule the comput-\n\ning load on all the computing resources available. Nevertheless, the usage of the cores\n\nremains low. The idle time for the three CPU cores varies between 41% and 79%, with\n\nmaximum value for the GPU.\n\nResults on the HPC node are summarized in Figure 7.6. All configurations show\n\na speedup over the baseline results. For the pure GPU experiments, the acceleration re-\n\nported (\u00d71.72) demonstrates the benefits from task-based programming even when only\n\nthe GPU card is used. In comparison with the commodity-based architecture, it bene-\n\nfits from larger bandwidth at the I/O bus level. This value is still far from the average\n\n\n\n103\n\nacceleration reported when the problem fits in the GPU memory (\u00d76.94).\n\nCombining one GPU and the eleven remaining CPU cores leads to an increase of\n\nthe speedup compared with the GPU only configuration. In this case, our implementation\n\nis able to smoothly benefits from this additional computing power by almost doubling the\n\nperformance level obtained with one GPU. Using both GPU and CPU cores could pro-\n\nvide more flexibility for the scheduling of the computing tasks. As a side effect, it also\n\nobserved a better usage of the GPU resources. The multi-GPU results confirm this trend\n\nwith a maximum speedup of \u00d725.22. Considering the various parameters that need to be\n\ntaken into account, any tentative of exhaustive scalability analysis would be false. Indeed,\n\nthis result should be compared to the corresponding speedup for the in-core dataset with\n\none GPU (\u00d76.94) to understand the remaining effort to fully optimized our implementa-\n\ntion on multi-GPU.\n\nFigure 7.6: Speedup for out-of-core dataset when running on the HPC node over multicore\nexecution.\n\n1.72 \n\n3.53 \n\n25.22 \n\n0.00\n\n5.00\n\n10.00\n\n15.00\n\n20.00\n\n25.00\n\n30.00\n\npure GPU 1 GPU + 11 CPU 8 GPU + 4 CPU\n\nSource: (MARTINEZ et al., 2015b)\n\n7.5 Summary of runtime parameters\n\nAt this point, we found that proposed implementation for heterogeneous archi-\n\ntectures is also influenced by an input configuration set. The parameters are determined\n\nby environment variables defined by StarPU as STARPU_NCPU, STARPU_NCUDA and\n\nSTARPU_SCHED, analogously to the environment variables in OpenMP, and changing\n\nthese parameters allows to improve the performance of the seismic stencil kernel. In this\n\nsense, the proposed implementation reach a well-performed execution when compared to\n\nmulticore architectures. The input parameters are listed in Table 7.4.\n\n\n\n104\n\nTable 7.4: List of runtime parameters that affect the performance of task-based imple-\nmentation on heterogeneous architectures\n\nParameter Description\n\nNumber of GPUs StarPU allows to use a multi-GPU cluster, this value is determined by\n\nSTARPU_NCUDA environment variable.\n\nNumber of CPU cores It depends on number of free CPU cores, because each GPU needs one\n\nCPU core for its management, and describes the available CPU cores\n\nto compute tasks, this value is determined by STARPU_NCPU environ-\n\nment variable\n\nTask size This parameter determines the number of points to be solved by each\n\ntask.\n\nNumber of tasks It depends on size of problem to be solved and can be defined by a rate\n\nbetween the data domain, the whole 3D grid, and the task size.\n\nScheduling algorithm The scheduling strategy distribute the tasks into CPUs and GPUs cores,\n\nit can be selected by STARPU_SCHED environment variable.\n\nMemory consumption Two situations are considered: in-core data, when the data domain from\n\ncomplete 3D grid fits on DRAM; out-of-core data, when the data do-\n\nmain requires several data transferences.\n\n7.6 Task-based implementation for energy efficiency\n\nAs a supplementary result, one of the advantages of task-based implementation\n\nis that can be executed on recent heterogeneous architectures oriented to energy saving.\n\nThe objective of this section is to demonstrate that Nvidia Jetson manycore architecture\n\nrepresents an alternative for energy efficient seismic wave modeling. We used a sim-\n\nulation scenario for the three-dimensional model that includes a Cartesian mesh of 7.2\n\nmillion grid points (300\u00d7300\u00d780). The memory consumption for this problem is 625\n\nMB of memory. Based on StarPU task-based implementation, the two computing stencils\n\n(velocity and stress components) must be scheduled to use 36 parallel computing tasks\n\nand 144 parallel communication tasks (data sharing between blocks). DMDAR sched-\n\nuler maps tasks onto workers using an history-based stencil performance mode such that\n\nper-worker task queues are sorted according to the number of already available depen-\n\ndent pieces of data. Experiments were executed on BRGM, Guane-1 and Tegra K1 nodes\n\nlisted in Table 5.1, from Chapter 5. In this case, we used three metrics to discuss the\n\nperformance:\n\n\n\n105\n\n\u2022 Time-to-solution (execution time for the simulation)\n\n\u2022 Energy consumption (using platform sensors by NVPROF).\n\n\u2022 Energy efficiency (FLOPS/Watt)\n\n7.6.1 Computing time\n\nThe results described in Figure 7.7 confirm the first assumption, Nvidia Jetson\n\nboard exhibits poor performance considering the time-to-solution metrics. The Guane-1\n\nnode is \u00d72.05 faster than the Nvidia Jetson board. For the commodity-based architecture,\n\nthe ratio is \u00d71.59x. This is mainly coming from the different levels of performance of the\n\nGPU available on each platform.\n\nFigure 7.7: Comparison of Time-to-solution metrics for Tegra K1 (Jetson), Guane-1 (HPC\nserver) and BRGM node (Desktop platform).\n\nTi\nm\n\ne \n(s\n\nec\n)\n\n0\n\n7.5\n\n15\n\n22.5\n\n30\n\nJetson Desktop HPC Server\n\n7.03\n11.96\n\n9.09\n\n6.204.98\n\n17.97\n\nProcessing Communication\n\nSource: (MARTINEZ et al., 2015a)\n\nIf we considered the ratio between computation time (tasks to solve velocity and\n\nstress stencils) and communication time (tasks to transfer data between global RAM and\n\nGPU RAM, and thread synchronization) Jetson board is processing 66.40% of the time,\n\nwhile desktop and server are processing only 29.39% and 53.11% of time respectively.\n\nThis is because the Jetson has a shared memory between CPU cores and GPU. As a result,\n\nit only pays the costs of threads synchronization of GPU stencils and CPU functions. Due\n\nto the size of the test-case (625 MB) and the speedup ratio between the GPU and the\n\nCPU cores, StarPU uses both CPU and GPU cores to schedule the computing tasks on the\n\nNvidia Jetson board. For the desktop machine and the server node, the runtime system\n\n\n\n106\n\nonly schedules the computation tasks on the GPU and the remaining communication tasks\n\non the CPU cores.\n\n7.6.2 Energy efficiency\n\nWe also analyzed the energy consumption. For Jetson board we assumed that\n\npower consumption corresponds to 4W because we can not measure it, this value repre-\n\nsents an upper-bound when CPU and GPU cores are working and interface ports (USB,\n\nHDMI) are disabled (NVIDIA, 2014a); for desktop and server machines power measures\n\nare measured from Nvidia Profiler. For the computation phase, power consumption is\n\n83.40W and 123.49W for the desktop machine and the server node respectively.\n\nIf we compared energy consumption presented on Figure 7.8 (left), Jetson board\n\nreduces the energy consumption by a factor \u00d712.21 in comparison with the server node\n\nand by a factor \u00d72.08 with respect to the desktop machine. Obviously, the desktop ma-\n\nchine is tuned for energy-efficiency contrary to high-end HPC node.\n\nFigure 7.8: Energy measures for the earthquake modeling on three heterogeneous archi-\ntectures.\n\nE\nne\n\nrg\ny \n\n(J\nou\n\nle\ns)\n\n0\n\n225\n\n450\n\n675\n\n900\n\nJetson Desktop HPC Server\n\n867.77\n\n415.16\n\n71.89\n\n(a) Consumption\n\nE\nffi\n\nci\nen\n\ncy\n (M\n\nFL\nO\n\nP\nS\n\n/W\n)\n\n0\n\n0.75\n\n1.5\n\n2.25\n\n3\n\nJetson Desktop HPC Server\n\n0.89\n0.46\n\n2.65\n\n(b) Efficiency\n\nSource: (MARTINEZ et al., 2015a)\n\nFinally, the flops/watt ratio for each platform (Figure 7.8, right) was measured.\n\nJetson board is \u00d75.82 more efficient than desktop machine and \u00d72.99 than a standard\n\nHPC node. One important remark should be underlined at this stage. The problem under\n\nstudy is tailored to fit in the memory available on the Nvidia Jetson Card (2 GB). This\n\nparameter is of great importance to discuss the overall performance as the size of the\n\nproblem (and the number of blocks) could significantly influence the speedup on GPU\n\narchitectures (MICH\u00c9A; KOMATITSCH, 2010; MARTINEZ et al., 2015b).\n\n\n\n107\n\n7.7 Concluding remarks\n\nWe introduced a task-based implementation and analyzed its performance, with\n\nthis approach we obtained a significant performance improvement when compared to mul-\n\nticore architectures. We demonstrated that changes on input configuration set at runtime,\n\nas scheduling policies combined with different task sizes, may considerably affect the\n\nefficiency and performance of the seismic wave kernels (MARTINEZ et al., 2015b). Re-\n\nvisiting the standard data-parallelism arising from the finite-difference numerical method,\n\nstencil computations, therefore, benefit from StarPU runtime system in order to smoothly\n\nschedule the DAG (Directed Acyclic Graph) on the current heterogeneous platform. In\n\nthis way, the numerical algorithm and the underlying architecture are decoupled by fully\n\nexploiting the versatility of modern runtime systems. This approach allows tackling the\n\ncomplexity of this memory-bound problem by minimizing data movements to and from\n\nthe accelerators.\n\nThe efficiency of stencil computation on two heterogeneous architectures, using\n\nthe maximum number of processing units available, consider a real problem that could\n\nnot fit in the DRAM memory, for example, a maximum speedup of 25.22 using four CPU\n\ncores and eight GPU in comparison with the same experiments using twelve CPU cores.\n\nUsing a commodity-based architecture with four CPU cores and one GPU was obtained\n\nan acceleration of 32% over the CPU version. The analysis of the performance underlines\n\nthe significant impact of the granularity and the scheduling strategy.\n\nA detailed comprehension of the tradeoff between the granularity and the schedul-\n\ning policies is necessary in order to tackle more complex architecture. For instance, the\n\nregularity of the finite-differences numerical method could allow deriving a cost-model\n\nthat could help the runtime system to improve task scheduling. This includes potential\n\nirregularity in the granularity of the task to maximize efficiency on multiple devices. In-\n\ncluding absorbing boundary conditions generates load imbalance that could be smoothly\n\nreduced by the fine-grained task-based programming model. Difficulties on this model\n\nare related to runtime configuration (i.e., scheduling algorithm, size and number of tasks,\n\nchoosing of processing units).\n\nAs an additional advantage of the proposed task-based implementation, a low-\n\npower manycore architecture can be an alternative to compute stencils of seismic model-\n\ning with a better energy efficiency, better usage of the available resources (CPU and GPU\n\ncores) and a significant reduction of the communication cost (33.60%). Good results in\n\n\n\n108\n\nterms of energy-to-solution compared to common HPC systems are possible. But the\n\nsize of problems that could be tackled is limited by the amount of memory available on\n\nthese boards. Finally, the emerging of integrated cores architectures that deliver higher\n\nbandwidth between GPU and CPU appears like an opportunity to tackle both the PCIe\n\nbottleneck and the energy-efficiency challenge.\n\n\n\n109\n\n8 RELATED WORK\n\nIn this chapter, we present the several works that are related to performance op-\n\ntimization of stencil computations. A large scientific literature has been devoted to the\n\nadaptation of stencil algorithms on HPC architectures. It involves from hardware im-\n\nplementations, low-level optimization, compiler flags, application tuning, programming\n\nof new algorithms, ML models to improve the performance of these applications, and\n\ntask-based implementations of numerical stencils.\n\n8.1 Perfomance improvement of stencil applications on multicore architectures\n\nParallelism and HPC are the basis to develop the stencil algorithm and multicore\n\narchitectures were extensively used to improve the performance of these computations.\n\nPerformance improvement can be reached by algorithm implementations and cache op-\n\ntimization is a common way to reach a better performance on multicore architectures.\n\nThese works oriented the performance characterization presented in Chapter 3.\n\nIn (DATTA et al., 2009), the authors use a methodology to optimize stencil com-\n\nputations for multiple architectures (multicore and accelerators). Their work reuse cache\n\nand methodologies across single and multiple stencil sweeps, examining cache-aware al-\n\ngorithms as well as cache-oblivious techniques. Their results demonstrated that recent\n\ntrends in the memory system organization have reduced the efficacy of traditional cache-\n\nblocking optimizations.\n\nIn (DUPROS; DO; AOCHI, 2013), the authors review the scalability issues of\n\nseismic wave propagation numerical kernels on x86 architectures, they have underlined\n\nthe limitations coming from the fine-grained parallelism leading to a degradation of the\n\nload balance.\n\nIn (MALAS et al., 2015), the authors combine the multicore temporal blocking\n\nand a diamond tiling to reduce the memory pressure, the results show performance ad-\n\nvantages in bandwidth-starved situations.\n\nIn (DUPROS et al., 2015) the authors also implement stencil algorithms focused\n\non cache improvement by spacetime blocking. The trend for these HPC applications is to\n\npay a higher cost in order to optimize the overall performance. In this work is explained\n\nthe Naive and Space tiling algorithm implementations analyzed in our research.\n\nIn (SAXENA; JIMACK; WALKLEY, 2016) we found another work related to\n\n\n\n110\n\ncache optimization, the authors optimize the cache reuse for stencil-based PDE discretiza-\n\ntions by a domain decomposition that minimizes cache misses in structured 3D grids.\n\nIn terms of regression models and performance prediction applied to multicore ar-\n\nchitectures, the impact of different optimizations is difficult to predict due to the influence\n\nof load imbalance, synchronization overhead, and cache locality.\n\nIn (RAHMAN; YI; QASEM, 2011), the authors present a performance study for\n\nstencil computations on the Intel Nehalem multicore architecture, they model the overall\n\nperformance of differently optimized code based on the impact of optimizations on in-\n\ndividual architectural components measured by hardware counters, and apply regression\n\nanalysis to a large collection of empirical data to derive the model and verify the precision\n\nof the approach.\n\nIn (STENGEL et al., 2015), the authors use the Execution-Cache-Memory (ECM)\n\nmodel, it delivers a prediction of the number of CPU cycles required to execute a cer-\n\ntain number of iterations nit of a given loop on a single core, and use this methodology\n\nto quantify the performance bottleneck of stencil algorithms on an Intel SandyBridge\n\nprocessor, and they study the impact of typical optimization approaches such as spatial\n\nblocking, strength reduction, and temporal blocking.\n\n8.2 Advanced optimizations, low-level and auto-tuning strategies\n\nApplication tuning represents a classical methodology to improve the performance\n\non multicore architectures. Finding the optimal value for each parameter requires to\n\nsearch on a large set of configurations and several heuristics or frameworks have been pro-\n\nposed to speed up the process of finding the best configuration for scientific applications.\n\nUnfortunately, application tuning leads to the exploration of a huge set of parameters, thus\n\nlimiting its interest on complex platforms.\n\nIn (DURSUN et al., 2009), the authors propose a multilevel parallelization frame-\n\nwork that combines inter-node parallelism by spatial decomposition, intra-chip paral-\n\nlelism through multithreading, and data-level parallelism via single-instruction multiple-\n\ndata (SIMD) techniques.\n\nIn (MERCERAT; GUILLOT; VILOTTE, 2009), the authors present an efficient\n\nnumerical spacetime decomposition for acoustic wave propagation based on the Parareal\n\nalgorithm, it is based on a decomposition of the time interval in time slices. It involves a\n\nserial prediction step based on a coarse approximation, and a correction step (computed\n\n\n\n111\n\nin parallel) based on a fine approximation within each time slice.\n\nIn (KAMIL et al., 2010), the authors present a stencil auto-tuning framework for\n\nmulticore architectures that converts a sequential stencil expression into tuned parallel\n\nimplementations. The algorithm creates a large space of preceding optimizations, run\n\neach one of these combinations and reports fastest parameter combination. Overall, the\n\nmain problem of these works is that the search domain can be very large and searching\n\nthe best configuration would take too much time.\n\nIn (CRUZ; ARAYA-POLO, 2011), the authors predict the performance behavior\n\nof stencil computations by using a model based on cache misses and prefetching. They\n\ncreate one configuration vector and one performance vector and use (BACH; JORDAN,\n\n2003) to obtain auto-tuned best configuration.\n\nIn (TANG et al., 2011), the authors present the Pochoir framework, it allows a pro-\n\ngrammer to write a simple specification of a stencil in a domain-specific stencil language\n\nembedded in C++ which the Pochoir compiler then translates into high-performing Cilk\n\ncode that employs an efficient parallel cache-oblivious algorithm.\n\nIn (CHRISTEN; SCHENK; CUI, 2012), the authors introduce a code generation\n\nand auto-tuning framework for stencil computations targeting modern multi and many-\n\ncore processors. The goals of the framework are productivity and portability for achieving\n\nhigh performance on the underlying platform.\n\nIn (MIJAKOVIC; FIRBACH; GERNDT, 2016), the authors develop a tuning frame-\n\nwork that integrates and automates performance analysis and performance tuning. They\n\nintroduce the Periscope Tuning Framework (PTF), a flexible plugin mechanism and pro-\n\nvides tuning plugins for various different tuning aspects. The output of the framework is\n\ntuning recommendations that can be integrated into the code.\n\nIn (BREUER; HEINECKE; BADER, 2016), the authors present a detailed de-\n\nscription of a clustered local time stepping scheme for the seismic simulation package\n\nSeisSol and optimize the performance by clustering elements of a similar time step. They\n\nturned the experiments on the SuperMUC Phase 2 (TOP500, 2017).\n\n8.3 Machine Learning approaches\n\nBecause ML is a methodology for optimization that could be applied to find pat-\n\nterns on a large set of input parameters, recent works use this approach to improve the\n\nperformance and to build regression models of HPC applications. Many of these works\n\n\n\n112\n\nuse cache-related metrics.\n\nIn (RAI et al., 2009), the authors compare ML algorithms for characterizing the\n\nshared L2 cache behavior of programs on multicore processors. The results show that\n\nregression models trained on a given L2 cache architecture are reasonably transferable to\n\nother L2 cache architectures.\n\nIn (GANAPATHI et al., 2009) the authors apply ML techniques to explore stencil\n\nconfigurations (code transformations, compiler flags, architectural features and optimiza-\n\ntion parameters). Their approach is able to select a suitable configuration that gives the\n\nbest execution time and energy consumption.\n\nIn (EOM et al., 2013), the authors compare workloads of applications executed on\n\nthe cloud, they use several ML techniques (Trees, Neural Networks, Bayesian methods)\n\nto improve scheduling decisions in mobile offloading. Parameters in input set are local\n\nexecution time, size of data to be transferred, network bandwidth and the number of\n\ninvocations for an argument setup.\n\nIn (PUSUKURI; GUPTA; BHUYAN, 2013), the authors introduce a framework\n\nfor co-scheduling of simultaneous programs based on supervised learning techniques for\n\nidentifying the effects of the interference between multithreaded programs on their per-\n\nformance, a statistical model is trained to predict similar output values when similar input\n\nvalues are observed.\n\nIn (WENG; LIU; GAUDIOT, 2013) the authors propose a dynamic scheduling\n\npolicy based on a regression model to ensure that is capable of responding to the changing\n\nbehaviors of threads during execution, their scheduling policy could achieve up to 29%\n\nspeedup.\n\nIn (SUKHIJA et al., 2014), the authors improve the performance by selecting a\n\nportfolio dynamic loop scheduling (DLS) using supervised ML techniques to build em-\n\npirical robustness prediction models that are used to predict DLS algorithm\u2019s robustness\n\nfor given scientific application characteristics and system availabilities.\n\nAnd in (CRUZ; ARAYA-POLO, 2015), the authors improve the performance of\n\nstencil computations by using a model based on hardware counter behavior and ML. They\n\nhave included several features in the model such as multi and many-core support, hard-\n\nware prefetching modeling, cache interference and capacity misses and other optimization\n\ntechniques such as spatial blocking and Semi-stencil. If we compare our proposed ML\n\nmodel with this work we have obtained a better accuracy of the prediction results.\n\n\n\n113\n\n8.4 Heterogeneous computing\n\nSeismic wave modeling faces a major challenge to exploit current heterogeneous\n\nsystems. Moreover, every vendor is actually working on next generation of machines that\n\nwill drive the community to the Exascale, with millions of heterogeneous cores. Several\n\nstrategies, frameworks, and programming models have been proposed mainly to optimize\n\nCPU and GPU implementations.\n\nIn (AUGONNET et al., 2011), the authors introduce a runtime system for hetero-\n\ngeneous platforms called StarPU and based on the integration of the data-management\n\nfacility with a task execution engine. It includes a high-level description of every piece\n\nof data manipulated by the task and how they are accessed, it also offers a low-level\n\nscheduling mechanism, as we presented in Chapter 7, so that scheduler programmers can\n\nuse them in a high level.\n\nIn (DURAN et al., 2011), the authors introduce OmpSs, a programming model\n\nbased on OpenMP and StarSs, that can also incorporate the use of OpenCL or CUDA\n\nkernels. It implements the data dependencies and offers asynchronous parallelism in the\n\nform of tasks. A task can be annotated with data directionality clauses that specify the\n\ndata used by it, and how it will be used (read-only, write-only, read-write)\n\nIn (GAUTIER et al., 2013), the authors introduce the XKaapi for data-flow task\n\nprogramming model on heterogeneous architectures, a tasking API that provides numer-\n\nical kernel designers with a convenient way to execute parallel tasks over the heteroge-\n\nneous hardware on the one hand, and easily develop and tune scheduling algorithms on\n\nthe other hand.\n\nIn (VASUDEVAN; VADHIYAR; KAL\u00c9, 2013), the authors introduce G-Charm,\n\na generic framework with an adaptive runtime system for efficient execution of message-\n\ndriven parallel applications on hybrid systems. The framework is based on a message-\n\ndriven programming environment. It includes dynamic scheduling of work on CPU and\n\nGPU cores, maximizing reuse of data present in GPU memory, data management in GPU\n\nmemory, and combining multiple kernels.\n\nIn (BOSILCA et al., 2013a), the authors introduce PaRSEC, an approach based\n\non task parallelism. This strategy allows the algorithm to be decoupled from the data\n\ndistribution and the underlying hardware, the algorithm is expressed as flows of data.\n\nPaRSEC is an event-driven system. When an event occurs, such as task completion, the\n\nruntime reacts by examining the data flow to discover what future tasks can be executed\n\n\n\n114\n\nbased on the data generated by the completed task.\n\nIn (LACOSTE et al., 2014), the authors compare StarPU and Parsec runtime sys-\n\ntems, they conclude that both runtime systems are able to benefit from heterogeneous\n\nplatforms with comparable levels of performance.\n\nThe runtime systems can be also defined for code optimization such as in BOAST,\n\nit applies an automatic S2S transformation to optimize the performance of HPC archi-\n\ntectures (CRONSIOE; VIDEAU; MARANGOZOVA-MARTIN, 2013; VIDEAU et al.,\n\n2018).\n\nSeveral references implement stencil applications on these architectures and these\n\nruntimes, but most of these works don\u2019t exploit all the computing resources available or\n\ndepend on a low-level programming approach.\n\nIn (MICIKEVICIUS, 2009), the authors describe a GPU parallelization approach\n\nfor the 3D finite difference stencil computation, and they also describe the approach for\n\nutilizing multiple GPUs to solve the problem, achieving linear scaling with GPUs by\n\nusing asynchronous communication and computation.\n\nIn (MICH\u00c9A; KOMATITSCH, 2010), the authors describe the implementation\n\nof the code in CUDA to simulate the propagation of seismic waves in a heterogeneous\n\nelastic medium. They also implement convolution perfectly matched layers on GPUs\n\nto efficiently absorb outgoing waves on the fictitious edges of the grid. We used this\n\nimplementation to analyze the standard implementation in Chapter 6\n\nIn (AGULLO et al., 2011a; AGULLO et al., 2011b), the authors implement LU\n\nand QR decomposition algorithm for heterogeneous architectures exploiting task-based\n\nparallelism on top of the StarPU runtime system and present two alternative approaches,\n\nrespectively based on static and dynamic scheduling.\n\nIn (ABDELKHALEK et al., 2012), the authors design a fast parallel simulator that\n\nsolves the acoustic wave equation on a GPU cluster. They considered a finite difference\n\napproach on a regular mesh, in both two dimensional and three dimensional cases, and\n\nstudied different implementations and their impact on the application performance\n\nIn (CALANDRA et al., 2013) the authors evaluate a 3D finite difference stencil,\n\nthat is optimized and tuned in OpenCL, executed on CPUs, APUs, and GPUs. Their\n\nresults show that APU integrated GPUs outperform CPUs and that integrated GPUs of\n\nupcoming APUs may match discrete GPUs for problems with high communication re-\n\nquirements.\n\nIn (CASTRO et al., 2014), the authors analyzed the use of a low-power manycore\n\n\n\n115\n\nprocessors for seismic wave propagation simulations, they look at its characteristics such\n\nas limited amount of on-chip memory and describe the solution to deal with the processor\n\nfeatures, and compared the performance and energy efficiency of a seismic wave prop-\n\nagation model on MPPA-256 to other commonplace platforms such as general-purpose\n\nprocessors and a GPUs.\n\nIn (BOILLOT et al., 2014), the authors study the applicability of task-based pro-\n\ngramming in the case of a Reverse Time Migration (RTM) application for Seismic Imag-\n\ning. The initial MPI-based application is turned into a task-based code executed on top of\n\nthe PaRSEC runtime system. Their results show that the approach can exploit much more\n\nefficiently complex hardware such as the Intel Xeon Phi accelerator.\n\nIn (ROTEN et al., 2016), the authors have implemented a seismic model in both\n\nthe CPU and GPU versions. The optimized CUDA kernels utilize the GPU memory\n\nbandwidth more efficiently and the application has resulted in a significant increase of\n\nperformance and accuracy for simulations in realistic earth structures. They turned the\n\nexperiments on the NCSA Blue Waters and the OLCF Titan (TOP500, 2017).\n\nIn (TSUBOI et al., 2016), the authors realize large-scale computations by opti-\n\nmizing a widely used community software code (SPECFEM3D_GLOBE) to efficiently\n\naddress all hardware parallelization, especially thread-level parallelization to solve the\n\nbottleneck of memory usage for coarse-grained parallelization. They perfomed the exper-\n\niments on the K computer (TOP500, 2017).\n\n\n\n116\n\n9 CONCLUSION AND PERSPECTIVES\n\nScientific applications have been developed to understand the physical phenom-\n\nena. In the case of geological studies, strong motion models have been used to mitigate\n\nthe risk of building damages derivated from earthquakes. Moreover, geophysics explo-\n\nration remains fundamental to the modern world to keep up with the demand for energetic\n\nresources, oil and gas industries rely on software as an economically viable way to reduce\n\nproduction costs. The fundamentals of many software mechanisms for geophysics are\n\nbased on simulation engines. For instance, on seismic imaging, geological modeling,\n\nmigration and inverse problems use simulators of wave propagation at the core.\n\nWave propagation model approximations are the current backbone for many geo-\n\nphysics simulations. These simulation engines are built based on PDEs solvers. The\n\nPDEs in each case define the accuracy of the approximation to the real physics when a\n\nwave travels through the earth. It has been extensively applied for earthquake modeling\n\nand imaging potential of oil and gas reservoirs, for the last years.\n\nThe most common numerical model used to solve the PDEs is the FDM method,\n\nit also lies at the heart of a significant fraction of numerical solvers in other fields (i.e.,\n\nfluids dynamic, or climate modeling); and solving the geophysics simulation models from\n\nan FDM method requires a huge quantity of computations. With this in mind, HPC ar-\n\nchitectures are exploited to develop the geophysics applications and the overall parallel\n\nmethodology is based on a classical Cartesian grid partitioning with the exchange of in-\n\nformation on common edges.\n\nOne major challenge in HPC applications is to obtain the optimal performance.\n\nThe trend at the hardware level is to increase the complexity of available computing node.\n\nThis includes several levels of hierarchical memories, increasing number of heteroge-\n\nneous cores or low-level optimization mechanisms. Another concern is coming from the\n\nincreasing gap between the computing power and the cost of data transfers.\n\nAt the software level, the challenge is to develop as much as possible algorithm\n\nimplementations independent of the knowledge of the underlying architecture. Many\n\nprogramming models that have been developed are oriented into the architecture. Princi-\n\npal methodologies are supported in shared-memory and message passing, for multicore\n\narchitectures, and streaming multiprocessing, for heterogeneous architectures. But, the\n\nevolution of HPC paradigms leads to progressively re-design the current applications that\n\nmainly exploit standard programming models.\n\n\n\n117\n\nAdditionally, in spite of the good speedups usually reported, the performance ob-\n\ntained with standard implementations of geophysics models on HPC nodes could remain\n\nnot optimal from the best performance. In this context, the performance improvement\n\nimplies to search in a large set of programming models, runtime configurations, and ar-\n\nchitectural features.\n\nConsequently, this research was addressed on the hypothesis that performance op-\n\ntimization of geophysics applications can be done by searching the optimal input config-\n\nuration set, at runtime. In this sense, our work was focused on developing a model based\n\non an ML approach to finding the input configuration, on multicore architectures, and\n\nresearching into new programming model implementations to exploit all the computing\n\nresources on heterogeneous architectures.\n\n9.1 Contributions\n\nThe first part was dedicated to the performance analysis and efficient program-\n\nming models of geophysics numerical kernels on multicore architectures (Chapter 2). We\n\npresented that the performance of numerical stencils if affected by the runtime parameters\n\n(Chapter 3). The challenge to find the optimal performance is related to searching in a\n\nlarge set of input configurations (number of threads, problem size, code optimization of\n\nlooping, scheduling, and implemented algorithms). We considered that the performance\n\nmeasures are not only related to the execution time and the speedup. Hardware counter\n\nevents (i.e., cache behavior) can also describe the performance of scientific applications,\n\nand understanding their correlations can help us to explain the architecture behavior.\n\nWe discussed that finding the optimal runtime set is quite difficult because there\n\nare several parameters that influence the performance measures. Thus, we introduced an\n\nML-based model to predict the application performance, to find the optimal input config-\n\nuration and to improve the performance of stencil applications on multicore architectures\n\n(Chapter 4). We presented that the ML-based model can be adapted according to the al-\n\ngorithm implementations (i.e., naive and space tiling) and the architectural features (i.e.,\n\nnumber of available hardware counters, and memory mode on many-core architectures).\n\nSince the prediction model have been trained, it can be used to find the input configu-\n\nration set to reach the optimal performance. The results of proposed ML model proved\n\nthat performance prediction of geophysics numerical kernels can be done by building a\n\nregression model with high accuracy (up to 99%), by using a tiny set of experiments (less\n\n\n\n118\n\nthan 1% of configuration set).\n\nThe second part was oriented to the performance analysis and efficient program-\n\nming models of geophysics numerical kernels on heterogeneous architectures (Chapter\n\n5). In this circumstances, we used a standard implementation of seismic wave propaga-\n\ntion, implemented in CUDA, to analyze the performance of the application (Chapter 6).\n\nAs we presented, the performance increments when we increment the number of available\n\naccelerators; but, there are other factors that also increase (data movement and commu-\n\nnications, unbalancing processing, and memory consumption), and it does not allow to\n\nreach an optimal performance. On the other hand, the CPU processors are only used to\n\nmanage the co-processing work, neither of them is used to stencil computing.\n\nTo exploit all the available computing power on heterogeneous architectures is\n\nnecessary to implement new programming models, beyond the standard implementations.\n\nIn this sense, we introduced a task-based implementation of a seismic wave propagation\n\nmodel (Chapter 7). This implementation allows a set of input parameters from a run-\n\ntime configuration set to improve the performance. We analyzed this implementation by\n\nchanging the input parameters: the type of processing units (CPU only, GPU only, and\n\nCPU/GPU hybrid), the memory consumption (related to data movement and communica-\n\ntions), the scheduling algorithms (related to cost of computing on CPU or GPU cores), and\n\nthe task size (related to the number of parallel task). This implementation is faster and\n\ngained a better performance when compared with standard implementations. The pro-\n\nposed heterogeneous task-based implementation can reach a performance improvement\n\nnear to 25 times when compared to multicore architectures.\n\n9.2 Future Work\n\nThis research can be extended in several ways, and we delight three possibilities\n\nas follows:\n\n\u2022 Exploring on new input configurations. We proposed an improvement of perfor-\n\nmance based on the finding of optimal input configuration at runtime. We proved\n\nthat in shared-memory programming the performance can be predicted by a trained\n\nML-based model, and this prediction helps to find the optimal performance. We\n\nused a set of available parameters in OpenMP defined by OMP_NUM_THREADS and\n\nOMP_SCHEDULE variables. We believe that other parameters can be included as in-\n\n\n\n119\n\nput variables of the ML-based model for performance prediction. These parameters\n\ncould be related to programming models (i.e., MPI-based, over-decomposition in\n\nvirtual processes), compiler options (i.e., optimization flags, SIMD vectorization),\n\nor architectural features (i.e., FPGAs, embedded systems).\n\n\u2022 Extending the ML-based model to heterogeneous architectures. The perfor-\n\nmance of our task-based implementation for the seismic wave propagation model is\n\naffected by an input configuration set. The different scheduling algorithms, the type\n\nof processing units and the variation in task size exhibited changes of application\n\nperformance. Making decisions, as choosing if computations run on GPU or CPU\n\ncores, depend on parameters like memory consumption. Expanding the possibilities\n\nin configuration runtime create a large set of input parameters. As we demonstrated,\n\nthis optimal input set can be found by an ML-based model. Then, one direction of\n\nfuture work could be to develop a prediction model based on available hardware\n\nperformance counters in heterogeneous architectures.\n\n\u2022 Developing a new model based on unsupervised ML algorithms. The main lim-\n\nitation of the proposed ML-based model is related to the time consumed in train-\n\ning and testing stages because we used a supervised method. We think that an\n\nunsupervised-based model would find the input set by auto-tuning techniques that\n\nconverge to optimal performance. In this context, we may address the research to-\n\nwards clustering methods, genetic algorithms, or neural networks as self-organizing\n\nmaps.\n\n\n\n120\n\nREFERENCES\n\nABDELKHALEK, R. \u00c9valuation des acc\u00e9l\u00e9rateurs de calcul GPGPU pour la\nmod\u00e9lisation sismique. Dissertation (Master) \u2014 ENSEIRB, Bordeaux, France, 2007.\n\nABDELKHALEK, R. et al. Fast seismic modeling and reverse time migration on a\ngpu cluster. In: 2009 International Conference on High Performance Computing\nSimulation. [S.l.: s.n.], 2009. p. 36\u201343.\n\nABDELKHALEK, R. et al. Fast seismic modeling and reverse time migration on\na graphics processing unit cluster. Concurrency and Computation: Practice and\nExperience, John Wiley &amp; Sons, Ltd, v. 24, n. 7, p. 739\u2013750, 2012.\n\nAGULLO, E. et al. LU factorization for accelerator-based systems. In: Proceedings\nof the 2011 9th IEEE/ACS International Conference on Computer Systems and\nApplications. Washington, DC, USA: IEEE Computer Society, 2011. (AICCSA \u201911), p.\n217\u2013224.\n\nAGULLO, E. et al. QR factorization on a multicore node enhanced with multiple GPU\naccelerators. In: 25th IEEE International Symposium on Parallel and Distributed\nProcessing, IPDPS 2011, Anchorage, Alaska, USA, 16-20 May, 2011 - Conference\nProceedings. [S.l.: s.n.], 2011. p. 932\u2013943.\n\nAUGONNET, C. et al. StarPU: A unified platform for task scheduling on heterogeneous\nmulticore architectures. Concurr. Comput. : Pract. Exper., John Wiley and Sons Ltd.,\nv. 23, n. 2, p. 187\u2013198, feb. 2011. ISSN 1532-0626.\n\nBACH, F. R.; JORDAN, M. I. Kernel independent component analysis. J. Mach. Learn.\nRes., JMLR.org, v. 3, p. 1\u201348, mar. 2003. ISSN 1532-4435.\n\nBALAJI, P. Opencl: the open computing language. In: . Programming Models for\nParallel Computing. MIT Press, 2015. p. 488\u2013. ISBN 9780262332248. Available from\nInternet:&lt;https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7352782>.\n\nBHUYAN, L. N.; YANG, Q.; AGRAWAL, D. P. Performance of multiprocessor\ninterconnection networks. Computer, v. 22, n. 2, p. 25\u201337, Feb 1989. ISSN 0018-9162.\n\nBLAKE, G.; DRESLINSKI, R. G.; MUDGE, T. A survey of multicore processors. IEEE\nSignal Processing Magazine, v. 26, n. 6, p. 26\u201337, November 2009. ISSN 1053-5888.\n\nBOILLOT, L. et al. Task-based programming for seismic imaging: Preliminary results.\nIn: High Performance Computing and Communications, 2014 IEEE 6th Intl Symp\non Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded\nSoftware and Syst (HPCC,CSS,ICESS), 2014 IEEE Intl Conf on. [S.l.: s.n.], 2014. p.\n1259\u20131266.\n\nBOITO, F. Z. et al. Automatic I/O scheduling algorithm selection for parallel file\nsystems. Concurrency and Computation: Practice and Experience, v. 28, n. 8, p.\n2457\u20132472, 2016. ISSN 1532-0634. Cpe.3606.\n\nBORDEAUX, U. de; CNRS; INRIA. StarPU Handbook. 2014.&lt;http://starpu.gforge.\ninria.fr/doc/starpu.pdf>.\n\nhttps://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7352782\nhttp://starpu.gforge.inria.fr/doc/starpu.pdf\nhttp://starpu.gforge.inria.fr/doc/starpu.pdf\n\n\n121\n\nBOSILCA, G. et al. Parsec: Exploiting heterogeneity to enhance scalability. Computing\nin Science Engineering, v. 15, n. 6, p. 36\u201345, Nov 2013. ISSN 1521-9615.\n\nBOSILCA, G. et al. Scalable dense linear algebra on heterogeneous hardware. Advances\nin Parallel Computing, 2013.\n\nBREUER, A.; HEINECKE, A.; BADER, M. Petascale local time stepping for the\nADER-DG finite element method. In: 2016 IEEE International Parallel and\nDistributed Processing Symposium, IPDPS 2016, Chicago, IL, USA, May 23-27,\n2016. [S.l.: s.n.], 2016. p. 854\u2013863.\n\nBUCHTY, R. et al. A survey on hardware-aware and heterogeneous computing on\nmulticore processors and accelerators. Concurrency and Computation: Practice and\nExperience, John Wiley &amp; Sons, Ltd, v. 24, n. 7, p. 663\u2013675, 2012. ISSN 1532-0634.\n\nCALANDRA, H. et al. Evaluation of successive CPUs/APUs/GPUs based on an OpenCL\nfinite difference stencil. In: Parallel, Distributed and Network-Based Processing\n(PDP), 2013 21st Euromicro International Conference on. [S.l.: s.n.], 2013. p.\n405\u2013409.\n\nCANTIELLO, P.; MARTINO, B. D.; MOSCATO, F. Compilers, techniques,\nand tools for supporting programming heterogeneous many multicore systems.\nIn: . Large Scale Network-Centric Distributed Systems. Wiley-IEEE\nPress, 2014. p. 31\u201351. ISBN 9781118640708. Available from Internet:&lt;https:\n//ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6674271>.\n\nCASTRO, M. et al. Energy efficient seismic wave propagation simulation on a low-power\nmanycore processor. In: Computer Architecture and High Performance Computing\n(SBAC-PAD), 2014 IEEE 26th International Symposium on. [S.l.: s.n.], 2014. p.\n57\u201364.\n\nCASTRO, M.; G\u00d3ES, L. F. W.; M\u00c9HAUT, J.-F. Adaptive thread mapping strategies for\ntransactional memory applications. Journal of Parallel and Distributed Computing,\nv. 74, n. 9, p. 2845 \u2013 2859, 2014. ISSN 0743-7315.\n\nCHRISTEN, M.; SCHENK, O.; BURKHART, H. Automatic code generation and\ntuning for stencil kernels on modern shared memory architectures. Comput. Sci.,\nSpringer-Verlag New York, Inc., v. 26, n. 3-4, p. 205\u2013210, jun. 2011.\n\nCHRISTEN, M.; SCHENK, O.; CUI, Y. Patus for convenient high-performance stencils:\nevaluation in earthquake simulations. In: Proceedings of the International Conference\non High Performance Computing, Networking, Storage and Analysis. [S.l.]: IEEE\nComputer Society Press, 2012. (SC \u201912), p. 11:1\u201311:10.\n\nCLARKE, L.; GLENDINNING, I.; HEMPEL, R. The mpi message passing interface\nstandard. In: DECKER, K. M.; REHMANN, R. M. (Ed.). Programming Environments\nfor Massively Parallel Distributed Systems. Basel: Birkh\u00e4user Basel, 1994. p.\n213\u2013218. ISBN 978-3-0348-8534-8.\n\nCORTES, C.; VAPNIK, V. Support-vector networks. Machine Learning, v. 20, n. 3, p.\n273\u2013297, 1995. ISSN 1573-0565.\n\nhttps://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6674271\nhttps://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6674271\n\n\n122\n\nCRONSIOE, J.; VIDEAU, B.; MARANGOZOVA-MARTIN, V. Boast: Bringing\noptimization through automatic source-to-source transformations. In: Embedded\nMulticore Socs (MCSoC), 2013 IEEE 7th International Symposium on. [S.l.: s.n.],\n2013. p. 129\u2013134.\n\nCRUZ, R. de la; ARAYA-POLO, M. Towards a multi-level cache performance model for\n3d stencil computation. Procedia Computer Science, v. 4, p. 2146 \u2013 2155, 2011. ISSN\n1877-0509.\n\nCRUZ, R. de la; ARAYA-POLO, M. Modeling stencil computations on modern hpc\narchitectures. In: . High Performance Computing Systems. Performance\nModeling, Benchmarking, and Simulation: 5th International Workshop, PMBS\n2014, New Orleans, LA, USA, November 16, 2014. Revised Selected Papers. Cham:\nSpringer International Publishing, 2015. p. 149\u2013171. ISBN 978-3-319-17248-4.\n\nDAGUM, L.; MENON, R. Openmp: An industry-standard api for shared-memory\nprogramming. IEEE Comput. Sci. Eng., IEEE Computer Society Press, Los Alamitos,\nCA, USA, v. 5, n. 1, p. 46\u201355, jan. 1998. ISSN 1070-9924.\n\nDATTA, K. et al. Optimization and performance modeling of stencil computations on\nmodern microprocessors. SIAM Rev., Society for Industrial and Applied Mathematics,\nPhiladelphia, PA, USA, v. 51, n. 1, p. 129\u2013159, feb. 2009. ISSN 0036-1445.\n\nDATTA, K. et al. Stencil computation optimization and auto-tuning on state-of-the-art\nmulticore architectures. In: Proceedings of the 2008 ACM/IEEE Conference on\nSupercomputing. Piscataway, NJ, USA: IEEE Press, 2008. (SC \u201908), p. 4:1\u20134:12. ISBN\n978-1-4244-2835-9.\n\nDATTA, K. et al. Auto-tuning stencil computations on multicore and accelerators. In:\n. Scientific Computing with Multicore and Accelerators. Taylor &amp; Francis\n\nGroup: CRC Press, 2010.\n\nDRUCKER, H. et al. Support vector regression machines. In: Advances in Neural\nInformation Processing Systems 9. [S.l.]: MIT Press, 1997. p. 155\u2013161.\n\nDUPROS, F. et al. Exploiting Intensive Multithreading for the Efficient Simulation of 3D\nSeismic Wave Propagation. In: Proceedings of the 11th IEEE CSE\u201908, International\nConference on Computational Science and Engineering. S\u00e3o Paulo, Brazil: [s.n.],\n2008. p. 253\u2013260.\n\nDUPROS, F. et al. Communication-avoiding seismic numerical kernels on multicore\nprocessors. In: High Performance Computing and Communications (HPCC), 2015\nIEEE 7th International Symposium on Cyberspace Safety and Security (CSS), 2015\nIEEE 12th International Conferen on Embedded Software and Systems (ICESS),\n2015 IEEE 17th International Conference on. [S.l.: s.n.], 2015. p. 330\u2013335.\n\nDUPROS, F.; DO, H.; AOCHI, H. On scalability issues of the elastodynamics\nequations on multicore platforms. In: Proceedings of the International Conference on\nComputational Science, ICCS 2013, Barcelona, Spain, 5-7 June, 2013. [S.l.: s.n.],\n2013. p. 1226\u20131234.\n\n\n\n123\n\nDURAN, A. et al. Ompss: A proposal for programming heterogeneous multi-core\narchitectures. Parallel Processing Letters, v. 21, n. 02, p. 173\u2013193, 2011.\n\nDURSUN, H. et al. A Multilevel Parallelization Framework for High-Order Stencil\nComputations. In: Euro-Par 2009 Parallel Processing, 15th International Euro-Par\nConference. Delft, The Netherlands: [s.n.], 2009. p. 642\u2013653.\n\nELANGOVAN, V. K.; BADIA, R. M.; PARRA, E. A. Ompss-opencl programming\nmodel for heterogeneous systems. In: . Languages and Compilers for Parallel\nComputing: 25th International Workshop, LCPC 2012, Tokyo, Japan, September\n11-13, 2012, Revised Selected Papers. Berlin, Heidelberg: Springer Berlin Heidelberg,\n2013. p. 96\u2013111. ISBN 978-3-642-37658-0.\n\nEOM, H. et al. Machine learning-based runtime scheduler for mobile offloading\nframework. In: Utility and Cloud Computing (UCC), 2013 IEEE/ACM 6th\nInternational Conference on. [S.l.: s.n.], 2013. p. 17\u201325.\n\nFERN\u00c1NDEZ, A. et al. Task-based programming with ompss and its application. In:\n. Euro-Par 2014: Parallel Processing Workshops: Euro-Par 2014 International\n\nWorkshops, Porto, Portugal, August 25-26, 2014, Revised Selected Papers, Part II.\nCham: Springer International Publishing, 2014. p. 601\u2013612. ISBN 978-3-319-14313-2.\n\nGAN, L. et al. Scaling and analyzing the stencil performance on multi-core and\nmany-core architectures. In: 2014 20th IEEE International Conference on Parallel\nand Distributed Systems (ICPADS). [S.l.: s.n.], 2014. p. 103\u2013110. ISSN 1521-9097.\n\nGANAPATHI, A. et al. A case for machine learning to optimize multicore performance.\nIn: Proceedings of the First USENIX Conference on Hot Topics in Parallelism.\nBerkeley, CA, USA: USENIX Association, 2009. (HotPar\u201909), p. 1\u20131.\n\nGANAPATHI, A. S. Predicting and Optimizing System Utilization and Performance\nvia Statistical Machine Learning. Thesis (PhD) \u2014 EECS Department, University of\nCalifornia, Berkeley, Dec 2009.\n\nGAUTIER, T. et al. Xkaapi: A runtime system for data-flow task programming on\nheterogeneous architectures. In: 27th IEEE International Symposium on Parallel and\nDistributed Processing, IPDPS 2013, Cambridge, MA, USA, May 20-24, 2013. [S.l.:\ns.n.], 2013. p. 1299\u20131308.\n\nGONZALEZ, R. C.; WOODS, R. E. Digital Image Processing (2nd Edition). Upper\nSaddle River, NJ, USA: Prentice-Hall, Inc., 2002. ISBN 9780201180756.\n\nGREEN500. Green 500 Supercomputer Sites. 2017.&lt;http://www.green500.org>.\n\nHARRIS, M. Maxwell: The Most Advanced CUDA GPU Ever Made. 2014.\n<https://devblogs.nvidia.com/maxwell-most-advanced-cuda-gpu-ever-made/>.\n\nHASSEN, S. B.; BAL, H. E.; JACOBS, C. J. H. A task- and data-parallel programming\nlanguage based on shared objects. ACM Trans. Program. Lang. Syst., ACM, New\nYork, NY, USA, v. 20, n. 6, p. 1131\u20131170, nov. 1998. ISSN 0164-0925. Available from\nInternet:&lt;http://doi.acm.org/10.1145/295656.295658>.\n\nhttp://www.green500.org\nhttps://devblogs.nvidia.com/maxwell-most-advanced-cuda-gpu-ever-made/\nhttp://doi.acm.org/10.1145/295656.295658\n\n\n124\n\nHWU, W. W. et al. Compiler technology for future microprocessors. Proceedings of the\nIEEE, v. 83, n. 12, p. 1625\u20131640, Dec 1995. ISSN 0018-9219.\n\nINC., T. K. G. OpenCL Reference Pages. 2018.&lt;http://man.opencl.org/>. Accessed:\n2018-01-01.\n\nINTEL. OpenMP* Loop Scheduling. 2014.&lt;https://software.intel.com/en-us/articles/\nopenmp-loop-scheduling>. [Online; accessed 11-jan-2018].\n\nINTEL. OpenMP Loop Scheduling. 2016.&lt;https://software.intel.com/en-us/articles/\nopenmp-loop-scheduling>. Accessed: 2016-01-01.\n\nINTEL. Intel R\u00a9 C++ Compiler 17.0 Developer Guide and Reference. 2018.\n<https://software.intel.com/sites/default/files/managed/08/ac/PDF_CPP_Compiler_UG_\n17_0.pdf>. [Online; accessed 1-march-2018].\n\nJAIN, A. K.; DUIN, R. P. W.; MAO, J. Statistical pattern recognition: a review. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, v. 22, n. 1, p. 4\u201337, Jan\n2000. ISSN 0162-8828.\n\nKAMIL, S. et al. An auto-tuning framework for parallel multicore stencil computations.\nIn: Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium\non. [S.l.: s.n.], 2010. p. 1\u201312. ISSN 1530-2075.\n\nKRAKIWSKY, S. E.; TURNER, L. E.; OKONIEWSKI, M. M. Graphics processor unit\n(GPU) acceleration of finite-difference time-domain (FDTD) algorithm. In: ISCAS (5).\n[S.l.: s.n.], 2004. p. 265\u2013268.\n\nKRISHNAN, S. P. T.; VEERAVALLI, B. Performance characterization and evaluation\nof hpc algorithms on dissimilar multicore architectures. In: 2014 IEEE Intl Conf on\nHigh Performance Computing and Communications, 2014 IEEE 6th Intl Symp on\nCyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software\nand Syst (HPCC,CSS,ICESS). [S.l.: s.n.], 2014. p. 1288\u20131295.\n\nKUMAR, M. S.; BALAMURUGAN, B. A review on performance evaluation techniques\nin cloud. In: 2017 Second International Conference on Recent Trends and Challenges\nin Computational Models (ICRTCCM). [S.l.: s.n.], 2017. p. 19\u201324.\n\nLACOSTE, X. et al. Taking advantage of hybrid systems for sparse direct solvers via\ntask-based runtimes. In: 2014 IEEE International Parallel &amp; Distributed Processing\nSymposium Workshops, Phoenix, AZ, USA, May 19-23, 2014. [S.l.: s.n.], 2014. p.\n29\u201338.\n\nLI, Y. et al. Capes: Unsupervised storage performance tuning using neural network-based\ndeep reinforcement learning. In: Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and Analysis. New York, NY,\nUSA: ACM, 2017. (SC \u201917), p. 42:1\u201342:14. ISBN 978-1-4503-5114-0. Available from\nInternet:&lt;http://doi.acm.org/10.1145/3126908.3126951>.\n\nLINDHOLM, E. et al. Nvidia tesla: A unified graphics and computing architecture.\nIEEE Micro, v. 28, n. 2, p. 39\u201355, March 2008. ISSN 0272-1732.\n\nhttp://man.opencl.org/\nhttps://software.intel.com/en-us/articles/openmp-loop-scheduling\nhttps://software.intel.com/en-us/articles/openmp-loop-scheduling\nhttps://software.intel.com/en-us/articles/openmp-loop-scheduling\nhttps://software.intel.com/en-us/articles/openmp-loop-scheduling\nhttps://software.intel.com/sites/default/files/managed/08/ac/PDF_CPP_Compiler_UG_17_0.pdf\nhttps://software.intel.com/sites/default/files/managed/08/ac/PDF_CPP_Compiler_UG_17_0.pdf\nhttp://doi.acm.org/10.1145/3126908.3126951\n\n\n125\n\nMACHADO, R. S. et al. Comparing performance of c compilers optimizations on\ndifferent multicore architectures. In: 2017 International Symposium on Computer\nArchitecture and High Performance Computing Workshops (SBAC-PADW). [S.l.:\ns.n.], 2017. p. 25\u201330.\n\nMAGNI, A.; DUBACH, C.; O\u2019BOYLE, M. Automatic optimization of thread-coarsening\nfor graphics processors. In: Proceedings of the 23rd International Conference\non Parallel Architectures and Compilation. New York, NY, USA: ACM, 2014.\n(PACT \u201914), p. 455\u2013466. ISBN 978-1-4503-2809-8. Available from Internet:\n<http://doi.acm.org/10.1145/2628071.2628087>.\n\nMALAS, T. M. et al. Multicore-optimized wavefront diamond blocking for optimizing\nstencil updates. SIAM J. Scientific Computing, v. 37, n. 4, 2015.\n\nMARTINEZ, V. et al. Stencil-based applications tuning for multi-core. In: Latin\nAmerican High Performance Computing Conference (CARLA 2016). [S.l.: s.n.],\n2016. p. 1\u201315. Oral presentation.\n\nMARTINEZ, V. et al. Performance improvement of stencil computations for multi-core\narchitectures based on machine learning. Procedia Computer Science, v. 108, p. 305 \u2013\n314, 2017. ISSN 1877-0509. International Conference on Computational Science, ICCS\n2017, 12-14 June 2017, Zurich, Switzerland.\n\nMARTINEZ, V. et al. Task-based programming on low-power nvidia jetson tk1\nmanycore architecture: Application to earthquake modeling. In: Latin American High\nPerformance Computing Conference (CARLA 2015). [S.l.: s.n.], 2015. p. 1\u201311. Oral\npresentation.\n\nMARTINEZ, V. et al. Towards seismic wave modeling on heterogeneous many-core\narchitectures using task-based runtime system. In: Computer Architecture and High\nPerformance Computing (SBAC-PAD), 2015 27th International Symposium on.\n[S.l.: s.n.], 2015. p. 1\u20138. ISSN 1550-6533.\n\nMARTINS, L. de O. et al. Accelerating curvature estimate in 3d seismic data using\nGPGPU. In: 26th IEEE International Symposium on Computer Architecture and\nHigh Performance Computing, SBAC-PAD 2014, Paris, France, October 22-24,\n2014. [S.l.: s.n.], 2014. p. 105\u2013111.\n\nMERCERAT, D.; GUILLOT, L.; VILOTTE, J.-P. Application of the Parareal Algorithm\nfor Acoustic Wave Propagation. In: Simos, T. E.; Psihoyios, G.; Tsitouras, C. (Ed.).\nAmerican Institute of Physics Conference Series. [S.l.: s.n.], 2009. (American Institute\nof Physics Conference Series, v. 1168), p. 1521\u20131524.\n\nMEYER, D. et al. e1071: Misc Functions of the Department of Statistics, Probability\nTheory Group (Formerly: E1071), TU Wien. [S.l.], 2015. R package version 1.6-7.\n\nMICH\u00c9A, D.; KOMATITSCH, D. Accelerating a 3D finite-difference wave propagation\ncode using GPU graphics cards. Geophysical Journal International, Blackwell\nPublishing Ltd, v. 182, n. 1, p. 389\u2013402, 2010.\n\nMICIKEVICIUS, P. 3D finite-difference computation on GPUs using CUDA.\nIn: Workshop on General Purpose Processing on Graphics Processing Units.\nWashington, USA: ACM, 2009. p. 79\u201384.\n\nhttp://doi.acm.org/10.1145/2628071.2628087\n\n\n126\n\nMIJAKOVIC, R.; FIRBACH, M.; GERNDT, M. An architecture for flexible auto-tuning:\nThe periscope tuning framework 2.0. In: International Conference on Green High\nPerformance Computing (ICGHPC). [S.l.: s.n.], 2016. p. 1\u20139.\n\nMITTAL, S. A survey of techniques for architecting and managing asymmetric multicore\nprocessors. ACM Comput. Surv., ACM, New York, NY, USA, v. 48, n. 3, p. 45:1\u201345:38,\nfeb. 2016. ISSN 0360-0300.\n\nMITTAL, S.; VETTER, J. S. A survey of cpu-gpu heterogeneous computing techniques.\nACM Comput. Surv., ACM, New York, NY, USA, v. 47, n. 4, p. 69:1\u201369:35, jul. 2015.\nISSN 0360-0300.\n\nMOCZO, P.; ROBERTSSON, J.; EISNER, L. The finite-difference time-domain method\nfor modeling of seismic wave propagation. In: Advances in Wave Propagation\nin Heterogeneous Media. [S.l.]: Elsevier - Academic Press, 2007, (Advances in\nGeophysics, v. 48). chp. 8, p. 421\u2013516.\n\nMUCCI, P. J. et al. Papi: A portable interface to hardware performance counters. In: In\nProceedings of the Department of Defense HPCMP Users Group Conference. [S.l.:\ns.n.], 1999. p. 7\u201310.\n\nMULLER, K. R. et al. An introduction to kernel-based learning algorithms. IEEE\nTransactions on Neural Networks, v. 12, n. 2, p. 181\u2013201, Mar 2001. ISSN 1045-9227.\n\nNGUYEN, A. et al. 3.5-d blocking optimization for stencil computations on modern\ncpus and gpus. In: 2010 ACM/IEEE International Conference for High Performance\nComputing, Networking, Storage and Analysis. [S.l.: s.n.], 2010. p. 1\u201313. ISSN\n2167-4329.\n\nNVIDIA. Whitepaper. NVIDIA\u2019s Next Generation CUDATMCompute Architecture:\nFermiTM. 2009.&lt;http://www.nvidia.com/content/pdf/fermi_white_papers/nvidia_fermi_\ncompute_architecture_whitepaper.pdf>.\n\nNVIDIA. NVIDIA: Jetson TK1 Development Kit (Specification). 2014.\n<http://developer.download.nvidia.com/embedded/jetson/TK1/docs/3_HWDesignDev/\nJTK1_DevKit_Specification.pdf>.\n\nNVIDIA. Whitepaper. NVIDIA\u2019s Next Generation CUDATMCompute Architecture:\nKeplerTMGK110/210. 2014.&lt;https://www.nvidia.com/content/dam/en-zz/Solutions/\nData-Center/documents/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.\npdf>.\n\nNVIDIA. CUDA Toolkit Documentation. 2016.&lt;http://docs.nvidia.com/cuda/index.\nhtml>. Accessed: 2016-01-01.\n\nNVIDIA. Whitepaper. NVIDIA Tesla P100. The Most Advanced Data-\ncenter Accelerator Ever Built. Featuring Pascal GP100, the World\u2019s\nFastest GPU. 2017.&lt;http://images.nvidia.com/content/pdf/tesla/whitepaper/\npascal-architecture-whitepaper-v1.2.pdf>.\n\nPATERSON, D. The Top 10 Innovations in the New NVIDIA Fermi Architecture,\nand the Top 3 Next Challenges. 2009.&lt;http://www.nvidia.com.br/content/PDF/fermi_\nwhite_papers/D.Patterson_Top10InnovationsInNVIDIAFermi.pdf>.\n\nhttp://www.nvidia.com/content/pdf/fermi_white_papers/nvidia_fermi_compute_architecture_whitepaper.pdf\nhttp://www.nvidia.com/content/pdf/fermi_white_papers/nvidia_fermi_compute_architecture_whitepaper.pdf\nhttp://developer.download.nvidia.com/embedded/jetson/TK1/docs/3_HWDesignDev/JTK1_DevKit_Specification.pdf\nhttp://developer.download.nvidia.com/embedded/jetson/TK1/docs/3_HWDesignDev/JTK1_DevKit_Specification.pdf\nhttps://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf\nhttps://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf\nhttps://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf\nhttp://docs.nvidia.com/cuda/index.html\nhttp://docs.nvidia.com/cuda/index.html\nhttp://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper-v1.2.pdf\nhttp://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper-v1.2.pdf\nhttp://www.nvidia.com.br/content/PDF/fermi_white_papers/D.Patterson_Top10InnovationsInNVIDIAFermi.pdf\nhttp://www.nvidia.com.br/content/PDF/fermi_white_papers/D.Patterson_Top10InnovationsInNVIDIAFermi.pdf\n\n\n127\n\nPEREZ, J. M.; BADIA, R. M.; LABARTA, J. A dependency-aware task-based\nprogramming environment for multi-core architectures. In: 2008 IEEE International\nConference on Cluster Computing. [S.l.: s.n.], 2008. p. 142\u2013151. ISSN 1552-5244.\n\nPEREZ, J. M.; BADIA, R. M.; LABARTA, J. Handling task dependencies under strided\nand aliased references. In: Proceedings of the 24th ACM International Conference\non Supercomputing. New York, NY, USA: ACM, 2010. (ICS \u201910), p. 263\u2013274. ISBN\n978-1-4503-0018-6.\n\nPUSUKURI, K. K.; GUPTA, R.; BHUYAN, L. N. Adapt: A framework for coscheduling\nmultithreaded programs. ACM Trans. Archit. Code Optim., ACM, New York, NY,\nUSA, v. 9, n. 4, p. 45:1\u201345:24, jan. 2013. ISSN 1544-3566.\n\nRAHMAN, S. M. F.; YI, Q.; QASEM, A. Understanding stencil code performance\non multicore architectures. In: Proceedings of the 8th ACM International\nConference on Computing Frontiers. New York, NY, USA: ACM, 2011.\n(CF \u201911), p. 30:1\u201330:10. ISBN 978-1-4503-0698-0. Available from Internet:\n<http://doi.acm.org/10.1145/2016604.2016641>.\n\nRAI, J. K. et al. On prediction accuracy of machine learning algorithms for characterizing\nshared l2 cache behavior of programs on multicore processors. In: Computational\nIntelligence, Communication Systems and Networks, 2009. CICSYN \u201909. First\nInternational Conference on. [S.l.: s.n.], 2009. p. 213\u2013219.\n\nROSALES, C. et al. Performance prediction of hpc applications on intel processors.\nIn: 2017 IEEE International Parallel and Distributed Processing Symposium\nWorkshops (IPDPSW). [S.l.: s.n.], 2017. p. 1325\u20131332.\n\nROTEN, D. et al. High-frequency nonlinear earthquake simulations on petascale\nheterogeneous supercomputers. In: Proceedings of the International Conference for\nHigh Performance Computing, Networking, Storage and Analysis, SC 2016, Salt\nLake City, UT, USA, November 13-18, 2016. [S.l.: s.n.], 2016. p. 957\u2013968.\n\nSAMUEL, A. L. Some studies in machine learning using the game of checkers. IBM\nJournal of Research and Development, v. 3, n. 3, p. 210\u2013229, July 1959. ISSN\n0018-8646.\n\nSANTANDER, U. I. de. Super Computaci\u00f3n y C\u00e1lculo Cient\u00edfico UIS. 2015.\n<http://www.sc3.uis.edu.co/servicios/hardware/>.\n\nSAXENA, G.; JIMACK, P. K.; WALKLEY, M. A. A cache-aware approach to domain\ndecomposition for stencil-based codes. In: 2016 International Conference on High\nPerformance Computing Simulation (HPCS). [S.l.: s.n.], 2016. p. 875\u2013885.\n\nSERPA, M. S. et al. Strategies to improve the performance of a geophysics model\nfor different manycore systems. In: 2017 International Symposium on Computer\nArchitecture and High Performance Computing Workshops (SBAC-PADW). [S.l.:\ns.n.], 2017. p. 49\u201354.\n\nSHUKLA, S. K.; MURTHY, C.; CHANDE, P. K. A survey of approaches used in\nparallel architectures and multi-core processors, for performance improvement. In: .\nProgress in Systems Engineering: Proceedings of the Twenty-Third International\n\nhttp://doi.acm.org/10.1145/2016604.2016641\nhttp://www.sc3.uis.edu.co/servicios/hardware/\n\n\n128\n\nConference on Systems Engineering. Cham: Springer International Publishing, 2015.\np. 537\u2013545. ISBN 978-3-319-08422-0.\n\nSODANI, A. et al. Knights landing: Second-generation intel xeon phi product. IEEE\nMicro, v. 36, n. 2, p. 34\u201346, Mar 2016. ISSN 0272-1732.\n\nSPAZIER, J.; CHRISTGAU, S.; SCHNOR, B. Efficient parallelization of matlab\nstencil applications for multi-core clusters. In: 2016 Sixth International Workshop on\nDomain-Specific Languages and High-Level Frameworks for High Performance\nComputing (WOLFHPC). [S.l.: s.n.], 2016. p. 20\u201329.\n\nSTALLMAN, R. M.; COMMUNITY the G. D. Using the GNU Compiler Collection.\nFor GCC version 6.4.0. 2017.&lt;https://gcc.gnu.org/onlinedocs/gcc-6.4.0/gcc.pdf>.\n[Online; accessed 1-dec-2017].\n\nSTENGEL, H. et al. Quantifying performance bottlenecks of stencil computations\nusing the execution-cache-memory model. In: Proceedings of the 29th ACM\non International Conference on Supercomputing. New York, NY, USA: ACM,\n2015. (ICS \u201915), p. 207\u2013216. ISBN 978-1-4503-3559-1. Available from Internet:\n<http://doi.acm.org/10.1145/2751205.2751240>.\n\nSTOJANOVIC, S. et al. An overview of selected hybrid and reconfigurable architectures.\nIn: Industrial Technology (ICIT), 2012 IEEE International Conference on. [S.l.:\ns.n.], 2012. p. 444\u2013449.\n\nSUKHIJA, N. et al. Portfolio-based selection of robust dynamic loop scheduling\nalgorithms using machine learning. In: Parallel Distributed Processing Symposium\nWorkshops (IPDPSW), 2014 IEEE International. [S.l.: s.n.], 2014. p. 1638\u20131647.\n\nTANG, Y. et al. The pochoir stencil compiler. In: ACM Symposium on Parallelism\nin Algorithms and Architectures. New York, NY, USA: ACM, 2011. (SPAA \u201911), p.\n117\u2013128. ISBN 978-1-4503-0743-7.\n\nTOP500. Top500 Supercomputer Sites. 2017.&lt;http://www.top500.org>.\n\nTSUBOI, S. et al. A 1.8 trillion degrees-of-freedom, 1.24 petaflops global seismic wave\nsimulation on the K computer. IJHPCA, v. 30, n. 4, p. 411\u2013422, 2016.\n\nVAPNIK, V. N. An overview of statistical learning theory. IEEE Transactions on\nNeural Networks, v. 10, n. 5, p. 988\u2013999, Sep 1999. ISSN 1045-9227.\n\nVASUDEVAN, R.; VADHIYAR, S. S.; KAL\u00c9, L. V. G-charm: an adaptive runtime\nsystem for message-driven parallel applications on hybrid systems. In: ICS 2013 -\nProceedings of the 2013 ACM International Conference on Supercomputing. [S.l.:\ns.n.], 2013. p. 349\u2013358.\n\nVIDEAU, B. et al. Boast. Int. J. High Perform. Comput. Appl., Sage Publications, Inc.,\nThousand Oaks, CA, USA, v. 32, n. 1, p. 28\u201344, jan. 2018. ISSN 1094-3420. Available\nfrom Internet:&lt;https://doi.org/10.1177/1094342017718068>.\n\nVILELA, R. F. Perfilagem do Problema de Resolu\u00e7\u00e3o da Equa\u00e7\u00e3o\nda Onda por Diferen\u00e7as Finitas em Coprocessador Xeon Phi.\nhttp://www.pee.ufrj.br/index.php/pt/producao-academica/dissertacoes-de-\nmestrado/2017/2016033170\u201353/file: [s.n.], 2017. Disserta\u00e7\u00e3o (mestrado).\n\nhttps://gcc.gnu.org/onlinedocs/gcc-6.4.0/gcc.pdf\nhttp://doi.acm.org/10.1145/2751205.2751240\nhttp://www.top500.org\nhttps://doi.org/10.1177/1094342017718068\n\n\n129\n\nVIRIEUX, J. P-SV wave propagation in heterogeneous media; velocity-stress\nfinite-difference method. Geophysics, v. 51, n. 4, p. 889\u2013901, 1986.\n\nVLADUIC, D.; CERNIVEC, A.; SLIVNIK, B. Improving job scheduling in grid\nenvironments with use of simple machine learning methods. In: International\nConference on Information Technology: New Generations. [S.l.: s.n.], 2009. p.\n177\u2013182.\n\nWANG, Z.; O\u2019BOYLE, M. F. P. Mapping parallelism to multi-cores: A machine\nlearning based approach. In: Proceedings of the 14th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming. New York, NY, USA: ACM, 2009.\n(PPoPP \u201909), p. 75\u201384. ISBN 978-1-60558-397-6.\n\nWENG, L.; LIU, C.; GAUDIOT, J.-L. Scheduling optimization in multicore\nmultithreaded microprocessors through dynamic modeling. In: Proceedings of the ACM\nInternational Conference on Computing Frontiers. New York, NY, USA: ACM, 2013.\n(CF \u201913), p. 5:1\u20135:10. ISBN 978-1-4503-2053-5.\n\nWILLIAMS, S.; WATERMAN, A.; PATTERSON, D. Roofline: An insightful visual\nperformance model for multicore architectures. Commun. ACM, ACM, New York,\nNY, USA, v. 52, n. 4, p. 65\u201376, abr. 2009. ISSN 0001-0782. Available from Internet:\n<http://doi.acm.org/10.1145/1498765.1498785>.\n\nXU, R.; WUNSCH, D. Survey of clustering algorithms. IEEE Transactions on Neural\nNetworks, v. 16, n. 3, p. 645\u2013678, May 2005. ISSN 1045-9227.\n\nZHU, X. et al. Analyzing mpi-3.0 process-level shared memory: A case study with\nstencil computations. In: 2015 15th IEEE/ACM International Symposium on Cluster,\nCloud and Grid Computing. [S.l.: s.n.], 2015. p. 1099\u20131106.\n\nhttp://doi.acm.org/10.1145/1498765.1498785\n\n\tAcknowledgment\n\tAbstract\n\tResumo\n\tList of Abbreviations and Acronyms\n\tList of Figures\n\tList of Tables\n\tContents\n\t1 Introduction\n\t1.1 Research issues\n\t1.1.1 Exploiting multicore architectures\n\t1.1.2 Machine Learning approaches on HPC platforms\n\t1.1.3 Heterogeneous architectures\n\t1.1.4 Research context\n\n\t1.2 Objectives and contributions\n\t1.3 Outline\n\n\t2 Multicore architectures and programming models\n\t2.1 Parallelism\n\t2.2 Multicore architectures\n\t2.2.1 Manycore architectures\n\n\t2.3 Programming models on HPC architectures\n\t2.3.1 Message Passing Interface\n\t2.3.2 Shared-Memory programming\n\t2.3.3 Impact of compilers\n\n\t2.4 Performance evaluation\n\t2.4.1 Hardware performance counters\n\t2.4.2 Roofline model\n\n\t2.5 Target machines\n\t2.6 Concluding remarks\n\n\t3 Numerical background: Geophysical Kernels on Multicore Platforms\n\t3.1 Stencil applications\n\t3.1.1 7-point Jacobi stencil\n\t3.1.2 Seismic wave propagation stencil\n\t3.1.3 Acoustic wave propagation stencil\n\n\t3.2 Standard implementations of numerical stencil\n\t3.2.1 Na\u00efve\n\t3.2.2 Space Tiling\n\n\t3.3 Performance charaterization of numerical stencils\n\t3.3.1 Scalability\n\n\t3.4 Concluding remarks\n\n\t4 Machine Learning Strategy for Performance Improvement on Multicore Architectures\n\t4.1 Performance improvement by Machine Learning models\n\t4.2 General model for performance prediction\n\t4.2.1 Architecture of geophysics prediction model\n\t4.2.2 Performance prediction on multicore architectures\n\t4.2.3 Performance prediction on manycore architectures\n\n\t4.3 Concluding remarks\n\n\t5 Heterogeneous Architectures and Programming Models\n\t5.1 Streaming Multiprocessors\n\t5.2 Programming models on heterogeneous architectures\n\t5.2.1 OpenCL programming model\n\n\t5.3 Evolution of NVIDIA GPU Architectures\n\t5.4 Assymetric low power architectures\n\t5.5 Target machines\n\t5.6 Concluding remarks\n\n\t6 Numerical Implementation of Geophysics Stencils on Heterogeneous Platforms\n\t6.1 Setup and Performance Measurement\n\t6.2 Elapsed Time\n\t6.3 GPU Load and Memory Usage\n\t6.4 Concluding Remarks\n\n\t7 Task-based Approach for Performance Improvement on Heterogeneous Platforms\n\t7.1 Runtime systems for task-based programming\n\t7.2 StarPU runtime system\n\t7.3 Elastodynamics over runtime system\n\t7.4 Experiments\n\t7.4.1 Scheduling strategies\n\t7.4.2 Size of the block\n\t7.4.3 In-core dataset\n\t7.4.4 Out-of-core dataset\n\n\t7.5 Summary of runtime parameters\n\t7.6 Task-based implementation for energy efficiency\n\t7.6.1 Computing time\n\t7.6.2 Energy efficiency\n\n\t7.7 Concluding remarks\n\n\t8 Related Work\n\t8.1 Perfomance improvement of stencil applications on multicore architectures\n\t8.2 Advanced optimizations, low-level and auto-tuning strategies\n\t8.3 Machine Learning approaches\n\t8.4 Heterogeneous computing\n\n\t9 Conclusion and Perspectives\n\t9.1 Contributions\n\t9.2 Future Work\n\n\tReferences"}]}}}